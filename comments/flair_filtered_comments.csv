Version,Commit Message
v0.15.1,mmap seems to be much more memory efficient
v0.15.1,Remove quotes from etag
v0.15.1,"If there is an etag, it's everything after the first period"
v0.15.1,"Otherwise, use None"
v0.15.1,"URL, so get it from the cache (downloading if necessary)"
v0.15.1,"File, and it exists."
v0.15.1,"File, but it doesn't exist."
v0.15.1,Something unknown
v0.15.1,Extract all the contents of zip file in current directory
v0.15.1,use model name as subfolder
v0.15.1,Lazy import
v0.15.1,output information
v0.15.1,Extract all the contents of zip file in current directory
v0.15.1,TODO(joelgrus): do we want to do checksums or anything like that?
v0.15.1,get cache path to put the file
v0.15.1,make HEAD request to check ETag
v0.15.1,add ETag to filename if it exists
v0.15.1,"etag = response.headers.get(""ETag"")"
v0.15.1,"Download to temporary file, then copy to cache dir once finished."
v0.15.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.15.1,GET file object
v0.15.1,These defaults are the same as the argument defaults in tqdm.
v0.15.1,load_big_file is a workaround byhttps://github.com/highway11git
v0.15.1,to load models on some Mac/Windows setups
v0.15.1,see https://github.com/zalandoresearch/flair/issues/351
v0.15.1,first determine the distribution of classes in the dataset
v0.15.1,weight for each sample
v0.15.1,Create blocks
v0.15.1,shuffle the blocks
v0.15.1,concatenate the shuffled blocks
v0.15.1,Create blocks
v0.15.1,shuffle the blocks
v0.15.1,concatenate the shuffled blocks
v0.15.1,Build the regular expression pattern dynamically based on the provided symbols
v0.15.1,This will match any character from the symbols list that doesn't have spaces around it
v0.15.1,"Add space before and after symbols, where necessary"
v0.15.1,Ensure that we are adding a space only if there isn't one already
v0.15.1,increment for last token in sentence if not followed by whitespace
v0.15.1,this is the default init size of a lmdb database for embeddings
v0.15.1,get db filename from embedding name
v0.15.1,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.15.1,SequenceTagger
v0.15.1,TextClassifier
v0.15.1,get db filename from embedding name
v0.15.1,if embedding database already exists
v0.15.1,"otherwise, push embedding to database"
v0.15.1,if embedding database already exists
v0.15.1,open the database in read mode
v0.15.1,we need to set self.k
v0.15.1,create and load the database in write mode
v0.15.1,"no idea why, but we need to close and reopen the environment to avoid"
v0.15.1,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.15.1,when opening new transaction !
v0.15.1,init dictionaries
v0.15.1,"in order to deal with unknown tokens, add <unk>"
v0.15.1,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.15.1,set 'add_unk' depending on whether <unk> is a key
v0.15.1,"if one embedding name, directly return it"
v0.15.1,"if multiple embedding names, concatenate them"
v0.15.1,First we remove any existing labels for this PartOfSentence in self.sentence
v0.15.1,labels also need to be deleted at Sentence object
v0.15.1,delete labels at object itself
v0.15.1,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.15.1,"therefore, labels get added only to the Sentence if it exists"
v0.15.1,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.15.1,"Therefore, labels get set only to the Sentence if it exists"
v0.15.1,"check if the span already exists. If so, return it"
v0.15.1,else make a new span
v0.15.1,"check if the relation already exists. If so, return it"
v0.15.1,else make a new relation
v0.15.1,private field for all known spans
v0.15.1,the tokenizer used for this sentence
v0.15.1,some sentences represent a document boundary (but most do not)
v0.15.1,internal variables to denote position inside dataset
v0.15.1,"if text is passed, instantiate sentence with tokens (words)"
v0.15.1,determine token positions and whitespace_after flag
v0.15.1,the last token has no whitespace after
v0.15.1,log a warning if the dataset is empty
v0.15.1,data with zero-width characters cannot be handled
v0.15.1,set token idx and sentence
v0.15.1,append token to sentence
v0.15.1,register token annotations on sentence
v0.15.1,move sentence embeddings to device
v0.15.1,also move token embeddings to device
v0.15.1,clear token embeddings
v0.15.1,infer whitespace after field
v0.15.1,"if sentence has no tokens, return empty string"
v0.15.1,"otherwise, return concatenation of tokens with the correct offsets"
v0.15.1,The sentence's start position is not propagated to its tokens.
v0.15.1,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.15.1,No character at the corresponding code point: remove it
v0.15.1,"if no label if specified, return all labels"
v0.15.1,"if the label type exists in the Sentence, return it"
v0.15.1,return empty list if none of the above
v0.15.1,labels also need to be deleted at all tokens
v0.15.1,labels also need to be deleted at all known spans
v0.15.1,remove spans without labels
v0.15.1,delete labels at object itself
v0.15.1,set name
v0.15.1,abort if no data is provided
v0.15.1,sample test data from train if none is provided
v0.15.1,sample dev data from train if none is provided
v0.15.1,set train dev and test data
v0.15.1,find out empty sentence indices
v0.15.1,create subset of non-empty sentence indices
v0.15.1,find out empty sentence indices
v0.15.1,create subset of non-empty sentence indices
v0.15.1,"first, determine the datapoint type by going through dataset until first label is found"
v0.15.1,count all label types per sentence
v0.15.1,go through all labels of label_type and count values
v0.15.1,special handling for Token-level annotations. Add all untagged as 'O' label
v0.15.1,"if an unk threshold is set, UNK all label values below this threshold"
v0.15.1,sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
v0.15.1,replace the old label with the new one
v0.15.1,keep track of the old (clean) label using another label type category
v0.15.1,keep track of how many labels in total are flipped
v0.15.1,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.15.1,replace the old label with the new one
v0.15.1,keep track of the old (clean) label using another label type category
v0.15.1,keep track of how many labels in total are flipped
v0.15.1,"add a dummy ""O"" to close final prediction"
v0.15.1,return complex list
v0.15.1,internal variables
v0.15.1,non-set tags are OUT tags
v0.15.1,anything that is not OUT is IN
v0.15.1,does this prediction start a new span?
v0.15.1,B- and S- always start new spans
v0.15.1,"if the predicted class changes, I- starts a new span"
v0.15.1,"if the predicted class changes and S- was previous tag, start a new span"
v0.15.1,if an existing span is ended (either by reaching O or starting a new span)
v0.15.1,determine score and value
v0.15.1,append to result list
v0.15.1,reset for-loop variables for new span
v0.15.1,remember previous tag
v0.15.1,global variable: cache_root
v0.15.1,Get the device from the environment variable
v0.15.1,global variable: device
v0.15.1,"No need for correctness checks, torch is doing it"
v0.15.1,global variable: version
v0.15.1,global variable: arrow symbol
v0.15.1,Attach optimizer
v0.15.1,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.15.1,if storage mode option 'none' delete everything
v0.15.1,"if dynamic embedding keys not passed, identify them automatically"
v0.15.1,always delete dynamic embeddings
v0.15.1,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.15.1,add tokens before the entity
v0.15.1,add new entity tokens
v0.15.1,add any remaining tokens to a new chunk
v0.15.1,optional metric space decoder if prototypes have different length than embedding
v0.15.1,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.15.1,"if set, create initial prototypes from normal distribution"
v0.15.1,"if set, use a radius"
v0.15.1,all parameters will be pushed internally to the specified device
v0.15.1,decode embeddings into prototype space
v0.15.1,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.15.1,all parameters will be pushed internally to the specified device
v0.15.1,Reset prototype updates
v0.15.1,verbalize BIOES labels
v0.15.1,"if label is not BIOES, use label itself"
v0.15.1,Always include the name of the Model class for which the state dict holds
v0.15.1,"this seems to just return model name, not a model with that name"
v0.15.1,"write out a ""model card"" if one is set"
v0.15.1,save model
v0.15.1,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.15.1,get all non-abstract subclasses
v0.15.1,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.15.1,"skip any invalid loadings, e.g. not found on HuggingFace hub"
v0.15.1,"if the model cannot be fetched, load as a file"
v0.15.1,try to get model class from state
v0.15.1,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.15.1,"skip any invalid loadings, e.g. not found on HuggingFace hub"
v0.15.1,"if this class is not abstract, fetch the model and load it"
v0.15.1,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.15.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.15.1,loss calculation
v0.15.1,variables for printing
v0.15.1,variables for computing scores
v0.15.1,remove any previously predicted labels
v0.15.1,predict for batch
v0.15.1,get the gold labels
v0.15.1,add to all_predicted_values
v0.15.1,make printout lines
v0.15.1,convert true and predicted values to two span-aligned lists
v0.15.1,delete excluded labels if exclude_labels is given
v0.15.1,"if after excluding labels, no label is left, ignore the datapoint"
v0.15.1,write all_predicted_values to out_file if set
v0.15.1,make the evaluation dictionary
v0.15.1,check if this is a multi-label problem
v0.15.1,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.15.1,multi-label problems require a multi-hot vector for each true and predicted label
v0.15.1,single-label problems can do with a single index for each true and predicted label
v0.15.1,"now, calculate evaluation numbers"
v0.15.1,there is at least one gold label or one prediction (default)
v0.15.1,compute accuracy separately as it is not always in classification_report (e.g. when micro avg exists)
v0.15.1,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.15.1,"The ""micro avg"" appears only in the classification report if no prediction is possible."
v0.15.1,"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
v0.15.1,"Create and populate score object for logging with all evaluation values, plus the loss"
v0.15.1,issue error and default all evaluation numbers to 0.
v0.15.1,check if there is a label mismatch
v0.15.1,print info
v0.15.1,set the embeddings
v0.15.1,initialize the label dictionary
v0.15.1,initialize the decoder
v0.15.1,set up multi-label logic
v0.15.1,init dropouts
v0.15.1,loss weights and loss function
v0.15.1,Initialize the weight tensor
v0.15.1,set up gradient reversal if so specified
v0.15.1,embed sentences
v0.15.1,get a tensor of data points
v0.15.1,do dropout
v0.15.1,make a forward pass to produce embedded data points and labels
v0.15.1,get the data points for which to predict labels
v0.15.1,get their gold labels as a tensor
v0.15.1,pass data points through network to get encoded data point tensor
v0.15.1,"decode, passing label tensor if needed, such as for prototype updates"
v0.15.1,an optional masking step (no masking in most cases)
v0.15.1,calculate the loss
v0.15.1,filter empty sentences
v0.15.1,reverse sort all sequences by their length
v0.15.1,progress bar for verbosity
v0.15.1,filter data points in batch
v0.15.1,stop if all sentences are empty
v0.15.1,pass data points through network and decode
v0.15.1,if anything could possibly be predicted
v0.15.1,remove previously predicted labels of this type
v0.15.1,filter data points that have labels outside of dictionary
v0.15.1,add DefaultClassifier arguments
v0.15.1,add variables of DefaultClassifier
v0.15.1,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.15.1,Get projected 1st dimension
v0.15.1,Compute bilinear form
v0.15.1,Arcosh
v0.15.1,Project the input data to n+1 dimensions
v0.15.1,"The first dimension, is recomputed in the distance module"
v0.15.1,header for 'weights.txt'
v0.15.1,"determine the column index of loss, f-score and accuracy for"
v0.15.1,"train, dev and test split"
v0.15.1,then get all relevant values from the tsv
v0.15.1,then get all relevant values from the tsv
v0.15.1,plot i
v0.15.1,save plots
v0.15.1,save plots
v0.15.1,plt.show()
v0.15.1,save plot
v0.15.1,auto-spawn on GPU if available
v0.15.1,progress bar for verbosity
v0.15.1,stop if all sentences are empty
v0.15.1,clearing token embeddings to save memory
v0.15.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.15.1,TODO: not saving lines yet
v0.15.1,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.15.1,- MaskedRelationClassifier ?
v0.15.1,This depends if this relation classification architecture should replace or offer as an alternative.
v0.15.1,Set label type and prepare label dictionary
v0.15.1,Initialize super default classifier
v0.15.1,Add the special tokens from the encoding strategy
v0.15.1,"Auto-spawn on GPU, if available"
v0.15.1,Only use entities labelled with the specified labels for each label type
v0.15.1,Only use entities above the specified threshold
v0.15.1,Use a dictionary to find gold relation annotations for a given entity pair
v0.15.1,Yield head and tail entity pairs from the cross product of all entities
v0.15.1,Remove identity relation entity pairs
v0.15.1,Remove entity pairs with labels that do not match any
v0.15.1,of the specified relations in `self.entity_pair_labels`
v0.15.1,"Obtain gold label, if existing"
v0.15.1,Preserve context around the entities. Always include their in-between context.
v0.15.1,Some sanity checks
v0.15.1,Sanity check: Do not create a labeled span if one entity contains the other
v0.15.1,Pre-compute non-leading head and tail tokens for entity masking
v0.15.1,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.15.1,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.15.1,"Therefore, we use the span's position in the sentence."
v0.15.1,Filter cases in which the distance between the two entities is too large
v0.15.1,Remove excess tokens left and right of entity pair to make encoded sentence shorter
v0.15.1,Create masked sentence
v0.15.1,Add gold relation annotation as sentence label
v0.15.1,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.15.1,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.15.1,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.15.1,"may get distributed into different splits. For training purposes, this is always undesired."
v0.15.1,Ensure that all sentences are encoded properly
v0.15.1,Deal with the case where all sentences are encoded sentences
v0.15.1,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.15.1,Deal with the case where all sentences are standard (non-encoded) sentences
v0.15.1,"For each encoded sentence, transfer its prediction onto the original relation"
v0.15.1,check if there is a label mismatch
v0.15.1,"if the gold label is O and is correctly predicted as no label, do not print out as this clutters"
v0.15.1,the output file with trivial predictions
v0.15.1,auto-spawn on GPU if available
v0.15.1,pad strings with whitespaces to longest sentence
v0.15.1,cut up the input into chunks of max charlength = chunk_size
v0.15.1,push each chunk through the RNN language model
v0.15.1,concatenate all chunks to make final output
v0.15.1,initial hidden state
v0.15.1,get predicted weights
v0.15.1,divide by temperature
v0.15.1,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.15.1,this makes a vector in which the largest value is 0
v0.15.1,compute word weights with exponential function
v0.15.1,try sampling multinomial distribution for next character
v0.15.1,print(word_idx)
v0.15.1,input ids
v0.15.1,push list of character IDs through model
v0.15.1,the target is always the next character
v0.15.1,use cross entropy loss to compare output of forward pass with targets
v0.15.1,exponentiate cross-entropy loss to calculate perplexity
v0.15.1,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.15.1,serialize the language models and the constructor arguments (but nothing else)
v0.15.1,special handling for deserializing language models
v0.15.1,re-initialize language model with constructor arguments
v0.15.1,copy over state dictionary to self
v0.15.1,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.15.1,"in their ""self.train()"" method)"
v0.15.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.15.1,"check if this is the case and if so, set it"
v0.15.1,Transform input data into TARS format
v0.15.1,"if there are no labels, return a random sample as negatives"
v0.15.1,"otherwise, go through all labels"
v0.15.1,make sure the probabilities always sum up to 1
v0.15.1,get and embed all labels by making a Sentence object that contains only the label text
v0.15.1,get each label embedding and scale between 0 and 1
v0.15.1,compute similarity matrix
v0.15.1,"the higher the similarity, the greater the chance that a label is"
v0.15.1,sampled as negative example
v0.15.1,make label dictionary if no Dictionary object is passed
v0.15.1,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.15.1,check if candidate_label_set is empty
v0.15.1,make list if only one candidate label is passed
v0.15.1,create label dictionary
v0.15.1,note current task
v0.15.1,create a temporary task
v0.15.1,make zero shot predictions
v0.15.1,switch to the pre-existing task
v0.15.1,prepare TARS dictionary
v0.15.1,initialize a bare-bones sequence tagger
v0.15.1,transformer separator
v0.15.1,Store task specific labels since TARS can handle multiple tasks
v0.15.1,make a tars sentence where all labels are O by default
v0.15.1,init new TARS classifier
v0.15.1,set all task information
v0.15.1,progress bar for verbosity
v0.15.1,stop if all sentences are empty
v0.15.1,always remove tags first
v0.15.1,go through each sentence in the batch
v0.15.1,always remove tags first
v0.15.1,get the span and its label
v0.15.1,determine whether tokens in this span already have a label
v0.15.1,only add if all tokens have no label
v0.15.1,make and add a corresponding predicted span
v0.15.1,set indices so that no token can be tagged twice
v0.15.1,clearing token embeddings to save memory
v0.15.1,"all labels default to ""O"""
v0.15.1,set gold token-level
v0.15.1,set predicted token-level
v0.15.1,now print labels in CoNLL format
v0.15.1,prepare TARS dictionary
v0.15.1,initialize a bare-bones sequence tagger
v0.15.1,transformer separator
v0.15.1,Store task specific labels since TARS can handle multiple tasks
v0.15.1,get the serialized embeddings
v0.15.1,remap state dict for models serialized with Flair <= 0.11.3
v0.15.1,init new TARS classifier
v0.15.1,set all task information
v0.15.1,with torch.no_grad():
v0.15.1,progress bar for verbosity
v0.15.1,stop if all sentences are empty
v0.15.1,always remove tags first
v0.15.1,go through each sentence in the batch
v0.15.1,always remove tags first
v0.15.1,add all labels that according to TARS match the text and are above threshold
v0.15.1,do not add labels below confidence threshold
v0.15.1,only use label with the highest confidence if enforcing single-label predictions
v0.15.1,add the label with the highest score even if below the threshold if force label is activated.
v0.15.1,remove previously added labels and only add the best label
v0.15.1,clearing token embeddings to save memory
v0.15.1,set separator to concatenate three sentences
v0.15.1,auto-spawn on GPU if available
v0.15.1,set separator to concatenate two sentences
v0.15.1,auto-spawn on GPU if available
v0.15.1,"If the concatenated version of the text pair does not exist yet, create it"
v0.15.1,pooling operation to get embeddings for entites
v0.15.1,set embeddings
v0.15.1,set relation and entity label types
v0.15.1,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.15.1,filter entity pairs according to their tags if set
v0.15.1,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.15.1,character dictionary for decoding and encoding
v0.15.1,make sure <unk> is in dictionary for handling of unknown characters
v0.15.1,add special symbols to dictionary if necessary and save respective indices
v0.15.1,---- ENCODER ----
v0.15.1,encoder character embeddings
v0.15.1,encoder pre-trained embeddings
v0.15.1,encoder RNN
v0.15.1,additional encoder linear layer if bidirectional encoding
v0.15.1,---- DECODER ----
v0.15.1,decoder: linear layers to transform vectors to and from alphabet_size
v0.15.1,when using attention we concatenate attention outcome and decoder hidden states
v0.15.1,decoder RNN
v0.15.1,loss and softmax
v0.15.1,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.15.1,add additional columns for special symbols if necessary
v0.15.1,initialize with dummy symbols
v0.15.1,encode inputs
v0.15.1,get labels (we assume each token has a lemma label)
v0.15.1,get char indices for labels of sentence
v0.15.1,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.15.1,max_sequence_length = length of longest label of sentence + 1
v0.15.1,get char embeddings
v0.15.1,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.15.1,take decoder input and initial hidden and pass through RNN
v0.15.1,"if all encoder outputs are provided, use attention"
v0.15.1,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.15.1,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.15.1,get all tokens
v0.15.1,encode input characters by sending them through RNN
v0.15.1,get one-hots for characters and add special symbols / padding
v0.15.1,determine length of each token
v0.15.1,embed sentences
v0.15.1,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.15.1,variable to store initial hidden states for decoder
v0.15.1,encode input characters by sending them through RNN
v0.15.1,test packing and padding
v0.15.1,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.15.1,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.15.1,decoder later with self.emb_to_hidden
v0.15.1,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.15.1,use token embedding as initial hidden state for decoder
v0.15.1,concatenate everything together and project to appropriate size for decoder
v0.15.1,variable to store initial hidden states for decoder
v0.15.1,encode input characters by sending them through RNN
v0.15.1,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.15.1,embed character one-hots
v0.15.1,send through encoder RNN (produces initial hidden for decoder)
v0.15.1,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.15.1,project 2*hidden_size to hidden_size
v0.15.1,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.15.1,later with self.emb_to_hidden
v0.15.1,use token embedding as initial hidden state for decoder
v0.15.1,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.15.1,concatenate everything together and project to appropriate size for decoder
v0.15.1,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.15.1,"create target vector (batch_size, max_label_seq_length + 1)"
v0.15.1,filter empty sentences
v0.15.1,max length of the predicted sequences
v0.15.1,for printing
v0.15.1,stop if all sentences are empty
v0.15.1,remove previously predicted labels of this type
v0.15.1,create list of tokens in batch
v0.15.1,encode inputs
v0.15.1,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.15.1,sequence length is always set to one in prediction
v0.15.1,option 1: greedy decoding
v0.15.1,predictions
v0.15.1,decode next character
v0.15.1,pick top beam size many outputs with highest probabilities
v0.15.1,option 2: beam search
v0.15.1,out_probs = self.softmax(output_vectors).squeeze(1)
v0.15.1,make sure no dummy symbol <> or start symbol <S> is predicted
v0.15.1,pick top beam size many outputs with highest probabilities
v0.15.1,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.15.1,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.15.1,keep scores of beam_size many hypothesis for each token in the batch
v0.15.1,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.15.1,save sequences so far
v0.15.1,keep track of how many hypothesis were completed for each token
v0.15.1,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.15.1,decode with log softmax
v0.15.1,make sure no dummy symbol <> or start symbol <S> is predicted
v0.15.1,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.15.1,"if the sequence is already ended, do not record as candidate"
v0.15.1,index of token in in list tokens_in_batch
v0.15.1,print(token_number)
v0.15.1,hypothesis score
v0.15.1,TODO: remove token if number of completed hypothesis exceeds given value
v0.15.1,set score of corresponding entry to -inf so it will not be expanded
v0.15.1,get leading_indices for next expansion
v0.15.1,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.15.1,take beam_size many copies of scores vector and add scores of possible new extensions
v0.15.1,"size (beam_size*batch_size, beam_size)"
v0.15.1,print(hypothesis_scores)
v0.15.1,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.15.1,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.15.1,print(hypothesis_scores_per_token)
v0.15.1,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.15.1,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.15.1,a list of length beam_size*batch_size
v0.15.1,"where the first three inidices belong to the first token, the next three to the second token,"
v0.15.1,and so on
v0.15.1,with these indices we can compute the tensors for the next iteration
v0.15.1,expand sequences with corresponding index
v0.15.1,add log-probabilities to the scores
v0.15.1,save new leading indices
v0.15.1,save corresponding hidden states
v0.15.1,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.15.1,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.15.1,get best final hypothesis for each token
v0.15.1,get characters from index sequences and add predicted label to token
v0.15.1,"Overwrites evaluate of parent class to remove the ""by class"" printout"
v0.15.1,set separator to concatenate two sentences
v0.15.1,init dropouts
v0.15.1,auto-spawn on GPU if available
v0.15.1,make a forward pass to produce embedded data points and labels
v0.15.1,get their gold labels as a tensor
v0.15.1,pass data points through network to get encoded data point tensor
v0.15.1,decode
v0.15.1,calculate the loss
v0.15.1,get a tensor of data points
v0.15.1,do dropout
v0.15.1,"If the concatenated version of the text pair does not exist yet, create it"
v0.15.1,add Model arguments
v0.15.1,progress bar for verbosity
v0.15.1,stop if all sentences are empty
v0.15.1,clearing token embeddings to save memory
v0.15.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.15.1,"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
v0.15.1,fields in case this is a span-prediction problem
v0.15.1,the label type
v0.15.1,all parameters will be pushed internally to the specified device
v0.15.1,special handling during training if this is a span prediction problem
v0.15.1,internal variables
v0.15.1,non-set tags are OUT tags
v0.15.1,anything that is not OUT is IN
v0.15.1,does this prediction start a new span?
v0.15.1,B- and S- always start new spans
v0.15.1,"if the predicted class changes, I- starts a new span"
v0.15.1,"if the predicted class changes and S- was previous tag, start a new span"
v0.15.1,if an existing span is ended (either by reaching O or starting a new span)
v0.15.1,reset for-loop variables for new span
v0.15.1,remember previous tag
v0.15.1,"if there is a span at end of sentence, add it"
v0.15.1,"all labels default to ""O"""
v0.15.1,set gold token-level
v0.15.1,set predicted token-level
v0.15.1,now print labels in CoNLL format
v0.15.1,print labels in CoNLL format
v0.15.1,internal candidate lists of generator
v0.15.1,load Zelda candidates if so passed
v0.15.1,create candidate lists
v0.15.1,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.15.1,create a new dictionary for lower cased mentions
v0.15.1,go through each mention and its candidates
v0.15.1,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.15.1,set lowercased version as map
v0.15.1,"only use span label type if there are predictions, otherwise search for output label type (training labels)"
v0.15.1,remap state dict for models serialized with Flair <= 0.11.3
v0.15.1,get the candidates
v0.15.1,"during training, add the gold value as candidate"
v0.15.1,----- Create the internal tag dictionary -----
v0.15.1,span-labels need special encoding (BIO or BIOES)
v0.15.1,the big question is whether the label dictionary should contain an UNK or not
v0.15.1,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.15.1,"with UNK, the model learns less well if there are no UNK examples"
v0.15.1,is this a span prediction problem?
v0.15.1,----- Embeddings -----
v0.15.1,----- Initial loss weights parameters -----
v0.15.1,----- RNN specific parameters -----
v0.15.1,----- Conditional Random Field parameters -----
v0.15.1,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.15.1,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.15.1,----- Dropout parameters -----
v0.15.1,dropouts
v0.15.1,remove word dropout if there is no contact over the sequence dimension.
v0.15.1,----- Model layers -----
v0.15.1,----- RNN layer -----
v0.15.1,"If shared RNN provided, else create one for model"
v0.15.1,Whether to train initial hidden state
v0.15.1,final linear map to tag space
v0.15.1,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.15.1,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.15.1,"if there are no sentences, there is no loss"
v0.15.1,forward pass to get scores
v0.15.1,calculate loss given scores and labels
v0.15.1,make a zero-padded tensor for the whole sentence
v0.15.1,linear map to tag space
v0.15.1,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.15.1,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.15.1,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.15.1,spans need to be encoded as token-level predictions
v0.15.1,all others are regular labels for each token
v0.15.1,make sure it's a list
v0.15.1,filter empty sentences
v0.15.1,reverse sort all sequences by their length
v0.15.1,progress bar for verbosity
v0.15.1,stop if all sentences are empty
v0.15.1,get features from forward propagation
v0.15.1,remove previously predicted labels of this type
v0.15.1,"if return_loss, get loss value"
v0.15.1,make predictions
v0.15.1,add predictions to Sentence
v0.15.1,BIOES-labels need to be converted to spans
v0.15.1,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.15.1,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.15.1,core Flair models on Huggingface ModelHub
v0.15.1,"Large NER models,"
v0.15.1,Multilingual NER models
v0.15.1,English POS models
v0.15.1,Multilingual POS models
v0.15.1,English SRL models
v0.15.1,English chunking models
v0.15.1,Language-specific NER models
v0.15.1,Language-specific POS models
v0.15.1,Historic German
v0.15.1,English NER models
v0.15.1,English SRL models
v0.15.1,Danish models
v0.15.1,German models
v0.15.1,Arabic models
v0.15.1,French models
v0.15.1,Dutch models
v0.15.1,Malayalam models
v0.15.1,Portuguese models
v0.15.1,Biomedical models
v0.15.1,check if model name is a valid local file
v0.15.1,"check if model key is remapped to HF key - if so, print out information"
v0.15.1,get mapped name
v0.15.1,"if not, check if model key is remapped to direct download location. If so, download model"
v0.15.1,"for all other cases (not local file or special download location), use HF model hub"
v0.15.1,## Demo: How to use in Flair
v0.15.1,load tagger
v0.15.1,make example sentence
v0.15.1,predict NER tags
v0.15.1,print sentence
v0.15.1,print predicted NER spans
v0.15.1,iterate over entities and print
v0.15.1,Lazy import
v0.15.1,Save model weight
v0.15.1,Determine if model card already exists
v0.15.1,Generate and save model card
v0.15.1,Upload files
v0.15.1,"all labels default to ""O"""
v0.15.1,set gold token-level
v0.15.1,set predicted token-level
v0.15.1,now print labels in CoNLL format
v0.15.1,print labels in CoNLL format
v0.15.1,Dense + sparse retrieval
v0.15.1,fetched from original repo to avoid download
v0.15.1,"just in case we add: fuzzy search, Levenstein, ..."
v0.15.1,"for now we always fall back to SapBERT,"
v0.15.1,but we should train our own models at some point
v0.15.1,NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
v0.15.1,NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`)
v0.15.1,Ab3P works on sentence-level and not on a single entity mention / name
v0.15.1,- so we just apply the wrapped text pre-processing here (if configured)
v0.15.1,NOTE: ensure correct similarity metric for pretrained model
v0.15.1,empty cuda cache if device is a cuda device
v0.15.1,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.15.1,NOTE: This is a hacky workaround for the fact that
v0.15.1,the `label_type`s in `Classifier.load('hunflair)` are
v0.15.1,"'diseases', 'genes', 'species', 'chemical' instead of 'ner'."
v0.15.1,We warn users once they need to update SequenceTagger model
v0.15.1,See: https://github.com/flairNLP/flair/pull/3387
v0.15.1,make sure sentences is a list of sentences
v0.15.1,Make sure entity label types are represented as dict
v0.15.1,Collect all entities based on entity type labels configuration
v0.15.1,Preprocess entity mentions
v0.15.1,Retrieve top-k concept / entity candidates
v0.15.1,Add a label annotation for each candidate
v0.15.1,load model by entity_type
v0.15.1,check if we have a hybrid pre-trained model
v0.15.1,the multi task model has several labels
v0.15.1,Add metrics so they will be available to _publish_eval_result.
v0.15.1,biomedical models
v0.15.1,entity linker
v0.15.1,print(match)
v0.15.1,print(span)
v0.15.1,auto-spawn on GPU if available
v0.15.1,remap state dict for models serialized with Flair <= 0.11.3
v0.15.1,English sentiment models
v0.15.1,Communicative Functions Model
v0.15.1,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.15.1,"may get distributed into different splits. For training purposes, this is always undesired."
v0.15.1,Prepend the task description prompt to the sentence text
v0.15.1,Make sure it's a list
v0.15.1,Reconstruct all annotations from the original sentence (necessary for learning classifiers)
v0.15.1,If all sentences are not augmented -> augment them
v0.15.1,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.15.1,"mypy does not infer the type of ""sentences"" restricted by code above"
v0.15.1,Compute prediction label type
v0.15.1,make sure it's a list
v0.15.1,"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences"
v0.15.1,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.15.1,Remove existing labels
v0.15.1,Augment sentences - copy all annotation of the given tag type
v0.15.1,Predict on augmented sentence and store it in an internal annotation layer / label
v0.15.1,Append predicted labels to the original sentences
v0.15.1,check if model name is a valid local file
v0.15.1,check if model name is a pre-configured hf model
v0.15.1,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.15.1,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.15.1,"Initially, get scores from <start> tag to all other tags"
v0.15.1,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.15.1,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.15.1,Create a tensor to hold accumulated sequence scores at each current tag
v0.15.1,Create a tensor to hold back-pointers
v0.15.1,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.15.1,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.15.1,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.15.1,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.15.1,"If sentence is over, add transition to STOP-tag"
v0.15.1,Decode/trace best path backwards
v0.15.1,Sanity check
v0.15.1,remove start-tag and backscore to stop-tag
v0.15.1,Max + Softmax to get confidence score for predicted label and append label to each token
v0.15.1,"Transitions are used in the following way: transitions[to, from]."
v0.15.1,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.15.1,to START-tag and from STOP-tag to any other tag to -10000.
v0.15.1,"if necessary, make batch_steps"
v0.15.1,break up the batch into slices of size
v0.15.1,mini_batch_chunk_size
v0.15.1,"if training also uses dev/train data, include in training set"
v0.15.1,evaluation and monitoring
v0.15.1,sampling and shuffling
v0.15.1,evaluation and monitoring
v0.15.1,when and what to save
v0.15.1,logging parameters
v0.15.1,acceleration
v0.15.1,plugins
v0.15.1,activate annealing plugin
v0.15.1,call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
v0.15.1,training parameters
v0.15.1,evaluation and monitoring
v0.15.1,sampling and shuffling
v0.15.1,evaluation and monitoring
v0.15.1,when and what to save
v0.15.1,logging parameters
v0.15.1,acceleration
v0.15.1,plugins
v0.15.1,annealing logic
v0.15.1,training parameters
v0.15.1,evaluation and monitoring
v0.15.1,sampling and shuffling
v0.15.1,evaluation and monitoring
v0.15.1,when and what to save
v0.15.1,logging parameters
v0.15.1,acceleration
v0.15.1,plugins
v0.15.1,training parameters
v0.15.1,evaluation and monitoring
v0.15.1,sampling and shuffling
v0.15.1,evaluation and monitoring
v0.15.1,when and what to save
v0.15.1,logging parameters
v0.15.1,acceleration
v0.15.1,plugins
v0.15.1,Create output folder
v0.15.1,=== START BLOCK: ACTIVATE PLUGINS === #
v0.15.1,We first activate all optional plugins. These take care of optional functionality such as various
v0.15.1,logging techniques and checkpointing
v0.15.1,log file plugin
v0.15.1,loss file plugin
v0.15.1,plugin for writing weights
v0.15.1,plugin for checkpointing
v0.15.1,=== END BLOCK: ACTIVATE PLUGINS === #
v0.15.1,derive parameters the function was called with (or defaults)
v0.15.1,initialize model card with these parameters
v0.15.1,Prepare training data and get dataset size
v0.15.1,"determine what splits (train, dev, test) to evaluate"
v0.15.1,determine how to determine best model and whether to save it
v0.15.1,instantiate the optimizer
v0.15.1,initialize sampler if provided
v0.15.1,init with default values if only class is provided
v0.15.1,set dataset to sample from
v0.15.1,configure special behavior to use multiple GPUs
v0.15.1,Guard against each process initializing corpus differently due to e.g. different random seeds
v0.15.1,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.15.1,Sanity checks
v0.15.1,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.15.1,-- AmpPlugin -> wraps with AMP
v0.15.1,-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
v0.15.1,At any point you can hit Ctrl + C to break out of training early.
v0.15.1,"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
v0.15.1,- LossFilePlugin -> get the current epoch for loss file logging
v0.15.1,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.15.1,log infos on training progress every `log_modulo` batches
v0.15.1,process mini-batches
v0.15.1,zero the gradients on the model and optimizer
v0.15.1,forward and backward for batch
v0.15.1,forward pass
v0.15.1,We need to __call__ ddp_model() because this triggers hooks that sync gradients.
v0.15.1,But that calls forward rather than forward_loss. So we patch forward to redirect
v0.15.1,to forward_loss. Then undo the patch in case forward_loss itself calls forward.
v0.15.1,identify dynamic embeddings (always deleted) on first sentence
v0.15.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.15.1,do the optimizer step
v0.15.1,DDP averages across processes but we want the sum
v0.15.1,- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
v0.15.1,- WeightExtractorPlugin -> extracts weights
v0.15.1,- CheckpointPlugin -> executes save_model_each_k_epochs
v0.15.1,- SchedulerPlugin -> log bad epochs
v0.15.1,Determine if this is the best model or if we need to anneal
v0.15.1,log results
v0.15.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.15.1,use DEV split to determine if this is the best model so far
v0.15.1,"if not using DEV score, determine best model using train loss"
v0.15.1,- LossFilePlugin -> somehow prints all relevant metrics
v0.15.1,- AnnealPlugin -> scheduler step
v0.15.1,- SWAPlugin -> restores SGD weights from SWA
v0.15.1,"if we do not use dev data for model selection, save final model"
v0.15.1,TensorboardLogger -> closes writer
v0.15.1,test best model if test data is present
v0.15.1,get and return the final test score of best model
v0.15.1,MetricHistoryPlugin -> stores the loss history in return_values
v0.15.1,"Store return values, as they will be erased by reset_training_attributes"
v0.15.1,get a random sample of training sentences
v0.15.1,create a model card for this model with Flair and PyTorch version
v0.15.1,record Transformers version if library is loaded
v0.15.1,remember the training parameters
v0.15.1,special rule for Path variables to make sure models can be deserialized on other OS
v0.15.1,classes are only serialized as names
v0.15.1,"TextDataset returns a list. valid and test are only one file,"
v0.15.1,so return the first element
v0.15.1,cast string to Path
v0.15.1,error message if the validation dataset is too small
v0.15.1,Shuffle training files randomly after serially iterating
v0.15.1,through corpus one
v0.15.1,"iterate through training data, starting at"
v0.15.1,self.split (for checkpointing)
v0.15.1,off by one for printing
v0.15.1,go into train mode
v0.15.1,reset variables
v0.15.1,not really sure what this does
v0.15.1,do the forward pass in the model
v0.15.1,try to predict the targets
v0.15.1,Backward
v0.15.1,`clip_grad_norm` helps prevent the exploding gradient
v0.15.1,problem in RNNs / LSTMs.
v0.15.1,We detach the hidden state from how it was
v0.15.1,previously produced.
v0.15.1,"If we didn't, the model would try backpropagating"
v0.15.1,all the way to start of the dataset.
v0.15.1,explicitly remove loss to clear up memory
v0.15.1,#########################################################
v0.15.1,Save the model if the validation loss is the best we've
v0.15.1,seen so far.
v0.15.1,#########################################################
v0.15.1,print info
v0.15.1,#########################################################
v0.15.1,##############################################################################
v0.15.1,final testing
v0.15.1,##############################################################################
v0.15.1,Turn on evaluation mode which disables dropout.
v0.15.1,Work out how cleanly we can divide the dataset into bsz parts.
v0.15.1,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.15.1,Evenly divide the data across the bsz batches.
v0.15.1,"no need to check for MetricName, as __add__ of other would be called in this case"
v0.15.1,"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
v0.15.1,instantiate plugin
v0.15.1,"Reset the flag, since an exception event might be dispatched"
v0.15.1,"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
v0.15.1,the callback
v0.15.1,go through all attributes
v0.15.1,get attribute hook events (may raise an AttributeError)
v0.15.1,register function as a hook
v0.15.1,"Decorator was used with parentheses, but no args"
v0.15.1,Decorator was used with args (strings specifiying the events)
v0.15.1,Decorator was used without args
v0.15.1,path to store the model
v0.15.1,special annealing modes
v0.15.1,determine the min learning rate
v0.15.1,"minimize training loss if training with dev data, else maximize dev score"
v0.15.1,instantiate the scheduler
v0.15.1,stop training if learning rate becomes too small
v0.15.1,reload last best model if annealing with restarts is enabled
v0.15.1,calculate warmup steps
v0.15.1,skip if no optimization has happened.
v0.15.1,saves the model with full vocab as checkpoints etc were created with reduced vocab.
v0.15.1,TODO: check if metric is in tracked metrics
v0.15.1,prepare loss logging file and set up header
v0.15.1,set up all metrics to collect
v0.15.1,set up headers
v0.15.1,name: HEADER
v0.15.1,Add all potentially relevant metrics. If a metric is not published
v0.15.1,"after the first epoch (when the header is written), the column is"
v0.15.1,removed at that point.
v0.15.1,initialize the first log line
v0.15.1,record is a list of scalars
v0.15.1,output log file
v0.15.1,remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
v0.15.1,make headers on epoch 1
v0.15.1,write header
v0.15.1,adjust alert level
v0.15.1,"the default model for ELMo is the 'original' model, which is very large"
v0.15.1,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.15.1,put on Cuda if available
v0.15.1,embed a dummy sentence to determine embedding_length
v0.15.1,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.15.1,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.15.1,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.15.1,news-english-forward
v0.15.1,news-english-backward
v0.15.1,news-english-forward
v0.15.1,news-english-backward
v0.15.1,mix-english-forward
v0.15.1,mix-english-backward
v0.15.1,mix-german-forward
v0.15.1,mix-german-backward
v0.15.1,common crawl Polish forward
v0.15.1,common crawl Polish backward
v0.15.1,Slovenian forward
v0.15.1,Slovenian backward
v0.15.1,Bulgarian forward
v0.15.1,Bulgarian backward
v0.15.1,Dutch forward
v0.15.1,Dutch backward
v0.15.1,Swedish forward
v0.15.1,Swedish backward
v0.15.1,French forward
v0.15.1,French backward
v0.15.1,Czech forward
v0.15.1,Czech backward
v0.15.1,Portuguese forward
v0.15.1,Portuguese backward
v0.15.1,initialize cache if use_cache set
v0.15.1,embed a dummy sentence to determine embedding_length
v0.15.1,set to eval mode
v0.15.1,Copy the object's state from self.__dict__ which contains
v0.15.1,all our instance attributes. Always use the dict.copy()
v0.15.1,method to avoid modifying the original state.
v0.15.1,Remove the unpicklable entries.
v0.15.1,"if cache is used, try setting embeddings from cache first"
v0.15.1,try populating embeddings from cache
v0.15.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.15.1,get hidden states from language model
v0.15.1,take first or last hidden states from language model as word representation
v0.15.1,if self.tokenized_lm or token.whitespace_after:
v0.15.1,"if only one sentence is passed, convert to list of sentence"
v0.15.1,bidirectional LSTM on top of embedding layer
v0.15.1,dropouts
v0.15.1,"first, sort sentences by number of tokens"
v0.15.1,go through each sentence in batch
v0.15.1,PADDING: pad shorter sentences out
v0.15.1,ADD TO SENTENCE LIST: add the representation
v0.15.1,--------------------------------------------------------------------
v0.15.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.15.1,--------------------------------------------------------------------
v0.15.1,--------------------------------------------------------------------
v0.15.1,FF PART
v0.15.1,--------------------------------------------------------------------
v0.15.1,use word dropout if set
v0.15.1,--------------------------------------------------------------------
v0.15.1,EXTRACT EMBEDDINGS FROM LSTM
v0.15.1,--------------------------------------------------------------------
v0.15.1,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.15.1,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.15.1,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.15.1,add positional encodings
v0.15.1,reshape the pixels into the sequence
v0.15.1,layer norm after convolution and positional encodings
v0.15.1,add <cls> token
v0.15.1,"transformer requires input in the shape [h*w+1, b, d]"
v0.15.1,the output is an embedding of <cls> token
v0.15.1,this parameter is fixed
v0.15.1,optional fine-tuning on top of embedding layer
v0.15.1,"if only one sentence is passed, convert to list of sentence"
v0.15.1,"if only one sentence is passed, convert to list of sentence"
v0.15.1,bidirectional RNN on top of embedding layer
v0.15.1,dropouts
v0.15.1,TODO: remove in future versions
v0.15.1,embed words in the sentence
v0.15.1,before-RNN dropout
v0.15.1,reproject if set
v0.15.1,push through RNN
v0.15.1,after-RNN dropout
v0.15.1,extract embeddings from RNN
v0.15.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.15.1,"check if this is the case and if so, set it"
v0.15.1,serialize the language models and the constructor arguments (but nothing else)
v0.15.1,re-initialize language model with constructor arguments
v0.15.1,special handling for deserializing language models
v0.15.1,copy over state dictionary to self
v0.15.1,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.15.1,"in their ""self.train()"" method)"
v0.15.1,IMPORTANT: add embeddings as torch modules
v0.15.1,iterate over sentences
v0.15.1,"if its a forward LM, take last state"
v0.15.1,"convert to plain strings, embedded in a list for the encode function"
v0.15.1,CNN
v0.15.1,dropouts
v0.15.1,TODO: remove in future versions
v0.15.1,embed words in the sentence
v0.15.1,before-RNN dropout
v0.15.1,reproject if set
v0.15.1,push CNN
v0.15.1,after-CNN dropout
v0.15.1,extract embeddings from CNN
v0.15.1,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.15.1,"if only one sentence is passed, convert to list of sentence"
v0.15.1,Expose base classses
v0.15.1,Expose document embedding classes
v0.15.1,Expose image embedding classes
v0.15.1,Expose legacy embedding classes
v0.15.1,Expose token embedding classes
v0.15.1,in some cases we need to insert zero vectors for tokens without embedding.
v0.15.1,sum embeddings for each token
v0.15.1,calculate the mean of subtokens
v0.15.1,Create a mask for valid tokens based on token_lengths
v0.15.1,padding
v0.15.1,remove special markup
v0.15.1,check if special tokens exist to circumvent error message
v0.15.1,iterate over subtokens and reconstruct tokens
v0.15.1,remove special markup
v0.15.1,check if reconstructed token is special begin token ([CLS] or similar)
v0.15.1,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.15.1,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.15.1,if tokens are unaccounted for
v0.15.1,check if all tokens were matched to subtokens
v0.15.1,The layoutlm tokenizer doesn't handle ocr themselves
v0.15.1,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.15.1,"cannot run `.encode` if ocr boxes are required, assume"
v0.15.1,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.15.1,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.15.1,in case of doubt: token embedding has higher priority than document embedding
v0.15.1,random check some tokens to save performance.
v0.15.1,Models such as FNet do not have an attention_mask
v0.15.1,set language IDs for XLM-style transformers
v0.15.1,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.15.1,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.15.1,set context if not set already
v0.15.1,flair specific pre-tokenization
v0.15.1,fields to store left and right context
v0.15.1,expand context only if context_length is set
v0.15.1,"if context_dropout is set, randomly deactivate left context during training"
v0.15.1,"if context_dropout is set, randomly deactivate right context during training"
v0.15.1,"if use_context_separator is set, add a [FLERT] token"
v0.15.1,return expanded sentence and context length information
v0.15.1,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.15.1,temporary fix to disable tokenizer parallelism warning
v0.15.1,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.15.1,do not print transformer warnings as these are confusing in this case
v0.15.1,load tokenizer and transformer model
v0.15.1,load tokenizer from inmemory zip-file
v0.15.1,if model is quantized by BitsAndBytes this will fail
v0.15.1,add adapters for finetuning
v0.15.1,peft_config: PeftConfig
v0.15.1,model name
v0.15.1,embedding parameters
v0.15.1,send mini-token through to check how many layers the model has
v0.15.1,return length
v0.15.1,"If we use a context separator, add a new special token"
v0.15.1,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.15.1,"when initializing, embeddings are in eval mode by default"
v0.15.1,in case of doubt: token embedding has higher priority than document embedding
v0.15.1,in case of doubt: token embedding has higher priority than document embedding
v0.15.1,legacy TransformerDocumentEmbedding
v0.15.1,legacy TransformerTokenEmbedding
v0.15.1,legacy Flair <= 0.12
v0.15.1,legacy Flair <= 0.7
v0.15.1,legacy TransformerTokenEmbedding
v0.15.1,Legacy TransformerDocumentEmbedding
v0.15.1,legacy TransformerTokenEmbedding
v0.15.1,legacy TransformerDocumentEmbedding
v0.15.1,some models like the tars model somehow lost this information.
v0.15.1,copy values from new embedding
v0.15.1,do not switch the attention implementation upon reload.
v0.15.1,those parameters are only from the super class and will be recreated in the constructor.
v0.15.1,cls first pooling can be done without recreating sentence hidden states
v0.15.1,make the tuple a tensor; makes working with it easier.
v0.15.1,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.15.1,only use layers that will be outputted
v0.15.1,this parameter is fixed
v0.15.1,IMPORTANT: add embeddings as torch modules
v0.15.1,"if only one sentence is passed, convert to list of sentence"
v0.15.1,make compatible with serialized models
v0.15.1,gensim version 4
v0.15.1,gensim version 3
v0.15.1,"if no embedding is set, the vocab and embedding length is required"
v0.15.1,GLOVE embeddings
v0.15.1,TURIAN embeddings
v0.15.1,KOMNINOS embeddings
v0.15.1,pubmed embeddings
v0.15.1,FT-CRAWL embeddings
v0.15.1,FT-CRAWL embeddings
v0.15.1,twitter embeddings
v0.15.1,two-letter language code wiki embeddings
v0.15.1,two-letter language code wiki embeddings
v0.15.1,two-letter language code crawl embeddings
v0.15.1,"this is required to force the module on the cpu,"
v0.15.1,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.15.1,self.to(..) actually sets the device properly
v0.15.1,this ignores the get_cached_vec method when loading older versions
v0.15.1,it is needed for compatibility reasons
v0.15.1,gensim version 4
v0.15.1,gensim version 3
v0.15.1,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.15.1,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.15.1,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.15.1,not finding them)
v0.15.1,use list of common characters if none provided
v0.15.1,translate words in sentence into ints using dictionary
v0.15.1,"sort words by length, for batching and masking"
v0.15.1,chars for rnn processing
v0.15.1,multilingual models
v0.15.1,English models
v0.15.1,Arabic
v0.15.1,Bulgarian
v0.15.1,Czech
v0.15.1,Danish
v0.15.1,German
v0.15.1,Spanish
v0.15.1,Basque
v0.15.1,Persian
v0.15.1,Finnish
v0.15.1,French
v0.15.1,Hebrew
v0.15.1,Hindi
v0.15.1,Croatian
v0.15.1,Indonesian
v0.15.1,Italian
v0.15.1,Japanese
v0.15.1,Malayalam
v0.15.1,Dutch
v0.15.1,Norwegian
v0.15.1,Polish
v0.15.1,Portuguese
v0.15.1,Pubmed
v0.15.1,Slovenian
v0.15.1,Swedish
v0.15.1,Tamil
v0.15.1,Spanish clinical
v0.15.1,CLEF HIPE Shared task
v0.15.1,Amharic
v0.15.1,Ukrainian
v0.15.1,load model if in pretrained model map
v0.15.1,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.15.1,CLEF HIPE models are lowercased
v0.15.1,embeddings are static if we don't do finetuning
v0.15.1,embed a dummy sentence to determine embedding_length
v0.15.1,set to eval mode
v0.15.1,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.15.1,gradients are enable if fine-tuning is enabled
v0.15.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.15.1,get hidden states from language model
v0.15.1,take first or last hidden states from language model as word representation
v0.15.1,offset mode that extracts at whitespace after last character
v0.15.1,offset mode that extracts at last character
v0.15.1,make compatible with old models
v0.15.1,use the character language model embeddings as basis
v0.15.1,length is twice the original character LM embedding length
v0.15.1,these fields are for the embedding memory
v0.15.1,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.15.1,we re-compute embeddings dynamically at each epoch
v0.15.1,set the memory method
v0.15.1,memory is wiped each time we do a training run
v0.15.1,"if we keep a pooling, it needs to be updated continuously"
v0.15.1,update embedding
v0.15.1,check token.text is empty or not
v0.15.1,set aggregation operation
v0.15.1,add embeddings after updating
v0.15.1,model architecture
v0.15.1,model architecture
v0.15.1,"""pl"","
v0.15.1,download if necessary
v0.15.1,load the model
v0.15.1,"this is required to force the module on the cpu,"
v0.15.1,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.15.1,self.to(..) actually sets the device properly
v0.15.1,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.15.1,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.15.1,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.15.1,not finding them)
v0.15.1,old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict
v0.15.1,"however they are already initialized rightfully, so we just set the state dict from our current state dict"
v0.15.1,GLOVE embeddings
v0.15.1,no need to recreate as NILCEmbeddings
v0.15.1,read in test file if exists
v0.15.1,read in dev file if exists
v0.15.1,"find train, dev and test files if not specified"
v0.15.1,Add tags for each annotated span
v0.15.1,Remove leading and trailing whitespaces from annotated spans
v0.15.1,Search start and end token index for current span
v0.15.1,If end index is not found set to last token
v0.15.1,Throw error if indices are not valid
v0.15.1,Add metadatas for sentence
v0.15.1,Currently all Jsonl Datasets are stored in Memory
v0.15.1,get train data
v0.15.1,read in test file if exists
v0.15.1,read in dev file if exists
v0.15.1,"find train, dev and test files if not specified"
v0.15.1,special key for space after
v0.15.1,special key for feature columns
v0.15.1,special key for dependency head id
v0.15.1,"store either Sentence objects in memory, or only file offsets"
v0.15.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.15.1,determine encoding of text file
v0.15.1,identify which columns are spans and which are word-level
v0.15.1,now load all sentences
v0.15.1,skip first line if to selected
v0.15.1,option 1: keep Sentence objects in memory
v0.15.1,pointer to previous
v0.15.1,parse next sentence
v0.15.1,quit if last sentence reached
v0.15.1,skip banned sentences
v0.15.1,set previous and next sentence for context
v0.15.1,append parsed sentence to list in memory
v0.15.1,option 2: keep source data in memory
v0.15.1,"read lines for next sentence, but don't parse"
v0.15.1,quit if last sentence reached
v0.15.1,append raw lines for each sentence
v0.15.1,we make a distinction between word-level tags and span-level tags
v0.15.1,read first sentence to determine which columns are span-labels
v0.15.1,skip first line if to selected
v0.15.1,check the first 5 sentences
v0.15.1,go through all annotations and identify word- and span-level annotations
v0.15.1,- if a column has at least one BIES we know it's a Span label
v0.15.1,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.15.1,- problem cases are columns for which we see only O - in this case we default to Span
v0.15.1,skip assigned columns
v0.15.1,the space after key is always word-levels
v0.15.1,"if at least one token has a BIES, we know it's a span label"
v0.15.1,"if at least one token has a label other than BIOES, we know it's a token label"
v0.15.1,all remaining columns that are not word-level are span-level
v0.15.1,for column in self.word_level_tag_columns:
v0.15.1,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.15.1,"if sentence ends, break"
v0.15.1,parse comments if possible
v0.15.1,"otherwise, this line is a token. parse and add to sentence"
v0.15.1,check if this sentence is a document boundary
v0.15.1,add span labels
v0.15.1,discard tags from tokens that are not added to the sentence
v0.15.1,parse relations if they are set
v0.15.1,head and tail span indices are 1-indexed and end index is inclusive
v0.15.1,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.15.1,"to set the metadata ""domain"" to ""de-orcas"""
v0.15.1,get fields from line
v0.15.1,get head_id if exists (only in dependency parses)
v0.15.1,initialize token
v0.15.1,go through all columns
v0.15.1,'feats' and 'misc' column should be split into different fields
v0.15.1,special handling for whitespace after
v0.15.1,add each other feature as label-value pair
v0.15.1,get the task name (e.g. 'ner')
v0.15.1,get the label value
v0.15.1,add label
v0.15.1,remap regular tag names
v0.15.1,"if in memory, retrieve parsed sentence"
v0.15.1,else skip to position in file where sentence begins
v0.15.1,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.15.1,use all domains
v0.15.1,iter over all domains / sources and create target files
v0.15.1,The conll representation of coref spans allows spans to
v0.15.1,"overlap. If spans end or begin at the same word, they are"
v0.15.1,"separated by a ""|""."
v0.15.1,The span begins at this word.
v0.15.1,The span begins and ends at this word (single word span).
v0.15.1,"The span is starting, so we record the index of the word."
v0.15.1,"The span for this id is ending, but didn't start at this word."
v0.15.1,Retrieve the start index from the document state and
v0.15.1,add the span to the clusters for this id.
v0.15.1,strip all bracketing information to
v0.15.1,get the actual propbank label.
v0.15.1,Entering into a span for a particular semantic role label.
v0.15.1,We append the label and set the current span for this annotation.
v0.15.1,"If there's no '(' token, but the current_span_label is not None,"
v0.15.1,then we are inside a span.
v0.15.1,We're outside a span.
v0.15.1,"Exiting a span, so we reset the current span label for this annotation."
v0.15.1,The words in the sentence.
v0.15.1,The pos tags of the words in the sentence.
v0.15.1,the pieces of the parse tree.
v0.15.1,The lemmatised form of the words in the sentence which
v0.15.1,have SRL or word sense information.
v0.15.1,The FrameNet ID of the predicate.
v0.15.1,"The sense of the word, if available."
v0.15.1,"The current speaker, if available."
v0.15.1,"Cluster id -> List of (start_index, end_index) spans."
v0.15.1,Cluster id -> List of start_indices which are open for this id.
v0.15.1,Replace brackets in text and pos tags
v0.15.1,with a different token for parse trees.
v0.15.1,only keep ')' if there are nested brackets with nothing in them.
v0.15.1,There are some bad annotations in the CONLL data.
v0.15.1,"They contain no information, so to make this explicit,"
v0.15.1,we just set the parse piece to be None which will result
v0.15.1,in the overall parse tree being None.
v0.15.1,"If this is the first word in the sentence, create"
v0.15.1,empty lists to collect the NER and SRL BIO labels.
v0.15.1,"We can't do this upfront, because we don't know how many"
v0.15.1,"components we are collecting, as a sentence can have"
v0.15.1,variable numbers of SRL frames.
v0.15.1,Create variables representing the current label for each label
v0.15.1,sequence we are collecting.
v0.15.1,"If any annotation marks this word as a verb predicate,"
v0.15.1,we need to record its index. This also has the side effect
v0.15.1,of ordering the verbal predicates by their location in the
v0.15.1,"sentence, automatically aligning them with the annotations."
v0.15.1,"this would not be reached if parse_pieces contained None, hence the cast"
v0.15.1,Non-empty line. Collect the annotation.
v0.15.1,Collect any stragglers or files which might not
v0.15.1,have the '#end document' format for the end of the file.
v0.15.1,this dataset name
v0.15.1,check if data there
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,check if data there
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,download files if not present locally
v0.15.1,we need to slightly modify the original files by adding some new lines after document separators
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,Set the base path for the dataset
v0.15.1,Define column format
v0.15.1,Define dataset name
v0.15.1,Define data folder path
v0.15.1,"Check if the train data file exists, otherwise download and prepare the dataset"
v0.15.1,Download and prepare the dataset
v0.15.1,Initialize the parent class with the specified parameters
v0.15.1,"Check if the line is a change, delete or add command (like 17721c17703,17705 or 5728d5727)"
v0.15.1,Append the previous change block to the changes list
v0.15.1,Start a new change block
v0.15.1,"Capture original lines (those marked with ""<"")"
v0.15.1,"Capture new lines (those marked with "">"")"
v0.15.1,Append the last change block to the changes list
v0.15.1,Apply each change in reverse order (important to avoid index shift issues)
v0.15.1,"Determine the type of the change: `c` for change, `d` for delete, `a` for add"
v0.15.1,"Example command: 17721c17703,17705"
v0.15.1,Example command: 5728d5727
v0.15.1,"Example command: 1000a1001,1002"
v0.15.1,Write the modified content to the output file
v0.15.1,Strip whitespace to check if the line is empty
v0.15.1,Write the first token followed by a newline if the line is not empty
v0.15.1,Write an empty line if the line is empty
v0.15.1,Strip the leading '[TOKEN]\t' from the annotation
v0.15.1,Create a temporary directory
v0.15.1,Check the contents of the temporary directory
v0.15.1,Extract only the tokens from the original CoNLL03 files
v0.15.1,Apply the downloaded patch files to apply our token modifications (e.g. line breaks)
v0.15.1,Merge the updated token files with the CleanCoNLL annotations
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,Remove CoNLL-U meta information in the last column
v0.15.1,column format
v0.15.1,dataset name
v0.15.1,data folder: default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,dataset name
v0.15.1,data folder: default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,entity_mapping
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,data validation
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download files if not present locallys
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,# download zip
v0.15.1,merge the files in one as the zip is containing multiples files
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,check if data there
v0.15.1,create folder
v0.15.1,download dataset
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download and parse data if necessary
v0.15.1,create train test dev if not exist
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,If the extracted corpus file is not yet present in dir
v0.15.1,download zip if necessary
v0.15.1,"extracted corpus is not present , so unpacking it."
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download zip
v0.15.1,unpacking the zip
v0.15.1,merge the files in one as the zip is containing multiples files
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.15.1,download files if not present locally
v0.15.1,we need to modify the original files by adding new lines after after the end of each sentence
v0.15.1,if only one language is given
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"use all languages if explicitly set to ""all"""
v0.15.1,download data if necessary
v0.15.1,initialize comlumncorpus and add it to list
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"For each language in languages, the file is downloaded if not existent"
v0.15.1,Then a comlumncorpus of that data is created and saved in a list
v0.15.1,this list is handed to the multicorpus
v0.15.1,list that contains the columncopora
v0.15.1,download data if necessary
v0.15.1,"if language not downloaded yet, download it"
v0.15.1,create folder
v0.15.1,get google drive id from list
v0.15.1,download from google drive
v0.15.1,unzip
v0.15.1,transform data into required format
v0.15.1,"the processed dataset has the additional ending ""_new"""
v0.15.1,remove the unprocessed dataset
v0.15.1,initialize comlumncorpus and add it to list
v0.15.1,if no languages are given as argument all languages used in XTREME will be loaded
v0.15.1,if only one language is given
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"For each language in languages, the file is downloaded if not existent"
v0.15.1,Then a comlumncorpus of that data is created and saved in a list
v0.15.1,This list is handed to the multicorpus
v0.15.1,list that contains the columncopora
v0.15.1,download data if necessary
v0.15.1,"if language not downloaded yet, download it"
v0.15.1,create folder
v0.15.1,download from HU Server
v0.15.1,unzip
v0.15.1,transform data into required format
v0.15.1,initialize comlumncorpus and add it to list
v0.15.1,if only one language is given
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,initialize comlumncorpus and add it to list
v0.15.1,download data if necessary
v0.15.1,unpack and write out in CoNLL column-like format
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,data is not in IOB2 format. Thus we transform it to IOB2
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,rename according to train - test - dev - convention
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,Add missing newline after header
v0.15.1,Workaround for empty tokens
v0.15.1,"Add ""real"" document marker"
v0.15.1,Dataset split mapping
v0.15.1,v2.0 only adds new language and splits for AJMC dataset
v0.15.1,Special document marker for sample splits in AJMC dataset
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"No test data is available, so do not shrink dev data for shared task preparation!"
v0.15.1,create dataset files from index and train/test splits
v0.15.1,news date is usually in 3rd or 4th sentence of each article
v0.15.1,"generate NoiseBench dataset variants, given CleanCoNLL, noisy label files and index file"
v0.15.1,"os.makedirs(os.path.join('data','noisebench'), exist_ok=True)"
v0.15.1,copy test set
v0.15.1,if only one language is given
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"use all languages if explicitly set to ""all"""
v0.15.1,download data if necessary
v0.15.1,initialize comlumncorpus and add it to list
v0.15.1,Get original version
v0.15.1,Add sentence boundary marker
v0.15.1,"Only allowed classes in course setting are: PER, LOC, ORG and MISC."
v0.15.1,"All other NEs are normalized to O, except EVENT and WOA are normalized to MISC (cf. Table 3 of paper)."
v0.15.1,this dataset name
v0.15.1,one name can map to multiple concepts
v0.15.1,NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.
v0.15.1,Use this class to load into memory all candidates
v0.15.1,"if identifier == ""MESH:D013749"":"
v0.15.1,# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.
v0.15.1,continue
v0.15.1,parse line
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download and parse data if necessary
v0.15.1,paths to train and test splits
v0.15.1,init corpus
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download and parse data if necessary
v0.15.1,iterate over all html files
v0.15.1,"get rid of html syntax, we only need the text"
v0.15.1,between all documents we write a separator symbol
v0.15.1,skip empty strings
v0.15.1,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.15.1,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.15.1,sentence splitting and tokenization
v0.15.1,iterate through all annotations and add to corresponding tokens
v0.15.1,find sentence to which annotation belongs
v0.15.1,position within corresponding sentence
v0.15.1,set annotation for tokens of entity mention
v0.15.1,write to out-file in column format
v0.15.1,"in case something goes wrong, delete the dataset and raise error"
v0.15.1,this dataset name
v0.15.1,download and parse data if necessary
v0.15.1,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.15.1,generate qid wikiname dictionaries
v0.15.1,merge dictionaries
v0.15.1,ignore first line
v0.15.1,commented and empty lines
v0.15.1,read all Q-IDs
v0.15.1,ignore first line
v0.15.1,request
v0.15.1,this dataset name
v0.15.1,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.15.1,like this we can quickly check if the corresponding page exists
v0.15.1,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.15.1,delete unprocessed file
v0.15.1,collect all wikiids
v0.15.1,create the dictionary
v0.15.1,request
v0.15.1,this dataset name
v0.15.1,names of raw text documents
v0.15.1,open output_file
v0.15.1,iterate through all documents
v0.15.1,split sentences and tokenize
v0.15.1,iterate through all annotations and add to corresponding tokens
v0.15.1,find sentence to which annotation belongs
v0.15.1,position within corresponding sentence
v0.15.1,set annotation for tokens of entity mention
v0.15.1,write to out file
v0.15.1,annotation from one annotator or two agreeing annotators
v0.15.1,this dataset name
v0.15.1,download and parse data if necessary
v0.15.1,this dataset name
v0.15.1,download and parse data if necessary
v0.15.1,First parse the post titles
v0.15.1,Keep track of how many and which entity mentions does a given post title have
v0.15.1,Check if the current post title has an entity link and parse accordingly
v0.15.1,Post titles with entity mentions (if any) are handled via this function
v0.15.1,Then parse the comments
v0.15.1,"Iterate over the comments.tsv file, until the end is reached"
v0.15.1,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.15.1,Each comment thread is handled as one 'document'.
v0.15.1,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.15.1,This if-condition is needed to handle this problem.
v0.15.1,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.15.1,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.15.1,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.15.1,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.15.1,and not just single letters into single rows.
v0.15.1,If there are annotated entity mentions for given post title or a comment thread
v0.15.1,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.15.1,Write the token with a corresponding tag to file
v0.15.1,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.15.1,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.15.1,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.15.1,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.15.1,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.15.1,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.15.1,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.15.1,Check if further annotations belong to the current post title or comment thread as well
v0.15.1,Stop when the end of an annotation file is reached
v0.15.1,Check if further annotations belong to the current sentence as well
v0.15.1,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.15.1,Docstart
v0.15.1,if there is more than one word in the chunk we write each in a separate line
v0.15.1,print(chunks)
v0.15.1,empty line after each sentence
v0.15.1,convert the file to CoNLL
v0.15.1,this dataset name
v0.15.1,"check if data there, if not, download the data"
v0.15.1,create folder
v0.15.1,download data
v0.15.1,transform data into column format if necessary
v0.15.1,if no filenames are specified we use all the data
v0.15.1,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.15.1,also we remove 'raganato_ALL' from filenames in case its in the list
v0.15.1,generate the test file
v0.15.1,make column file and save to data_folder
v0.15.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.1,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.1,create folder
v0.15.1,download data
v0.15.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.1,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.1,create folder
v0.15.1,download data
v0.15.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.1,generate the test file
v0.15.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.1,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.1,create folder
v0.15.1,download data
v0.15.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.1,generate the test file
v0.15.1,default dataset folder is the cache root
v0.15.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.1,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.1,create folder
v0.15.1,download data
v0.15.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.1,generate the test file
v0.15.1,default dataset folder is the cache root
v0.15.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.1,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.1,create folder
v0.15.1,download data
v0.15.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.1,generate the test file
v0.15.1,default dataset folder is the cache root
v0.15.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.1,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.1,create folder
v0.15.1,download data
v0.15.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.1,generate the test file
v0.15.1,TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146
v0.15.1,+1 assumes the title and abstract will be joined by a space.
v0.15.1,"We need a unique identifier for this entity, so build it from the document id and entity id"
v0.15.1,The user can provide a callable that returns the database name.
v0.15.1,some entities are not linked and
v0.15.1,some entities are linked to multiple normalized ids
v0.15.1,passages must not overlap and spans must cover the entire document
v0.15.1,entities
v0.15.1,parse db ids
v0.15.1,Some of the entities have a off-by-one error. Correct these annotations!
v0.15.1,"passage offsets/lengths do not connect, recalculate them for this schema."
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,if True:
v0.15.1,write CoNLL-U Plus header
v0.15.1,"Some special cases (e.g., missing spaces before entity marker)"
v0.15.1,necessary if text should be whitespace tokenizeable
v0.15.1,Handle case where tail may occur before the head
v0.15.1,this dataset name
v0.15.1,write CoNLL-U Plus header
v0.15.1,this dataset name
v0.15.1,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.15.1,download data if necessary
v0.15.1,write CoNLL-U Plus header
v0.15.1,The span has ended.
v0.15.1,We are entering a new span; reset indices
v0.15.1,and active tag to new span.
v0.15.1,We're inside a span.
v0.15.1,Last token might have been a part of a valid span.
v0.15.1,this dataset name
v0.15.1,write CoNLL-U Plus header
v0.15.1,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.15.1,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.15.1,target_file_path = Path(data_folder) / target_filename
v0.15.1,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.15.1,# write CoNLL-U Plus header
v0.15.1,"target_file.write(""# global.columns = id form ner\n"")"
v0.15.1,for example in json.load(source_file):
v0.15.1,token_list = self._tacred_example_to_token_list(example)
v0.15.1,target_file.write(token_list.serialize())
v0.15.1,check if first tag row is already occupied
v0.15.1,"if first tag row is occupied, use second tag row"
v0.15.1,hardcoded mapping TODO: perhaps find nicer solution
v0.15.1,remap regular tag names
v0.15.1,else skip to position in file where sentence begins
v0.15.1,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.15.1,read in dev file if exists
v0.15.1,read in test file if exists
v0.15.1,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.15.1,"find train, dev and test files if not specified"
v0.15.1,use test_file to create test split if available
v0.15.1,use dev_file to create test split if available
v0.15.1,"if data point contains black-listed label, do not use"
v0.15.1,first check if valid sentence
v0.15.1,"if so, add to indices"
v0.15.1,"find train, dev and test files if not specified"
v0.15.1,variables
v0.15.1,different handling of in_memory data than streaming data
v0.15.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.15.1,test if format is OK
v0.15.1,test if at least one label given
v0.15.1,make sentence from text (and filter for length)
v0.15.1,"if a pair column is defined, make a sentence pair object"
v0.15.1,noinspection PyDefaultArgument
v0.15.1,dataset name includes the split size
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download each of the 28 splits
v0.15.1,create dataset directory if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,download data from same source as in huggingface's implementations
v0.15.1,read label order
v0.15.1,"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
v0.15.1,"Re-map to [0, 1, 2, 3]."
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,handle labels file
v0.15.1,handle data file
v0.15.1,Create flair compatible labels
v0.15.1,"by default, map point score to POSITIVE / NEGATIVE values"
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,create dataset directory if necessary
v0.15.1,create train.txt file from CSV
v0.15.1,create test.txt file from CSV
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,create dataset directory if necessary
v0.15.1,create train.txt file by iterating over pos and neg file
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,create dataset directory if necessary
v0.15.1,create train.txt file by iterating over pos and neg file
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,create dataset directory if necessary
v0.15.1,create train.txt file by iterating over pos and neg file
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,create dataset directory if necessary
v0.15.1,create train.txt file by iterating over pos and neg file
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,create train dev and test files in fasttext format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download senteval datasets if necessary und unzip
v0.15.1,convert to FastText format
v0.15.1,download data if necessary
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,move original .tsv files to another folder
v0.15.1,create train and dev splits in fasttext format
v0.15.1,create eval_dataset file with no labels
v0.15.1,download zip archive
v0.15.1,unpack file in datasets directory (zip archive contains a directory named SST-2)
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,download datasets if necessary
v0.15.1,create dataset directory if necessary
v0.15.1,create correctly formated txt files
v0.15.1,multiple labels are possible
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,Create flair compatible labels
v0.15.1,TREC-6 : NUM:dist -> __label__NUM
v0.15.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,Create flair compatible labels
v0.15.1,TREC-6 : NUM:dist -> __label__NUM
v0.15.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,create a separate directory for different tasks
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,check if dataset is supported
v0.15.1,set file names
v0.15.1,set file names
v0.15.1,download and unzip in file structure if necessary
v0.15.1,instantiate corpus
v0.15.1,"find train, dev and test files if not specified"
v0.15.1,"create DataPairDataset for train, test and dev file, if they are given"
v0.15.1,stop if file does not exist
v0.15.1,create a DataPair object from strings
v0.15.1,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.15.1,"find train, dev, and test files if not specified"
v0.15.1,"create DataTripleDataset for train, test, and dev files, if they are given"
v0.15.1,stop if the file does not exist
v0.15.1,create a DataTriple object from strings
v0.15.1,"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings"
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,"rename test file to eval_dataset, since it has no labels"
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.15.1,dev sets include 5 different annotations but we will only keep the gold label
v0.15.1,"rename test file to eval_dataset, since it has no labels"
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get test and dev sets
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,"rename test file to eval_dataset, since it has no labels"
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,"rename test file to eval_dataset, since it has no labels"
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,"rename test file to eval_dataset, since it has no labels"
v0.15.1,"if data is not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,"rename test file to eval_dataset, since it has no labels"
v0.15.1,"if data not downloaded yet, download it"
v0.15.1,get the zip file
v0.15.1,"the downloaded files have json format, we transform them to tsv"
v0.15.1,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.15.1,remove json file
v0.15.1,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.15.1,with sum of all entity lengths as secondary key
v0.15.1,calculate offset without current text
v0.15.1,because we stick all passages of a document together
v0.15.1,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.15.1,Try to fix incorrect annotations
v0.15.1,print(
v0.15.1,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.15.1,)
v0.15.1,Ignore empty lines or relation annotations
v0.15.1,FIX annotation of whitespaces (necessary for PDR)
v0.15.1,Add task description for multi-task learning
v0.15.1,One token may contain multiple entities -> deque all of them
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.15.1,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,Edge case: last token starts a new entity
v0.15.1,Last document in file
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,In the huner split files there is no information whether a given id originates
v0.15.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.15.1,splitting here
v0.15.1,Edge case: last token starts a new entity
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,Read texts
v0.15.1,Read annotations
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,We need to apply a patch to correct the original training file
v0.15.1,Articles title
v0.15.1,Article abstract
v0.15.1,Entity annotations
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,Edge case: last token starts a new entity
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,Incomplete article
v0.15.1,Invalid XML syntax
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,if len(mid) != 3:
v0.15.1,continue
v0.15.1,Try to fix entity offsets
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,There is still one illegal annotation in the file ..
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"Abstract first, title second to prevent issues with sentence splitting"
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,"Filter for specific entity types, by default no entities will be filtered"
v0.15.1,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.15.1,train and dev split of V2 will be train in V4
v0.15.1,test split of V2 will be dev in V4
v0.15.1,New documents in V4 will become test documents
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,column format
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,column format
v0.15.1,build dataset name and full huggingface reference name
v0.15.1,Download data if necessary
v0.15.1,"Some datasets in BigBio only have train or test splits, not both"
v0.15.1,"If only test split, assign it to train split"
v0.15.1,"If only train split, sample other from it (sample_missing_splits=True)"
v0.15.1,Not every dataset has a dev / validation set!
v0.15.1,Perform type mapping if necessary
v0.15.1,return None
v0.15.1,TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?
v0.15.1,"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG"
v0.15.1,"""cancer"": ""disease"",  # BioNLP ST 2013 CG"
v0.15.1,"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG"
v0.15.1,"""gene"": ""gene"",  # NLM Gene"
v0.15.1,"""chemical"": ""chemical"",  # NLM Chem"
v0.15.1,"""cellline"": ""cell_line"",  # Cell Finder"
v0.15.1,"""species"": ""species"",  # Cell Finder"
v0.15.1,"""protein"": ""gene"",  # BioID"
v0.15.1,"Collect all texts of the document, each passage will be"
v0.15.1,a text in our internal format
v0.15.1,Sort passages by start offset
v0.15.1,Transform all entity annotations into internal format
v0.15.1,Find the passage of the entity (necessary for offset adaption)
v0.15.1,Adapt entity offsets according to passage offsets
v0.15.1,FIXME: This is just for debugging purposes
v0.15.1,passage_text = id_to_text[passage_id]
v0.15.1,doc_text = passage_text[entity_offset[0] : entity_offset[1]]
v0.15.1,"mention_text = entity[""text""][0]"
v0.15.1,if doc_text != mention_text:
v0.15.1,"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
v0.15.1,Get element in the middle
v0.15.1,Is the mention with the passage offsets?
v0.15.1,"If element is smaller than mid, then it can only"
v0.15.1,be present in left subarray
v0.15.1,Else the element can only be present in right subarray
v0.15.1,TODO whether cell or cell line is the correct tag
v0.15.1,TODO whether cell or cell line is the correct tag
v0.15.1,Special case for ProGene: We need to use the split_0_train and split_0_test splits
v0.15.1,as they are currently provided in BigBio
v0.15.1,cache Feidegger config file
v0.15.1,cache Feidegger images
v0.15.1,replace image URL with local cached file
v0.15.1,append Sentence-Image data point
v0.15.1,cast to list if necessary
v0.15.1,cast to list if necessary
v0.15.1,"first, check if pymongo is installed"
v0.15.1,automatically identify train / test / dev files
v0.15.1,"if no test file is found, take any file with 'test' in name"
v0.15.1,Expose base classses
v0.15.1,Expose all biomedical data sets used for the evaluation of BioBERT
v0.15.1,-
v0.15.1,-
v0.15.1,-
v0.15.1,-
v0.15.1,Expose all biomedical data sets using the HUNER splits
v0.15.1,Expose all biomedical data sets
v0.15.1,Expose all document classification datasets
v0.15.1,word sense disambiguation
v0.15.1,Expose all entity linking datasets
v0.15.1,Expose all relation extraction datasets
v0.15.1,universal proposition banks
v0.15.1,keyphrase detection datasets
v0.15.1,other NER datasets
v0.15.1,standard NER datasets
v0.15.1,Expose all sequence labeling datasets
v0.15.1,Expose all text-image datasets
v0.15.1,Expose all text-text datasets
v0.15.1,Expose all treebanks
v0.15.1,"find train, dev and test files if not specified"
v0.15.1,get train data
v0.15.1,get test data
v0.15.1,get dev data
v0.15.1,option 1: read only sentence boundaries as offset positions
v0.15.1,option 2: keep everything in memory
v0.15.1,"if in memory, retrieve parsed sentence"
v0.15.1,else skip to position in file where sentence begins
v0.15.1,current token ID
v0.15.1,handling for the awful UD multiword format
v0.15.1,end of sentence
v0.15.1,comments or ellipsis
v0.15.1,if token is a multi-word
v0.15.1,normal single-word tokens
v0.15.1,"if we don't split multiwords, skip over component words"
v0.15.1,add token
v0.15.1,add morphological tags
v0.15.1,derive whitespace logic for multiwords
v0.15.1,"if multi-word equals component tokens, there should be no whitespace"
v0.15.1,go through all tokens in subword and set whitespace_after information
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,this dataset name
v0.15.1,default dataset folder is the cache root
v0.15.1,download data if necessary
v0.15.1,"finally, print model card for information"
v0.15.1,Note: Multi-GPU can affect corpus loading
v0.15.1,This code will run multiple times -- each GPU gets its own process and each process runs this code. We need to
v0.15.1,"ensure that the corpus has the same elements and order on all processes, despite sampling. We do that by using"
v0.15.1,the same seed on all processes.
v0.15.1,Note: Multi-GPU can affect choice of batch size.
v0.15.1,"In order to compare batch updates fairly between single and multi-GPU training, we should:"
v0.15.1,1) Step the optimizer after the same number of examples to achieve com
v0.15.1,2) Process the same number of examples in each forward pass
v0.15.1,"e.g. Suppose your machine has 2 GPUs. If multi_gpu=False, the first gpu will process 32 examples, then the"
v0.15.1,"first gpu will process another 32 examples, then the optimizer will step. If multi_gpu=True, each gpu will"
v0.15.1,"process 32 examples at the same time, then the optimizer will step."
v0.15.1,noqa: INP001
v0.15.1,-- Project information -----------------------------------------------------
v0.15.1,"The full version, including alpha/beta/rc tags"
v0.15.1,use smv_current_version as the git url
v0.15.1,-- General configuration ---------------------------------------------------
v0.15.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.15.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.15.1,ones.
v0.15.1,"Add any paths that contain templates here, relative to this directory."
v0.15.1,"List of patterns, relative to source directory, that match files and"
v0.15.1,directories to ignore when looking for source files.
v0.15.1,This pattern also affects html_static_path and html_extra_path.
v0.15.1,-- Options for HTML output -------------------------------------------------
v0.15.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.15.1,a list of builtin themes.
v0.15.1,
v0.15.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.15.1,"relative to this directory. They are copied after the builtin static files,"
v0.15.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.15.1,Napoleon settings
v0.15.1,Whitelist pattern for tags (set to None to ignore all tags)
v0.15.1,Whitelist pattern for branches (set to None to ignore all branches)
v0.15.1,Whitelist pattern for remotes (set to None to use local branches only)
v0.15.1,Pattern for released versions
v0.15.1,Format for versioned output directories inside the build directory
v0.15.1,Determines whether remote or local git branches/tags are preferred if their output dirs conflict
v0.15.1,test corpus
v0.15.1,create a TARS classifier
v0.15.1,check if right number of classes
v0.15.1,switch to task with only one label
v0.15.1,check if right number of classes
v0.15.1,switch to task with three labels provided as list
v0.15.1,check if right number of classes
v0.15.1,switch to task with four labels provided as set
v0.15.1,check if right number of classes
v0.15.1,switch to task with two labels provided as Dictionary
v0.15.1,check if right number of classes
v0.15.1,test corpus
v0.15.1,create a TARS classifier
v0.15.1,switch to a new task (TARS can do multiple tasks so you must define one)
v0.15.1,initialize the text classifier trainer
v0.15.1,start the training
v0.15.1,"With end symbol, without start symbol, padding in front"
v0.15.1,"Without end symbol, with start symbol, padding in back"
v0.15.1,"Without end symbol, without start symbol, padding in front"
v0.15.1,initialize trainer
v0.15.1,initialize trainer
v0.15.1,initialize trainer
v0.15.1,clean up directory
v0.15.1,clean up directory
v0.15.1,example sentence
v0.15.1,set 4 labels for 2 tokens ('love' is tagged twice)
v0.15.1,check if there are three POS labels with correct text and values
v0.15.1,check if there are is one SENTIMENT label with correct text and values
v0.15.1,check if all tokens are correctly labeled
v0.15.1,remove the pos label from the last word
v0.15.1,there should be 2 POS labels left
v0.15.1,now remove all pos tags
v0.15.1,set 3 labels for 2 spans (HU is tagged twice)
v0.15.1,check if there are three labels with correct text and values
v0.15.1,check if there are two spans with correct text and values
v0.15.1,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.15.1,should be only one NER label left
v0.15.1,and only one NER span
v0.15.1,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.15.1,check if there are three labels with correct text and values
v0.15.1,check if there are two spans with correct text and values
v0.15.1,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.15.1,should be only one NER label left
v0.15.1,and only one NER span
v0.15.1,but there is also one orgtype span and label
v0.15.1,and only one NER span
v0.15.1,let's add the NER tag back
v0.15.1,check if there are three labels with correct text and values
v0.15.1,check if there are two spans with correct text and values
v0.15.1,now remove all NER tags
v0.15.1,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.15.1,create two relation label
v0.15.1,there should be two relation labels
v0.15.1,there should be one syntactic labels
v0.15.1,"there should be two relations, one with two and one with one label"
v0.15.1,example sentence
v0.15.1,add another topic label
v0.15.1,example sentence
v0.15.1,has sentiment value
v0.15.1,has 4 part of speech tags
v0.15.1,has 1 NER tag
v0.15.1,should be in total 6 labels
v0.15.1,example sentence
v0.15.1,add two NER labels
v0.15.1,get the four labels
v0.15.1,check that only two of the respective data points are equal
v0.15.1,make a sentence and some right context
v0.15.1,TODO: is this desirable? Or should two sentences with same text be considered same objects?
v0.15.1,Initializing a Sentence this way assumes that there is a space after each token
v0.15.1,get default dictionary
v0.15.1,init forward LM with 128 hidden states and 1 layer
v0.15.1,get the example corpus and process at character level in forward direction
v0.15.1,train the language model
v0.15.1,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.15.1,clean up results directory
v0.15.1,get default dictionary
v0.15.1,init forward LM with 128 hidden states and 1 layer
v0.15.1,get the example corpus and process at character level in forward direction
v0.15.1,train the language model
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,now exclude -DOCSTART- sentences
v0.15.1,now load whole documents as sentences
v0.15.1,ban each boundary but set each sentence to be independent
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,load column dataset with one entry
v0.15.1,load column dataset with two entries
v0.15.1,load column dataset with three entries
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,check if Token labels are correct
v0.15.1,"get training, test and dev data"
v0.15.1,check if Token labels for frames are correct
v0.15.1,"get training, test and dev data"
v0.15.1,"get training, test and dev data"
v0.15.1,get two corpora as one
v0.15.1,"get training, test and dev data for full English UD corpus from web"
v0.15.1,clean up data directory
v0.15.1,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.15.1,"""2"","
v0.15.1,"""0"","
v0.15.1,"""4"","
v0.15.1,"""2"","
v0.15.1,"""2"","
v0.15.1,"""2"","
v0.15.1,]
v0.15.1,This test only covers basic universal dependencies datasets.
v0.15.1,"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
v0.15.1,"Here, we use the default token annotation fields."
v0.15.1,This test covers the complete HIPE 2022 dataset.
v0.15.1,https://github.com/hipe-eval/HIPE-2022-data
v0.15.1,"Includes variant with document separator, and all versions of the dataset."
v0.15.1,"We have manually checked, that these numbers are correct:"
v0.15.1,"+1 offset, because of missing EOS marker at EOD"
v0.15.1,Test data for v2.1 release
v0.15.1,This test covers the complete ICDAR Europeana corpus:
v0.15.1,https://github.com/stefan-it/historic-domain-adaptation-icdar
v0.15.1,"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
v0.15.1,This test covers the NERMuD dataset. Official stats can be found here:
v0.15.1,https://github.com/dhfbk/KIND/tree/main/evalita-2023
v0.15.1,Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
v0.15.1,This test covers the complete MasakhaPOS dataset.
v0.15.1,"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
v0.15.1,--- Embeddings that are shared by both models --- #
v0.15.1,--- Task 1: Sentiment Analysis (5-class) --- #
v0.15.1,Define corpus and model
v0.15.1,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.15.1,Define corpus and model
v0.15.1,-- Define mapping (which tagger should train on which model) -- #
v0.15.1,-- Create model trainer and train -- #
v0.15.1,NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
v0.15.1,clean up file
v0.15.1,get features from forward propagation
v0.15.1,reverse sort all sequences by their length
v0.15.1,remove previously predicted labels of this type
v0.15.1,no need for label_dict
v0.15.1,"pretrained_model = ""tars-ner""  # disabled due to too much space requirements."
v0.15.1,check if right number of classes
v0.15.1,switch to task with only one label
v0.15.1,check if right number of classes
v0.15.1,switch to task with three labels provided as list
v0.15.1,check if right number of classes
v0.15.1,switch to task with four labels provided as set
v0.15.1,check if right number of classes
v0.15.1,switch to task with two labels provided as Dictionary
v0.15.1,check if right number of classes
v0.15.1,Intel ----founded_by---> Gordon Moore
v0.15.1,Intel ----founded_by---> Robert Noyce
v0.15.1,"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
v0.15.1,Check sentence masking and relation label annotation on
v0.15.1,"training, validation and test dataset (in this test the splits are the same)"
v0.15.1,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.15.1,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.15.1,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.15.1,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany ."""
v0.15.1,"Entity pair permutations of: ""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin ."""
v0.15.1,This sentence is only included if we transform the corpus with cross augmentation
v0.15.1,"""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin ."""
v0.15.1,"""The German - American computer scientist Joseph Weizenbaum ( 8 January 1923 - 5 March 2008 ) was born in Berlin ."""
v0.15.1,Simulate training epoch
v0.15.1,Simulate training batch
v0.15.1,"pretrained_model = ""tars-base""  # disabled due to too much space requirements."
v0.15.1,Ensure this is an example that predicts no classes in multilabel
v0.15.1,check if right number of classes
v0.15.1,switch to task with only one label
v0.15.1,check if right number of classes
v0.15.1,switch to task with three labels provided as list
v0.15.1,check if right number of classes
v0.15.1,switch to task with four labels provided as set
v0.15.1,check if right number of classes
v0.15.1,switch to task with two labels provided as Dictionary
v0.15.1,check if right number of classes
v0.15.1,ensure that the prepared tensors is what we expect
v0.15.1,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.15.1,previous and next sentence as context
v0.15.1,test expansion for sentence without context
v0.15.1,test expansion for with previous and next as context
v0.15.1,test expansion if first sentence is document boundary
v0.15.1,test expansion if we don't use context
v0.15.1,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.15.1,dummy model with embeddings
v0.15.1,save the dummy and load it again
v0.15.1,check that context_length and use_context_separator is the same for both
v0.15.0,mmap seems to be much more memory efficient
v0.15.0,Remove quotes from etag
v0.15.0,"If there is an etag, it's everything after the first period"
v0.15.0,"Otherwise, use None"
v0.15.0,"URL, so get it from the cache (downloading if necessary)"
v0.15.0,"File, and it exists."
v0.15.0,"File, but it doesn't exist."
v0.15.0,Something unknown
v0.15.0,Extract all the contents of zip file in current directory
v0.15.0,use model name as subfolder
v0.15.0,Lazy import
v0.15.0,output information
v0.15.0,Extract all the contents of zip file in current directory
v0.15.0,TODO(joelgrus): do we want to do checksums or anything like that?
v0.15.0,get cache path to put the file
v0.15.0,make HEAD request to check ETag
v0.15.0,add ETag to filename if it exists
v0.15.0,"etag = response.headers.get(""ETag"")"
v0.15.0,"Download to temporary file, then copy to cache dir once finished."
v0.15.0,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.15.0,GET file object
v0.15.0,These defaults are the same as the argument defaults in tqdm.
v0.15.0,load_big_file is a workaround byhttps://github.com/highway11git
v0.15.0,to load models on some Mac/Windows setups
v0.15.0,see https://github.com/zalandoresearch/flair/issues/351
v0.15.0,first determine the distribution of classes in the dataset
v0.15.0,weight for each sample
v0.15.0,Create blocks
v0.15.0,shuffle the blocks
v0.15.0,concatenate the shuffled blocks
v0.15.0,Create blocks
v0.15.0,shuffle the blocks
v0.15.0,concatenate the shuffled blocks
v0.15.0,increment for last token in sentence if not followed by whitespace
v0.15.0,this is the default init size of a lmdb database for embeddings
v0.15.0,get db filename from embedding name
v0.15.0,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.15.0,SequenceTagger
v0.15.0,TextClassifier
v0.15.0,get db filename from embedding name
v0.15.0,if embedding database already exists
v0.15.0,"otherwise, push embedding to database"
v0.15.0,if embedding database already exists
v0.15.0,open the database in read mode
v0.15.0,we need to set self.k
v0.15.0,create and load the database in write mode
v0.15.0,"no idea why, but we need to close and reopen the environment to avoid"
v0.15.0,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.15.0,when opening new transaction !
v0.15.0,init dictionaries
v0.15.0,"in order to deal with unknown tokens, add <unk>"
v0.15.0,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.15.0,set 'add_unk' depending on whether <unk> is a key
v0.15.0,"if one embedding name, directly return it"
v0.15.0,"if multiple embedding names, concatenate them"
v0.15.0,First we remove any existing labels for this PartOfSentence in self.sentence
v0.15.0,labels also need to be deleted at Sentence object
v0.15.0,delete labels at object itself
v0.15.0,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.15.0,"therefore, labels get added only to the Sentence if it exists"
v0.15.0,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.15.0,"Therefore, labels get set only to the Sentence if it exists"
v0.15.0,"check if the span already exists. If so, return it"
v0.15.0,else make a new span
v0.15.0,"check if the relation already exists. If so, return it"
v0.15.0,else make a new relation
v0.15.0,private field for all known spans
v0.15.0,the tokenizer used for this sentence
v0.15.0,some sentences represent a document boundary (but most do not)
v0.15.0,internal variables to denote position inside dataset
v0.15.0,"if text is passed, instantiate sentence with tokens (words)"
v0.15.0,determine token positions and whitespace_after flag
v0.15.0,the last token has no whitespace after
v0.15.0,log a warning if the dataset is empty
v0.15.0,data with zero-width characters cannot be handled
v0.15.0,set token idx and sentence
v0.15.0,append token to sentence
v0.15.0,register token annotations on sentence
v0.15.0,move sentence embeddings to device
v0.15.0,also move token embeddings to device
v0.15.0,clear token embeddings
v0.15.0,infer whitespace after field
v0.15.0,"if sentence has no tokens, return empty string"
v0.15.0,"otherwise, return concatenation of tokens with the correct offsets"
v0.15.0,The sentence's start position is not propagated to its tokens.
v0.15.0,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.15.0,No character at the corresponding code point: remove it
v0.15.0,"if no label if specified, return all labels"
v0.15.0,"if the label type exists in the Sentence, return it"
v0.15.0,return empty list if none of the above
v0.15.0,labels also need to be deleted at all tokens
v0.15.0,labels also need to be deleted at all known spans
v0.15.0,remove spans without labels
v0.15.0,delete labels at object itself
v0.15.0,set name
v0.15.0,abort if no data is provided
v0.15.0,sample test data from train if none is provided
v0.15.0,sample dev data from train if none is provided
v0.15.0,set train dev and test data
v0.15.0,find out empty sentence indices
v0.15.0,create subset of non-empty sentence indices
v0.15.0,find out empty sentence indices
v0.15.0,create subset of non-empty sentence indices
v0.15.0,"first, determine the datapoint type by going through dataset until first label is found"
v0.15.0,count all label types per sentence
v0.15.0,go through all labels of label_type and count values
v0.15.0,special handling for Token-level annotations. Add all untagged as 'O' label
v0.15.0,"if an unk threshold is set, UNK all label values below this threshold"
v0.15.0,sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
v0.15.0,replace the old label with the new one
v0.15.0,keep track of the old (clean) label using another label type category
v0.15.0,keep track of how many labels in total are flipped
v0.15.0,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.15.0,replace the old label with the new one
v0.15.0,keep track of the old (clean) label using another label type category
v0.15.0,keep track of how many labels in total are flipped
v0.15.0,"add a dummy ""O"" to close final prediction"
v0.15.0,return complex list
v0.15.0,internal variables
v0.15.0,non-set tags are OUT tags
v0.15.0,anything that is not OUT is IN
v0.15.0,does this prediction start a new span?
v0.15.0,B- and S- always start new spans
v0.15.0,"if the predicted class changes, I- starts a new span"
v0.15.0,"if the predicted class changes and S- was previous tag, start a new span"
v0.15.0,if an existing span is ended (either by reaching O or starting a new span)
v0.15.0,determine score and value
v0.15.0,append to result list
v0.15.0,reset for-loop variables for new span
v0.15.0,remember previous tag
v0.15.0,global variable: cache_root
v0.15.0,Get the device from the environment variable
v0.15.0,global variable: device
v0.15.0,"No need for correctness checks, torch is doing it"
v0.15.0,global variable: version
v0.15.0,global variable: arrow symbol
v0.15.0,dummy return to fulfill trainer.train() needs
v0.15.0,print(vec)
v0.15.0,Attach optimizer
v0.15.0,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.15.0,if memory mode option 'none' delete everything
v0.15.0,"if dynamic embedding keys not passed, identify them automatically"
v0.15.0,always delete dynamic embeddings
v0.15.0,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.15.0,optional metric space decoder if prototypes have different length than embedding
v0.15.0,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.15.0,"if set, create initial prototypes from normal distribution"
v0.15.0,"if set, use a radius"
v0.15.0,all parameters will be pushed internally to the specified device
v0.15.0,decode embeddings into prototype space
v0.15.0,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.15.0,verbalize BIOES labels
v0.15.0,"if label is not BIOES, use label itself"
v0.15.0,Always include the name of the Model class for which the state dict holds
v0.15.0,"this seems to just return model name, not a model with that name"
v0.15.0,"write out a ""model card"" if one is set"
v0.15.0,save model
v0.15.0,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.15.0,get all non-abstract subclasses
v0.15.0,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.15.0,"skip any invalid loadings, e.g. not found on HuggingFace hub"
v0.15.0,"if the model cannot be fetched, load as a file"
v0.15.0,try to get model class from state
v0.15.0,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.15.0,"skip any invalid loadings, e.g. not found on HuggingFace hub"
v0.15.0,"if this class is not abstract, fetch the model and load it"
v0.15.0,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.15.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.15.0,loss calculation
v0.15.0,variables for printing
v0.15.0,variables for computing scores
v0.15.0,remove any previously predicted labels
v0.15.0,predict for batch
v0.15.0,get the gold labels
v0.15.0,add to all_predicted_values
v0.15.0,make printout lines
v0.15.0,convert true and predicted values to two span-aligned lists
v0.15.0,delete excluded labels if exclude_labels is given
v0.15.0,"if after excluding labels, no label is left, ignore the datapoint"
v0.15.0,write all_predicted_values to out_file if set
v0.15.0,make the evaluation dictionary
v0.15.0,check if this is a multi-label problem
v0.15.0,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.15.0,multi-label problems require a multi-hot vector for each true and predicted label
v0.15.0,single-label problems can do with a single index for each true and predicted label
v0.15.0,"now, calculate evaluation numbers"
v0.15.0,there is at least one gold label or one prediction (default)
v0.15.0,compute accuracy separately as it is not always in classification_report (e.g. when micro avg exists)
v0.15.0,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.15.0,"The ""micro avg"" appears only in the classification report if no prediction is possible."
v0.15.0,"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
v0.15.0,"Create and populate score object for logging with all evaluation values, plus the loss"
v0.15.0,issue error and default all evaluation numbers to 0.
v0.15.0,check if there is a label mismatch
v0.15.0,print info
v0.15.0,set the embeddings
v0.15.0,initialize the label dictionary
v0.15.0,initialize the decoder
v0.15.0,set up multi-label logic
v0.15.0,init dropouts
v0.15.0,loss weights and loss function
v0.15.0,Initialize the weight tensor
v0.15.0,set up gradient reversal if so specified
v0.15.0,embed sentences
v0.15.0,get a tensor of data points
v0.15.0,do dropout
v0.15.0,make a forward pass to produce embedded data points and labels
v0.15.0,get the data points for which to predict labels
v0.15.0,get their gold labels as a tensor
v0.15.0,pass data points through network to get encoded data point tensor
v0.15.0,decode
v0.15.0,an optional masking step (no masking in most cases)
v0.15.0,calculate the loss
v0.15.0,filter empty sentences
v0.15.0,reverse sort all sequences by their length
v0.15.0,progress bar for verbosity
v0.15.0,filter data points in batch
v0.15.0,stop if all sentences are empty
v0.15.0,pass data points through network and decode
v0.15.0,if anything could possibly be predicted
v0.15.0,remove previously predicted labels of this type
v0.15.0,filter data points that have labels outside of dictionary
v0.15.0,add DefaultClassifier arguments
v0.15.0,add variables of DefaultClassifier
v0.15.0,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.15.0,Get projected 1st dimension
v0.15.0,Compute bilinear form
v0.15.0,Arcosh
v0.15.0,Project the input data to n+1 dimensions
v0.15.0,"The first dimension, is recomputed in the distance module"
v0.15.0,header for 'weights.txt'
v0.15.0,"determine the column index of loss, f-score and accuracy for"
v0.15.0,"train, dev and test split"
v0.15.0,then get all relevant values from the tsv
v0.15.0,then get all relevant values from the tsv
v0.15.0,plot i
v0.15.0,save plots
v0.15.0,save plots
v0.15.0,plt.show()
v0.15.0,save plot
v0.15.0,auto-spawn on GPU if available
v0.15.0,progress bar for verbosity
v0.15.0,stop if all sentences are empty
v0.15.0,clearing token embeddings to save memory
v0.15.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.15.0,TODO: not saving lines yet
v0.15.0,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.15.0,- MaskedRelationClassifier ?
v0.15.0,This depends if this relation classification architecture should replace or offer as an alternative.
v0.15.0,Set label type and prepare label dictionary
v0.15.0,Initialize super default classifier
v0.15.0,Add the special tokens from the encoding strategy
v0.15.0,"Auto-spawn on GPU, if available"
v0.15.0,Only use entities labelled with the specified labels for each label type
v0.15.0,Only use entities above the specified threshold
v0.15.0,Use a dictionary to find gold relation annotations for a given entity pair
v0.15.0,Yield head and tail entity pairs from the cross product of all entities
v0.15.0,Remove identity relation entity pairs
v0.15.0,Remove entity pairs with labels that do not match any
v0.15.0,of the specified relations in `self.entity_pair_labels`
v0.15.0,"Obtain gold label, if existing"
v0.15.0,Some sanity checks
v0.15.0,Pre-compute non-leading head and tail tokens for entity masking
v0.15.0,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.15.0,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.15.0,"Therefore, we use the span's position in the sentence."
v0.15.0,Create masked sentence
v0.15.0,Add gold relation annotation as sentence label
v0.15.0,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.15.0,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.15.0,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.15.0,"may get distributed into different splits. For training purposes, this is always undesired."
v0.15.0,Ensure that all sentences are encoded properly
v0.15.0,Deal with the case where all sentences are encoded sentences
v0.15.0,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.15.0,Deal with the case where all sentences are standard (non-encoded) sentences
v0.15.0,"For each encoded sentence, transfer its prediction onto the original relation"
v0.15.0,auto-spawn on GPU if available
v0.15.0,pad strings with whitespaces to longest sentence
v0.15.0,cut up the input into chunks of max charlength = chunk_size
v0.15.0,push each chunk through the RNN language model
v0.15.0,concatenate all chunks to make final output
v0.15.0,initial hidden state
v0.15.0,get predicted weights
v0.15.0,divide by temperature
v0.15.0,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.15.0,this makes a vector in which the largest value is 0
v0.15.0,compute word weights with exponential function
v0.15.0,try sampling multinomial distribution for next character
v0.15.0,print(word_idx)
v0.15.0,input ids
v0.15.0,push list of character IDs through model
v0.15.0,the target is always the next character
v0.15.0,use cross entropy loss to compare output of forward pass with targets
v0.15.0,exponentiate cross-entropy loss to calculate perplexity
v0.15.0,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.15.0,serialize the language models and the constructor arguments (but nothing else)
v0.15.0,special handling for deserializing language models
v0.15.0,re-initialize language model with constructor arguments
v0.15.0,copy over state dictionary to self
v0.15.0,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.15.0,"in their ""self.train()"" method)"
v0.15.0,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.15.0,"check if this is the case and if so, set it"
v0.15.0,Transform input data into TARS format
v0.15.0,"if there are no labels, return a random sample as negatives"
v0.15.0,"otherwise, go through all labels"
v0.15.0,make sure the probabilities always sum up to 1
v0.15.0,get and embed all labels by making a Sentence object that contains only the label text
v0.15.0,get each label embedding and scale between 0 and 1
v0.15.0,compute similarity matrix
v0.15.0,"the higher the similarity, the greater the chance that a label is"
v0.15.0,sampled as negative example
v0.15.0,make label dictionary if no Dictionary object is passed
v0.15.0,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.15.0,check if candidate_label_set is empty
v0.15.0,make list if only one candidate label is passed
v0.15.0,create label dictionary
v0.15.0,note current task
v0.15.0,create a temporary task
v0.15.0,make zero shot predictions
v0.15.0,switch to the pre-existing task
v0.15.0,prepare TARS dictionary
v0.15.0,initialize a bare-bones sequence tagger
v0.15.0,transformer separator
v0.15.0,Store task specific labels since TARS can handle multiple tasks
v0.15.0,make a tars sentence where all labels are O by default
v0.15.0,init new TARS classifier
v0.15.0,set all task information
v0.15.0,progress bar for verbosity
v0.15.0,stop if all sentences are empty
v0.15.0,always remove tags first
v0.15.0,go through each sentence in the batch
v0.15.0,always remove tags first
v0.15.0,get the span and its label
v0.15.0,determine whether tokens in this span already have a label
v0.15.0,only add if all tokens have no label
v0.15.0,make and add a corresponding predicted span
v0.15.0,set indices so that no token can be tagged twice
v0.15.0,clearing token embeddings to save memory
v0.15.0,"all labels default to ""O"""
v0.15.0,set gold token-level
v0.15.0,set predicted token-level
v0.15.0,now print labels in CoNLL format
v0.15.0,prepare TARS dictionary
v0.15.0,initialize a bare-bones sequence tagger
v0.15.0,transformer separator
v0.15.0,Store task specific labels since TARS can handle multiple tasks
v0.15.0,get the serialized embeddings
v0.15.0,remap state dict for models serialized with Flair <= 0.11.3
v0.15.0,init new TARS classifier
v0.15.0,set all task information
v0.15.0,with torch.no_grad():
v0.15.0,progress bar for verbosity
v0.15.0,stop if all sentences are empty
v0.15.0,always remove tags first
v0.15.0,go through each sentence in the batch
v0.15.0,always remove tags first
v0.15.0,add all labels that according to TARS match the text and are above threshold
v0.15.0,do not add labels below confidence threshold
v0.15.0,only use label with the highest confidence if enforcing single-label predictions
v0.15.0,add the label with the highest score even if below the threshold if force label is activated.
v0.15.0,remove previously added labels and only add the best label
v0.15.0,clearing token embeddings to save memory
v0.15.0,set separator to concatenate three sentences
v0.15.0,auto-spawn on GPU if available
v0.15.0,set separator to concatenate two sentences
v0.15.0,auto-spawn on GPU if available
v0.15.0,"If the concatenated version of the text pair does not exist yet, create it"
v0.15.0,pooling operation to get embeddings for entites
v0.15.0,set embeddings
v0.15.0,set relation and entity label types
v0.15.0,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.15.0,filter entity pairs according to their tags if set
v0.15.0,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.15.0,character dictionary for decoding and encoding
v0.15.0,make sure <unk> is in dictionary for handling of unknown characters
v0.15.0,add special symbols to dictionary if necessary and save respective indices
v0.15.0,---- ENCODER ----
v0.15.0,encoder character embeddings
v0.15.0,encoder pre-trained embeddings
v0.15.0,encoder RNN
v0.15.0,additional encoder linear layer if bidirectional encoding
v0.15.0,---- DECODER ----
v0.15.0,decoder: linear layers to transform vectors to and from alphabet_size
v0.15.0,when using attention we concatenate attention outcome and decoder hidden states
v0.15.0,decoder RNN
v0.15.0,loss and softmax
v0.15.0,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.15.0,add additional columns for special symbols if necessary
v0.15.0,initialize with dummy symbols
v0.15.0,encode inputs
v0.15.0,get labels (we assume each token has a lemma label)
v0.15.0,get char indices for labels of sentence
v0.15.0,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.15.0,max_sequence_length = length of longest label of sentence + 1
v0.15.0,get char embeddings
v0.15.0,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.15.0,take decoder input and initial hidden and pass through RNN
v0.15.0,"if all encoder outputs are provided, use attention"
v0.15.0,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.15.0,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.15.0,get all tokens
v0.15.0,encode input characters by sending them through RNN
v0.15.0,get one-hots for characters and add special symbols / padding
v0.15.0,determine length of each token
v0.15.0,embed sentences
v0.15.0,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.15.0,variable to store initial hidden states for decoder
v0.15.0,encode input characters by sending them through RNN
v0.15.0,test packing and padding
v0.15.0,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.15.0,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.15.0,decoder later with self.emb_to_hidden
v0.15.0,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.15.0,use token embedding as initial hidden state for decoder
v0.15.0,concatenate everything together and project to appropriate size for decoder
v0.15.0,variable to store initial hidden states for decoder
v0.15.0,encode input characters by sending them through RNN
v0.15.0,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.15.0,embed character one-hots
v0.15.0,send through encoder RNN (produces initial hidden for decoder)
v0.15.0,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.15.0,project 2*hidden_size to hidden_size
v0.15.0,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.15.0,later with self.emb_to_hidden
v0.15.0,use token embedding as initial hidden state for decoder
v0.15.0,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.15.0,concatenate everything together and project to appropriate size for decoder
v0.15.0,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.15.0,"create target vector (batch_size, max_label_seq_length + 1)"
v0.15.0,filter empty sentences
v0.15.0,max length of the predicted sequences
v0.15.0,for printing
v0.15.0,stop if all sentences are empty
v0.15.0,remove previously predicted labels of this type
v0.15.0,create list of tokens in batch
v0.15.0,encode inputs
v0.15.0,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.15.0,sequence length is always set to one in prediction
v0.15.0,option 1: greedy decoding
v0.15.0,predictions
v0.15.0,decode next character
v0.15.0,pick top beam size many outputs with highest probabilities
v0.15.0,option 2: beam search
v0.15.0,out_probs = self.softmax(output_vectors).squeeze(1)
v0.15.0,make sure no dummy symbol <> or start symbol <S> is predicted
v0.15.0,pick top beam size many outputs with highest probabilities
v0.15.0,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.15.0,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.15.0,keep scores of beam_size many hypothesis for each token in the batch
v0.15.0,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.15.0,save sequences so far
v0.15.0,keep track of how many hypothesis were completed for each token
v0.15.0,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.15.0,decode with log softmax
v0.15.0,make sure no dummy symbol <> or start symbol <S> is predicted
v0.15.0,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.15.0,"if the sequence is already ended, do not record as candidate"
v0.15.0,index of token in in list tokens_in_batch
v0.15.0,print(token_number)
v0.15.0,hypothesis score
v0.15.0,TODO: remove token if number of completed hypothesis exceeds given value
v0.15.0,set score of corresponding entry to -inf so it will not be expanded
v0.15.0,get leading_indices for next expansion
v0.15.0,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.15.0,take beam_size many copies of scores vector and add scores of possible new extensions
v0.15.0,"size (beam_size*batch_size, beam_size)"
v0.15.0,print(hypothesis_scores)
v0.15.0,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.15.0,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.15.0,print(hypothesis_scores_per_token)
v0.15.0,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.15.0,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.15.0,a list of length beam_size*batch_size
v0.15.0,"where the first three inidices belong to the first token, the next three to the second token,"
v0.15.0,and so on
v0.15.0,with these indices we can compute the tensors for the next iteration
v0.15.0,expand sequences with corresponding index
v0.15.0,add log-probabilities to the scores
v0.15.0,save new leading indices
v0.15.0,save corresponding hidden states
v0.15.0,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.15.0,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.15.0,get best final hypothesis for each token
v0.15.0,get characters from index sequences and add predicted label to token
v0.15.0,"Overwrites evaluate of parent class to remove the ""by class"" printout"
v0.15.0,set separator to concatenate two sentences
v0.15.0,init dropouts
v0.15.0,auto-spawn on GPU if available
v0.15.0,make a forward pass to produce embedded data points and labels
v0.15.0,get their gold labels as a tensor
v0.15.0,pass data points through network to get encoded data point tensor
v0.15.0,decode
v0.15.0,calculate the loss
v0.15.0,get a tensor of data points
v0.15.0,do dropout
v0.15.0,"If the concatenated version of the text pair does not exist yet, create it"
v0.15.0,add Model arguments
v0.15.0,progress bar for verbosity
v0.15.0,stop if all sentences are empty
v0.15.0,clearing token embeddings to save memory
v0.15.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.15.0,"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
v0.15.0,fields in case this is a span-prediction problem
v0.15.0,the label type
v0.15.0,all parameters will be pushed internally to the specified device
v0.15.0,special handling during training if this is a span prediction problem
v0.15.0,internal variables
v0.15.0,non-set tags are OUT tags
v0.15.0,anything that is not OUT is IN
v0.15.0,does this prediction start a new span?
v0.15.0,B- and S- always start new spans
v0.15.0,"if the predicted class changes, I- starts a new span"
v0.15.0,"if the predicted class changes and S- was previous tag, start a new span"
v0.15.0,if an existing span is ended (either by reaching O or starting a new span)
v0.15.0,reset for-loop variables for new span
v0.15.0,remember previous tag
v0.15.0,"if there is a span at end of sentence, add it"
v0.15.0,"all labels default to ""O"""
v0.15.0,set gold token-level
v0.15.0,set predicted token-level
v0.15.0,now print labels in CoNLL format
v0.15.0,print labels in CoNLL format
v0.15.0,internal candidate lists of generator
v0.15.0,load Zelda candidates if so passed
v0.15.0,create candidate lists
v0.15.0,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.15.0,create a new dictionary for lower cased mentions
v0.15.0,go through each mention and its candidates
v0.15.0,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.15.0,set lowercased version as map
v0.15.0,"only use span label type if there are predictions, otherwise search for output label type (training labels)"
v0.15.0,remap state dict for models serialized with Flair <= 0.11.3
v0.15.0,get the candidates
v0.15.0,"during training, add the gold value as candidate"
v0.15.0,----- Create the internal tag dictionary -----
v0.15.0,span-labels need special encoding (BIO or BIOES)
v0.15.0,the big question is whether the label dictionary should contain an UNK or not
v0.15.0,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.15.0,"with UNK, the model learns less well if there are no UNK examples"
v0.15.0,is this a span prediction problem?
v0.15.0,----- Embeddings -----
v0.15.0,----- Initial loss weights parameters -----
v0.15.0,----- RNN specific parameters -----
v0.15.0,----- Conditional Random Field parameters -----
v0.15.0,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.15.0,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.15.0,----- Dropout parameters -----
v0.15.0,dropouts
v0.15.0,remove word dropout if there is no contact over the sequence dimension.
v0.15.0,----- Model layers -----
v0.15.0,----- RNN layer -----
v0.15.0,"If shared RNN provided, else create one for model"
v0.15.0,Whether to train initial hidden state
v0.15.0,final linear map to tag space
v0.15.0,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.15.0,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.15.0,"if there are no sentences, there is no loss"
v0.15.0,forward pass to get scores
v0.15.0,calculate loss given scores and labels
v0.15.0,make a zero-padded tensor for the whole sentence
v0.15.0,linear map to tag space
v0.15.0,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.15.0,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.15.0,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.15.0,spans need to be encoded as token-level predictions
v0.15.0,all others are regular labels for each token
v0.15.0,make sure it's a list
v0.15.0,filter empty sentences
v0.15.0,reverse sort all sequences by their length
v0.15.0,progress bar for verbosity
v0.15.0,stop if all sentences are empty
v0.15.0,get features from forward propagation
v0.15.0,remove previously predicted labels of this type
v0.15.0,"if return_loss, get loss value"
v0.15.0,make predictions
v0.15.0,add predictions to Sentence
v0.15.0,BIOES-labels need to be converted to spans
v0.15.0,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.15.0,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.15.0,core Flair models on Huggingface ModelHub
v0.15.0,"Large NER models,"
v0.15.0,Multilingual NER models
v0.15.0,English POS models
v0.15.0,Multilingual POS models
v0.15.0,English SRL models
v0.15.0,English chunking models
v0.15.0,Language-specific NER models
v0.15.0,Language-specific POS models
v0.15.0,Historic German
v0.15.0,English NER models
v0.15.0,English SRL models
v0.15.0,Danish models
v0.15.0,German models
v0.15.0,Arabic models
v0.15.0,French models
v0.15.0,Dutch models
v0.15.0,Malayalam models
v0.15.0,Portuguese models
v0.15.0,Biomedical models
v0.15.0,check if model name is a valid local file
v0.15.0,"check if model key is remapped to HF key - if so, print out information"
v0.15.0,get mapped name
v0.15.0,"if not, check if model key is remapped to direct download location. If so, download model"
v0.15.0,"for all other cases (not local file or special download location), use HF model hub"
v0.15.0,## Demo: How to use in Flair
v0.15.0,load tagger
v0.15.0,make example sentence
v0.15.0,predict NER tags
v0.15.0,print sentence
v0.15.0,print predicted NER spans
v0.15.0,iterate over entities and print
v0.15.0,Lazy import
v0.15.0,Save model weight
v0.15.0,Determine if model card already exists
v0.15.0,Generate and save model card
v0.15.0,Upload files
v0.15.0,"all labels default to ""O"""
v0.15.0,set gold token-level
v0.15.0,set predicted token-level
v0.15.0,now print labels in CoNLL format
v0.15.0,print labels in CoNLL format
v0.15.0,Dense + sparse retrieval
v0.15.0,fetched from original repo to avoid download
v0.15.0,"just in case we add: fuzzy search, Levenstein, ..."
v0.15.0,"for now we always fall back to SapBERT,"
v0.15.0,but we should train our own models at some point
v0.15.0,NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
v0.15.0,NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`)
v0.15.0,Ab3P works on sentence-level and not on a single entity mention / name
v0.15.0,- so we just apply the wrapped text pre-processing here (if configured)
v0.15.0,NOTE: ensure correct similarity metric for pretrained model
v0.15.0,empty cuda cache if device is a cuda device
v0.15.0,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.15.0,NOTE: This is a hacky workaround for the fact that
v0.15.0,the `label_type`s in `Classifier.load('hunflair)` are
v0.15.0,"'diseases', 'genes', 'species', 'chemical' instead of 'ner'."
v0.15.0,We warn users once they need to update SequenceTagger model
v0.15.0,See: https://github.com/flairNLP/flair/pull/3387
v0.15.0,make sure sentences is a list of sentences
v0.15.0,Make sure entity label types are represented as dict
v0.15.0,Collect all entities based on entity type labels configuration
v0.15.0,Preprocess entity mentions
v0.15.0,Retrieve top-k concept / entity candidates
v0.15.0,Add a label annotation for each candidate
v0.15.0,load model by entity_type
v0.15.0,check if we have a hybrid pre-trained model
v0.15.0,the multi task model has several labels
v0.15.0,biomedical models
v0.15.0,entity linker
v0.15.0,auto-spawn on GPU if available
v0.15.0,remap state dict for models serialized with Flair <= 0.11.3
v0.15.0,English sentiment models
v0.15.0,Communicative Functions Model
v0.15.0,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.15.0,"may get distributed into different splits. For training purposes, this is always undesired."
v0.15.0,Prepend the task description prompt to the sentence text
v0.15.0,Make sure it's a list
v0.15.0,Reconstruct all annotations from the original sentence (necessary for learning classifiers)
v0.15.0,If all sentences are not augmented -> augment them
v0.15.0,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.15.0,"mypy does not infer the type of ""sentences"" restricted by code above"
v0.15.0,Compute prediction label type
v0.15.0,make sure it's a list
v0.15.0,"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences"
v0.15.0,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.15.0,Remove existing labels
v0.15.0,Augment sentences - copy all annotation of the given tag type
v0.15.0,Predict on augmented sentence and store it in an internal annotation layer / label
v0.15.0,Append predicted labels to the original sentences
v0.15.0,check if model name is a valid local file
v0.15.0,check if model name is a pre-configured hf model
v0.15.0,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.15.0,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.15.0,"Initially, get scores from <start> tag to all other tags"
v0.15.0,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.15.0,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.15.0,Create a tensor to hold accumulated sequence scores at each current tag
v0.15.0,Create a tensor to hold back-pointers
v0.15.0,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.15.0,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.15.0,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.15.0,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.15.0,"If sentence is over, add transition to STOP-tag"
v0.15.0,Decode/trace best path backwards
v0.15.0,Sanity check
v0.15.0,remove start-tag and backscore to stop-tag
v0.15.0,Max + Softmax to get confidence score for predicted label and append label to each token
v0.15.0,"Transitions are used in the following way: transitions[to, from]."
v0.15.0,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.15.0,to START-tag and from STOP-tag to any other tag to -10000.
v0.15.0,"if necessary, make batch_steps"
v0.15.0,break up the batch into slices of size
v0.15.0,mini_batch_chunk_size
v0.15.0,"if training also uses dev/train data, include in training set"
v0.15.0,evaluation and monitoring
v0.15.0,sampling and shuffling
v0.15.0,evaluation and monitoring
v0.15.0,when and what to save
v0.15.0,logging parameters
v0.15.0,acceleration
v0.15.0,plugins
v0.15.0,activate annealing plugin
v0.15.0,call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
v0.15.0,training parameters
v0.15.0,evaluation and monitoring
v0.15.0,sampling and shuffling
v0.15.0,evaluation and monitoring
v0.15.0,when and what to save
v0.15.0,logging parameters
v0.15.0,acceleration
v0.15.0,plugins
v0.15.0,annealing logic
v0.15.0,training parameters
v0.15.0,evaluation and monitoring
v0.15.0,sampling and shuffling
v0.15.0,evaluation and monitoring
v0.15.0,when and what to save
v0.15.0,logging parameters
v0.15.0,acceleration
v0.15.0,plugins
v0.15.0,training parameters
v0.15.0,evaluation and monitoring
v0.15.0,sampling and shuffling
v0.15.0,evaluation and monitoring
v0.15.0,when and what to save
v0.15.0,logging parameters
v0.15.0,acceleration
v0.15.0,plugins
v0.15.0,Create output folder
v0.15.0,=== START BLOCK: ACTIVATE PLUGINS === #
v0.15.0,We first activate all optional plugins. These take care of optional functionality such as various
v0.15.0,logging techniques and checkpointing
v0.15.0,log file plugin
v0.15.0,loss file plugin
v0.15.0,plugin for writing weights
v0.15.0,plugin for checkpointing
v0.15.0,=== END BLOCK: ACTIVATE PLUGINS === #
v0.15.0,derive parameters the function was called with (or defaults)
v0.15.0,initialize model card with these parameters
v0.15.0,Prepare training data and get dataset size
v0.15.0,"determine what splits (train, dev, test) to evaluate"
v0.15.0,determine how to determine best model and whether to save it
v0.15.0,instantiate the optimizer
v0.15.0,initialize sampler if provided
v0.15.0,init with default values if only class is provided
v0.15.0,set dataset to sample from
v0.15.0,configure special behavior to use multiple GPUs
v0.15.0,Guard against each process initializing corpus differently due to e.g. different random seeds
v0.15.0,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.15.0,Sanity checks
v0.15.0,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.15.0,-- AmpPlugin -> wraps with AMP
v0.15.0,-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
v0.15.0,At any point you can hit Ctrl + C to break out of training early.
v0.15.0,"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
v0.15.0,- LossFilePlugin -> get the current epoch for loss file logging
v0.15.0,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.15.0,log infos on training progress every `log_modulo` batches
v0.15.0,process mini-batches
v0.15.0,zero the gradients on the model and optimizer
v0.15.0,forward and backward for batch
v0.15.0,forward pass
v0.15.0,We need to __call__ ddp_model() because this triggers hooks that sync gradients.
v0.15.0,But that calls forward rather than forward_loss. So we patch forward to redirect
v0.15.0,to forward_loss. Then undo the patch in case forward_loss itself calls forward.
v0.15.0,identify dynamic embeddings (always deleted) on first sentence
v0.15.0,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.15.0,do the optimizer step
v0.15.0,DDP averages across processes but we want the sum
v0.15.0,- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
v0.15.0,- WeightExtractorPlugin -> extracts weights
v0.15.0,- CheckpointPlugin -> executes save_model_each_k_epochs
v0.15.0,- SchedulerPlugin -> log bad epochs
v0.15.0,Determine if this is the best model or if we need to anneal
v0.15.0,log results
v0.15.0,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.15.0,use DEV split to determine if this is the best model so far
v0.15.0,"if not using DEV score, determine best model using train loss"
v0.15.0,- LossFilePlugin -> somehow prints all relevant metrics
v0.15.0,- AnnealPlugin -> scheduler step
v0.15.0,- SWAPlugin -> restores SGD weights from SWA
v0.15.0,"if we do not use dev data for model selection, save final model"
v0.15.0,TensorboardLogger -> closes writer
v0.15.0,test best model if test data is present
v0.15.0,get and return the final test score of best model
v0.15.0,MetricHistoryPlugin -> stores the loss history in return_values
v0.15.0,"Store return values, as they will be erased by reset_training_attributes"
v0.15.0,get a random sample of training sentences
v0.15.0,create a model card for this model with Flair and PyTorch version
v0.15.0,record Transformers version if library is loaded
v0.15.0,remember all parameters used in train() call
v0.15.0,"TextDataset returns a list. valid and test are only one file,"
v0.15.0,so return the first element
v0.15.0,cast string to Path
v0.15.0,error message if the validation dataset is too small
v0.15.0,Shuffle training files randomly after serially iterating
v0.15.0,through corpus one
v0.15.0,"iterate through training data, starting at"
v0.15.0,self.split (for checkpointing)
v0.15.0,off by one for printing
v0.15.0,go into train mode
v0.15.0,reset variables
v0.15.0,not really sure what this does
v0.15.0,do the forward pass in the model
v0.15.0,try to predict the targets
v0.15.0,Backward
v0.15.0,`clip_grad_norm` helps prevent the exploding gradient
v0.15.0,problem in RNNs / LSTMs.
v0.15.0,We detach the hidden state from how it was
v0.15.0,previously produced.
v0.15.0,"If we didn't, the model would try backpropagating"
v0.15.0,all the way to start of the dataset.
v0.15.0,explicitly remove loss to clear up memory
v0.15.0,#########################################################
v0.15.0,Save the model if the validation loss is the best we've
v0.15.0,seen so far.
v0.15.0,#########################################################
v0.15.0,print info
v0.15.0,#########################################################
v0.15.0,##############################################################################
v0.15.0,final testing
v0.15.0,##############################################################################
v0.15.0,Turn on evaluation mode which disables dropout.
v0.15.0,Work out how cleanly we can divide the dataset into bsz parts.
v0.15.0,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.15.0,Evenly divide the data across the bsz batches.
v0.15.0,"no need to check for MetricName, as __add__ of other would be called in this case"
v0.15.0,"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
v0.15.0,instantiate plugin
v0.15.0,"Reset the flag, since an exception event might be dispatched"
v0.15.0,"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
v0.15.0,the callback
v0.15.0,go through all attributes
v0.15.0,get attribute hook events (may raise an AttributeError)
v0.15.0,register function as a hook
v0.15.0,"Decorator was used with parentheses, but no args"
v0.15.0,Decorator was used with args (strings specifiying the events)
v0.15.0,Decorator was used without args
v0.15.0,path to store the model
v0.15.0,special annealing modes
v0.15.0,determine the min learning rate
v0.15.0,"minimize training loss if training with dev data, else maximize dev score"
v0.15.0,instantiate the scheduler
v0.15.0,stop training if learning rate becomes too small
v0.15.0,reload last best model if annealing with restarts is enabled
v0.15.0,calculate warmup steps
v0.15.0,skip if no optimization has happened.
v0.15.0,saves the model with full vocab as checkpoints etc were created with reduced vocab.
v0.15.0,TODO: check if metric is in tracked metrics
v0.15.0,prepare loss logging file and set up header
v0.15.0,set up all metrics to collect
v0.15.0,set up headers
v0.15.0,name: HEADER
v0.15.0,Add all potentially relevant metrics. If a metric is not published
v0.15.0,"after the first epoch (when the header is written), the column is"
v0.15.0,removed at that point.
v0.15.0,initialize the first log line
v0.15.0,record is a list of scalars
v0.15.0,output log file
v0.15.0,remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
v0.15.0,make headers on epoch 1
v0.15.0,write header
v0.15.0,adjust alert level
v0.15.0,"the default model for ELMo is the 'original' model, which is very large"
v0.15.0,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.15.0,put on Cuda if available
v0.15.0,embed a dummy sentence to determine embedding_length
v0.15.0,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.15.0,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.15.0,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.15.0,news-english-forward
v0.15.0,news-english-backward
v0.15.0,news-english-forward
v0.15.0,news-english-backward
v0.15.0,mix-english-forward
v0.15.0,mix-english-backward
v0.15.0,mix-german-forward
v0.15.0,mix-german-backward
v0.15.0,common crawl Polish forward
v0.15.0,common crawl Polish backward
v0.15.0,Slovenian forward
v0.15.0,Slovenian backward
v0.15.0,Bulgarian forward
v0.15.0,Bulgarian backward
v0.15.0,Dutch forward
v0.15.0,Dutch backward
v0.15.0,Swedish forward
v0.15.0,Swedish backward
v0.15.0,French forward
v0.15.0,French backward
v0.15.0,Czech forward
v0.15.0,Czech backward
v0.15.0,Portuguese forward
v0.15.0,Portuguese backward
v0.15.0,initialize cache if use_cache set
v0.15.0,embed a dummy sentence to determine embedding_length
v0.15.0,set to eval mode
v0.15.0,Copy the object's state from self.__dict__ which contains
v0.15.0,all our instance attributes. Always use the dict.copy()
v0.15.0,method to avoid modifying the original state.
v0.15.0,Remove the unpicklable entries.
v0.15.0,"if cache is used, try setting embeddings from cache first"
v0.15.0,try populating embeddings from cache
v0.15.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.15.0,get hidden states from language model
v0.15.0,take first or last hidden states from language model as word representation
v0.15.0,if self.tokenized_lm or token.whitespace_after:
v0.15.0,"if only one sentence is passed, convert to list of sentence"
v0.15.0,bidirectional LSTM on top of embedding layer
v0.15.0,dropouts
v0.15.0,"first, sort sentences by number of tokens"
v0.15.0,go through each sentence in batch
v0.15.0,PADDING: pad shorter sentences out
v0.15.0,ADD TO SENTENCE LIST: add the representation
v0.15.0,--------------------------------------------------------------------
v0.15.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.15.0,--------------------------------------------------------------------
v0.15.0,--------------------------------------------------------------------
v0.15.0,FF PART
v0.15.0,--------------------------------------------------------------------
v0.15.0,use word dropout if set
v0.15.0,--------------------------------------------------------------------
v0.15.0,EXTRACT EMBEDDINGS FROM LSTM
v0.15.0,--------------------------------------------------------------------
v0.15.0,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.15.0,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.15.0,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.15.0,add positional encodings
v0.15.0,reshape the pixels into the sequence
v0.15.0,layer norm after convolution and positional encodings
v0.15.0,add <cls> token
v0.15.0,"transformer requires input in the shape [h*w+1, b, d]"
v0.15.0,the output is an embedding of <cls> token
v0.15.0,this parameter is fixed
v0.15.0,optional fine-tuning on top of embedding layer
v0.15.0,"if only one sentence is passed, convert to list of sentence"
v0.15.0,"if only one sentence is passed, convert to list of sentence"
v0.15.0,bidirectional RNN on top of embedding layer
v0.15.0,dropouts
v0.15.0,TODO: remove in future versions
v0.15.0,embed words in the sentence
v0.15.0,before-RNN dropout
v0.15.0,reproject if set
v0.15.0,push through RNN
v0.15.0,after-RNN dropout
v0.15.0,extract embeddings from RNN
v0.15.0,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.15.0,"check if this is the case and if so, set it"
v0.15.0,serialize the language models and the constructor arguments (but nothing else)
v0.15.0,re-initialize language model with constructor arguments
v0.15.0,special handling for deserializing language models
v0.15.0,copy over state dictionary to self
v0.15.0,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.15.0,"in their ""self.train()"" method)"
v0.15.0,IMPORTANT: add embeddings as torch modules
v0.15.0,iterate over sentences
v0.15.0,"if its a forward LM, take last state"
v0.15.0,"convert to plain strings, embedded in a list for the encode function"
v0.15.0,CNN
v0.15.0,dropouts
v0.15.0,TODO: remove in future versions
v0.15.0,embed words in the sentence
v0.15.0,before-RNN dropout
v0.15.0,reproject if set
v0.15.0,push CNN
v0.15.0,after-CNN dropout
v0.15.0,extract embeddings from CNN
v0.15.0,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.15.0,"if only one sentence is passed, convert to list of sentence"
v0.15.0,Expose base classses
v0.15.0,Expose document embedding classes
v0.15.0,Expose image embedding classes
v0.15.0,Expose legacy embedding classes
v0.15.0,Expose token embedding classes
v0.15.0,in some cases we need to insert zero vectors for tokens without embedding.
v0.15.0,sum embeddings for each token
v0.15.0,calculate the mean of subtokens
v0.15.0,Create a mask for valid tokens based on token_lengths
v0.15.0,padding
v0.15.0,remove special markup
v0.15.0,check if special tokens exist to circumvent error message
v0.15.0,iterate over subtokens and reconstruct tokens
v0.15.0,remove special markup
v0.15.0,check if reconstructed token is special begin token ([CLS] or similar)
v0.15.0,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.15.0,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.15.0,if tokens are unaccounted for
v0.15.0,check if all tokens were matched to subtokens
v0.15.0,The layoutlm tokenizer doesn't handle ocr themselves
v0.15.0,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.15.0,"cannot run `.encode` if ocr boxes are required, assume"
v0.15.0,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.15.0,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.15.0,in case of doubt: token embedding has higher priority than document embedding
v0.15.0,random check some tokens to save performance.
v0.15.0,Models such as FNet do not have an attention_mask
v0.15.0,set language IDs for XLM-style transformers
v0.15.0,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.15.0,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.15.0,set context if not set already
v0.15.0,flair specific pre-tokenization
v0.15.0,fields to store left and right context
v0.15.0,expand context only if context_length is set
v0.15.0,"if context_dropout is set, randomly deactivate left context during training"
v0.15.0,"if context_dropout is set, randomly deactivate right context during training"
v0.15.0,"if use_context_separator is set, add a [FLERT] token"
v0.15.0,return expanded sentence and context length information
v0.15.0,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.15.0,temporary fix to disable tokenizer parallelism warning
v0.15.0,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.15.0,do not print transformer warnings as these are confusing in this case
v0.15.0,load tokenizer and transformer model
v0.15.0,load tokenizer from inmemory zip-file
v0.15.0,if model is quantized by BitsAndBytes this will fail
v0.15.0,add adapters for finetuning
v0.15.0,peft_config: PeftConfig
v0.15.0,model name
v0.15.0,embedding parameters
v0.15.0,send mini-token through to check how many layers the model has
v0.15.0,return length
v0.15.0,"If we use a context separator, add a new special token"
v0.15.0,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.15.0,"when initializing, embeddings are in eval mode by default"
v0.15.0,in case of doubt: token embedding has higher priority than document embedding
v0.15.0,in case of doubt: token embedding has higher priority than document embedding
v0.15.0,legacy TransformerDocumentEmbedding
v0.15.0,legacy TransformerTokenEmbedding
v0.15.0,legacy Flair <= 0.12
v0.15.0,legacy Flair <= 0.7
v0.15.0,legacy TransformerTokenEmbedding
v0.15.0,Legacy TransformerDocumentEmbedding
v0.15.0,legacy TransformerTokenEmbedding
v0.15.0,legacy TransformerDocumentEmbedding
v0.15.0,some models like the tars model somehow lost this information.
v0.15.0,copy values from new embedding
v0.15.0,do not switch the attention implementation upon reload.
v0.15.0,those parameters are only from the super class and will be recreated in the constructor.
v0.15.0,cls first pooling can be done without recreating sentence hidden states
v0.15.0,make the tuple a tensor; makes working with it easier.
v0.15.0,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.15.0,only use layers that will be outputted
v0.15.0,this parameter is fixed
v0.15.0,IMPORTANT: add embeddings as torch modules
v0.15.0,"if only one sentence is passed, convert to list of sentence"
v0.15.0,make compatible with serialized models
v0.15.0,gensim version 4
v0.15.0,gensim version 3
v0.15.0,"if no embedding is set, the vocab and embedding length is required"
v0.15.0,GLOVE embeddings
v0.15.0,TURIAN embeddings
v0.15.0,KOMNINOS embeddings
v0.15.0,pubmed embeddings
v0.15.0,FT-CRAWL embeddings
v0.15.0,FT-CRAWL embeddings
v0.15.0,twitter embeddings
v0.15.0,two-letter language code wiki embeddings
v0.15.0,two-letter language code wiki embeddings
v0.15.0,two-letter language code crawl embeddings
v0.15.0,"this is required to force the module on the cpu,"
v0.15.0,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.15.0,self.to(..) actually sets the device properly
v0.15.0,this ignores the get_cached_vec method when loading older versions
v0.15.0,it is needed for compatibility reasons
v0.15.0,gensim version 4
v0.15.0,gensim version 3
v0.15.0,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.15.0,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.15.0,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.15.0,not finding them)
v0.15.0,use list of common characters if none provided
v0.15.0,translate words in sentence into ints using dictionary
v0.15.0,"sort words by length, for batching and masking"
v0.15.0,chars for rnn processing
v0.15.0,multilingual models
v0.15.0,English models
v0.15.0,Arabic
v0.15.0,Bulgarian
v0.15.0,Czech
v0.15.0,Danish
v0.15.0,German
v0.15.0,Spanish
v0.15.0,Basque
v0.15.0,Persian
v0.15.0,Finnish
v0.15.0,French
v0.15.0,Hebrew
v0.15.0,Hindi
v0.15.0,Croatian
v0.15.0,Indonesian
v0.15.0,Italian
v0.15.0,Japanese
v0.15.0,Malayalam
v0.15.0,Dutch
v0.15.0,Norwegian
v0.15.0,Polish
v0.15.0,Portuguese
v0.15.0,Pubmed
v0.15.0,Slovenian
v0.15.0,Swedish
v0.15.0,Tamil
v0.15.0,Spanish clinical
v0.15.0,CLEF HIPE Shared task
v0.15.0,Amharic
v0.15.0,Ukrainian
v0.15.0,load model if in pretrained model map
v0.15.0,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.15.0,CLEF HIPE models are lowercased
v0.15.0,embeddings are static if we don't do finetuning
v0.15.0,embed a dummy sentence to determine embedding_length
v0.15.0,set to eval mode
v0.15.0,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.15.0,gradients are enable if fine-tuning is enabled
v0.15.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.15.0,get hidden states from language model
v0.15.0,take first or last hidden states from language model as word representation
v0.15.0,offset mode that extracts at whitespace after last character
v0.15.0,offset mode that extracts at last character
v0.15.0,make compatible with old models
v0.15.0,use the character language model embeddings as basis
v0.15.0,length is twice the original character LM embedding length
v0.15.0,these fields are for the embedding memory
v0.15.0,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.15.0,we re-compute embeddings dynamically at each epoch
v0.15.0,set the memory method
v0.15.0,memory is wiped each time we do a training run
v0.15.0,"if we keep a pooling, it needs to be updated continuously"
v0.15.0,update embedding
v0.15.0,check token.text is empty or not
v0.15.0,set aggregation operation
v0.15.0,add embeddings after updating
v0.15.0,model architecture
v0.15.0,model architecture
v0.15.0,"""pl"","
v0.15.0,download if necessary
v0.15.0,load the model
v0.15.0,"this is required to force the module on the cpu,"
v0.15.0,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.15.0,self.to(..) actually sets the device properly
v0.15.0,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.15.0,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.15.0,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.15.0,not finding them)
v0.15.0,old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict
v0.15.0,"however they are already initialized rightfully, so we just set the state dict from our current state dict"
v0.15.0,GLOVE embeddings
v0.15.0,no need to recreate as NILCEmbeddings
v0.15.0,read in test file if exists
v0.15.0,read in dev file if exists
v0.15.0,"find train, dev and test files if not specified"
v0.15.0,Add tags for each annotated span
v0.15.0,Remove leading and trailing whitespaces from annotated spans
v0.15.0,Search start and end token index for current span
v0.15.0,If end index is not found set to last token
v0.15.0,Throw error if indices are not valid
v0.15.0,Add metadatas for sentence
v0.15.0,Currently all Jsonl Datasets are stored in Memory
v0.15.0,get train data
v0.15.0,read in test file if exists
v0.15.0,read in dev file if exists
v0.15.0,"find train, dev and test files if not specified"
v0.15.0,special key for space after
v0.15.0,special key for feature columns
v0.15.0,special key for dependency head id
v0.15.0,"store either Sentence objects in memory, or only file offsets"
v0.15.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.15.0,determine encoding of text file
v0.15.0,identify which columns are spans and which are word-level
v0.15.0,now load all sentences
v0.15.0,skip first line if to selected
v0.15.0,option 1: keep Sentence objects in memory
v0.15.0,pointer to previous
v0.15.0,parse next sentence
v0.15.0,quit if last sentence reached
v0.15.0,skip banned sentences
v0.15.0,set previous and next sentence for context
v0.15.0,append parsed sentence to list in memory
v0.15.0,option 2: keep source data in memory
v0.15.0,"read lines for next sentence, but don't parse"
v0.15.0,quit if last sentence reached
v0.15.0,append raw lines for each sentence
v0.15.0,we make a distinction between word-level tags and span-level tags
v0.15.0,read first sentence to determine which columns are span-labels
v0.15.0,skip first line if to selected
v0.15.0,check the first 5 sentences
v0.15.0,go through all annotations and identify word- and span-level annotations
v0.15.0,- if a column has at least one BIES we know it's a Span label
v0.15.0,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.15.0,- problem cases are columns for which we see only O - in this case we default to Span
v0.15.0,skip assigned columns
v0.15.0,the space after key is always word-levels
v0.15.0,"if at least one token has a BIES, we know it's a span label"
v0.15.0,"if at least one token has a label other than BIOES, we know it's a token label"
v0.15.0,all remaining columns that are not word-level are span-level
v0.15.0,for column in self.word_level_tag_columns:
v0.15.0,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.15.0,"if sentence ends, break"
v0.15.0,parse comments if possible
v0.15.0,"otherwise, this line is a token. parse and add to sentence"
v0.15.0,check if this sentence is a document boundary
v0.15.0,add span labels
v0.15.0,discard tags from tokens that are not added to the sentence
v0.15.0,parse relations if they are set
v0.15.0,head and tail span indices are 1-indexed and end index is inclusive
v0.15.0,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.15.0,"to set the metadata ""domain"" to ""de-orcas"""
v0.15.0,get fields from line
v0.15.0,get head_id if exists (only in dependency parses)
v0.15.0,initialize token
v0.15.0,go through all columns
v0.15.0,'feats' and 'misc' column should be split into different fields
v0.15.0,special handling for whitespace after
v0.15.0,add each other feature as label-value pair
v0.15.0,get the task name (e.g. 'ner')
v0.15.0,get the label value
v0.15.0,add label
v0.15.0,remap regular tag names
v0.15.0,"if in memory, retrieve parsed sentence"
v0.15.0,else skip to position in file where sentence begins
v0.15.0,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.15.0,use all domains
v0.15.0,iter over all domains / sources and create target files
v0.15.0,The conll representation of coref spans allows spans to
v0.15.0,"overlap. If spans end or begin at the same word, they are"
v0.15.0,"separated by a ""|""."
v0.15.0,The span begins at this word.
v0.15.0,The span begins and ends at this word (single word span).
v0.15.0,"The span is starting, so we record the index of the word."
v0.15.0,"The span for this id is ending, but didn't start at this word."
v0.15.0,Retrieve the start index from the document state and
v0.15.0,add the span to the clusters for this id.
v0.15.0,strip all bracketing information to
v0.15.0,get the actual propbank label.
v0.15.0,Entering into a span for a particular semantic role label.
v0.15.0,We append the label and set the current span for this annotation.
v0.15.0,"If there's no '(' token, but the current_span_label is not None,"
v0.15.0,then we are inside a span.
v0.15.0,We're outside a span.
v0.15.0,"Exiting a span, so we reset the current span label for this annotation."
v0.15.0,The words in the sentence.
v0.15.0,The pos tags of the words in the sentence.
v0.15.0,the pieces of the parse tree.
v0.15.0,The lemmatised form of the words in the sentence which
v0.15.0,have SRL or word sense information.
v0.15.0,The FrameNet ID of the predicate.
v0.15.0,"The sense of the word, if available."
v0.15.0,"The current speaker, if available."
v0.15.0,"Cluster id -> List of (start_index, end_index) spans."
v0.15.0,Cluster id -> List of start_indices which are open for this id.
v0.15.0,Replace brackets in text and pos tags
v0.15.0,with a different token for parse trees.
v0.15.0,only keep ')' if there are nested brackets with nothing in them.
v0.15.0,There are some bad annotations in the CONLL data.
v0.15.0,"They contain no information, so to make this explicit,"
v0.15.0,we just set the parse piece to be None which will result
v0.15.0,in the overall parse tree being None.
v0.15.0,"If this is the first word in the sentence, create"
v0.15.0,empty lists to collect the NER and SRL BIO labels.
v0.15.0,"We can't do this upfront, because we don't know how many"
v0.15.0,"components we are collecting, as a sentence can have"
v0.15.0,variable numbers of SRL frames.
v0.15.0,Create variables representing the current label for each label
v0.15.0,sequence we are collecting.
v0.15.0,"If any annotation marks this word as a verb predicate,"
v0.15.0,we need to record its index. This also has the side effect
v0.15.0,of ordering the verbal predicates by their location in the
v0.15.0,"sentence, automatically aligning them with the annotations."
v0.15.0,"this would not be reached if parse_pieces contained None, hence the cast"
v0.15.0,Non-empty line. Collect the annotation.
v0.15.0,Collect any stragglers or files which might not
v0.15.0,have the '#end document' format for the end of the file.
v0.15.0,this dataset name
v0.15.0,check if data there
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,check if data there
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,download files if not present locally
v0.15.0,we need to slightly modify the original files by adding some new lines after document separators
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,Set the base path for the dataset
v0.15.0,Define column format
v0.15.0,Define dataset name
v0.15.0,Define data folder path
v0.15.0,"Check if the train data file exists, otherwise download and prepare the dataset"
v0.15.0,Download and prepare the dataset
v0.15.0,Initialize the parent class with the specified parameters
v0.15.0,"Check if the line is a change, delete or add command (like 17721c17703,17705 or 5728d5727)"
v0.15.0,Append the previous change block to the changes list
v0.15.0,Start a new change block
v0.15.0,"Capture original lines (those marked with ""<"")"
v0.15.0,"Capture new lines (those marked with "">"")"
v0.15.0,Append the last change block to the changes list
v0.15.0,Apply each change in reverse order (important to avoid index shift issues)
v0.15.0,"Determine the type of the change: `c` for change, `d` for delete, `a` for add"
v0.15.0,"Example command: 17721c17703,17705"
v0.15.0,Example command: 5728d5727
v0.15.0,"Example command: 1000a1001,1002"
v0.15.0,Write the modified content to the output file
v0.15.0,Strip whitespace to check if the line is empty
v0.15.0,Write the first token followed by a newline if the line is not empty
v0.15.0,Write an empty line if the line is empty
v0.15.0,Strip the leading '[TOKEN]\t' from the annotation
v0.15.0,Create a temporary directory
v0.15.0,Check the contents of the temporary directory
v0.15.0,Extract only the tokens from the original CoNLL03 files
v0.15.0,Apply the downloaded patch files to apply our token modifications (e.g. line breaks)
v0.15.0,Merge the updated token files with the CleanCoNLL annotations
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,Remove CoNLL-U meta information in the last column
v0.15.0,column format
v0.15.0,dataset name
v0.15.0,data folder: default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,dataset name
v0.15.0,data folder: default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,entity_mapping
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,data validation
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download files if not present locallys
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,# download zip
v0.15.0,merge the files in one as the zip is containing multiples files
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,check if data there
v0.15.0,create folder
v0.15.0,download dataset
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download and parse data if necessary
v0.15.0,create train test dev if not exist
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,If the extracted corpus file is not yet present in dir
v0.15.0,download zip if necessary
v0.15.0,"extracted corpus is not present , so unpacking it."
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download zip
v0.15.0,unpacking the zip
v0.15.0,merge the files in one as the zip is containing multiples files
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.15.0,download files if not present locally
v0.15.0,we need to modify the original files by adding new lines after after the end of each sentence
v0.15.0,if only one language is given
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,"use all languages if explicitly set to ""all"""
v0.15.0,download data if necessary
v0.15.0,initialize comlumncorpus and add it to list
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,"For each language in languages, the file is downloaded if not existent"
v0.15.0,Then a comlumncorpus of that data is created and saved in a list
v0.15.0,this list is handed to the multicorpus
v0.15.0,list that contains the columncopora
v0.15.0,download data if necessary
v0.15.0,"if language not downloaded yet, download it"
v0.15.0,create folder
v0.15.0,get google drive id from list
v0.15.0,download from google drive
v0.15.0,unzip
v0.15.0,transform data into required format
v0.15.0,"the processed dataset has the additional ending ""_new"""
v0.15.0,remove the unprocessed dataset
v0.15.0,initialize comlumncorpus and add it to list
v0.15.0,if no languages are given as argument all languages used in XTREME will be loaded
v0.15.0,if only one language is given
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,"For each language in languages, the file is downloaded if not existent"
v0.15.0,Then a comlumncorpus of that data is created and saved in a list
v0.15.0,This list is handed to the multicorpus
v0.15.0,list that contains the columncopora
v0.15.0,download data if necessary
v0.15.0,"if language not downloaded yet, download it"
v0.15.0,create folder
v0.15.0,download from HU Server
v0.15.0,unzip
v0.15.0,transform data into required format
v0.15.0,initialize comlumncorpus and add it to list
v0.15.0,if only one language is given
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,initialize comlumncorpus and add it to list
v0.15.0,download data if necessary
v0.15.0,unpack and write out in CoNLL column-like format
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,data is not in IOB2 format. Thus we transform it to IOB2
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,rename according to train - test - dev - convention
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,Add missing newline after header
v0.15.0,Workaround for empty tokens
v0.15.0,"Add ""real"" document marker"
v0.15.0,Dataset split mapping
v0.15.0,v2.0 only adds new language and splits for AJMC dataset
v0.15.0,Special document marker for sample splits in AJMC dataset
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,create dataset files from index and train/test splits
v0.15.0,news date is usually in 3rd or 4th sentence of each article
v0.15.0,"generate NoiseBench dataset variants, given CleanCoNLL, noisy label files and index file"
v0.15.0,"os.makedirs(os.path.join('data','noisebench'), exist_ok=True)"
v0.15.0,copy test set
v0.15.0,if only one language is given
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,"use all languages if explicitly set to ""all"""
v0.15.0,download data if necessary
v0.15.0,initialize comlumncorpus and add it to list
v0.15.0,this dataset name
v0.15.0,one name can map to multiple concepts
v0.15.0,NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.
v0.15.0,Use this class to load into memory all candidates
v0.15.0,"if identifier == ""MESH:D013749"":"
v0.15.0,# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.
v0.15.0,continue
v0.15.0,parse line
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download and parse data if necessary
v0.15.0,paths to train and test splits
v0.15.0,init corpus
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download and parse data if necessary
v0.15.0,iterate over all html files
v0.15.0,"get rid of html syntax, we only need the text"
v0.15.0,between all documents we write a separator symbol
v0.15.0,skip empty strings
v0.15.0,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.15.0,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.15.0,sentence splitting and tokenization
v0.15.0,iterate through all annotations and add to corresponding tokens
v0.15.0,find sentence to which annotation belongs
v0.15.0,position within corresponding sentence
v0.15.0,set annotation for tokens of entity mention
v0.15.0,write to out-file in column format
v0.15.0,"in case something goes wrong, delete the dataset and raise error"
v0.15.0,this dataset name
v0.15.0,download and parse data if necessary
v0.15.0,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.15.0,generate qid wikiname dictionaries
v0.15.0,merge dictionaries
v0.15.0,ignore first line
v0.15.0,commented and empty lines
v0.15.0,read all Q-IDs
v0.15.0,ignore first line
v0.15.0,request
v0.15.0,this dataset name
v0.15.0,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.15.0,like this we can quickly check if the corresponding page exists
v0.15.0,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.15.0,delete unprocessed file
v0.15.0,collect all wikiids
v0.15.0,create the dictionary
v0.15.0,request
v0.15.0,this dataset name
v0.15.0,names of raw text documents
v0.15.0,open output_file
v0.15.0,iterate through all documents
v0.15.0,split sentences and tokenize
v0.15.0,iterate through all annotations and add to corresponding tokens
v0.15.0,find sentence to which annotation belongs
v0.15.0,position within corresponding sentence
v0.15.0,set annotation for tokens of entity mention
v0.15.0,write to out file
v0.15.0,annotation from one annotator or two agreeing annotators
v0.15.0,this dataset name
v0.15.0,download and parse data if necessary
v0.15.0,this dataset name
v0.15.0,download and parse data if necessary
v0.15.0,First parse the post titles
v0.15.0,Keep track of how many and which entity mentions does a given post title have
v0.15.0,Check if the current post title has an entity link and parse accordingly
v0.15.0,Post titles with entity mentions (if any) are handled via this function
v0.15.0,Then parse the comments
v0.15.0,"Iterate over the comments.tsv file, until the end is reached"
v0.15.0,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.15.0,Each comment thread is handled as one 'document'.
v0.15.0,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.15.0,This if-condition is needed to handle this problem.
v0.15.0,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.15.0,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.15.0,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.15.0,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.15.0,and not just single letters into single rows.
v0.15.0,If there are annotated entity mentions for given post title or a comment thread
v0.15.0,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.15.0,Write the token with a corresponding tag to file
v0.15.0,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.15.0,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.15.0,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.15.0,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.15.0,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.15.0,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.15.0,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.15.0,Check if further annotations belong to the current post title or comment thread as well
v0.15.0,Stop when the end of an annotation file is reached
v0.15.0,Check if further annotations belong to the current sentence as well
v0.15.0,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.15.0,Docstart
v0.15.0,if there is more than one word in the chunk we write each in a separate line
v0.15.0,print(chunks)
v0.15.0,empty line after each sentence
v0.15.0,convert the file to CoNLL
v0.15.0,this dataset name
v0.15.0,"check if data there, if not, download the data"
v0.15.0,create folder
v0.15.0,download data
v0.15.0,transform data into column format if necessary
v0.15.0,if no filenames are specified we use all the data
v0.15.0,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.15.0,also we remove 'raganato_ALL' from filenames in case its in the list
v0.15.0,generate the test file
v0.15.0,make column file and save to data_folder
v0.15.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.0,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.0,create folder
v0.15.0,download data
v0.15.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.0,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.0,create folder
v0.15.0,download data
v0.15.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.0,generate the test file
v0.15.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.0,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.0,create folder
v0.15.0,download data
v0.15.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.0,generate the test file
v0.15.0,default dataset folder is the cache root
v0.15.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.0,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.0,create folder
v0.15.0,download data
v0.15.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.0,generate the test file
v0.15.0,default dataset folder is the cache root
v0.15.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.0,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.0,create folder
v0.15.0,download data
v0.15.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.0,generate the test file
v0.15.0,default dataset folder is the cache root
v0.15.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.15.0,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.15.0,create folder
v0.15.0,download data
v0.15.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.15.0,generate the test file
v0.15.0,TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146
v0.15.0,+1 assumes the title and abstract will be joined by a space.
v0.15.0,"We need a unique identifier for this entity, so build it from the document id and entity id"
v0.15.0,The user can provide a callable that returns the database name.
v0.15.0,some entities are not linked and
v0.15.0,some entities are linked to multiple normalized ids
v0.15.0,passages must not overlap and spans must cover the entire document
v0.15.0,entities
v0.15.0,parse db ids
v0.15.0,Some of the entities have a off-by-one error. Correct these annotations!
v0.15.0,"passage offsets/lengths do not connect, recalculate them for this schema."
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,if True:
v0.15.0,write CoNLL-U Plus header
v0.15.0,"Some special cases (e.g., missing spaces before entity marker)"
v0.15.0,necessary if text should be whitespace tokenizeable
v0.15.0,Handle case where tail may occur before the head
v0.15.0,this dataset name
v0.15.0,write CoNLL-U Plus header
v0.15.0,this dataset name
v0.15.0,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.15.0,download data if necessary
v0.15.0,write CoNLL-U Plus header
v0.15.0,The span has ended.
v0.15.0,We are entering a new span; reset indices
v0.15.0,and active tag to new span.
v0.15.0,We're inside a span.
v0.15.0,Last token might have been a part of a valid span.
v0.15.0,this dataset name
v0.15.0,write CoNLL-U Plus header
v0.15.0,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.15.0,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.15.0,target_file_path = Path(data_folder) / target_filename
v0.15.0,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.15.0,# write CoNLL-U Plus header
v0.15.0,"target_file.write(""# global.columns = id form ner\n"")"
v0.15.0,for example in json.load(source_file):
v0.15.0,token_list = self._tacred_example_to_token_list(example)
v0.15.0,target_file.write(token_list.serialize())
v0.15.0,check if first tag row is already occupied
v0.15.0,"if first tag row is occupied, use second tag row"
v0.15.0,hardcoded mapping TODO: perhaps find nicer solution
v0.15.0,remap regular tag names
v0.15.0,else skip to position in file where sentence begins
v0.15.0,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.15.0,read in dev file if exists
v0.15.0,read in test file if exists
v0.15.0,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.15.0,"find train, dev and test files if not specified"
v0.15.0,use test_file to create test split if available
v0.15.0,use dev_file to create test split if available
v0.15.0,"if data point contains black-listed label, do not use"
v0.15.0,first check if valid sentence
v0.15.0,"if so, add to indices"
v0.15.0,"find train, dev and test files if not specified"
v0.15.0,variables
v0.15.0,different handling of in_memory data than streaming data
v0.15.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.15.0,test if format is OK
v0.15.0,test if at least one label given
v0.15.0,make sentence from text (and filter for length)
v0.15.0,"if a pair column is defined, make a sentence pair object"
v0.15.0,noinspection PyDefaultArgument
v0.15.0,dataset name includes the split size
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download each of the 28 splits
v0.15.0,create dataset directory if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,download data from same source as in huggingface's implementations
v0.15.0,read label order
v0.15.0,"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
v0.15.0,"Re-map to [0, 1, 2, 3]."
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,handle labels file
v0.15.0,handle data file
v0.15.0,Create flair compatible labels
v0.15.0,"by default, map point score to POSITIVE / NEGATIVE values"
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,create dataset directory if necessary
v0.15.0,create train.txt file from CSV
v0.15.0,create test.txt file from CSV
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,create dataset directory if necessary
v0.15.0,create train.txt file by iterating over pos and neg file
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,create dataset directory if necessary
v0.15.0,create train.txt file by iterating over pos and neg file
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,create dataset directory if necessary
v0.15.0,create train.txt file by iterating over pos and neg file
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,create dataset directory if necessary
v0.15.0,create train.txt file by iterating over pos and neg file
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,create train dev and test files in fasttext format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download senteval datasets if necessary und unzip
v0.15.0,convert to FastText format
v0.15.0,download data if necessary
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,move original .tsv files to another folder
v0.15.0,create train and dev splits in fasttext format
v0.15.0,create eval_dataset file with no labels
v0.15.0,download zip archive
v0.15.0,unpack file in datasets directory (zip archive contains a directory named SST-2)
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,download datasets if necessary
v0.15.0,create dataset directory if necessary
v0.15.0,create correctly formated txt files
v0.15.0,multiple labels are possible
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,Create flair compatible labels
v0.15.0,TREC-6 : NUM:dist -> __label__NUM
v0.15.0,TREC-50: NUM:dist -> __label__NUM:dist
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,Create flair compatible labels
v0.15.0,TREC-6 : NUM:dist -> __label__NUM
v0.15.0,TREC-50: NUM:dist -> __label__NUM:dist
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,create a separate directory for different tasks
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,check if dataset is supported
v0.15.0,set file names
v0.15.0,set file names
v0.15.0,download and unzip in file structure if necessary
v0.15.0,instantiate corpus
v0.15.0,"find train, dev and test files if not specified"
v0.15.0,"create DataPairDataset for train, test and dev file, if they are given"
v0.15.0,stop if file does not exist
v0.15.0,create a DataPair object from strings
v0.15.0,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.15.0,"find train, dev, and test files if not specified"
v0.15.0,"create DataTripleDataset for train, test, and dev files, if they are given"
v0.15.0,stop if the file does not exist
v0.15.0,create a DataTriple object from strings
v0.15.0,"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings"
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,"rename test file to eval_dataset, since it has no labels"
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.15.0,dev sets include 5 different annotations but we will only keep the gold label
v0.15.0,"rename test file to eval_dataset, since it has no labels"
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get test and dev sets
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,"rename test file to eval_dataset, since it has no labels"
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,"rename test file to eval_dataset, since it has no labels"
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,"rename test file to eval_dataset, since it has no labels"
v0.15.0,"if data is not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,"rename test file to eval_dataset, since it has no labels"
v0.15.0,"if data not downloaded yet, download it"
v0.15.0,get the zip file
v0.15.0,"the downloaded files have json format, we transform them to tsv"
v0.15.0,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.15.0,remove json file
v0.15.0,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.15.0,with sum of all entity lengths as secondary key
v0.15.0,calculate offset without current text
v0.15.0,because we stick all passages of a document together
v0.15.0,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.15.0,Try to fix incorrect annotations
v0.15.0,print(
v0.15.0,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.15.0,)
v0.15.0,Ignore empty lines or relation annotations
v0.15.0,FIX annotation of whitespaces (necessary for PDR)
v0.15.0,Add task description for multi-task learning
v0.15.0,One token may contain multiple entities -> deque all of them
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.15.0,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,Edge case: last token starts a new entity
v0.15.0,Last document in file
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,In the huner split files there is no information whether a given id originates
v0.15.0,from the train or test file of the original corpus - so we have to adapt corpus
v0.15.0,splitting here
v0.15.0,Edge case: last token starts a new entity
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,Read texts
v0.15.0,Read annotations
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,We need to apply a patch to correct the original training file
v0.15.0,Articles title
v0.15.0,Article abstract
v0.15.0,Entity annotations
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,Edge case: last token starts a new entity
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,Incomplete article
v0.15.0,Invalid XML syntax
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,if len(mid) != 3:
v0.15.0,continue
v0.15.0,Try to fix entity offsets
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,There is still one illegal annotation in the file ..
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,"Abstract first, title second to prevent issues with sentence splitting"
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,"Filter for specific entity types, by default no entities will be filtered"
v0.15.0,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.15.0,train and dev split of V2 will be train in V4
v0.15.0,test split of V2 will be dev in V4
v0.15.0,New documents in V4 will become test documents
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,column format
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,column format
v0.15.0,build dataset name and full huggingface reference name
v0.15.0,Download data if necessary
v0.15.0,"Some datasets in BigBio only have train or test splits, not both"
v0.15.0,"If only test split, assign it to train split"
v0.15.0,"If only train split, sample other from it (sample_missing_splits=True)"
v0.15.0,Not every dataset has a dev / validation set!
v0.15.0,Perform type mapping if necessary
v0.15.0,return None
v0.15.0,TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?
v0.15.0,"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG"
v0.15.0,"""cancer"": ""disease"",  # BioNLP ST 2013 CG"
v0.15.0,"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG"
v0.15.0,"""gene"": ""gene"",  # NLM Gene"
v0.15.0,"""chemical"": ""chemical"",  # NLM Chem"
v0.15.0,"""cellline"": ""cell_line"",  # Cell Finder"
v0.15.0,"""species"": ""species"",  # Cell Finder"
v0.15.0,"""protein"": ""gene"",  # BioID"
v0.15.0,"Collect all texts of the document, each passage will be"
v0.15.0,a text in our internal format
v0.15.0,Sort passages by start offset
v0.15.0,Transform all entity annotations into internal format
v0.15.0,Find the passage of the entity (necessary for offset adaption)
v0.15.0,Adapt entity offsets according to passage offsets
v0.15.0,FIXME: This is just for debugging purposes
v0.15.0,passage_text = id_to_text[passage_id]
v0.15.0,doc_text = passage_text[entity_offset[0] : entity_offset[1]]
v0.15.0,"mention_text = entity[""text""][0]"
v0.15.0,if doc_text != mention_text:
v0.15.0,"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
v0.15.0,Get element in the middle
v0.15.0,Is the mention with the passage offsets?
v0.15.0,"If element is smaller than mid, then it can only"
v0.15.0,be present in left subarray
v0.15.0,Else the element can only be present in right subarray
v0.15.0,TODO whether cell or cell line is the correct tag
v0.15.0,TODO whether cell or cell line is the correct tag
v0.15.0,Special case for ProGene: We need to use the split_0_train and split_0_test splits
v0.15.0,as they are currently provided in BigBio
v0.15.0,cache Feidegger config file
v0.15.0,cache Feidegger images
v0.15.0,replace image URL with local cached file
v0.15.0,append Sentence-Image data point
v0.15.0,cast to list if necessary
v0.15.0,cast to list if necessary
v0.15.0,"first, check if pymongo is installed"
v0.15.0,automatically identify train / test / dev files
v0.15.0,"if no test file is found, take any file with 'test' in name"
v0.15.0,Expose base classses
v0.15.0,Expose all biomedical data sets used for the evaluation of BioBERT
v0.15.0,-
v0.15.0,-
v0.15.0,-
v0.15.0,-
v0.15.0,Expose all biomedical data sets using the HUNER splits
v0.15.0,Expose all biomedical data sets
v0.15.0,Expose all document classification datasets
v0.15.0,word sense disambiguation
v0.15.0,Expose all entity linking datasets
v0.15.0,Expose all relation extraction datasets
v0.15.0,universal proposition banks
v0.15.0,keyphrase detection datasets
v0.15.0,other NER datasets
v0.15.0,standard NER datasets
v0.15.0,Expose all sequence labeling datasets
v0.15.0,Expose all text-image datasets
v0.15.0,Expose all text-text datasets
v0.15.0,Expose all treebanks
v0.15.0,"find train, dev and test files if not specified"
v0.15.0,get train data
v0.15.0,get test data
v0.15.0,get dev data
v0.15.0,option 1: read only sentence boundaries as offset positions
v0.15.0,option 2: keep everything in memory
v0.15.0,"if in memory, retrieve parsed sentence"
v0.15.0,else skip to position in file where sentence begins
v0.15.0,current token ID
v0.15.0,handling for the awful UD multiword format
v0.15.0,end of sentence
v0.15.0,comments or ellipsis
v0.15.0,if token is a multi-word
v0.15.0,normal single-word tokens
v0.15.0,"if we don't split multiwords, skip over component words"
v0.15.0,add token
v0.15.0,add morphological tags
v0.15.0,derive whitespace logic for multiwords
v0.15.0,"if multi-word equals component tokens, there should be no whitespace"
v0.15.0,go through all tokens in subword and set whitespace_after information
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,this dataset name
v0.15.0,default dataset folder is the cache root
v0.15.0,download data if necessary
v0.15.0,"finally, print model card for information"
v0.15.0,Note: Multi-GPU can affect corpus loading
v0.15.0,This code will run multiple times -- each GPU gets its own process and each process runs this code. We need to
v0.15.0,"ensure that the corpus has the same elements and order on all processes, despite sampling. We do that by using"
v0.15.0,the same seed on all processes.
v0.15.0,Note: Multi-GPU can affect choice of batch size.
v0.15.0,"In order to compare batch updates fairly between single and multi-GPU training, we should:"
v0.15.0,1) Step the optimizer after the same number of examples to achieve com
v0.15.0,2) Process the same number of examples in each forward pass
v0.15.0,"e.g. Suppose your machine has 2 GPUs. If multi_gpu=False, the first gpu will process 32 examples, then the"
v0.15.0,"first gpu will process another 32 examples, then the optimizer will step. If multi_gpu=True, each gpu will"
v0.15.0,"process 32 examples at the same time, then the optimizer will step."
v0.15.0,noqa: INP001
v0.15.0,-- Project information -----------------------------------------------------
v0.15.0,"The full version, including alpha/beta/rc tags"
v0.15.0,use smv_current_version as the git url
v0.15.0,-- General configuration ---------------------------------------------------
v0.15.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.15.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.15.0,ones.
v0.15.0,"Add any paths that contain templates here, relative to this directory."
v0.15.0,"List of patterns, relative to source directory, that match files and"
v0.15.0,directories to ignore when looking for source files.
v0.15.0,This pattern also affects html_static_path and html_extra_path.
v0.15.0,-- Options for HTML output -------------------------------------------------
v0.15.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.15.0,a list of builtin themes.
v0.15.0,
v0.15.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.15.0,"relative to this directory. They are copied after the builtin static files,"
v0.15.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.15.0,Napoleon settings
v0.15.0,Whitelist pattern for tags (set to None to ignore all tags)
v0.15.0,Whitelist pattern for branches (set to None to ignore all branches)
v0.15.0,Whitelist pattern for remotes (set to None to use local branches only)
v0.15.0,Pattern for released versions
v0.15.0,Format for versioned output directories inside the build directory
v0.15.0,Determines whether remote or local git branches/tags are preferred if their output dirs conflict
v0.15.0,test corpus
v0.15.0,create a TARS classifier
v0.15.0,check if right number of classes
v0.15.0,switch to task with only one label
v0.15.0,check if right number of classes
v0.15.0,switch to task with three labels provided as list
v0.15.0,check if right number of classes
v0.15.0,switch to task with four labels provided as set
v0.15.0,check if right number of classes
v0.15.0,switch to task with two labels provided as Dictionary
v0.15.0,check if right number of classes
v0.15.0,test corpus
v0.15.0,create a TARS classifier
v0.15.0,switch to a new task (TARS can do multiple tasks so you must define one)
v0.15.0,initialize the text classifier trainer
v0.15.0,start the training
v0.15.0,"With end symbol, without start symbol, padding in front"
v0.15.0,"Without end symbol, with start symbol, padding in back"
v0.15.0,"Without end symbol, without start symbol, padding in front"
v0.15.0,initialize trainer
v0.15.0,initialize trainer
v0.15.0,initialize trainer
v0.15.0,clean up directory
v0.15.0,clean up directory
v0.15.0,example sentence
v0.15.0,set 4 labels for 2 tokens ('love' is tagged twice)
v0.15.0,check if there are three POS labels with correct text and values
v0.15.0,check if there are is one SENTIMENT label with correct text and values
v0.15.0,check if all tokens are correctly labeled
v0.15.0,remove the pos label from the last word
v0.15.0,there should be 2 POS labels left
v0.15.0,now remove all pos tags
v0.15.0,set 3 labels for 2 spans (HU is tagged twice)
v0.15.0,check if there are three labels with correct text and values
v0.15.0,check if there are two spans with correct text and values
v0.15.0,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.15.0,should be only one NER label left
v0.15.0,and only one NER span
v0.15.0,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.15.0,check if there are three labels with correct text and values
v0.15.0,check if there are two spans with correct text and values
v0.15.0,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.15.0,should be only one NER label left
v0.15.0,and only one NER span
v0.15.0,but there is also one orgtype span and label
v0.15.0,and only one NER span
v0.15.0,let's add the NER tag back
v0.15.0,check if there are three labels with correct text and values
v0.15.0,check if there are two spans with correct text and values
v0.15.0,now remove all NER tags
v0.15.0,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.15.0,create two relation label
v0.15.0,there should be two relation labels
v0.15.0,there should be one syntactic labels
v0.15.0,"there should be two relations, one with two and one with one label"
v0.15.0,example sentence
v0.15.0,add another topic label
v0.15.0,example sentence
v0.15.0,has sentiment value
v0.15.0,has 4 part of speech tags
v0.15.0,has 1 NER tag
v0.15.0,should be in total 6 labels
v0.15.0,example sentence
v0.15.0,add two NER labels
v0.15.0,get the four labels
v0.15.0,check that only two of the respective data points are equal
v0.15.0,make a sentence and some right context
v0.15.0,TODO: is this desirable? Or should two sentences with same text be considered same objects?
v0.15.0,Initializing a Sentence this way assumes that there is a space after each token
v0.15.0,get default dictionary
v0.15.0,init forward LM with 128 hidden states and 1 layer
v0.15.0,get the example corpus and process at character level in forward direction
v0.15.0,train the language model
v0.15.0,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.15.0,clean up results directory
v0.15.0,get default dictionary
v0.15.0,init forward LM with 128 hidden states and 1 layer
v0.15.0,get the example corpus and process at character level in forward direction
v0.15.0,train the language model
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,load column dataset with one entry
v0.15.0,load column dataset with two entries
v0.15.0,load column dataset with three entries
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,check if Token labels are correct
v0.15.0,"get training, test and dev data"
v0.15.0,check if Token labels for frames are correct
v0.15.0,"get training, test and dev data"
v0.15.0,"get training, test and dev data"
v0.15.0,get two corpora as one
v0.15.0,"get training, test and dev data for full English UD corpus from web"
v0.15.0,clean up data directory
v0.15.0,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.15.0,"""2"","
v0.15.0,"""0"","
v0.15.0,"""4"","
v0.15.0,"""2"","
v0.15.0,"""2"","
v0.15.0,"""2"","
v0.15.0,]
v0.15.0,This test only covers basic universal dependencies datasets.
v0.15.0,"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
v0.15.0,"Here, we use the default token annotation fields."
v0.15.0,This test covers the complete HIPE 2022 dataset.
v0.15.0,https://github.com/hipe-eval/HIPE-2022-data
v0.15.0,"Includes variant with document separator, and all versions of the dataset."
v0.15.0,"We have manually checked, that these numbers are correct:"
v0.15.0,"+1 offset, because of missing EOS marker at EOD"
v0.15.0,Test data for v2.1 release
v0.15.0,This test covers the complete ICDAR Europeana corpus:
v0.15.0,https://github.com/stefan-it/historic-domain-adaptation-icdar
v0.15.0,"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
v0.15.0,This test covers the NERMuD dataset. Official stats can be found here:
v0.15.0,https://github.com/dhfbk/KIND/tree/main/evalita-2023
v0.15.0,Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
v0.15.0,This test covers the complete MasakhaPOS dataset.
v0.15.0,"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
v0.15.0,--- Embeddings that are shared by both models --- #
v0.15.0,--- Task 1: Sentiment Analysis (5-class) --- #
v0.15.0,Define corpus and model
v0.15.0,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.15.0,Define corpus and model
v0.15.0,-- Define mapping (which tagger should train on which model) -- #
v0.15.0,-- Create model trainer and train -- #
v0.15.0,NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
v0.15.0,clean up file
v0.15.0,get features from forward propagation
v0.15.0,reverse sort all sequences by their length
v0.15.0,remove previously predicted labels of this type
v0.15.0,no need for label_dict
v0.15.0,"pretrained_model = ""tars-ner""  # disabled due to too much space requirements."
v0.15.0,check if right number of classes
v0.15.0,switch to task with only one label
v0.15.0,check if right number of classes
v0.15.0,switch to task with three labels provided as list
v0.15.0,check if right number of classes
v0.15.0,switch to task with four labels provided as set
v0.15.0,check if right number of classes
v0.15.0,switch to task with two labels provided as Dictionary
v0.15.0,check if right number of classes
v0.15.0,Intel ----founded_by---> Gordon Moore
v0.15.0,Intel ----founded_by---> Robert Noyce
v0.15.0,"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
v0.15.0,Check sentence masking and relation label annotation on
v0.15.0,"training, validation and test dataset (in this test the splits are the same)"
v0.15.0,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.15.0,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.15.0,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.15.0,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
v0.15.0,This sentence is only included if we transform the corpus with cross augmentation
v0.15.0,"pretrained_model = ""tars-base""  # disabled due to too much space requirements."
v0.15.0,Ensure this is an example that predicts no classes in multilabel
v0.15.0,check if right number of classes
v0.15.0,switch to task with only one label
v0.15.0,check if right number of classes
v0.15.0,switch to task with three labels provided as list
v0.15.0,check if right number of classes
v0.15.0,switch to task with four labels provided as set
v0.15.0,check if right number of classes
v0.15.0,switch to task with two labels provided as Dictionary
v0.15.0,check if right number of classes
v0.15.0,ensure that the prepared tensors is what we expect
v0.15.0,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.15.0,previous and next sentence as context
v0.15.0,test expansion for sentence without context
v0.15.0,test expansion for with previous and next as context
v0.15.0,test expansion if first sentence is document boundary
v0.15.0,test expansion if we don't use context
v0.15.0,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.15.0,dummy model with embeddings
v0.15.0,save the dummy and load it again
v0.15.0,check that context_length and use_context_separator is the same for both
v0.14.0,mmap seems to be much more memory efficient
v0.14.0,Remove quotes from etag
v0.14.0,"If there is an etag, it's everything after the first period"
v0.14.0,"Otherwise, use None"
v0.14.0,"URL, so get it from the cache (downloading if necessary)"
v0.14.0,"File, and it exists."
v0.14.0,"File, but it doesn't exist."
v0.14.0,Something unknown
v0.14.0,Extract all the contents of zip file in current directory
v0.14.0,use model name as subfolder
v0.14.0,Lazy import
v0.14.0,output information
v0.14.0,Extract all the contents of zip file in current directory
v0.14.0,TODO(joelgrus): do we want to do checksums or anything like that?
v0.14.0,get cache path to put the file
v0.14.0,make HEAD request to check ETag
v0.14.0,add ETag to filename if it exists
v0.14.0,"etag = response.headers.get(""ETag"")"
v0.14.0,"Download to temporary file, then copy to cache dir once finished."
v0.14.0,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.14.0,GET file object
v0.14.0,These defaults are the same as the argument defaults in tqdm.
v0.14.0,load_big_file is a workaround byhttps://github.com/highway11git
v0.14.0,to load models on some Mac/Windows setups
v0.14.0,see https://github.com/zalandoresearch/flair/issues/351
v0.14.0,first determine the distribution of classes in the dataset
v0.14.0,weight for each sample
v0.14.0,Create blocks
v0.14.0,shuffle the blocks
v0.14.0,concatenate the shuffled blocks
v0.14.0,Create blocks
v0.14.0,shuffle the blocks
v0.14.0,concatenate the shuffled blocks
v0.14.0,increment for last token in sentence if not followed by whitespace
v0.14.0,this is the default init size of a lmdb database for embeddings
v0.14.0,get db filename from embedding name
v0.14.0,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.14.0,SequenceTagger
v0.14.0,TextClassifier
v0.14.0,get db filename from embedding name
v0.14.0,if embedding database already exists
v0.14.0,"otherwise, push embedding to database"
v0.14.0,if embedding database already exists
v0.14.0,open the database in read mode
v0.14.0,we need to set self.k
v0.14.0,create and load the database in write mode
v0.14.0,"no idea why, but we need to close and reopen the environment to avoid"
v0.14.0,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.14.0,when opening new transaction !
v0.14.0,init dictionaries
v0.14.0,"in order to deal with unknown tokens, add <unk>"
v0.14.0,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.14.0,set 'add_unk' depending on whether <unk> is a key
v0.14.0,"if one embedding name, directly return it"
v0.14.0,"if multiple embedding names, concatenate them"
v0.14.0,First we remove any existing labels for this PartOfSentence in self.sentence
v0.14.0,labels also need to be deleted at Sentence object
v0.14.0,delete labels at object itself
v0.14.0,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.14.0,"therefore, labels get added only to the Sentence if it exists"
v0.14.0,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.14.0,"Therefore, labels get set only to the Sentence if it exists"
v0.14.0,"check if the span already exists. If so, return it"
v0.14.0,else make a new span
v0.14.0,"check if the relation already exists. If so, return it"
v0.14.0,else make a new relation
v0.14.0,private field for all known spans
v0.14.0,the tokenizer used for this sentence
v0.14.0,some sentences represent a document boundary (but most do not)
v0.14.0,internal variables to denote position inside dataset
v0.14.0,"if text is passed, instantiate sentence with tokens (words)"
v0.14.0,determine token positions and whitespace_after flag
v0.14.0,the last token has no whitespace after
v0.14.0,log a warning if the dataset is empty
v0.14.0,data with zero-width characters cannot be handled
v0.14.0,set token idx and sentence
v0.14.0,append token to sentence
v0.14.0,register token annotations on sentence
v0.14.0,move sentence embeddings to device
v0.14.0,also move token embeddings to device
v0.14.0,clear token embeddings
v0.14.0,infer whitespace after field
v0.14.0,"if sentence has no tokens, return empty string"
v0.14.0,"otherwise, return concatenation of tokens with the correct offsets"
v0.14.0,The sentence's start position is not propagated to its tokens.
v0.14.0,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.14.0,No character at the corresponding code point: remove it
v0.14.0,"if no label if specified, return all labels"
v0.14.0,"if the label type exists in the Sentence, return it"
v0.14.0,return empty list if none of the above
v0.14.0,labels also need to be deleted at all tokens
v0.14.0,labels also need to be deleted at all known spans
v0.14.0,remove spans without labels
v0.14.0,delete labels at object itself
v0.14.0,set name
v0.14.0,abort if no data is provided
v0.14.0,sample test data from train if none is provided
v0.14.0,sample dev data from train if none is provided
v0.14.0,set train dev and test data
v0.14.0,find out empty sentence indices
v0.14.0,create subset of non-empty sentence indices
v0.14.0,find out empty sentence indices
v0.14.0,create subset of non-empty sentence indices
v0.14.0,"first, determine the datapoint type by going through dataset until first label is found"
v0.14.0,count all label types per sentence
v0.14.0,go through all labels of label_type and count values
v0.14.0,special handling for Token-level annotations. Add all untagged as 'O' label
v0.14.0,"if an unk threshold is set, UNK all label values below this threshold"
v0.14.0,sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
v0.14.0,replace the old label with the new one
v0.14.0,keep track of the old (clean) label using another label type category
v0.14.0,keep track of how many labels in total are flipped
v0.14.0,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.14.0,replace the old label with the new one
v0.14.0,keep track of the old (clean) label using another label type category
v0.14.0,keep track of how many labels in total are flipped
v0.14.0,"add a dummy ""O"" to close final prediction"
v0.14.0,return complex list
v0.14.0,internal variables
v0.14.0,non-set tags are OUT tags
v0.14.0,anything that is not OUT is IN
v0.14.0,does this prediction start a new span?
v0.14.0,B- and S- always start new spans
v0.14.0,"if the predicted class changes, I- starts a new span"
v0.14.0,"if the predicted class changes and S- was previous tag, start a new span"
v0.14.0,if an existing span is ended (either by reaching O or starting a new span)
v0.14.0,determine score and value
v0.14.0,append to result list
v0.14.0,reset for-loop variables for new span
v0.14.0,remember previous tag
v0.14.0,global variable: cache_root
v0.14.0,Get the device from the environment variable
v0.14.0,global variable: device
v0.14.0,"No need for correctness checks, torch is doing it"
v0.14.0,global variable: version
v0.14.0,global variable: arrow symbol
v0.14.0,dummy return to fulfill trainer.train() needs
v0.14.0,print(vec)
v0.14.0,Attach optimizer
v0.14.0,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.14.0,if memory mode option 'none' delete everything
v0.14.0,"if dynamic embedding keys not passed, identify them automatically"
v0.14.0,always delete dynamic embeddings
v0.14.0,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.14.0,optional metric space decoder if prototypes have different length than embedding
v0.14.0,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.14.0,"if set, create initial prototypes from normal distribution"
v0.14.0,"if set, use a radius"
v0.14.0,all parameters will be pushed internally to the specified device
v0.14.0,decode embeddings into prototype space
v0.14.0,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.14.0,verbalize BIOES labels
v0.14.0,"if label is not BIOES, use label itself"
v0.14.0,Always include the name of the Model class for which the state dict holds
v0.14.0,"write out a ""model card"" if one is set"
v0.14.0,save model
v0.14.0,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.14.0,get all non-abstract subclasses
v0.14.0,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.14.0,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.14.0,"if the model cannot be fetched, load as a file"
v0.14.0,try to get model class from state
v0.14.0,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.14.0,"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
v0.14.0,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.14.0,"if this class is not abstract, fetch the model and load it"
v0.14.0,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.14.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.14.0,loss calculation
v0.14.0,variables for printing
v0.14.0,variables for computing scores
v0.14.0,remove any previously predicted labels
v0.14.0,predict for batch
v0.14.0,get the gold labels
v0.14.0,add to all_predicted_values
v0.14.0,make printout lines
v0.14.0,convert true and predicted values to two span-aligned lists
v0.14.0,delete exluded labels if exclude_labels is given
v0.14.0,"if after excluding labels, no label is left, ignore the datapoint"
v0.14.0,write all_predicted_values to out_file if set
v0.14.0,make the evaluation dictionary
v0.14.0,check if this is a multi-label problem
v0.14.0,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.14.0,multi-label problems require a multi-hot vector for each true and predicted label
v0.14.0,single-label problems can do with a single index for each true and predicted label
v0.14.0,"now, calculate evaluation numbers"
v0.14.0,there is at least one gold label or one prediction (default)
v0.14.0,compute accuracy separately as it is not always in classification_report (e.. when micro avg exists)
v0.14.0,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.14.0,"The ""micro avg"" appears only in the classification report if no prediction is possible."
v0.14.0,"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
v0.14.0,"Create and populate score object for logging with all evaluation values, plus the loss"
v0.14.0,issue error and default all evaluation numbers to 0.
v0.14.0,check if there is a label mismatch
v0.14.0,print info
v0.14.0,set the embeddings
v0.14.0,initialize the label dictionary
v0.14.0,initialize the decoder
v0.14.0,set up multi-label logic
v0.14.0,init dropouts
v0.14.0,loss weights and loss function
v0.14.0,Initialize the weight tensor
v0.14.0,set up gradient reversal if so specified
v0.14.0,embed sentences
v0.14.0,get a tensor of data points
v0.14.0,do dropout
v0.14.0,make a forward pass to produce embedded data points and labels
v0.14.0,get the data points for which to predict labels
v0.14.0,get their gold labels as a tensor
v0.14.0,pass data points through network to get encoded data point tensor
v0.14.0,decode
v0.14.0,an optional masking step (no masking in most cases)
v0.14.0,calculate the loss
v0.14.0,filter empty sentences
v0.14.0,reverse sort all sequences by their length
v0.14.0,progress bar for verbosity
v0.14.0,filter data points in batch
v0.14.0,stop if all sentences are empty
v0.14.0,pass data points through network and decode
v0.14.0,if anything could possibly be predicted
v0.14.0,remove previously predicted labels of this type
v0.14.0,filter data points that have labels outside of dictionary
v0.14.0,add DefaultClassifier arguments
v0.14.0,add variables of DefaultClassifier
v0.14.0,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.14.0,Get projected 1st dimension
v0.14.0,Compute bilinear form
v0.14.0,Arcosh
v0.14.0,Project the input data to n+1 dimensions
v0.14.0,"The first dimension, is recomputed in the distance module"
v0.14.0,header for 'weights.txt'
v0.14.0,"determine the column index of loss, f-score and accuracy for"
v0.14.0,"train, dev and test split"
v0.14.0,then get all relevant values from the tsv
v0.14.0,then get all relevant values from the tsv
v0.14.0,plot i
v0.14.0,save plots
v0.14.0,save plots
v0.14.0,plt.show()
v0.14.0,save plot
v0.14.0,auto-spawn on GPU if available
v0.14.0,progress bar for verbosity
v0.14.0,stop if all sentences are empty
v0.14.0,clearing token embeddings to save memory
v0.14.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.14.0,TODO: not saving lines yet
v0.14.0,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.14.0,- MaskedRelationClassifier ?
v0.14.0,This depends if this relation classification architecture should replace or offer as an alternative.
v0.14.0,Set label type and prepare label dictionary
v0.14.0,Initialize super default classifier
v0.14.0,Add the special tokens from the encoding strategy
v0.14.0,"Auto-spawn on GPU, if available"
v0.14.0,Only use entities labelled with the specified labels for each label type
v0.14.0,Only use entities above the specified threshold
v0.14.0,Use a dictionary to find gold relation annotations for a given entity pair
v0.14.0,Yield head and tail entity pairs from the cross product of all entities
v0.14.0,Remove identity relation entity pairs
v0.14.0,Remove entity pairs with labels that do not match any
v0.14.0,of the specified relations in `self.entity_pair_labels`
v0.14.0,"Obtain gold label, if existing"
v0.14.0,Some sanity checks
v0.14.0,Pre-compute non-leading head and tail tokens for entity masking
v0.14.0,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.14.0,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.14.0,"Therefore, we use the span's position in the sentence."
v0.14.0,Create masked sentence
v0.14.0,Add gold relation annotation as sentence label
v0.14.0,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.14.0,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.14.0,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.14.0,"may get distributed into different splits. For training purposes, this is always undesired."
v0.14.0,Ensure that all sentences are encoded properly
v0.14.0,Deal with the case where all sentences are encoded sentences
v0.14.0,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.14.0,Deal with the case where all sentences are standard (non-encoded) sentences
v0.14.0,"For each encoded sentence, transfer its prediction onto the original relation"
v0.14.0,auto-spawn on GPU if available
v0.14.0,pad strings with whitespaces to longest sentence
v0.14.0,cut up the input into chunks of max charlength = chunk_size
v0.14.0,push each chunk through the RNN language model
v0.14.0,concatenate all chunks to make final output
v0.14.0,initial hidden state
v0.14.0,get predicted weights
v0.14.0,divide by temperature
v0.14.0,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.14.0,this makes a vector in which the largest value is 0
v0.14.0,compute word weights with exponential function
v0.14.0,try sampling multinomial distribution for next character
v0.14.0,print(word_idx)
v0.14.0,input ids
v0.14.0,push list of character IDs through model
v0.14.0,the target is always the next character
v0.14.0,use cross entropy loss to compare output of forward pass with targets
v0.14.0,exponentiate cross-entropy loss to calculate perplexity
v0.14.0,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.14.0,serialize the language models and the constructor arguments (but nothing else)
v0.14.0,special handling for deserializing language models
v0.14.0,re-initialize language model with constructor arguments
v0.14.0,copy over state dictionary to self
v0.14.0,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.14.0,"in their ""self.train()"" method)"
v0.14.0,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.14.0,"check if this is the case and if so, set it"
v0.14.0,Transform input data into TARS format
v0.14.0,"if there are no labels, return a random sample as negatives"
v0.14.0,"otherwise, go through all labels"
v0.14.0,make sure the probabilities always sum up to 1
v0.14.0,get and embed all labels by making a Sentence object that contains only the label text
v0.14.0,get each label embedding and scale between 0 and 1
v0.14.0,compute similarity matrix
v0.14.0,"the higher the similarity, the greater the chance that a label is"
v0.14.0,sampled as negative example
v0.14.0,make label dictionary if no Dictionary object is passed
v0.14.0,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.14.0,check if candidate_label_set is empty
v0.14.0,make list if only one candidate label is passed
v0.14.0,create label dictionary
v0.14.0,note current task
v0.14.0,create a temporary task
v0.14.0,make zero shot predictions
v0.14.0,switch to the pre-existing task
v0.14.0,prepare TARS dictionary
v0.14.0,initialize a bare-bones sequence tagger
v0.14.0,transformer separator
v0.14.0,Store task specific labels since TARS can handle multiple tasks
v0.14.0,make a tars sentence where all labels are O by default
v0.14.0,init new TARS classifier
v0.14.0,set all task information
v0.14.0,progress bar for verbosity
v0.14.0,stop if all sentences are empty
v0.14.0,always remove tags first
v0.14.0,go through each sentence in the batch
v0.14.0,always remove tags first
v0.14.0,get the span and its label
v0.14.0,determine whether tokens in this span already have a label
v0.14.0,only add if all tokens have no label
v0.14.0,make and add a corresponding predicted span
v0.14.0,set indices so that no token can be tagged twice
v0.14.0,clearing token embeddings to save memory
v0.14.0,"all labels default to ""O"""
v0.14.0,set gold token-level
v0.14.0,set predicted token-level
v0.14.0,now print labels in CoNLL format
v0.14.0,prepare TARS dictionary
v0.14.0,initialize a bare-bones sequence tagger
v0.14.0,transformer separator
v0.14.0,Store task specific labels since TARS can handle multiple tasks
v0.14.0,get the serialized embeddings
v0.14.0,remap state dict for models serialized with Flair <= 0.11.3
v0.14.0,init new TARS classifier
v0.14.0,set all task information
v0.14.0,with torch.no_grad():
v0.14.0,progress bar for verbosity
v0.14.0,stop if all sentences are empty
v0.14.0,always remove tags first
v0.14.0,go through each sentence in the batch
v0.14.0,always remove tags first
v0.14.0,add all labels that according to TARS match the text and are above threshold
v0.14.0,do not add labels below confidence threshold
v0.14.0,only use label with the highest confidence if enforcing single-label predictions
v0.14.0,add the label with the highest score even if below the threshold if force label is activated.
v0.14.0,remove previously added labels and only add the best label
v0.14.0,clearing token embeddings to save memory
v0.14.0,set separator to concatenate three sentences
v0.14.0,auto-spawn on GPU if available
v0.14.0,set separator to concatenate two sentences
v0.14.0,auto-spawn on GPU if available
v0.14.0,"If the concatenated version of the text pair does not exist yet, create it"
v0.14.0,pooling operation to get embeddings for entites
v0.14.0,set embeddings
v0.14.0,set relation and entity label types
v0.14.0,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.14.0,filter entity pairs according to their tags if set
v0.14.0,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.14.0,character dictionary for decoding and encoding
v0.14.0,make sure <unk> is in dictionary for handling of unknown characters
v0.14.0,add special symbols to dictionary if necessary and save respective indices
v0.14.0,---- ENCODER ----
v0.14.0,encoder character embeddings
v0.14.0,encoder pre-trained embeddings
v0.14.0,encoder RNN
v0.14.0,additional encoder linear layer if bidirectional encoding
v0.14.0,---- DECODER ----
v0.14.0,decoder: linear layers to transform vectors to and from alphabet_size
v0.14.0,when using attention we concatenate attention outcome and decoder hidden states
v0.14.0,decoder RNN
v0.14.0,loss and softmax
v0.14.0,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.14.0,add additional columns for special symbols if necessary
v0.14.0,initialize with dummy symbols
v0.14.0,encode inputs
v0.14.0,get labels (we assume each token has a lemma label)
v0.14.0,get char indices for labels of sentence
v0.14.0,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.14.0,max_sequence_length = length of longest label of sentence + 1
v0.14.0,get char embeddings
v0.14.0,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.14.0,take decoder input and initial hidden and pass through RNN
v0.14.0,"if all encoder outputs are provided, use attention"
v0.14.0,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.14.0,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.14.0,get all tokens
v0.14.0,encode input characters by sending them through RNN
v0.14.0,get one-hots for characters and add special symbols / padding
v0.14.0,determine length of each token
v0.14.0,embed sentences
v0.14.0,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.14.0,variable to store initial hidden states for decoder
v0.14.0,encode input characters by sending them through RNN
v0.14.0,test packing and padding
v0.14.0,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.14.0,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.14.0,decoder later with self.emb_to_hidden
v0.14.0,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.14.0,use token embedding as initial hidden state for decoder
v0.14.0,concatenate everything together and project to appropriate size for decoder
v0.14.0,variable to store initial hidden states for decoder
v0.14.0,encode input characters by sending them through RNN
v0.14.0,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.14.0,embed character one-hots
v0.14.0,send through encoder RNN (produces initial hidden for decoder)
v0.14.0,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.14.0,project 2*hidden_size to hidden_size
v0.14.0,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.14.0,later with self.emb_to_hidden
v0.14.0,use token embedding as initial hidden state for decoder
v0.14.0,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.14.0,concatenate everything together and project to appropriate size for decoder
v0.14.0,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.14.0,"create target vector (batch_size, max_label_seq_length + 1)"
v0.14.0,filter empty sentences
v0.14.0,max length of the predicted sequences
v0.14.0,for printing
v0.14.0,stop if all sentences are empty
v0.14.0,remove previously predicted labels of this type
v0.14.0,create list of tokens in batch
v0.14.0,encode inputs
v0.14.0,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.14.0,sequence length is always set to one in prediction
v0.14.0,option 1: greedy decoding
v0.14.0,predictions
v0.14.0,decode next character
v0.14.0,pick top beam size many outputs with highest probabilities
v0.14.0,option 2: beam search
v0.14.0,out_probs = self.softmax(output_vectors).squeeze(1)
v0.14.0,make sure no dummy symbol <> or start symbol <S> is predicted
v0.14.0,pick top beam size many outputs with highest probabilities
v0.14.0,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.14.0,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.14.0,keep scores of beam_size many hypothesis for each token in the batch
v0.14.0,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.14.0,save sequences so far
v0.14.0,keep track of how many hypothesis were completed for each token
v0.14.0,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.14.0,decode with log softmax
v0.14.0,make sure no dummy symbol <> or start symbol <S> is predicted
v0.14.0,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.14.0,"if the sequence is already ended, do not record as candidate"
v0.14.0,index of token in in list tokens_in_batch
v0.14.0,print(token_number)
v0.14.0,hypothesis score
v0.14.0,TODO: remove token if number of completed hypothesis exceeds given value
v0.14.0,set score of corresponding entry to -inf so it will not be expanded
v0.14.0,get leading_indices for next expansion
v0.14.0,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.14.0,take beam_size many copies of scores vector and add scores of possible new extensions
v0.14.0,"size (beam_size*batch_size, beam_size)"
v0.14.0,print(hypothesis_scores)
v0.14.0,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.14.0,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.14.0,print(hypothesis_scores_per_token)
v0.14.0,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.14.0,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.14.0,a list of length beam_size*batch_size
v0.14.0,"where the first three inidices belong to the first token, the next three to the second token,"
v0.14.0,and so on
v0.14.0,with these indices we can compute the tensors for the next iteration
v0.14.0,expand sequences with corresponding index
v0.14.0,add log-probabilities to the scores
v0.14.0,save new leading indices
v0.14.0,save corresponding hidden states
v0.14.0,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.14.0,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.14.0,get best final hypothesis for each token
v0.14.0,get characters from index sequences and add predicted label to token
v0.14.0,"Overwrites evaluate of parent class to remove the ""by class"" printout"
v0.14.0,set separator to concatenate two sentences
v0.14.0,init dropouts
v0.14.0,auto-spawn on GPU if available
v0.14.0,make a forward pass to produce embedded data points and labels
v0.14.0,get their gold labels as a tensor
v0.14.0,pass data points through network to get encoded data point tensor
v0.14.0,decode
v0.14.0,calculate the loss
v0.14.0,get a tensor of data points
v0.14.0,do dropout
v0.14.0,"If the concatenated version of the text pair does not exist yet, create it"
v0.14.0,add DefaultClassifier arguments
v0.14.0,progress bar for verbosity
v0.14.0,stop if all sentences are empty
v0.14.0,clearing token embeddings to save memory
v0.14.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.14.0,"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
v0.14.0,fields in case this is a span-prediction problem
v0.14.0,the label type
v0.14.0,all parameters will be pushed internally to the specified device
v0.14.0,special handling during training if this is a span prediction problem
v0.14.0,internal variables
v0.14.0,non-set tags are OUT tags
v0.14.0,anything that is not OUT is IN
v0.14.0,does this prediction start a new span?
v0.14.0,B- and S- always start new spans
v0.14.0,"if the predicted class changes, I- starts a new span"
v0.14.0,"if the predicted class changes and S- was previous tag, start a new span"
v0.14.0,if an existing span is ended (either by reaching O or starting a new span)
v0.14.0,reset for-loop variables for new span
v0.14.0,remember previous tag
v0.14.0,"if there is a span at end of sentence, add it"
v0.14.0,"all labels default to ""O"""
v0.14.0,set gold token-level
v0.14.0,set predicted token-level
v0.14.0,now print labels in CoNLL format
v0.14.0,print labels in CoNLL format
v0.14.0,internal candidate lists of generator
v0.14.0,load Zelda candidates if so passed
v0.14.0,create candidate lists
v0.14.0,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.14.0,create a new dictionary for lower cased mentions
v0.14.0,go through each mention and its candidates
v0.14.0,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.14.0,set lowercased version as map
v0.14.0,"only use span label type if there are predictions, otherwise search for output label type (training labels)"
v0.14.0,remap state dict for models serialized with Flair <= 0.11.3
v0.14.0,get the candidates
v0.14.0,"during training, add the gold value as candidate"
v0.14.0,----- Create the internal tag dictionary -----
v0.14.0,span-labels need special encoding (BIO or BIOES)
v0.14.0,the big question is whether the label dictionary should contain an UNK or not
v0.14.0,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.14.0,"with UNK, the model learns less well if there are no UNK examples"
v0.14.0,is this a span prediction problem?
v0.14.0,----- Embeddings -----
v0.14.0,----- Initial loss weights parameters -----
v0.14.0,----- RNN specific parameters -----
v0.14.0,----- Conditional Random Field parameters -----
v0.14.0,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.14.0,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.14.0,----- Dropout parameters -----
v0.14.0,dropouts
v0.14.0,remove word dropout if there is no contact over the sequence dimension.
v0.14.0,----- Model layers -----
v0.14.0,----- RNN layer -----
v0.14.0,"If shared RNN provided, else create one for model"
v0.14.0,Whether to train initial hidden state
v0.14.0,final linear map to tag space
v0.14.0,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.14.0,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.14.0,"if there are no sentences, there is no loss"
v0.14.0,forward pass to get scores
v0.14.0,calculate loss given scores and labels
v0.14.0,make a zero-padded tensor for the whole sentence
v0.14.0,linear map to tag space
v0.14.0,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.14.0,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.14.0,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.14.0,spans need to be encoded as token-level predictions
v0.14.0,all others are regular labels for each token
v0.14.0,make sure it's a list
v0.14.0,filter empty sentences
v0.14.0,reverse sort all sequences by their length
v0.14.0,progress bar for verbosity
v0.14.0,stop if all sentences are empty
v0.14.0,get features from forward propagation
v0.14.0,remove previously predicted labels of this type
v0.14.0,"if return_loss, get loss value"
v0.14.0,make predictions
v0.14.0,add predictions to Sentence
v0.14.0,BIOES-labels need to be converted to spans
v0.14.0,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.14.0,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.14.0,core Flair models on Huggingface ModelHub
v0.14.0,"Large NER models,"
v0.14.0,Multilingual NER models
v0.14.0,English POS models
v0.14.0,Multilingual POS models
v0.14.0,English SRL models
v0.14.0,English chunking models
v0.14.0,Language-specific NER models
v0.14.0,Language-specific POS models
v0.14.0,Historic German
v0.14.0,English NER models
v0.14.0,English SRL models
v0.14.0,Danish models
v0.14.0,German models
v0.14.0,Arabic models
v0.14.0,French models
v0.14.0,Dutch models
v0.14.0,Malayalam models
v0.14.0,Portuguese models
v0.14.0,Biomedical models
v0.14.0,check if model name is a valid local file
v0.14.0,"check if model key is remapped to HF key - if so, print out information"
v0.14.0,get mapped name
v0.14.0,"if not, check if model key is remapped to direct download location. If so, download model"
v0.14.0,"for all other cases (not local file or special download location), use HF model hub"
v0.14.0,## Demo: How to use in Flair
v0.14.0,load tagger
v0.14.0,make example sentence
v0.14.0,predict NER tags
v0.14.0,print sentence
v0.14.0,print predicted NER spans
v0.14.0,iterate over entities and print
v0.14.0,Lazy import
v0.14.0,Save model weight
v0.14.0,Determine if model card already exists
v0.14.0,Generate and save model card
v0.14.0,Upload files
v0.14.0,"all labels default to ""O"""
v0.14.0,set gold token-level
v0.14.0,set predicted token-level
v0.14.0,now print labels in CoNLL format
v0.14.0,print labels in CoNLL format
v0.14.0,Dense + sparse retrieval
v0.14.0,fetched from original repo to avoid download
v0.14.0,"just in case we add: fuzzy search, Levenstein, ..."
v0.14.0,"for now we always fall back to SapBERT,"
v0.14.0,but we should train our own models at some point
v0.14.0,NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
v0.14.0,NOTE: Avoid emtpy string if mentions are just punctuations (e.g. `-` or `(`)
v0.14.0,Ab3P works on sentence-level and not on a single entity mention / name
v0.14.0,- so we just apply the wrapped text pre-processing here (if configured)
v0.14.0,NOTE: ensure correct similarity metric for pretrained model
v0.14.0,empty cuda cache if device is a cuda device
v0.14.0,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.14.0,NOTE: This is a hacky workaround for the fact that
v0.14.0,the `label_type`s in `Classifier.load('hunflair)` are
v0.14.0,"'diseases', 'genes', 'species', 'chemical' instead of 'ner'."
v0.14.0,We warn users once they need to update SequenceTagger model
v0.14.0,See: https://github.com/flairNLP/flair/pull/3387
v0.14.0,make sure sentences is a list of sentences
v0.14.0,Make sure entity label types are represented as dict
v0.14.0,Collect all entities based on entity type labels configuration
v0.14.0,Preprocess entity mentions
v0.14.0,Retrieve top-k concept / entity candidates
v0.14.0,Add a label annotation for each candidate
v0.14.0,load model by entity_type
v0.14.0,check if we have a hybrid pre-trained model
v0.14.0,the multi task model has several labels
v0.14.0,biomedical models
v0.14.0,entity linker
v0.14.0,auto-spawn on GPU if available
v0.14.0,remap state dict for models serialized with Flair <= 0.11.3
v0.14.0,English sentiment models
v0.14.0,Communicative Functions Model
v0.14.0,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.14.0,"may get distributed into different splits. For training purposes, this is always undesired."
v0.14.0,Prepend the task description prompt to the sentence text
v0.14.0,Make sure it's a list
v0.14.0,Reconstruct all annotations from the original sentence (necessary for learning classifiers)
v0.14.0,If all sentences are not augmented -> augment them
v0.14.0,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.14.0,"mypy does not infer the type of ""sentences"" restricted by code above"
v0.14.0,Compute prediction label type
v0.14.0,make sure it's a list
v0.14.0,"If all sentences are already augmented (i.e. compatible with this class), just forward the sentences"
v0.14.0,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.14.0,Remove existing labels
v0.14.0,Augment sentences - copy all annotation of the given tag type
v0.14.0,Predict on augmented sentence and store it in an internal annotation layer / label
v0.14.0,Append predicted labels to the original sentences
v0.14.0,check if model name is a valid local file
v0.14.0,check if model name is a pre-configured hf model
v0.14.0,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.14.0,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.14.0,"Initially, get scores from <start> tag to all other tags"
v0.14.0,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.14.0,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.14.0,Create a tensor to hold accumulated sequence scores at each current tag
v0.14.0,Create a tensor to hold back-pointers
v0.14.0,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.14.0,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.14.0,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.14.0,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.14.0,"If sentence is over, add transition to STOP-tag"
v0.14.0,Decode/trace best path backwards
v0.14.0,Sanity check
v0.14.0,remove start-tag and backscore to stop-tag
v0.14.0,Max + Softmax to get confidence score for predicted label and append label to each token
v0.14.0,"Transitions are used in the following way: transitions[to, from]."
v0.14.0,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.14.0,to START-tag and from STOP-tag to any other tag to -10000.
v0.14.0,"if necessary, make batch_steps"
v0.14.0,break up the batch into slices of size
v0.14.0,mini_batch_chunk_size
v0.14.0,"if training also uses dev/train data, include in training set"
v0.14.0,evaluation and monitoring
v0.14.0,sampling and shuffling
v0.14.0,evaluation and monitoring
v0.14.0,when and what to save
v0.14.0,logging parameters
v0.14.0,plugins
v0.14.0,activate annealing plugin
v0.14.0,call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
v0.14.0,training parameters
v0.14.0,evaluation and monitoring
v0.14.0,sampling and shuffling
v0.14.0,evaluation and monitoring
v0.14.0,when and what to save
v0.14.0,logging parameters
v0.14.0,amp
v0.14.0,plugins
v0.14.0,annealing logic
v0.14.0,training parameters
v0.14.0,evaluation and monitoring
v0.14.0,sampling and shuffling
v0.14.0,evaluation and monitoring
v0.14.0,when and what to save
v0.14.0,logging parameters
v0.14.0,amp
v0.14.0,plugins
v0.14.0,training parameters
v0.14.0,evaluation and monitoring
v0.14.0,sampling and shuffling
v0.14.0,evaluation and monitoring
v0.14.0,when and what to save
v0.14.0,logging parameters
v0.14.0,amp
v0.14.0,plugins
v0.14.0,Create output folder
v0.14.0,=== START BLOCK: ACTIVATE PLUGINS === #
v0.14.0,We first activate all optional plugins. These take care of optional functionality such as various
v0.14.0,logging techniques and checkpointing
v0.14.0,log file plugin
v0.14.0,loss file plugin
v0.14.0,plugin for writing weights
v0.14.0,plugin for checkpointing
v0.14.0,=== END BLOCK: ACTIVATE PLUGINS === #
v0.14.0,derive parameters the function was called with (or defaults)
v0.14.0,initialize model card with these parameters
v0.14.0,Prepare training data and get dataset size
v0.14.0,"determine what splits (train, dev, test) to evaluate"
v0.14.0,determine how to determine best model and whether to save it
v0.14.0,instantiate the optimizer
v0.14.0,initialize sampler if provided
v0.14.0,init with default values if only class is provided
v0.14.0,set dataset to sample from
v0.14.0,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.14.0,Sanity checks
v0.14.0,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.14.0,-- AmpPlugin -> wraps with AMP
v0.14.0,-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
v0.14.0,At any point you can hit Ctrl + C to break out of training early.
v0.14.0,"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
v0.14.0,- LossFilePlugin -> get the current epoch for loss file logging
v0.14.0,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.14.0,log infos on training progress every `log_modulo` batches
v0.14.0,process mini-batches
v0.14.0,zero the gradients on the model and optimizer
v0.14.0,forward and backward for batch
v0.14.0,forward pass
v0.14.0,identify dynamic embeddings (always deleted) on first sentence
v0.14.0,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.14.0,do the optimizer step
v0.14.0,- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
v0.14.0,- WeightExtractorPlugin -> extracts weights
v0.14.0,- CheckpointPlugin -> executes save_model_each_k_epochs
v0.14.0,- SchedulerPlugin -> log bad epochs
v0.14.0,Determine if this is the best model or if we need to anneal
v0.14.0,log results
v0.14.0,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.14.0,use DEV split to determine if this is the best model so far
v0.14.0,"if not using DEV score, determine best model using train loss"
v0.14.0,- LossFilePlugin -> somehow prints all relevant metrics
v0.14.0,- AnnealPlugin -> scheduler step
v0.14.0,- SWAPlugin -> restores SGD weights from SWA
v0.14.0,"if we do not use dev data for model selection, save final model"
v0.14.0,TensorboardLogger -> closes writer
v0.14.0,test best model if test data is present
v0.14.0,get and return the final test score of best model
v0.14.0,MetricHistoryPlugin -> stores the loss history in return_values
v0.14.0,"Store return values, as they will be erased by reset_training_attributes"
v0.14.0,get a random sample of training sentences
v0.14.0,create a model card for this model with Flair and PyTorch version
v0.14.0,record Transformers version if library is loaded
v0.14.0,remember all parameters used in train() call
v0.14.0,"TextDataset returns a list. valid and test are only one file,"
v0.14.0,so return the first element
v0.14.0,cast string to Path
v0.14.0,error message if the validation dataset is too small
v0.14.0,Shuffle training files randomly after serially iterating
v0.14.0,through corpus one
v0.14.0,"iterate through training data, starting at"
v0.14.0,self.split (for checkpointing)
v0.14.0,off by one for printing
v0.14.0,go into train mode
v0.14.0,reset variables
v0.14.0,not really sure what this does
v0.14.0,do the forward pass in the model
v0.14.0,try to predict the targets
v0.14.0,Backward
v0.14.0,`clip_grad_norm` helps prevent the exploding gradient
v0.14.0,problem in RNNs / LSTMs.
v0.14.0,We detach the hidden state from how it was
v0.14.0,previously produced.
v0.14.0,"If we didn't, the model would try backpropagating"
v0.14.0,all the way to start of the dataset.
v0.14.0,explicitly remove loss to clear up memory
v0.14.0,#########################################################
v0.14.0,Save the model if the validation loss is the best we've
v0.14.0,seen so far.
v0.14.0,#########################################################
v0.14.0,print info
v0.14.0,#########################################################
v0.14.0,##############################################################################
v0.14.0,final testing
v0.14.0,##############################################################################
v0.14.0,Turn on evaluation mode which disables dropout.
v0.14.0,Work out how cleanly we can divide the dataset into bsz parts.
v0.14.0,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.14.0,Evenly divide the data across the bsz batches.
v0.14.0,"no need to check for MetricName, as __add__ of other would be called in this case"
v0.14.0,"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
v0.14.0,instantiate plugin
v0.14.0,"Reset the flag, since an exception event might be dispatched"
v0.14.0,"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
v0.14.0,the callback
v0.14.0,go through all attributes
v0.14.0,get attribute hook events (may raise an AttributeError)
v0.14.0,register function as a hook
v0.14.0,"Decorator was used with parentheses, but no args"
v0.14.0,Decorator was used with args (strings specifiying the events)
v0.14.0,Decorator was used without args
v0.14.0,path to store the model
v0.14.0,special annealing modes
v0.14.0,determine the min learning rate
v0.14.0,"minimize training loss if training with dev data, else maximize dev score"
v0.14.0,instantiate the scheduler
v0.14.0,stop training if learning rate becomes too small
v0.14.0,reload last best model if annealing with restarts is enabled
v0.14.0,calculate warmup steps
v0.14.0,skip if no optimization has happened.
v0.14.0,saves the model with full vocab as checkpoints etc were created with reduced vocab.
v0.14.0,TODO: check if metric is in tracked metrics
v0.14.0,prepare loss logging file and set up header
v0.14.0,set up all metrics to collect
v0.14.0,set up headers
v0.14.0,name: HEADER
v0.14.0,Add all potentially relevant metrics. If a metric is not published
v0.14.0,"after the first epoch (when the header is written), the column is"
v0.14.0,removed at that point.
v0.14.0,initialize the first log line
v0.14.0,record is a list of scalars
v0.14.0,output log file
v0.14.0,remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
v0.14.0,make headers on epoch 1
v0.14.0,write header
v0.14.0,adjust alert level
v0.14.0,"the default model for ELMo is the 'original' model, which is very large"
v0.14.0,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.14.0,put on Cuda if available
v0.14.0,embed a dummy sentence to determine embedding_length
v0.14.0,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.14.0,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.14.0,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.14.0,news-english-forward
v0.14.0,news-english-backward
v0.14.0,news-english-forward
v0.14.0,news-english-backward
v0.14.0,mix-english-forward
v0.14.0,mix-english-backward
v0.14.0,mix-german-forward
v0.14.0,mix-german-backward
v0.14.0,common crawl Polish forward
v0.14.0,common crawl Polish backward
v0.14.0,Slovenian forward
v0.14.0,Slovenian backward
v0.14.0,Bulgarian forward
v0.14.0,Bulgarian backward
v0.14.0,Dutch forward
v0.14.0,Dutch backward
v0.14.0,Swedish forward
v0.14.0,Swedish backward
v0.14.0,French forward
v0.14.0,French backward
v0.14.0,Czech forward
v0.14.0,Czech backward
v0.14.0,Portuguese forward
v0.14.0,Portuguese backward
v0.14.0,initialize cache if use_cache set
v0.14.0,embed a dummy sentence to determine embedding_length
v0.14.0,set to eval mode
v0.14.0,Copy the object's state from self.__dict__ which contains
v0.14.0,all our instance attributes. Always use the dict.copy()
v0.14.0,method to avoid modifying the original state.
v0.14.0,Remove the unpicklable entries.
v0.14.0,"if cache is used, try setting embeddings from cache first"
v0.14.0,try populating embeddings from cache
v0.14.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.14.0,get hidden states from language model
v0.14.0,take first or last hidden states from language model as word representation
v0.14.0,if self.tokenized_lm or token.whitespace_after:
v0.14.0,"if only one sentence is passed, convert to list of sentence"
v0.14.0,bidirectional LSTM on top of embedding layer
v0.14.0,dropouts
v0.14.0,"first, sort sentences by number of tokens"
v0.14.0,go through each sentence in batch
v0.14.0,PADDING: pad shorter sentences out
v0.14.0,ADD TO SENTENCE LIST: add the representation
v0.14.0,--------------------------------------------------------------------
v0.14.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.14.0,--------------------------------------------------------------------
v0.14.0,--------------------------------------------------------------------
v0.14.0,FF PART
v0.14.0,--------------------------------------------------------------------
v0.14.0,use word dropout if set
v0.14.0,--------------------------------------------------------------------
v0.14.0,EXTRACT EMBEDDINGS FROM LSTM
v0.14.0,--------------------------------------------------------------------
v0.14.0,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.14.0,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.14.0,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.14.0,add positional encodings
v0.14.0,reshape the pixels into the sequence
v0.14.0,layer norm after convolution and positional encodings
v0.14.0,add <cls> token
v0.14.0,"transformer requires input in the shape [h*w+1, b, d]"
v0.14.0,the output is an embedding of <cls> token
v0.14.0,this parameter is fixed
v0.14.0,optional fine-tuning on top of embedding layer
v0.14.0,"if only one sentence is passed, convert to list of sentence"
v0.14.0,"if only one sentence is passed, convert to list of sentence"
v0.14.0,bidirectional RNN on top of embedding layer
v0.14.0,dropouts
v0.14.0,TODO: remove in future versions
v0.14.0,embed words in the sentence
v0.14.0,before-RNN dropout
v0.14.0,reproject if set
v0.14.0,push through RNN
v0.14.0,after-RNN dropout
v0.14.0,extract embeddings from RNN
v0.14.0,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.14.0,"check if this is the case and if so, set it"
v0.14.0,serialize the language models and the constructor arguments (but nothing else)
v0.14.0,re-initialize language model with constructor arguments
v0.14.0,special handling for deserializing language models
v0.14.0,copy over state dictionary to self
v0.14.0,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.14.0,"in their ""self.train()"" method)"
v0.14.0,IMPORTANT: add embeddings as torch modules
v0.14.0,iterate over sentences
v0.14.0,"if its a forward LM, take last state"
v0.14.0,"convert to plain strings, embedded in a list for the encode function"
v0.14.0,CNN
v0.14.0,dropouts
v0.14.0,TODO: remove in future versions
v0.14.0,embed words in the sentence
v0.14.0,before-RNN dropout
v0.14.0,reproject if set
v0.14.0,push CNN
v0.14.0,after-CNN dropout
v0.14.0,extract embeddings from CNN
v0.14.0,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.14.0,"if only one sentence is passed, convert to list of sentence"
v0.14.0,Expose base classses
v0.14.0,Expose document embedding classes
v0.14.0,Expose image embedding classes
v0.14.0,Expose legacy embedding classes
v0.14.0,Expose token embedding classes
v0.14.0,in some cases we need to insert zero vectors for tokens without embedding.
v0.14.0,padding
v0.14.0,remove special markup
v0.14.0,check if special tokens exist to circumvent error message
v0.14.0,iterate over subtokens and reconstruct tokens
v0.14.0,remove special markup
v0.14.0,check if reconstructed token is special begin token ([CLS] or similar)
v0.14.0,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.14.0,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.14.0,if tokens are unaccounted for
v0.14.0,check if all tokens were matched to subtokens
v0.14.0,The layoutlm tokenizer doesn't handle ocr themselves
v0.14.0,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.14.0,"cannot run `.encode` if ocr boxes are required, assume"
v0.14.0,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.14.0,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.14.0,in case of doubt: token embedding has higher priority than document embedding
v0.14.0,random check some tokens to save performance.
v0.14.0,Models such as FNet do not have an attention_mask
v0.14.0,set language IDs for XLM-style transformers
v0.14.0,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.14.0,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.14.0,set context if not set already
v0.14.0,flair specific pre-tokenization
v0.14.0,fields to store left and right context
v0.14.0,expand context only if context_length is set
v0.14.0,"if context_dropout is set, randomly deactivate left context during training"
v0.14.0,"if context_dropout is set, randomly deactivate right context during training"
v0.14.0,"if use_context_separator is set, add a [FLERT] token"
v0.14.0,return expanded sentence and context length information
v0.14.0,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.14.0,temporary fix to disable tokenizer parallelism warning
v0.14.0,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.14.0,do not print transformer warnings as these are confusing in this case
v0.14.0,load tokenizer and transformer model
v0.14.0,load tokenizer from inmemory zip-file
v0.14.0,if model is quantized by BitsAndBytes this will fail
v0.14.0,add adapters for finetuning
v0.14.0,peft_config: PeftConfig
v0.14.0,model name
v0.14.0,embedding parameters
v0.14.0,send mini-token through to check how many layers the model has
v0.14.0,return length
v0.14.0,"If we use a context separator, add a new special token"
v0.14.0,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.14.0,"when initializing, embeddings are in eval mode by default"
v0.14.0,in case of doubt: token embedding has higher priority than document embedding
v0.14.0,in case of doubt: token embedding has higher priority than document embedding
v0.14.0,legacy TransformerDocumentEmbedding
v0.14.0,legacy TransformerTokenEmbedding
v0.14.0,legacy Flair <= 0.12
v0.14.0,legacy Flair <= 0.7
v0.14.0,legacy TransformerTokenEmbedding
v0.14.0,Legacy TransformerDocumentEmbedding
v0.14.0,legacy TransformerTokenEmbedding
v0.14.0,legacy TransformerDocumentEmbedding
v0.14.0,some models like the tars model somehow lost this information.
v0.14.0,copy values from new embedding
v0.14.0,those parameters are only from the super class and will be recreated in the constructor.
v0.14.0,cls first pooling can be done without recreating sentence hidden states
v0.14.0,make the tuple a tensor; makes working with it easier.
v0.14.0,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.14.0,only use layers that will be outputted
v0.14.0,this parameter is fixed
v0.14.0,IMPORTANT: add embeddings as torch modules
v0.14.0,"if only one sentence is passed, convert to list of sentence"
v0.14.0,make compatible with serialized models
v0.14.0,gensim version 4
v0.14.0,gensim version 3
v0.14.0,"if no embedding is set, the vocab and embedding length is required"
v0.14.0,GLOVE embeddings
v0.14.0,TURIAN embeddings
v0.14.0,KOMNINOS embeddings
v0.14.0,pubmed embeddings
v0.14.0,FT-CRAWL embeddings
v0.14.0,FT-CRAWL embeddings
v0.14.0,twitter embeddings
v0.14.0,two-letter language code wiki embeddings
v0.14.0,two-letter language code wiki embeddings
v0.14.0,two-letter language code crawl embeddings
v0.14.0,"this is required to force the module on the cpu,"
v0.14.0,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.14.0,self.to(..) actually sets the device properly
v0.14.0,this ignores the get_cached_vec method when loading older versions
v0.14.0,it is needed for compatibility reasons
v0.14.0,gensim version 4
v0.14.0,gensim version 3
v0.14.0,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.14.0,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.14.0,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.14.0,not finding them)
v0.14.0,use list of common characters if none provided
v0.14.0,translate words in sentence into ints using dictionary
v0.14.0,"sort words by length, for batching and masking"
v0.14.0,chars for rnn processing
v0.14.0,multilingual models
v0.14.0,English models
v0.14.0,Arabic
v0.14.0,Bulgarian
v0.14.0,Czech
v0.14.0,Danish
v0.14.0,German
v0.14.0,Spanish
v0.14.0,Basque
v0.14.0,Persian
v0.14.0,Finnish
v0.14.0,French
v0.14.0,Hebrew
v0.14.0,Hindi
v0.14.0,Croatian
v0.14.0,Indonesian
v0.14.0,Italian
v0.14.0,Japanese
v0.14.0,Malayalam
v0.14.0,Dutch
v0.14.0,Norwegian
v0.14.0,Polish
v0.14.0,Portuguese
v0.14.0,Pubmed
v0.14.0,Slovenian
v0.14.0,Swedish
v0.14.0,Tamil
v0.14.0,Spanish clinical
v0.14.0,CLEF HIPE Shared task
v0.14.0,Amharic
v0.14.0,Ukrainian
v0.14.0,load model if in pretrained model map
v0.14.0,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.14.0,CLEF HIPE models are lowercased
v0.14.0,embeddings are static if we don't do finetuning
v0.14.0,embed a dummy sentence to determine embedding_length
v0.14.0,set to eval mode
v0.14.0,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.14.0,gradients are enable if fine-tuning is enabled
v0.14.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.14.0,get hidden states from language model
v0.14.0,take first or last hidden states from language model as word representation
v0.14.0,offset mode that extracts at whitespace after last character
v0.14.0,offset mode that extracts at last character
v0.14.0,make compatible with old models
v0.14.0,use the character language model embeddings as basis
v0.14.0,length is twice the original character LM embedding length
v0.14.0,these fields are for the embedding memory
v0.14.0,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.14.0,we re-compute embeddings dynamically at each epoch
v0.14.0,set the memory method
v0.14.0,memory is wiped each time we do a training run
v0.14.0,"if we keep a pooling, it needs to be updated continuously"
v0.14.0,update embedding
v0.14.0,check token.text is empty or not
v0.14.0,set aggregation operation
v0.14.0,add embeddings after updating
v0.14.0,model architecture
v0.14.0,model architecture
v0.14.0,"""pl"","
v0.14.0,download if necessary
v0.14.0,load the model
v0.14.0,"this is required to force the module on the cpu,"
v0.14.0,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.14.0,self.to(..) actually sets the device properly
v0.14.0,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.14.0,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.14.0,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.14.0,not finding them)
v0.14.0,old embeddings do not have a torch-embedding and therefore do not store the weights in the saved torch state_dict
v0.14.0,"however they are already initialized rightfully, so we just set the state dict from our current state dict"
v0.14.0,GLOVE embeddings
v0.14.0,no need to recreate as NILCEmbeddings
v0.14.0,read in test file if exists
v0.14.0,read in dev file if exists
v0.14.0,"find train, dev and test files if not specified"
v0.14.0,Add tags for each annotated span
v0.14.0,Remove leading and trailing whitespaces from annotated spans
v0.14.0,Search start and end token index for current span
v0.14.0,If end index is not found set to last token
v0.14.0,Throw error if indices are not valid
v0.14.0,Add metadatas for sentence
v0.14.0,Currently all Jsonl Datasets are stored in Memory
v0.14.0,get train data
v0.14.0,read in test file if exists
v0.14.0,read in dev file if exists
v0.14.0,"find train, dev and test files if not specified"
v0.14.0,special key for space after
v0.14.0,special key for feature columns
v0.14.0,special key for dependency head id
v0.14.0,"store either Sentence objects in memory, or only file offsets"
v0.14.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.14.0,determine encoding of text file
v0.14.0,identify which columns are spans and which are word-level
v0.14.0,now load all sentences
v0.14.0,skip first line if to selected
v0.14.0,option 1: keep Sentence objects in memory
v0.14.0,pointer to previous
v0.14.0,parse next sentence
v0.14.0,quit if last sentence reached
v0.14.0,skip banned sentences
v0.14.0,set previous and next sentence for context
v0.14.0,append parsed sentence to list in memory
v0.14.0,option 2: keep source data in memory
v0.14.0,"read lines for next sentence, but don't parse"
v0.14.0,quit if last sentence reached
v0.14.0,append raw lines for each sentence
v0.14.0,we make a distinction between word-level tags and span-level tags
v0.14.0,read first sentence to determine which columns are span-labels
v0.14.0,skip first line if to selected
v0.14.0,check the first 5 sentences
v0.14.0,go through all annotations and identify word- and span-level annotations
v0.14.0,- if a column has at least one BIES we know it's a Span label
v0.14.0,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.14.0,- problem cases are columns for which we see only O - in this case we default to Span
v0.14.0,skip assigned columns
v0.14.0,the space after key is always word-levels
v0.14.0,"if at least one token has a BIES, we know it's a span label"
v0.14.0,"if at least one token has a label other than BIOES, we know it's a token label"
v0.14.0,all remaining columns that are not word-level are span-level
v0.14.0,for column in self.word_level_tag_columns:
v0.14.0,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.14.0,"if sentence ends, break"
v0.14.0,parse comments if possible
v0.14.0,"otherwise, this line is a token. parse and add to sentence"
v0.14.0,check if this sentence is a document boundary
v0.14.0,add span labels
v0.14.0,discard tags from tokens that are not added to the sentence
v0.14.0,parse relations if they are set
v0.14.0,head and tail span indices are 1-indexed and end index is inclusive
v0.14.0,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.14.0,"to set the metadata ""domain"" to ""de-orcas"""
v0.14.0,get fields from line
v0.14.0,get head_id if exists (only in dependency parses)
v0.14.0,initialize token
v0.14.0,go through all columns
v0.14.0,'feats' and 'misc' column should be split into different fields
v0.14.0,special handling for whitespace after
v0.14.0,add each other feature as label-value pair
v0.14.0,get the task name (e.g. 'ner')
v0.14.0,get the label value
v0.14.0,add label
v0.14.0,remap regular tag names
v0.14.0,"if in memory, retrieve parsed sentence"
v0.14.0,else skip to position in file where sentence begins
v0.14.0,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.14.0,use all domains
v0.14.0,iter over all domains / sources and create target files
v0.14.0,The conll representation of coref spans allows spans to
v0.14.0,"overlap. If spans end or begin at the same word, they are"
v0.14.0,"separated by a ""|""."
v0.14.0,The span begins at this word.
v0.14.0,The span begins and ends at this word (single word span).
v0.14.0,"The span is starting, so we record the index of the word."
v0.14.0,"The span for this id is ending, but didn't start at this word."
v0.14.0,Retrieve the start index from the document state and
v0.14.0,add the span to the clusters for this id.
v0.14.0,strip all bracketing information to
v0.14.0,get the actual propbank label.
v0.14.0,Entering into a span for a particular semantic role label.
v0.14.0,We append the label and set the current span for this annotation.
v0.14.0,"If there's no '(' token, but the current_span_label is not None,"
v0.14.0,then we are inside a span.
v0.14.0,We're outside a span.
v0.14.0,"Exiting a span, so we reset the current span label for this annotation."
v0.14.0,The words in the sentence.
v0.14.0,The pos tags of the words in the sentence.
v0.14.0,the pieces of the parse tree.
v0.14.0,The lemmatised form of the words in the sentence which
v0.14.0,have SRL or word sense information.
v0.14.0,The FrameNet ID of the predicate.
v0.14.0,"The sense of the word, if available."
v0.14.0,"The current speaker, if available."
v0.14.0,"Cluster id -> List of (start_index, end_index) spans."
v0.14.0,Cluster id -> List of start_indices which are open for this id.
v0.14.0,Replace brackets in text and pos tags
v0.14.0,with a different token for parse trees.
v0.14.0,only keep ')' if there are nested brackets with nothing in them.
v0.14.0,There are some bad annotations in the CONLL data.
v0.14.0,"They contain no information, so to make this explicit,"
v0.14.0,we just set the parse piece to be None which will result
v0.14.0,in the overall parse tree being None.
v0.14.0,"If this is the first word in the sentence, create"
v0.14.0,empty lists to collect the NER and SRL BIO labels.
v0.14.0,"We can't do this upfront, because we don't know how many"
v0.14.0,"components we are collecting, as a sentence can have"
v0.14.0,variable numbers of SRL frames.
v0.14.0,Create variables representing the current label for each label
v0.14.0,sequence we are collecting.
v0.14.0,"If any annotation marks this word as a verb predicate,"
v0.14.0,we need to record its index. This also has the side effect
v0.14.0,of ordering the verbal predicates by their location in the
v0.14.0,"sentence, automatically aligning them with the annotations."
v0.14.0,"this would not be reached if parse_pieces contained None, hence the cast"
v0.14.0,Non-empty line. Collect the annotation.
v0.14.0,Collect any stragglers or files which might not
v0.14.0,have the '#end document' format for the end of the file.
v0.14.0,this dataset name
v0.14.0,check if data there
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,check if data there
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,download files if not present locally
v0.14.0,we need to slightly modify the original files by adding some new lines after document separators
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,Remove CoNLL-U meta information in the last column
v0.14.0,column format
v0.14.0,dataset name
v0.14.0,data folder: default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,dataset name
v0.14.0,data folder: default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,entity_mapping
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,data validation
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download files if not present locallys
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,# download zip
v0.14.0,merge the files in one as the zip is containing multiples files
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,check if data there
v0.14.0,create folder
v0.14.0,download dataset
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download and parse data if necessary
v0.14.0,create train test dev if not exist
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,If the extracted corpus file is not yet present in dir
v0.14.0,download zip if necessary
v0.14.0,"extracted corpus is not present , so unpacking it."
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download zip
v0.14.0,unpacking the zip
v0.14.0,merge the files in one as the zip is containing multiples files
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.14.0,download files if not present locally
v0.14.0,we need to modify the original files by adding new lines after after the end of each sentence
v0.14.0,if only one language is given
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,"use all languages if explicitly set to ""all"""
v0.14.0,download data if necessary
v0.14.0,initialize comlumncorpus and add it to list
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,"For each language in languages, the file is downloaded if not existent"
v0.14.0,Then a comlumncorpus of that data is created and saved in a list
v0.14.0,this list is handed to the multicorpus
v0.14.0,list that contains the columncopora
v0.14.0,download data if necessary
v0.14.0,"if language not downloaded yet, download it"
v0.14.0,create folder
v0.14.0,get google drive id from list
v0.14.0,download from google drive
v0.14.0,unzip
v0.14.0,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.14.0,transform data into required format
v0.14.0,"the processed dataset has the additional ending ""_new"""
v0.14.0,remove the unprocessed dataset
v0.14.0,initialize comlumncorpus and add it to list
v0.14.0,if no languages are given as argument all languages used in XTREME will be loaded
v0.14.0,if only one language is given
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,"For each language in languages, the file is downloaded if not existent"
v0.14.0,Then a comlumncorpus of that data is created and saved in a list
v0.14.0,This list is handed to the multicorpus
v0.14.0,list that contains the columncopora
v0.14.0,download data if necessary
v0.14.0,"if language not downloaded yet, download it"
v0.14.0,create folder
v0.14.0,download from HU Server
v0.14.0,unzip
v0.14.0,transform data into required format
v0.14.0,initialize comlumncorpus and add it to list
v0.14.0,if only one language is given
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,initialize comlumncorpus and add it to list
v0.14.0,download data if necessary
v0.14.0,unpack and write out in CoNLL column-like format
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,data is not in IOB2 format. Thus we transform it to IOB2
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,rename according to train - test - dev - convention
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,Add missing newline after header
v0.14.0,Workaround for empty tokens
v0.14.0,"Add ""real"" document marker"
v0.14.0,Dataset split mapping
v0.14.0,v2.0 only adds new language and splits for AJMC dataset
v0.14.0,Special document marker for sample splits in AJMC dataset
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,if only one language is given
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,"use all languages if explicitly set to ""all"""
v0.14.0,download data if necessary
v0.14.0,initialize comlumncorpus and add it to list
v0.14.0,this dataset name
v0.14.0,one name can map to multiple concepts
v0.14.0,NOTE: EntityLinkingDictionary are lazy-loaded from a preprocessed file.
v0.14.0,Use this class to load into memory all candidates
v0.14.0,"if identifier == ""MESH:D013749"":"
v0.14.0,# This MeSH ID was used by MeSH when this chemical was part of the MeSH controlled vocabulary.
v0.14.0,continue
v0.14.0,parse line
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download and parse data if necessary
v0.14.0,paths to train and test splits
v0.14.0,init corpus
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download and parse data if necessary
v0.14.0,iterate over all html files
v0.14.0,"get rid of html syntax, we only need the text"
v0.14.0,between all documents we write a separator symbol
v0.14.0,skip empty strings
v0.14.0,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.14.0,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.14.0,sentence splitting and tokenization
v0.14.0,iterate through all annotations and add to corresponding tokens
v0.14.0,find sentence to which annotation belongs
v0.14.0,position within corresponding sentence
v0.14.0,set annotation for tokens of entity mention
v0.14.0,write to out-file in column format
v0.14.0,"in case something goes wrong, delete the dataset and raise error"
v0.14.0,this dataset name
v0.14.0,download and parse data if necessary
v0.14.0,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.14.0,generate qid wikiname dictionaries
v0.14.0,merge dictionaries
v0.14.0,ignore first line
v0.14.0,commented and empty lines
v0.14.0,read all Q-IDs
v0.14.0,ignore first line
v0.14.0,request
v0.14.0,this dataset name
v0.14.0,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.14.0,like this we can quickly check if the corresponding page exists
v0.14.0,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.14.0,delete unprocessed file
v0.14.0,collect all wikiids
v0.14.0,create the dictionary
v0.14.0,request
v0.14.0,this dataset name
v0.14.0,names of raw text documents
v0.14.0,open output_file
v0.14.0,iterate through all documents
v0.14.0,split sentences and tokenize
v0.14.0,iterate through all annotations and add to corresponding tokens
v0.14.0,find sentence to which annotation belongs
v0.14.0,position within corresponding sentence
v0.14.0,set annotation for tokens of entity mention
v0.14.0,write to out file
v0.14.0,annotation from one annotator or two agreeing annotators
v0.14.0,this dataset name
v0.14.0,download and parse data if necessary
v0.14.0,this dataset name
v0.14.0,download and parse data if necessary
v0.14.0,First parse the post titles
v0.14.0,Keep track of how many and which entity mentions does a given post title have
v0.14.0,Check if the current post title has an entity link and parse accordingly
v0.14.0,Post titles with entity mentions (if any) are handled via this function
v0.14.0,Then parse the comments
v0.14.0,"Iterate over the comments.tsv file, until the end is reached"
v0.14.0,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.14.0,Each comment thread is handled as one 'document'.
v0.14.0,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.14.0,This if-condition is needed to handle this problem.
v0.14.0,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.14.0,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.14.0,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.14.0,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.14.0,and not just single letters into single rows.
v0.14.0,If there are annotated entity mentions for given post title or a comment thread
v0.14.0,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.14.0,Write the token with a corresponding tag to file
v0.14.0,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.14.0,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.14.0,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.14.0,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.14.0,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.14.0,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.14.0,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.14.0,Check if further annotations belong to the current post title or comment thread as well
v0.14.0,Stop when the end of an annotation file is reached
v0.14.0,Check if further annotations belong to the current sentence as well
v0.14.0,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.14.0,Docstart
v0.14.0,if there is more than one word in the chunk we write each in a separate line
v0.14.0,print(chunks)
v0.14.0,empty line after each sentence
v0.14.0,convert the file to CoNLL
v0.14.0,this dataset name
v0.14.0,"check if data there, if not, download the data"
v0.14.0,create folder
v0.14.0,download data
v0.14.0,transform data into column format if necessary
v0.14.0,if no filenames are specified we use all the data
v0.14.0,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.14.0,also we remove 'raganato_ALL' from filenames in case its in the list
v0.14.0,generate the test file
v0.14.0,make column file and save to data_folder
v0.14.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.14.0,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.14.0,create folder
v0.14.0,download data
v0.14.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.14.0,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.14.0,create folder
v0.14.0,download data
v0.14.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.14.0,generate the test file
v0.14.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.14.0,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.14.0,create folder
v0.14.0,download data
v0.14.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.14.0,generate the test file
v0.14.0,default dataset folder is the cache root
v0.14.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.14.0,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.14.0,create folder
v0.14.0,download data
v0.14.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.14.0,generate the test file
v0.14.0,default dataset folder is the cache root
v0.14.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.14.0,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.14.0,create folder
v0.14.0,download data
v0.14.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.14.0,generate the test file
v0.14.0,default dataset folder is the cache root
v0.14.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.14.0,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.14.0,create folder
v0.14.0,download data
v0.14.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.14.0,generate the test file
v0.14.0,TODO: Adapt this following: https://github.com/flairNLP/flair/pull/3146
v0.14.0,+1 assumes the title and abstract will be joined by a space.
v0.14.0,"We need a unique identifier for this entity, so build it from the document id and entity id"
v0.14.0,The user can provide a callable that returns the database name.
v0.14.0,some entities are not linked and
v0.14.0,some entities are linked to multiple normalized ids
v0.14.0,passages must not overlap and spans must cover the entire document
v0.14.0,entities
v0.14.0,parse db ids
v0.14.0,Some of the entities have a off-by-one error. Correct these annotations!
v0.14.0,"passage offsets/lengths do not connect, recalculate them for this schema."
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,if True:
v0.14.0,write CoNLL-U Plus header
v0.14.0,"Some special cases (e.g., missing spaces before entity marker)"
v0.14.0,necessary if text should be whitespace tokenizeable
v0.14.0,Handle case where tail may occur before the head
v0.14.0,this dataset name
v0.14.0,write CoNLL-U Plus header
v0.14.0,this dataset name
v0.14.0,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.14.0,download data if necessary
v0.14.0,write CoNLL-U Plus header
v0.14.0,The span has ended.
v0.14.0,We are entering a new span; reset indices
v0.14.0,and active tag to new span.
v0.14.0,We're inside a span.
v0.14.0,Last token might have been a part of a valid span.
v0.14.0,this dataset name
v0.14.0,write CoNLL-U Plus header
v0.14.0,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.14.0,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.14.0,target_file_path = Path(data_folder) / target_filename
v0.14.0,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.14.0,# write CoNLL-U Plus header
v0.14.0,"target_file.write(""# global.columns = id form ner\n"")"
v0.14.0,for example in json.load(source_file):
v0.14.0,token_list = self._tacred_example_to_token_list(example)
v0.14.0,target_file.write(token_list.serialize())
v0.14.0,check if first tag row is already occupied
v0.14.0,"if first tag row is occupied, use second tag row"
v0.14.0,hardcoded mapping TODO: perhaps find nicer solution
v0.14.0,remap regular tag names
v0.14.0,else skip to position in file where sentence begins
v0.14.0,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.14.0,read in dev file if exists
v0.14.0,read in test file if exists
v0.14.0,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.14.0,"find train, dev and test files if not specified"
v0.14.0,use test_file to create test split if available
v0.14.0,use dev_file to create test split if available
v0.14.0,"if data point contains black-listed label, do not use"
v0.14.0,first check if valid sentence
v0.14.0,"if so, add to indices"
v0.14.0,"find train, dev and test files if not specified"
v0.14.0,variables
v0.14.0,different handling of in_memory data than streaming data
v0.14.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.14.0,test if format is OK
v0.14.0,test if at least one label given
v0.14.0,make sentence from text (and filter for length)
v0.14.0,"if a pair column is defined, make a sentence pair object"
v0.14.0,noinspection PyDefaultArgument
v0.14.0,dataset name includes the split size
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download each of the 28 splits
v0.14.0,create dataset directory if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,download data from same source as in huggingface's implementations
v0.14.0,read label order
v0.14.0,"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
v0.14.0,"Re-map to [0, 1, 2, 3]."
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,handle labels file
v0.14.0,handle data file
v0.14.0,Create flair compatible labels
v0.14.0,"by default, map point score to POSITIVE / NEGATIVE values"
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,create dataset directory if necessary
v0.14.0,create train.txt file from CSV
v0.14.0,create test.txt file from CSV
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,create dataset directory if necessary
v0.14.0,create train.txt file by iterating over pos and neg file
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,create dataset directory if necessary
v0.14.0,create train.txt file by iterating over pos and neg file
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,create dataset directory if necessary
v0.14.0,create train.txt file by iterating over pos and neg file
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,create dataset directory if necessary
v0.14.0,create train.txt file by iterating over pos and neg file
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,create train dev and test files in fasttext format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download senteval datasets if necessary und unzip
v0.14.0,convert to FastText format
v0.14.0,download data if necessary
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,move original .tsv files to another folder
v0.14.0,create train and dev splits in fasttext format
v0.14.0,create eval_dataset file with no labels
v0.14.0,download zip archive
v0.14.0,unpack file in datasets directory (zip archive contains a directory named SST-2)
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,download datasets if necessary
v0.14.0,create dataset directory if necessary
v0.14.0,create correctly formated txt files
v0.14.0,multiple labels are possible
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,Create flair compatible labels
v0.14.0,TREC-6 : NUM:dist -> __label__NUM
v0.14.0,TREC-50: NUM:dist -> __label__NUM:dist
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,Create flair compatible labels
v0.14.0,TREC-6 : NUM:dist -> __label__NUM
v0.14.0,TREC-50: NUM:dist -> __label__NUM:dist
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,create a separate directory for different tasks
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,check if dataset is supported
v0.14.0,set file names
v0.14.0,set file names
v0.14.0,download and unzip in file structure if necessary
v0.14.0,instantiate corpus
v0.14.0,"find train, dev and test files if not specified"
v0.14.0,"create DataPairDataset for train, test and dev file, if they are given"
v0.14.0,stop if file does not exist
v0.14.0,create a DataPair object from strings
v0.14.0,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.14.0,"find train, dev, and test files if not specified"
v0.14.0,"create DataTripleDataset for train, test, and dev files, if they are given"
v0.14.0,stop if the file does not exist
v0.14.0,create a DataTriple object from strings
v0.14.0,"if in_memory is True we return a DataTriple, otherwise we create one from the lists of strings"
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,"rename test file to eval_dataset, since it has no labels"
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.14.0,dev sets include 5 different annotations but we will only keep the gold label
v0.14.0,"rename test file to eval_dataset, since it has no labels"
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get test and dev sets
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,"rename test file to eval_dataset, since it has no labels"
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,"rename test file to eval_dataset, since it has no labels"
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,"rename test file to eval_dataset, since it has no labels"
v0.14.0,"if data is not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,"rename test file to eval_dataset, since it has no labels"
v0.14.0,"if data not downloaded yet, download it"
v0.14.0,get the zip file
v0.14.0,"the downloaded files have json format, we transform them to tsv"
v0.14.0,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.14.0,remove json file
v0.14.0,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.14.0,with sum of all entity lengths as secondary key
v0.14.0,calculate offset without current text
v0.14.0,because we stick all passages of a document together
v0.14.0,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.14.0,Try to fix incorrect annotations
v0.14.0,print(
v0.14.0,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.14.0,)
v0.14.0,Ignore empty lines or relation annotations
v0.14.0,FIX annotation of whitespaces (necessary for PDR)
v0.14.0,Add task description for multi-task learning
v0.14.0,One token may contain multiple entities -> deque all of them
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.14.0,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,Edge case: last token starts a new entity
v0.14.0,Last document in file
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,In the huner split files there is no information whether a given id originates
v0.14.0,from the train or test file of the original corpus - so we have to adapt corpus
v0.14.0,splitting here
v0.14.0,Edge case: last token starts a new entity
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,Read texts
v0.14.0,Read annotations
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,We need to apply a patch to correct the original training file
v0.14.0,Articles title
v0.14.0,Article abstract
v0.14.0,Entity annotations
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,Edge case: last token starts a new entity
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,Incomplete article
v0.14.0,Invalid XML syntax
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,if len(mid) != 3:
v0.14.0,continue
v0.14.0,Try to fix entity offsets
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,There is still one illegal annotation in the file ..
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,"Abstract first, title second to prevent issues with sentence splitting"
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,"Filter for specific entity types, by default no entities will be filtered"
v0.14.0,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.14.0,train and dev split of V2 will be train in V4
v0.14.0,test split of V2 will be dev in V4
v0.14.0,New documents in V4 will become test documents
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,column format
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,column format
v0.14.0,build dataset name and full huggingface reference name
v0.14.0,Download data if necessary
v0.14.0,"Some datasets in BigBio only have train or test splits, not both"
v0.14.0,"If only test split, assign it to train split"
v0.14.0,"If only train split, sample other from it (sample_missing_splits=True)"
v0.14.0,Not every dataset has a dev / validation set!
v0.14.0,Perform type mapping if necessary
v0.14.0,return None
v0.14.0,TODO: Add entity type mapping for all remaining bigbio datasets not in HunFlair?
v0.14.0,"""simple_chemical"": ""chemical"",  # BioNLP ST 2013 CG"
v0.14.0,"""cancer"": ""disease"",  # BioNLP ST 2013 CG"
v0.14.0,"""gene_or_gene_product"": ""gene"",  # BioNLP ST 2013 CG"
v0.14.0,"""gene"": ""gene"",  # NLM Gene"
v0.14.0,"""chemical"": ""chemical"",  # NLM Chem"
v0.14.0,"""cellline"": ""cell_line"",  # Cell Finder"
v0.14.0,"""species"": ""species"",  # Cell Finder"
v0.14.0,"""protein"": ""gene"",  # BioID"
v0.14.0,"Collect all texts of the document, each passage will be"
v0.14.0,a text in our internal format
v0.14.0,Sort passages by start offset
v0.14.0,Transform all entity annotations into internal format
v0.14.0,Find the passage of the entity (necessary for offset adaption)
v0.14.0,Adapt entity offsets according to passage offsets
v0.14.0,FIXME: This is just for debugging purposes
v0.14.0,passage_text = id_to_text[passage_id]
v0.14.0,doc_text = passage_text[entity_offset[0] : entity_offset[1]]
v0.14.0,"mention_text = entity[""text""][0]"
v0.14.0,if doc_text != mention_text:
v0.14.0,"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
v0.14.0,Get element in the middle
v0.14.0,Is the mention with the passage offsets?
v0.14.0,"If element is smaller than mid, then it can only"
v0.14.0,be present in left subarray
v0.14.0,Else the element can only be present in right subarray
v0.14.0,TODO whether cell or cell line is the correct tag
v0.14.0,TODO whether cell or cell line is the correct tag
v0.14.0,Special case for ProGene: We need to use the split_0_train and split_0_test splits
v0.14.0,as they are currently provided in BigBio
v0.14.0,cache Feidegger config file
v0.14.0,cache Feidegger images
v0.14.0,replace image URL with local cached file
v0.14.0,append Sentence-Image data point
v0.14.0,cast to list if necessary
v0.14.0,cast to list if necessary
v0.14.0,"first, check if pymongo is installed"
v0.14.0,automatically identify train / test / dev files
v0.14.0,"if no test file is found, take any file with 'test' in name"
v0.14.0,Expose base classses
v0.14.0,Expose all biomedical data sets used for the evaluation of BioBERT
v0.14.0,-
v0.14.0,-
v0.14.0,-
v0.14.0,-
v0.14.0,Expose all biomedical data sets using the HUNER splits
v0.14.0,Expose all biomedical data sets
v0.14.0,Expose all document classification datasets
v0.14.0,word sense disambiguation
v0.14.0,Expose all entity linking datasets
v0.14.0,Expose all relation extraction datasets
v0.14.0,universal proposition banks
v0.14.0,keyphrase detection datasets
v0.14.0,other NER datasets
v0.14.0,standard NER datasets
v0.14.0,Expose all sequence labeling datasets
v0.14.0,Expose all text-image datasets
v0.14.0,Expose all text-text datasets
v0.14.0,Expose all treebanks
v0.14.0,"find train, dev and test files if not specified"
v0.14.0,get train data
v0.14.0,get test data
v0.14.0,get dev data
v0.14.0,option 1: read only sentence boundaries as offset positions
v0.14.0,option 2: keep everything in memory
v0.14.0,"if in memory, retrieve parsed sentence"
v0.14.0,else skip to position in file where sentence begins
v0.14.0,current token ID
v0.14.0,handling for the awful UD multiword format
v0.14.0,end of sentence
v0.14.0,comments or ellipsis
v0.14.0,if token is a multi-word
v0.14.0,normal single-word tokens
v0.14.0,"if we don't split multiwords, skip over component words"
v0.14.0,add token
v0.14.0,add morphological tags
v0.14.0,derive whitespace logic for multiwords
v0.14.0,"if multi-word equals component tokens, there should be no whitespace"
v0.14.0,go through all tokens in subword and set whitespace_after information
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,this dataset name
v0.14.0,default dataset folder is the cache root
v0.14.0,download data if necessary
v0.14.0,"finally, print model card for information"
v0.14.0,noqa: INP001
v0.14.0,-- Project information -----------------------------------------------------
v0.14.0,"The full version, including alpha/beta/rc tags"
v0.14.0,use smv_current_version as the git url
v0.14.0,-- General configuration ---------------------------------------------------
v0.14.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.14.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.14.0,ones.
v0.14.0,"Add any paths that contain templates here, relative to this directory."
v0.14.0,"List of patterns, relative to source directory, that match files and"
v0.14.0,directories to ignore when looking for source files.
v0.14.0,This pattern also affects html_static_path and html_extra_path.
v0.14.0,-- Options for HTML output -------------------------------------------------
v0.14.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.14.0,a list of builtin themes.
v0.14.0,
v0.14.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.14.0,"relative to this directory. They are copied after the builtin static files,"
v0.14.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.14.0,Napoleon settings
v0.14.0,Whitelist pattern for tags (set to None to ignore all tags)
v0.14.0,Whitelist pattern for branches (set to None to ignore all branches)
v0.14.0,Whitelist pattern for remotes (set to None to use local branches only)
v0.14.0,Pattern for released versions
v0.14.0,Format for versioned output directories inside the build directory
v0.14.0,Determines whether remote or local git branches/tags are preferred if their output dirs conflict
v0.14.0,test corpus
v0.14.0,create a TARS classifier
v0.14.0,check if right number of classes
v0.14.0,switch to task with only one label
v0.14.0,check if right number of classes
v0.14.0,switch to task with three labels provided as list
v0.14.0,check if right number of classes
v0.14.0,switch to task with four labels provided as set
v0.14.0,check if right number of classes
v0.14.0,switch to task with two labels provided as Dictionary
v0.14.0,check if right number of classes
v0.14.0,test corpus
v0.14.0,create a TARS classifier
v0.14.0,switch to a new task (TARS can do multiple tasks so you must define one)
v0.14.0,initialize the text classifier trainer
v0.14.0,start the training
v0.14.0,"With end symbol, without start symbol, padding in front"
v0.14.0,"Without end symbol, with start symbol, padding in back"
v0.14.0,"Without end symbol, without start symbol, padding in front"
v0.14.0,initialize trainer
v0.14.0,initialize trainer
v0.14.0,initialize trainer
v0.14.0,increment for last token in sentence if not followed by whitespace
v0.14.0,clean up directory
v0.14.0,clean up directory
v0.14.0,example sentence
v0.14.0,set 4 labels for 2 tokens ('love' is tagged twice)
v0.14.0,check if there are three POS labels with correct text and values
v0.14.0,check if there are is one SENTIMENT label with correct text and values
v0.14.0,check if all tokens are correctly labeled
v0.14.0,remove the pos label from the last word
v0.14.0,there should be 2 POS labels left
v0.14.0,now remove all pos tags
v0.14.0,set 3 labels for 2 spans (HU is tagged twice)
v0.14.0,check if there are three labels with correct text and values
v0.14.0,check if there are two spans with correct text and values
v0.14.0,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.14.0,should be only one NER label left
v0.14.0,and only one NER span
v0.14.0,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.14.0,check if there are three labels with correct text and values
v0.14.0,check if there are two spans with correct text and values
v0.14.0,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.14.0,should be only one NER label left
v0.14.0,and only one NER span
v0.14.0,but there is also one orgtype span and label
v0.14.0,and only one NER span
v0.14.0,let's add the NER tag back
v0.14.0,check if there are three labels with correct text and values
v0.14.0,check if there are two spans with correct text and values
v0.14.0,now remove all NER tags
v0.14.0,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.14.0,create two relation label
v0.14.0,there should be two relation labels
v0.14.0,there should be one syntactic labels
v0.14.0,"there should be two relations, one with two and one with one label"
v0.14.0,example sentence
v0.14.0,add another topic label
v0.14.0,example sentence
v0.14.0,has sentiment value
v0.14.0,has 4 part of speech tags
v0.14.0,has 1 NER tag
v0.14.0,should be in total 6 labels
v0.14.0,example sentence
v0.14.0,add two NER labels
v0.14.0,get the four labels
v0.14.0,check that only two of the respective data points are equal
v0.14.0,make a sentence and some right context
v0.14.0,TODO: is this desirable? Or should two sentences with same text be considered same objects?
v0.14.0,Initializing a Sentence this way assumes that there is a space after each token
v0.14.0,get default dictionary
v0.14.0,init forward LM with 128 hidden states and 1 layer
v0.14.0,get the example corpus and process at character level in forward direction
v0.14.0,train the language model
v0.14.0,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.14.0,clean up results directory
v0.14.0,get default dictionary
v0.14.0,init forward LM with 128 hidden states and 1 layer
v0.14.0,get the example corpus and process at character level in forward direction
v0.14.0,train the language model
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,load column dataset with one entry
v0.14.0,load column dataset with two entries
v0.14.0,load column dataset with three entries
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,check if Token labels are correct
v0.14.0,"get training, test and dev data"
v0.14.0,check if Token labels for frames are correct
v0.14.0,"get training, test and dev data"
v0.14.0,"get training, test and dev data"
v0.14.0,get two corpora as one
v0.14.0,"get training, test and dev data for full English UD corpus from web"
v0.14.0,clean up data directory
v0.14.0,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.14.0,"""2"","
v0.14.0,"""0"","
v0.14.0,"""4"","
v0.14.0,"""2"","
v0.14.0,"""2"","
v0.14.0,"""2"","
v0.14.0,]
v0.14.0,This test only covers basic universal dependencies datasets.
v0.14.0,"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
v0.14.0,"Here, we use the default token annotation fields."
v0.14.0,This test covers the complete HIPE 2022 dataset.
v0.14.0,https://github.com/hipe-eval/HIPE-2022-data
v0.14.0,"Includes variant with document separator, and all versions of the dataset."
v0.14.0,"We have manually checked, that these numbers are correct:"
v0.14.0,"+1 offset, because of missing EOS marker at EOD"
v0.14.0,Test data for v2.1 release
v0.14.0,This test covers the complete ICDAR Europeana corpus:
v0.14.0,https://github.com/stefan-it/historic-domain-adaptation-icdar
v0.14.0,"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
v0.14.0,This test covers the NERMuD dataset. Official stats can be found here:
v0.14.0,https://github.com/dhfbk/KIND/tree/main/evalita-2023
v0.14.0,Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
v0.14.0,This test covers the complete MasakhaPOS dataset.
v0.14.0,"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
v0.14.0,--- Embeddings that are shared by both models --- #
v0.14.0,--- Task 1: Sentiment Analysis (5-class) --- #
v0.14.0,Define corpus and model
v0.14.0,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.14.0,Define corpus and model
v0.14.0,-- Define mapping (which tagger should train on which model) -- #
v0.14.0,-- Create model trainer and train -- #
v0.14.0,NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)
v0.14.0,clean up file
v0.14.0,no need for label_dict
v0.14.0,check if right number of classes
v0.14.0,switch to task with only one label
v0.14.0,check if right number of classes
v0.14.0,switch to task with three labels provided as list
v0.14.0,check if right number of classes
v0.14.0,switch to task with four labels provided as set
v0.14.0,check if right number of classes
v0.14.0,switch to task with two labels provided as Dictionary
v0.14.0,check if right number of classes
v0.14.0,Intel ----founded_by---> Gordon Moore
v0.14.0,Intel ----founded_by---> Robert Noyce
v0.14.0,"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
v0.14.0,Check sentence masking and relation label annotation on
v0.14.0,"training, validation and test dataset (in this test the splits are the same)"
v0.14.0,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.14.0,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.14.0,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.14.0,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
v0.14.0,This sentence is only included if we transform the corpus with cross augmentation
v0.14.0,Ensure this is an example that predicts no classes in multilabel
v0.14.0,check if right number of classes
v0.14.0,switch to task with only one label
v0.14.0,check if right number of classes
v0.14.0,switch to task with three labels provided as list
v0.14.0,check if right number of classes
v0.14.0,switch to task with four labels provided as set
v0.14.0,check if right number of classes
v0.14.0,switch to task with two labels provided as Dictionary
v0.14.0,check if right number of classes
v0.14.0,ensure that the prepared tensors is what we expect
v0.14.0,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.14.0,previous and next sentence as context
v0.14.0,test expansion for sentence without context
v0.14.0,test expansion for with previous and next as context
v0.14.0,test expansion if first sentence is document boundary
v0.14.0,test expansion if we don't use context
v0.14.0,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.14.0,dummy model with embeddings
v0.14.0,save the dummy and load it again
v0.14.0,check that context_length and use_context_separator is the same for both
v0.13.1,mmap seems to be much more memory efficient
v0.13.1,Remove quotes from etag
v0.13.1,"If there is an etag, it's everything after the first period"
v0.13.1,"Otherwise, use None"
v0.13.1,"URL, so get it from the cache (downloading if necessary)"
v0.13.1,"File, and it exists."
v0.13.1,"File, but it doesn't exist."
v0.13.1,Something unknown
v0.13.1,Extract all the contents of zip file in current directory
v0.13.1,Extract all the contents of zip file in current directory
v0.13.1,TODO(joelgrus): do we want to do checksums or anything like that?
v0.13.1,get cache path to put the file
v0.13.1,make HEAD request to check ETag
v0.13.1,add ETag to filename if it exists
v0.13.1,"etag = response.headers.get(""ETag"")"
v0.13.1,"Download to temporary file, then copy to cache dir once finished."
v0.13.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.13.1,GET file object
v0.13.1,These defaults are the same as the argument defaults in tqdm.
v0.13.1,load_big_file is a workaround byhttps://github.com/highway11git
v0.13.1,to load models on some Mac/Windows setups
v0.13.1,see https://github.com/zalandoresearch/flair/issues/351
v0.13.1,first determine the distribution of classes in the dataset
v0.13.1,weight for each sample
v0.13.1,Create blocks
v0.13.1,shuffle the blocks
v0.13.1,concatenate the shuffled blocks
v0.13.1,Create blocks
v0.13.1,shuffle the blocks
v0.13.1,concatenate the shuffled blocks
v0.13.1,increment for last token in sentence if not followed by whitespace
v0.13.1,this is the default init size of a lmdb database for embeddings
v0.13.1,get db filename from embedding name
v0.13.1,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.13.1,SequenceTagger
v0.13.1,TextClassifier
v0.13.1,get db filename from embedding name
v0.13.1,if embedding database already exists
v0.13.1,"otherwise, push embedding to database"
v0.13.1,if embedding database already exists
v0.13.1,open the database in read mode
v0.13.1,we need to set self.k
v0.13.1,create and load the database in write mode
v0.13.1,"no idea why, but we need to close and reopen the environment to avoid"
v0.13.1,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.13.1,when opening new transaction !
v0.13.1,init dictionaries
v0.13.1,"in order to deal with unknown tokens, add <unk>"
v0.13.1,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.13.1,set 'add_unk' depending on whether <unk> is a key
v0.13.1,"if one embedding name, directly return it"
v0.13.1,"if multiple embedding names, concatenate them"
v0.13.1,First we remove any existing labels for this PartOfSentence in self.sentence
v0.13.1,labels also need to be deleted at Sentence object
v0.13.1,delete labels at object itself
v0.13.1,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.13.1,"therefore, labels get added only to the Sentence if it exists"
v0.13.1,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.13.1,"Therefore, labels get set only to the Sentence if it exists"
v0.13.1,"check if the span already exists. If so, return it"
v0.13.1,else make a new span
v0.13.1,"check if the relation already exists. If so, return it"
v0.13.1,else make a new relation
v0.13.1,private field for all known spans
v0.13.1,the tokenizer used for this sentence
v0.13.1,some sentences represent a document boundary (but most do not)
v0.13.1,internal variables to denote position inside dataset
v0.13.1,"if text is passed, instantiate sentence with tokens (words)"
v0.13.1,determine token positions and whitespace_after flag
v0.13.1,the last token has no whitespace after
v0.13.1,log a warning if the dataset is empty
v0.13.1,data with zero-width characters cannot be handled
v0.13.1,set token idx and sentence
v0.13.1,append token to sentence
v0.13.1,register token annotations on sentence
v0.13.1,move sentence embeddings to device
v0.13.1,also move token embeddings to device
v0.13.1,clear token embeddings
v0.13.1,infer whitespace after field
v0.13.1,"if sentence has no tokens, return empty string"
v0.13.1,"otherwise, return concatenation of tokens with the correct offsets"
v0.13.1,The sentence's start position is not propagated to its tokens.
v0.13.1,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.13.1,No character at the corresponding code point: remove it
v0.13.1,"if no label if specified, return all labels"
v0.13.1,"if the label type exists in the Sentence, return it"
v0.13.1,return empty list if none of the above
v0.13.1,labels also need to be deleted at all tokens
v0.13.1,labels also need to be deleted at all known spans
v0.13.1,remove spans without labels
v0.13.1,delete labels at object itself
v0.13.1,set name
v0.13.1,abort if no data is provided
v0.13.1,sample test data from train if none is provided
v0.13.1,sample dev data from train if none is provided
v0.13.1,set train dev and test data
v0.13.1,find out empty sentence indices
v0.13.1,create subset of non-empty sentence indices
v0.13.1,find out empty sentence indices
v0.13.1,create subset of non-empty sentence indices
v0.13.1,"first, determine the datapoint type by going through dataset until first label is found"
v0.13.1,count all label types per sentence
v0.13.1,go through all labels of label_type and count values
v0.13.1,special handling for Token-level annotations. Add all untagged as 'O' label
v0.13.1,"if an unk threshold is set, UNK all label values below this threshold"
v0.13.1,sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
v0.13.1,replace the old label with the new one
v0.13.1,keep track of the old (clean) label using another label type category
v0.13.1,keep track of how many labels in total are flipped
v0.13.1,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.13.1,replace the old label with the new one
v0.13.1,keep track of the old (clean) label using another label type category
v0.13.1,keep track of how many labels in total are flipped
v0.13.1,"add a dummy ""O"" to close final prediction"
v0.13.1,return complex list
v0.13.1,internal variables
v0.13.1,non-set tags are OUT tags
v0.13.1,anything that is not OUT is IN
v0.13.1,does this prediction start a new span?
v0.13.1,B- and S- always start new spans
v0.13.1,"if the predicted class changes, I- starts a new span"
v0.13.1,"if the predicted class changes and S- was previous tag, start a new span"
v0.13.1,if an existing span is ended (either by reaching O or starting a new span)
v0.13.1,determine score and value
v0.13.1,append to result list
v0.13.1,reset for-loop variables for new span
v0.13.1,remember previous tag
v0.13.1,global variable: cache_root
v0.13.1,global variable: device
v0.13.1,"No need for correctness checks, torch is doing it"
v0.13.1,global variable: version
v0.13.1,global variable: arrow symbol
v0.13.1,dummy return to fulfill trainer.train() needs
v0.13.1,print(vec)
v0.13.1,Attach optimizer
v0.13.1,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.13.1,if memory mode option 'none' delete everything
v0.13.1,"if dynamic embedding keys not passed, identify them automatically"
v0.13.1,always delete dynamic embeddings
v0.13.1,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.13.1,optional metric space decoder if prototypes have different length than embedding
v0.13.1,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.13.1,"if set, create initial prototypes from normal distribution"
v0.13.1,"if set, use a radius"
v0.13.1,all parameters will be pushed internally to the specified device
v0.13.1,decode embeddings into prototype space
v0.13.1,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.13.1,verbalize BIOES labels
v0.13.1,"if label is not BIOES, use label itself"
v0.13.1,Always include the name of the Model class for which the state dict holds
v0.13.1,"write out a ""model card"" if one is set"
v0.13.1,save model
v0.13.1,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.13.1,get all non-abstract subclasses
v0.13.1,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.13.1,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.13.1,"if the model cannot be fetched, load as a file"
v0.13.1,try to get model class from state
v0.13.1,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.13.1,"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
v0.13.1,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.13.1,"if this class is not abstract, fetch the model and load it"
v0.13.1,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.13.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.13.1,loss calculation
v0.13.1,variables for printing
v0.13.1,variables for computing scores
v0.13.1,remove any previously predicted labels
v0.13.1,predict for batch
v0.13.1,get the gold labels
v0.13.1,add to all_predicted_values
v0.13.1,make printout lines
v0.13.1,convert true and predicted values to two span-aligned lists
v0.13.1,delete exluded labels if exclude_labels is given
v0.13.1,"if after excluding labels, no label is left, ignore the datapoint"
v0.13.1,write all_predicted_values to out_file if set
v0.13.1,make the evaluation dictionary
v0.13.1,check if this is a multi-label problem
v0.13.1,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.13.1,multi-label problems require a multi-hot vector for each true and predicted label
v0.13.1,single-label problems can do with a single index for each true and predicted label
v0.13.1,"now, calculate evaluation numbers"
v0.13.1,there is at least one gold label or one prediction (default)
v0.13.1,compute accuracy separately as it is not always in classification_report (e.. when micro avg exists)
v0.13.1,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.13.1,"The ""micro avg"" appears only in the classification report if no prediction is possible."
v0.13.1,"Otherwise, it is identical to the ""macro avg"". In this case, we add it to the report."
v0.13.1,"Create and populate score object for logging with all evaluation values, plus the loss"
v0.13.1,issue error and default all evaluation numbers to 0.
v0.13.1,check if there is a label mismatch
v0.13.1,print info
v0.13.1,set the embeddings
v0.13.1,initialize the label dictionary
v0.13.1,initialize the decoder
v0.13.1,set up multi-label logic
v0.13.1,init dropouts
v0.13.1,loss weights and loss function
v0.13.1,Initialize the weight tensor
v0.13.1,set up gradient reversal if so specified
v0.13.1,embed sentences
v0.13.1,get a tensor of data points
v0.13.1,do dropout
v0.13.1,make a forward pass to produce embedded data points and labels
v0.13.1,get the data points for which to predict labels
v0.13.1,get their gold labels as a tensor
v0.13.1,pass data points through network to get encoded data point tensor
v0.13.1,decode
v0.13.1,an optional masking step (no masking in most cases)
v0.13.1,calculate the loss
v0.13.1,filter empty sentences
v0.13.1,reverse sort all sequences by their length
v0.13.1,progress bar for verbosity
v0.13.1,filter data points in batch
v0.13.1,stop if all sentences are empty
v0.13.1,pass data points through network and decode
v0.13.1,if anything could possibly be predicted
v0.13.1,remove previously predicted labels of this type
v0.13.1,filter data points that have labels outside of dictionary
v0.13.1,add DefaultClassifier arguments
v0.13.1,add variables of DefaultClassifier
v0.13.1,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.13.1,Get projected 1st dimension
v0.13.1,Compute bilinear form
v0.13.1,Arcosh
v0.13.1,Project the input data to n+1 dimensions
v0.13.1,"The first dimension, is recomputed in the distance module"
v0.13.1,header for 'weights.txt'
v0.13.1,"determine the column index of loss, f-score and accuracy for"
v0.13.1,"train, dev and test split"
v0.13.1,then get all relevant values from the tsv
v0.13.1,then get all relevant values from the tsv
v0.13.1,plot i
v0.13.1,save plots
v0.13.1,save plots
v0.13.1,plt.show()
v0.13.1,save plot
v0.13.1,auto-spawn on GPU if available
v0.13.1,progress bar for verbosity
v0.13.1,stop if all sentences are empty
v0.13.1,clearing token embeddings to save memory
v0.13.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.13.1,TODO: not saving lines yet
v0.13.1,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.13.1,- MaskedRelationClassifier ?
v0.13.1,This depends if this relation classification architecture should replace or offer as an alternative.
v0.13.1,Set label type and prepare label dictionary
v0.13.1,Initialize super default classifier
v0.13.1,Add the special tokens from the encoding strategy
v0.13.1,"Auto-spawn on GPU, if available"
v0.13.1,Only use entities labelled with the specified labels for each label type
v0.13.1,Only use entities above the specified threshold
v0.13.1,Use a dictionary to find gold relation annotations for a given entity pair
v0.13.1,Yield head and tail entity pairs from the cross product of all entities
v0.13.1,Remove identity relation entity pairs
v0.13.1,Remove entity pairs with labels that do not match any
v0.13.1,of the specified relations in `self.entity_pair_labels`
v0.13.1,"Obtain gold label, if existing"
v0.13.1,Some sanity checks
v0.13.1,Pre-compute non-leading head and tail tokens for entity masking
v0.13.1,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.13.1,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.13.1,"Therefore, we use the span's position in the sentence."
v0.13.1,Create masked sentence
v0.13.1,Add gold relation annotation as sentence label
v0.13.1,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.13.1,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.13.1,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.13.1,"may get distributed into different splits. For training purposes, this is always undesired."
v0.13.1,Ensure that all sentences are encoded properly
v0.13.1,Deal with the case where all sentences are encoded sentences
v0.13.1,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.13.1,Deal with the case where all sentences are standard (non-encoded) sentences
v0.13.1,"For each encoded sentence, transfer its prediction onto the original relation"
v0.13.1,auto-spawn on GPU if available
v0.13.1,pad strings with whitespaces to longest sentence
v0.13.1,cut up the input into chunks of max charlength = chunk_size
v0.13.1,push each chunk through the RNN language model
v0.13.1,concatenate all chunks to make final output
v0.13.1,initial hidden state
v0.13.1,get predicted weights
v0.13.1,divide by temperature
v0.13.1,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.13.1,this makes a vector in which the largest value is 0
v0.13.1,compute word weights with exponential function
v0.13.1,try sampling multinomial distribution for next character
v0.13.1,print(word_idx)
v0.13.1,input ids
v0.13.1,push list of character IDs through model
v0.13.1,the target is always the next character
v0.13.1,use cross entropy loss to compare output of forward pass with targets
v0.13.1,exponentiate cross-entropy loss to calculate perplexity
v0.13.1,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.13.1,serialize the language models and the constructor arguments (but nothing else)
v0.13.1,special handling for deserializing language models
v0.13.1,re-initialize language model with constructor arguments
v0.13.1,copy over state dictionary to self
v0.13.1,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.13.1,"in their ""self.train()"" method)"
v0.13.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.13.1,"check if this is the case and if so, set it"
v0.13.1,Transform input data into TARS format
v0.13.1,"if there are no labels, return a random sample as negatives"
v0.13.1,"otherwise, go through all labels"
v0.13.1,make sure the probabilities always sum up to 1
v0.13.1,get and embed all labels by making a Sentence object that contains only the label text
v0.13.1,get each label embedding and scale between 0 and 1
v0.13.1,compute similarity matrix
v0.13.1,"the higher the similarity, the greater the chance that a label is"
v0.13.1,sampled as negative example
v0.13.1,make label dictionary if no Dictionary object is passed
v0.13.1,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.13.1,check if candidate_label_set is empty
v0.13.1,make list if only one candidate label is passed
v0.13.1,create label dictionary
v0.13.1,note current task
v0.13.1,create a temporary task
v0.13.1,make zero shot predictions
v0.13.1,switch to the pre-existing task
v0.13.1,prepare TARS dictionary
v0.13.1,initialize a bare-bones sequence tagger
v0.13.1,transformer separator
v0.13.1,Store task specific labels since TARS can handle multiple tasks
v0.13.1,make a tars sentence where all labels are O by default
v0.13.1,init new TARS classifier
v0.13.1,set all task information
v0.13.1,progress bar for verbosity
v0.13.1,stop if all sentences are empty
v0.13.1,always remove tags first
v0.13.1,go through each sentence in the batch
v0.13.1,always remove tags first
v0.13.1,get the span and its label
v0.13.1,determine whether tokens in this span already have a label
v0.13.1,only add if all tokens have no label
v0.13.1,make and add a corresponding predicted span
v0.13.1,set indices so that no token can be tagged twice
v0.13.1,clearing token embeddings to save memory
v0.13.1,"all labels default to ""O"""
v0.13.1,set gold token-level
v0.13.1,set predicted token-level
v0.13.1,now print labels in CoNLL format
v0.13.1,prepare TARS dictionary
v0.13.1,initialize a bare-bones sequence tagger
v0.13.1,transformer separator
v0.13.1,Store task specific labels since TARS can handle multiple tasks
v0.13.1,get the serialized embeddings
v0.13.1,remap state dict for models serialized with Flair <= 0.11.3
v0.13.1,init new TARS classifier
v0.13.1,set all task information
v0.13.1,with torch.no_grad():
v0.13.1,progress bar for verbosity
v0.13.1,stop if all sentences are empty
v0.13.1,always remove tags first
v0.13.1,go through each sentence in the batch
v0.13.1,always remove tags first
v0.13.1,add all labels that according to TARS match the text and are above threshold
v0.13.1,do not add labels below confidence threshold
v0.13.1,only use label with the highest confidence if enforcing single-label predictions
v0.13.1,add the label with the highest score even if below the threshold if force label is activated.
v0.13.1,remove previously added labels and only add the best label
v0.13.1,clearing token embeddings to save memory
v0.13.1,set separator to concatenate two sentences
v0.13.1,auto-spawn on GPU if available
v0.13.1,pooling operation to get embeddings for entites
v0.13.1,set embeddings
v0.13.1,set relation and entity label types
v0.13.1,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.13.1,filter entity pairs according to their tags if set
v0.13.1,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.13.1,character dictionary for decoding and encoding
v0.13.1,make sure <unk> is in dictionary for handling of unknown characters
v0.13.1,add special symbols to dictionary if necessary and save respective indices
v0.13.1,---- ENCODER ----
v0.13.1,encoder character embeddings
v0.13.1,encoder pre-trained embeddings
v0.13.1,encoder RNN
v0.13.1,additional encoder linear layer if bidirectional encoding
v0.13.1,---- DECODER ----
v0.13.1,decoder: linear layers to transform vectors to and from alphabet_size
v0.13.1,when using attention we concatenate attention outcome and decoder hidden states
v0.13.1,decoder RNN
v0.13.1,loss and softmax
v0.13.1,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.13.1,add additional columns for special symbols if necessary
v0.13.1,initialize with dummy symbols
v0.13.1,encode inputs
v0.13.1,get labels (we assume each token has a lemma label)
v0.13.1,get char indices for labels of sentence
v0.13.1,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.13.1,max_sequence_length = length of longest label of sentence + 1
v0.13.1,get char embeddings
v0.13.1,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.13.1,take decoder input and initial hidden and pass through RNN
v0.13.1,"if all encoder outputs are provided, use attention"
v0.13.1,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.13.1,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.13.1,get all tokens
v0.13.1,encode input characters by sending them through RNN
v0.13.1,get one-hots for characters and add special symbols / padding
v0.13.1,determine length of each token
v0.13.1,embed sentences
v0.13.1,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.13.1,variable to store initial hidden states for decoder
v0.13.1,encode input characters by sending them through RNN
v0.13.1,test packing and padding
v0.13.1,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.13.1,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.13.1,decoder later with self.emb_to_hidden
v0.13.1,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.13.1,use token embedding as initial hidden state for decoder
v0.13.1,concatenate everything together and project to appropriate size for decoder
v0.13.1,variable to store initial hidden states for decoder
v0.13.1,encode input characters by sending them through RNN
v0.13.1,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.13.1,embed character one-hots
v0.13.1,send through encoder RNN (produces initial hidden for decoder)
v0.13.1,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.13.1,project 2*hidden_size to hidden_size
v0.13.1,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.13.1,later with self.emb_to_hidden
v0.13.1,use token embedding as initial hidden state for decoder
v0.13.1,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.13.1,concatenate everything together and project to appropriate size for decoder
v0.13.1,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.13.1,"create target vector (batch_size, max_label_seq_length + 1)"
v0.13.1,filter empty sentences
v0.13.1,max length of the predicted sequences
v0.13.1,for printing
v0.13.1,stop if all sentences are empty
v0.13.1,remove previously predicted labels of this type
v0.13.1,create list of tokens in batch
v0.13.1,encode inputs
v0.13.1,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.13.1,sequence length is always set to one in prediction
v0.13.1,option 1: greedy decoding
v0.13.1,predictions
v0.13.1,decode next character
v0.13.1,pick top beam size many outputs with highest probabilities
v0.13.1,option 2: beam search
v0.13.1,out_probs = self.softmax(output_vectors).squeeze(1)
v0.13.1,make sure no dummy symbol <> or start symbol <S> is predicted
v0.13.1,pick top beam size many outputs with highest probabilities
v0.13.1,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.13.1,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.13.1,keep scores of beam_size many hypothesis for each token in the batch
v0.13.1,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.13.1,save sequences so far
v0.13.1,keep track of how many hypothesis were completed for each token
v0.13.1,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.13.1,decode with log softmax
v0.13.1,make sure no dummy symbol <> or start symbol <S> is predicted
v0.13.1,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.13.1,"if the sequence is already ended, do not record as candidate"
v0.13.1,index of token in in list tokens_in_batch
v0.13.1,print(token_number)
v0.13.1,hypothesis score
v0.13.1,TODO: remove token if number of completed hypothesis exceeds given value
v0.13.1,set score of corresponding entry to -inf so it will not be expanded
v0.13.1,get leading_indices for next expansion
v0.13.1,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.13.1,take beam_size many copies of scores vector and add scores of possible new extensions
v0.13.1,"size (beam_size*batch_size, beam_size)"
v0.13.1,print(hypothesis_scores)
v0.13.1,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.13.1,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.13.1,print(hypothesis_scores_per_token)
v0.13.1,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.13.1,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.13.1,a list of length beam_size*batch_size
v0.13.1,"where the first three inidices belong to the first token, the next three to the second token,"
v0.13.1,and so on
v0.13.1,with these indices we can compute the tensors for the next iteration
v0.13.1,expand sequences with corresponding index
v0.13.1,add log-probabilities to the scores
v0.13.1,save new leading indices
v0.13.1,save corresponding hidden states
v0.13.1,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.13.1,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.13.1,get best final hypothesis for each token
v0.13.1,get characters from index sequences and add predicted label to token
v0.13.1,"Overwrites evaluate of parent class to remove the ""by class"" printout"
v0.13.1,set separator to concatenate two sentences
v0.13.1,init dropouts
v0.13.1,auto-spawn on GPU if available
v0.13.1,make a forward pass to produce embedded data points and labels
v0.13.1,get their gold labels as a tensor
v0.13.1,pass data points through network to get encoded data point tensor
v0.13.1,decode
v0.13.1,calculate the loss
v0.13.1,get a tensor of data points
v0.13.1,do dropout
v0.13.1,add DefaultClassifier arguments
v0.13.1,progress bar for verbosity
v0.13.1,stop if all sentences are empty
v0.13.1,clearing token embeddings to save memory
v0.13.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.13.1,"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
v0.13.1,fields in case this is a span-prediction problem
v0.13.1,the label type
v0.13.1,all parameters will be pushed internally to the specified device
v0.13.1,special handling during training if this is a span prediction problem
v0.13.1,internal variables
v0.13.1,non-set tags are OUT tags
v0.13.1,anything that is not OUT is IN
v0.13.1,does this prediction start a new span?
v0.13.1,B- and S- always start new spans
v0.13.1,"if the predicted class changes, I- starts a new span"
v0.13.1,"if the predicted class changes and S- was previous tag, start a new span"
v0.13.1,if an existing span is ended (either by reaching O or starting a new span)
v0.13.1,reset for-loop variables for new span
v0.13.1,remember previous tag
v0.13.1,"if there is a span at end of sentence, add it"
v0.13.1,"all labels default to ""O"""
v0.13.1,set gold token-level
v0.13.1,set predicted token-level
v0.13.1,now print labels in CoNLL format
v0.13.1,print labels in CoNLL format
v0.13.1,internal candidate lists of generator
v0.13.1,load Zelda candidates if so passed
v0.13.1,create candidate lists
v0.13.1,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.13.1,create a new dictionary for lower cased mentions
v0.13.1,go through each mention and its candidates
v0.13.1,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.13.1,set lowercased version as map
v0.13.1,remap state dict for models serialized with Flair <= 0.11.3
v0.13.1,get the candidates
v0.13.1,"during training, add the gold value as candidate"
v0.13.1,----- Create the internal tag dictionary -----
v0.13.1,span-labels need special encoding (BIO or BIOES)
v0.13.1,the big question is whether the label dictionary should contain an UNK or not
v0.13.1,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.13.1,"with UNK, the model learns less well if there are no UNK examples"
v0.13.1,is this a span prediction problem?
v0.13.1,----- Embeddings -----
v0.13.1,----- Initial loss weights parameters -----
v0.13.1,----- RNN specific parameters -----
v0.13.1,----- Conditional Random Field parameters -----
v0.13.1,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.13.1,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.13.1,----- Dropout parameters -----
v0.13.1,dropouts
v0.13.1,remove word dropout if there is no contact over the sequence dimension.
v0.13.1,----- Model layers -----
v0.13.1,----- RNN layer -----
v0.13.1,"If shared RNN provided, else create one for model"
v0.13.1,Whether to train initial hidden state
v0.13.1,final linear map to tag space
v0.13.1,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.13.1,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.13.1,"if there are no sentences, there is no loss"
v0.13.1,forward pass to get scores
v0.13.1,calculate loss given scores and labels
v0.13.1,make a zero-padded tensor for the whole sentence
v0.13.1,linear map to tag space
v0.13.1,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.13.1,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.13.1,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.13.1,spans need to be encoded as token-level predictions
v0.13.1,all others are regular labels for each token
v0.13.1,make sure it's a list
v0.13.1,filter empty sentences
v0.13.1,reverse sort all sequences by their length
v0.13.1,progress bar for verbosity
v0.13.1,stop if all sentences are empty
v0.13.1,get features from forward propagation
v0.13.1,remove previously predicted labels of this type
v0.13.1,"if return_loss, get loss value"
v0.13.1,make predictions
v0.13.1,add predictions to Sentence
v0.13.1,BIOES-labels need to be converted to spans
v0.13.1,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.13.1,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.13.1,core Flair models on Huggingface ModelHub
v0.13.1,"Large NER models,"
v0.13.1,Multilingual NER models
v0.13.1,English POS models
v0.13.1,Multilingual POS models
v0.13.1,English SRL models
v0.13.1,English chunking models
v0.13.1,Language-specific NER models
v0.13.1,Language-specific POS models
v0.13.1,English NER models
v0.13.1,Multilingual NER models
v0.13.1,English POS models
v0.13.1,Multilingual POS models
v0.13.1,English SRL models
v0.13.1,English chunking models
v0.13.1,Danish models
v0.13.1,German models
v0.13.1,French models
v0.13.1,Dutch models
v0.13.1,Malayalam models
v0.13.1,Portuguese models
v0.13.1,Keyphase models
v0.13.1,Biomedical models
v0.13.1,check if model name is a valid local file
v0.13.1,"check if model key is remapped to HF key - if so, print out information"
v0.13.1,get mapped name
v0.13.1,use mapped name instead
v0.13.1,"if not, check if model key is remapped to direct download location. If so, download model"
v0.13.1,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.13.1,"for all other cases (not local file or special download location), use HF model hub"
v0.13.1,"if not a local file, get from model hub"
v0.13.1,use model name as subfolder
v0.13.1,Lazy import
v0.13.1,output information
v0.13.1,## Demo: How to use in Flair
v0.13.1,load tagger
v0.13.1,make example sentence
v0.13.1,predict NER tags
v0.13.1,print sentence
v0.13.1,print predicted NER spans
v0.13.1,iterate over entities and print
v0.13.1,Lazy import
v0.13.1,Save model weight
v0.13.1,Determine if model card already exists
v0.13.1,Generate and save model card
v0.13.1,Upload files
v0.13.1,"all labels default to ""O"""
v0.13.1,set gold token-level
v0.13.1,set predicted token-level
v0.13.1,now print labels in CoNLL format
v0.13.1,print labels in CoNLL format
v0.13.1,the multi task model has several labels
v0.13.1,biomedical models
v0.13.1,entity linker
v0.13.1,auto-spawn on GPU if available
v0.13.1,remap state dict for models serialized with Flair <= 0.11.3
v0.13.1,English sentiment models
v0.13.1,Communicative Functions Model
v0.13.1,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.13.1,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.13.1,"Initially, get scores from <start> tag to all other tags"
v0.13.1,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.13.1,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.13.1,Create a tensor to hold accumulated sequence scores at each current tag
v0.13.1,Create a tensor to hold back-pointers
v0.13.1,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.13.1,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.13.1,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.13.1,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.13.1,"If sentence is over, add transition to STOP-tag"
v0.13.1,Decode/trace best path backwards
v0.13.1,Sanity check
v0.13.1,remove start-tag and backscore to stop-tag
v0.13.1,Max + Softmax to get confidence score for predicted label and append label to each token
v0.13.1,"Transitions are used in the following way: transitions[to, from]."
v0.13.1,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.13.1,to START-tag and from STOP-tag to any other tag to -10000.
v0.13.1,"if necessary, make batch_steps"
v0.13.1,break up the batch into slices of size
v0.13.1,mini_batch_chunk_size
v0.13.1,"if training also uses dev/train data, include in training set"
v0.13.1,evaluation and monitoring
v0.13.1,sampling and shuffling
v0.13.1,evaluation and monitoring
v0.13.1,when and what to save
v0.13.1,logging parameters
v0.13.1,plugins
v0.13.1,activate annealing plugin
v0.13.1,call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
v0.13.1,training parameters
v0.13.1,evaluation and monitoring
v0.13.1,sampling and shuffling
v0.13.1,evaluation and monitoring
v0.13.1,when and what to save
v0.13.1,logging parameters
v0.13.1,amp
v0.13.1,plugins
v0.13.1,annealing logic
v0.13.1,training parameters
v0.13.1,evaluation and monitoring
v0.13.1,sampling and shuffling
v0.13.1,evaluation and monitoring
v0.13.1,when and what to save
v0.13.1,logging parameters
v0.13.1,amp
v0.13.1,plugins
v0.13.1,training parameters
v0.13.1,evaluation and monitoring
v0.13.1,sampling and shuffling
v0.13.1,evaluation and monitoring
v0.13.1,when and what to save
v0.13.1,logging parameters
v0.13.1,amp
v0.13.1,plugins
v0.13.1,Create output folder
v0.13.1,=== START BLOCK: ACTIVATE PLUGINS === #
v0.13.1,We first activate all optional plugins. These take care of optional functionality such as various
v0.13.1,logging techniques and checkpointing
v0.13.1,log file plugin
v0.13.1,loss file plugin
v0.13.1,plugin for writing weights
v0.13.1,plugin for checkpointing
v0.13.1,=== END BLOCK: ACTIVATE PLUGINS === #
v0.13.1,derive parameters the function was called with (or defaults)
v0.13.1,initialize model card with these parameters
v0.13.1,Prepare training data and get dataset size
v0.13.1,"determine what splits (train, dev, test) to evaluate"
v0.13.1,determine how to determine best model and whether to save it
v0.13.1,instantiate the optimizer
v0.13.1,initialize sampler if provided
v0.13.1,init with default values if only class is provided
v0.13.1,set dataset to sample from
v0.13.1,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.13.1,Sanity checks
v0.13.1,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.13.1,-- AmpPlugin -> wraps with AMP
v0.13.1,-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
v0.13.1,At any point you can hit Ctrl + C to break out of training early.
v0.13.1,"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
v0.13.1,- LossFilePlugin -> get the current epoch for loss file logging
v0.13.1,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.13.1,log infos on training progress every `log_modulo` batches
v0.13.1,process mini-batches
v0.13.1,zero the gradients on the model and optimizer
v0.13.1,forward and backward for batch
v0.13.1,forward pass
v0.13.1,identify dynamic embeddings (always deleted) on first sentence
v0.13.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.13.1,do the optimizer step
v0.13.1,- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
v0.13.1,- WeightExtractorPlugin -> extracts weights
v0.13.1,- CheckpointPlugin -> executes save_model_each_k_epochs
v0.13.1,- SchedulerPlugin -> log bad epochs
v0.13.1,Determine if this is the best model or if we need to anneal
v0.13.1,log results
v0.13.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.13.1,use DEV split to determine if this is the best model so far
v0.13.1,"if not using DEV score, determine best model using train loss"
v0.13.1,- LossFilePlugin -> somehow prints all relevant metrics
v0.13.1,- AnnealPlugin -> scheduler step
v0.13.1,- SWAPlugin -> restores SGD weights from SWA
v0.13.1,"if we do not use dev data for model selection, save final model"
v0.13.1,TensorboardLogger -> closes writer
v0.13.1,test best model if test data is present
v0.13.1,get and return the final test score of best model
v0.13.1,MetricHistoryPlugin -> stores the loss history in return_values
v0.13.1,"Store return values, as they will be erased by reset_training_attributes"
v0.13.1,get a random sample of training sentences
v0.13.1,create a model card for this model with Flair and PyTorch version
v0.13.1,record Transformers version if library is loaded
v0.13.1,remember all parameters used in train() call
v0.13.1,"TextDataset returns a list. valid and test are only one file,"
v0.13.1,so return the first element
v0.13.1,cast string to Path
v0.13.1,error message if the validation dataset is too small
v0.13.1,Shuffle training files randomly after serially iterating
v0.13.1,through corpus one
v0.13.1,"iterate through training data, starting at"
v0.13.1,self.split (for checkpointing)
v0.13.1,off by one for printing
v0.13.1,go into train mode
v0.13.1,reset variables
v0.13.1,not really sure what this does
v0.13.1,do the forward pass in the model
v0.13.1,try to predict the targets
v0.13.1,Backward
v0.13.1,`clip_grad_norm` helps prevent the exploding gradient
v0.13.1,problem in RNNs / LSTMs.
v0.13.1,We detach the hidden state from how it was
v0.13.1,previously produced.
v0.13.1,"If we didn't, the model would try backpropagating"
v0.13.1,all the way to start of the dataset.
v0.13.1,explicitly remove loss to clear up memory
v0.13.1,#########################################################
v0.13.1,Save the model if the validation loss is the best we've
v0.13.1,seen so far.
v0.13.1,#########################################################
v0.13.1,print info
v0.13.1,#########################################################
v0.13.1,##############################################################################
v0.13.1,final testing
v0.13.1,##############################################################################
v0.13.1,Turn on evaluation mode which disables dropout.
v0.13.1,Work out how cleanly we can divide the dataset into bsz parts.
v0.13.1,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.13.1,Evenly divide the data across the bsz batches.
v0.13.1,"no need to check for MetricName, as __add__ of other would be called in this case"
v0.13.1,"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
v0.13.1,instantiate plugin
v0.13.1,"Reset the flag, since an exception event might be dispatched"
v0.13.1,"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
v0.13.1,the callback
v0.13.1,go through all attributes
v0.13.1,get attribute hook events (may raise an AttributeError)
v0.13.1,register function as a hook
v0.13.1,"Decorator was used with parentheses, but no args"
v0.13.1,Decorator was used with args (strings specifiying the events)
v0.13.1,Decorator was used without args
v0.13.1,path to store the model
v0.13.1,special annealing modes
v0.13.1,determine the min learning rate
v0.13.1,"minimize training loss if training with dev data, else maximize dev score"
v0.13.1,instantiate the scheduler
v0.13.1,stop training if learning rate becomes too small
v0.13.1,reload last best model if annealing with restarts is enabled
v0.13.1,calculate warmup steps
v0.13.1,skip if no optimization has happened.
v0.13.1,saves the model with full vocab as checkpoints etc were created with reduced vocab.
v0.13.1,TODO: check if metric is in tracked metrics
v0.13.1,prepare loss logging file and set up header
v0.13.1,set up all metrics to collect
v0.13.1,set up headers
v0.13.1,name: HEADER
v0.13.1,Add all potentially relevant metrics. If a metric is not published
v0.13.1,"after the first epoch (when the header is written), the column is"
v0.13.1,removed at that point.
v0.13.1,initialize the first log line
v0.13.1,record is a list of scalars
v0.13.1,output log file
v0.13.1,remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
v0.13.1,make headers on epoch 1
v0.13.1,write header
v0.13.1,adjust alert level
v0.13.1,"the default model for ELMo is the 'original' model, which is very large"
v0.13.1,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.13.1,put on Cuda if available
v0.13.1,embed a dummy sentence to determine embedding_length
v0.13.1,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.13.1,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.13.1,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.13.1,news-english-forward
v0.13.1,news-english-backward
v0.13.1,news-english-forward
v0.13.1,news-english-backward
v0.13.1,mix-english-forward
v0.13.1,mix-english-backward
v0.13.1,mix-german-forward
v0.13.1,mix-german-backward
v0.13.1,common crawl Polish forward
v0.13.1,common crawl Polish backward
v0.13.1,Slovenian forward
v0.13.1,Slovenian backward
v0.13.1,Bulgarian forward
v0.13.1,Bulgarian backward
v0.13.1,Dutch forward
v0.13.1,Dutch backward
v0.13.1,Swedish forward
v0.13.1,Swedish backward
v0.13.1,French forward
v0.13.1,French backward
v0.13.1,Czech forward
v0.13.1,Czech backward
v0.13.1,Portuguese forward
v0.13.1,Portuguese backward
v0.13.1,initialize cache if use_cache set
v0.13.1,embed a dummy sentence to determine embedding_length
v0.13.1,set to eval mode
v0.13.1,Copy the object's state from self.__dict__ which contains
v0.13.1,all our instance attributes. Always use the dict.copy()
v0.13.1,method to avoid modifying the original state.
v0.13.1,Remove the unpicklable entries.
v0.13.1,"if cache is used, try setting embeddings from cache first"
v0.13.1,try populating embeddings from cache
v0.13.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.13.1,get hidden states from language model
v0.13.1,take first or last hidden states from language model as word representation
v0.13.1,if self.tokenized_lm or token.whitespace_after:
v0.13.1,"if only one sentence is passed, convert to list of sentence"
v0.13.1,bidirectional LSTM on top of embedding layer
v0.13.1,dropouts
v0.13.1,"first, sort sentences by number of tokens"
v0.13.1,go through each sentence in batch
v0.13.1,PADDING: pad shorter sentences out
v0.13.1,ADD TO SENTENCE LIST: add the representation
v0.13.1,--------------------------------------------------------------------
v0.13.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.13.1,--------------------------------------------------------------------
v0.13.1,--------------------------------------------------------------------
v0.13.1,FF PART
v0.13.1,--------------------------------------------------------------------
v0.13.1,use word dropout if set
v0.13.1,--------------------------------------------------------------------
v0.13.1,EXTRACT EMBEDDINGS FROM LSTM
v0.13.1,--------------------------------------------------------------------
v0.13.1,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.13.1,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.13.1,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.13.1,add positional encodings
v0.13.1,reshape the pixels into the sequence
v0.13.1,layer norm after convolution and positional encodings
v0.13.1,add <cls> token
v0.13.1,"transformer requires input in the shape [h*w+1, b, d]"
v0.13.1,the output is an embedding of <cls> token
v0.13.1,this parameter is fixed
v0.13.1,optional fine-tuning on top of embedding layer
v0.13.1,"if only one sentence is passed, convert to list of sentence"
v0.13.1,"if only one sentence is passed, convert to list of sentence"
v0.13.1,bidirectional RNN on top of embedding layer
v0.13.1,dropouts
v0.13.1,TODO: remove in future versions
v0.13.1,embed words in the sentence
v0.13.1,before-RNN dropout
v0.13.1,reproject if set
v0.13.1,push through RNN
v0.13.1,after-RNN dropout
v0.13.1,extract embeddings from RNN
v0.13.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.13.1,"check if this is the case and if so, set it"
v0.13.1,serialize the language models and the constructor arguments (but nothing else)
v0.13.1,re-initialize language model with constructor arguments
v0.13.1,special handling for deserializing language models
v0.13.1,copy over state dictionary to self
v0.13.1,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.13.1,"in their ""self.train()"" method)"
v0.13.1,IMPORTANT: add embeddings as torch modules
v0.13.1,iterate over sentences
v0.13.1,"if its a forward LM, take last state"
v0.13.1,"convert to plain strings, embedded in a list for the encode function"
v0.13.1,CNN
v0.13.1,dropouts
v0.13.1,TODO: remove in future versions
v0.13.1,embed words in the sentence
v0.13.1,before-RNN dropout
v0.13.1,reproject if set
v0.13.1,push CNN
v0.13.1,after-CNN dropout
v0.13.1,extract embeddings from CNN
v0.13.1,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.13.1,"if only one sentence is passed, convert to list of sentence"
v0.13.1,Expose base classses
v0.13.1,Expose document embedding classes
v0.13.1,Expose image embedding classes
v0.13.1,Expose legacy embedding classes
v0.13.1,Expose token embedding classes
v0.13.1,in some cases we need to insert zero vectors for tokens without embedding.
v0.13.1,padding
v0.13.1,remove special markup
v0.13.1,check if special tokens exist to circumvent error message
v0.13.1,iterate over subtokens and reconstruct tokens
v0.13.1,remove special markup
v0.13.1,check if reconstructed token is special begin token ([CLS] or similar)
v0.13.1,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.13.1,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.13.1,if tokens are unaccounted for
v0.13.1,check if all tokens were matched to subtokens
v0.13.1,The layoutlm tokenizer doesn't handle ocr themselves
v0.13.1,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.13.1,"cannot run `.encode` if ocr boxes are required, assume"
v0.13.1,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.13.1,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.13.1,in case of doubt: token embedding has higher priority than document embedding
v0.13.1,random check some tokens to save performance.
v0.13.1,Models such as FNet do not have an attention_mask
v0.13.1,set language IDs for XLM-style transformers
v0.13.1,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.13.1,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.13.1,set context if not set already
v0.13.1,flair specific pre-tokenization
v0.13.1,fields to store left and right context
v0.13.1,expand context only if context_length is set
v0.13.1,"if context_dropout is set, randomly deactivate left context during training"
v0.13.1,"if context_dropout is set, randomly deactivate right context during training"
v0.13.1,"if use_context_separator is set, add a [FLERT] token"
v0.13.1,return expanded sentence and context length information
v0.13.1,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.13.1,temporary fix to disable tokenizer parallelism warning
v0.13.1,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.13.1,do not print transformer warnings as these are confusing in this case
v0.13.1,load tokenizer and transformer model
v0.13.1,load tokenizer from inmemory zip-file
v0.13.1,model name
v0.13.1,embedding parameters
v0.13.1,send mini-token through to check how many layers the model has
v0.13.1,return length
v0.13.1,"If we use a context separator, add a new special token"
v0.13.1,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.13.1,"when initializing, embeddings are in eval mode by default"
v0.13.1,in case of doubt: token embedding has higher priority than document embedding
v0.13.1,in case of doubt: token embedding has higher priority than document embedding
v0.13.1,legacy TransformerDocumentEmbedding
v0.13.1,legacy TransformerTokenEmbedding
v0.13.1,legacy Flair <= 0.12
v0.13.1,legacy Flair <= 0.7
v0.13.1,legacy TransformerTokenEmbedding
v0.13.1,Legacy TransformerDocumentEmbedding
v0.13.1,legacy TransformerTokenEmbedding
v0.13.1,legacy TransformerDocumentEmbedding
v0.13.1,some models like the tars model somehow lost this information.
v0.13.1,copy values from new embedding
v0.13.1,those parameters are only from the super class and will be recreated in the constructor.
v0.13.1,cls first pooling can be done without recreating sentence hidden states
v0.13.1,make the tuple a tensor; makes working with it easier.
v0.13.1,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.13.1,only use layers that will be outputted
v0.13.1,this parameter is fixed
v0.13.1,IMPORTANT: add embeddings as torch modules
v0.13.1,"if only one sentence is passed, convert to list of sentence"
v0.13.1,make compatible with serialized models
v0.13.1,gensim version 4
v0.13.1,gensim version 3
v0.13.1,"if no embedding is set, the vocab and embedding length is requried"
v0.13.1,GLOVE embeddings
v0.13.1,TURIAN embeddings
v0.13.1,KOMNINOS embeddings
v0.13.1,pubmed embeddings
v0.13.1,FT-CRAWL embeddings
v0.13.1,FT-CRAWL embeddings
v0.13.1,twitter embeddings
v0.13.1,two-letter language code wiki embeddings
v0.13.1,two-letter language code wiki embeddings
v0.13.1,two-letter language code crawl embeddings
v0.13.1,"this is required to force the module on the cpu,"
v0.13.1,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.13.1,self.to(..) actually sets the device properly
v0.13.1,this ignores the get_cached_vec method when loading older versions
v0.13.1,it is needed for compatibility reasons
v0.13.1,gensim version 4
v0.13.1,gensim version 3
v0.13.1,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.13.1,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.13.1,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.13.1,not finding them)
v0.13.1,use list of common characters if none provided
v0.13.1,translate words in sentence into ints using dictionary
v0.13.1,"sort words by length, for batching and masking"
v0.13.1,chars for rnn processing
v0.13.1,multilingual models
v0.13.1,English models
v0.13.1,Arabic
v0.13.1,Bulgarian
v0.13.1,Czech
v0.13.1,Danish
v0.13.1,German
v0.13.1,Spanish
v0.13.1,Basque
v0.13.1,Persian
v0.13.1,Finnish
v0.13.1,French
v0.13.1,Hebrew
v0.13.1,Hindi
v0.13.1,Croatian
v0.13.1,Indonesian
v0.13.1,Italian
v0.13.1,Japanese
v0.13.1,Malayalam
v0.13.1,Dutch
v0.13.1,Norwegian
v0.13.1,Polish
v0.13.1,Portuguese
v0.13.1,Pubmed
v0.13.1,Slovenian
v0.13.1,Swedish
v0.13.1,Tamil
v0.13.1,Spanish clinical
v0.13.1,CLEF HIPE Shared task
v0.13.1,Amharic
v0.13.1,Ukrainian
v0.13.1,load model if in pretrained model map
v0.13.1,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.13.1,CLEF HIPE models are lowercased
v0.13.1,embeddings are static if we don't do finetuning
v0.13.1,embed a dummy sentence to determine embedding_length
v0.13.1,set to eval mode
v0.13.1,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.13.1,gradients are enable if fine-tuning is enabled
v0.13.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.13.1,get hidden states from language model
v0.13.1,take first or last hidden states from language model as word representation
v0.13.1,offset mode that extracts at whitespace after last character
v0.13.1,offset mode that extracts at last character
v0.13.1,make compatible with old models
v0.13.1,use the character language model embeddings as basis
v0.13.1,length is twice the original character LM embedding length
v0.13.1,these fields are for the embedding memory
v0.13.1,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.13.1,we re-compute embeddings dynamically at each epoch
v0.13.1,set the memory method
v0.13.1,memory is wiped each time we do a training run
v0.13.1,"if we keep a pooling, it needs to be updated continuously"
v0.13.1,update embedding
v0.13.1,check token.text is empty or not
v0.13.1,set aggregation operation
v0.13.1,add embeddings after updating
v0.13.1,model architecture
v0.13.1,model architecture
v0.13.1,"""pl"","
v0.13.1,download if necessary
v0.13.1,load the model
v0.13.1,"TODO: keep for backwards compatibility, but remove in future"
v0.13.1,save the sentence piece model as binary file (not as path which may change)
v0.13.1,write out the binary sentence piece model into the expected directory
v0.13.1,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.13.1,"otherwise, use normal process and potentially trigger another download"
v0.13.1,"once the modes if there, load it with sentence piece"
v0.13.1,empty words get no embedding
v0.13.1,all other words get embedded
v0.13.1,GLOVE embeddings
v0.13.1,no need to recreate as NILCEmbeddings
v0.13.1,read in test file if exists
v0.13.1,read in dev file if exists
v0.13.1,"find train, dev and test files if not specified"
v0.13.1,Add tags for each annotated span
v0.13.1,Remove leading and trailing whitespaces from annotated spans
v0.13.1,Search start and end token index for current span
v0.13.1,If end index is not found set to last token
v0.13.1,Throw error if indices are not valid
v0.13.1,Add metadatas for sentence
v0.13.1,Currently all Jsonl Datasets are stored in Memory
v0.13.1,get train data
v0.13.1,read in test file if exists
v0.13.1,read in dev file if exists
v0.13.1,"find train, dev and test files if not specified"
v0.13.1,special key for space after
v0.13.1,special key for feature columns
v0.13.1,special key for dependency head id
v0.13.1,"store either Sentence objects in memory, or only file offsets"
v0.13.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.13.1,determine encoding of text file
v0.13.1,identify which columns are spans and which are word-level
v0.13.1,now load all sentences
v0.13.1,skip first line if to selected
v0.13.1,option 1: keep Sentence objects in memory
v0.13.1,pointer to previous
v0.13.1,parse next sentence
v0.13.1,quit if last sentence reached
v0.13.1,skip banned sentences
v0.13.1,set previous and next sentence for context
v0.13.1,append parsed sentence to list in memory
v0.13.1,option 2: keep source data in memory
v0.13.1,"read lines for next sentence, but don't parse"
v0.13.1,quit if last sentence reached
v0.13.1,append raw lines for each sentence
v0.13.1,we make a distinction between word-level tags and span-level tags
v0.13.1,read first sentence to determine which columns are span-labels
v0.13.1,skip first line if to selected
v0.13.1,check the first 5 sentences
v0.13.1,go through all annotations and identify word- and span-level annotations
v0.13.1,- if a column has at least one BIES we know it's a Span label
v0.13.1,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.13.1,- problem cases are columns for which we see only O - in this case we default to Span
v0.13.1,skip assigned columns
v0.13.1,the space after key is always word-levels
v0.13.1,"if at least one token has a BIES, we know it's a span label"
v0.13.1,"if at least one token has a label other than BIOES, we know it's a token label"
v0.13.1,all remaining columns that are not word-level are span-level
v0.13.1,for column in self.word_level_tag_columns:
v0.13.1,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.13.1,"if sentence ends, break"
v0.13.1,parse comments if possible
v0.13.1,"otherwise, this line is a token. parse and add to sentence"
v0.13.1,check if this sentence is a document boundary
v0.13.1,add span labels
v0.13.1,discard tags from tokens that are not added to the sentence
v0.13.1,parse relations if they are set
v0.13.1,head and tail span indices are 1-indexed and end index is inclusive
v0.13.1,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.13.1,"to set the metadata ""domain"" to ""de-orcas"""
v0.13.1,get fields from line
v0.13.1,get head_id if exists (only in dependency parses)
v0.13.1,initialize token
v0.13.1,go through all columns
v0.13.1,'feats' and 'misc' column should be split into different fields
v0.13.1,special handling for whitespace after
v0.13.1,add each other feature as label-value pair
v0.13.1,get the task name (e.g. 'ner')
v0.13.1,get the label value
v0.13.1,add label
v0.13.1,remap regular tag names
v0.13.1,"if in memory, retrieve parsed sentence"
v0.13.1,else skip to position in file where sentence begins
v0.13.1,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.13.1,use all domains
v0.13.1,iter over all domains / sources and create target files
v0.13.1,The conll representation of coref spans allows spans to
v0.13.1,"overlap. If spans end or begin at the same word, they are"
v0.13.1,"separated by a ""|""."
v0.13.1,The span begins at this word.
v0.13.1,The span begins and ends at this word (single word span).
v0.13.1,"The span is starting, so we record the index of the word."
v0.13.1,"The span for this id is ending, but didn't start at this word."
v0.13.1,Retrieve the start index from the document state and
v0.13.1,add the span to the clusters for this id.
v0.13.1,strip all bracketing information to
v0.13.1,get the actual propbank label.
v0.13.1,Entering into a span for a particular semantic role label.
v0.13.1,We append the label and set the current span for this annotation.
v0.13.1,"If there's no '(' token, but the current_span_label is not None,"
v0.13.1,then we are inside a span.
v0.13.1,We're outside a span.
v0.13.1,"Exiting a span, so we reset the current span label for this annotation."
v0.13.1,The words in the sentence.
v0.13.1,The pos tags of the words in the sentence.
v0.13.1,the pieces of the parse tree.
v0.13.1,The lemmatised form of the words in the sentence which
v0.13.1,have SRL or word sense information.
v0.13.1,The FrameNet ID of the predicate.
v0.13.1,"The sense of the word, if available."
v0.13.1,"The current speaker, if available."
v0.13.1,"Cluster id -> List of (start_index, end_index) spans."
v0.13.1,Cluster id -> List of start_indices which are open for this id.
v0.13.1,Replace brackets in text and pos tags
v0.13.1,with a different token for parse trees.
v0.13.1,only keep ')' if there are nested brackets with nothing in them.
v0.13.1,There are some bad annotations in the CONLL data.
v0.13.1,"They contain no information, so to make this explicit,"
v0.13.1,we just set the parse piece to be None which will result
v0.13.1,in the overall parse tree being None.
v0.13.1,"If this is the first word in the sentence, create"
v0.13.1,empty lists to collect the NER and SRL BIO labels.
v0.13.1,"We can't do this upfront, because we don't know how many"
v0.13.1,"components we are collecting, as a sentence can have"
v0.13.1,variable numbers of SRL frames.
v0.13.1,Create variables representing the current label for each label
v0.13.1,sequence we are collecting.
v0.13.1,"If any annotation marks this word as a verb predicate,"
v0.13.1,we need to record its index. This also has the side effect
v0.13.1,of ordering the verbal predicates by their location in the
v0.13.1,"sentence, automatically aligning them with the annotations."
v0.13.1,"this would not be reached if parse_pieces contained None, hence the cast"
v0.13.1,Non-empty line. Collect the annotation.
v0.13.1,Collect any stragglers or files which might not
v0.13.1,have the '#end document' format for the end of the file.
v0.13.1,this dataset name
v0.13.1,check if data there
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,check if data there
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,download files if not present locally
v0.13.1,we need to slightly modify the original files by adding some new lines after document separators
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,Remove CoNLL-U meta information in the last column
v0.13.1,column format
v0.13.1,dataset name
v0.13.1,data folder: default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,dataset name
v0.13.1,data folder: default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,entity_mapping
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,data validation
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download files if not present locallys
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,# download zip
v0.13.1,merge the files in one as the zip is containing multiples files
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,check if data there
v0.13.1,create folder
v0.13.1,download dataset
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download and parse data if necessary
v0.13.1,create train test dev if not exist
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,If the extracted corpus file is not yet present in dir
v0.13.1,download zip if necessary
v0.13.1,"extracted corpus is not present , so unpacking it."
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download zip
v0.13.1,unpacking the zip
v0.13.1,merge the files in one as the zip is containing multiples files
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.13.1,download files if not present locally
v0.13.1,we need to modify the original files by adding new lines after after the end of each sentence
v0.13.1,if only one language is given
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,"use all languages if explicitly set to ""all"""
v0.13.1,download data if necessary
v0.13.1,initialize comlumncorpus and add it to list
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,"For each language in languages, the file is downloaded if not existent"
v0.13.1,Then a comlumncorpus of that data is created and saved in a list
v0.13.1,this list is handed to the multicorpus
v0.13.1,list that contains the columncopora
v0.13.1,download data if necessary
v0.13.1,"if language not downloaded yet, download it"
v0.13.1,create folder
v0.13.1,get google drive id from list
v0.13.1,download from google drive
v0.13.1,unzip
v0.13.1,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.13.1,transform data into required format
v0.13.1,"the processed dataset has the additional ending ""_new"""
v0.13.1,remove the unprocessed dataset
v0.13.1,initialize comlumncorpus and add it to list
v0.13.1,if no languages are given as argument all languages used in XTREME will be loaded
v0.13.1,if only one language is given
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,"For each language in languages, the file is downloaded if not existent"
v0.13.1,Then a comlumncorpus of that data is created and saved in a list
v0.13.1,This list is handed to the multicorpus
v0.13.1,list that contains the columncopora
v0.13.1,download data if necessary
v0.13.1,"if language not downloaded yet, download it"
v0.13.1,create folder
v0.13.1,download from HU Server
v0.13.1,unzip
v0.13.1,transform data into required format
v0.13.1,initialize comlumncorpus and add it to list
v0.13.1,if only one language is given
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,initialize comlumncorpus and add it to list
v0.13.1,download data if necessary
v0.13.1,unpack and write out in CoNLL column-like format
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,data is not in IOB2 format. Thus we transform it to IOB2
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,rename according to train - test - dev - convention
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,Add missing newline after header
v0.13.1,Workaround for empty tokens
v0.13.1,"Add ""real"" document marker"
v0.13.1,Dataset split mapping
v0.13.1,v2.0 only adds new language and splits for AJMC dataset
v0.13.1,Special document marker for sample splits in AJMC dataset
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,if only one language is given
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,"use all languages if explicitly set to ""all"""
v0.13.1,download data if necessary
v0.13.1,initialize comlumncorpus and add it to list
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download and parse data if necessary
v0.13.1,paths to train and test splits
v0.13.1,init corpus
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download and parse data if necessary
v0.13.1,iterate over all html files
v0.13.1,"get rid of html syntax, we only need the text"
v0.13.1,between all documents we write a separator symbol
v0.13.1,skip empty strings
v0.13.1,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.13.1,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.13.1,sentence splitting and tokenization
v0.13.1,iterate through all annotations and add to corresponding tokens
v0.13.1,find sentence to which annotation belongs
v0.13.1,position within corresponding sentence
v0.13.1,set annotation for tokens of entity mention
v0.13.1,write to out-file in column format
v0.13.1,"in case something goes wrong, delete the dataset and raise error"
v0.13.1,this dataset name
v0.13.1,download and parse data if necessary
v0.13.1,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.13.1,generate qid wikiname dictionaries
v0.13.1,merge dictionaries
v0.13.1,ignore first line
v0.13.1,commented and empty lines
v0.13.1,read all Q-IDs
v0.13.1,ignore first line
v0.13.1,request
v0.13.1,this dataset name
v0.13.1,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.13.1,like this we can quickly check if the corresponding page exists
v0.13.1,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.13.1,delete unprocessed file
v0.13.1,collect all wikiids
v0.13.1,create the dictionary
v0.13.1,request
v0.13.1,this dataset name
v0.13.1,names of raw text documents
v0.13.1,open output_file
v0.13.1,iterate through all documents
v0.13.1,split sentences and tokenize
v0.13.1,iterate through all annotations and add to corresponding tokens
v0.13.1,find sentence to which annotation belongs
v0.13.1,position within corresponding sentence
v0.13.1,set annotation for tokens of entity mention
v0.13.1,write to out file
v0.13.1,annotation from one annotator or two agreeing annotators
v0.13.1,this dataset name
v0.13.1,download and parse data if necessary
v0.13.1,this dataset name
v0.13.1,download and parse data if necessary
v0.13.1,First parse the post titles
v0.13.1,Keep track of how many and which entity mentions does a given post title have
v0.13.1,Check if the current post title has an entity link and parse accordingly
v0.13.1,Post titles with entity mentions (if any) are handled via this function
v0.13.1,Then parse the comments
v0.13.1,"Iterate over the comments.tsv file, until the end is reached"
v0.13.1,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.13.1,Each comment thread is handled as one 'document'.
v0.13.1,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.13.1,This if-condition is needed to handle this problem.
v0.13.1,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.13.1,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.13.1,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.13.1,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.13.1,and not just single letters into single rows.
v0.13.1,If there are annotated entity mentions for given post title or a comment thread
v0.13.1,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.13.1,Write the token with a corresponding tag to file
v0.13.1,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.13.1,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.13.1,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.13.1,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.13.1,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.13.1,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.13.1,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.13.1,Check if further annotations belong to the current post title or comment thread as well
v0.13.1,Stop when the end of an annotation file is reached
v0.13.1,Check if further annotations belong to the current sentence as well
v0.13.1,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.13.1,Docstart
v0.13.1,if there is more than one word in the chunk we write each in a separate line
v0.13.1,print(chunks)
v0.13.1,empty line after each sentence
v0.13.1,convert the file to CoNLL
v0.13.1,this dataset name
v0.13.1,"check if data there, if not, download the data"
v0.13.1,create folder
v0.13.1,download data
v0.13.1,transform data into column format if necessary
v0.13.1,if no filenames are specified we use all the data
v0.13.1,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.13.1,also we remove 'raganato_ALL' from filenames in case its in the list
v0.13.1,generate the test file
v0.13.1,make column file and save to data_folder
v0.13.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.1,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.1,create folder
v0.13.1,download data
v0.13.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.1,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.1,create folder
v0.13.1,download data
v0.13.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.1,generate the test file
v0.13.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.1,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.1,create folder
v0.13.1,download data
v0.13.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.1,generate the test file
v0.13.1,default dataset folder is the cache root
v0.13.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.1,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.1,create folder
v0.13.1,download data
v0.13.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.1,generate the test file
v0.13.1,default dataset folder is the cache root
v0.13.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.1,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.1,create folder
v0.13.1,download data
v0.13.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.1,generate the test file
v0.13.1,default dataset folder is the cache root
v0.13.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.1,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.1,create folder
v0.13.1,download data
v0.13.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.1,generate the test file
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,if True:
v0.13.1,write CoNLL-U Plus header
v0.13.1,"Some special cases (e.g., missing spaces before entity marker)"
v0.13.1,necessary if text should be whitespace tokenizeable
v0.13.1,Handle case where tail may occur before the head
v0.13.1,this dataset name
v0.13.1,write CoNLL-U Plus header
v0.13.1,this dataset name
v0.13.1,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.13.1,download data if necessary
v0.13.1,write CoNLL-U Plus header
v0.13.1,The span has ended.
v0.13.1,We are entering a new span; reset indices
v0.13.1,and active tag to new span.
v0.13.1,We're inside a span.
v0.13.1,Last token might have been a part of a valid span.
v0.13.1,this dataset name
v0.13.1,write CoNLL-U Plus header
v0.13.1,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.13.1,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.13.1,target_file_path = Path(data_folder) / target_filename
v0.13.1,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.13.1,# write CoNLL-U Plus header
v0.13.1,"target_file.write(""# global.columns = id form ner\n"")"
v0.13.1,for example in json.load(source_file):
v0.13.1,token_list = self._tacred_example_to_token_list(example)
v0.13.1,target_file.write(token_list.serialize())
v0.13.1,check if first tag row is already occupied
v0.13.1,"if first tag row is occupied, use second tag row"
v0.13.1,hardcoded mapping TODO: perhaps find nicer solution
v0.13.1,remap regular tag names
v0.13.1,else skip to position in file where sentence begins
v0.13.1,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.13.1,read in dev file if exists
v0.13.1,read in test file if exists
v0.13.1,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.13.1,"find train, dev and test files if not specified"
v0.13.1,use test_file to create test split if available
v0.13.1,use dev_file to create test split if available
v0.13.1,"if data point contains black-listed label, do not use"
v0.13.1,first check if valid sentence
v0.13.1,"if so, add to indices"
v0.13.1,"find train, dev and test files if not specified"
v0.13.1,variables
v0.13.1,different handling of in_memory data than streaming data
v0.13.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.13.1,test if format is OK
v0.13.1,test if at least one label given
v0.13.1,make sentence from text (and filter for length)
v0.13.1,"if a pair column is defined, make a sentence pair object"
v0.13.1,noinspection PyDefaultArgument
v0.13.1,dataset name includes the split size
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download each of the 28 splits
v0.13.1,create dataset directory if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,download data from same source as in huggingface's implementations
v0.13.1,read label order
v0.13.1,"Original labels are [1, 2, 3, 4] -> ['World', 'Sports', 'Business', 'Sci/Tech']"
v0.13.1,"Re-map to [0, 1, 2, 3]."
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,handle labels file
v0.13.1,handle data file
v0.13.1,Create flair compatible labels
v0.13.1,"by default, map point score to POSITIVE / NEGATIVE values"
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,create dataset directory if necessary
v0.13.1,create train.txt file from CSV
v0.13.1,create test.txt file from CSV
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,create dataset directory if necessary
v0.13.1,create train.txt file by iterating over pos and neg file
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,create dataset directory if necessary
v0.13.1,create train.txt file by iterating over pos and neg file
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,create dataset directory if necessary
v0.13.1,create train.txt file by iterating over pos and neg file
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,create dataset directory if necessary
v0.13.1,create train.txt file by iterating over pos and neg file
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,create train dev and test files in fasttext format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download senteval datasets if necessary und unzip
v0.13.1,convert to FastText format
v0.13.1,download data if necessary
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,move original .tsv files to another folder
v0.13.1,create train and dev splits in fasttext format
v0.13.1,create eval_dataset file with no labels
v0.13.1,download zip archive
v0.13.1,unpack file in datasets directory (zip archive contains a directory named SST-2)
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,download datasets if necessary
v0.13.1,create dataset directory if necessary
v0.13.1,create correctly formated txt files
v0.13.1,multiple labels are possible
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,Create flair compatible labels
v0.13.1,TREC-6 : NUM:dist -> __label__NUM
v0.13.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,Create flair compatible labels
v0.13.1,TREC-6 : NUM:dist -> __label__NUM
v0.13.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,create a separate directory for different tasks
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,check if dataset is supported
v0.13.1,set file names
v0.13.1,set file names
v0.13.1,download and unzip in file structure if necessary
v0.13.1,instantiate corpus
v0.13.1,"find train, dev and test files if not specified"
v0.13.1,"create DataPairDataset for train, test and dev file, if they are given"
v0.13.1,stop if file does not exist
v0.13.1,create a DataPair object from strings
v0.13.1,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,"rename test file to eval_dataset, since it has no labels"
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.13.1,dev sets include 5 different annotations but we will only keep the gold label
v0.13.1,"rename test file to eval_dataset, since it has no labels"
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get test and dev sets
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,"rename test file to eval_dataset, since it has no labels"
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,"rename test file to eval_dataset, since it has no labels"
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,"rename test file to eval_dataset, since it has no labels"
v0.13.1,"if data is not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,"rename test file to eval_dataset, since it has no labels"
v0.13.1,"if data not downloaded yet, download it"
v0.13.1,get the zip file
v0.13.1,"the downloaded files have json format, we transform them to tsv"
v0.13.1,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.13.1,remove json file
v0.13.1,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.13.1,with sum of all entity lengths as secondary key
v0.13.1,calculate offset without current text
v0.13.1,because we stick all passages of a document together
v0.13.1,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.13.1,Try to fix incorrect annotations
v0.13.1,print(
v0.13.1,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.13.1,)
v0.13.1,Ignore empty lines or relation annotations
v0.13.1,FIX annotation of whitespaces (necessary for PDR)
v0.13.1,One token may contain multiple entities -> deque all of them
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.13.1,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,Edge case: last token starts a new entity
v0.13.1,Last document in file
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,In the huner split files there is no information whether a given id originates
v0.13.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.13.1,splitting here
v0.13.1,In the huner split files there is no information whether a given id originates
v0.13.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.13.1,splitting here
v0.13.1,In the huner split files there is no information whether a given id originates
v0.13.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.13.1,splitting here
v0.13.1,Edge case: last token starts a new entity
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,Read texts
v0.13.1,Read annotations
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,We need to apply a patch to correct the original training file
v0.13.1,Articles title
v0.13.1,Article abstract
v0.13.1,Entity annotations
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,Edge case: last token starts a new entity
v0.13.1,Map all entities to chemicals
v0.13.1,Map all entities to disease
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,Incomplete article
v0.13.1,Invalid XML syntax
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,if len(mid) != 3:
v0.13.1,continue
v0.13.1,Try to fix entity offsets
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,There is still one illegal annotation in the file ..
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,"Abstract first, title second to prevent issues with sentence splitting"
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,"Filter for specific entity types, by default no entities will be filtered"
v0.13.1,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.13.1,train and dev split of V2 will be train in V4
v0.13.1,test split of V2 will be dev in V4
v0.13.1,New documents in V4 will become test documents
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,column format
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,column format
v0.13.1,build dataset name and full huggingface reference name
v0.13.1,Download data if necessary
v0.13.1,"Some datasets in BigBio only have train or test splits, not both"
v0.13.1,"If only test split, assign it to train split"
v0.13.1,"If only train split, sample other from it (sample_missing_splits=True)"
v0.13.1,Not every dataset has a dev / validation set!
v0.13.1,Perform type mapping if necessary
v0.13.1,"Collect all texts of the document, each passage will be"
v0.13.1,a text in our internal format
v0.13.1,Sort passages by start offset
v0.13.1,Transform all entity annotations into internal format
v0.13.1,Find the passage of the entity (necessary for offset adaption)
v0.13.1,Adapt entity offsets according to passage offsets
v0.13.1,FIXME: This is just for debugging purposes
v0.13.1,passage_text = id_to_text[passage_id]
v0.13.1,doc_text = passage_text[entity_offset[0] : entity_offset[1]]
v0.13.1,"mention_text = entity[""text""][0]"
v0.13.1,if doc_text != mention_text:
v0.13.1,"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
v0.13.1,Check base case
v0.13.1,Get element in the middle
v0.13.1,Is the mention with the passage offsets?
v0.13.1,"If element is smaller than mid, then it can only"
v0.13.1,be present in left subarray
v0.13.1,Else the element can only be present in right subarray
v0.13.1,Special case for ProGene: We need to use the split_0_train and split_0_test splits
v0.13.1,as they are currently provided in BigBio
v0.13.1,cache Feidegger config file
v0.13.1,cache Feidegger images
v0.13.1,replace image URL with local cached file
v0.13.1,append Sentence-Image data point
v0.13.1,cast to list if necessary
v0.13.1,cast to list if necessary
v0.13.1,"first, check if pymongo is installed"
v0.13.1,automatically identify train / test / dev files
v0.13.1,"if no test file is found, take any file with 'test' in name"
v0.13.1,Expose base classses
v0.13.1,Expose all biomedical data sets used for the evaluation of BioBERT
v0.13.1,-
v0.13.1,-
v0.13.1,-
v0.13.1,-
v0.13.1,Expose all biomedical data sets using the HUNER splits
v0.13.1,Expose all biomedical data sets
v0.13.1,Expose all document classification datasets
v0.13.1,word sense disambiguation
v0.13.1,Expose all entity linking datasets
v0.13.1,Expose all relation extraction datasets
v0.13.1,universal proposition banks
v0.13.1,keyphrase detection datasets
v0.13.1,other NER datasets
v0.13.1,standard NER datasets
v0.13.1,Expose all sequence labeling datasets
v0.13.1,Expose all text-image datasets
v0.13.1,Expose all text-text datasets
v0.13.1,Expose all treebanks
v0.13.1,"find train, dev and test files if not specified"
v0.13.1,get train data
v0.13.1,get test data
v0.13.1,get dev data
v0.13.1,option 1: read only sentence boundaries as offset positions
v0.13.1,option 2: keep everything in memory
v0.13.1,"if in memory, retrieve parsed sentence"
v0.13.1,else skip to position in file where sentence begins
v0.13.1,current token ID
v0.13.1,handling for the awful UD multiword format
v0.13.1,end of sentence
v0.13.1,comments or ellipsis
v0.13.1,if token is a multi-word
v0.13.1,normal single-word tokens
v0.13.1,"if we don't split multiwords, skip over component words"
v0.13.1,add token
v0.13.1,add morphological tags
v0.13.1,derive whitespace logic for multiwords
v0.13.1,print(token)
v0.13.1,print(current_multiword_last_token)
v0.13.1,print(current_multiword_first_token)
v0.13.1,"if multi-word equals component tokens, there should be no whitespace"
v0.13.1,go through all tokens in subword and set whitespace_after information
v0.13.1,print(i)
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,download data if necessary
v0.13.1,this dataset name
v0.13.1,default dataset folder is the cache root
v0.13.1,download data if necessary
v0.13.1,"finally, print model card for information"
v0.13.1,noqa: INP001
v0.13.1,-- Project information -----------------------------------------------------
v0.13.1,"The full version, including alpha/beta/rc tags"
v0.13.1,use smv_current_version as the git url
v0.13.1,-- General configuration ---------------------------------------------------
v0.13.1,"Add any Sphinx extension module names here, as strings. They can be"
v0.13.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.13.1,ones.
v0.13.1,"Add any paths that contain templates here, relative to this directory."
v0.13.1,"List of patterns, relative to source directory, that match files and"
v0.13.1,directories to ignore when looking for source files.
v0.13.1,This pattern also affects html_static_path and html_extra_path.
v0.13.1,-- Options for HTML output -------------------------------------------------
v0.13.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.13.1,a list of builtin themes.
v0.13.1,
v0.13.1,"Add any paths that contain custom static files (such as style sheets) here,"
v0.13.1,"relative to this directory. They are copied after the builtin static files,"
v0.13.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.13.1,Napoleon settings
v0.13.1,Whitelist pattern for tags (set to None to ignore all tags)
v0.13.1,Whitelist pattern for branches (set to None to ignore all branches)
v0.13.1,Whitelist pattern for remotes (set to None to use local branches only)
v0.13.1,Pattern for released versions
v0.13.1,Format for versioned output directories inside the build directory
v0.13.1,Determines whether remote or local git branches/tags are preferred if their output dirs conflict
v0.13.1,test corpus
v0.13.1,create a TARS classifier
v0.13.1,check if right number of classes
v0.13.1,switch to task with only one label
v0.13.1,check if right number of classes
v0.13.1,switch to task with three labels provided as list
v0.13.1,check if right number of classes
v0.13.1,switch to task with four labels provided as set
v0.13.1,check if right number of classes
v0.13.1,switch to task with two labels provided as Dictionary
v0.13.1,check if right number of classes
v0.13.1,test corpus
v0.13.1,create a TARS classifier
v0.13.1,switch to a new task (TARS can do multiple tasks so you must define one)
v0.13.1,initialize the text classifier trainer
v0.13.1,start the training
v0.13.1,"With end symbol, without start symbol, padding in front"
v0.13.1,"Without end symbol, with start symbol, padding in back"
v0.13.1,"Without end symbol, without start symbol, padding in front"
v0.13.1,initialize trainer
v0.13.1,initialize trainer
v0.13.1,initialize trainer
v0.13.1,increment for last token in sentence if not followed by whitespace
v0.13.1,clean up directory
v0.13.1,clean up directory
v0.13.1,example sentence
v0.13.1,set 4 labels for 2 tokens ('love' is tagged twice)
v0.13.1,check if there are three POS labels with correct text and values
v0.13.1,check if there are is one SENTIMENT label with correct text and values
v0.13.1,check if all tokens are correctly labeled
v0.13.1,remove the pos label from the last word
v0.13.1,there should be 2 POS labels left
v0.13.1,now remove all pos tags
v0.13.1,set 3 labels for 2 spans (HU is tagged twice)
v0.13.1,check if there are three labels with correct text and values
v0.13.1,check if there are two spans with correct text and values
v0.13.1,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.13.1,should be only one NER label left
v0.13.1,and only one NER span
v0.13.1,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.13.1,check if there are three labels with correct text and values
v0.13.1,check if there are two spans with correct text and values
v0.13.1,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.13.1,should be only one NER label left
v0.13.1,and only one NER span
v0.13.1,but there is also one orgtype span and label
v0.13.1,and only one NER span
v0.13.1,let's add the NER tag back
v0.13.1,check if there are three labels with correct text and values
v0.13.1,check if there are two spans with correct text and values
v0.13.1,now remove all NER tags
v0.13.1,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.13.1,create two relation label
v0.13.1,there should be two relation labels
v0.13.1,there should be one syntactic labels
v0.13.1,"there should be two relations, one with two and one with one label"
v0.13.1,example sentence
v0.13.1,add another topic label
v0.13.1,example sentence
v0.13.1,has sentiment value
v0.13.1,has 4 part of speech tags
v0.13.1,has 1 NER tag
v0.13.1,should be in total 6 labels
v0.13.1,example sentence
v0.13.1,add two NER labels
v0.13.1,get the four labels
v0.13.1,check that only two of the respective data points are equal
v0.13.1,make a sentence and some right context
v0.13.1,TODO: is this desirable? Or should two sentences with same text be considered same objects?
v0.13.1,Initializing a Sentence this way assumes that there is a space after each token
v0.13.1,get default dictionary
v0.13.1,init forward LM with 128 hidden states and 1 layer
v0.13.1,get the example corpus and process at character level in forward direction
v0.13.1,train the language model
v0.13.1,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.13.1,clean up results directory
v0.13.1,get default dictionary
v0.13.1,init forward LM with 128 hidden states and 1 layer
v0.13.1,get the example corpus and process at character level in forward direction
v0.13.1,train the language model
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,load column dataset with one entry
v0.13.1,load column dataset with two entries
v0.13.1,load column dataset with three entries
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,check if Token labels are correct
v0.13.1,"get training, test and dev data"
v0.13.1,check if Token labels for frames are correct
v0.13.1,"get training, test and dev data"
v0.13.1,"get training, test and dev data"
v0.13.1,get two corpora as one
v0.13.1,"get training, test and dev data for full English UD corpus from web"
v0.13.1,clean up data directory
v0.13.1,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.13.1,"""2"","
v0.13.1,"""0"","
v0.13.1,"""4"","
v0.13.1,"""2"","
v0.13.1,"""2"","
v0.13.1,"""2"","
v0.13.1,]
v0.13.1,This test only covers basic universal dependencies datasets.
v0.13.1,"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
v0.13.1,"Here, we use the default token annotation fields."
v0.13.1,This test covers the complete HIPE 2022 dataset.
v0.13.1,https://github.com/hipe-eval/HIPE-2022-data
v0.13.1,"Includes variant with document separator, and all versions of the dataset."
v0.13.1,"We have manually checked, that these numbers are correct:"
v0.13.1,"+1 offset, because of missing EOS marker at EOD"
v0.13.1,Test data for v2.1 release
v0.13.1,This test covers the complete ICDAR Europeana corpus:
v0.13.1,https://github.com/stefan-it/historic-domain-adaptation-icdar
v0.13.1,"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
v0.13.1,This test covers the NERMuD dataset. Official stats can be found here:
v0.13.1,https://github.com/dhfbk/KIND/tree/main/evalita-2023
v0.13.1,Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
v0.13.1,This test covers the complete MasakhaPOS dataset.
v0.13.1,"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
v0.13.1,--- Embeddings that are shared by both models --- #
v0.13.1,--- Task 1: Sentiment Analysis (5-class) --- #
v0.13.1,Define corpus and model
v0.13.1,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.13.1,Define corpus and model
v0.13.1,-- Define mapping (which tagger should train on which model) -- #
v0.13.1,-- Create model trainer and train -- #
v0.13.1,clean up file
v0.13.1,no need for label_dict
v0.13.1,check if right number of classes
v0.13.1,switch to task with only one label
v0.13.1,check if right number of classes
v0.13.1,switch to task with three labels provided as list
v0.13.1,check if right number of classes
v0.13.1,switch to task with four labels provided as set
v0.13.1,check if right number of classes
v0.13.1,switch to task with two labels provided as Dictionary
v0.13.1,check if right number of classes
v0.13.1,Intel ----founded_by---> Gordon Moore
v0.13.1,Intel ----founded_by---> Robert Noyce
v0.13.1,"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
v0.13.1,Check sentence masking and relation label annotation on
v0.13.1,"training, validation and test dataset (in this test the splits are the same)"
v0.13.1,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.13.1,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.13.1,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.13.1,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
v0.13.1,This sentence is only included if we transform the corpus with cross augmentation
v0.13.1,Ensure this is an example that predicts no classes in multilabel
v0.13.1,check if right number of classes
v0.13.1,switch to task with only one label
v0.13.1,check if right number of classes
v0.13.1,switch to task with three labels provided as list
v0.13.1,check if right number of classes
v0.13.1,switch to task with four labels provided as set
v0.13.1,check if right number of classes
v0.13.1,switch to task with two labels provided as Dictionary
v0.13.1,check if right number of classes
v0.13.1,ensure that the prepared tensors is what we expect
v0.13.1,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.13.1,previous and next sentence as context
v0.13.1,test expansion for sentence without context
v0.13.1,test expansion for with previous and next as context
v0.13.1,test expansion if first sentence is document boundary
v0.13.1,test expansion if we don't use context
v0.13.1,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.13.1,dummy model with embeddings
v0.13.1,save the dummy and load it again
v0.13.1,check that context_length and use_context_separator is the same for both
v0.13.0,mmap seems to be much more memory efficient
v0.13.0,Remove quotes from etag
v0.13.0,"If there is an etag, it's everything after the first period"
v0.13.0,"Otherwise, use None"
v0.13.0,"URL, so get it from the cache (downloading if necessary)"
v0.13.0,"File, and it exists."
v0.13.0,"File, but it doesn't exist."
v0.13.0,Something unknown
v0.13.0,Extract all the contents of zip file in current directory
v0.13.0,Extract all the contents of zip file in current directory
v0.13.0,TODO(joelgrus): do we want to do checksums or anything like that?
v0.13.0,get cache path to put the file
v0.13.0,make HEAD request to check ETag
v0.13.0,add ETag to filename if it exists
v0.13.0,"etag = response.headers.get(""ETag"")"
v0.13.0,"Download to temporary file, then copy to cache dir once finished."
v0.13.0,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.13.0,GET file object
v0.13.0,These defaults are the same as the argument defaults in tqdm.
v0.13.0,load_big_file is a workaround byhttps://github.com/highway11git
v0.13.0,to load models on some Mac/Windows setups
v0.13.0,see https://github.com/zalandoresearch/flair/issues/351
v0.13.0,first determine the distribution of classes in the dataset
v0.13.0,weight for each sample
v0.13.0,Create blocks
v0.13.0,shuffle the blocks
v0.13.0,concatenate the shuffled blocks
v0.13.0,Create blocks
v0.13.0,shuffle the blocks
v0.13.0,concatenate the shuffled blocks
v0.13.0,increment for last token in sentence if not followed by whitespace
v0.13.0,this is the default init size of a lmdb database for embeddings
v0.13.0,get db filename from embedding name
v0.13.0,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.13.0,SequenceTagger
v0.13.0,TextClassifier
v0.13.0,get db filename from embedding name
v0.13.0,if embedding database already exists
v0.13.0,"otherwise, push embedding to database"
v0.13.0,if embedding database already exists
v0.13.0,open the database in read mode
v0.13.0,we need to set self.k
v0.13.0,create and load the database in write mode
v0.13.0,"no idea why, but we need to close and reopen the environment to avoid"
v0.13.0,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.13.0,when opening new transaction !
v0.13.0,init dictionaries
v0.13.0,"in order to deal with unknown tokens, add <unk>"
v0.13.0,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.13.0,set 'add_unk' depending on whether <unk> is a key
v0.13.0,"if one embedding name, directly return it"
v0.13.0,"if multiple embedding names, concatenate them"
v0.13.0,First we remove any existing labels for this PartOfSentence in self.sentence
v0.13.0,labels also need to be deleted at Sentence object
v0.13.0,delete labels at object itself
v0.13.0,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.13.0,"therefore, labels get added only to the Sentence if it exists"
v0.13.0,The Token is a special _PartOfSentence in that it may be initialized without a Sentence.
v0.13.0,"Therefore, labels get set only to the Sentence if it exists"
v0.13.0,"check if the span already exists. If so, return it"
v0.13.0,else make a new span
v0.13.0,"check if the relation already exists. If so, return it"
v0.13.0,else make a new relation
v0.13.0,private field for all known spans
v0.13.0,the tokenizer used for this sentence
v0.13.0,some sentences represent a document boundary (but most do not)
v0.13.0,internal variables to denote position inside dataset
v0.13.0,"if text is passed, instantiate sentence with tokens (words)"
v0.13.0,determine token positions and whitespace_after flag
v0.13.0,the last token has no whitespace after
v0.13.0,log a warning if the dataset is empty
v0.13.0,data with zero-width characters cannot be handled
v0.13.0,set token idx and sentence
v0.13.0,append token to sentence
v0.13.0,register token annotations on sentence
v0.13.0,move sentence embeddings to device
v0.13.0,also move token embeddings to device
v0.13.0,clear token embeddings
v0.13.0,infer whitespace after field
v0.13.0,"if sentence has no tokens, return empty string"
v0.13.0,"otherwise, return concatenation of tokens with the correct offsets"
v0.13.0,The sentence's start position is not propagated to its tokens.
v0.13.0,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.13.0,No character at the corresponding code point: remove it
v0.13.0,"if no label if specified, return all labels"
v0.13.0,"if the label type exists in the Sentence, return it"
v0.13.0,return empty list if none of the above
v0.13.0,labels also need to be deleted at all tokens
v0.13.0,labels also need to be deleted at all known spans
v0.13.0,remove spans without labels
v0.13.0,delete labels at object itself
v0.13.0,set name
v0.13.0,abort if no data is provided
v0.13.0,sample test data from train if none is provided
v0.13.0,sample dev data from train if none is provided
v0.13.0,set train dev and test data
v0.13.0,find out empty sentence indices
v0.13.0,create subset of non-empty sentence indices
v0.13.0,find out empty sentence indices
v0.13.0,create subset of non-empty sentence indices
v0.13.0,"first, determine the datapoint type by going through dataset until first label is found"
v0.13.0,count all label types per sentence
v0.13.0,go through all labels of label_type and count values
v0.13.0,special handling for Token-level annotations. Add all untagged as 'O' label
v0.13.0,"if an unk threshold is set, UNK all label values below this threshold"
v0.13.0,sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
v0.13.0,replace the old label with the new one
v0.13.0,keep track of the old (clean) label using another label type category
v0.13.0,keep track of how many labels in total are flipped
v0.13.0,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.13.0,replace the old label with the new one
v0.13.0,keep track of the old (clean) label using another label type category
v0.13.0,keep track of how many labels in total are flipped
v0.13.0,"add a dummy ""O"" to close final prediction"
v0.13.0,return complex list
v0.13.0,internal variables
v0.13.0,non-set tags are OUT tags
v0.13.0,anything that is not OUT is IN
v0.13.0,does this prediction start a new span?
v0.13.0,B- and S- always start new spans
v0.13.0,"if the predicted class changes, I- starts a new span"
v0.13.0,"if the predicted class changes and S- was previous tag, start a new span"
v0.13.0,if an existing span is ended (either by reaching O or starting a new span)
v0.13.0,determine score and value
v0.13.0,append to result list
v0.13.0,reset for-loop variables for new span
v0.13.0,remember previous tag
v0.13.0,global variable: cache_root
v0.13.0,global variable: device
v0.13.0,"No need for correctness checks, torch is doing it"
v0.13.0,global variable: version
v0.13.0,global variable: arrow symbol
v0.13.0,dummy return to fulfill trainer.train() needs
v0.13.0,print(vec)
v0.13.0,Attach optimizer
v0.13.0,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.13.0,if memory mode option 'none' delete everything
v0.13.0,"if dynamic embedding keys not passed, identify them automatically"
v0.13.0,always delete dynamic embeddings
v0.13.0,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.13.0,optional metric space decoder if prototypes have different length than embedding
v0.13.0,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.13.0,"if set, create initial prototypes from normal distribution"
v0.13.0,"if set, use a radius"
v0.13.0,all parameters will be pushed internally to the specified device
v0.13.0,decode embeddings into prototype space
v0.13.0,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.13.0,verbalize BIOES labels
v0.13.0,"if label is not BIOES, use label itself"
v0.13.0,Always include the name of the Model class for which the state dict holds
v0.13.0,"write out a ""model card"" if one is set"
v0.13.0,save model
v0.13.0,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.13.0,get all non-abstract subclasses
v0.13.0,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.13.0,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.13.0,"if the model cannot be fetched, load as a file"
v0.13.0,try to get model class from state
v0.13.0,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.13.0,"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
v0.13.0,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.13.0,"if this class is not abstract, fetch the model and load it"
v0.13.0,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.13.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.13.0,loss calculation
v0.13.0,variables for printing
v0.13.0,variables for computing scores
v0.13.0,remove any previously predicted labels
v0.13.0,predict for batch
v0.13.0,get the gold labels
v0.13.0,add to all_predicted_values
v0.13.0,make printout lines
v0.13.0,convert true and predicted values to two span-aligned lists
v0.13.0,delete exluded labels if exclude_labels is given
v0.13.0,"if after excluding labels, no label is left, ignore the datapoint"
v0.13.0,write all_predicted_values to out_file if set
v0.13.0,make the evaluation dictionary
v0.13.0,check if this is a multi-label problem
v0.13.0,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.13.0,multi-label problems require a multi-hot vector for each true and predicted label
v0.13.0,single-label problems can do with a single index for each true and predicted label
v0.13.0,"now, calculate evaluation numbers"
v0.13.0,there is at least one gold label or one prediction (default)
v0.13.0,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.13.0,"micro average is only computed if zero-label exists (for instance ""O"")"
v0.13.0,if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
v0.13.0,same for the main score
v0.13.0,issue error and default all evaluation numbers to 0.
v0.13.0,check if there is a label mismatch
v0.13.0,print info
v0.13.0,set the embeddings
v0.13.0,initialize the label dictionary
v0.13.0,initialize the decoder
v0.13.0,set up multi-label logic
v0.13.0,init dropouts
v0.13.0,loss weights and loss function
v0.13.0,Initialize the weight tensor
v0.13.0,set up gradient reversal if so specified
v0.13.0,embed sentences
v0.13.0,get a tensor of data points
v0.13.0,do dropout
v0.13.0,make a forward pass to produce embedded data points and labels
v0.13.0,get the data points for which to predict labels
v0.13.0,get their gold labels as a tensor
v0.13.0,pass data points through network to get encoded data point tensor
v0.13.0,decode
v0.13.0,an optional masking step (no masking in most cases)
v0.13.0,calculate the loss
v0.13.0,filter empty sentences
v0.13.0,reverse sort all sequences by their length
v0.13.0,progress bar for verbosity
v0.13.0,filter data points in batch
v0.13.0,stop if all sentences are empty
v0.13.0,pass data points through network and decode
v0.13.0,if anything could possibly be predicted
v0.13.0,remove previously predicted labels of this type
v0.13.0,filter data points that have labels outside of dictionary
v0.13.0,add DefaultClassifier arguments
v0.13.0,add variables of DefaultClassifier
v0.13.0,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.13.0,Get projected 1st dimension
v0.13.0,Compute bilinear form
v0.13.0,Arcosh
v0.13.0,Project the input data to n+1 dimensions
v0.13.0,"The first dimension, is recomputed in the distance module"
v0.13.0,header for 'weights.txt'
v0.13.0,"determine the column index of loss, f-score and accuracy for"
v0.13.0,"train, dev and test split"
v0.13.0,then get all relevant values from the tsv
v0.13.0,then get all relevant values from the tsv
v0.13.0,plot i
v0.13.0,save plots
v0.13.0,save plots
v0.13.0,plt.show()
v0.13.0,save plot
v0.13.0,auto-spawn on GPU if available
v0.13.0,progress bar for verbosity
v0.13.0,stop if all sentences are empty
v0.13.0,clearing token embeddings to save memory
v0.13.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.13.0,TODO: not saving lines yet
v0.13.0,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.13.0,- MaskedRelationClassifier ?
v0.13.0,This depends if this relation classification architecture should replace or offer as an alternative.
v0.13.0,Set label type and prepare label dictionary
v0.13.0,Initialize super default classifier
v0.13.0,Add the special tokens from the encoding strategy
v0.13.0,"Auto-spawn on GPU, if available"
v0.13.0,Only use entities labelled with the specified labels for each label type
v0.13.0,Only use entities above the specified threshold
v0.13.0,Use a dictionary to find gold relation annotations for a given entity pair
v0.13.0,Yield head and tail entity pairs from the cross product of all entities
v0.13.0,Remove identity relation entity pairs
v0.13.0,Remove entity pairs with labels that do not match any
v0.13.0,of the specified relations in `self.entity_pair_labels`
v0.13.0,"Obtain gold label, if existing"
v0.13.0,Some sanity checks
v0.13.0,Pre-compute non-leading head and tail tokens for entity masking
v0.13.0,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.13.0,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.13.0,"Therefore, we use the span's position in the sentence."
v0.13.0,Create masked sentence
v0.13.0,Add gold relation annotation as sentence label
v0.13.0,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.13.0,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.13.0,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.13.0,"may get distributed into different splits. For training purposes, this is always undesired."
v0.13.0,Ensure that all sentences are encoded properly
v0.13.0,Deal with the case where all sentences are encoded sentences
v0.13.0,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.13.0,Deal with the case where all sentences are standard (non-encoded) sentences
v0.13.0,"For each encoded sentence, transfer its prediction onto the original relation"
v0.13.0,auto-spawn on GPU if available
v0.13.0,pad strings with whitespaces to longest sentence
v0.13.0,cut up the input into chunks of max charlength = chunk_size
v0.13.0,push each chunk through the RNN language model
v0.13.0,concatenate all chunks to make final output
v0.13.0,initial hidden state
v0.13.0,get predicted weights
v0.13.0,divide by temperature
v0.13.0,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.13.0,this makes a vector in which the largest value is 0
v0.13.0,compute word weights with exponential function
v0.13.0,try sampling multinomial distribution for next character
v0.13.0,print(word_idx)
v0.13.0,input ids
v0.13.0,push list of character IDs through model
v0.13.0,the target is always the next character
v0.13.0,use cross entropy loss to compare output of forward pass with targets
v0.13.0,exponentiate cross-entropy loss to calculate perplexity
v0.13.0,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.13.0,serialize the language models and the constructor arguments (but nothing else)
v0.13.0,special handling for deserializing language models
v0.13.0,re-initialize language model with constructor arguments
v0.13.0,copy over state dictionary to self
v0.13.0,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.13.0,"in their ""self.train()"" method)"
v0.13.0,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.13.0,"check if this is the case and if so, set it"
v0.13.0,Transform input data into TARS format
v0.13.0,"if there are no labels, return a random sample as negatives"
v0.13.0,"otherwise, go through all labels"
v0.13.0,make sure the probabilities always sum up to 1
v0.13.0,get and embed all labels by making a Sentence object that contains only the label text
v0.13.0,get each label embedding and scale between 0 and 1
v0.13.0,compute similarity matrix
v0.13.0,"the higher the similarity, the greater the chance that a label is"
v0.13.0,sampled as negative example
v0.13.0,make label dictionary if no Dictionary object is passed
v0.13.0,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.13.0,check if candidate_label_set is empty
v0.13.0,make list if only one candidate label is passed
v0.13.0,create label dictionary
v0.13.0,note current task
v0.13.0,create a temporary task
v0.13.0,make zero shot predictions
v0.13.0,switch to the pre-existing task
v0.13.0,prepare TARS dictionary
v0.13.0,initialize a bare-bones sequence tagger
v0.13.0,transformer separator
v0.13.0,Store task specific labels since TARS can handle multiple tasks
v0.13.0,make a tars sentence where all labels are O by default
v0.13.0,init new TARS classifier
v0.13.0,set all task information
v0.13.0,progress bar for verbosity
v0.13.0,stop if all sentences are empty
v0.13.0,always remove tags first
v0.13.0,go through each sentence in the batch
v0.13.0,always remove tags first
v0.13.0,get the span and its label
v0.13.0,determine whether tokens in this span already have a label
v0.13.0,only add if all tokens have no label
v0.13.0,make and add a corresponding predicted span
v0.13.0,set indices so that no token can be tagged twice
v0.13.0,clearing token embeddings to save memory
v0.13.0,"all labels default to ""O"""
v0.13.0,set gold token-level
v0.13.0,set predicted token-level
v0.13.0,now print labels in CoNLL format
v0.13.0,prepare TARS dictionary
v0.13.0,initialize a bare-bones sequence tagger
v0.13.0,transformer separator
v0.13.0,Store task specific labels since TARS can handle multiple tasks
v0.13.0,get the serialized embeddings
v0.13.0,remap state dict for models serialized with Flair <= 0.11.3
v0.13.0,init new TARS classifier
v0.13.0,set all task information
v0.13.0,with torch.no_grad():
v0.13.0,progress bar for verbosity
v0.13.0,stop if all sentences are empty
v0.13.0,always remove tags first
v0.13.0,go through each sentence in the batch
v0.13.0,always remove tags first
v0.13.0,add all labels that according to TARS match the text and are above threshold
v0.13.0,do not add labels below confidence threshold
v0.13.0,only use label with the highest confidence if enforcing single-label predictions
v0.13.0,add the label with the highest score even if below the threshold if force label is activated.
v0.13.0,remove previously added labels and only add the best label
v0.13.0,clearing token embeddings to save memory
v0.13.0,set separator to concatenate two sentences
v0.13.0,auto-spawn on GPU if available
v0.13.0,pooling operation to get embeddings for entites
v0.13.0,set embeddings
v0.13.0,set relation and entity label types
v0.13.0,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.13.0,filter entity pairs according to their tags if set
v0.13.0,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.13.0,character dictionary for decoding and encoding
v0.13.0,make sure <unk> is in dictionary for handling of unknown characters
v0.13.0,add special symbols to dictionary if necessary and save respective indices
v0.13.0,---- ENCODER ----
v0.13.0,encoder character embeddings
v0.13.0,encoder pre-trained embeddings
v0.13.0,encoder RNN
v0.13.0,additional encoder linear layer if bidirectional encoding
v0.13.0,---- DECODER ----
v0.13.0,decoder: linear layers to transform vectors to and from alphabet_size
v0.13.0,when using attention we concatenate attention outcome and decoder hidden states
v0.13.0,decoder RNN
v0.13.0,loss and softmax
v0.13.0,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.13.0,add additional columns for special symbols if necessary
v0.13.0,initialize with dummy symbols
v0.13.0,encode inputs
v0.13.0,get labels (we assume each token has a lemma label)
v0.13.0,get char indices for labels of sentence
v0.13.0,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.13.0,max_sequence_length = length of longest label of sentence + 1
v0.13.0,get char embeddings
v0.13.0,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.13.0,take decoder input and initial hidden and pass through RNN
v0.13.0,"if all encoder outputs are provided, use attention"
v0.13.0,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.13.0,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.13.0,get all tokens
v0.13.0,encode input characters by sending them through RNN
v0.13.0,get one-hots for characters and add special symbols / padding
v0.13.0,determine length of each token
v0.13.0,embed sentences
v0.13.0,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.13.0,variable to store initial hidden states for decoder
v0.13.0,encode input characters by sending them through RNN
v0.13.0,test packing and padding
v0.13.0,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.13.0,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.13.0,decoder later with self.emb_to_hidden
v0.13.0,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.13.0,use token embedding as initial hidden state for decoder
v0.13.0,concatenate everything together and project to appropriate size for decoder
v0.13.0,variable to store initial hidden states for decoder
v0.13.0,encode input characters by sending them through RNN
v0.13.0,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.13.0,embed character one-hots
v0.13.0,send through encoder RNN (produces initial hidden for decoder)
v0.13.0,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.13.0,project 2*hidden_size to hidden_size
v0.13.0,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.13.0,later with self.emb_to_hidden
v0.13.0,use token embedding as initial hidden state for decoder
v0.13.0,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.13.0,concatenate everything together and project to appropriate size for decoder
v0.13.0,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.13.0,"create target vector (batch_size, max_label_seq_length + 1)"
v0.13.0,filter empty sentences
v0.13.0,max length of the predicted sequences
v0.13.0,for printing
v0.13.0,stop if all sentences are empty
v0.13.0,remove previously predicted labels of this type
v0.13.0,create list of tokens in batch
v0.13.0,encode inputs
v0.13.0,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.13.0,sequence length is always set to one in prediction
v0.13.0,option 1: greedy decoding
v0.13.0,predictions
v0.13.0,decode next character
v0.13.0,pick top beam size many outputs with highest probabilities
v0.13.0,option 2: beam search
v0.13.0,out_probs = self.softmax(output_vectors).squeeze(1)
v0.13.0,make sure no dummy symbol <> or start symbol <S> is predicted
v0.13.0,pick top beam size many outputs with highest probabilities
v0.13.0,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.13.0,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.13.0,keep scores of beam_size many hypothesis for each token in the batch
v0.13.0,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.13.0,save sequences so far
v0.13.0,keep track of how many hypothesis were completed for each token
v0.13.0,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.13.0,decode with log softmax
v0.13.0,make sure no dummy symbol <> or start symbol <S> is predicted
v0.13.0,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.13.0,"if the sequence is already ended, do not record as candidate"
v0.13.0,index of token in in list tokens_in_batch
v0.13.0,print(token_number)
v0.13.0,hypothesis score
v0.13.0,TODO: remove token if number of completed hypothesis exceeds given value
v0.13.0,set score of corresponding entry to -inf so it will not be expanded
v0.13.0,get leading_indices for next expansion
v0.13.0,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.13.0,take beam_size many copies of scores vector and add scores of possible new extensions
v0.13.0,"size (beam_size*batch_size, beam_size)"
v0.13.0,print(hypothesis_scores)
v0.13.0,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.13.0,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.13.0,print(hypothesis_scores_per_token)
v0.13.0,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.13.0,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.13.0,a list of length beam_size*batch_size
v0.13.0,"where the first three inidices belong to the first token, the next three to the second token,"
v0.13.0,and so on
v0.13.0,with these indices we can compute the tensors for the next iteration
v0.13.0,expand sequences with corresponding index
v0.13.0,add log-probabilities to the scores
v0.13.0,save new leading indices
v0.13.0,save corresponding hidden states
v0.13.0,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.13.0,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.13.0,get best final hypothesis for each token
v0.13.0,get characters from index sequences and add predicted label to token
v0.13.0,"Overwrites evaluate of parent class to remove the ""by class"" printout"
v0.13.0,set separator to concatenate two sentences
v0.13.0,init dropouts
v0.13.0,auto-spawn on GPU if available
v0.13.0,make a forward pass to produce embedded data points and labels
v0.13.0,get their gold labels as a tensor
v0.13.0,pass data points through network to get encoded data point tensor
v0.13.0,decode
v0.13.0,calculate the loss
v0.13.0,get a tensor of data points
v0.13.0,do dropout
v0.13.0,add DefaultClassifier arguments
v0.13.0,progress bar for verbosity
v0.13.0,stop if all sentences are empty
v0.13.0,clearing token embeddings to save memory
v0.13.0,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.13.0,"if the classifier predicts BIO/BIOES span labels, the internal label dictionary must be computed"
v0.13.0,fields in case this is a span-prediction problem
v0.13.0,the label type
v0.13.0,all parameters will be pushed internally to the specified device
v0.13.0,special handling during training if this is a span prediction problem
v0.13.0,internal variables
v0.13.0,non-set tags are OUT tags
v0.13.0,anything that is not OUT is IN
v0.13.0,does this prediction start a new span?
v0.13.0,B- and S- always start new spans
v0.13.0,"if the predicted class changes, I- starts a new span"
v0.13.0,"if the predicted class changes and S- was previous tag, start a new span"
v0.13.0,if an existing span is ended (either by reaching O or starting a new span)
v0.13.0,reset for-loop variables for new span
v0.13.0,remember previous tag
v0.13.0,"if there is a span at end of sentence, add it"
v0.13.0,"all labels default to ""O"""
v0.13.0,set gold token-level
v0.13.0,set predicted token-level
v0.13.0,now print labels in CoNLL format
v0.13.0,print labels in CoNLL format
v0.13.0,internal candidate lists of generator
v0.13.0,load Zelda candidates if so passed
v0.13.0,create candidate lists
v0.13.0,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.13.0,create a new dictionary for lower cased mentions
v0.13.0,go through each mention and its candidates
v0.13.0,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.13.0,set lowercased version as map
v0.13.0,remap state dict for models serialized with Flair <= 0.11.3
v0.13.0,get the candidates
v0.13.0,"during training, add the gold value as candidate"
v0.13.0,----- Create the internal tag dictionary -----
v0.13.0,span-labels need special encoding (BIO or BIOES)
v0.13.0,the big question is whether the label dictionary should contain an UNK or not
v0.13.0,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.13.0,"with UNK, the model learns less well if there are no UNK examples"
v0.13.0,is this a span prediction problem?
v0.13.0,----- Embeddings -----
v0.13.0,----- Initial loss weights parameters -----
v0.13.0,----- RNN specific parameters -----
v0.13.0,----- Conditional Random Field parameters -----
v0.13.0,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.13.0,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.13.0,----- Dropout parameters -----
v0.13.0,dropouts
v0.13.0,remove word dropout if there is no contact over the sequence dimension.
v0.13.0,----- Model layers -----
v0.13.0,----- RNN layer -----
v0.13.0,"If shared RNN provided, else create one for model"
v0.13.0,Whether to train initial hidden state
v0.13.0,final linear map to tag space
v0.13.0,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.13.0,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.13.0,"if there are no sentences, there is no loss"
v0.13.0,forward pass to get scores
v0.13.0,calculate loss given scores and labels
v0.13.0,make a zero-padded tensor for the whole sentence
v0.13.0,linear map to tag space
v0.13.0,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.13.0,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.13.0,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.13.0,spans need to be encoded as token-level predictions
v0.13.0,all others are regular labels for each token
v0.13.0,make sure it's a list
v0.13.0,filter empty sentences
v0.13.0,reverse sort all sequences by their length
v0.13.0,progress bar for verbosity
v0.13.0,stop if all sentences are empty
v0.13.0,get features from forward propagation
v0.13.0,remove previously predicted labels of this type
v0.13.0,"if return_loss, get loss value"
v0.13.0,make predictions
v0.13.0,add predictions to Sentence
v0.13.0,BIOES-labels need to be converted to spans
v0.13.0,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.13.0,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.13.0,core Flair models on Huggingface ModelHub
v0.13.0,"Large NER models,"
v0.13.0,Multilingual NER models
v0.13.0,English POS models
v0.13.0,Multilingual POS models
v0.13.0,English SRL models
v0.13.0,English chunking models
v0.13.0,Language-specific NER models
v0.13.0,Language-specific POS models
v0.13.0,English NER models
v0.13.0,Multilingual NER models
v0.13.0,English POS models
v0.13.0,Multilingual POS models
v0.13.0,English SRL models
v0.13.0,English chunking models
v0.13.0,Danish models
v0.13.0,German models
v0.13.0,French models
v0.13.0,Dutch models
v0.13.0,Malayalam models
v0.13.0,Portuguese models
v0.13.0,Keyphase models
v0.13.0,Biomedical models
v0.13.0,check if model name is a valid local file
v0.13.0,"check if model key is remapped to HF key - if so, print out information"
v0.13.0,get mapped name
v0.13.0,use mapped name instead
v0.13.0,"if not, check if model key is remapped to direct download location. If so, download model"
v0.13.0,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.13.0,"for all other cases (not local file or special download location), use HF model hub"
v0.13.0,"if not a local file, get from model hub"
v0.13.0,use model name as subfolder
v0.13.0,Lazy import
v0.13.0,output information
v0.13.0,## Demo: How to use in Flair
v0.13.0,load tagger
v0.13.0,make example sentence
v0.13.0,predict NER tags
v0.13.0,print sentence
v0.13.0,print predicted NER spans
v0.13.0,iterate over entities and print
v0.13.0,Lazy import
v0.13.0,Save model weight
v0.13.0,Determine if model card already exists
v0.13.0,Generate and save model card
v0.13.0,Upload files
v0.13.0,"all labels default to ""O"""
v0.13.0,set gold token-level
v0.13.0,set predicted token-level
v0.13.0,now print labels in CoNLL format
v0.13.0,print labels in CoNLL format
v0.13.0,the multi task model has several labels
v0.13.0,biomedical models
v0.13.0,entity linker
v0.13.0,auto-spawn on GPU if available
v0.13.0,remap state dict for models serialized with Flair <= 0.11.3
v0.13.0,English sentiment models
v0.13.0,Communicative Functions Model
v0.13.0,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.13.0,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.13.0,"Initially, get scores from <start> tag to all other tags"
v0.13.0,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.13.0,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.13.0,Create a tensor to hold accumulated sequence scores at each current tag
v0.13.0,Create a tensor to hold back-pointers
v0.13.0,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.13.0,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.13.0,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.13.0,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.13.0,"If sentence is over, add transition to STOP-tag"
v0.13.0,Decode/trace best path backwards
v0.13.0,Sanity check
v0.13.0,remove start-tag and backscore to stop-tag
v0.13.0,Max + Softmax to get confidence score for predicted label and append label to each token
v0.13.0,"Transitions are used in the following way: transitions[to, from]."
v0.13.0,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.13.0,to START-tag and from STOP-tag to any other tag to -10000.
v0.13.0,"if necessary, make batch_steps"
v0.13.0,break up the batch into slices of size
v0.13.0,mini_batch_chunk_size
v0.13.0,"if training also uses dev/train data, include in training set"
v0.13.0,evaluation and monitoring
v0.13.0,sampling and shuffling
v0.13.0,evaluation and monitoring
v0.13.0,when and what to save
v0.13.0,logging parameters
v0.13.0,plugins
v0.13.0,activate annealing plugin
v0.13.0,call self.train_custom with all parameters (minus the ones specific to the AnnealingPlugin)
v0.13.0,training parameters
v0.13.0,evaluation and monitoring
v0.13.0,sampling and shuffling
v0.13.0,evaluation and monitoring
v0.13.0,when and what to save
v0.13.0,logging parameters
v0.13.0,amp
v0.13.0,plugins
v0.13.0,annealing logic
v0.13.0,training parameters
v0.13.0,evaluation and monitoring
v0.13.0,sampling and shuffling
v0.13.0,evaluation and monitoring
v0.13.0,when and what to save
v0.13.0,logging parameters
v0.13.0,amp
v0.13.0,plugins
v0.13.0,training parameters
v0.13.0,evaluation and monitoring
v0.13.0,sampling and shuffling
v0.13.0,evaluation and monitoring
v0.13.0,when and what to save
v0.13.0,logging parameters
v0.13.0,amp
v0.13.0,plugins
v0.13.0,Create output folder
v0.13.0,=== START BLOCK: ACTIVATE PLUGINS === #
v0.13.0,We first activate all optional plugins. These take care of optional functionality such as various
v0.13.0,logging techniques and checkpointing
v0.13.0,log file plugin
v0.13.0,loss file plugin
v0.13.0,plugin for writing weights
v0.13.0,plugin for checkpointing
v0.13.0,=== END BLOCK: ACTIVATE PLUGINS === #
v0.13.0,derive parameters the function was called with (or defaults)
v0.13.0,initialize model card with these parameters
v0.13.0,Prepare training data and get dataset size
v0.13.0,"determine what splits (train, dev, test) to evaluate"
v0.13.0,determine how to determine best model and whether to save it
v0.13.0,instantiate the optimizer
v0.13.0,initialize sampler if provided
v0.13.0,init with default values if only class is provided
v0.13.0,set dataset to sample from
v0.13.0,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.13.0,Sanity checks
v0.13.0,"Sanity conversion: if flair.device was set as a string, convert to torch.device"
v0.13.0,-- AmpPlugin -> wraps with AMP
v0.13.0,-- AnnealingPlugin -> initialize schedulers (requires instantiated optimizer)
v0.13.0,At any point you can hit Ctrl + C to break out of training early.
v0.13.0,"- SchedulerPlugin -> load state for anneal_with_restarts, batch_growth_annealing, logic for early stopping"
v0.13.0,- LossFilePlugin -> get the current epoch for loss file logging
v0.13.0,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.13.0,log infos on training progress every `log_modulo` batches
v0.13.0,process mini-batches
v0.13.0,zero the gradients on the model and optimizer
v0.13.0,forward and backward for batch
v0.13.0,forward pass
v0.13.0,identify dynamic embeddings (always deleted) on first sentence
v0.13.0,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.13.0,do the optimizer step
v0.13.0,- SchedulerPlugin -> do the scheduler step if one-cycle or linear decay
v0.13.0,- WeightExtractorPlugin -> extracts weights
v0.13.0,- CheckpointPlugin -> executes save_model_each_k_epochs
v0.13.0,- SchedulerPlugin -> log bad epochs
v0.13.0,Determine if this is the best model or if we need to anneal
v0.13.0,log results
v0.13.0,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.13.0,use DEV split to determine if this is the best model so far
v0.13.0,"if not using DEV score, determine best model using train loss"
v0.13.0,- LossFilePlugin -> somehow prints all relevant metrics
v0.13.0,- AnnealPlugin -> scheduler step
v0.13.0,- SWAPlugin -> restores SGD weights from SWA
v0.13.0,"if we do not use dev data for model selection, save final model"
v0.13.0,TensorboardLogger -> closes writer
v0.13.0,test best model if test data is present
v0.13.0,get and return the final test score of best model
v0.13.0,MetricHistoryPlugin -> stores the loss history in return_values
v0.13.0,"Store return values, as they will be erased by reset_training_attributes"
v0.13.0,get a random sample of training sentences
v0.13.0,create a model card for this model with Flair and PyTorch version
v0.13.0,record Transformers version if library is loaded
v0.13.0,remember all parameters used in train() call
v0.13.0,"TextDataset returns a list. valid and test are only one file,"
v0.13.0,so return the first element
v0.13.0,cast string to Path
v0.13.0,error message if the validation dataset is too small
v0.13.0,Shuffle training files randomly after serially iterating
v0.13.0,through corpus one
v0.13.0,"iterate through training data, starting at"
v0.13.0,self.split (for checkpointing)
v0.13.0,off by one for printing
v0.13.0,go into train mode
v0.13.0,reset variables
v0.13.0,not really sure what this does
v0.13.0,do the forward pass in the model
v0.13.0,try to predict the targets
v0.13.0,Backward
v0.13.0,`clip_grad_norm` helps prevent the exploding gradient
v0.13.0,problem in RNNs / LSTMs.
v0.13.0,We detach the hidden state from how it was
v0.13.0,previously produced.
v0.13.0,"If we didn't, the model would try backpropagating"
v0.13.0,all the way to start of the dataset.
v0.13.0,explicitly remove loss to clear up memory
v0.13.0,#########################################################
v0.13.0,Save the model if the validation loss is the best we've
v0.13.0,seen so far.
v0.13.0,#########################################################
v0.13.0,print info
v0.13.0,#########################################################
v0.13.0,##############################################################################
v0.13.0,final testing
v0.13.0,##############################################################################
v0.13.0,Turn on evaluation mode which disables dropout.
v0.13.0,Work out how cleanly we can divide the dataset into bsz parts.
v0.13.0,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.13.0,Evenly divide the data across the bsz batches.
v0.13.0,"no need to check for MetricName, as __add__ of other would be called in this case"
v0.13.0,"This flag tracks, whether an event is currently being processed (otherwise it is added to the queue)"
v0.13.0,instantiate plugin
v0.13.0,"Reset the flag, since an exception event might be dispatched"
v0.13.0,"If there is no **kw argument in the callback, check if any of the passed kw args is not accepted by"
v0.13.0,the callback
v0.13.0,go through all attributes
v0.13.0,get attribute hook events (may raise an AttributeError)
v0.13.0,register function as a hook
v0.13.0,"Decorator was used with parentheses, but no args"
v0.13.0,Decorator was used with args (strings specifiying the events)
v0.13.0,Decorator was used without args
v0.13.0,path to store the model
v0.13.0,special annealing modes
v0.13.0,determine the min learning rate
v0.13.0,"minimize training loss if training with dev data, else maximize dev score"
v0.13.0,instantiate the scheduler
v0.13.0,stop training if learning rate becomes too small
v0.13.0,reload last best model if annealing with restarts is enabled
v0.13.0,calculate warmup steps
v0.13.0,skip if no optimization has happened.
v0.13.0,saves the model with full vocab as checkpoints etc were created with reduced vocab.
v0.13.0,TODO: check if metric is in tracked metrics
v0.13.0,prepare loss logging file and set up header
v0.13.0,set up all metrics to collect
v0.13.0,set up headers
v0.13.0,name: HEADER
v0.13.0,Add all potentially relevant metrics. If a metric is not published
v0.13.0,"after the first epoch (when the header is written), the column is"
v0.13.0,removed at that point.
v0.13.0,initialize the first log line
v0.13.0,record is a list of scalars
v0.13.0,output log file
v0.13.0,remove columns where no value was found on the first epoch (could be != 1 if training was resumed)
v0.13.0,make headers on epoch 1
v0.13.0,write header
v0.13.0,adjust alert level
v0.13.0,"the default model for ELMo is the 'original' model, which is very large"
v0.13.0,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.13.0,put on Cuda if available
v0.13.0,embed a dummy sentence to determine embedding_length
v0.13.0,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.13.0,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.13.0,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.13.0,news-english-forward
v0.13.0,news-english-backward
v0.13.0,news-english-forward
v0.13.0,news-english-backward
v0.13.0,mix-english-forward
v0.13.0,mix-english-backward
v0.13.0,mix-german-forward
v0.13.0,mix-german-backward
v0.13.0,common crawl Polish forward
v0.13.0,common crawl Polish backward
v0.13.0,Slovenian forward
v0.13.0,Slovenian backward
v0.13.0,Bulgarian forward
v0.13.0,Bulgarian backward
v0.13.0,Dutch forward
v0.13.0,Dutch backward
v0.13.0,Swedish forward
v0.13.0,Swedish backward
v0.13.0,French forward
v0.13.0,French backward
v0.13.0,Czech forward
v0.13.0,Czech backward
v0.13.0,Portuguese forward
v0.13.0,Portuguese backward
v0.13.0,initialize cache if use_cache set
v0.13.0,embed a dummy sentence to determine embedding_length
v0.13.0,set to eval mode
v0.13.0,Copy the object's state from self.__dict__ which contains
v0.13.0,all our instance attributes. Always use the dict.copy()
v0.13.0,method to avoid modifying the original state.
v0.13.0,Remove the unpicklable entries.
v0.13.0,"if cache is used, try setting embeddings from cache first"
v0.13.0,try populating embeddings from cache
v0.13.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.13.0,get hidden states from language model
v0.13.0,take first or last hidden states from language model as word representation
v0.13.0,if self.tokenized_lm or token.whitespace_after:
v0.13.0,"if only one sentence is passed, convert to list of sentence"
v0.13.0,bidirectional LSTM on top of embedding layer
v0.13.0,dropouts
v0.13.0,"first, sort sentences by number of tokens"
v0.13.0,go through each sentence in batch
v0.13.0,PADDING: pad shorter sentences out
v0.13.0,ADD TO SENTENCE LIST: add the representation
v0.13.0,--------------------------------------------------------------------
v0.13.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.13.0,--------------------------------------------------------------------
v0.13.0,--------------------------------------------------------------------
v0.13.0,FF PART
v0.13.0,--------------------------------------------------------------------
v0.13.0,use word dropout if set
v0.13.0,--------------------------------------------------------------------
v0.13.0,EXTRACT EMBEDDINGS FROM LSTM
v0.13.0,--------------------------------------------------------------------
v0.13.0,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.13.0,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.13.0,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.13.0,add positional encodings
v0.13.0,reshape the pixels into the sequence
v0.13.0,layer norm after convolution and positional encodings
v0.13.0,add <cls> token
v0.13.0,"transformer requires input in the shape [h*w+1, b, d]"
v0.13.0,the output is an embedding of <cls> token
v0.13.0,this parameter is fixed
v0.13.0,optional fine-tuning on top of embedding layer
v0.13.0,"if only one sentence is passed, convert to list of sentence"
v0.13.0,"if only one sentence is passed, convert to list of sentence"
v0.13.0,bidirectional RNN on top of embedding layer
v0.13.0,dropouts
v0.13.0,TODO: remove in future versions
v0.13.0,embed words in the sentence
v0.13.0,before-RNN dropout
v0.13.0,reproject if set
v0.13.0,push through RNN
v0.13.0,after-RNN dropout
v0.13.0,extract embeddings from RNN
v0.13.0,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.13.0,"check if this is the case and if so, set it"
v0.13.0,serialize the language models and the constructor arguments (but nothing else)
v0.13.0,re-initialize language model with constructor arguments
v0.13.0,special handling for deserializing language models
v0.13.0,copy over state dictionary to self
v0.13.0,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.13.0,"in their ""self.train()"" method)"
v0.13.0,IMPORTANT: add embeddings as torch modules
v0.13.0,iterate over sentences
v0.13.0,"if its a forward LM, take last state"
v0.13.0,"convert to plain strings, embedded in a list for the encode function"
v0.13.0,CNN
v0.13.0,dropouts
v0.13.0,TODO: remove in future versions
v0.13.0,embed words in the sentence
v0.13.0,before-RNN dropout
v0.13.0,reproject if set
v0.13.0,push CNN
v0.13.0,after-CNN dropout
v0.13.0,extract embeddings from CNN
v0.13.0,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.13.0,"if only one sentence is passed, convert to list of sentence"
v0.13.0,Expose base classses
v0.13.0,Expose document embedding classes
v0.13.0,Expose image embedding classes
v0.13.0,Expose legacy embedding classes
v0.13.0,Expose token embedding classes
v0.13.0,in some cases we need to insert zero vectors for tokens without embedding.
v0.13.0,padding
v0.13.0,remove special markup
v0.13.0,check if special tokens exist to circumvent error message
v0.13.0,iterate over subtokens and reconstruct tokens
v0.13.0,remove special markup
v0.13.0,check if reconstructed token is special begin token ([CLS] or similar)
v0.13.0,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.13.0,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.13.0,if tokens are unaccounted for
v0.13.0,check if all tokens were matched to subtokens
v0.13.0,The layoutlm tokenizer doesn't handle ocr themselves
v0.13.0,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.13.0,"cannot run `.encode` if ocr boxes are required, assume"
v0.13.0,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.13.0,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.13.0,in case of doubt: token embedding has higher priority than document embedding
v0.13.0,random check some tokens to save performance.
v0.13.0,Models such as FNet do not have an attention_mask
v0.13.0,set language IDs for XLM-style transformers
v0.13.0,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.13.0,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.13.0,set context if not set already
v0.13.0,flair specific pre-tokenization
v0.13.0,fields to store left and right context
v0.13.0,expand context only if context_length is set
v0.13.0,"if context_dropout is set, randomly deactivate left context during training"
v0.13.0,"if context_dropout is set, randomly deactivate right context during training"
v0.13.0,"if use_context_separator is set, add a [FLERT] token"
v0.13.0,return expanded sentence and context length information
v0.13.0,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.13.0,temporary fix to disable tokenizer parallelism warning
v0.13.0,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.13.0,do not print transformer warnings as these are confusing in this case
v0.13.0,load tokenizer and transformer model
v0.13.0,load tokenizer from inmemory zip-file
v0.13.0,model name
v0.13.0,embedding parameters
v0.13.0,send mini-token through to check how many layers the model has
v0.13.0,return length
v0.13.0,"If we use a context separator, add a new special token"
v0.13.0,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.13.0,"when initializing, embeddings are in eval mode by default"
v0.13.0,in case of doubt: token embedding has higher priority than document embedding
v0.13.0,in case of doubt: token embedding has higher priority than document embedding
v0.13.0,legacy TransformerDocumentEmbedding
v0.13.0,legacy TransformerTokenEmbedding
v0.13.0,legacy Flair <= 0.12
v0.13.0,legacy Flair <= 0.7
v0.13.0,legacy TransformerTokenEmbedding
v0.13.0,Legacy TransformerDocumentEmbedding
v0.13.0,legacy TransformerTokenEmbedding
v0.13.0,legacy TransformerDocumentEmbedding
v0.13.0,some models like the tars model somehow lost this information.
v0.13.0,copy values from new embedding
v0.13.0,those parameters are only from the super class and will be recreated in the constructor.
v0.13.0,cls first pooling can be done without recreating sentence hidden states
v0.13.0,make the tuple a tensor; makes working with it easier.
v0.13.0,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.13.0,only use layers that will be outputted
v0.13.0,this parameter is fixed
v0.13.0,IMPORTANT: add embeddings as torch modules
v0.13.0,"if only one sentence is passed, convert to list of sentence"
v0.13.0,make compatible with serialized models
v0.13.0,gensim version 4
v0.13.0,gensim version 3
v0.13.0,"if no embedding is set, the vocab and embedding length is requried"
v0.13.0,GLOVE embeddings
v0.13.0,TURIAN embeddings
v0.13.0,KOMNINOS embeddings
v0.13.0,pubmed embeddings
v0.13.0,FT-CRAWL embeddings
v0.13.0,FT-CRAWL embeddings
v0.13.0,twitter embeddings
v0.13.0,two-letter language code wiki embeddings
v0.13.0,two-letter language code wiki embeddings
v0.13.0,two-letter language code crawl embeddings
v0.13.0,"this is required to force the module on the cpu,"
v0.13.0,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.13.0,self.to(..) actually sets the device properly
v0.13.0,this ignores the get_cached_vec method when loading older versions
v0.13.0,it is needed for compatibility reasons
v0.13.0,gensim version 4
v0.13.0,gensim version 3
v0.13.0,"when loading the old versions from pickle, the embeddings might not be added as pytorch module."
v0.13.0,"we do this delayed, when the weights are collected (e.g. for saving), as doing this earlier might"
v0.13.0,lead to issues while loading (trying to load weights that weren't stored as python weights and therefore
v0.13.0,not finding them)
v0.13.0,use list of common characters if none provided
v0.13.0,translate words in sentence into ints using dictionary
v0.13.0,"sort words by length, for batching and masking"
v0.13.0,chars for rnn processing
v0.13.0,multilingual models
v0.13.0,English models
v0.13.0,Arabic
v0.13.0,Bulgarian
v0.13.0,Czech
v0.13.0,Danish
v0.13.0,German
v0.13.0,Spanish
v0.13.0,Basque
v0.13.0,Persian
v0.13.0,Finnish
v0.13.0,French
v0.13.0,Hebrew
v0.13.0,Hindi
v0.13.0,Croatian
v0.13.0,Indonesian
v0.13.0,Italian
v0.13.0,Japanese
v0.13.0,Malayalam
v0.13.0,Dutch
v0.13.0,Norwegian
v0.13.0,Polish
v0.13.0,Portuguese
v0.13.0,Pubmed
v0.13.0,Slovenian
v0.13.0,Swedish
v0.13.0,Tamil
v0.13.0,Spanish clinical
v0.13.0,CLEF HIPE Shared task
v0.13.0,Amharic
v0.13.0,Ukrainian
v0.13.0,load model if in pretrained model map
v0.13.0,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.13.0,CLEF HIPE models are lowercased
v0.13.0,embeddings are static if we don't do finetuning
v0.13.0,embed a dummy sentence to determine embedding_length
v0.13.0,set to eval mode
v0.13.0,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.13.0,gradients are enable if fine-tuning is enabled
v0.13.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.13.0,get hidden states from language model
v0.13.0,take first or last hidden states from language model as word representation
v0.13.0,offset mode that extracts at whitespace after last character
v0.13.0,offset mode that extracts at last character
v0.13.0,make compatible with old models
v0.13.0,use the character language model embeddings as basis
v0.13.0,length is twice the original character LM embedding length
v0.13.0,these fields are for the embedding memory
v0.13.0,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.13.0,we re-compute embeddings dynamically at each epoch
v0.13.0,set the memory method
v0.13.0,memory is wiped each time we do a training run
v0.13.0,"if we keep a pooling, it needs to be updated continuously"
v0.13.0,update embedding
v0.13.0,check token.text is empty or not
v0.13.0,set aggregation operation
v0.13.0,add embeddings after updating
v0.13.0,model architecture
v0.13.0,model architecture
v0.13.0,"""pl"","
v0.13.0,download if necessary
v0.13.0,load the model
v0.13.0,"TODO: keep for backwards compatibility, but remove in future"
v0.13.0,save the sentence piece model as binary file (not as path which may change)
v0.13.0,write out the binary sentence piece model into the expected directory
v0.13.0,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.13.0,"otherwise, use normal process and potentially trigger another download"
v0.13.0,"once the modes if there, load it with sentence piece"
v0.13.0,empty words get no embedding
v0.13.0,all other words get embedded
v0.13.0,GLOVE embeddings
v0.13.0,no need to recreate as NILCEmbeddings
v0.13.0,read in test file if exists
v0.13.0,read in dev file if exists
v0.13.0,"find train, dev and test files if not specified"
v0.13.0,Add tags for each annotated span
v0.13.0,Remove leading and trailing whitespaces from annotated spans
v0.13.0,Search start and end token index for current span
v0.13.0,If end index is not found set to last token
v0.13.0,Throw error if indices are not valid
v0.13.0,Add metadatas for sentence
v0.13.0,Currently all Jsonl Datasets are stored in Memory
v0.13.0,get train data
v0.13.0,read in test file if exists
v0.13.0,read in dev file if exists
v0.13.0,"find train, dev and test files if not specified"
v0.13.0,special key for space after
v0.13.0,special key for feature columns
v0.13.0,special key for dependency head id
v0.13.0,"store either Sentence objects in memory, or only file offsets"
v0.13.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.13.0,determine encoding of text file
v0.13.0,identify which columns are spans and which are word-level
v0.13.0,now load all sentences
v0.13.0,skip first line if to selected
v0.13.0,option 1: keep Sentence objects in memory
v0.13.0,pointer to previous
v0.13.0,parse next sentence
v0.13.0,quit if last sentence reached
v0.13.0,skip banned sentences
v0.13.0,set previous and next sentence for context
v0.13.0,append parsed sentence to list in memory
v0.13.0,option 2: keep source data in memory
v0.13.0,"read lines for next sentence, but don't parse"
v0.13.0,quit if last sentence reached
v0.13.0,append raw lines for each sentence
v0.13.0,we make a distinction between word-level tags and span-level tags
v0.13.0,read first sentence to determine which columns are span-labels
v0.13.0,skip first line if to selected
v0.13.0,check the first 5 sentences
v0.13.0,go through all annotations and identify word- and span-level annotations
v0.13.0,- if a column has at least one BIES we know it's a Span label
v0.13.0,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.13.0,- problem cases are columns for which we see only O - in this case we default to Span
v0.13.0,skip assigned columns
v0.13.0,the space after key is always word-levels
v0.13.0,"if at least one token has a BIES, we know it's a span label"
v0.13.0,"if at least one token has a label other than BIOES, we know it's a token label"
v0.13.0,all remaining columns that are not word-level are span-level
v0.13.0,for column in self.word_level_tag_columns:
v0.13.0,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.13.0,"if sentence ends, break"
v0.13.0,parse comments if possible
v0.13.0,"otherwise, this line is a token. parse and add to sentence"
v0.13.0,check if this sentence is a document boundary
v0.13.0,add span labels
v0.13.0,discard tags from tokens that are not added to the sentence
v0.13.0,parse relations if they are set
v0.13.0,head and tail span indices are 1-indexed and end index is inclusive
v0.13.0,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.13.0,"to set the metadata ""domain"" to ""de-orcas"""
v0.13.0,get fields from line
v0.13.0,get head_id if exists (only in dependency parses)
v0.13.0,initialize token
v0.13.0,go through all columns
v0.13.0,'feats' and 'misc' column should be split into different fields
v0.13.0,special handling for whitespace after
v0.13.0,add each other feature as label-value pair
v0.13.0,get the task name (e.g. 'ner')
v0.13.0,get the label value
v0.13.0,add label
v0.13.0,remap regular tag names
v0.13.0,"if in memory, retrieve parsed sentence"
v0.13.0,else skip to position in file where sentence begins
v0.13.0,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.13.0,use all domains
v0.13.0,iter over all domains / sources and create target files
v0.13.0,The conll representation of coref spans allows spans to
v0.13.0,"overlap. If spans end or begin at the same word, they are"
v0.13.0,"separated by a ""|""."
v0.13.0,The span begins at this word.
v0.13.0,The span begins and ends at this word (single word span).
v0.13.0,"The span is starting, so we record the index of the word."
v0.13.0,"The span for this id is ending, but didn't start at this word."
v0.13.0,Retrieve the start index from the document state and
v0.13.0,add the span to the clusters for this id.
v0.13.0,strip all bracketing information to
v0.13.0,get the actual propbank label.
v0.13.0,Entering into a span for a particular semantic role label.
v0.13.0,We append the label and set the current span for this annotation.
v0.13.0,"If there's no '(' token, but the current_span_label is not None,"
v0.13.0,then we are inside a span.
v0.13.0,We're outside a span.
v0.13.0,"Exiting a span, so we reset the current span label for this annotation."
v0.13.0,The words in the sentence.
v0.13.0,The pos tags of the words in the sentence.
v0.13.0,the pieces of the parse tree.
v0.13.0,The lemmatised form of the words in the sentence which
v0.13.0,have SRL or word sense information.
v0.13.0,The FrameNet ID of the predicate.
v0.13.0,"The sense of the word, if available."
v0.13.0,"The current speaker, if available."
v0.13.0,"Cluster id -> List of (start_index, end_index) spans."
v0.13.0,Cluster id -> List of start_indices which are open for this id.
v0.13.0,Replace brackets in text and pos tags
v0.13.0,with a different token for parse trees.
v0.13.0,only keep ')' if there are nested brackets with nothing in them.
v0.13.0,There are some bad annotations in the CONLL data.
v0.13.0,"They contain no information, so to make this explicit,"
v0.13.0,we just set the parse piece to be None which will result
v0.13.0,in the overall parse tree being None.
v0.13.0,"If this is the first word in the sentence, create"
v0.13.0,empty lists to collect the NER and SRL BIO labels.
v0.13.0,"We can't do this upfront, because we don't know how many"
v0.13.0,"components we are collecting, as a sentence can have"
v0.13.0,variable numbers of SRL frames.
v0.13.0,Create variables representing the current label for each label
v0.13.0,sequence we are collecting.
v0.13.0,"If any annotation marks this word as a verb predicate,"
v0.13.0,we need to record its index. This also has the side effect
v0.13.0,of ordering the verbal predicates by their location in the
v0.13.0,"sentence, automatically aligning them with the annotations."
v0.13.0,"this would not be reached if parse_pieces contained None, hence the cast"
v0.13.0,Non-empty line. Collect the annotation.
v0.13.0,Collect any stragglers or files which might not
v0.13.0,have the '#end document' format for the end of the file.
v0.13.0,this dataset name
v0.13.0,check if data there
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,check if data there
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,download files if not present locally
v0.13.0,we need to slightly modify the original files by adding some new lines after document separators
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,Remove CoNLL-U meta information in the last column
v0.13.0,column format
v0.13.0,dataset name
v0.13.0,data folder: default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,dataset name
v0.13.0,data folder: default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,entity_mapping
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,data validation
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download files if not present locallys
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,# download zip
v0.13.0,merge the files in one as the zip is containing multiples files
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,check if data there
v0.13.0,create folder
v0.13.0,download dataset
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download and parse data if necessary
v0.13.0,create train test dev if not exist
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,If the extracted corpus file is not yet present in dir
v0.13.0,download zip if necessary
v0.13.0,"extracted corpus is not present , so unpacking it."
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download zip
v0.13.0,unpacking the zip
v0.13.0,merge the files in one as the zip is containing multiples files
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.13.0,download files if not present locally
v0.13.0,we need to modify the original files by adding new lines after after the end of each sentence
v0.13.0,if only one language is given
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,"use all languages if explicitly set to ""all"""
v0.13.0,download data if necessary
v0.13.0,initialize comlumncorpus and add it to list
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,"For each language in languages, the file is downloaded if not existent"
v0.13.0,Then a comlumncorpus of that data is created and saved in a list
v0.13.0,this list is handed to the multicorpus
v0.13.0,list that contains the columncopora
v0.13.0,download data if necessary
v0.13.0,"if language not downloaded yet, download it"
v0.13.0,create folder
v0.13.0,get google drive id from list
v0.13.0,download from google drive
v0.13.0,unzip
v0.13.0,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.13.0,transform data into required format
v0.13.0,"the processed dataset has the additional ending ""_new"""
v0.13.0,remove the unprocessed dataset
v0.13.0,initialize comlumncorpus and add it to list
v0.13.0,if no languages are given as argument all languages used in XTREME will be loaded
v0.13.0,if only one language is given
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,"For each language in languages, the file is downloaded if not existent"
v0.13.0,Then a comlumncorpus of that data is created and saved in a list
v0.13.0,This list is handed to the multicorpus
v0.13.0,list that contains the columncopora
v0.13.0,download data if necessary
v0.13.0,"if language not downloaded yet, download it"
v0.13.0,create folder
v0.13.0,download from HU Server
v0.13.0,unzip
v0.13.0,transform data into required format
v0.13.0,initialize comlumncorpus and add it to list
v0.13.0,if only one language is given
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,initialize comlumncorpus and add it to list
v0.13.0,download data if necessary
v0.13.0,unpack and write out in CoNLL column-like format
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,data is not in IOB2 format. Thus we transform it to IOB2
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,rename according to train - test - dev - convention
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,Add missing newline after header
v0.13.0,Workaround for empty tokens
v0.13.0,"Add ""real"" document marker"
v0.13.0,Dataset split mapping
v0.13.0,v2.0 only adds new language and splits for AJMC dataset
v0.13.0,Special document marker for sample splits in AJMC dataset
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,if only one language is given
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,"use all languages if explicitly set to ""all"""
v0.13.0,download data if necessary
v0.13.0,initialize comlumncorpus and add it to list
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download and parse data if necessary
v0.13.0,paths to train and test splits
v0.13.0,init corpus
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download and parse data if necessary
v0.13.0,iterate over all html files
v0.13.0,"get rid of html syntax, we only need the text"
v0.13.0,between all documents we write a separator symbol
v0.13.0,skip empty strings
v0.13.0,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.13.0,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.13.0,sentence splitting and tokenization
v0.13.0,iterate through all annotations and add to corresponding tokens
v0.13.0,find sentence to which annotation belongs
v0.13.0,position within corresponding sentence
v0.13.0,set annotation for tokens of entity mention
v0.13.0,write to out-file in column format
v0.13.0,"in case something goes wrong, delete the dataset and raise error"
v0.13.0,this dataset name
v0.13.0,download and parse data if necessary
v0.13.0,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.13.0,generate qid wikiname dictionaries
v0.13.0,merge dictionaries
v0.13.0,ignore first line
v0.13.0,commented and empty lines
v0.13.0,read all Q-IDs
v0.13.0,ignore first line
v0.13.0,request
v0.13.0,this dataset name
v0.13.0,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.13.0,like this we can quickly check if the corresponding page exists
v0.13.0,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.13.0,delete unprocessed file
v0.13.0,collect all wikiids
v0.13.0,create the dictionary
v0.13.0,request
v0.13.0,this dataset name
v0.13.0,names of raw text documents
v0.13.0,open output_file
v0.13.0,iterate through all documents
v0.13.0,split sentences and tokenize
v0.13.0,iterate through all annotations and add to corresponding tokens
v0.13.0,find sentence to which annotation belongs
v0.13.0,position within corresponding sentence
v0.13.0,set annotation for tokens of entity mention
v0.13.0,write to out file
v0.13.0,annotation from one annotator or two agreeing annotators
v0.13.0,this dataset name
v0.13.0,download and parse data if necessary
v0.13.0,this dataset name
v0.13.0,download and parse data if necessary
v0.13.0,First parse the post titles
v0.13.0,Keep track of how many and which entity mentions does a given post title have
v0.13.0,Check if the current post title has an entity link and parse accordingly
v0.13.0,Post titles with entity mentions (if any) are handled via this function
v0.13.0,Then parse the comments
v0.13.0,"Iterate over the comments.tsv file, until the end is reached"
v0.13.0,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.13.0,Each comment thread is handled as one 'document'.
v0.13.0,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.13.0,This if-condition is needed to handle this problem.
v0.13.0,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.13.0,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.13.0,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.13.0,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.13.0,and not just single letters into single rows.
v0.13.0,If there are annotated entity mentions for given post title or a comment thread
v0.13.0,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.13.0,Write the token with a corresponding tag to file
v0.13.0,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.13.0,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.13.0,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.13.0,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.13.0,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.13.0,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.13.0,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.13.0,Check if further annotations belong to the current post title or comment thread as well
v0.13.0,Stop when the end of an annotation file is reached
v0.13.0,Check if further annotations belong to the current sentence as well
v0.13.0,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.13.0,Docstart
v0.13.0,if there is more than one word in the chunk we write each in a separate line
v0.13.0,print(chunks)
v0.13.0,empty line after each sentence
v0.13.0,convert the file to CoNLL
v0.13.0,this dataset name
v0.13.0,"check if data there, if not, download the data"
v0.13.0,create folder
v0.13.0,download data
v0.13.0,transform data into column format if necessary
v0.13.0,if no filenames are specified we use all the data
v0.13.0,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.13.0,also we remove 'raganato_ALL' from filenames in case its in the list
v0.13.0,generate the test file
v0.13.0,make column file and save to data_folder
v0.13.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.0,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.0,create folder
v0.13.0,download data
v0.13.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.0,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.0,create folder
v0.13.0,download data
v0.13.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.0,generate the test file
v0.13.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.0,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.0,create folder
v0.13.0,download data
v0.13.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.0,generate the test file
v0.13.0,default dataset folder is the cache root
v0.13.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.0,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.0,create folder
v0.13.0,download data
v0.13.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.0,generate the test file
v0.13.0,default dataset folder is the cache root
v0.13.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.0,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.0,create folder
v0.13.0,download data
v0.13.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.0,generate the test file
v0.13.0,default dataset folder is the cache root
v0.13.0,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.13.0,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.13.0,create folder
v0.13.0,download data
v0.13.0,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.13.0,generate the test file
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,if True:
v0.13.0,write CoNLL-U Plus header
v0.13.0,"Some special cases (e.g., missing spaces before entity marker)"
v0.13.0,necessary if text should be whitespace tokenizeable
v0.13.0,Handle case where tail may occur before the head
v0.13.0,this dataset name
v0.13.0,write CoNLL-U Plus header
v0.13.0,this dataset name
v0.13.0,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.13.0,download data if necessary
v0.13.0,write CoNLL-U Plus header
v0.13.0,The span has ended.
v0.13.0,We are entering a new span; reset indices
v0.13.0,and active tag to new span.
v0.13.0,We're inside a span.
v0.13.0,Last token might have been a part of a valid span.
v0.13.0,this dataset name
v0.13.0,write CoNLL-U Plus header
v0.13.0,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.13.0,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.13.0,target_file_path = Path(data_folder) / target_filename
v0.13.0,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.13.0,# write CoNLL-U Plus header
v0.13.0,"target_file.write(""# global.columns = id form ner\n"")"
v0.13.0,for example in json.load(source_file):
v0.13.0,token_list = self._tacred_example_to_token_list(example)
v0.13.0,target_file.write(token_list.serialize())
v0.13.0,check if first tag row is already occupied
v0.13.0,"if first tag row is occupied, use second tag row"
v0.13.0,hardcoded mapping TODO: perhaps find nicer solution
v0.13.0,remap regular tag names
v0.13.0,else skip to position in file where sentence begins
v0.13.0,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.13.0,read in dev file if exists
v0.13.0,read in test file if exists
v0.13.0,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.13.0,"find train, dev and test files if not specified"
v0.13.0,use test_file to create test split if available
v0.13.0,use dev_file to create test split if available
v0.13.0,"if data point contains black-listed label, do not use"
v0.13.0,first check if valid sentence
v0.13.0,"if so, add to indices"
v0.13.0,"find train, dev and test files if not specified"
v0.13.0,variables
v0.13.0,different handling of in_memory data than streaming data
v0.13.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.13.0,test if format is OK
v0.13.0,test if at least one label given
v0.13.0,make sentence from text (and filter for length)
v0.13.0,"if a pair column is defined, make a sentence pair object"
v0.13.0,noinspection PyDefaultArgument
v0.13.0,dataset name includes the split size
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download each of the 28 splits
v0.13.0,create dataset directory if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,handle labels file
v0.13.0,handle data file
v0.13.0,Create flair compatible labels
v0.13.0,"by default, map point score to POSITIVE / NEGATIVE values"
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,create dataset directory if necessary
v0.13.0,create train.txt file from CSV
v0.13.0,create test.txt file from CSV
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,create dataset directory if necessary
v0.13.0,create train.txt file by iterating over pos and neg file
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,create dataset directory if necessary
v0.13.0,create train.txt file by iterating over pos and neg file
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,create dataset directory if necessary
v0.13.0,create train.txt file by iterating over pos and neg file
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,create dataset directory if necessary
v0.13.0,create train.txt file by iterating over pos and neg file
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,create train dev and test files in fasttext format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download senteval datasets if necessary und unzip
v0.13.0,convert to FastText format
v0.13.0,download data if necessary
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,move original .tsv files to another folder
v0.13.0,create train and dev splits in fasttext format
v0.13.0,create eval_dataset file with no labels
v0.13.0,download zip archive
v0.13.0,unpack file in datasets directory (zip archive contains a directory named SST-2)
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,download datasets if necessary
v0.13.0,create dataset directory if necessary
v0.13.0,create correctly formated txt files
v0.13.0,multiple labels are possible
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,Create flair compatible labels
v0.13.0,TREC-6 : NUM:dist -> __label__NUM
v0.13.0,TREC-50: NUM:dist -> __label__NUM:dist
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,Create flair compatible labels
v0.13.0,TREC-6 : NUM:dist -> __label__NUM
v0.13.0,TREC-50: NUM:dist -> __label__NUM:dist
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,create a separate directory for different tasks
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,check if dataset is supported
v0.13.0,set file names
v0.13.0,set file names
v0.13.0,download and unzip in file structure if necessary
v0.13.0,instantiate corpus
v0.13.0,"find train, dev and test files if not specified"
v0.13.0,"create DataPairDataset for train, test and dev file, if they are given"
v0.13.0,stop if file does not exist
v0.13.0,create a DataPair object from strings
v0.13.0,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,"rename test file to eval_dataset, since it has no labels"
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.13.0,dev sets include 5 different annotations but we will only keep the gold label
v0.13.0,"rename test file to eval_dataset, since it has no labels"
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get test and dev sets
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,"rename test file to eval_dataset, since it has no labels"
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,"rename test file to eval_dataset, since it has no labels"
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,"rename test file to eval_dataset, since it has no labels"
v0.13.0,"if data is not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,"rename test file to eval_dataset, since it has no labels"
v0.13.0,"if data not downloaded yet, download it"
v0.13.0,get the zip file
v0.13.0,"the downloaded files have json format, we transform them to tsv"
v0.13.0,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.13.0,remove json file
v0.13.0,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.13.0,with sum of all entity lengths as secondary key
v0.13.0,calculate offset without current text
v0.13.0,because we stick all passages of a document together
v0.13.0,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.13.0,Try to fix incorrect annotations
v0.13.0,print(
v0.13.0,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.13.0,)
v0.13.0,Ignore empty lines or relation annotations
v0.13.0,FIX annotation of whitespaces (necessary for PDR)
v0.13.0,One token may contain multiple entities -> deque all of them
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.13.0,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,Edge case: last token starts a new entity
v0.13.0,Last document in file
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,In the huner split files there is no information whether a given id originates
v0.13.0,from the train or test file of the original corpus - so we have to adapt corpus
v0.13.0,splitting here
v0.13.0,In the huner split files there is no information whether a given id originates
v0.13.0,from the train or test file of the original corpus - so we have to adapt corpus
v0.13.0,splitting here
v0.13.0,In the huner split files there is no information whether a given id originates
v0.13.0,from the train or test file of the original corpus - so we have to adapt corpus
v0.13.0,splitting here
v0.13.0,Edge case: last token starts a new entity
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,Read texts
v0.13.0,Read annotations
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,We need to apply a patch to correct the original training file
v0.13.0,Articles title
v0.13.0,Article abstract
v0.13.0,Entity annotations
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,Edge case: last token starts a new entity
v0.13.0,Map all entities to chemicals
v0.13.0,Map all entities to disease
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,Incomplete article
v0.13.0,Invalid XML syntax
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,if len(mid) != 3:
v0.13.0,continue
v0.13.0,Try to fix entity offsets
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,There is still one illegal annotation in the file ..
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,"Abstract first, title second to prevent issues with sentence splitting"
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,"Filter for specific entity types, by default no entities will be filtered"
v0.13.0,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.13.0,train and dev split of V2 will be train in V4
v0.13.0,test split of V2 will be dev in V4
v0.13.0,New documents in V4 will become test documents
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,column format
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,column format
v0.13.0,build dataset name and full huggingface reference name
v0.13.0,Download data if necessary
v0.13.0,"Some datasets in BigBio only have train or test splits, not both"
v0.13.0,"If only test split, assign it to train split"
v0.13.0,"If only train split, sample other from it (sample_missing_splits=True)"
v0.13.0,Not every dataset has a dev / validation set!
v0.13.0,Perform type mapping if necessary
v0.13.0,"Collect all texts of the document, each passage will be"
v0.13.0,a text in our internal format
v0.13.0,Sort passages by start offset
v0.13.0,Transform all entity annotations into internal format
v0.13.0,Find the passage of the entity (necessary for offset adaption)
v0.13.0,Adapt entity offsets according to passage offsets
v0.13.0,FIXME: This is just for debugging purposes
v0.13.0,passage_text = id_to_text[passage_id]
v0.13.0,doc_text = passage_text[entity_offset[0] : entity_offset[1]]
v0.13.0,"mention_text = entity[""text""][0]"
v0.13.0,if doc_text != mention_text:
v0.13.0,"print(f""Annotation error ({document['document_id']}) - Doc: {doc_text} vs. Mention: {mention_text}"")"
v0.13.0,Check base case
v0.13.0,Get element in the middle
v0.13.0,Is the mention with the passage offsets?
v0.13.0,"If element is smaller than mid, then it can only"
v0.13.0,be present in left subarray
v0.13.0,Else the element can only be present in right subarray
v0.13.0,Special case for ProGene: We need to use the split_0_train and split_0_test splits
v0.13.0,as they are currently provided in BigBio
v0.13.0,cache Feidegger config file
v0.13.0,cache Feidegger images
v0.13.0,replace image URL with local cached file
v0.13.0,append Sentence-Image data point
v0.13.0,cast to list if necessary
v0.13.0,cast to list if necessary
v0.13.0,"first, check if pymongo is installed"
v0.13.0,automatically identify train / test / dev files
v0.13.0,"if no test file is found, take any file with 'test' in name"
v0.13.0,Expose base classses
v0.13.0,Expose all biomedical data sets used for the evaluation of BioBERT
v0.13.0,-
v0.13.0,-
v0.13.0,-
v0.13.0,-
v0.13.0,Expose all biomedical data sets using the HUNER splits
v0.13.0,Expose all biomedical data sets
v0.13.0,Expose all document classification datasets
v0.13.0,word sense disambiguation
v0.13.0,Expose all entity linking datasets
v0.13.0,Expose all relation extraction datasets
v0.13.0,universal proposition banks
v0.13.0,keyphrase detection datasets
v0.13.0,other NER datasets
v0.13.0,standard NER datasets
v0.13.0,Expose all sequence labeling datasets
v0.13.0,Expose all text-image datasets
v0.13.0,Expose all text-text datasets
v0.13.0,Expose all treebanks
v0.13.0,"find train, dev and test files if not specified"
v0.13.0,get train data
v0.13.0,get test data
v0.13.0,get dev data
v0.13.0,option 1: read only sentence boundaries as offset positions
v0.13.0,option 2: keep everything in memory
v0.13.0,"if in memory, retrieve parsed sentence"
v0.13.0,else skip to position in file where sentence begins
v0.13.0,current token ID
v0.13.0,handling for the awful UD multiword format
v0.13.0,end of sentence
v0.13.0,comments or ellipsis
v0.13.0,if token is a multi-word
v0.13.0,normal single-word tokens
v0.13.0,"if we don't split multiwords, skip over component words"
v0.13.0,add token
v0.13.0,add morphological tags
v0.13.0,derive whitespace logic for multiwords
v0.13.0,print(token)
v0.13.0,print(current_multiword_last_token)
v0.13.0,print(current_multiword_first_token)
v0.13.0,"if multi-word equals component tokens, there should be no whitespace"
v0.13.0,go through all tokens in subword and set whitespace_after information
v0.13.0,print(i)
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,download data if necessary
v0.13.0,this dataset name
v0.13.0,default dataset folder is the cache root
v0.13.0,download data if necessary
v0.13.0,"finally, print model card for information"
v0.13.0,noqa: INP001
v0.13.0,-- Project information -----------------------------------------------------
v0.13.0,"The full version, including alpha/beta/rc tags"
v0.13.0,use smv_current_version as the git url
v0.13.0,-- General configuration ---------------------------------------------------
v0.13.0,"Add any Sphinx extension module names here, as strings. They can be"
v0.13.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v0.13.0,ones.
v0.13.0,"Add any paths that contain templates here, relative to this directory."
v0.13.0,"List of patterns, relative to source directory, that match files and"
v0.13.0,directories to ignore when looking for source files.
v0.13.0,This pattern also affects html_static_path and html_extra_path.
v0.13.0,-- Options for HTML output -------------------------------------------------
v0.13.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v0.13.0,a list of builtin themes.
v0.13.0,
v0.13.0,"Add any paths that contain custom static files (such as style sheets) here,"
v0.13.0,"relative to this directory. They are copied after the builtin static files,"
v0.13.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v0.13.0,Napoleon settings
v0.13.0,Whitelist pattern for tags (set to None to ignore all tags)
v0.13.0,Whitelist pattern for branches (set to None to ignore all branches)
v0.13.0,Whitelist pattern for remotes (set to None to use local branches only)
v0.13.0,Pattern for released versions
v0.13.0,Format for versioned output directories inside the build directory
v0.13.0,Determines whether remote or local git branches/tags are preferred if their output dirs conflict
v0.13.0,test corpus
v0.13.0,create a TARS classifier
v0.13.0,check if right number of classes
v0.13.0,switch to task with only one label
v0.13.0,check if right number of classes
v0.13.0,switch to task with three labels provided as list
v0.13.0,check if right number of classes
v0.13.0,switch to task with four labels provided as set
v0.13.0,check if right number of classes
v0.13.0,switch to task with two labels provided as Dictionary
v0.13.0,check if right number of classes
v0.13.0,test corpus
v0.13.0,create a TARS classifier
v0.13.0,switch to a new task (TARS can do multiple tasks so you must define one)
v0.13.0,initialize the text classifier trainer
v0.13.0,start the training
v0.13.0,"With end symbol, without start symbol, padding in front"
v0.13.0,"Without end symbol, with start symbol, padding in back"
v0.13.0,"Without end symbol, without start symbol, padding in front"
v0.13.0,initialize trainer
v0.13.0,initialize trainer
v0.13.0,initialize trainer
v0.13.0,increment for last token in sentence if not followed by whitespace
v0.13.0,clean up directory
v0.13.0,clean up directory
v0.13.0,example sentence
v0.13.0,set 4 labels for 2 tokens ('love' is tagged twice)
v0.13.0,check if there are three POS labels with correct text and values
v0.13.0,check if there are is one SENTIMENT label with correct text and values
v0.13.0,check if all tokens are correctly labeled
v0.13.0,remove the pos label from the last word
v0.13.0,there should be 2 POS labels left
v0.13.0,now remove all pos tags
v0.13.0,set 3 labels for 2 spans (HU is tagged twice)
v0.13.0,check if there are three labels with correct text and values
v0.13.0,check if there are two spans with correct text and values
v0.13.0,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.13.0,should be only one NER label left
v0.13.0,and only one NER span
v0.13.0,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.13.0,check if there are three labels with correct text and values
v0.13.0,check if there are two spans with correct text and values
v0.13.0,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.13.0,should be only one NER label left
v0.13.0,and only one NER span
v0.13.0,but there is also one orgtype span and label
v0.13.0,and only one NER span
v0.13.0,let's add the NER tag back
v0.13.0,check if there are three labels with correct text and values
v0.13.0,check if there are two spans with correct text and values
v0.13.0,now remove all NER tags
v0.13.0,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.13.0,create two relation label
v0.13.0,there should be two relation labels
v0.13.0,there should be one syntactic labels
v0.13.0,"there should be two relations, one with two and one with one label"
v0.13.0,example sentence
v0.13.0,add another topic label
v0.13.0,example sentence
v0.13.0,has sentiment value
v0.13.0,has 4 part of speech tags
v0.13.0,has 1 NER tag
v0.13.0,should be in total 6 labels
v0.13.0,example sentence
v0.13.0,add two NER labels
v0.13.0,get the four labels
v0.13.0,check that only two of the respective data points are equal
v0.13.0,make a sentence and some right context
v0.13.0,TODO: is this desirable? Or should two sentences with same text be considered same objects?
v0.13.0,Initializing a Sentence this way assumes that there is a space after each token
v0.13.0,get default dictionary
v0.13.0,init forward LM with 128 hidden states and 1 layer
v0.13.0,get the example corpus and process at character level in forward direction
v0.13.0,train the language model
v0.13.0,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.13.0,clean up results directory
v0.13.0,get default dictionary
v0.13.0,init forward LM with 128 hidden states and 1 layer
v0.13.0,get the example corpus and process at character level in forward direction
v0.13.0,train the language model
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,load column dataset with one entry
v0.13.0,load column dataset with two entries
v0.13.0,load column dataset with three entries
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,check if Token labels are correct
v0.13.0,"get training, test and dev data"
v0.13.0,check if Token labels for frames are correct
v0.13.0,"get training, test and dev data"
v0.13.0,"get training, test and dev data"
v0.13.0,get two corpora as one
v0.13.0,"get training, test and dev data for full English UD corpus from web"
v0.13.0,clean up data directory
v0.13.0,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.13.0,"""2"","
v0.13.0,"""0"","
v0.13.0,"""4"","
v0.13.0,"""2"","
v0.13.0,"""2"","
v0.13.0,"""2"","
v0.13.0,]
v0.13.0,This test only covers basic universal dependencies datasets.
v0.13.0,"For example, multi-word tokens or the ""deps"" column sentence annotations are not supported yet."
v0.13.0,"Here, we use the default token annotation fields."
v0.13.0,This test covers the complete HIPE 2022 dataset.
v0.13.0,https://github.com/hipe-eval/HIPE-2022-data
v0.13.0,"Includes variant with document separator, and all versions of the dataset."
v0.13.0,"We have manually checked, that these numbers are correct:"
v0.13.0,"+1 offset, because of missing EOS marker at EOD"
v0.13.0,Test data for v2.1 release
v0.13.0,This test covers the complete ICDAR Europeana corpus:
v0.13.0,https://github.com/stefan-it/historic-domain-adaptation-icdar
v0.13.0,"This test covers the complete MasakhaNER dataset, including support for v1 and v2."
v0.13.0,This test covers the NERMuD dataset. Official stats can be found here:
v0.13.0,https://github.com/dhfbk/KIND/tree/main/evalita-2023
v0.13.0,Number of instances per dataset split are taken from https://huggingface.co/datasets/elenanereiss/german-ler
v0.13.0,This test covers the complete MasakhaPOS dataset.
v0.13.0,"See MobIE paper (https://aclanthology.org/2021.konvens-1.22/), table 2"
v0.13.0,--- Embeddings that are shared by both models --- #
v0.13.0,--- Task 1: Sentiment Analysis (5-class) --- #
v0.13.0,Define corpus and model
v0.13.0,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.13.0,Define corpus and model
v0.13.0,-- Define mapping (which tagger should train on which model) -- #
v0.13.0,-- Create model trainer and train -- #
v0.13.0,clean up file
v0.13.0,no need for label_dict
v0.13.0,check if right number of classes
v0.13.0,switch to task with only one label
v0.13.0,check if right number of classes
v0.13.0,switch to task with three labels provided as list
v0.13.0,check if right number of classes
v0.13.0,switch to task with four labels provided as set
v0.13.0,check if right number of classes
v0.13.0,switch to task with two labels provided as Dictionary
v0.13.0,check if right number of classes
v0.13.0,Intel ----founded_by---> Gordon Moore
v0.13.0,Intel ----founded_by---> Robert Noyce
v0.13.0,"Ground truth is a set of tuples of (<Sentence Text>, <Relation Label Values>)"
v0.13.0,Check sentence masking and relation label annotation on
v0.13.0,"training, validation and test dataset (in this test the splits are the same)"
v0.13.0,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.13.0,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.13.0,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.13.0,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
v0.13.0,This sentence is only included if we transform the corpus with cross augmentation
v0.13.0,Ensure this is an example that predicts no classes in multilabel
v0.13.0,check if right number of classes
v0.13.0,switch to task with only one label
v0.13.0,check if right number of classes
v0.13.0,switch to task with three labels provided as list
v0.13.0,check if right number of classes
v0.13.0,switch to task with four labels provided as set
v0.13.0,check if right number of classes
v0.13.0,switch to task with two labels provided as Dictionary
v0.13.0,check if right number of classes
v0.13.0,ensure that the prepared tensors is what we expect
v0.13.0,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.13.0,previous and next sentence as context
v0.13.0,test expansion for sentence without context
v0.13.0,test expansion for with previous and next as context
v0.13.0,test expansion if first sentence is document boundary
v0.13.0,test expansion if we don't use context
v0.13.0,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.13.0,dummy model with embeddings
v0.13.0,save the dummy and load it again
v0.13.0,check that context_length and use_context_separator is the same for both
v0.12.2,mmap seems to be much more memory efficient
v0.12.2,Remove quotes from etag
v0.12.2,"If there is an etag, it's everything after the first period"
v0.12.2,"Otherwise, use None"
v0.12.2,"URL, so get it from the cache (downloading if necessary)"
v0.12.2,"File, and it exists."
v0.12.2,"File, but it doesn't exist."
v0.12.2,Something unknown
v0.12.2,Extract all the contents of zip file in current directory
v0.12.2,Extract all the contents of zip file in current directory
v0.12.2,TODO(joelgrus): do we want to do checksums or anything like that?
v0.12.2,get cache path to put the file
v0.12.2,make HEAD request to check ETag
v0.12.2,add ETag to filename if it exists
v0.12.2,"etag = response.headers.get(""ETag"")"
v0.12.2,"Download to temporary file, then copy to cache dir once finished."
v0.12.2,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.12.2,GET file object
v0.12.2,These defaults are the same as the argument defaults in tqdm.
v0.12.2,load_big_file is a workaround byhttps://github.com/highway11git
v0.12.2,to load models on some Mac/Windows setups
v0.12.2,see https://github.com/zalandoresearch/flair/issues/351
v0.12.2,first determine the distribution of classes in the dataset
v0.12.2,weight for each sample
v0.12.2,Create blocks
v0.12.2,shuffle the blocks
v0.12.2,concatenate the shuffled blocks
v0.12.2,Create blocks
v0.12.2,shuffle the blocks
v0.12.2,concatenate the shuffled blocks
v0.12.2,increment for last token in sentence if not followed by whitespace
v0.12.2,this is the default init size of a lmdb database for embeddings
v0.12.2,get db filename from embedding name
v0.12.2,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.12.2,SequenceTagger
v0.12.2,TextClassifier
v0.12.2,get db filename from embedding name
v0.12.2,if embedding database already exists
v0.12.2,"otherwise, push embedding to database"
v0.12.2,if embedding database already exists
v0.12.2,open the database in read mode
v0.12.2,we need to set self.k
v0.12.2,create and load the database in write mode
v0.12.2,"no idea why, but we need to close and reopen the environment to avoid"
v0.12.2,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.12.2,when opening new transaction !
v0.12.2,init dictionaries
v0.12.2,"in order to deal with unknown tokens, add <unk>"
v0.12.2,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.12.2,set 'add_unk' depending on whether <unk> is a key
v0.12.2,"if one embedding name, directly return it"
v0.12.2,"if multiple embedding names, concatenate them"
v0.12.2,First we remove any existing labels for this PartOfSentence in self.sentence
v0.12.2,labels also need to be deleted at Sentence object
v0.12.2,delete labels at object itself
v0.12.2,"check if the span already exists. If so, return it"
v0.12.2,else make a new span
v0.12.2,"check if the relation already exists. If so, return it"
v0.12.2,else make a new relation
v0.12.2,private field for all known spans
v0.12.2,the tokenizer used for this sentence
v0.12.2,some sentences represent a document boundary (but most do not)
v0.12.2,internal variables to denote position inside dataset
v0.12.2,"if text is passed, instantiate sentence with tokens (words)"
v0.12.2,determine token positions and whitespace_after flag
v0.12.2,the last token has no whitespace after
v0.12.2,log a warning if the dataset is empty
v0.12.2,data with zero-width characters cannot be handled
v0.12.2,set token idx and sentence
v0.12.2,append token to sentence
v0.12.2,register token annotations on sentence
v0.12.2,move sentence embeddings to device
v0.12.2,also move token embeddings to device
v0.12.2,clear token embeddings
v0.12.2,infer whitespace after field
v0.12.2,"if sentence has no tokens, return empty string"
v0.12.2,"otherwise, return concatenation of tokens with the correct offsets"
v0.12.2,The sentence's start position is not propagated to its tokens.
v0.12.2,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.12.2,No character at the corresponding code point: remove it
v0.12.2,"if no label if specified, return all labels"
v0.12.2,"if the label type exists in the Sentence, return it"
v0.12.2,return empty list if none of the above
v0.12.2,labels also need to be deleted at all tokens
v0.12.2,labels also need to be deleted at all known spans
v0.12.2,remove spans without labels
v0.12.2,delete labels at object itself
v0.12.2,set name
v0.12.2,abort if no data is provided
v0.12.2,sample test data from train if none is provided
v0.12.2,sample dev data from train if none is provided
v0.12.2,set train dev and test data
v0.12.2,find out empty sentence indices
v0.12.2,create subset of non-empty sentence indices
v0.12.2,find out empty sentence indices
v0.12.2,create subset of non-empty sentence indices
v0.12.2,count all label types per sentence
v0.12.2,go through all labels of label_type and count values
v0.12.2,check if there are any span labels
v0.12.2,"if an unk threshold is set, UNK all label values below this threshold"
v0.12.2,sample randomly from a label distribution according to the probabilities defined by the noise transition matrix
v0.12.2,replace the old label with the new one
v0.12.2,keep track of the old (clean) label using another label type category
v0.12.2,keep track of how many labels in total are flipped
v0.12.2,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.12.2,replace the old label with the new one
v0.12.2,keep track of the old (clean) label using another label type category
v0.12.2,keep track of how many labels in total are flipped
v0.12.2,Make the tag dictionary
v0.12.2,"add a dummy ""O"" to close final prediction"
v0.12.2,return complex list
v0.12.2,internal variables
v0.12.2,non-set tags are OUT tags
v0.12.2,anything that is not OUT is IN
v0.12.2,does this prediction start a new span?
v0.12.2,begin and single tags start new spans
v0.12.2,"in IOB format, an I tag starts a span if it follows an O or is a different span"
v0.12.2,single tags that change prediction start new spans
v0.12.2,if an existing span is ended (either by reaching O or starting a new span)
v0.12.2,determine score and value
v0.12.2,append to result list
v0.12.2,reset for-loop variables for new span
v0.12.2,remember previous tag
v0.12.2,global variable: cache_root
v0.12.2,global variable: device
v0.12.2,"No need for correctness checks, torch is doing it"
v0.12.2,global variable: version
v0.12.2,global variable: arrow symbol
v0.12.2,dummy return to fulfill trainer.train() needs
v0.12.2,print(vec)
v0.12.2,Attach optimizer
v0.12.2,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.12.2,if memory mode option 'none' delete everything
v0.12.2,"if dynamic embedding keys not passed, identify them automatically"
v0.12.2,always delete dynamic embeddings
v0.12.2,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.12.2,optional metric space decoder if prototypes have different length than embedding
v0.12.2,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.12.2,"if set, create initial prototypes from normal distribution"
v0.12.2,"if set, use a radius"
v0.12.2,all parameters will be pushed internally to the specified device
v0.12.2,decode embeddings into prototype space
v0.12.2,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.12.2,Always include the name of the Model class for which the state dict holds
v0.12.2,"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
v0.12.2,"write out a ""model card"" if one is set"
v0.12.2,special handling for optimizer:
v0.12.2,remember optimizer class and state dictionary
v0.12.2,save model
v0.12.2,restore optimizer and scheduler to model card if set
v0.12.2,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.12.2,get all non-abstract subclasses
v0.12.2,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.12.2,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.12.2,"if the model cannot be fetched, load as a file"
v0.12.2,try to get model class from state
v0.12.2,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.12.2,"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
v0.12.2,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.12.2,"if this class is not abstract, fetch the model and load it"
v0.12.2,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.12.2,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.12.2,loss calculation
v0.12.2,variables for printing
v0.12.2,variables for computing scores
v0.12.2,remove any previously predicted labels
v0.12.2,predict for batch
v0.12.2,get the gold labels
v0.12.2,add to all_predicted_values
v0.12.2,make printout lines
v0.12.2,convert true and predicted values to two span-aligned lists
v0.12.2,delete exluded labels if exclude_labels is given
v0.12.2,"if after excluding labels, no label is left, ignore the datapoint"
v0.12.2,write all_predicted_values to out_file if set
v0.12.2,make the evaluation dictionary
v0.12.2,check if this is a multi-label problem
v0.12.2,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.12.2,multi-label problems require a multi-hot vector for each true and predicted label
v0.12.2,single-label problems can do with a single index for each true and predicted label
v0.12.2,"now, calculate evaluation numbers"
v0.12.2,there is at least one gold label or one prediction (default)
v0.12.2,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.12.2,"micro average is only computed if zero-label exists (for instance ""O"")"
v0.12.2,if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
v0.12.2,same for the main score
v0.12.2,issue error and default all evaluation numbers to 0.
v0.12.2,line for log file
v0.12.2,check if there is a label mismatch
v0.12.2,print info
v0.12.2,set the embeddings
v0.12.2,initialize the label dictionary
v0.12.2,initialize the decoder
v0.12.2,set up multi-label logic
v0.12.2,init dropouts
v0.12.2,loss weights and loss function
v0.12.2,Initialize the weight tensor
v0.12.2,set up gradient reversal if so specified
v0.12.2,embed sentences
v0.12.2,get a tensor of data points
v0.12.2,do dropout
v0.12.2,make a forward pass to produce embedded data points and labels
v0.12.2,get the data points for which to predict labels
v0.12.2,get their gold labels as a tensor
v0.12.2,pass data points through network to get encoded data point tensor
v0.12.2,decode
v0.12.2,an optional masking step (no masking in most cases)
v0.12.2,calculate the loss
v0.12.2,filter empty sentences
v0.12.2,reverse sort all sequences by their length
v0.12.2,progress bar for verbosity
v0.12.2,filter data points in batch
v0.12.2,stop if all sentences are empty
v0.12.2,pass data points through network and decode
v0.12.2,if anything could possibly be predicted
v0.12.2,remove previously predicted labels of this type
v0.12.2,add DefaultClassifier arguments
v0.12.2,add variables of DefaultClassifier
v0.12.2,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.12.2,Get projected 1st dimension
v0.12.2,Compute bilinear form
v0.12.2,Arcosh
v0.12.2,Project the input data to n+1 dimensions
v0.12.2,"The first dimension, is recomputed in the distance module"
v0.12.2,header for 'weights.txt'
v0.12.2,"determine the column index of loss, f-score and accuracy for"
v0.12.2,"train, dev and test split"
v0.12.2,then get all relevant values from the tsv
v0.12.2,then get all relevant values from the tsv
v0.12.2,plot i
v0.12.2,save plots
v0.12.2,save plots
v0.12.2,plt.show()
v0.12.2,save plot
v0.12.2,take the average over the last three scores of training
v0.12.2,take average over the scores from the different training runs
v0.12.2,auto-spawn on GPU if available
v0.12.2,progress bar for verbosity
v0.12.2,stop if all sentences are empty
v0.12.2,clearing token embeddings to save memory
v0.12.2,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.12.2,TODO: not saving lines yet
v0.12.2,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.12.2,- MaskedRelationClassifier ?
v0.12.2,This depends if this relation classification architecture should replace or offer as an alternative.
v0.12.2,Set label type and prepare label dictionary
v0.12.2,Initialize super default classifier
v0.12.2,Add the special tokens from the encoding strategy
v0.12.2,"Auto-spawn on GPU, if available"
v0.12.2,Only use entities labelled with the specified labels for each label type
v0.12.2,Only use entities above the specified threshold
v0.12.2,Use a dictionary to find gold relation annotations for a given entity pair
v0.12.2,Yield head and tail entity pairs from the cross product of all entities
v0.12.2,Remove identity relation entity pairs
v0.12.2,Remove entity pairs with labels that do not match any
v0.12.2,of the specified relations in `self.entity_pair_labels`
v0.12.2,"Obtain gold label, if existing"
v0.12.2,Some sanity checks
v0.12.2,Pre-compute non-leading head and tail tokens for entity masking
v0.12.2,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.12.2,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.12.2,"Therefore, we use the span's position in the sentence."
v0.12.2,Create masked sentence
v0.12.2,Add gold relation annotation as sentence label
v0.12.2,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.12.2,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.12.2,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.12.2,"may get distributed into different splits. For training purposes, this is always undesired."
v0.12.2,Ensure that all sentences are encoded properly
v0.12.2,Deal with the case where all sentences are encoded sentences
v0.12.2,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.12.2,Deal with the case where all sentences are standard (non-encoded) sentences
v0.12.2,"For each encoded sentence, transfer its prediction onto the original relation"
v0.12.2,auto-spawn on GPU if available
v0.12.2,pad strings with whitespaces to longest sentence
v0.12.2,cut up the input into chunks of max charlength = chunk_size
v0.12.2,push each chunk through the RNN language model
v0.12.2,concatenate all chunks to make final output
v0.12.2,initial hidden state
v0.12.2,get predicted weights
v0.12.2,divide by temperature
v0.12.2,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.12.2,this makes a vector in which the largest value is 0
v0.12.2,compute word weights with exponential function
v0.12.2,try sampling multinomial distribution for next character
v0.12.2,print(word_idx)
v0.12.2,input ids
v0.12.2,push list of character IDs through model
v0.12.2,the target is always the next character
v0.12.2,use cross entropy loss to compare output of forward pass with targets
v0.12.2,exponentiate cross-entropy loss to calculate perplexity
v0.12.2,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.12.2,serialize the language models and the constructor arguments (but nothing else)
v0.12.2,special handling for deserializing language models
v0.12.2,re-initialize language model with constructor arguments
v0.12.2,copy over state dictionary to self
v0.12.2,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.12.2,"in their ""self.train()"" method)"
v0.12.2,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.12.2,"check if this is the case and if so, set it"
v0.12.2,Transform input data into TARS format
v0.12.2,"if there are no labels, return a random sample as negatives"
v0.12.2,"otherwise, go through all labels"
v0.12.2,make sure the probabilities always sum up to 1
v0.12.2,get and embed all labels by making a Sentence object that contains only the label text
v0.12.2,get each label embedding and scale between 0 and 1
v0.12.2,compute similarity matrix
v0.12.2,"the higher the similarity, the greater the chance that a label is"
v0.12.2,sampled as negative example
v0.12.2,make label dictionary if no Dictionary object is passed
v0.12.2,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.12.2,check if candidate_label_set is empty
v0.12.2,make list if only one candidate label is passed
v0.12.2,create label dictionary
v0.12.2,note current task
v0.12.2,create a temporary task
v0.12.2,make zero shot predictions
v0.12.2,switch to the pre-existing task
v0.12.2,prepare TARS dictionary
v0.12.2,initialize a bare-bones sequence tagger
v0.12.2,transformer separator
v0.12.2,Store task specific labels since TARS can handle multiple tasks
v0.12.2,make a tars sentence where all labels are O by default
v0.12.2,init new TARS classifier
v0.12.2,set all task information
v0.12.2,return
v0.12.2,with torch.no_grad():
v0.12.2,progress bar for verbosity
v0.12.2,stop if all sentences are empty
v0.12.2,go through each sentence in the batch
v0.12.2,always remove tags first
v0.12.2,get the span and its label
v0.12.2,determine whether tokens in this span already have a label
v0.12.2,only add if all tokens have no label
v0.12.2,make and add a corresponding predicted span
v0.12.2,set indices so that no token can be tagged twice
v0.12.2,clearing token embeddings to save memory
v0.12.2,"all labels default to ""O"""
v0.12.2,set gold token-level
v0.12.2,set predicted token-level
v0.12.2,now print labels in CoNLL format
v0.12.2,prepare TARS dictionary
v0.12.2,initialize a bare-bones sequence tagger
v0.12.2,transformer separator
v0.12.2,Store task specific labels since TARS can handle multiple tasks
v0.12.2,get the serialized embeddings
v0.12.2,remap state dict for models serialized with Flair <= 0.11.3
v0.12.2,init new TARS classifier
v0.12.2,set all task information
v0.12.2,with torch.no_grad():
v0.12.2,progress bar for verbosity
v0.12.2,stop if all sentences are empty
v0.12.2,go through each sentence in the batch
v0.12.2,always remove tags first
v0.12.2,add all labels that according to TARS match the text and are above threshold
v0.12.2,do not add labels below confidence threshold
v0.12.2,only use label with highest confidence if enforcing single-label predictions
v0.12.2,get all label scores and do an argmax to get the best label
v0.12.2,remove previously added labels and only add the best label
v0.12.2,clearing token embeddings to save memory
v0.12.2,set separator to concatenate two sentences
v0.12.2,auto-spawn on GPU if available
v0.12.2,pooling operation to get embeddings for entites
v0.12.2,set embeddings
v0.12.2,set relation and entity label types
v0.12.2,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.12.2,filter entity pairs according to their tags if set
v0.12.2,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.12.2,character dictionary for decoding and encoding
v0.12.2,make sure <unk> is in dictionary for handling of unknown characters
v0.12.2,add special symbols to dictionary if necessary and save respective indices
v0.12.2,---- ENCODER ----
v0.12.2,encoder character embeddings
v0.12.2,encoder pre-trained embeddings
v0.12.2,encoder RNN
v0.12.2,additional encoder linear layer if bidirectional encoding
v0.12.2,---- DECODER ----
v0.12.2,decoder: linear layers to transform vectors to and from alphabet_size
v0.12.2,when using attention we concatenate attention outcome and decoder hidden states
v0.12.2,decoder RNN
v0.12.2,loss and softmax
v0.12.2,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.12.2,add additional columns for special symbols if necessary
v0.12.2,initialize with dummy symbols
v0.12.2,encode inputs
v0.12.2,get labels (we assume each token has a lemma label)
v0.12.2,get char indices for labels of sentence
v0.12.2,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.12.2,max_sequence_length = length of longest label of sentence + 1
v0.12.2,get char embeddings
v0.12.2,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.12.2,take decoder input and initial hidden and pass through RNN
v0.12.2,"if all encoder outputs are provided, use attention"
v0.12.2,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.12.2,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.12.2,get all tokens
v0.12.2,encode input characters by sending them through RNN
v0.12.2,get one-hots for characters and add special symbols / padding
v0.12.2,determine length of each token
v0.12.2,embed sentences
v0.12.2,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.12.2,variable to store initial hidden states for decoder
v0.12.2,encode input characters by sending them through RNN
v0.12.2,test packing and padding
v0.12.2,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.12.2,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.12.2,decoder later with self.emb_to_hidden
v0.12.2,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.12.2,use token embedding as initial hidden state for decoder
v0.12.2,concatenate everything together and project to appropriate size for decoder
v0.12.2,variable to store initial hidden states for decoder
v0.12.2,encode input characters by sending them through RNN
v0.12.2,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.12.2,embed character one-hots
v0.12.2,send through encoder RNN (produces initial hidden for decoder)
v0.12.2,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.12.2,project 2*hidden_size to hidden_size
v0.12.2,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.12.2,later with self.emb_to_hidden
v0.12.2,use token embedding as initial hidden state for decoder
v0.12.2,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.12.2,concatenate everything together and project to appropriate size for decoder
v0.12.2,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.12.2,"create target vector (batch_size, max_label_seq_length + 1)"
v0.12.2,filter empty sentences
v0.12.2,max length of the predicted sequences
v0.12.2,for printing
v0.12.2,stop if all sentences are empty
v0.12.2,remove previously predicted labels of this type
v0.12.2,create list of tokens in batch
v0.12.2,encode inputs
v0.12.2,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.12.2,sequence length is always set to one in prediction
v0.12.2,option 1: greedy decoding
v0.12.2,predictions
v0.12.2,decode next character
v0.12.2,pick top beam size many outputs with highest probabilities
v0.12.2,option 2: beam search
v0.12.2,out_probs = self.softmax(output_vectors).squeeze(1)
v0.12.2,make sure no dummy symbol <> or start symbol <S> is predicted
v0.12.2,pick top beam size many outputs with highest probabilities
v0.12.2,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.12.2,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.12.2,keep scores of beam_size many hypothesis for each token in the batch
v0.12.2,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.12.2,save sequences so far
v0.12.2,keep track of how many hypothesis were completed for each token
v0.12.2,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.12.2,decode with log softmax
v0.12.2,make sure no dummy symbol <> or start symbol <S> is predicted
v0.12.2,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.12.2,"if the sequence is already ended, do not record as candidate"
v0.12.2,index of token in in list tokens_in_batch
v0.12.2,print(token_number)
v0.12.2,hypothesis score
v0.12.2,TODO: remove token if number of completed hypothesis exceeds given value
v0.12.2,set score of corresponding entry to -inf so it will not be expanded
v0.12.2,get leading_indices for next expansion
v0.12.2,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.12.2,take beam_size many copies of scores vector and add scores of possible new extensions
v0.12.2,"size (beam_size*batch_size, beam_size)"
v0.12.2,print(hypothesis_scores)
v0.12.2,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.12.2,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.12.2,print(hypothesis_scores_per_token)
v0.12.2,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.12.2,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.12.2,a list of length beam_size*batch_size
v0.12.2,"where the first three inidices belong to the first token, the next three to the second token,"
v0.12.2,and so on
v0.12.2,with these indices we can compute the tensors for the next iteration
v0.12.2,expand sequences with corresponding index
v0.12.2,add log-probabilities to the scores
v0.12.2,save new leading indices
v0.12.2,save corresponding hidden states
v0.12.2,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.12.2,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.12.2,get best final hypothesis for each token
v0.12.2,get characters from index sequences and add predicted label to token
v0.12.2,dictionaries
v0.12.2,all parameters will be pushed internally to the specified device
v0.12.2,now print labels in CoNLL format
v0.12.2,internal candidate lists of generator
v0.12.2,load Zelda candidates if so passed
v0.12.2,create candidate lists
v0.12.2,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.12.2,create a new dictionary for lower cased mentions
v0.12.2,go through each mention and its candidates
v0.12.2,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.12.2,set lowercased version as map
v0.12.2,remap state dict for models serialized with Flair <= 0.11.3
v0.12.2,get the candidates
v0.12.2,"during training, add the gold value as candidate"
v0.12.2,----- Create the internal tag dictionary -----
v0.12.2,span-labels need special encoding (BIO or BIOES)
v0.12.2,the big question is whether the label dictionary should contain an UNK or not
v0.12.2,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.12.2,"with UNK, the model learns less well if there are no UNK examples"
v0.12.2,is this a span prediction problem?
v0.12.2,----- Embeddings -----
v0.12.2,----- Initial loss weights parameters -----
v0.12.2,----- RNN specific parameters -----
v0.12.2,----- Conditional Random Field parameters -----
v0.12.2,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.12.2,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.12.2,----- Dropout parameters -----
v0.12.2,dropouts
v0.12.2,remove word dropout if there is no contact over the sequence dimension.
v0.12.2,----- Model layers -----
v0.12.2,----- RNN layer -----
v0.12.2,"If shared RNN provided, else create one for model"
v0.12.2,Whether to train initial hidden state
v0.12.2,final linear map to tag space
v0.12.2,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.12.2,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.12.2,"if there are no sentences, there is no loss"
v0.12.2,forward pass to get scores
v0.12.2,calculate loss given scores and labels
v0.12.2,make a zero-padded tensor for the whole sentence
v0.12.2,linear map to tag space
v0.12.2,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.12.2,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.12.2,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.12.2,spans need to be encoded as token-level predictions
v0.12.2,all others are regular labels for each token
v0.12.2,make sure it's a list
v0.12.2,filter empty sentences
v0.12.2,reverse sort all sequences by their length
v0.12.2,progress bar for verbosity
v0.12.2,stop if all sentences are empty
v0.12.2,get features from forward propagation
v0.12.2,remove previously predicted labels of this type
v0.12.2,"if return_loss, get loss value"
v0.12.2,make predictions
v0.12.2,add predictions to Sentence
v0.12.2,BIOES-labels need to be converted to spans
v0.12.2,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.12.2,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.12.2,core Flair models on Huggingface ModelHub
v0.12.2,"Large NER models,"
v0.12.2,Multilingual NER models
v0.12.2,English POS models
v0.12.2,Multilingual POS models
v0.12.2,English SRL models
v0.12.2,English chunking models
v0.12.2,Language-specific NER models
v0.12.2,Language-specific POS models
v0.12.2,English NER models
v0.12.2,Multilingual NER models
v0.12.2,English POS models
v0.12.2,Multilingual POS models
v0.12.2,English SRL models
v0.12.2,English chunking models
v0.12.2,Danish models
v0.12.2,German models
v0.12.2,French models
v0.12.2,Dutch models
v0.12.2,Malayalam models
v0.12.2,Portuguese models
v0.12.2,Keyphase models
v0.12.2,Biomedical models
v0.12.2,check if model name is a valid local file
v0.12.2,"check if model key is remapped to HF key - if so, print out information"
v0.12.2,get mapped name
v0.12.2,use mapped name instead
v0.12.2,"if not, check if model key is remapped to direct download location. If so, download model"
v0.12.2,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.12.2,"for all other cases (not local file or special download location), use HF model hub"
v0.12.2,"if not a local file, get from model hub"
v0.12.2,use model name as subfolder
v0.12.2,Lazy import
v0.12.2,output information
v0.12.2,## Demo: How to use in Flair
v0.12.2,load tagger
v0.12.2,make example sentence
v0.12.2,predict NER tags
v0.12.2,print sentence
v0.12.2,print predicted NER spans
v0.12.2,iterate over entities and print
v0.12.2,Lazy import
v0.12.2,Save model weight
v0.12.2,Determine if model card already exists
v0.12.2,Generate and save model card
v0.12.2,Upload files
v0.12.2,"all labels default to ""O"""
v0.12.2,set gold token-level
v0.12.2,set predicted token-level
v0.12.2,now print labels in CoNLL format
v0.12.2,print labels in CoNLL format
v0.12.2,the multi task model has several labels
v0.12.2,biomedical models
v0.12.2,entity linker
v0.12.2,auto-spawn on GPU if available
v0.12.2,remap state dict for models serialized with Flair <= 0.11.3
v0.12.2,English sentiment models
v0.12.2,Communicative Functions Model
v0.12.2,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.12.2,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.12.2,"Initially, get scores from <start> tag to all other tags"
v0.12.2,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.12.2,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.12.2,Create a tensor to hold accumulated sequence scores at each current tag
v0.12.2,Create a tensor to hold back-pointers
v0.12.2,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.12.2,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.12.2,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.12.2,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.12.2,"If sentence is over, add transition to STOP-tag"
v0.12.2,Decode/trace best path backwards
v0.12.2,Sanity check
v0.12.2,remove start-tag and backscore to stop-tag
v0.12.2,Max + Softmax to get confidence score for predicted label and append label to each token
v0.12.2,"Transitions are used in the following way: transitions[to, from]."
v0.12.2,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.12.2,to START-tag and from STOP-tag to any other tag to -10000.
v0.12.2,create a model card for this model with Flair and PyTorch version
v0.12.2,also record Transformers version if library is loaded
v0.12.2,remember all parameters used in train() call
v0.12.2,add model card to model
v0.12.2,"if optimizer class is passed, instantiate:"
v0.12.2,"determine what splits (train, dev, test) to evaluate and log"
v0.12.2,prepare loss logging file and set up header
v0.12.2,"from here on, use list of learning rates"
v0.12.2,load existing optimizer state dictionary if it exists
v0.12.2,"minimize training loss if training with dev data, else maximize dev score"
v0.12.2,"if scheduler is passed as a class, instantiate"
v0.12.2,"if we load a checkpoint, we have already trained for epoch"
v0.12.2,"Determine whether to log ""bad epochs"" information"
v0.12.2,load existing scheduler state dictionary if it exists
v0.12.2,update optimizer and scheduler in model card
v0.12.2,"if training also uses dev/train data, include in training set"
v0.12.2,initialize sampler if provided
v0.12.2,init with default values if only class is provided
v0.12.2,set dataset to sample from
v0.12.2,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.12.2,At any point you can hit Ctrl + C to break out of training early.
v0.12.2,update epoch in model card
v0.12.2,get new learning rate
v0.12.2,reload last best model if annealing with restarts is enabled
v0.12.2,stop training if learning rate becomes too small
v0.12.2,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.12.2,process mini-batches
v0.12.2,zero the gradients on the model and optimizer
v0.12.2,"if necessary, make batch_steps"
v0.12.2,forward and backward for batch
v0.12.2,forward pass
v0.12.2,Backward
v0.12.2,identify dynamic embeddings (always deleted) on first sentence
v0.12.2,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.2,do the optimizer step
v0.12.2,do the scheduler step if one-cycle or linear decay
v0.12.2,get new learning rate
v0.12.2,evaluate on train / dev / test split depending on training settings
v0.12.2,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.2,calculate scores using dev data if available
v0.12.2,append dev score to score history
v0.12.2,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.2,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.2,determine if this is the best model or if we need to anneal
v0.12.2,default mode: anneal against dev score
v0.12.2,alternative: anneal against dev loss
v0.12.2,alternative: anneal against train loss
v0.12.2,determine bad epoch number
v0.12.2,lr unchanged
v0.12.2,log bad epochs
v0.12.2,output log file
v0.12.2,make headers on first epoch
v0.12.2,"if checkpoint is enabled, save model at each epoch"
v0.12.2,Check whether to save best model
v0.12.2,"if we do not use dev data for model selection, save final model"
v0.12.2,test best model if test data is present
v0.12.2,recover all arguments that were used to train this model
v0.12.2,you can overwrite params with your own
v0.12.2,surface nested arguments
v0.12.2,resume training with these parameters
v0.12.2,"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name"
v0.12.2,get and return the final test score of best model
v0.12.2,cast string to Path
v0.12.2,forward pass
v0.12.2,update optimizer and scheduler
v0.12.2,"TextDataset returns a list. valid and test are only one file,"
v0.12.2,so return the first element
v0.12.2,cast string to Path
v0.12.2,error message if the validation dataset is too small
v0.12.2,Shuffle training files randomly after serially iterating
v0.12.2,through corpus one
v0.12.2,"iterate through training data, starting at"
v0.12.2,self.split (for checkpointing)
v0.12.2,off by one for printing
v0.12.2,go into train mode
v0.12.2,reset variables
v0.12.2,not really sure what this does
v0.12.2,do the forward pass in the model
v0.12.2,try to predict the targets
v0.12.2,Backward
v0.12.2,`clip_grad_norm` helps prevent the exploding gradient
v0.12.2,problem in RNNs / LSTMs.
v0.12.2,We detach the hidden state from how it was
v0.12.2,previously produced.
v0.12.2,"If we didn't, the model would try backpropagating"
v0.12.2,all the way to start of the dataset.
v0.12.2,explicitly remove loss to clear up memory
v0.12.2,#########################################################
v0.12.2,Save the model if the validation loss is the best we've
v0.12.2,seen so far.
v0.12.2,#########################################################
v0.12.2,print info
v0.12.2,#########################################################
v0.12.2,##############################################################################
v0.12.2,final testing
v0.12.2,##############################################################################
v0.12.2,Turn on evaluation mode which disables dropout.
v0.12.2,Work out how cleanly we can divide the dataset into bsz parts.
v0.12.2,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.12.2,Evenly divide the data across the bsz batches.
v0.12.2,"the default model for ELMo is the 'original' model, which is very large"
v0.12.2,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.12.2,put on Cuda if available
v0.12.2,embed a dummy sentence to determine embedding_length
v0.12.2,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.12.2,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.12.2,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.12.2,news-english-forward
v0.12.2,news-english-backward
v0.12.2,news-english-forward
v0.12.2,news-english-backward
v0.12.2,mix-english-forward
v0.12.2,mix-english-backward
v0.12.2,mix-german-forward
v0.12.2,mix-german-backward
v0.12.2,common crawl Polish forward
v0.12.2,common crawl Polish backward
v0.12.2,Slovenian forward
v0.12.2,Slovenian backward
v0.12.2,Bulgarian forward
v0.12.2,Bulgarian backward
v0.12.2,Dutch forward
v0.12.2,Dutch backward
v0.12.2,Swedish forward
v0.12.2,Swedish backward
v0.12.2,French forward
v0.12.2,French backward
v0.12.2,Czech forward
v0.12.2,Czech backward
v0.12.2,Portuguese forward
v0.12.2,Portuguese backward
v0.12.2,initialize cache if use_cache set
v0.12.2,embed a dummy sentence to determine embedding_length
v0.12.2,set to eval mode
v0.12.2,Copy the object's state from self.__dict__ which contains
v0.12.2,all our instance attributes. Always use the dict.copy()
v0.12.2,method to avoid modifying the original state.
v0.12.2,Remove the unpicklable entries.
v0.12.2,"if cache is used, try setting embeddings from cache first"
v0.12.2,try populating embeddings from cache
v0.12.2,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.12.2,get hidden states from language model
v0.12.2,take first or last hidden states from language model as word representation
v0.12.2,if self.tokenized_lm or token.whitespace_after:
v0.12.2,1-camembert-base -> camembert-base
v0.12.2,1-xlm-roberta-large -> xlm-roberta-large
v0.12.2,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.12.2,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.12.2,tokens are attended to.
v0.12.2,Zero-pad up to the sequence length.
v0.12.2,"first, find longest sentence in batch"
v0.12.2,prepare id maps for BERT model
v0.12.2,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.12.2,get aggregated embeddings for each BERT-subtoken in sentence
v0.12.2,get the current sentence object
v0.12.2,add concatenated embedding to sentence
v0.12.2,use first subword embedding if pooling operation is 'first'
v0.12.2,"otherwise, do a mean over all subwords in token"
v0.12.2,"if only one sentence is passed, convert to list of sentence"
v0.12.2,bidirectional LSTM on top of embedding layer
v0.12.2,dropouts
v0.12.2,"first, sort sentences by number of tokens"
v0.12.2,go through each sentence in batch
v0.12.2,PADDING: pad shorter sentences out
v0.12.2,ADD TO SENTENCE LIST: add the representation
v0.12.2,--------------------------------------------------------------------
v0.12.2,GET REPRESENTATION FOR ENTIRE BATCH
v0.12.2,--------------------------------------------------------------------
v0.12.2,--------------------------------------------------------------------
v0.12.2,FF PART
v0.12.2,--------------------------------------------------------------------
v0.12.2,use word dropout if set
v0.12.2,--------------------------------------------------------------------
v0.12.2,EXTRACT EMBEDDINGS FROM LSTM
v0.12.2,--------------------------------------------------------------------
v0.12.2,embed a dummy sentence to determine embedding_length
v0.12.2,Avoid conflicts with flair's Token class
v0.12.2,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.12.2,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.12.2,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.12.2,add positional encodings
v0.12.2,reshape the pixels into the sequence
v0.12.2,layer norm after convolution and positional encodings
v0.12.2,add <cls> token
v0.12.2,"transformer requires input in the shape [h*w+1, b, d]"
v0.12.2,the output is an embedding of <cls> token
v0.12.2,this parameter is fixed
v0.12.2,optional fine-tuning on top of embedding layer
v0.12.2,"if only one sentence is passed, convert to list of sentence"
v0.12.2,"if only one sentence is passed, convert to list of sentence"
v0.12.2,bidirectional RNN on top of embedding layer
v0.12.2,dropouts
v0.12.2,TODO: remove in future versions
v0.12.2,embed words in the sentence
v0.12.2,before-RNN dropout
v0.12.2,reproject if set
v0.12.2,push through RNN
v0.12.2,after-RNN dropout
v0.12.2,extract embeddings from RNN
v0.12.2,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.12.2,"check if this is the case and if so, set it"
v0.12.2,serialize the language models and the constructor arguments (but nothing else)
v0.12.2,re-initialize language model with constructor arguments
v0.12.2,special handling for deserializing language models
v0.12.2,copy over state dictionary to self
v0.12.2,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.12.2,"in their ""self.train()"" method)"
v0.12.2,IMPORTANT: add embeddings as torch modules
v0.12.2,iterate over sentences
v0.12.2,"if its a forward LM, take last state"
v0.12.2,"convert to plain strings, embedded in a list for the encode function"
v0.12.2,CNN
v0.12.2,dropouts
v0.12.2,TODO: remove in future versions
v0.12.2,embed words in the sentence
v0.12.2,before-RNN dropout
v0.12.2,reproject if set
v0.12.2,push CNN
v0.12.2,after-CNN dropout
v0.12.2,extract embeddings from CNN
v0.12.2,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.12.2,"if only one sentence is passed, convert to list of sentence"
v0.12.2,Expose base classses
v0.12.2,Expose document embedding classes
v0.12.2,Expose image embedding classes
v0.12.2,Expose legacy embedding classes
v0.12.2,Expose token embedding classes
v0.12.2,in some cases we need to insert zero vectors for tokens without embedding.
v0.12.2,padding
v0.12.2,remove special markup
v0.12.2,check if special tokens exist to circumvent error message
v0.12.2,iterate over subtokens and reconstruct tokens
v0.12.2,remove special markup
v0.12.2,check if reconstructed token is special begin token ([CLS] or similar)
v0.12.2,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.12.2,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.12.2,if tokens are unaccounted for
v0.12.2,check if all tokens were matched to subtokens
v0.12.2,The layoutlm tokenizer doesn't handle ocr themselves
v0.12.2,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.12.2,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.12.2,in case of doubt: token embedding has higher priority than document embedding
v0.12.2,random check some tokens to save performance.
v0.12.2,Models such as FNet do not have an attention_mask
v0.12.2,set language IDs for XLM-style transformers
v0.12.2,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.12.2,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.12.2,set context if not set already
v0.12.2,flair specific pre-tokenization
v0.12.2,fields to store left and right context
v0.12.2,expand context only if context_length is set
v0.12.2,"if context_dropout is set, randomly deactivate left context during training"
v0.12.2,"if context_dropout is set, randomly deactivate right context during training"
v0.12.2,"if use_context_separator is set, add a [FLERT] token"
v0.12.2,return expanded sentence and context length information
v0.12.2,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.12.2,temporary fix to disable tokenizer parallelism warning
v0.12.2,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.12.2,do not print transformer warnings as these are confusing in this case
v0.12.2,load tokenizer and transformer model
v0.12.2,load tokenizer from inmemory zip-file
v0.12.2,model name
v0.12.2,embedding parameters
v0.12.2,send mini-token through to check how many layers the model has
v0.12.2,return length
v0.12.2,"If we use a context separator, add a new special token"
v0.12.2,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.12.2,"when initializing, embeddings are in eval mode by default"
v0.12.2,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.12.2,"cannot run `.encode` if ocr boxes are required, assume"
v0.12.2,in case of doubt: token embedding has higher priority than document embedding
v0.12.2,in case of doubt: token embedding has higher priority than document embedding
v0.12.2,legacy TransformerDocumentEmbedding
v0.12.2,legacy TransformerTokenEmbedding
v0.12.2,legacy Flair <= 0.12
v0.12.2,legacy Flair <= 0.7
v0.12.2,legacy TransformerTokenEmbedding
v0.12.2,Legacy TransformerDocumentEmbedding
v0.12.2,legacy TransformerTokenEmbedding
v0.12.2,legacy TransformerDocumentEmbedding
v0.12.2,copy values from new embedding
v0.12.2,cls first pooling can be done without recreating sentence hidden states
v0.12.2,make the tuple a tensor; makes working with it easier.
v0.12.2,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.12.2,only use layers that will be outputted
v0.12.2,this parameter is fixed
v0.12.2,IMPORTANT: add embeddings as torch modules
v0.12.2,"if only one sentence is passed, convert to list of sentence"
v0.12.2,make compatible with serialized models
v0.12.2,gensim version 4
v0.12.2,gensim version 3
v0.12.2,"if no embedding is set, the vocab and embedding length is requried"
v0.12.2,GLOVE embeddings
v0.12.2,TURIAN embeddings
v0.12.2,KOMNINOS embeddings
v0.12.2,pubmed embeddings
v0.12.2,FT-CRAWL embeddings
v0.12.2,FT-CRAWL embeddings
v0.12.2,twitter embeddings
v0.12.2,two-letter language code wiki embeddings
v0.12.2,two-letter language code wiki embeddings
v0.12.2,two-letter language code crawl embeddings
v0.12.2,fix serialized models
v0.12.2,"this is required to force the module on the cpu,"
v0.12.2,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.12.2,self.to(..) actually sets the device properly
v0.12.2,this ignores the get_cached_vec method when loading older versions
v0.12.2,it is needed for compatibility reasons
v0.12.2,gensim version 4
v0.12.2,gensim version 3
v0.12.2,use list of common characters if none provided
v0.12.2,translate words in sentence into ints using dictionary
v0.12.2,"sort words by length, for batching and masking"
v0.12.2,chars for rnn processing
v0.12.2,multilingual models
v0.12.2,English models
v0.12.2,Arabic
v0.12.2,Bulgarian
v0.12.2,Czech
v0.12.2,Danish
v0.12.2,German
v0.12.2,Spanish
v0.12.2,Basque
v0.12.2,Persian
v0.12.2,Finnish
v0.12.2,French
v0.12.2,Hebrew
v0.12.2,Hindi
v0.12.2,Croatian
v0.12.2,Indonesian
v0.12.2,Italian
v0.12.2,Japanese
v0.12.2,Malayalam
v0.12.2,Dutch
v0.12.2,Norwegian
v0.12.2,Polish
v0.12.2,Portuguese
v0.12.2,Pubmed
v0.12.2,Slovenian
v0.12.2,Swedish
v0.12.2,Tamil
v0.12.2,Spanish clinical
v0.12.2,CLEF HIPE Shared task
v0.12.2,Amharic
v0.12.2,Ukrainian
v0.12.2,load model if in pretrained model map
v0.12.2,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.12.2,CLEF HIPE models are lowercased
v0.12.2,embeddings are static if we don't do finetuning
v0.12.2,embed a dummy sentence to determine embedding_length
v0.12.2,set to eval mode
v0.12.2,make compatible with serialized models (TODO: remove)
v0.12.2,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.12.2,make compatible with serialized models (TODO: remove)
v0.12.2,gradients are enable if fine-tuning is enabled
v0.12.2,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.12.2,get hidden states from language model
v0.12.2,take first or last hidden states from language model as word representation
v0.12.2,offset mode that extracts at whitespace after last character
v0.12.2,offset mode that extracts at last character
v0.12.2,use the character language model embeddings as basis
v0.12.2,length is twice the original character LM embedding length
v0.12.2,these fields are for the embedding memory
v0.12.2,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.12.2,we re-compute embeddings dynamically at each epoch
v0.12.2,set the memory method
v0.12.2,memory is wiped each time we do a training run
v0.12.2,"if we keep a pooling, it needs to be updated continuously"
v0.12.2,update embedding
v0.12.2,check token.text is empty or not
v0.12.2,set aggregation operation
v0.12.2,add embeddings after updating
v0.12.2,model architecture
v0.12.2,model architecture
v0.12.2,"""pl"","
v0.12.2,download if necessary
v0.12.2,load the model
v0.12.2,"TODO: keep for backwards compatibility, but remove in future"
v0.12.2,save the sentence piece model as binary file (not as path which may change)
v0.12.2,write out the binary sentence piece model into the expected directory
v0.12.2,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.12.2,"otherwise, use normal process and potentially trigger another download"
v0.12.2,"once the modes if there, load it with sentence piece"
v0.12.2,empty words get no embedding
v0.12.2,all other words get embedded
v0.12.2,GLOVE embeddings
v0.12.2,no need to recreate as NILCEmbeddings
v0.12.2,read in test file if exists
v0.12.2,read in dev file if exists
v0.12.2,"find train, dev and test files if not specified"
v0.12.2,Add tags for each annotated span
v0.12.2,Remove leading and trailing whitespaces from annotated spans
v0.12.2,Search start and end token index for current span
v0.12.2,If end index is not found set to last token
v0.12.2,Throw error if indices are not valid
v0.12.2,get train data
v0.12.2,read in test file if exists
v0.12.2,read in dev file if exists
v0.12.2,"find train, dev and test files if not specified"
v0.12.2,special key for space after
v0.12.2,special key for feature columns
v0.12.2,special key for dependency head id
v0.12.2,"store either Sentence objects in memory, or only file offsets"
v0.12.2,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.12.2,determine encoding of text file
v0.12.2,identify which columns are spans and which are word-level
v0.12.2,now load all sentences
v0.12.2,skip first line if to selected
v0.12.2,option 1: keep Sentence objects in memory
v0.12.2,pointer to previous
v0.12.2,parse next sentence
v0.12.2,quit if last sentence reached
v0.12.2,skip banned sentences
v0.12.2,set previous and next sentence for context
v0.12.2,append parsed sentence to list in memory
v0.12.2,option 2: keep source data in memory
v0.12.2,"read lines for next sentence, but don't parse"
v0.12.2,quit if last sentence reached
v0.12.2,append raw lines for each sentence
v0.12.2,we make a distinction between word-level tags and span-level tags
v0.12.2,read first sentence to determine which columns are span-labels
v0.12.2,skip first line if to selected
v0.12.2,check the first 5 sentences
v0.12.2,go through all annotations and identify word- and span-level annotations
v0.12.2,- if a column has at least one BIES we know it's a Span label
v0.12.2,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.12.2,- problem cases are columns for which we see only O - in this case we default to Span
v0.12.2,skip assigned columns
v0.12.2,the space after key is always word-levels
v0.12.2,"if at least one token has a BIES, we know it's a span label"
v0.12.2,"if at least one token has a label other than BIOES, we know it's a token label"
v0.12.2,all remaining columns that are not word-level are span-level
v0.12.2,for column in self.word_level_tag_columns:
v0.12.2,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.12.2,"if sentence ends, break"
v0.12.2,parse comments if possible
v0.12.2,"otherwise, this line is a token. parse and add to sentence"
v0.12.2,check if this sentence is a document boundary
v0.12.2,add span labels
v0.12.2,discard tags from tokens that are not added to the sentence
v0.12.2,parse relations if they are set
v0.12.2,head and tail span indices are 1-indexed and end index is inclusive
v0.12.2,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.12.2,"to set the metadata ""domain"" to ""de-orcas"""
v0.12.2,get fields from line
v0.12.2,get head_id if exists (only in dependency parses)
v0.12.2,initialize token
v0.12.2,go through all columns
v0.12.2,'feats' and 'misc' column should be split into different fields
v0.12.2,special handling for whitespace after
v0.12.2,add each other feature as label-value pair
v0.12.2,get the task name (e.g. 'ner')
v0.12.2,get the label value
v0.12.2,add label
v0.12.2,remap regular tag names
v0.12.2,"if in memory, retrieve parsed sentence"
v0.12.2,else skip to position in file where sentence begins
v0.12.2,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.12.2,use all domains
v0.12.2,iter over all domains / sources and create target files
v0.12.2,Parameters
v0.12.2,The conll representation of coref spans allows spans to
v0.12.2,"overlap. If spans end or begin at the same word, they are"
v0.12.2,"separated by a ""|""."
v0.12.2,The span begins at this word.
v0.12.2,The span begins and ends at this word (single word span).
v0.12.2,"The span is starting, so we record the index of the word."
v0.12.2,"The span for this id is ending, but didn't start at this word."
v0.12.2,Retrieve the start index from the document state and
v0.12.2,add the span to the clusters for this id.
v0.12.2,Parameters
v0.12.2,strip all bracketing information to
v0.12.2,get the actual propbank label.
v0.12.2,Entering into a span for a particular semantic role label.
v0.12.2,We append the label and set the current span for this annotation.
v0.12.2,"If there's no '(' token, but the current_span_label is not None,"
v0.12.2,then we are inside a span.
v0.12.2,We're outside a span.
v0.12.2,"Exiting a span, so we reset the current span label for this annotation."
v0.12.2,The words in the sentence.
v0.12.2,The pos tags of the words in the sentence.
v0.12.2,the pieces of the parse tree.
v0.12.2,The lemmatised form of the words in the sentence which
v0.12.2,have SRL or word sense information.
v0.12.2,The FrameNet ID of the predicate.
v0.12.2,"The sense of the word, if available."
v0.12.2,"The current speaker, if available."
v0.12.2,"Cluster id -> List of (start_index, end_index) spans."
v0.12.2,Cluster id -> List of start_indices which are open for this id.
v0.12.2,Replace brackets in text and pos tags
v0.12.2,with a different token for parse trees.
v0.12.2,only keep ')' if there are nested brackets with nothing in them.
v0.12.2,There are some bad annotations in the CONLL data.
v0.12.2,"They contain no information, so to make this explicit,"
v0.12.2,we just set the parse piece to be None which will result
v0.12.2,in the overall parse tree being None.
v0.12.2,"If this is the first word in the sentence, create"
v0.12.2,empty lists to collect the NER and SRL BIO labels.
v0.12.2,"We can't do this upfront, because we don't know how many"
v0.12.2,"components we are collecting, as a sentence can have"
v0.12.2,variable numbers of SRL frames.
v0.12.2,Create variables representing the current label for each label
v0.12.2,sequence we are collecting.
v0.12.2,"If any annotation marks this word as a verb predicate,"
v0.12.2,we need to record its index. This also has the side effect
v0.12.2,of ordering the verbal predicates by their location in the
v0.12.2,"sentence, automatically aligning them with the annotations."
v0.12.2,"this would not be reached if parse_pieces contained None, hence the cast"
v0.12.2,Non-empty line. Collect the annotation.
v0.12.2,Collect any stragglers or files which might not
v0.12.2,have the '#end document' format for the end of the file.
v0.12.2,this dataset name
v0.12.2,check if data there
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,check if data there
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,download files if not present locally
v0.12.2,we need to slightly modify the original files by adding some new lines after document separators
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,Remove CoNLL-U meta information in the last column
v0.12.2,column format
v0.12.2,dataset name
v0.12.2,data folder: default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,dataset name
v0.12.2,data folder: default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,entity_mapping
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,data validation
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download files if not present locallys
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,# download zip
v0.12.2,merge the files in one as the zip is containing multiples files
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,check if data there
v0.12.2,create folder
v0.12.2,download dataset
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download and parse data if necessary
v0.12.2,create train test dev if not exist
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,If the extracted corpus file is not yet present in dir
v0.12.2,download zip if necessary
v0.12.2,"extracted corpus is not present , so unpacking it."
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download zip
v0.12.2,unpacking the zip
v0.12.2,merge the files in one as the zip is containing multiples files
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.12.2,download files if not present locally
v0.12.2,we need to modify the original files by adding new lines after after the end of each sentence
v0.12.2,if only one language is given
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,"use all languages if explicitly set to ""all"""
v0.12.2,download data if necessary
v0.12.2,initialize comlumncorpus and add it to list
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,"For each language in languages, the file is downloaded if not existent"
v0.12.2,Then a comlumncorpus of that data is created and saved in a list
v0.12.2,this list is handed to the multicorpus
v0.12.2,list that contains the columncopora
v0.12.2,download data if necessary
v0.12.2,"if language not downloaded yet, download it"
v0.12.2,create folder
v0.12.2,get google drive id from list
v0.12.2,download from google drive
v0.12.2,unzip
v0.12.2,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.12.2,transform data into required format
v0.12.2,"the processed dataset has the additional ending ""_new"""
v0.12.2,remove the unprocessed dataset
v0.12.2,initialize comlumncorpus and add it to list
v0.12.2,if no languages are given as argument all languages used in XTREME will be loaded
v0.12.2,if only one language is given
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,"For each language in languages, the file is downloaded if not existent"
v0.12.2,Then a comlumncorpus of that data is created and saved in a list
v0.12.2,This list is handed to the multicorpus
v0.12.2,list that contains the columncopora
v0.12.2,download data if necessary
v0.12.2,"if language not downloaded yet, download it"
v0.12.2,create folder
v0.12.2,download from HU Server
v0.12.2,unzip
v0.12.2,transform data into required format
v0.12.2,initialize comlumncorpus and add it to list
v0.12.2,if only one language is given
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,initialize comlumncorpus and add it to list
v0.12.2,download data if necessary
v0.12.2,unpack and write out in CoNLL column-like format
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,data is not in IOB2 format. Thus we transform it to IOB2
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,rename according to train - test - dev - convention
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,Add missing newline after header
v0.12.2,Workaround for empty tokens
v0.12.2,"Add ""real"" document marker"
v0.12.2,Dataset split mapping
v0.12.2,v2.0 only adds new language and splits for AJMC dataset
v0.12.2,Special document marker for sample splits in AJMC dataset
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download and parse data if necessary
v0.12.2,paths to train and test splits
v0.12.2,init corpus
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download and parse data if necessary
v0.12.2,iterate over all html files
v0.12.2,"get rid of html syntax, we only need the text"
v0.12.2,between all documents we write a separator symbol
v0.12.2,skip empty strings
v0.12.2,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.12.2,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.12.2,sentence splitting and tokenization
v0.12.2,iterate through all annotations and add to corresponding tokens
v0.12.2,find sentence to which annotation belongs
v0.12.2,position within corresponding sentence
v0.12.2,set annotation for tokens of entity mention
v0.12.2,write to out-file in column format
v0.12.2,"in case something goes wrong, delete the dataset and raise error"
v0.12.2,this dataset name
v0.12.2,download and parse data if necessary
v0.12.2,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.12.2,generate qid wikiname dictionaries
v0.12.2,merge dictionaries
v0.12.2,ignore first line
v0.12.2,commented and empty lines
v0.12.2,read all Q-IDs
v0.12.2,ignore first line
v0.12.2,request
v0.12.2,this dataset name
v0.12.2,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.12.2,like this we can quickly check if the corresponding page exists
v0.12.2,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.12.2,delete unprocessed file
v0.12.2,collect all wikiids
v0.12.2,create the dictionary
v0.12.2,request
v0.12.2,this dataset name
v0.12.2,names of raw text documents
v0.12.2,open output_file
v0.12.2,iterate through all documents
v0.12.2,split sentences and tokenize
v0.12.2,iterate through all annotations and add to corresponding tokens
v0.12.2,find sentence to which annotation belongs
v0.12.2,position within corresponding sentence
v0.12.2,set annotation for tokens of entity mention
v0.12.2,write to out file
v0.12.2,this dataset name
v0.12.2,download and parse data if necessary
v0.12.2,this dataset name
v0.12.2,download and parse data if necessary
v0.12.2,First parse the post titles
v0.12.2,Keep track of how many and which entity mentions does a given post title have
v0.12.2,Check if the current post title has an entity link and parse accordingly
v0.12.2,Post titles with entity mentions (if any) are handled via this function
v0.12.2,Then parse the comments
v0.12.2,"Iterate over the comments.tsv file, until the end is reached"
v0.12.2,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.12.2,Each comment thread is handled as one 'document'.
v0.12.2,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.12.2,This if-condition is needed to handle this problem.
v0.12.2,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.12.2,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.12.2,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.12.2,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.12.2,and not just single letters into single rows.
v0.12.2,If there are annotated entity mentions for given post title or a comment thread
v0.12.2,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.12.2,Write the token with a corresponding tag to file
v0.12.2,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.12.2,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.12.2,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.12.2,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.12.2,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.12.2,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.12.2,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.12.2,Check if further annotations belong to the current post title or comment thread as well
v0.12.2,Stop when the end of an annotation file is reached
v0.12.2,Check if further annotations belong to the current sentence as well
v0.12.2,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.12.2,Docstart
v0.12.2,if there is more than one word in the chunk we write each in a separate line
v0.12.2,print(chunks)
v0.12.2,empty line after each sentence
v0.12.2,convert the file to CoNLL
v0.12.2,this dataset name
v0.12.2,"check if data there, if not, download the data"
v0.12.2,create folder
v0.12.2,download data
v0.12.2,transform data into column format if necessary
v0.12.2,if no filenames are specified we use all the data
v0.12.2,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.12.2,also we remove 'raganato_ALL' from filenames in case its in the list
v0.12.2,generate the test file
v0.12.2,make column file and save to data_folder
v0.12.2,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.2,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.2,create folder
v0.12.2,download data
v0.12.2,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.2,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.2,create folder
v0.12.2,download data
v0.12.2,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.2,generate the test file
v0.12.2,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.2,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.2,create folder
v0.12.2,download data
v0.12.2,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.2,generate the test file
v0.12.2,default dataset folder is the cache root
v0.12.2,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.2,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.2,create folder
v0.12.2,download data
v0.12.2,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.2,generate the test file
v0.12.2,default dataset folder is the cache root
v0.12.2,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.2,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.2,create folder
v0.12.2,download data
v0.12.2,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.2,generate the test file
v0.12.2,default dataset folder is the cache root
v0.12.2,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.2,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.2,create folder
v0.12.2,download data
v0.12.2,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.2,generate the test file
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,if True:
v0.12.2,write CoNLL-U Plus header
v0.12.2,"Some special cases (e.g., missing spaces before entity marker)"
v0.12.2,necessary if text should be whitespace tokenizeable
v0.12.2,Handle case where tail may occur before the head
v0.12.2,this dataset name
v0.12.2,write CoNLL-U Plus header
v0.12.2,this dataset name
v0.12.2,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.12.2,download data if necessary
v0.12.2,write CoNLL-U Plus header
v0.12.2,The span has ended.
v0.12.2,We are entering a new span; reset indices
v0.12.2,and active tag to new span.
v0.12.2,We're inside a span.
v0.12.2,Last token might have been a part of a valid span.
v0.12.2,this dataset name
v0.12.2,write CoNLL-U Plus header
v0.12.2,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.12.2,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.12.2,target_file_path = Path(data_folder) / target_filename
v0.12.2,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.12.2,# write CoNLL-U Plus header
v0.12.2,"target_file.write(""# global.columns = id form ner\n"")"
v0.12.2,for example in json.load(source_file):
v0.12.2,token_list = self._tacred_example_to_token_list(example)
v0.12.2,target_file.write(token_list.serialize())
v0.12.2,check if first tag row is already occupied
v0.12.2,"if first tag row is occupied, use second tag row"
v0.12.2,hardcoded mapping TODO: perhaps find nicer solution
v0.12.2,remap regular tag names
v0.12.2,else skip to position in file where sentence begins
v0.12.2,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.12.2,read in dev file if exists
v0.12.2,read in test file if exists
v0.12.2,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.12.2,"find train, dev and test files if not specified"
v0.12.2,use test_file to create test split if available
v0.12.2,use dev_file to create test split if available
v0.12.2,"if data point contains black-listed label, do not use"
v0.12.2,first check if valid sentence
v0.12.2,"if so, add to indices"
v0.12.2,"find train, dev and test files if not specified"
v0.12.2,variables
v0.12.2,different handling of in_memory data than streaming data
v0.12.2,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.12.2,test if format is OK
v0.12.2,test if at least one label given
v0.12.2,make sentence from text (and filter for length)
v0.12.2,"if a pair column is defined, make a sentence pair object"
v0.12.2,noinspection PyDefaultArgument
v0.12.2,dataset name includes the split size
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download each of the 28 splits
v0.12.2,create dataset directory if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,handle labels file
v0.12.2,handle data file
v0.12.2,Create flair compatible labels
v0.12.2,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,create dataset directory if necessary
v0.12.2,create train.txt file from CSV
v0.12.2,create test.txt file from CSV
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,create dataset directory if necessary
v0.12.2,create train.txt file by iterating over pos and neg file
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,create dataset directory if necessary
v0.12.2,create train.txt file by iterating over pos and neg file
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,create dataset directory if necessary
v0.12.2,create train.txt file by iterating over pos and neg file
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,create dataset directory if necessary
v0.12.2,create train.txt file by iterating over pos and neg file
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,create train dev and test files in fasttext format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download senteval datasets if necessary und unzip
v0.12.2,convert to FastText format
v0.12.2,download data if necessary
v0.12.2,"if data is not downloaded yet, download it"
v0.12.2,get the zip file
v0.12.2,move original .tsv files to another folder
v0.12.2,create train and dev splits in fasttext format
v0.12.2,create eval_dataset file with no labels
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,download datasets if necessary
v0.12.2,create dataset directory if necessary
v0.12.2,create correctly formated txt files
v0.12.2,multiple labels are possible
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,Create flair compatible labels
v0.12.2,TREC-6 : NUM:dist -> __label__NUM
v0.12.2,TREC-50: NUM:dist -> __label__NUM:dist
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,Create flair compatible labels
v0.12.2,TREC-6 : NUM:dist -> __label__NUM
v0.12.2,TREC-50: NUM:dist -> __label__NUM:dist
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,create a separate directory for different tasks
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,check if dataset is supported
v0.12.2,set file names
v0.12.2,set file names
v0.12.2,download and unzip in file structure if necessary
v0.12.2,instantiate corpus
v0.12.2,"find train, dev and test files if not specified"
v0.12.2,"create DataPairDataset for train, test and dev file, if they are given"
v0.12.2,stop if file does not exist
v0.12.2,create a DataPair object from strings
v0.12.2,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.12.2,"if data is not downloaded yet, download it"
v0.12.2,get the zip file
v0.12.2,"rename test file to eval_dataset, since it has no labels"
v0.12.2,"if data is not downloaded yet, download it"
v0.12.2,get the zip file
v0.12.2,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.12.2,dev sets include 5 different annotations but we will only keep the gold label
v0.12.2,"rename test file to eval_dataset, since it has no labels"
v0.12.2,"if data is not downloaded yet, download it"
v0.12.2,get test and dev sets
v0.12.2,"if data is not downloaded yet, download it"
v0.12.2,get the zip file
v0.12.2,"rename test file to eval_dataset, since it has no labels"
v0.12.2,"if data is not downloaded yet, download it"
v0.12.2,get the zip file
v0.12.2,"rename test file to eval_dataset, since it has no labels"
v0.12.2,"if data is not downloaded yet, download it"
v0.12.2,get the zip file
v0.12.2,"rename test file to eval_dataset, since it has no labels"
v0.12.2,"if data not downloaded yet, download it"
v0.12.2,get the zip file
v0.12.2,"the downloaded files have json format, we transform them to tsv"
v0.12.2,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.12.2,remove json file
v0.12.2,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.12.2,with sum of all entity lengths as secondary key
v0.12.2,calculate offset without current text
v0.12.2,because we stick all passages of a document together
v0.12.2,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.12.2,Try to fix incorrect annotations
v0.12.2,print(
v0.12.2,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.12.2,)
v0.12.2,Ignore empty lines or relation annotations
v0.12.2,FIX annotation of whitespaces (necessary for PDR)
v0.12.2,One token may contain multiple entities -> deque all of them
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.12.2,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,Edge case: last token starts a new entity
v0.12.2,Last document in file
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,In the huner split files there is no information whether a given id originates
v0.12.2,from the train or test file of the original corpus - so we have to adapt corpus
v0.12.2,splitting here
v0.12.2,In the huner split files there is no information whether a given id originates
v0.12.2,from the train or test file of the original corpus - so we have to adapt corpus
v0.12.2,splitting here
v0.12.2,In the huner split files there is no information whether a given id originates
v0.12.2,from the train or test file of the original corpus - so we have to adapt corpus
v0.12.2,splitting here
v0.12.2,Edge case: last token starts a new entity
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,Read texts
v0.12.2,Read annotations
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,We need to apply a patch to correct the original training file
v0.12.2,Articles title
v0.12.2,Article abstract
v0.12.2,Entity annotations
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,Edge case: last token starts a new entity
v0.12.2,Map all entities to chemicals
v0.12.2,Map all entities to disease
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,Incomplete article
v0.12.2,Invalid XML syntax
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,if len(mid) != 3:
v0.12.2,continue
v0.12.2,Try to fix entity offsets
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,There is still one illegal annotation in the file ..
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,"Abstract first, title second to prevent issues with sentence splitting"
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,"Filter for specific entity types, by default no entities will be filtered"
v0.12.2,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.12.2,train and dev split of V2 will be train in V4
v0.12.2,test split of V2 will be dev in V4
v0.12.2,New documents in V4 will become test documents
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,column format
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,cache Feidegger config file
v0.12.2,cache Feidegger images
v0.12.2,replace image URL with local cached file
v0.12.2,append Sentence-Image data point
v0.12.2,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.12.2,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.12.2,"if so, num_workers is set to 0 for faster processing"
v0.12.2,cast to list if necessary
v0.12.2,cast to list if necessary
v0.12.2,"first, check if pymongo is installed"
v0.12.2,automatically identify train / test / dev files
v0.12.2,"if no test file is found, take any file with 'test' in name"
v0.12.2,Expose base classses
v0.12.2,Expose all biomedical data sets used for the evaluation of BioBERT
v0.12.2,-
v0.12.2,-
v0.12.2,-
v0.12.2,-
v0.12.2,Expose all biomedical data sets using the HUNER splits
v0.12.2,Expose all biomedical data sets
v0.12.2,Expose all document classification datasets
v0.12.2,word sense disambiguation
v0.12.2,Expose all entity linking datasets
v0.12.2,Expose all relation extraction datasets
v0.12.2,universal proposition banks
v0.12.2,keyphrase detection datasets
v0.12.2,other NER datasets
v0.12.2,standard NER datasets
v0.12.2,Expose all sequence labeling datasets
v0.12.2,Expose all text-image datasets
v0.12.2,Expose all text-text datasets
v0.12.2,Expose all treebanks
v0.12.2,"find train, dev and test files if not specified"
v0.12.2,get train data
v0.12.2,get test data
v0.12.2,get dev data
v0.12.2,option 1: read only sentence boundaries as offset positions
v0.12.2,option 2: keep everything in memory
v0.12.2,"if in memory, retrieve parsed sentence"
v0.12.2,else skip to position in file where sentence begins
v0.12.2,current token ID
v0.12.2,handling for the awful UD multiword format
v0.12.2,end of sentence
v0.12.2,comments
v0.12.2,ellipsis
v0.12.2,if token is a multi-word
v0.12.2,normal single-word tokens
v0.12.2,"if we don't split multiwords, skip over component words"
v0.12.2,add token
v0.12.2,add morphological tags
v0.12.2,derive whitespace logic for multiwords
v0.12.2,print(token)
v0.12.2,print(current_multiword_last_token)
v0.12.2,print(current_multiword_first_token)
v0.12.2,"if multi-word equals component tokens, there should be no whitespace"
v0.12.2,go through all tokens in subword and set whitespace_after information
v0.12.2,print(i)
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,download data if necessary
v0.12.2,this dataset name
v0.12.2,default dataset folder is the cache root
v0.12.2,download data if necessary
v0.12.2,"finally, print model card for information"
v0.12.2,test corpus
v0.12.2,create a TARS classifier
v0.12.2,check if right number of classes
v0.12.2,switch to task with only one label
v0.12.2,check if right number of classes
v0.12.2,switch to task with three labels provided as list
v0.12.2,check if right number of classes
v0.12.2,switch to task with four labels provided as set
v0.12.2,check if right number of classes
v0.12.2,switch to task with two labels provided as Dictionary
v0.12.2,check if right number of classes
v0.12.2,test corpus
v0.12.2,create a TARS classifier
v0.12.2,switch to a new task (TARS can do multiple tasks so you must define one)
v0.12.2,initialize the text classifier trainer
v0.12.2,start the training
v0.12.2,"With end symbol, without start symbol, padding in front"
v0.12.2,"Without end symbol, with start symbol, padding in back"
v0.12.2,"Without end symbol, without start symbol, padding in front"
v0.12.2,initialize trainer
v0.12.2,initialize trainer
v0.12.2,train model for 2 epochs
v0.12.2,load the checkpoint model and train until epoch 4
v0.12.2,clean up results directory
v0.12.2,initialize trainer
v0.12.2,initialize trainer
v0.12.2,increment for last token in sentence if not followed by whitespace
v0.12.2,clean up directory
v0.12.2,clean up directory
v0.12.2,example sentence
v0.12.2,set 4 labels for 2 tokens ('love' is tagged twice)
v0.12.2,check if there are three POS labels with correct text and values
v0.12.2,check if there are is one SENTIMENT label with correct text and values
v0.12.2,check if all tokens are correctly labeled
v0.12.2,remove the pos label from the last word
v0.12.2,there should be 2 POS labels left
v0.12.2,now remove all pos tags
v0.12.2,set 3 labels for 2 spans (HU is tagged twice)
v0.12.2,check if there are three labels with correct text and values
v0.12.2,check if there are two spans with correct text and values
v0.12.2,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.12.2,should be only one NER label left
v0.12.2,and only one NER span
v0.12.2,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.12.2,check if there are three labels with correct text and values
v0.12.2,check if there are two spans with correct text and values
v0.12.2,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.12.2,should be only one NER label left
v0.12.2,and only one NER span
v0.12.2,but there is also one orgtype span and label
v0.12.2,and only one NER span
v0.12.2,let's add the NER tag back
v0.12.2,check if there are three labels with correct text and values
v0.12.2,check if there are two spans with correct text and values
v0.12.2,now remove all NER tags
v0.12.2,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.12.2,create two relation label
v0.12.2,there should be two relation labels
v0.12.2,there should be one syntactic labels
v0.12.2,"there should be two relations, one with two and one with one label"
v0.12.2,example sentence
v0.12.2,add another topic label
v0.12.2,example sentence
v0.12.2,has sentiment value
v0.12.2,has 4 part of speech tags
v0.12.2,has 1 NER tag
v0.12.2,should be in total 6 labels
v0.12.2,example sentence
v0.12.2,add two NER labels
v0.12.2,get the four labels
v0.12.2,check that only two of the respective data points are equal
v0.12.2,make a sentence and some right context
v0.12.2,TODO: is this desirable? Or should two sentences with same text be considered same objects?
v0.12.2,Initializing a Sentence this way assumes that there is a space after each token
v0.12.2,get default dictionary
v0.12.2,init forward LM with 128 hidden states and 1 layer
v0.12.2,get the example corpus and process at character level in forward direction
v0.12.2,train the language model
v0.12.2,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.12.2,clean up results directory
v0.12.2,get default dictionary
v0.12.2,init forward LM with 128 hidden states and 1 layer
v0.12.2,get the example corpus and process at character level in forward direction
v0.12.2,train the language model
v0.12.2,define search space
v0.12.2,sequence tagger parameter
v0.12.2,model trainer parameter
v0.12.2,training parameter
v0.12.2,find best parameter settings
v0.12.2,clean up results directory
v0.12.2,document embeddings parameter
v0.12.2,training parameter
v0.12.2,clean up results directory
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,load column dataset with one entry
v0.12.2,load column dataset with two entries
v0.12.2,load column dataset with three entries
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,check if Token labels are correct
v0.12.2,"get training, test and dev data"
v0.12.2,check if Token labels for frames are correct
v0.12.2,"get training, test and dev data"
v0.12.2,"get training, test and dev data"
v0.12.2,get two corpora as one
v0.12.2,"get training, test and dev data for full English UD corpus from web"
v0.12.2,clean up data directory
v0.12.2,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.12.2,"""2"","
v0.12.2,"""0"","
v0.12.2,"""4"","
v0.12.2,"""2"","
v0.12.2,"""2"","
v0.12.2,"""2"","
v0.12.2,]
v0.12.2,"Here, we use the default token annotation fields."
v0.12.2,"We have manually checked, that these numbers are correct:"
v0.12.2,"+1 offset, because of missing EOS marker at EOD"
v0.12.2,Test data for v2.1 release
v0.12.2,--- Embeddings that are shared by both models --- #
v0.12.2,--- Task 1: Sentiment Analysis (5-class) --- #
v0.12.2,Define corpus and model
v0.12.2,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.12.2,Define corpus and model
v0.12.2,-- Define mapping (which tagger should train on which model) -- #
v0.12.2,-- Create model trainer and train -- #
v0.12.2,clean up file
v0.12.2,no need for label_dict
v0.12.2,check if right number of classes
v0.12.2,switch to task with only one label
v0.12.2,check if right number of classes
v0.12.2,switch to task with three labels provided as list
v0.12.2,check if right number of classes
v0.12.2,switch to task with four labels provided as set
v0.12.2,check if right number of classes
v0.12.2,switch to task with two labels provided as Dictionary
v0.12.2,check if right number of classes
v0.12.2,Intel ----founded_by---> Gordon Moore
v0.12.2,Intel ----founded_by---> Robert Noyce
v0.12.2,Check sentence masking and relation label annotation on
v0.12.2,"training, validation and test dataset (in this test the splits are the same)"
v0.12.2,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.12.2,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.12.2,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.12.2,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
v0.12.2,This sentence is only included if we transform the corpus with cross augmentation
v0.12.2,Ensure this is an example that predicts no classes in multilabel
v0.12.2,check if right number of classes
v0.12.2,switch to task with only one label
v0.12.2,check if right number of classes
v0.12.2,switch to task with three labels provided as list
v0.12.2,check if right number of classes
v0.12.2,switch to task with four labels provided as set
v0.12.2,check if right number of classes
v0.12.2,switch to task with two labels provided as Dictionary
v0.12.2,check if right number of classes
v0.12.2,ensure that the prepared tensors is what we expect
v0.12.2,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.12.2,previous and next sentence as context
v0.12.2,test expansion for sentence without context
v0.12.2,test expansion for with previous and next as context
v0.12.2,test expansion if first sentence is document boundary
v0.12.2,test expansion if we don't use context
v0.12.2,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.12.2,dummy model with embeddings
v0.12.2,save the dummy and load it again
v0.12.2,check that context_length and use_context_separator is the same for both
v0.12.1,mmap seems to be much more memory efficient
v0.12.1,Remove quotes from etag
v0.12.1,"If there is an etag, it's everything after the first period"
v0.12.1,"Otherwise, use None"
v0.12.1,"URL, so get it from the cache (downloading if necessary)"
v0.12.1,"File, and it exists."
v0.12.1,"File, but it doesn't exist."
v0.12.1,Something unknown
v0.12.1,Extract all the contents of zip file in current directory
v0.12.1,Extract all the contents of zip file in current directory
v0.12.1,TODO(joelgrus): do we want to do checksums or anything like that?
v0.12.1,get cache path to put the file
v0.12.1,make HEAD request to check ETag
v0.12.1,add ETag to filename if it exists
v0.12.1,"etag = response.headers.get(""ETag"")"
v0.12.1,"Download to temporary file, then copy to cache dir once finished."
v0.12.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.12.1,GET file object
v0.12.1,These defaults are the same as the argument defaults in tqdm.
v0.12.1,load_big_file is a workaround byhttps://github.com/highway11git
v0.12.1,to load models on some Mac/Windows setups
v0.12.1,see https://github.com/zalandoresearch/flair/issues/351
v0.12.1,first determine the distribution of classes in the dataset
v0.12.1,weight for each sample
v0.12.1,Create blocks
v0.12.1,shuffle the blocks
v0.12.1,concatenate the shuffled blocks
v0.12.1,Create blocks
v0.12.1,shuffle the blocks
v0.12.1,concatenate the shuffled blocks
v0.12.1,increment for last token in sentence if not followed by whitespace
v0.12.1,this is the default init size of a lmdb database for embeddings
v0.12.1,get db filename from embedding name
v0.12.1,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.12.1,SequenceTagger
v0.12.1,TextClassifier
v0.12.1,get db filename from embedding name
v0.12.1,if embedding database already exists
v0.12.1,"otherwise, push embedding to database"
v0.12.1,if embedding database already exists
v0.12.1,open the database in read mode
v0.12.1,we need to set self.k
v0.12.1,create and load the database in write mode
v0.12.1,"no idea why, but we need to close and reopen the environment to avoid"
v0.12.1,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.12.1,when opening new transaction !
v0.12.1,init dictionaries
v0.12.1,"in order to deal with unknown tokens, add <unk>"
v0.12.1,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.12.1,set 'add_unk' depending on whether <unk> is a key
v0.12.1,"if one embedding name, directly return it"
v0.12.1,"if multiple embedding names, concatenate them"
v0.12.1,TODO: does it make sense to exclude labels? Two data points of identical text (but different labels)
v0.12.1,would be equal now.
v0.12.1,First we remove any existing labels for this PartOfSentence in self.sentence
v0.12.1,labels also need to be deleted at Sentence object
v0.12.1,delete labels at object itself
v0.12.1,private field for all known spans
v0.12.1,the tokenizer used for this sentence
v0.12.1,some sentences represent a document boundary (but most do not)
v0.12.1,internal variables to denote position inside dataset
v0.12.1,"if text is passed, instantiate sentence with tokens (words)"
v0.12.1,determine token positions and whitespace_after flag
v0.12.1,the last token has no whitespace after
v0.12.1,log a warning if the dataset is empty
v0.12.1,data with zero-width characters cannot be handled
v0.12.1,set token idx and sentence
v0.12.1,append token to sentence
v0.12.1,register token annotations on sentence
v0.12.1,move sentence embeddings to device
v0.12.1,also move token embeddings to device
v0.12.1,clear token embeddings
v0.12.1,infer whitespace after field
v0.12.1,"if sentence has no tokens, return empty string"
v0.12.1,"otherwise, return concatenation of tokens with the correct offsets"
v0.12.1,The sentence's start position is not propagated to its tokens.
v0.12.1,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.12.1,No character at the corresponding code point: remove it
v0.12.1,"if no label if specified, return all labels"
v0.12.1,"if the label type exists in the Sentence, return it"
v0.12.1,return empty list if none of the above
v0.12.1,labels also need to be deleted at all tokens
v0.12.1,labels also need to be deleted at all known spans
v0.12.1,remove spans without labels
v0.12.1,delete labels at object itself
v0.12.1,set name
v0.12.1,abort if no data is provided
v0.12.1,sample test data from train if none is provided
v0.12.1,sample dev data from train if none is provided
v0.12.1,set train dev and test data
v0.12.1,find out empty sentence indices
v0.12.1,create subset of non-empty sentence indices
v0.12.1,find out empty sentence indices
v0.12.1,create subset of non-empty sentence indices
v0.12.1,count all label types per sentence
v0.12.1,go through all labels of label_type and count values
v0.12.1,check if there are any span labels
v0.12.1,"if an unk threshold is set, UNK all label values below this threshold"
v0.12.1,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.12.1,replace the old label with the new one
v0.12.1,keep track of the old (clean) label using another label type category
v0.12.1,keep track of how many labels in total are flipped
v0.12.1,Make the tag dictionary
v0.12.1,"add a dummy ""O"" to close final prediction"
v0.12.1,return complex list
v0.12.1,internal variables
v0.12.1,non-set tags are OUT tags
v0.12.1,anything that is not OUT is IN
v0.12.1,does this prediction start a new span?
v0.12.1,begin and single tags start new spans
v0.12.1,"in IOB format, an I tag starts a span if it follows an O or is a different span"
v0.12.1,single tags that change prediction start new spans
v0.12.1,if an existing span is ended (either by reaching O or starting a new span)
v0.12.1,determine score and value
v0.12.1,append to result list
v0.12.1,reset for-loop variables for new span
v0.12.1,remember previous tag
v0.12.1,global variable: cache_root
v0.12.1,global variable: device
v0.12.1,global variable: version
v0.12.1,global variable: arrow symbol
v0.12.1,dummy return to fulfill trainer.train() needs
v0.12.1,print(vec)
v0.12.1,Attach optimizer
v0.12.1,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.12.1,if memory mode option 'none' delete everything
v0.12.1,"if dynamic embedding keys not passed, identify them automatically"
v0.12.1,always delete dynamic embeddings
v0.12.1,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.12.1,optional metric space decoder if prototypes have different length than embedding
v0.12.1,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.12.1,"if set, create initial prototypes from normal distribution"
v0.12.1,"if set, use a radius"
v0.12.1,all parameters will be pushed internally to the specified device
v0.12.1,decode embeddings into prototype space
v0.12.1,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.12.1,Always include the name of the Model class for which the state dict holds
v0.12.1,"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
v0.12.1,"write out a ""model card"" if one is set"
v0.12.1,special handling for optimizer:
v0.12.1,remember optimizer class and state dictionary
v0.12.1,save model
v0.12.1,restore optimizer and scheduler to model card if set
v0.12.1,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.12.1,get all non-abstract subclasses
v0.12.1,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.12.1,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.12.1,"if the model cannot be fetched, load as a file"
v0.12.1,try to get model class from state
v0.12.1,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.12.1,"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
v0.12.1,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.12.1,"if this class is not abstract, fetch the model and load it"
v0.12.1,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.12.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.12.1,loss calculation
v0.12.1,variables for printing
v0.12.1,variables for computing scores
v0.12.1,remove any previously predicted labels
v0.12.1,predict for batch
v0.12.1,get the gold labels
v0.12.1,add to all_predicted_values
v0.12.1,make printout lines
v0.12.1,convert true and predicted values to two span-aligned lists
v0.12.1,delete exluded labels if exclude_labels is given
v0.12.1,"if after excluding labels, no label is left, ignore the datapoint"
v0.12.1,write all_predicted_values to out_file if set
v0.12.1,make the evaluation dictionary
v0.12.1,check if this is a multi-label problem
v0.12.1,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.12.1,multi-label problems require a multi-hot vector for each true and predicted label
v0.12.1,single-label problems can do with a single index for each true and predicted label
v0.12.1,"now, calculate evaluation numbers"
v0.12.1,there is at least one gold label or one prediction (default)
v0.12.1,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.12.1,"micro average is only computed if zero-label exists (for instance ""O"")"
v0.12.1,if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
v0.12.1,same for the main score
v0.12.1,issue error and default all evaluation numbers to 0.
v0.12.1,line for log file
v0.12.1,check if there is a label mismatch
v0.12.1,print info
v0.12.1,set the embeddings
v0.12.1,initialize the label dictionary
v0.12.1,initialize the decoder
v0.12.1,set up multi-label logic
v0.12.1,init dropouts
v0.12.1,loss weights and loss function
v0.12.1,Initialize the weight tensor
v0.12.1,set up gradient reversal if so specified
v0.12.1,embed sentences
v0.12.1,get a tensor of data points
v0.12.1,do dropout
v0.12.1,make a forward pass to produce embedded data points and labels
v0.12.1,get the data points for which to predict labels
v0.12.1,get their gold labels as a tensor
v0.12.1,pass data points through network to get encoded data point tensor
v0.12.1,decode
v0.12.1,an optional masking step (no masking in most cases)
v0.12.1,calculate the loss
v0.12.1,filter empty sentences
v0.12.1,reverse sort all sequences by their length
v0.12.1,progress bar for verbosity
v0.12.1,filter data points in batch
v0.12.1,stop if all sentences are empty
v0.12.1,pass data points through network and decode
v0.12.1,if anything could possibly be predicted
v0.12.1,remove previously predicted labels of this type
v0.12.1,add DefaultClassifier arguments
v0.12.1,add variables of DefaultClassifier
v0.12.1,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.12.1,Get projected 1st dimension
v0.12.1,Compute bilinear form
v0.12.1,Arcosh
v0.12.1,Project the input data to n+1 dimensions
v0.12.1,"The first dimension, is recomputed in the distance module"
v0.12.1,header for 'weights.txt'
v0.12.1,"determine the column index of loss, f-score and accuracy for"
v0.12.1,"train, dev and test split"
v0.12.1,then get all relevant values from the tsv
v0.12.1,then get all relevant values from the tsv
v0.12.1,plot i
v0.12.1,save plots
v0.12.1,save plots
v0.12.1,plt.show()
v0.12.1,save plot
v0.12.1,take the average over the last three scores of training
v0.12.1,take average over the scores from the different training runs
v0.12.1,auto-spawn on GPU if available
v0.12.1,progress bar for verbosity
v0.12.1,stop if all sentences are empty
v0.12.1,clearing token embeddings to save memory
v0.12.1,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.12.1,TODO: not saving lines yet
v0.12.1,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.12.1,- MaskedRelationClassifier ?
v0.12.1,This depends if this relation classification architecture should replace or offer as an alternative.
v0.12.1,Set label type and prepare label dictionary
v0.12.1,Initialize super default classifier
v0.12.1,Add the special tokens from the encoding strategy
v0.12.1,"Auto-spawn on GPU, if available"
v0.12.1,Only use entities labelled with the specified labels for each label type
v0.12.1,Only use entities above the specified threshold
v0.12.1,Use a dictionary to find gold relation annotations for a given entity pair
v0.12.1,Yield head and tail entity pairs from the cross product of all entities
v0.12.1,Remove identity relation entity pairs
v0.12.1,Remove entity pairs with labels that do not match any
v0.12.1,of the specified relations in `self.entity_pair_labels`
v0.12.1,"Obtain gold label, if existing"
v0.12.1,Some sanity checks
v0.12.1,Pre-compute non-leading head and tail tokens for entity masking
v0.12.1,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.12.1,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.12.1,"Therefore, we use the span's position in the sentence."
v0.12.1,Create masked sentence
v0.12.1,Add gold relation annotation as sentence label
v0.12.1,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.12.1,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.12.1,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.12.1,"may get distributed into different splits. For training purposes, this is always undesired."
v0.12.1,Ensure that all sentences are encoded properly
v0.12.1,Deal with the case where all sentences are encoded sentences
v0.12.1,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.12.1,Deal with the case where all sentences are standard (non-encoded) sentences
v0.12.1,"For each encoded sentence, transfer its prediction onto the original relation"
v0.12.1,auto-spawn on GPU if available
v0.12.1,pad strings with whitespaces to longest sentence
v0.12.1,cut up the input into chunks of max charlength = chunk_size
v0.12.1,push each chunk through the RNN language model
v0.12.1,concatenate all chunks to make final output
v0.12.1,initial hidden state
v0.12.1,get predicted weights
v0.12.1,divide by temperature
v0.12.1,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.12.1,this makes a vector in which the largest value is 0
v0.12.1,compute word weights with exponential function
v0.12.1,try sampling multinomial distribution for next character
v0.12.1,print(word_idx)
v0.12.1,input ids
v0.12.1,push list of character IDs through model
v0.12.1,the target is always the next character
v0.12.1,use cross entropy loss to compare output of forward pass with targets
v0.12.1,exponentiate cross-entropy loss to calculate perplexity
v0.12.1,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.12.1,serialize the language models and the constructor arguments (but nothing else)
v0.12.1,special handling for deserializing language models
v0.12.1,re-initialize language model with constructor arguments
v0.12.1,copy over state dictionary to self
v0.12.1,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.12.1,"in their ""self.train()"" method)"
v0.12.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.12.1,"check if this is the case and if so, set it"
v0.12.1,Transform input data into TARS format
v0.12.1,"if there are no labels, return a random sample as negatives"
v0.12.1,"otherwise, go through all labels"
v0.12.1,make sure the probabilities always sum up to 1
v0.12.1,get and embed all labels by making a Sentence object that contains only the label text
v0.12.1,get each label embedding and scale between 0 and 1
v0.12.1,compute similarity matrix
v0.12.1,"the higher the similarity, the greater the chance that a label is"
v0.12.1,sampled as negative example
v0.12.1,make label dictionary if no Dictionary object is passed
v0.12.1,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.12.1,check if candidate_label_set is empty
v0.12.1,make list if only one candidate label is passed
v0.12.1,create label dictionary
v0.12.1,note current task
v0.12.1,create a temporary task
v0.12.1,make zero shot predictions
v0.12.1,switch to the pre-existing task
v0.12.1,prepare TARS dictionary
v0.12.1,initialize a bare-bones sequence tagger
v0.12.1,transformer separator
v0.12.1,Store task specific labels since TARS can handle multiple tasks
v0.12.1,make a tars sentence where all labels are O by default
v0.12.1,init new TARS classifier
v0.12.1,set all task information
v0.12.1,return
v0.12.1,with torch.no_grad():
v0.12.1,progress bar for verbosity
v0.12.1,stop if all sentences are empty
v0.12.1,go through each sentence in the batch
v0.12.1,always remove tags first
v0.12.1,get the span and its label
v0.12.1,determine whether tokens in this span already have a label
v0.12.1,only add if all tokens have no label
v0.12.1,make and add a corresponding predicted span
v0.12.1,set indices so that no token can be tagged twice
v0.12.1,clearing token embeddings to save memory
v0.12.1,"all labels default to ""O"""
v0.12.1,set gold token-level
v0.12.1,set predicted token-level
v0.12.1,now print labels in CoNLL format
v0.12.1,prepare TARS dictionary
v0.12.1,initialize a bare-bones sequence tagger
v0.12.1,transformer separator
v0.12.1,Store task specific labels since TARS can handle multiple tasks
v0.12.1,get the serialized embeddings
v0.12.1,remap state dict for models serialized with Flair <= 0.11.3
v0.12.1,init new TARS classifier
v0.12.1,set all task information
v0.12.1,with torch.no_grad():
v0.12.1,progress bar for verbosity
v0.12.1,stop if all sentences are empty
v0.12.1,go through each sentence in the batch
v0.12.1,always remove tags first
v0.12.1,add all labels that according to TARS match the text and are above threshold
v0.12.1,do not add labels below confidence threshold
v0.12.1,only use label with highest confidence if enforcing single-label predictions
v0.12.1,get all label scores and do an argmax to get the best label
v0.12.1,remove previously added labels and only add the best label
v0.12.1,clearing token embeddings to save memory
v0.12.1,set separator to concatenate two sentences
v0.12.1,auto-spawn on GPU if available
v0.12.1,pooling operation to get embeddings for entites
v0.12.1,set embeddings
v0.12.1,set relation and entity label types
v0.12.1,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.12.1,filter entity pairs according to their tags if set
v0.12.1,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.12.1,character dictionary for decoding and encoding
v0.12.1,make sure <unk> is in dictionary for handling of unknown characters
v0.12.1,add special symbols to dictionary if necessary and save respective indices
v0.12.1,---- ENCODER ----
v0.12.1,encoder character embeddings
v0.12.1,encoder pre-trained embeddings
v0.12.1,encoder RNN
v0.12.1,additional encoder linear layer if bidirectional encoding
v0.12.1,---- DECODER ----
v0.12.1,decoder: linear layers to transform vectors to and from alphabet_size
v0.12.1,when using attention we concatenate attention outcome and decoder hidden states
v0.12.1,decoder RNN
v0.12.1,loss and softmax
v0.12.1,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.12.1,add additional columns for special symbols if necessary
v0.12.1,initialize with dummy symbols
v0.12.1,encode inputs
v0.12.1,get labels (we assume each token has a lemma label)
v0.12.1,get char indices for labels of sentence
v0.12.1,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.12.1,max_sequence_length = length of longest label of sentence + 1
v0.12.1,get char embeddings
v0.12.1,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.12.1,take decoder input and initial hidden and pass through RNN
v0.12.1,"if all encoder outputs are provided, use attention"
v0.12.1,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.12.1,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.12.1,get all tokens
v0.12.1,encode input characters by sending them through RNN
v0.12.1,get one-hots for characters and add special symbols / padding
v0.12.1,determine length of each token
v0.12.1,embed sentences
v0.12.1,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.12.1,variable to store initial hidden states for decoder
v0.12.1,encode input characters by sending them through RNN
v0.12.1,test packing and padding
v0.12.1,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.12.1,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.12.1,decoder later with self.emb_to_hidden
v0.12.1,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.12.1,use token embedding as initial hidden state for decoder
v0.12.1,concatenate everything together and project to appropriate size for decoder
v0.12.1,variable to store initial hidden states for decoder
v0.12.1,encode input characters by sending them through RNN
v0.12.1,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.12.1,embed character one-hots
v0.12.1,send through encoder RNN (produces initial hidden for decoder)
v0.12.1,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.12.1,project 2*hidden_size to hidden_size
v0.12.1,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.12.1,later with self.emb_to_hidden
v0.12.1,use token embedding as initial hidden state for decoder
v0.12.1,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.12.1,concatenate everything together and project to appropriate size for decoder
v0.12.1,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.12.1,"create target vector (batch_size, max_label_seq_length + 1)"
v0.12.1,filter empty sentences
v0.12.1,max length of the predicted sequences
v0.12.1,for printing
v0.12.1,stop if all sentences are empty
v0.12.1,remove previously predicted labels of this type
v0.12.1,create list of tokens in batch
v0.12.1,encode inputs
v0.12.1,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.12.1,sequence length is always set to one in prediction
v0.12.1,option 1: greedy decoding
v0.12.1,predictions
v0.12.1,decode next character
v0.12.1,pick top beam size many outputs with highest probabilities
v0.12.1,option 2: beam search
v0.12.1,out_probs = self.softmax(output_vectors).squeeze(1)
v0.12.1,make sure no dummy symbol <> or start symbol <S> is predicted
v0.12.1,pick top beam size many outputs with highest probabilities
v0.12.1,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.12.1,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.12.1,keep scores of beam_size many hypothesis for each token in the batch
v0.12.1,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.12.1,save sequences so far
v0.12.1,keep track of how many hypothesis were completed for each token
v0.12.1,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.12.1,decode with log softmax
v0.12.1,make sure no dummy symbol <> or start symbol <S> is predicted
v0.12.1,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.12.1,"if the sequence is already ended, do not record as candidate"
v0.12.1,index of token in in list tokens_in_batch
v0.12.1,print(token_number)
v0.12.1,hypothesis score
v0.12.1,TODO: remove token if number of completed hypothesis exceeds given value
v0.12.1,set score of corresponding entry to -inf so it will not be expanded
v0.12.1,get leading_indices for next expansion
v0.12.1,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.12.1,take beam_size many copies of scores vector and add scores of possible new extensions
v0.12.1,"size (beam_size*batch_size, beam_size)"
v0.12.1,print(hypothesis_scores)
v0.12.1,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.12.1,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.12.1,print(hypothesis_scores_per_token)
v0.12.1,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.12.1,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.12.1,a list of length beam_size*batch_size
v0.12.1,"where the first three inidices belong to the first token, the next three to the second token,"
v0.12.1,and so on
v0.12.1,with these indices we can compute the tensors for the next iteration
v0.12.1,expand sequences with corresponding index
v0.12.1,add log-probabilities to the scores
v0.12.1,save new leading indices
v0.12.1,save corresponding hidden states
v0.12.1,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.12.1,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.12.1,get best final hypothesis for each token
v0.12.1,get characters from index sequences and add predicted label to token
v0.12.1,dictionaries
v0.12.1,all parameters will be pushed internally to the specified device
v0.12.1,now print labels in CoNLL format
v0.12.1,internal candidate lists of generator
v0.12.1,load Zelda candidates if so passed
v0.12.1,create candidate lists
v0.12.1,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.12.1,create a new dictionary for lower cased mentions
v0.12.1,go through each mention and its candidates
v0.12.1,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.12.1,set lowercased version as map
v0.12.1,remap state dict for models serialized with Flair <= 0.11.3
v0.12.1,get the candidates
v0.12.1,"during training, add the gold value as candidate"
v0.12.1,----- Create the internal tag dictionary -----
v0.12.1,span-labels need special encoding (BIO or BIOES)
v0.12.1,the big question is whether the label dictionary should contain an UNK or not
v0.12.1,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.12.1,"with UNK, the model learns less well if there are no UNK examples"
v0.12.1,is this a span prediction problem?
v0.12.1,----- Embeddings -----
v0.12.1,----- Initial loss weights parameters -----
v0.12.1,----- RNN specific parameters -----
v0.12.1,----- Conditional Random Field parameters -----
v0.12.1,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.12.1,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.12.1,----- Dropout parameters -----
v0.12.1,dropouts
v0.12.1,remove word dropout if there is no contact over the sequence dimension.
v0.12.1,----- Model layers -----
v0.12.1,----- RNN layer -----
v0.12.1,"If shared RNN provided, else create one for model"
v0.12.1,Whether to train initial hidden state
v0.12.1,final linear map to tag space
v0.12.1,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.12.1,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.12.1,"if there are no sentences, there is no loss"
v0.12.1,forward pass to get scores
v0.12.1,calculate loss given scores and labels
v0.12.1,make a zero-padded tensor for the whole sentence
v0.12.1,linear map to tag space
v0.12.1,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.12.1,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.12.1,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.12.1,spans need to be encoded as token-level predictions
v0.12.1,all others are regular labels for each token
v0.12.1,make sure it's a list
v0.12.1,filter empty sentences
v0.12.1,reverse sort all sequences by their length
v0.12.1,progress bar for verbosity
v0.12.1,stop if all sentences are empty
v0.12.1,get features from forward propagation
v0.12.1,remove previously predicted labels of this type
v0.12.1,"if return_loss, get loss value"
v0.12.1,make predictions
v0.12.1,add predictions to Sentence
v0.12.1,BIOES-labels need to be converted to spans
v0.12.1,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.12.1,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.12.1,core Flair models on Huggingface ModelHub
v0.12.1,"Large NER models,"
v0.12.1,Multilingual NER models
v0.12.1,English POS models
v0.12.1,Multilingual POS models
v0.12.1,English SRL models
v0.12.1,English chunking models
v0.12.1,Language-specific NER models
v0.12.1,Language-specific POS models
v0.12.1,English NER models
v0.12.1,Multilingual NER models
v0.12.1,English POS models
v0.12.1,Multilingual POS models
v0.12.1,English SRL models
v0.12.1,English chunking models
v0.12.1,Danish models
v0.12.1,German models
v0.12.1,French models
v0.12.1,Dutch models
v0.12.1,Malayalam models
v0.12.1,Portuguese models
v0.12.1,Keyphase models
v0.12.1,Biomedical models
v0.12.1,check if model name is a valid local file
v0.12.1,"check if model key is remapped to HF key - if so, print out information"
v0.12.1,get mapped name
v0.12.1,use mapped name instead
v0.12.1,"if not, check if model key is remapped to direct download location. If so, download model"
v0.12.1,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.12.1,"for all other cases (not local file or special download location), use HF model hub"
v0.12.1,"if not a local file, get from model hub"
v0.12.1,use model name as subfolder
v0.12.1,Lazy import
v0.12.1,output information
v0.12.1,## Demo: How to use in Flair
v0.12.1,load tagger
v0.12.1,make example sentence
v0.12.1,predict NER tags
v0.12.1,print sentence
v0.12.1,print predicted NER spans
v0.12.1,iterate over entities and print
v0.12.1,Lazy import
v0.12.1,Save model weight
v0.12.1,Determine if model card already exists
v0.12.1,Generate and save model card
v0.12.1,Upload files
v0.12.1,"all labels default to ""O"""
v0.12.1,set gold token-level
v0.12.1,set predicted token-level
v0.12.1,now print labels in CoNLL format
v0.12.1,print labels in CoNLL format
v0.12.1,the multi task model has several labels
v0.12.1,biomedical models
v0.12.1,entity linker
v0.12.1,auto-spawn on GPU if available
v0.12.1,remap state dict for models serialized with Flair <= 0.11.3
v0.12.1,English sentiment models
v0.12.1,Communicative Functions Model
v0.12.1,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.12.1,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.12.1,"Initially, get scores from <start> tag to all other tags"
v0.12.1,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.12.1,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.12.1,Create a tensor to hold accumulated sequence scores at each current tag
v0.12.1,Create a tensor to hold back-pointers
v0.12.1,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.12.1,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.12.1,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.12.1,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.12.1,"If sentence is over, add transition to STOP-tag"
v0.12.1,Decode/trace best path backwards
v0.12.1,Sanity check
v0.12.1,remove start-tag and backscore to stop-tag
v0.12.1,Max + Softmax to get confidence score for predicted label and append label to each token
v0.12.1,"Transitions are used in the following way: transitions[to, from]."
v0.12.1,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.12.1,to START-tag and from STOP-tag to any other tag to -10000.
v0.12.1,create a model card for this model with Flair and PyTorch version
v0.12.1,also record Transformers version if library is loaded
v0.12.1,remember all parameters used in train() call
v0.12.1,add model card to model
v0.12.1,"if optimizer class is passed, instantiate:"
v0.12.1,"determine what splits (train, dev, test) to evaluate and log"
v0.12.1,prepare loss logging file and set up header
v0.12.1,"from here on, use list of learning rates"
v0.12.1,load existing optimizer state dictionary if it exists
v0.12.1,"minimize training loss if training with dev data, else maximize dev score"
v0.12.1,"if scheduler is passed as a class, instantiate"
v0.12.1,"if we load a checkpoint, we have already trained for epoch"
v0.12.1,"Determine whether to log ""bad epochs"" information"
v0.12.1,load existing scheduler state dictionary if it exists
v0.12.1,update optimizer and scheduler in model card
v0.12.1,"if training also uses dev/train data, include in training set"
v0.12.1,initialize sampler if provided
v0.12.1,init with default values if only class is provided
v0.12.1,set dataset to sample from
v0.12.1,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.12.1,At any point you can hit Ctrl + C to break out of training early.
v0.12.1,update epoch in model card
v0.12.1,get new learning rate
v0.12.1,reload last best model if annealing with restarts is enabled
v0.12.1,stop training if learning rate becomes too small
v0.12.1,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.12.1,process mini-batches
v0.12.1,zero the gradients on the model and optimizer
v0.12.1,"if necessary, make batch_steps"
v0.12.1,forward and backward for batch
v0.12.1,forward pass
v0.12.1,Backward
v0.12.1,identify dynamic embeddings (always deleted) on first sentence
v0.12.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.1,do the optimizer step
v0.12.1,do the scheduler step if one-cycle or linear decay
v0.12.1,get new learning rate
v0.12.1,evaluate on train / dev / test split depending on training settings
v0.12.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.1,calculate scores using dev data if available
v0.12.1,append dev score to score history
v0.12.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12.1,determine if this is the best model or if we need to anneal
v0.12.1,default mode: anneal against dev score
v0.12.1,alternative: anneal against dev loss
v0.12.1,alternative: anneal against train loss
v0.12.1,determine bad epoch number
v0.12.1,lr unchanged
v0.12.1,log bad epochs
v0.12.1,output log file
v0.12.1,make headers on first epoch
v0.12.1,"if checkpoint is enabled, save model at each epoch"
v0.12.1,Check whether to save best model
v0.12.1,"if we do not use dev data for model selection, save final model"
v0.12.1,test best model if test data is present
v0.12.1,recover all arguments that were used to train this model
v0.12.1,you can overwrite params with your own
v0.12.1,surface nested arguments
v0.12.1,resume training with these parameters
v0.12.1,"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name"
v0.12.1,get and return the final test score of best model
v0.12.1,cast string to Path
v0.12.1,forward pass
v0.12.1,update optimizer and scheduler
v0.12.1,"TextDataset returns a list. valid and test are only one file,"
v0.12.1,so return the first element
v0.12.1,cast string to Path
v0.12.1,error message if the validation dataset is too small
v0.12.1,Shuffle training files randomly after serially iterating
v0.12.1,through corpus one
v0.12.1,"iterate through training data, starting at"
v0.12.1,self.split (for checkpointing)
v0.12.1,off by one for printing
v0.12.1,go into train mode
v0.12.1,reset variables
v0.12.1,not really sure what this does
v0.12.1,do the forward pass in the model
v0.12.1,try to predict the targets
v0.12.1,Backward
v0.12.1,`clip_grad_norm` helps prevent the exploding gradient
v0.12.1,problem in RNNs / LSTMs.
v0.12.1,We detach the hidden state from how it was
v0.12.1,previously produced.
v0.12.1,"If we didn't, the model would try backpropagating"
v0.12.1,all the way to start of the dataset.
v0.12.1,explicitly remove loss to clear up memory
v0.12.1,#########################################################
v0.12.1,Save the model if the validation loss is the best we've
v0.12.1,seen so far.
v0.12.1,#########################################################
v0.12.1,print info
v0.12.1,#########################################################
v0.12.1,##############################################################################
v0.12.1,final testing
v0.12.1,##############################################################################
v0.12.1,Turn on evaluation mode which disables dropout.
v0.12.1,Work out how cleanly we can divide the dataset into bsz parts.
v0.12.1,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.12.1,Evenly divide the data across the bsz batches.
v0.12.1,"the default model for ELMo is the 'original' model, which is very large"
v0.12.1,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.12.1,put on Cuda if available
v0.12.1,embed a dummy sentence to determine embedding_length
v0.12.1,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.12.1,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.12.1,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.12.1,news-english-forward
v0.12.1,news-english-backward
v0.12.1,news-english-forward
v0.12.1,news-english-backward
v0.12.1,mix-english-forward
v0.12.1,mix-english-backward
v0.12.1,mix-german-forward
v0.12.1,mix-german-backward
v0.12.1,common crawl Polish forward
v0.12.1,common crawl Polish backward
v0.12.1,Slovenian forward
v0.12.1,Slovenian backward
v0.12.1,Bulgarian forward
v0.12.1,Bulgarian backward
v0.12.1,Dutch forward
v0.12.1,Dutch backward
v0.12.1,Swedish forward
v0.12.1,Swedish backward
v0.12.1,French forward
v0.12.1,French backward
v0.12.1,Czech forward
v0.12.1,Czech backward
v0.12.1,Portuguese forward
v0.12.1,Portuguese backward
v0.12.1,initialize cache if use_cache set
v0.12.1,embed a dummy sentence to determine embedding_length
v0.12.1,set to eval mode
v0.12.1,Copy the object's state from self.__dict__ which contains
v0.12.1,all our instance attributes. Always use the dict.copy()
v0.12.1,method to avoid modifying the original state.
v0.12.1,Remove the unpicklable entries.
v0.12.1,"if cache is used, try setting embeddings from cache first"
v0.12.1,try populating embeddings from cache
v0.12.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.12.1,get hidden states from language model
v0.12.1,take first or last hidden states from language model as word representation
v0.12.1,if self.tokenized_lm or token.whitespace_after:
v0.12.1,1-camembert-base -> camembert-base
v0.12.1,1-xlm-roberta-large -> xlm-roberta-large
v0.12.1,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.12.1,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.12.1,tokens are attended to.
v0.12.1,Zero-pad up to the sequence length.
v0.12.1,"first, find longest sentence in batch"
v0.12.1,prepare id maps for BERT model
v0.12.1,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.12.1,get aggregated embeddings for each BERT-subtoken in sentence
v0.12.1,get the current sentence object
v0.12.1,add concatenated embedding to sentence
v0.12.1,use first subword embedding if pooling operation is 'first'
v0.12.1,"otherwise, do a mean over all subwords in token"
v0.12.1,"if only one sentence is passed, convert to list of sentence"
v0.12.1,bidirectional LSTM on top of embedding layer
v0.12.1,dropouts
v0.12.1,"first, sort sentences by number of tokens"
v0.12.1,go through each sentence in batch
v0.12.1,PADDING: pad shorter sentences out
v0.12.1,ADD TO SENTENCE LIST: add the representation
v0.12.1,--------------------------------------------------------------------
v0.12.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.12.1,--------------------------------------------------------------------
v0.12.1,--------------------------------------------------------------------
v0.12.1,FF PART
v0.12.1,--------------------------------------------------------------------
v0.12.1,use word dropout if set
v0.12.1,--------------------------------------------------------------------
v0.12.1,EXTRACT EMBEDDINGS FROM LSTM
v0.12.1,--------------------------------------------------------------------
v0.12.1,embed a dummy sentence to determine embedding_length
v0.12.1,Avoid conflicts with flair's Token class
v0.12.1,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.12.1,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.12.1,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.12.1,add positional encodings
v0.12.1,reshape the pixels into the sequence
v0.12.1,layer norm after convolution and positional encodings
v0.12.1,add <cls> token
v0.12.1,"transformer requires input in the shape [h*w+1, b, d]"
v0.12.1,the output is an embedding of <cls> token
v0.12.1,this parameter is fixed
v0.12.1,optional fine-tuning on top of embedding layer
v0.12.1,"if only one sentence is passed, convert to list of sentence"
v0.12.1,"if only one sentence is passed, convert to list of sentence"
v0.12.1,bidirectional RNN on top of embedding layer
v0.12.1,dropouts
v0.12.1,TODO: remove in future versions
v0.12.1,embed words in the sentence
v0.12.1,before-RNN dropout
v0.12.1,reproject if set
v0.12.1,push through RNN
v0.12.1,after-RNN dropout
v0.12.1,extract embeddings from RNN
v0.12.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.12.1,"check if this is the case and if so, set it"
v0.12.1,serialize the language models and the constructor arguments (but nothing else)
v0.12.1,re-initialize language model with constructor arguments
v0.12.1,special handling for deserializing language models
v0.12.1,copy over state dictionary to self
v0.12.1,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.12.1,"in their ""self.train()"" method)"
v0.12.1,IMPORTANT: add embeddings as torch modules
v0.12.1,iterate over sentences
v0.12.1,"if its a forward LM, take last state"
v0.12.1,"convert to plain strings, embedded in a list for the encode function"
v0.12.1,CNN
v0.12.1,dropouts
v0.12.1,TODO: remove in future versions
v0.12.1,embed words in the sentence
v0.12.1,before-RNN dropout
v0.12.1,reproject if set
v0.12.1,push CNN
v0.12.1,after-CNN dropout
v0.12.1,extract embeddings from CNN
v0.12.1,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.12.1,"if only one sentence is passed, convert to list of sentence"
v0.12.1,Expose base classses
v0.12.1,Expose document embedding classes
v0.12.1,Expose image embedding classes
v0.12.1,Expose legacy embedding classes
v0.12.1,Expose token embedding classes
v0.12.1,in some cases we need to insert zero vectors for tokens without embedding.
v0.12.1,padding
v0.12.1,remove special markup
v0.12.1,check if special tokens exist to circumvent error message
v0.12.1,iterate over subtokens and reconstruct tokens
v0.12.1,remove special markup
v0.12.1,check if reconstructed token is special begin token ([CLS] or similar)
v0.12.1,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.12.1,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.12.1,if tokens are unaccounted for
v0.12.1,check if all tokens were matched to subtokens
v0.12.1,The layoutlm tokenizer doesn't handle ocr themselves
v0.12.1,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.12.1,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.12.1,in case of doubt: token embedding has higher priority than document embedding
v0.12.1,random check some tokens to save performance.
v0.12.1,Models such as FNet do not have an attention_mask
v0.12.1,set language IDs for XLM-style transformers
v0.12.1,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.12.1,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.12.1,set context if not set already
v0.12.1,flair specific pre-tokenization
v0.12.1,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.12.1,temporary fix to disable tokenizer parallelism warning
v0.12.1,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.12.1,do not print transformer warnings as these are confusing in this case
v0.12.1,load tokenizer and transformer model
v0.12.1,load tokenizer from inmemory zip-file
v0.12.1,model name
v0.12.1,embedding parameters
v0.12.1,send mini-token through to check how many layers the model has
v0.12.1,return length
v0.12.1,"If we use a context separator, add a new special token"
v0.12.1,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.12.1,"when initializing, embeddings are in eval mode by default"
v0.12.1,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.12.1,"cannot run `.encode` if ocr boxes are required, assume"
v0.12.1,in case of doubt: token embedding has higher priority than document embedding
v0.12.1,in case of doubt: token embedding has higher priority than document embedding
v0.12.1,legacy TransformerDocumentEmbedding
v0.12.1,legacy TransformerTokenEmbedding
v0.12.1,legacy Flair <= 0.12
v0.12.1,legacy Flair <= 0.7
v0.12.1,legacy TransformerTokenEmbedding
v0.12.1,Legacy TransformerDocumentEmbedding
v0.12.1,legacy TransformerTokenEmbedding
v0.12.1,legacy TransformerDocumentEmbedding
v0.12.1,copy values from new embedding
v0.12.1,cls first pooling can be done without recreating sentence hidden states
v0.12.1,make the tuple a tensor; makes working with it easier.
v0.12.1,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.12.1,only use layers that will be outputted
v0.12.1,this parameter is fixed
v0.12.1,IMPORTANT: add embeddings as torch modules
v0.12.1,"if only one sentence is passed, convert to list of sentence"
v0.12.1,make compatible with serialized models
v0.12.1,gensim version 4
v0.12.1,gensim version 3
v0.12.1,"if no embedding is set, the vocab and embedding length is requried"
v0.12.1,GLOVE embeddings
v0.12.1,TURIAN embeddings
v0.12.1,KOMNINOS embeddings
v0.12.1,pubmed embeddings
v0.12.1,FT-CRAWL embeddings
v0.12.1,FT-CRAWL embeddings
v0.12.1,twitter embeddings
v0.12.1,two-letter language code wiki embeddings
v0.12.1,two-letter language code wiki embeddings
v0.12.1,two-letter language code crawl embeddings
v0.12.1,fix serialized models
v0.12.1,"this is required to force the module on the cpu,"
v0.12.1,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.12.1,self.to(..) actually sets the device properly
v0.12.1,this ignores the get_cached_vec method when loading older versions
v0.12.1,it is needed for compatibility reasons
v0.12.1,gensim version 4
v0.12.1,gensim version 3
v0.12.1,use list of common characters if none provided
v0.12.1,translate words in sentence into ints using dictionary
v0.12.1,"sort words by length, for batching and masking"
v0.12.1,chars for rnn processing
v0.12.1,multilingual models
v0.12.1,English models
v0.12.1,Arabic
v0.12.1,Bulgarian
v0.12.1,Czech
v0.12.1,Danish
v0.12.1,German
v0.12.1,Spanish
v0.12.1,Basque
v0.12.1,Persian
v0.12.1,Finnish
v0.12.1,French
v0.12.1,Hebrew
v0.12.1,Hindi
v0.12.1,Croatian
v0.12.1,Indonesian
v0.12.1,Italian
v0.12.1,Japanese
v0.12.1,Malayalam
v0.12.1,Dutch
v0.12.1,Norwegian
v0.12.1,Polish
v0.12.1,Portuguese
v0.12.1,Pubmed
v0.12.1,Slovenian
v0.12.1,Swedish
v0.12.1,Tamil
v0.12.1,Spanish clinical
v0.12.1,CLEF HIPE Shared task
v0.12.1,Amharic
v0.12.1,Ukrainian
v0.12.1,load model if in pretrained model map
v0.12.1,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.12.1,CLEF HIPE models are lowercased
v0.12.1,embeddings are static if we don't do finetuning
v0.12.1,embed a dummy sentence to determine embedding_length
v0.12.1,set to eval mode
v0.12.1,make compatible with serialized models (TODO: remove)
v0.12.1,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.12.1,make compatible with serialized models (TODO: remove)
v0.12.1,gradients are enable if fine-tuning is enabled
v0.12.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.12.1,get hidden states from language model
v0.12.1,take first or last hidden states from language model as word representation
v0.12.1,offset mode that extracts at whitespace after last character
v0.12.1,offset mode that extracts at last character
v0.12.1,use the character language model embeddings as basis
v0.12.1,length is twice the original character LM embedding length
v0.12.1,these fields are for the embedding memory
v0.12.1,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.12.1,we re-compute embeddings dynamically at each epoch
v0.12.1,set the memory method
v0.12.1,memory is wiped each time we do a training run
v0.12.1,"if we keep a pooling, it needs to be updated continuously"
v0.12.1,update embedding
v0.12.1,check token.text is empty or not
v0.12.1,set aggregation operation
v0.12.1,add embeddings after updating
v0.12.1,model architecture
v0.12.1,model architecture
v0.12.1,"""pl"","
v0.12.1,download if necessary
v0.12.1,load the model
v0.12.1,"TODO: keep for backwards compatibility, but remove in future"
v0.12.1,save the sentence piece model as binary file (not as path which may change)
v0.12.1,write out the binary sentence piece model into the expected directory
v0.12.1,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.12.1,"otherwise, use normal process and potentially trigger another download"
v0.12.1,"once the modes if there, load it with sentence piece"
v0.12.1,empty words get no embedding
v0.12.1,all other words get embedded
v0.12.1,GLOVE embeddings
v0.12.1,no need to recreate as NILCEmbeddings
v0.12.1,read in test file if exists
v0.12.1,read in dev file if exists
v0.12.1,"find train, dev and test files if not specified"
v0.12.1,Add tags for each annotated span
v0.12.1,Remove leading and trailing whitespaces from annotated spans
v0.12.1,Search start and end token index for current span
v0.12.1,If end index is not found set to last token
v0.12.1,Throw error if indices are not valid
v0.12.1,get train data
v0.12.1,read in test file if exists
v0.12.1,read in dev file if exists
v0.12.1,"find train, dev and test files if not specified"
v0.12.1,special key for space after
v0.12.1,special key for feature columns
v0.12.1,special key for dependency head id
v0.12.1,"store either Sentence objects in memory, or only file offsets"
v0.12.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.12.1,determine encoding of text file
v0.12.1,identify which columns are spans and which are word-level
v0.12.1,now load all sentences
v0.12.1,skip first line if to selected
v0.12.1,option 1: keep Sentence objects in memory
v0.12.1,pointer to previous
v0.12.1,parse next sentence
v0.12.1,quit if last sentence reached
v0.12.1,skip banned sentences
v0.12.1,set previous and next sentence for context
v0.12.1,append parsed sentence to list in memory
v0.12.1,option 2: keep source data in memory
v0.12.1,"read lines for next sentence, but don't parse"
v0.12.1,quit if last sentence reached
v0.12.1,append raw lines for each sentence
v0.12.1,we make a distinction between word-level tags and span-level tags
v0.12.1,read first sentence to determine which columns are span-labels
v0.12.1,skip first line if to selected
v0.12.1,check the first 5 sentences
v0.12.1,go through all annotations and identify word- and span-level annotations
v0.12.1,- if a column has at least one BIES we know it's a Span label
v0.12.1,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.12.1,- problem cases are columns for which we see only O - in this case we default to Span
v0.12.1,skip assigned columns
v0.12.1,the space after key is always word-levels
v0.12.1,"if at least one token has a BIES, we know it's a span label"
v0.12.1,"if at least one token has a label other than BIOES, we know it's a token label"
v0.12.1,all remaining columns that are not word-level are span-level
v0.12.1,for column in self.word_level_tag_columns:
v0.12.1,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.12.1,"if sentence ends, break"
v0.12.1,parse comments if possible
v0.12.1,"otherwise, this line is a token. parse and add to sentence"
v0.12.1,check if this sentence is a document boundary
v0.12.1,add span labels
v0.12.1,discard tags from tokens that are not added to the sentence
v0.12.1,parse relations if they are set
v0.12.1,head and tail span indices are 1-indexed and end index is inclusive
v0.12.1,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.12.1,"to set the metadata ""domain"" to ""de-orcas"""
v0.12.1,get fields from line
v0.12.1,get head_id if exists (only in dependency parses)
v0.12.1,initialize token
v0.12.1,go through all columns
v0.12.1,'feats' and 'misc' column should be split into different fields
v0.12.1,special handling for whitespace after
v0.12.1,add each other feature as label-value pair
v0.12.1,get the task name (e.g. 'ner')
v0.12.1,get the label value
v0.12.1,add label
v0.12.1,remap regular tag names
v0.12.1,"if in memory, retrieve parsed sentence"
v0.12.1,else skip to position in file where sentence begins
v0.12.1,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.12.1,use all domains
v0.12.1,iter over all domains / sources and create target files
v0.12.1,Parameters
v0.12.1,The conll representation of coref spans allows spans to
v0.12.1,"overlap. If spans end or begin at the same word, they are"
v0.12.1,"separated by a ""|""."
v0.12.1,The span begins at this word.
v0.12.1,The span begins and ends at this word (single word span).
v0.12.1,"The span is starting, so we record the index of the word."
v0.12.1,"The span for this id is ending, but didn't start at this word."
v0.12.1,Retrieve the start index from the document state and
v0.12.1,add the span to the clusters for this id.
v0.12.1,Parameters
v0.12.1,strip all bracketing information to
v0.12.1,get the actual propbank label.
v0.12.1,Entering into a span for a particular semantic role label.
v0.12.1,We append the label and set the current span for this annotation.
v0.12.1,"If there's no '(' token, but the current_span_label is not None,"
v0.12.1,then we are inside a span.
v0.12.1,We're outside a span.
v0.12.1,"Exiting a span, so we reset the current span label for this annotation."
v0.12.1,The words in the sentence.
v0.12.1,The pos tags of the words in the sentence.
v0.12.1,the pieces of the parse tree.
v0.12.1,The lemmatised form of the words in the sentence which
v0.12.1,have SRL or word sense information.
v0.12.1,The FrameNet ID of the predicate.
v0.12.1,"The sense of the word, if available."
v0.12.1,"The current speaker, if available."
v0.12.1,"Cluster id -> List of (start_index, end_index) spans."
v0.12.1,Cluster id -> List of start_indices which are open for this id.
v0.12.1,Replace brackets in text and pos tags
v0.12.1,with a different token for parse trees.
v0.12.1,only keep ')' if there are nested brackets with nothing in them.
v0.12.1,There are some bad annotations in the CONLL data.
v0.12.1,"They contain no information, so to make this explicit,"
v0.12.1,we just set the parse piece to be None which will result
v0.12.1,in the overall parse tree being None.
v0.12.1,"If this is the first word in the sentence, create"
v0.12.1,empty lists to collect the NER and SRL BIO labels.
v0.12.1,"We can't do this upfront, because we don't know how many"
v0.12.1,"components we are collecting, as a sentence can have"
v0.12.1,variable numbers of SRL frames.
v0.12.1,Create variables representing the current label for each label
v0.12.1,sequence we are collecting.
v0.12.1,"If any annotation marks this word as a verb predicate,"
v0.12.1,we need to record its index. This also has the side effect
v0.12.1,of ordering the verbal predicates by their location in the
v0.12.1,"sentence, automatically aligning them with the annotations."
v0.12.1,"this would not be reached if parse_pieces contained None, hence the cast"
v0.12.1,Non-empty line. Collect the annotation.
v0.12.1,Collect any stragglers or files which might not
v0.12.1,have the '#end document' format for the end of the file.
v0.12.1,this dataset name
v0.12.1,check if data there
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,check if data there
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,download files if not present locally
v0.12.1,we need to slightly modify the original files by adding some new lines after document separators
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,Remove CoNLL-U meta information in the last column
v0.12.1,column format
v0.12.1,dataset name
v0.12.1,data folder: default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,dataset name
v0.12.1,data folder: default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,entity_mapping
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,data validation
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download files if not present locallys
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,# download zip
v0.12.1,merge the files in one as the zip is containing multiples files
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,check if data there
v0.12.1,create folder
v0.12.1,download dataset
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download and parse data if necessary
v0.12.1,create train test dev if not exist
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,If the extracted corpus file is not yet present in dir
v0.12.1,download zip if necessary
v0.12.1,"extracted corpus is not present , so unpacking it."
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download zip
v0.12.1,unpacking the zip
v0.12.1,merge the files in one as the zip is containing multiples files
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.12.1,download files if not present locally
v0.12.1,we need to modify the original files by adding new lines after after the end of each sentence
v0.12.1,if only one language is given
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,"use all languages if explicitly set to ""all"""
v0.12.1,download data if necessary
v0.12.1,initialize comlumncorpus and add it to list
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,"For each language in languages, the file is downloaded if not existent"
v0.12.1,Then a comlumncorpus of that data is created and saved in a list
v0.12.1,this list is handed to the multicorpus
v0.12.1,list that contains the columncopora
v0.12.1,download data if necessary
v0.12.1,"if language not downloaded yet, download it"
v0.12.1,create folder
v0.12.1,get google drive id from list
v0.12.1,download from google drive
v0.12.1,unzip
v0.12.1,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.12.1,transform data into required format
v0.12.1,"the processed dataset has the additional ending ""_new"""
v0.12.1,remove the unprocessed dataset
v0.12.1,initialize comlumncorpus and add it to list
v0.12.1,if no languages are given as argument all languages used in XTREME will be loaded
v0.12.1,if only one language is given
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,"For each language in languages, the file is downloaded if not existent"
v0.12.1,Then a comlumncorpus of that data is created and saved in a list
v0.12.1,This list is handed to the multicorpus
v0.12.1,list that contains the columncopora
v0.12.1,download data if necessary
v0.12.1,"if language not downloaded yet, download it"
v0.12.1,create folder
v0.12.1,download from HU Server
v0.12.1,unzip
v0.12.1,transform data into required format
v0.12.1,initialize comlumncorpus and add it to list
v0.12.1,if only one language is given
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,initialize comlumncorpus and add it to list
v0.12.1,download data if necessary
v0.12.1,unpack and write out in CoNLL column-like format
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,data is not in IOB2 format. Thus we transform it to IOB2
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,rename according to train - test - dev - convention
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,Add missing newline after header
v0.12.1,Workaround for empty tokens
v0.12.1,"Add ""real"" document marker"
v0.12.1,Dataset split mapping
v0.12.1,v2.0 only adds new language and splits for AJMC dataset
v0.12.1,Special document marker for sample splits in AJMC dataset
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download and parse data if necessary
v0.12.1,paths to train and test splits
v0.12.1,init corpus
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download and parse data if necessary
v0.12.1,iterate over all html files
v0.12.1,"get rid of html syntax, we only need the text"
v0.12.1,between all documents we write a separator symbol
v0.12.1,skip empty strings
v0.12.1,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.12.1,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.12.1,sentence splitting and tokenization
v0.12.1,iterate through all annotations and add to corresponding tokens
v0.12.1,find sentence to which annotation belongs
v0.12.1,position within corresponding sentence
v0.12.1,set annotation for tokens of entity mention
v0.12.1,write to out-file in column format
v0.12.1,"in case something goes wrong, delete the dataset and raise error"
v0.12.1,this dataset name
v0.12.1,download and parse data if necessary
v0.12.1,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.12.1,generate qid wikiname dictionaries
v0.12.1,merge dictionaries
v0.12.1,ignore first line
v0.12.1,commented and empty lines
v0.12.1,read all Q-IDs
v0.12.1,ignore first line
v0.12.1,request
v0.12.1,this dataset name
v0.12.1,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.12.1,like this we can quickly check if the corresponding page exists
v0.12.1,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.12.1,delete unprocessed file
v0.12.1,collect all wikiids
v0.12.1,create the dictionary
v0.12.1,request
v0.12.1,this dataset name
v0.12.1,names of raw text documents
v0.12.1,open output_file
v0.12.1,iterate through all documents
v0.12.1,split sentences and tokenize
v0.12.1,iterate through all annotations and add to corresponding tokens
v0.12.1,find sentence to which annotation belongs
v0.12.1,position within corresponding sentence
v0.12.1,set annotation for tokens of entity mention
v0.12.1,write to out file
v0.12.1,this dataset name
v0.12.1,download and parse data if necessary
v0.12.1,this dataset name
v0.12.1,download and parse data if necessary
v0.12.1,First parse the post titles
v0.12.1,Keep track of how many and which entity mentions does a given post title have
v0.12.1,Check if the current post title has an entity link and parse accordingly
v0.12.1,Post titles with entity mentions (if any) are handled via this function
v0.12.1,Then parse the comments
v0.12.1,"Iterate over the comments.tsv file, until the end is reached"
v0.12.1,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.12.1,Each comment thread is handled as one 'document'.
v0.12.1,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.12.1,This if-condition is needed to handle this problem.
v0.12.1,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.12.1,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.12.1,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.12.1,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.12.1,and not just single letters into single rows.
v0.12.1,If there are annotated entity mentions for given post title or a comment thread
v0.12.1,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.12.1,Write the token with a corresponding tag to file
v0.12.1,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.12.1,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.12.1,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.12.1,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.12.1,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.12.1,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.12.1,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.12.1,Check if further annotations belong to the current post title or comment thread as well
v0.12.1,Stop when the end of an annotation file is reached
v0.12.1,Check if further annotations belong to the current sentence as well
v0.12.1,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.12.1,Docstart
v0.12.1,if there is more than one word in the chunk we write each in a separate line
v0.12.1,print(chunks)
v0.12.1,empty line after each sentence
v0.12.1,convert the file to CoNLL
v0.12.1,this dataset name
v0.12.1,"check if data there, if not, download the data"
v0.12.1,create folder
v0.12.1,download data
v0.12.1,transform data into column format if necessary
v0.12.1,if no filenames are specified we use all the data
v0.12.1,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.12.1,also we remove 'raganato_ALL' from filenames in case its in the list
v0.12.1,generate the test file
v0.12.1,make column file and save to data_folder
v0.12.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.1,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.1,create folder
v0.12.1,download data
v0.12.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.1,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.1,create folder
v0.12.1,download data
v0.12.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.1,generate the test file
v0.12.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.1,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.1,create folder
v0.12.1,download data
v0.12.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.1,generate the test file
v0.12.1,default dataset folder is the cache root
v0.12.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.1,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.1,create folder
v0.12.1,download data
v0.12.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.1,generate the test file
v0.12.1,default dataset folder is the cache root
v0.12.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.1,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.1,create folder
v0.12.1,download data
v0.12.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.1,generate the test file
v0.12.1,default dataset folder is the cache root
v0.12.1,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12.1,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12.1,create folder
v0.12.1,download data
v0.12.1,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12.1,generate the test file
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,if True:
v0.12.1,write CoNLL-U Plus header
v0.12.1,"Some special cases (e.g., missing spaces before entity marker)"
v0.12.1,necessary if text should be whitespace tokenizeable
v0.12.1,Handle case where tail may occur before the head
v0.12.1,this dataset name
v0.12.1,write CoNLL-U Plus header
v0.12.1,this dataset name
v0.12.1,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.12.1,download data if necessary
v0.12.1,write CoNLL-U Plus header
v0.12.1,The span has ended.
v0.12.1,We are entering a new span; reset indices
v0.12.1,and active tag to new span.
v0.12.1,We're inside a span.
v0.12.1,Last token might have been a part of a valid span.
v0.12.1,this dataset name
v0.12.1,write CoNLL-U Plus header
v0.12.1,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.12.1,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.12.1,target_file_path = Path(data_folder) / target_filename
v0.12.1,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.12.1,# write CoNLL-U Plus header
v0.12.1,"target_file.write(""# global.columns = id form ner\n"")"
v0.12.1,for example in json.load(source_file):
v0.12.1,token_list = self._tacred_example_to_token_list(example)
v0.12.1,target_file.write(token_list.serialize())
v0.12.1,check if first tag row is already occupied
v0.12.1,"if first tag row is occupied, use second tag row"
v0.12.1,hardcoded mapping TODO: perhaps find nicer solution
v0.12.1,remap regular tag names
v0.12.1,else skip to position in file where sentence begins
v0.12.1,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.12.1,read in dev file if exists
v0.12.1,read in test file if exists
v0.12.1,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.12.1,"find train, dev and test files if not specified"
v0.12.1,use test_file to create test split if available
v0.12.1,use dev_file to create test split if available
v0.12.1,"if data point contains black-listed label, do not use"
v0.12.1,first check if valid sentence
v0.12.1,"if so, add to indices"
v0.12.1,"find train, dev and test files if not specified"
v0.12.1,variables
v0.12.1,different handling of in_memory data than streaming data
v0.12.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.12.1,test if format is OK
v0.12.1,test if at least one label given
v0.12.1,make sentence from text (and filter for length)
v0.12.1,"if a pair column is defined, make a sentence pair object"
v0.12.1,noinspection PyDefaultArgument
v0.12.1,dataset name includes the split size
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download each of the 28 splits
v0.12.1,create dataset directory if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,handle labels file
v0.12.1,handle data file
v0.12.1,Create flair compatible labels
v0.12.1,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,create dataset directory if necessary
v0.12.1,create train.txt file from CSV
v0.12.1,create test.txt file from CSV
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,create dataset directory if necessary
v0.12.1,create train.txt file by iterating over pos and neg file
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,create dataset directory if necessary
v0.12.1,create train.txt file by iterating over pos and neg file
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,create dataset directory if necessary
v0.12.1,create train.txt file by iterating over pos and neg file
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,create dataset directory if necessary
v0.12.1,create train.txt file by iterating over pos and neg file
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,create train dev and test files in fasttext format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download senteval datasets if necessary und unzip
v0.12.1,convert to FastText format
v0.12.1,download data if necessary
v0.12.1,"if data is not downloaded yet, download it"
v0.12.1,get the zip file
v0.12.1,move original .tsv files to another folder
v0.12.1,create train and dev splits in fasttext format
v0.12.1,create eval_dataset file with no labels
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,download datasets if necessary
v0.12.1,create dataset directory if necessary
v0.12.1,create correctly formated txt files
v0.12.1,multiple labels are possible
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,Create flair compatible labels
v0.12.1,TREC-6 : NUM:dist -> __label__NUM
v0.12.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,Create flair compatible labels
v0.12.1,TREC-6 : NUM:dist -> __label__NUM
v0.12.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,create a separate directory for different tasks
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,check if dataset is supported
v0.12.1,set file names
v0.12.1,set file names
v0.12.1,download and unzip in file structure if necessary
v0.12.1,instantiate corpus
v0.12.1,"find train, dev and test files if not specified"
v0.12.1,"create DataPairDataset for train, test and dev file, if they are given"
v0.12.1,stop if file does not exist
v0.12.1,create a DataPair object from strings
v0.12.1,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.12.1,"if data is not downloaded yet, download it"
v0.12.1,get the zip file
v0.12.1,"rename test file to eval_dataset, since it has no labels"
v0.12.1,"if data is not downloaded yet, download it"
v0.12.1,get the zip file
v0.12.1,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.12.1,dev sets include 5 different annotations but we will only keep the gold label
v0.12.1,"rename test file to eval_dataset, since it has no labels"
v0.12.1,"if data is not downloaded yet, download it"
v0.12.1,get test and dev sets
v0.12.1,"if data is not downloaded yet, download it"
v0.12.1,get the zip file
v0.12.1,"rename test file to eval_dataset, since it has no labels"
v0.12.1,"if data is not downloaded yet, download it"
v0.12.1,get the zip file
v0.12.1,"rename test file to eval_dataset, since it has no labels"
v0.12.1,"if data is not downloaded yet, download it"
v0.12.1,get the zip file
v0.12.1,"rename test file to eval_dataset, since it has no labels"
v0.12.1,"if data not downloaded yet, download it"
v0.12.1,get the zip file
v0.12.1,"the downloaded files have json format, we transform them to tsv"
v0.12.1,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.12.1,remove json file
v0.12.1,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.12.1,with sum of all entity lengths as secondary key
v0.12.1,calculate offset without current text
v0.12.1,because we stick all passages of a document together
v0.12.1,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.12.1,Try to fix incorrect annotations
v0.12.1,print(
v0.12.1,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.12.1,)
v0.12.1,Ignore empty lines or relation annotations
v0.12.1,FIX annotation of whitespaces (necessary for PDR)
v0.12.1,One token may contain multiple entities -> deque all of them
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.12.1,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,Edge case: last token starts a new entity
v0.12.1,Last document in file
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,In the huner split files there is no information whether a given id originates
v0.12.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.12.1,splitting here
v0.12.1,In the huner split files there is no information whether a given id originates
v0.12.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.12.1,splitting here
v0.12.1,In the huner split files there is no information whether a given id originates
v0.12.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.12.1,splitting here
v0.12.1,Edge case: last token starts a new entity
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,Read texts
v0.12.1,Read annotations
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,We need to apply a patch to correct the original training file
v0.12.1,Articles title
v0.12.1,Article abstract
v0.12.1,Entity annotations
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,Edge case: last token starts a new entity
v0.12.1,Map all entities to chemicals
v0.12.1,Map all entities to disease
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,Incomplete article
v0.12.1,Invalid XML syntax
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,if len(mid) != 3:
v0.12.1,continue
v0.12.1,Try to fix entity offsets
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,There is still one illegal annotation in the file ..
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,"Abstract first, title second to prevent issues with sentence splitting"
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,"Filter for specific entity types, by default no entities will be filtered"
v0.12.1,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.12.1,train and dev split of V2 will be train in V4
v0.12.1,test split of V2 will be dev in V4
v0.12.1,New documents in V4 will become test documents
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,column format
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,cache Feidegger config file
v0.12.1,cache Feidegger images
v0.12.1,replace image URL with local cached file
v0.12.1,append Sentence-Image data point
v0.12.1,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.12.1,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.12.1,"if so, num_workers is set to 0 for faster processing"
v0.12.1,cast to list if necessary
v0.12.1,cast to list if necessary
v0.12.1,"first, check if pymongo is installed"
v0.12.1,automatically identify train / test / dev files
v0.12.1,"if no test file is found, take any file with 'test' in name"
v0.12.1,Expose base classses
v0.12.1,Expose all biomedical data sets used for the evaluation of BioBERT
v0.12.1,-
v0.12.1,-
v0.12.1,-
v0.12.1,-
v0.12.1,Expose all biomedical data sets using the HUNER splits
v0.12.1,Expose all biomedical data sets
v0.12.1,Expose all document classification datasets
v0.12.1,word sense disambiguation
v0.12.1,Expose all entity linking datasets
v0.12.1,Expose all relation extraction datasets
v0.12.1,universal proposition banks
v0.12.1,keyphrase detection datasets
v0.12.1,other NER datasets
v0.12.1,standard NER datasets
v0.12.1,Expose all sequence labeling datasets
v0.12.1,Expose all text-image datasets
v0.12.1,Expose all text-text datasets
v0.12.1,Expose all treebanks
v0.12.1,"find train, dev and test files if not specified"
v0.12.1,get train data
v0.12.1,get test data
v0.12.1,get dev data
v0.12.1,option 1: read only sentence boundaries as offset positions
v0.12.1,option 2: keep everything in memory
v0.12.1,"if in memory, retrieve parsed sentence"
v0.12.1,else skip to position in file where sentence begins
v0.12.1,current token ID
v0.12.1,handling for the awful UD multiword format
v0.12.1,end of sentence
v0.12.1,comments
v0.12.1,ellipsis
v0.12.1,if token is a multi-word
v0.12.1,normal single-word tokens
v0.12.1,"if we don't split multiwords, skip over component words"
v0.12.1,add token
v0.12.1,add morphological tags
v0.12.1,derive whitespace logic for multiwords
v0.12.1,print(token)
v0.12.1,print(current_multiword_last_token)
v0.12.1,print(current_multiword_first_token)
v0.12.1,"if multi-word equals component tokens, there should be no whitespace"
v0.12.1,go through all tokens in subword and set whitespace_after information
v0.12.1,print(i)
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,download data if necessary
v0.12.1,this dataset name
v0.12.1,default dataset folder is the cache root
v0.12.1,download data if necessary
v0.12.1,"finally, print model card for information"
v0.12.1,test corpus
v0.12.1,create a TARS classifier
v0.12.1,check if right number of classes
v0.12.1,switch to task with only one label
v0.12.1,check if right number of classes
v0.12.1,switch to task with three labels provided as list
v0.12.1,check if right number of classes
v0.12.1,switch to task with four labels provided as set
v0.12.1,check if right number of classes
v0.12.1,switch to task with two labels provided as Dictionary
v0.12.1,check if right number of classes
v0.12.1,test corpus
v0.12.1,create a TARS classifier
v0.12.1,switch to a new task (TARS can do multiple tasks so you must define one)
v0.12.1,initialize the text classifier trainer
v0.12.1,start the training
v0.12.1,"With end symbol, without start symbol, padding in front"
v0.12.1,"Without end symbol, with start symbol, padding in back"
v0.12.1,"Without end symbol, without start symbol, padding in front"
v0.12.1,initialize trainer
v0.12.1,initialize trainer
v0.12.1,train model for 2 epochs
v0.12.1,load the checkpoint model and train until epoch 4
v0.12.1,clean up results directory
v0.12.1,initialize trainer
v0.12.1,initialize trainer
v0.12.1,increment for last token in sentence if not followed by whitespace
v0.12.1,clean up directory
v0.12.1,clean up directory
v0.12.1,example sentence
v0.12.1,set 4 labels for 2 tokens ('love' is tagged twice)
v0.12.1,check if there are three POS labels with correct text and values
v0.12.1,check if there are is one SENTIMENT label with correct text and values
v0.12.1,check if all tokens are correctly labeled
v0.12.1,remove the pos label from the last word
v0.12.1,there should be 2 POS labels left
v0.12.1,now remove all pos tags
v0.12.1,set 3 labels for 2 spans (HU is tagged twice)
v0.12.1,check if there are three labels with correct text and values
v0.12.1,check if there are two spans with correct text and values
v0.12.1,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.12.1,should be only one NER label left
v0.12.1,and only one NER span
v0.12.1,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.12.1,check if there are three labels with correct text and values
v0.12.1,check if there are two spans with correct text and values
v0.12.1,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.12.1,should be only one NER label left
v0.12.1,and only one NER span
v0.12.1,but there is also one orgtype span and label
v0.12.1,and only one NER span
v0.12.1,let's add the NER tag back
v0.12.1,check if there are three labels with correct text and values
v0.12.1,check if there are two spans with correct text and values
v0.12.1,now remove all NER tags
v0.12.1,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.12.1,create two relation label
v0.12.1,there should be two relation labels
v0.12.1,there should be one syntactic labels
v0.12.1,"there should be two relations, one with two and one with one label"
v0.12.1,example sentence
v0.12.1,add another topic label
v0.12.1,example sentence
v0.12.1,has sentiment value
v0.12.1,has 4 part of speech tags
v0.12.1,has 1 NER tag
v0.12.1,should be in total 6 labels
v0.12.1,example sentence
v0.12.1,add two NER labels
v0.12.1,get the four labels
v0.12.1,check that only two of the respective data points are equal
v0.12.1,make a sentence and some right context
v0.12.1,TODO: is this desirable? Or should two sentences with same text still be considered different objects?
v0.12.1,Initializing a Sentence this way assumes that there is a space after each token
v0.12.1,get default dictionary
v0.12.1,init forward LM with 128 hidden states and 1 layer
v0.12.1,get the example corpus and process at character level in forward direction
v0.12.1,train the language model
v0.12.1,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.12.1,clean up results directory
v0.12.1,get default dictionary
v0.12.1,init forward LM with 128 hidden states and 1 layer
v0.12.1,get the example corpus and process at character level in forward direction
v0.12.1,train the language model
v0.12.1,define search space
v0.12.1,sequence tagger parameter
v0.12.1,model trainer parameter
v0.12.1,training parameter
v0.12.1,find best parameter settings
v0.12.1,clean up results directory
v0.12.1,document embeddings parameter
v0.12.1,training parameter
v0.12.1,clean up results directory
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,load column dataset with one entry
v0.12.1,load column dataset with two entries
v0.12.1,load column dataset with three entries
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,check if Token labels are correct
v0.12.1,"get training, test and dev data"
v0.12.1,check if Token labels for frames are correct
v0.12.1,"get training, test and dev data"
v0.12.1,"get training, test and dev data"
v0.12.1,get two corpora as one
v0.12.1,"get training, test and dev data for full English UD corpus from web"
v0.12.1,clean up data directory
v0.12.1,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.12.1,"""2"","
v0.12.1,"""0"","
v0.12.1,"""4"","
v0.12.1,"""2"","
v0.12.1,"""2"","
v0.12.1,"""2"","
v0.12.1,]
v0.12.1,"Here, we use the default token annotation fields."
v0.12.1,"We have manually checked, that these numbers are correct:"
v0.12.1,"+1 offset, because of missing EOS marker at EOD"
v0.12.1,Test data for v2.1 release
v0.12.1,--- Embeddings that are shared by both models --- #
v0.12.1,--- Task 1: Sentiment Analysis (5-class) --- #
v0.12.1,Define corpus and model
v0.12.1,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.12.1,Define corpus and model
v0.12.1,-- Define mapping (which tagger should train on which model) -- #
v0.12.1,-- Create model trainer and train -- #
v0.12.1,load dataset
v0.12.1,tagger without CRF
v0.12.1,train
v0.12.1,check if loaded model can predict
v0.12.1,check if loaded model successfully fit the training data
v0.12.1,load dataset
v0.12.1,tagger without CRF
v0.12.1,train
v0.12.1,check if loaded model can predict
v0.12.1,check if loaded model successfully fit the training data
v0.12.1,load dataset
v0.12.1,tagger without CRF
v0.12.1,train
v0.12.1,check if loaded model can predict
v0.12.1,check if loaded model successfully fit the training data
v0.12.1,load dataset
v0.12.1,tagger without CRF
v0.12.1,train
v0.12.1,check if loaded model can predict
v0.12.1,check if loaded model successfully fit the training data
v0.12.1,check if model can predict
v0.12.1,load model
v0.12.1,chcek if model predicts correct label
v0.12.1,check if loaded model successfully fit the training data
v0.12.1,check if model can predict
v0.12.1,load model
v0.12.1,chcek if model predicts correct label
v0.12.1,check if loaded model successfully fit the training data
v0.12.1,check if model can predict
v0.12.1,load model
v0.12.1,chcek if model predicts correct label
v0.12.1,check if loaded model successfully fit the training data
v0.12.1,clean up file
v0.12.1,no need for label_dict
v0.12.1,check if right number of classes
v0.12.1,switch to task with only one label
v0.12.1,check if right number of classes
v0.12.1,switch to task with three labels provided as list
v0.12.1,check if right number of classes
v0.12.1,switch to task with four labels provided as set
v0.12.1,check if right number of classes
v0.12.1,switch to task with two labels provided as Dictionary
v0.12.1,check if right number of classes
v0.12.1,Intel ----founded_by---> Gordon Moore
v0.12.1,Intel ----founded_by---> Robert Noyce
v0.12.1,Check sentence masking and relation label annotation on
v0.12.1,"training, validation and test dataset (in this test the splits are the same)"
v0.12.1,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.12.1,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.12.1,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.12.1,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
v0.12.1,This sentence is only included if we transform the corpus with cross augmentation
v0.12.1,Ensure this is an example that predicts no classes in multilabel
v0.12.1,check if right number of classes
v0.12.1,switch to task with only one label
v0.12.1,check if right number of classes
v0.12.1,switch to task with three labels provided as list
v0.12.1,check if right number of classes
v0.12.1,switch to task with four labels provided as set
v0.12.1,check if right number of classes
v0.12.1,switch to task with two labels provided as Dictionary
v0.12.1,check if right number of classes
v0.12.1,ensure that the prepared tensors is what we expect
v0.12.1,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.12.1,previous and next sentence as context
v0.12.1,test expansion for sentence without context
v0.12.1,test expansion for with previous and next as context
v0.12.1,test expansion if first sentence is document boundary
v0.12.1,test expansion if we don't use context
v0.12.1,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.12.1,dummy model with embeddings
v0.12.1,save the dummy and load it again
v0.12.1,check that context_length and use_context_separator is the same for both
v0.12,mmap seems to be much more memory efficient
v0.12,Remove quotes from etag
v0.12,"If there is an etag, it's everything after the first period"
v0.12,"Otherwise, use None"
v0.12,"URL, so get it from the cache (downloading if necessary)"
v0.12,"File, and it exists."
v0.12,"File, but it doesn't exist."
v0.12,Something unknown
v0.12,Extract all the contents of zip file in current directory
v0.12,Extract all the contents of zip file in current directory
v0.12,TODO(joelgrus): do we want to do checksums or anything like that?
v0.12,get cache path to put the file
v0.12,make HEAD request to check ETag
v0.12,add ETag to filename if it exists
v0.12,"etag = response.headers.get(""ETag"")"
v0.12,"Download to temporary file, then copy to cache dir once finished."
v0.12,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.12,GET file object
v0.12,These defaults are the same as the argument defaults in tqdm.
v0.12,load_big_file is a workaround byhttps://github.com/highway11git
v0.12,to load models on some Mac/Windows setups
v0.12,see https://github.com/zalandoresearch/flair/issues/351
v0.12,first determine the distribution of classes in the dataset
v0.12,weight for each sample
v0.12,Create blocks
v0.12,shuffle the blocks
v0.12,concatenate the shuffled blocks
v0.12,Create blocks
v0.12,shuffle the blocks
v0.12,concatenate the shuffled blocks
v0.12,increment for last token in sentence if not followed by whitespace
v0.12,this is the default init size of a lmdb database for embeddings
v0.12,get db filename from embedding name
v0.12,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.12,SequenceTagger
v0.12,TextClassifier
v0.12,get db filename from embedding name
v0.12,if embedding database already exists
v0.12,"otherwise, push embedding to database"
v0.12,if embedding database already exists
v0.12,open the database in read mode
v0.12,we need to set self.k
v0.12,create and load the database in write mode
v0.12,"no idea why, but we need to close and reopen the environment to avoid"
v0.12,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.12,when opening new transaction !
v0.12,init dictionaries
v0.12,"in order to deal with unknown tokens, add <unk>"
v0.12,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.12,set 'add_unk' depending on whether <unk> is a key
v0.12,"if one embedding name, directly return it"
v0.12,"if multiple embedding names, concatenate them"
v0.12,TODO: does it make sense to exclude labels? Two data points of identical text (but different labels)
v0.12,would be equal now.
v0.12,First we remove any existing labels for this PartOfSentence in self.sentence
v0.12,labels also need to be deleted at Sentence object
v0.12,delete labels at object itself
v0.12,private field for all known spans
v0.12,the tokenizer used for this sentence
v0.12,some sentences represent a document boundary (but most do not)
v0.12,internal variables to denote position inside dataset
v0.12,"if text is passed, instantiate sentence with tokens (words)"
v0.12,determine token positions and whitespace_after flag
v0.12,the last token has no whitespace after
v0.12,log a warning if the dataset is empty
v0.12,data with zero-width characters cannot be handled
v0.12,set token idx and sentence
v0.12,append token to sentence
v0.12,register token annotations on sentence
v0.12,move sentence embeddings to device
v0.12,also move token embeddings to device
v0.12,clear token embeddings
v0.12,infer whitespace after field
v0.12,"if sentence has no tokens, return empty string"
v0.12,"otherwise, return concatenation of tokens with the correct offsets"
v0.12,The sentence's start position is not propagated to its tokens.
v0.12,"Therefore, we need to add the sentence's start position to its last token's end position, including whitespaces."
v0.12,No character at the corresponding code point: remove it
v0.12,"if no label if specified, return all labels"
v0.12,"if the label type exists in the Sentence, return it"
v0.12,return empty list if none of the above
v0.12,labels also need to be deleted at all tokens
v0.12,labels also need to be deleted at all known spans
v0.12,remove spans without labels
v0.12,delete labels at object itself
v0.12,set name
v0.12,abort if no data is provided
v0.12,sample test data from train if none is provided
v0.12,sample dev data from train if none is provided
v0.12,set train dev and test data
v0.12,find out empty sentence indices
v0.12,create subset of non-empty sentence indices
v0.12,find out empty sentence indices
v0.12,create subset of non-empty sentence indices
v0.12,count all label types per sentence
v0.12,go through all labels of label_type and count values
v0.12,check if there are any span labels
v0.12,"if an unk threshold is set, UNK all label values below this threshold"
v0.12,sample randomly from a label distribution according to the probabilities defined by the desired noise share
v0.12,replace the old label with the new one
v0.12,keep track of the old (clean) label using another label type category
v0.12,keep track of how many labels in total are flipped
v0.12,Make the tag dictionary
v0.12,"add a dummy ""O"" to close final prediction"
v0.12,return complex list
v0.12,internal variables
v0.12,non-set tags are OUT tags
v0.12,anything that is not OUT is IN
v0.12,does this prediction start a new span?
v0.12,begin and single tags start new spans
v0.12,"in IOB format, an I tag starts a span if it follows an O or is a different span"
v0.12,single tags that change prediction start new spans
v0.12,if an existing span is ended (either by reaching O or starting a new span)
v0.12,determine score and value
v0.12,append to result list
v0.12,reset for-loop variables for new span
v0.12,remember previous tag
v0.12,global variable: cache_root
v0.12,global variable: device
v0.12,global variable: version
v0.12,global variable: arrow symbol
v0.12,dummy return to fulfill trainer.train() needs
v0.12,print(vec)
v0.12,Attach optimizer
v0.12,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.12,if memory mode option 'none' delete everything
v0.12,"if dynamic embedding keys not passed, identify them automatically"
v0.12,always delete dynamic embeddings
v0.12,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.12,optional metric space decoder if prototypes have different length than embedding
v0.12,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.12,"if set, create initial prototypes from normal distribution"
v0.12,"if set, use a radius"
v0.12,all parameters will be pushed internally to the specified device
v0.12,decode embeddings into prototype space
v0.12,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.12,Always include the name of the Model class for which the state dict holds
v0.12,"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
v0.12,"write out a ""model card"" if one is set"
v0.12,special handling for optimizer:
v0.12,remember optimizer class and state dictionary
v0.12,save model
v0.12,restore optimizer and scheduler to model card if set
v0.12,"if this class is abstract, go through all inheriting classes and try to fetch and load the model"
v0.12,get all non-abstract subclasses
v0.12,"try to fetch the model for each subclass. if fetching is possible, load model and return it"
v0.12,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.12,"if the model cannot be fetched, load as a file"
v0.12,try to get model class from state
v0.12,"older (flair 11.3 and below) models do not contain cls information. In this case, try all subclasses"
v0.12,"if str(model_cls) == ""<class 'flair.models.pairwise_classification_model.TextPairClassifier'>"": continue"
v0.12,"skip any invalid loadings, e.g. not found on huggingface hub"
v0.12,"if this class is not abstract, fetch the model and load it"
v0.12,"make sure <unk> is contained in gold_label_dictionary, if given"
v0.12,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.12,loss calculation
v0.12,variables for printing
v0.12,variables for computing scores
v0.12,remove any previously predicted labels
v0.12,predict for batch
v0.12,get the gold labels
v0.12,add to all_predicted_values
v0.12,make printout lines
v0.12,convert true and predicted values to two span-aligned lists
v0.12,delete exluded labels if exclude_labels is given
v0.12,"if after excluding labels, no label is left, ignore the datapoint"
v0.12,write all_predicted_values to out_file if set
v0.12,make the evaluation dictionary
v0.12,check if this is a multi-label problem
v0.12,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.12,multi-label problems require a multi-hot vector for each true and predicted label
v0.12,single-label problems can do with a single index for each true and predicted label
v0.12,"now, calculate evaluation numbers"
v0.12,there is at least one gold label or one prediction (default)
v0.12,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.12,"micro average is only computed if zero-label exists (for instance ""O"")"
v0.12,if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
v0.12,same for the main score
v0.12,issue error and default all evaluation numbers to 0.
v0.12,line for log file
v0.12,check if there is a label mismatch
v0.12,print info
v0.12,set the embeddings
v0.12,initialize the label dictionary
v0.12,initialize the decoder
v0.12,set up multi-label logic
v0.12,init dropouts
v0.12,loss weights and loss function
v0.12,Initialize the weight tensor
v0.12,set up gradient reversal if so specified
v0.12,embed sentences
v0.12,get a tensor of data points
v0.12,do dropout
v0.12,make a forward pass to produce embedded data points and labels
v0.12,get the data points for which to predict labels
v0.12,get their gold labels as a tensor
v0.12,pass data points through network to get encoded data point tensor
v0.12,decode
v0.12,an optional masking step (no masking in most cases)
v0.12,calculate the loss
v0.12,filter empty sentences
v0.12,reverse sort all sequences by their length
v0.12,progress bar for verbosity
v0.12,filter data points in batch
v0.12,stop if all sentences are empty
v0.12,pass data points through network and decode
v0.12,if anything could possibly be predicted
v0.12,remove previously predicted labels of this type
v0.12,add DefaultClassifier arguments
v0.12,add variables of DefaultClassifier
v0.12,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.12,Get projected 1st dimension
v0.12,Compute bilinear form
v0.12,Arcosh
v0.12,Project the input data to n+1 dimensions
v0.12,"The first dimension, is recomputed in the distance module"
v0.12,header for 'weights.txt'
v0.12,"determine the column index of loss, f-score and accuracy for"
v0.12,"train, dev and test split"
v0.12,then get all relevant values from the tsv
v0.12,then get all relevant values from the tsv
v0.12,plot i
v0.12,save plots
v0.12,save plots
v0.12,plt.show()
v0.12,save plot
v0.12,take the average over the last three scores of training
v0.12,take average over the scores from the different training runs
v0.12,auto-spawn on GPU if available
v0.12,progress bar for verbosity
v0.12,stop if all sentences are empty
v0.12,clearing token embeddings to save memory
v0.12,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.12,TODO: not saving lines yet
v0.12,TODO: This closely shadows the RelationExtractor name. Maybe we need a better name here.
v0.12,- MaskedRelationClassifier ?
v0.12,This depends if this relation classification architecture should replace or offer as an alternative.
v0.12,Set label type and prepare label dictionary
v0.12,Initialize super default classifier
v0.12,Add the special tokens from the encoding strategy
v0.12,"Auto-spawn on GPU, if available"
v0.12,Only use entities labelled with the specified labels for each label type
v0.12,Only use entities above the specified threshold
v0.12,Use a dictionary to find gold relation annotations for a given entity pair
v0.12,Yield head and tail entity pairs from the cross product of all entities
v0.12,Remove identity relation entity pairs
v0.12,Remove entity pairs with labels that do not match any
v0.12,of the specified relations in `self.entity_pair_labels`
v0.12,"Obtain gold label, if existing"
v0.12,Some sanity checks
v0.12,Pre-compute non-leading head and tail tokens for entity masking
v0.12,We can not use the plaintext of the head/tail span in the sentence as the mask/marker
v0.12,since there may be multiple occurrences of the same entity mentioned in the sentence.
v0.12,"Therefore, we use the span's position in the sentence."
v0.12,Create masked sentence
v0.12,Add gold relation annotation as sentence label
v0.12,"Using the sentence label instead of annotating a separate `Relation` object is easier to manage since,"
v0.12,"during prediction, the forward pass does not need any knowledge about the entities in the sentence."
v0.12,"If we sample missing splits, the encoded sentences that correspond to the same original sentences"
v0.12,"may get distributed into different splits. For training purposes, this is always undesired."
v0.12,Ensure that all sentences are encoded properly
v0.12,Deal with the case where all sentences are encoded sentences
v0.12,"mypy does not infer the type of ""sentences"" restricted by the if statement"
v0.12,Deal with the case where all sentences are standard (non-encoded) sentences
v0.12,"For each encoded sentence, transfer its prediction onto the original relation"
v0.12,auto-spawn on GPU if available
v0.12,pad strings with whitespaces to longest sentence
v0.12,cut up the input into chunks of max charlength = chunk_size
v0.12,push each chunk through the RNN language model
v0.12,concatenate all chunks to make final output
v0.12,initial hidden state
v0.12,get predicted weights
v0.12,divide by temperature
v0.12,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.12,this makes a vector in which the largest value is 0
v0.12,compute word weights with exponential function
v0.12,try sampling multinomial distribution for next character
v0.12,print(word_idx)
v0.12,input ids
v0.12,push list of character IDs through model
v0.12,the target is always the next character
v0.12,use cross entropy loss to compare output of forward pass with targets
v0.12,exponentiate cross-entropy loss to calculate perplexity
v0.12,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.12,serialize the language models and the constructor arguments (but nothing else)
v0.12,special handling for deserializing language models
v0.12,re-initialize language model with constructor arguments
v0.12,copy over state dictionary to self
v0.12,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.12,"in their ""self.train()"" method)"
v0.12,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.12,"check if this is the case and if so, set it"
v0.12,Transform input data into TARS format
v0.12,"if there are no labels, return a random sample as negatives"
v0.12,"otherwise, go through all labels"
v0.12,make sure the probabilities always sum up to 1
v0.12,get and embed all labels by making a Sentence object that contains only the label text
v0.12,get each label embedding and scale between 0 and 1
v0.12,compute similarity matrix
v0.12,"the higher the similarity, the greater the chance that a label is"
v0.12,sampled as negative example
v0.12,make label dictionary if no Dictionary object is passed
v0.12,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.12,check if candidate_label_set is empty
v0.12,make list if only one candidate label is passed
v0.12,create label dictionary
v0.12,note current task
v0.12,create a temporary task
v0.12,make zero shot predictions
v0.12,switch to the pre-existing task
v0.12,prepare TARS dictionary
v0.12,initialize a bare-bones sequence tagger
v0.12,transformer separator
v0.12,Store task specific labels since TARS can handle multiple tasks
v0.12,make a tars sentence where all labels are O by default
v0.12,init new TARS classifier
v0.12,set all task information
v0.12,return
v0.12,with torch.no_grad():
v0.12,progress bar for verbosity
v0.12,stop if all sentences are empty
v0.12,go through each sentence in the batch
v0.12,always remove tags first
v0.12,get the span and its label
v0.12,determine whether tokens in this span already have a label
v0.12,only add if all tokens have no label
v0.12,make and add a corresponding predicted span
v0.12,set indices so that no token can be tagged twice
v0.12,clearing token embeddings to save memory
v0.12,"all labels default to ""O"""
v0.12,set gold token-level
v0.12,set predicted token-level
v0.12,now print labels in CoNLL format
v0.12,prepare TARS dictionary
v0.12,initialize a bare-bones sequence tagger
v0.12,transformer separator
v0.12,Store task specific labels since TARS can handle multiple tasks
v0.12,get the serialized embeddings
v0.12,remap state dict for models serialized with Flair <= 0.11.3
v0.12,init new TARS classifier
v0.12,set all task information
v0.12,with torch.no_grad():
v0.12,progress bar for verbosity
v0.12,stop if all sentences are empty
v0.12,go through each sentence in the batch
v0.12,always remove tags first
v0.12,add all labels that according to TARS match the text and are above threshold
v0.12,do not add labels below confidence threshold
v0.12,only use label with highest confidence if enforcing single-label predictions
v0.12,get all label scores and do an argmax to get the best label
v0.12,remove previously added labels and only add the best label
v0.12,clearing token embeddings to save memory
v0.12,set separator to concatenate two sentences
v0.12,auto-spawn on GPU if available
v0.12,pooling operation to get embeddings for entites
v0.12,set embeddings
v0.12,set relation and entity label types
v0.12,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.12,filter entity pairs according to their tags if set
v0.12,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.12,character dictionary for decoding and encoding
v0.12,make sure <unk> is in dictionary for handling of unknown characters
v0.12,add special symbols to dictionary if necessary and save respective indices
v0.12,---- ENCODER ----
v0.12,encoder character embeddings
v0.12,encoder pre-trained embeddings
v0.12,encoder RNN
v0.12,additional encoder linear layer if bidirectional encoding
v0.12,---- DECODER ----
v0.12,decoder: linear layers to transform vectors to and from alphabet_size
v0.12,when using attention we concatenate attention outcome and decoder hidden states
v0.12,decoder RNN
v0.12,loss and softmax
v0.12,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.12,add additional columns for special symbols if necessary
v0.12,initialize with dummy symbols
v0.12,encode inputs
v0.12,get labels (we assume each token has a lemma label)
v0.12,get char indices for labels of sentence
v0.12,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.12,max_sequence_length = length of longest label of sentence + 1
v0.12,get char embeddings
v0.12,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.12,take decoder input and initial hidden and pass through RNN
v0.12,"if all encoder outputs are provided, use attention"
v0.12,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.12,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.12,get all tokens
v0.12,encode input characters by sending them through RNN
v0.12,get one-hots for characters and add special symbols / padding
v0.12,determine length of each token
v0.12,embed sentences
v0.12,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.12,variable to store initial hidden states for decoder
v0.12,encode input characters by sending them through RNN
v0.12,test packing and padding
v0.12,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.12,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.12,decoder later with self.emb_to_hidden
v0.12,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.12,use token embedding as initial hidden state for decoder
v0.12,concatenate everything together and project to appropriate size for decoder
v0.12,variable to store initial hidden states for decoder
v0.12,encode input characters by sending them through RNN
v0.12,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.12,embed character one-hots
v0.12,send through encoder RNN (produces initial hidden for decoder)
v0.12,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.12,project 2*hidden_size to hidden_size
v0.12,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.12,later with self.emb_to_hidden
v0.12,use token embedding as initial hidden state for decoder
v0.12,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.12,concatenate everything together and project to appropriate size for decoder
v0.12,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.12,"create target vector (batch_size, max_label_seq_length + 1)"
v0.12,filter empty sentences
v0.12,max length of the predicted sequences
v0.12,for printing
v0.12,stop if all sentences are empty
v0.12,remove previously predicted labels of this type
v0.12,create list of tokens in batch
v0.12,encode inputs
v0.12,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.12,sequence length is always set to one in prediction
v0.12,option 1: greedy decoding
v0.12,predictions
v0.12,decode next character
v0.12,pick top beam size many outputs with highest probabilities
v0.12,option 2: beam search
v0.12,out_probs = self.softmax(output_vectors).squeeze(1)
v0.12,make sure no dummy symbol <> or start symbol <S> is predicted
v0.12,pick top beam size many outputs with highest probabilities
v0.12,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.12,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.12,keep scores of beam_size many hypothesis for each token in the batch
v0.12,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.12,save sequences so far
v0.12,keep track of how many hypothesis were completed for each token
v0.12,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.12,decode with log softmax
v0.12,make sure no dummy symbol <> or start symbol <S> is predicted
v0.12,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.12,"if the sequence is already ended, do not record as candidate"
v0.12,index of token in in list tokens_in_batch
v0.12,print(token_number)
v0.12,hypothesis score
v0.12,TODO: remove token if number of completed hypothesis exceeds given value
v0.12,set score of corresponding entry to -inf so it will not be expanded
v0.12,get leading_indices for next expansion
v0.12,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.12,take beam_size many copies of scores vector and add scores of possible new extensions
v0.12,"size (beam_size*batch_size, beam_size)"
v0.12,print(hypothesis_scores)
v0.12,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.12,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.12,print(hypothesis_scores_per_token)
v0.12,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.12,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.12,a list of length beam_size*batch_size
v0.12,"where the first three inidices belong to the first token, the next three to the second token,"
v0.12,and so on
v0.12,with these indices we can compute the tensors for the next iteration
v0.12,expand sequences with corresponding index
v0.12,add log-probabilities to the scores
v0.12,save new leading indices
v0.12,save corresponding hidden states
v0.12,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.12,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.12,get best final hypothesis for each token
v0.12,get characters from index sequences and add predicted label to token
v0.12,dictionaries
v0.12,all parameters will be pushed internally to the specified device
v0.12,now print labels in CoNLL format
v0.12,internal candidate lists of generator
v0.12,load Zelda candidates if so passed
v0.12,create candidate lists
v0.12,"if lower casing is enabled, create candidate lists of lower cased versions"
v0.12,create a new dictionary for lower cased mentions
v0.12,go through each mention and its candidates
v0.12,"check if backoff mention already seen. If so, add candidates. Else, create new entry."
v0.12,set lowercased version as map
v0.12,remap state dict for models serialized with Flair <= 0.11.3
v0.12,get the candidates
v0.12,"during training, add the gold value as candidate"
v0.12,----- Create the internal tag dictionary -----
v0.12,span-labels need special encoding (BIO or BIOES)
v0.12,the big question is whether the label dictionary should contain an UNK or not
v0.12,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.12,"with UNK, the model learns less well if there are no UNK examples"
v0.12,is this a span prediction problem?
v0.12,----- Embeddings -----
v0.12,----- Initial loss weights parameters -----
v0.12,----- RNN specific parameters -----
v0.12,----- Conditional Random Field parameters -----
v0.12,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.12,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.12,----- Dropout parameters -----
v0.12,dropouts
v0.12,remove word dropout if there is no contact over the sequence dimension.
v0.12,----- Model layers -----
v0.12,----- RNN layer -----
v0.12,"If shared RNN provided, else create one for model"
v0.12,Whether to train initial hidden state
v0.12,final linear map to tag space
v0.12,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.12,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.12,"if there are no sentences, there is no loss"
v0.12,forward pass to get scores
v0.12,calculate loss given scores and labels
v0.12,make a zero-padded tensor for the whole sentence
v0.12,linear map to tag space
v0.12,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.12,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.12,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.12,spans need to be encoded as token-level predictions
v0.12,all others are regular labels for each token
v0.12,make sure it's a list
v0.12,filter empty sentences
v0.12,reverse sort all sequences by their length
v0.12,progress bar for verbosity
v0.12,stop if all sentences are empty
v0.12,get features from forward propagation
v0.12,remove previously predicted labels of this type
v0.12,"if return_loss, get loss value"
v0.12,make predictions
v0.12,add predictions to Sentence
v0.12,BIOES-labels need to be converted to spans
v0.12,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.12,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.12,core Flair models on Huggingface ModelHub
v0.12,"Large NER models,"
v0.12,Multilingual NER models
v0.12,English POS models
v0.12,Multilingual POS models
v0.12,English SRL models
v0.12,English chunking models
v0.12,Language-specific NER models
v0.12,Language-specific POS models
v0.12,English NER models
v0.12,Multilingual NER models
v0.12,English POS models
v0.12,Multilingual POS models
v0.12,English SRL models
v0.12,English chunking models
v0.12,Danish models
v0.12,German models
v0.12,French models
v0.12,Dutch models
v0.12,Malayalam models
v0.12,Portuguese models
v0.12,Keyphase models
v0.12,Biomedical models
v0.12,check if model name is a valid local file
v0.12,"check if model key is remapped to HF key - if so, print out information"
v0.12,get mapped name
v0.12,use mapped name instead
v0.12,"if not, check if model key is remapped to direct download location. If so, download model"
v0.12,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.12,"for all other cases (not local file or special download location), use HF model hub"
v0.12,"if not a local file, get from model hub"
v0.12,use model name as subfolder
v0.12,Lazy import
v0.12,output information
v0.12,## Demo: How to use in Flair
v0.12,load tagger
v0.12,make example sentence
v0.12,predict NER tags
v0.12,print sentence
v0.12,print predicted NER spans
v0.12,iterate over entities and print
v0.12,Lazy import
v0.12,Save model weight
v0.12,Determine if model card already exists
v0.12,Generate and save model card
v0.12,Upload files
v0.12,"all labels default to ""O"""
v0.12,set gold token-level
v0.12,set predicted token-level
v0.12,now print labels in CoNLL format
v0.12,print labels in CoNLL format
v0.12,the multi task model has several labels
v0.12,biomedical models
v0.12,entity linker
v0.12,auto-spawn on GPU if available
v0.12,remap state dict for models serialized with Flair <= 0.11.3
v0.12,English sentiment models
v0.12,Communicative Functions Model
v0.12,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.12,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.12,"Initially, get scores from <start> tag to all other tags"
v0.12,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.12,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.12,Create a tensor to hold accumulated sequence scores at each current tag
v0.12,Create a tensor to hold back-pointers
v0.12,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.12,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.12,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.12,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.12,"If sentence is over, add transition to STOP-tag"
v0.12,Decode/trace best path backwards
v0.12,Sanity check
v0.12,remove start-tag and backscore to stop-tag
v0.12,Max + Softmax to get confidence score for predicted label and append label to each token
v0.12,"Transitions are used in the following way: transitions[to, from]."
v0.12,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.12,to START-tag and from STOP-tag to any other tag to -10000.
v0.12,create a model card for this model with Flair and PyTorch version
v0.12,also record Transformers version if library is loaded
v0.12,remember all parameters used in train() call
v0.12,add model card to model
v0.12,"if optimizer class is passed, instantiate:"
v0.12,"determine what splits (train, dev, test) to evaluate and log"
v0.12,prepare loss logging file and set up header
v0.12,"from here on, use list of learning rates"
v0.12,load existing optimizer state dictionary if it exists
v0.12,"minimize training loss if training with dev data, else maximize dev score"
v0.12,"if scheduler is passed as a class, instantiate"
v0.12,"if we load a checkpoint, we have already trained for epoch"
v0.12,"Determine whether to log ""bad epochs"" information"
v0.12,load existing scheduler state dictionary if it exists
v0.12,update optimizer and scheduler in model card
v0.12,"if training also uses dev/train data, include in training set"
v0.12,initialize sampler if provided
v0.12,init with default values if only class is provided
v0.12,set dataset to sample from
v0.12,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.12,At any point you can hit Ctrl + C to break out of training early.
v0.12,update epoch in model card
v0.12,get new learning rate
v0.12,reload last best model if annealing with restarts is enabled
v0.12,stop training if learning rate becomes too small
v0.12,"if shuffle_first_epoch==False, the first epoch is not shuffled"
v0.12,process mini-batches
v0.12,zero the gradients on the model and optimizer
v0.12,"if necessary, make batch_steps"
v0.12,forward and backward for batch
v0.12,forward pass
v0.12,Backward
v0.12,identify dynamic embeddings (always deleted) on first sentence
v0.12,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12,do the optimizer step
v0.12,do the scheduler step if one-cycle or linear decay
v0.12,get new learning rate
v0.12,evaluate on train / dev / test split depending on training settings
v0.12,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12,calculate scores using dev data if available
v0.12,append dev score to score history
v0.12,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.12,determine if this is the best model or if we need to anneal
v0.12,default mode: anneal against dev score
v0.12,alternative: anneal against dev loss
v0.12,alternative: anneal against train loss
v0.12,determine bad epoch number
v0.12,lr unchanged
v0.12,log bad epochs
v0.12,output log file
v0.12,make headers on first epoch
v0.12,"if checkpoint is enabled, save model at each epoch"
v0.12,Check whether to save best model
v0.12,"if we do not use dev data for model selection, save final model"
v0.12,test best model if test data is present
v0.12,recover all arguments that were used to train this model
v0.12,you can overwrite params with your own
v0.12,surface nested arguments
v0.12,resume training with these parameters
v0.12,"If set, add a factor to the learning rate of all parameters with 'embeddings' not in name"
v0.12,get and return the final test score of best model
v0.12,cast string to Path
v0.12,forward pass
v0.12,update optimizer and scheduler
v0.12,"TextDataset returns a list. valid and test are only one file,"
v0.12,so return the first element
v0.12,cast string to Path
v0.12,error message if the validation dataset is too small
v0.12,Shuffle training files randomly after serially iterating
v0.12,through corpus one
v0.12,"iterate through training data, starting at"
v0.12,self.split (for checkpointing)
v0.12,off by one for printing
v0.12,go into train mode
v0.12,reset variables
v0.12,not really sure what this does
v0.12,do the forward pass in the model
v0.12,try to predict the targets
v0.12,Backward
v0.12,`clip_grad_norm` helps prevent the exploding gradient
v0.12,problem in RNNs / LSTMs.
v0.12,We detach the hidden state from how it was
v0.12,previously produced.
v0.12,"If we didn't, the model would try backpropagating"
v0.12,all the way to start of the dataset.
v0.12,explicitly remove loss to clear up memory
v0.12,#########################################################
v0.12,Save the model if the validation loss is the best we've
v0.12,seen so far.
v0.12,#########################################################
v0.12,print info
v0.12,#########################################################
v0.12,##############################################################################
v0.12,final testing
v0.12,##############################################################################
v0.12,Turn on evaluation mode which disables dropout.
v0.12,Work out how cleanly we can divide the dataset into bsz parts.
v0.12,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.12,Evenly divide the data across the bsz batches.
v0.12,"the default model for ELMo is the 'original' model, which is very large"
v0.12,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.12,put on Cuda if available
v0.12,embed a dummy sentence to determine embedding_length
v0.12,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.12,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.12,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.12,news-english-forward
v0.12,news-english-backward
v0.12,news-english-forward
v0.12,news-english-backward
v0.12,mix-english-forward
v0.12,mix-english-backward
v0.12,mix-german-forward
v0.12,mix-german-backward
v0.12,common crawl Polish forward
v0.12,common crawl Polish backward
v0.12,Slovenian forward
v0.12,Slovenian backward
v0.12,Bulgarian forward
v0.12,Bulgarian backward
v0.12,Dutch forward
v0.12,Dutch backward
v0.12,Swedish forward
v0.12,Swedish backward
v0.12,French forward
v0.12,French backward
v0.12,Czech forward
v0.12,Czech backward
v0.12,Portuguese forward
v0.12,Portuguese backward
v0.12,initialize cache if use_cache set
v0.12,embed a dummy sentence to determine embedding_length
v0.12,set to eval mode
v0.12,Copy the object's state from self.__dict__ which contains
v0.12,all our instance attributes. Always use the dict.copy()
v0.12,method to avoid modifying the original state.
v0.12,Remove the unpicklable entries.
v0.12,"if cache is used, try setting embeddings from cache first"
v0.12,try populating embeddings from cache
v0.12,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.12,get hidden states from language model
v0.12,take first or last hidden states from language model as word representation
v0.12,if self.tokenized_lm or token.whitespace_after:
v0.12,1-camembert-base -> camembert-base
v0.12,1-xlm-roberta-large -> xlm-roberta-large
v0.12,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.12,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.12,tokens are attended to.
v0.12,Zero-pad up to the sequence length.
v0.12,"first, find longest sentence in batch"
v0.12,prepare id maps for BERT model
v0.12,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.12,get aggregated embeddings for each BERT-subtoken in sentence
v0.12,get the current sentence object
v0.12,add concatenated embedding to sentence
v0.12,use first subword embedding if pooling operation is 'first'
v0.12,"otherwise, do a mean over all subwords in token"
v0.12,"if only one sentence is passed, convert to list of sentence"
v0.12,bidirectional LSTM on top of embedding layer
v0.12,dropouts
v0.12,"first, sort sentences by number of tokens"
v0.12,go through each sentence in batch
v0.12,PADDING: pad shorter sentences out
v0.12,ADD TO SENTENCE LIST: add the representation
v0.12,--------------------------------------------------------------------
v0.12,GET REPRESENTATION FOR ENTIRE BATCH
v0.12,--------------------------------------------------------------------
v0.12,--------------------------------------------------------------------
v0.12,FF PART
v0.12,--------------------------------------------------------------------
v0.12,use word dropout if set
v0.12,--------------------------------------------------------------------
v0.12,EXTRACT EMBEDDINGS FROM LSTM
v0.12,--------------------------------------------------------------------
v0.12,embed a dummy sentence to determine embedding_length
v0.12,Avoid conflicts with flair's Token class
v0.12,"legacy pickle-like saving for image embeddings, as implementation details are not obvious"
v0.12,"legacy pickle-like loading for image embeddings, as implementation details are not obvious"
v0.12,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.12,add positional encodings
v0.12,reshape the pixels into the sequence
v0.12,layer norm after convolution and positional encodings
v0.12,add <cls> token
v0.12,"transformer requires input in the shape [h*w+1, b, d]"
v0.12,the output is an embedding of <cls> token
v0.12,this parameter is fixed
v0.12,optional fine-tuning on top of embedding layer
v0.12,"if only one sentence is passed, convert to list of sentence"
v0.12,"if only one sentence is passed, convert to list of sentence"
v0.12,bidirectional RNN on top of embedding layer
v0.12,dropouts
v0.12,TODO: remove in future versions
v0.12,embed words in the sentence
v0.12,before-RNN dropout
v0.12,reproject if set
v0.12,push through RNN
v0.12,after-RNN dropout
v0.12,extract embeddings from RNN
v0.12,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.12,"check if this is the case and if so, set it"
v0.12,serialize the language models and the constructor arguments (but nothing else)
v0.12,re-initialize language model with constructor arguments
v0.12,special handling for deserializing language models
v0.12,copy over state dictionary to self
v0.12,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.12,"in their ""self.train()"" method)"
v0.12,IMPORTANT: add embeddings as torch modules
v0.12,iterate over sentences
v0.12,"if its a forward LM, take last state"
v0.12,"convert to plain strings, embedded in a list for the encode function"
v0.12,CNN
v0.12,dropouts
v0.12,TODO: remove in future versions
v0.12,embed words in the sentence
v0.12,before-RNN dropout
v0.12,reproject if set
v0.12,push CNN
v0.12,after-CNN dropout
v0.12,extract embeddings from CNN
v0.12,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.12,"if only one sentence is passed, convert to list of sentence"
v0.12,Expose base classses
v0.12,Expose document embedding classes
v0.12,Expose image embedding classes
v0.12,Expose legacy embedding classes
v0.12,Expose token embedding classes
v0.12,in some cases we need to insert zero vectors for tokens without embedding.
v0.12,padding
v0.12,remove special markup
v0.12,check if special tokens exist to circumvent error message
v0.12,iterate over subtokens and reconstruct tokens
v0.12,remove special markup
v0.12,check if reconstructed token is special begin token ([CLS] or similar)
v0.12,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.12,"we cannot handle unk_tokens perfectly, so let's assume that one unk_token corresponds to one token."
v0.12,if tokens are unaccounted for
v0.12,check if all tokens were matched to subtokens
v0.12,The layoutlm tokenizer doesn't handle ocr themselves
v0.12,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.12,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.12,in case of doubt: token embedding has higher priority than document embedding
v0.12,random check some tokens to save performance.
v0.12,Models such as FNet do not have an attention_mask
v0.12,set language IDs for XLM-style transformers
v0.12,"word_ids is only supported for fast rust tokenizers. Some models like ""xlm-mlm-ende-1024"" do not have"
v0.12,"a fast tokenizer implementation, hence we need to fall back to our own reconstruction of word_ids."
v0.12,set context if not set already
v0.12,flair specific pre-tokenization
v0.12,"onnx prepares numpy arrays, no mather if it runs on gpu or cpu, the input is on cpu first."
v0.12,temporary fix to disable tokenizer parallelism warning
v0.12,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.12,do not print transformer warnings as these are confusing in this case
v0.12,load tokenizer and transformer model
v0.12,load tokenizer from inmemory zip-file
v0.12,model name
v0.12,embedding parameters
v0.12,send mini-token through to check how many layers the model has
v0.12,return length
v0.12,"If we use a context separator, add a new special token"
v0.12,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.12,"when initializing, embeddings are in eval mode by default"
v0.12,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.12,"cannot run `.encode` if ocr boxes are required, assume"
v0.12,in case of doubt: token embedding has higher priority than document embedding
v0.12,in case of doubt: token embedding has higher priority than document embedding
v0.12,legacy TransformerDocumentEmbedding
v0.12,legacy TransformerTokenEmbedding
v0.12,legacy Flair <= 0.12
v0.12,legacy Flair <= 0.7
v0.12,legacy TransformerTokenEmbedding
v0.12,Legacy TransformerDocumentEmbedding
v0.12,legacy TransformerTokenEmbedding
v0.12,legacy TransformerDocumentEmbedding
v0.12,copy values from new embedding
v0.12,cls first pooling can be done without recreating sentence hidden states
v0.12,make the tuple a tensor; makes working with it easier.
v0.12,"for multimodal models like layoutlmv3, we truncate the image embeddings as they are only used via attention"
v0.12,only use layers that will be outputted
v0.12,this parameter is fixed
v0.12,IMPORTANT: add embeddings as torch modules
v0.12,"if only one sentence is passed, convert to list of sentence"
v0.12,make compatible with serialized models
v0.12,gensim version 4
v0.12,gensim version 3
v0.12,"if no embedding is set, the vocab and embedding length is requried"
v0.12,GLOVE embeddings
v0.12,TURIAN embeddings
v0.12,KOMNINOS embeddings
v0.12,pubmed embeddings
v0.12,FT-CRAWL embeddings
v0.12,FT-CRAWL embeddings
v0.12,twitter embeddings
v0.12,two-letter language code wiki embeddings
v0.12,two-letter language code wiki embeddings
v0.12,two-letter language code crawl embeddings
v0.12,fix serialized models
v0.12,"this is required to force the module on the cpu,"
v0.12,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.12,self.to(..) actually sets the device properly
v0.12,this ignores the get_cached_vec method when loading older versions
v0.12,it is needed for compatibility reasons
v0.12,gensim version 4
v0.12,gensim version 3
v0.12,use list of common characters if none provided
v0.12,translate words in sentence into ints using dictionary
v0.12,"sort words by length, for batching and masking"
v0.12,chars for rnn processing
v0.12,multilingual models
v0.12,English models
v0.12,Arabic
v0.12,Bulgarian
v0.12,Czech
v0.12,Danish
v0.12,German
v0.12,Spanish
v0.12,Basque
v0.12,Persian
v0.12,Finnish
v0.12,French
v0.12,Hebrew
v0.12,Hindi
v0.12,Croatian
v0.12,Indonesian
v0.12,Italian
v0.12,Japanese
v0.12,Malayalam
v0.12,Dutch
v0.12,Norwegian
v0.12,Polish
v0.12,Portuguese
v0.12,Pubmed
v0.12,Slovenian
v0.12,Swedish
v0.12,Tamil
v0.12,Spanish clinical
v0.12,CLEF HIPE Shared task
v0.12,Amharic
v0.12,Ukrainian
v0.12,load model if in pretrained model map
v0.12,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.12,CLEF HIPE models are lowercased
v0.12,embeddings are static if we don't do finetuning
v0.12,embed a dummy sentence to determine embedding_length
v0.12,set to eval mode
v0.12,make compatible with serialized models (TODO: remove)
v0.12,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.12,make compatible with serialized models (TODO: remove)
v0.12,gradients are enable if fine-tuning is enabled
v0.12,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.12,get hidden states from language model
v0.12,take first or last hidden states from language model as word representation
v0.12,offset mode that extracts at whitespace after last character
v0.12,offset mode that extracts at last character
v0.12,use the character language model embeddings as basis
v0.12,length is twice the original character LM embedding length
v0.12,these fields are for the embedding memory
v0.12,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.12,we re-compute embeddings dynamically at each epoch
v0.12,set the memory method
v0.12,memory is wiped each time we do a training run
v0.12,"if we keep a pooling, it needs to be updated continuously"
v0.12,update embedding
v0.12,check token.text is empty or not
v0.12,set aggregation operation
v0.12,add embeddings after updating
v0.12,model architecture
v0.12,model architecture
v0.12,"""pl"","
v0.12,download if necessary
v0.12,load the model
v0.12,"TODO: keep for backwards compatibility, but remove in future"
v0.12,save the sentence piece model as binary file (not as path which may change)
v0.12,write out the binary sentence piece model into the expected directory
v0.12,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.12,"otherwise, use normal process and potentially trigger another download"
v0.12,"once the modes if there, load it with sentence piece"
v0.12,empty words get no embedding
v0.12,all other words get embedded
v0.12,GLOVE embeddings
v0.12,no need to recreate as NILCEmbeddings
v0.12,read in test file if exists
v0.12,read in dev file if exists
v0.12,"find train, dev and test files if not specified"
v0.12,Add tags for each annotated span
v0.12,Remove leading and trailing whitespaces from annotated spans
v0.12,Search start and end token index for current span
v0.12,If end index is not found set to last token
v0.12,Throw error if indices are not valid
v0.12,get train data
v0.12,read in test file if exists
v0.12,read in dev file if exists
v0.12,"find train, dev and test files if not specified"
v0.12,special key for space after
v0.12,special key for feature columns
v0.12,special key for dependency head id
v0.12,"store either Sentence objects in memory, or only file offsets"
v0.12,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.12,determine encoding of text file
v0.12,identify which columns are spans and which are word-level
v0.12,now load all sentences
v0.12,skip first line if to selected
v0.12,option 1: keep Sentence objects in memory
v0.12,pointer to previous
v0.12,parse next sentence
v0.12,quit if last sentence reached
v0.12,skip banned sentences
v0.12,set previous and next sentence for context
v0.12,append parsed sentence to list in memory
v0.12,option 2: keep source data in memory
v0.12,"read lines for next sentence, but don't parse"
v0.12,quit if last sentence reached
v0.12,append raw lines for each sentence
v0.12,we make a distinction between word-level tags and span-level tags
v0.12,read first sentence to determine which columns are span-labels
v0.12,skip first line if to selected
v0.12,check the first 5 sentences
v0.12,go through all annotations and identify word- and span-level annotations
v0.12,- if a column has at least one BIES we know it's a Span label
v0.12,"- if a column has at least one tag that is not BIOES, we know it's a Token label"
v0.12,- problem cases are columns for which we see only O - in this case we default to Span
v0.12,skip assigned columns
v0.12,the space after key is always word-levels
v0.12,"if at least one token has a BIES, we know it's a span label"
v0.12,"if at least one token has a label other than BIOES, we know it's a token label"
v0.12,all remaining columns that are not word-level are span-level
v0.12,for column in self.word_level_tag_columns:
v0.12,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.12,"if sentence ends, break"
v0.12,parse comments if possible
v0.12,"otherwise, this line is a token. parse and add to sentence"
v0.12,check if this sentence is a document boundary
v0.12,add span labels
v0.12,discard tags from tokens that are not added to the sentence
v0.12,parse relations if they are set
v0.12,head and tail span indices are 1-indexed and end index is inclusive
v0.12,parse comments such as '# id cd27886d-6895-4d02-a8df-e5fa763fa88f	domain=de-orcas'
v0.12,"to set the metadata ""domain"" to ""de-orcas"""
v0.12,get fields from line
v0.12,get head_id if exists (only in dependency parses)
v0.12,initialize token
v0.12,go through all columns
v0.12,'feats' and 'misc' column should be split into different fields
v0.12,special handling for whitespace after
v0.12,add each other feature as label-value pair
v0.12,get the task name (e.g. 'ner')
v0.12,get the label value
v0.12,add label
v0.12,remap regular tag names
v0.12,"if in memory, retrieve parsed sentence"
v0.12,else skip to position in file where sentence begins
v0.12,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.12,use all domains
v0.12,iter over all domains / sources and create target files
v0.12,Parameters
v0.12,The conll representation of coref spans allows spans to
v0.12,"overlap. If spans end or begin at the same word, they are"
v0.12,"separated by a ""|""."
v0.12,The span begins at this word.
v0.12,The span begins and ends at this word (single word span).
v0.12,"The span is starting, so we record the index of the word."
v0.12,"The span for this id is ending, but didn't start at this word."
v0.12,Retrieve the start index from the document state and
v0.12,add the span to the clusters for this id.
v0.12,Parameters
v0.12,strip all bracketing information to
v0.12,get the actual propbank label.
v0.12,Entering into a span for a particular semantic role label.
v0.12,We append the label and set the current span for this annotation.
v0.12,"If there's no '(' token, but the current_span_label is not None,"
v0.12,then we are inside a span.
v0.12,We're outside a span.
v0.12,"Exiting a span, so we reset the current span label for this annotation."
v0.12,The words in the sentence.
v0.12,The pos tags of the words in the sentence.
v0.12,the pieces of the parse tree.
v0.12,The lemmatised form of the words in the sentence which
v0.12,have SRL or word sense information.
v0.12,The FrameNet ID of the predicate.
v0.12,"The sense of the word, if available."
v0.12,"The current speaker, if available."
v0.12,"Cluster id -> List of (start_index, end_index) spans."
v0.12,Cluster id -> List of start_indices which are open for this id.
v0.12,Replace brackets in text and pos tags
v0.12,with a different token for parse trees.
v0.12,only keep ')' if there are nested brackets with nothing in them.
v0.12,There are some bad annotations in the CONLL data.
v0.12,"They contain no information, so to make this explicit,"
v0.12,we just set the parse piece to be None which will result
v0.12,in the overall parse tree being None.
v0.12,"If this is the first word in the sentence, create"
v0.12,empty lists to collect the NER and SRL BIO labels.
v0.12,"We can't do this upfront, because we don't know how many"
v0.12,"components we are collecting, as a sentence can have"
v0.12,variable numbers of SRL frames.
v0.12,Create variables representing the current label for each label
v0.12,sequence we are collecting.
v0.12,"If any annotation marks this word as a verb predicate,"
v0.12,we need to record its index. This also has the side effect
v0.12,of ordering the verbal predicates by their location in the
v0.12,"sentence, automatically aligning them with the annotations."
v0.12,"this would not be reached if parse_pieces contained None, hence the cast"
v0.12,Non-empty line. Collect the annotation.
v0.12,Collect any stragglers or files which might not
v0.12,have the '#end document' format for the end of the file.
v0.12,this dataset name
v0.12,check if data there
v0.12,column format
v0.12,this dataset name
v0.12,check if data there
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,download files if not present locally
v0.12,we need to slightly modify the original files by adding some new lines after document separators
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,Remove CoNLL-U meta information in the last column
v0.12,column format
v0.12,dataset name
v0.12,data folder: default dataset folder is the cache root
v0.12,download data if necessary
v0.12,column format
v0.12,dataset name
v0.12,data folder: default dataset folder is the cache root
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,entity_mapping
v0.12,this dataset name
v0.12,download data if necessary
v0.12,data validation
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download files if not present locallys
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,# download zip
v0.12,merge the files in one as the zip is containing multiples files
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,check if data there
v0.12,create folder
v0.12,download dataset
v0.12,column format
v0.12,this dataset name
v0.12,download and parse data if necessary
v0.12,create train test dev if not exist
v0.12,column format
v0.12,this dataset name
v0.12,If the extracted corpus file is not yet present in dir
v0.12,download zip if necessary
v0.12,"extracted corpus is not present , so unpacking it."
v0.12,column format
v0.12,this dataset name
v0.12,download zip
v0.12,unpacking the zip
v0.12,merge the files in one as the zip is containing multiples files
v0.12,column format
v0.12,this dataset name
v0.12,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.12,download files if not present locally
v0.12,we need to modify the original files by adding new lines after after the end of each sentence
v0.12,if only one language is given
v0.12,column format
v0.12,this dataset name
v0.12,"use all languages if explicitly set to ""all"""
v0.12,download data if necessary
v0.12,initialize comlumncorpus and add it to list
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,"For each language in languages, the file is downloaded if not existent"
v0.12,Then a comlumncorpus of that data is created and saved in a list
v0.12,this list is handed to the multicorpus
v0.12,list that contains the columncopora
v0.12,download data if necessary
v0.12,"if language not downloaded yet, download it"
v0.12,create folder
v0.12,get google drive id from list
v0.12,download from google drive
v0.12,unzip
v0.12,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.12,transform data into required format
v0.12,"the processed dataset has the additional ending ""_new"""
v0.12,remove the unprocessed dataset
v0.12,initialize comlumncorpus and add it to list
v0.12,if no languages are given as argument all languages used in XTREME will be loaded
v0.12,if only one language is given
v0.12,column format
v0.12,this dataset name
v0.12,"For each language in languages, the file is downloaded if not existent"
v0.12,Then a comlumncorpus of that data is created and saved in a list
v0.12,This list is handed to the multicorpus
v0.12,list that contains the columncopora
v0.12,download data if necessary
v0.12,"if language not downloaded yet, download it"
v0.12,create folder
v0.12,download from HU Server
v0.12,unzip
v0.12,transform data into required format
v0.12,initialize comlumncorpus and add it to list
v0.12,if only one language is given
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,initialize comlumncorpus and add it to list
v0.12,download data if necessary
v0.12,unpack and write out in CoNLL column-like format
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,data is not in IOB2 format. Thus we transform it to IOB2
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,rename according to train - test - dev - convention
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,Add missing newline after header
v0.12,Workaround for empty tokens
v0.12,"Add ""real"" document marker"
v0.12,Dataset split mapping
v0.12,v2.0 only adds new language and splits for AJMC dataset
v0.12,Special document marker for sample splits in AJMC dataset
v0.12,column format
v0.12,this dataset name
v0.12,download data if necessary
v0.12,column format
v0.12,this dataset name
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download and parse data if necessary
v0.12,paths to train and test splits
v0.12,init corpus
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download and parse data if necessary
v0.12,iterate over all html files
v0.12,"get rid of html syntax, we only need the text"
v0.12,between all documents we write a separator symbol
v0.12,skip empty strings
v0.12,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.12,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.12,sentence splitting and tokenization
v0.12,iterate through all annotations and add to corresponding tokens
v0.12,find sentence to which annotation belongs
v0.12,position within corresponding sentence
v0.12,set annotation for tokens of entity mention
v0.12,write to out-file in column format
v0.12,"in case something goes wrong, delete the dataset and raise error"
v0.12,this dataset name
v0.12,download and parse data if necessary
v0.12,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.12,generate qid wikiname dictionaries
v0.12,merge dictionaries
v0.12,ignore first line
v0.12,commented and empty lines
v0.12,read all Q-IDs
v0.12,ignore first line
v0.12,request
v0.12,this dataset name
v0.12,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.12,like this we can quickly check if the corresponding page exists
v0.12,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.12,delete unprocessed file
v0.12,collect all wikiids
v0.12,create the dictionary
v0.12,request
v0.12,this dataset name
v0.12,names of raw text documents
v0.12,open output_file
v0.12,iterate through all documents
v0.12,split sentences and tokenize
v0.12,iterate through all annotations and add to corresponding tokens
v0.12,find sentence to which annotation belongs
v0.12,position within corresponding sentence
v0.12,set annotation for tokens of entity mention
v0.12,write to out file
v0.12,this dataset name
v0.12,download and parse data if necessary
v0.12,this dataset name
v0.12,download and parse data if necessary
v0.12,First parse the post titles
v0.12,Keep track of how many and which entity mentions does a given post title have
v0.12,Check if the current post title has an entity link and parse accordingly
v0.12,Post titles with entity mentions (if any) are handled via this function
v0.12,Then parse the comments
v0.12,"Iterate over the comments.tsv file, until the end is reached"
v0.12,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.12,Each comment thread is handled as one 'document'.
v0.12,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.12,This if-condition is needed to handle this problem.
v0.12,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.12,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.12,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.12,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.12,and not just single letters into single rows.
v0.12,If there are annotated entity mentions for given post title or a comment thread
v0.12,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.12,Write the token with a corresponding tag to file
v0.12,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.12,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.12,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.12,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.12,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.12,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.12,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.12,Check if further annotations belong to the current post title or comment thread as well
v0.12,Stop when the end of an annotation file is reached
v0.12,Check if further annotations belong to the current sentence as well
v0.12,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.12,Docstart
v0.12,if there is more than one word in the chunk we write each in a separate line
v0.12,print(chunks)
v0.12,empty line after each sentence
v0.12,convert the file to CoNLL
v0.12,this dataset name
v0.12,"check if data there, if not, download the data"
v0.12,create folder
v0.12,download data
v0.12,transform data into column format if necessary
v0.12,if no filenames are specified we use all the data
v0.12,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.12,also we remove 'raganato_ALL' from filenames in case its in the list
v0.12,generate the test file
v0.12,make column file and save to data_folder
v0.12,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12,create folder
v0.12,download data
v0.12,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12,create folder
v0.12,download data
v0.12,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12,generate the test file
v0.12,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12,create folder
v0.12,download data
v0.12,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12,generate the test file
v0.12,default dataset folder is the cache root
v0.12,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12,create folder
v0.12,download data
v0.12,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12,generate the test file
v0.12,default dataset folder is the cache root
v0.12,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12,create folder
v0.12,download data
v0.12,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12,generate the test file
v0.12,default dataset folder is the cache root
v0.12,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.12,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.12,create folder
v0.12,download data
v0.12,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.12,generate the test file
v0.12,this dataset name
v0.12,download data if necessary
v0.12,if True:
v0.12,write CoNLL-U Plus header
v0.12,"Some special cases (e.g., missing spaces before entity marker)"
v0.12,necessary if text should be whitespace tokenizeable
v0.12,Handle case where tail may occur before the head
v0.12,this dataset name
v0.12,write CoNLL-U Plus header
v0.12,this dataset name
v0.12,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.12,download data if necessary
v0.12,write CoNLL-U Plus header
v0.12,The span has ended.
v0.12,We are entering a new span; reset indices
v0.12,and active tag to new span.
v0.12,We're inside a span.
v0.12,Last token might have been a part of a valid span.
v0.12,this dataset name
v0.12,write CoNLL-U Plus header
v0.12,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.12,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.12,target_file_path = Path(data_folder) / target_filename
v0.12,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.12,# write CoNLL-U Plus header
v0.12,"target_file.write(""# global.columns = id form ner\n"")"
v0.12,for example in json.load(source_file):
v0.12,token_list = self._tacred_example_to_token_list(example)
v0.12,target_file.write(token_list.serialize())
v0.12,check if first tag row is already occupied
v0.12,"if first tag row is occupied, use second tag row"
v0.12,hardcoded mapping TODO: perhaps find nicer solution
v0.12,remap regular tag names
v0.12,else skip to position in file where sentence begins
v0.12,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.12,read in dev file if exists
v0.12,read in test file if exists
v0.12,the url is copied from https://huggingface.co/datasets/darentang/sroie/blob/main/sroie.py#L44
v0.12,"find train, dev and test files if not specified"
v0.12,use test_file to create test split if available
v0.12,use dev_file to create test split if available
v0.12,"if data point contains black-listed label, do not use"
v0.12,first check if valid sentence
v0.12,"if so, add to indices"
v0.12,"find train, dev and test files if not specified"
v0.12,variables
v0.12,different handling of in_memory data than streaming data
v0.12,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.12,test if format is OK
v0.12,test if at least one label given
v0.12,make sentence from text (and filter for length)
v0.12,"if a pair column is defined, make a sentence pair object"
v0.12,noinspection PyDefaultArgument
v0.12,dataset name includes the split size
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download each of the 28 splits
v0.12,create dataset directory if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,handle labels file
v0.12,handle data file
v0.12,Create flair compatible labels
v0.12,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,create dataset directory if necessary
v0.12,create train.txt file from CSV
v0.12,create test.txt file from CSV
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,create dataset directory if necessary
v0.12,create train.txt file by iterating over pos and neg file
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,create dataset directory if necessary
v0.12,create train.txt file by iterating over pos and neg file
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,create dataset directory if necessary
v0.12,create train.txt file by iterating over pos and neg file
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,create dataset directory if necessary
v0.12,create train.txt file by iterating over pos and neg file
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,create train dev and test files in fasttext format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download senteval datasets if necessary und unzip
v0.12,convert to FastText format
v0.12,download data if necessary
v0.12,"if data is not downloaded yet, download it"
v0.12,get the zip file
v0.12,move original .tsv files to another folder
v0.12,create train and dev splits in fasttext format
v0.12,create eval_dataset file with no labels
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,download datasets if necessary
v0.12,create dataset directory if necessary
v0.12,create correctly formated txt files
v0.12,multiple labels are possible
v0.12,this dataset name
v0.12,download data if necessary
v0.12,Create flair compatible labels
v0.12,TREC-6 : NUM:dist -> __label__NUM
v0.12,TREC-50: NUM:dist -> __label__NUM:dist
v0.12,this dataset name
v0.12,download data if necessary
v0.12,Create flair compatible labels
v0.12,TREC-6 : NUM:dist -> __label__NUM
v0.12,TREC-50: NUM:dist -> __label__NUM:dist
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,create a separate directory for different tasks
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,check if dataset is supported
v0.12,set file names
v0.12,set file names
v0.12,download and unzip in file structure if necessary
v0.12,instantiate corpus
v0.12,"find train, dev and test files if not specified"
v0.12,"create DataPairDataset for train, test and dev file, if they are given"
v0.12,stop if file does not exist
v0.12,create a DataPair object from strings
v0.12,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.12,"if data is not downloaded yet, download it"
v0.12,get the zip file
v0.12,"rename test file to eval_dataset, since it has no labels"
v0.12,"if data is not downloaded yet, download it"
v0.12,get the zip file
v0.12,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.12,dev sets include 5 different annotations but we will only keep the gold label
v0.12,"rename test file to eval_dataset, since it has no labels"
v0.12,"if data is not downloaded yet, download it"
v0.12,get test and dev sets
v0.12,"if data is not downloaded yet, download it"
v0.12,get the zip file
v0.12,"rename test file to eval_dataset, since it has no labels"
v0.12,"if data is not downloaded yet, download it"
v0.12,get the zip file
v0.12,"rename test file to eval_dataset, since it has no labels"
v0.12,"if data is not downloaded yet, download it"
v0.12,get the zip file
v0.12,"rename test file to eval_dataset, since it has no labels"
v0.12,"if data not downloaded yet, download it"
v0.12,get the zip file
v0.12,"the downloaded files have json format, we transform them to tsv"
v0.12,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.12,remove json file
v0.12,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.12,with sum of all entity lengths as secondary key
v0.12,calculate offset without current text
v0.12,because we stick all passages of a document together
v0.12,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.12,Try to fix incorrect annotations
v0.12,print(
v0.12,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.12,)
v0.12,Ignore empty lines or relation annotations
v0.12,FIX annotation of whitespaces (necessary for PDR)
v0.12,One token may contain multiple entities -> deque all of them
v0.12,column format
v0.12,this dataset name
v0.12,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.12,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,Edge case: last token starts a new entity
v0.12,Last document in file
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,In the huner split files there is no information whether a given id originates
v0.12,from the train or test file of the original corpus - so we have to adapt corpus
v0.12,splitting here
v0.12,In the huner split files there is no information whether a given id originates
v0.12,from the train or test file of the original corpus - so we have to adapt corpus
v0.12,splitting here
v0.12,In the huner split files there is no information whether a given id originates
v0.12,from the train or test file of the original corpus - so we have to adapt corpus
v0.12,splitting here
v0.12,Edge case: last token starts a new entity
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,Read texts
v0.12,Read annotations
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,We need to apply a patch to correct the original training file
v0.12,Articles title
v0.12,Article abstract
v0.12,Entity annotations
v0.12,column format
v0.12,this dataset name
v0.12,Edge case: last token starts a new entity
v0.12,Map all entities to chemicals
v0.12,Map all entities to disease
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,Incomplete article
v0.12,Invalid XML syntax
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,if len(mid) != 3:
v0.12,continue
v0.12,Try to fix entity offsets
v0.12,column format
v0.12,this dataset name
v0.12,There is still one illegal annotation in the file ..
v0.12,column format
v0.12,this dataset name
v0.12,"Abstract first, title second to prevent issues with sentence splitting"
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,"Filter for specific entity types, by default no entities will be filtered"
v0.12,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.12,train and dev split of V2 will be train in V4
v0.12,test split of V2 will be dev in V4
v0.12,New documents in V4 will become test documents
v0.12,column format
v0.12,this dataset name
v0.12,column format
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,cache Feidegger config file
v0.12,cache Feidegger images
v0.12,replace image URL with local cached file
v0.12,append Sentence-Image data point
v0.12,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.12,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.12,"if so, num_workers is set to 0 for faster processing"
v0.12,cast to list if necessary
v0.12,cast to list if necessary
v0.12,"first, check if pymongo is installed"
v0.12,automatically identify train / test / dev files
v0.12,"if no test file is found, take any file with 'test' in name"
v0.12,Expose base classses
v0.12,Expose all biomedical data sets used for the evaluation of BioBERT
v0.12,-
v0.12,-
v0.12,-
v0.12,-
v0.12,Expose all biomedical data sets using the HUNER splits
v0.12,Expose all biomedical data sets
v0.12,Expose all document classification datasets
v0.12,word sense disambiguation
v0.12,Expose all entity linking datasets
v0.12,Expose all relation extraction datasets
v0.12,universal proposition banks
v0.12,keyphrase detection datasets
v0.12,other NER datasets
v0.12,standard NER datasets
v0.12,Expose all sequence labeling datasets
v0.12,Expose all text-image datasets
v0.12,Expose all text-text datasets
v0.12,Expose all treebanks
v0.12,"find train, dev and test files if not specified"
v0.12,get train data
v0.12,get test data
v0.12,get dev data
v0.12,option 1: read only sentence boundaries as offset positions
v0.12,option 2: keep everything in memory
v0.12,"if in memory, retrieve parsed sentence"
v0.12,else skip to position in file where sentence begins
v0.12,current token ID
v0.12,handling for the awful UD multiword format
v0.12,end of sentence
v0.12,comments
v0.12,ellipsis
v0.12,if token is a multi-word
v0.12,normal single-word tokens
v0.12,"if we don't split multiwords, skip over component words"
v0.12,add token
v0.12,add morphological tags
v0.12,derive whitespace logic for multiwords
v0.12,print(token)
v0.12,print(current_multiword_last_token)
v0.12,print(current_multiword_first_token)
v0.12,"if multi-word equals component tokens, there should be no whitespace"
v0.12,go through all tokens in subword and set whitespace_after information
v0.12,print(i)
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,this dataset name
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,download data if necessary
v0.12,this dataset name
v0.12,default dataset folder is the cache root
v0.12,download data if necessary
v0.12,"finally, print model card for information"
v0.12,test corpus
v0.12,create a TARS classifier
v0.12,check if right number of classes
v0.12,switch to task with only one label
v0.12,check if right number of classes
v0.12,switch to task with three labels provided as list
v0.12,check if right number of classes
v0.12,switch to task with four labels provided as set
v0.12,check if right number of classes
v0.12,switch to task with two labels provided as Dictionary
v0.12,check if right number of classes
v0.12,test corpus
v0.12,create a TARS classifier
v0.12,switch to a new task (TARS can do multiple tasks so you must define one)
v0.12,initialize the text classifier trainer
v0.12,start the training
v0.12,"With end symbol, without start symbol, padding in front"
v0.12,"Without end symbol, with start symbol, padding in back"
v0.12,"Without end symbol, without start symbol, padding in front"
v0.12,initialize trainer
v0.12,initialize trainer
v0.12,train model for 2 epochs
v0.12,load the checkpoint model and train until epoch 4
v0.12,clean up results directory
v0.12,initialize trainer
v0.12,initialize trainer
v0.12,increment for last token in sentence if not followed by whitespace
v0.12,clean up directory
v0.12,clean up directory
v0.12,example sentence
v0.12,set 4 labels for 2 tokens ('love' is tagged twice)
v0.12,check if there are three POS labels with correct text and values
v0.12,check if there are is one SENTIMENT label with correct text and values
v0.12,check if all tokens are correctly labeled
v0.12,remove the pos label from the last word
v0.12,there should be 2 POS labels left
v0.12,now remove all pos tags
v0.12,set 3 labels for 2 spans (HU is tagged twice)
v0.12,check if there are three labels with correct text and values
v0.12,check if there are two spans with correct text and values
v0.12,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.12,should be only one NER label left
v0.12,and only one NER span
v0.12,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.12,check if there are three labels with correct text and values
v0.12,check if there are two spans with correct text and values
v0.12,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.12,should be only one NER label left
v0.12,and only one NER span
v0.12,but there is also one orgtype span and label
v0.12,and only one NER span
v0.12,let's add the NER tag back
v0.12,check if there are three labels with correct text and values
v0.12,check if there are two spans with correct text and values
v0.12,now remove all NER tags
v0.12,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.12,create two relation label
v0.12,there should be two relation labels
v0.12,there should be one syntactic labels
v0.12,"there should be two relations, one with two and one with one label"
v0.12,example sentence
v0.12,add another topic label
v0.12,example sentence
v0.12,has sentiment value
v0.12,has 4 part of speech tags
v0.12,has 1 NER tag
v0.12,should be in total 6 labels
v0.12,example sentence
v0.12,add two NER labels
v0.12,get the four labels
v0.12,check that only two of the respective data points are equal
v0.12,make a sentence and some right context
v0.12,TODO: is this desirable? Or should two sentences with same text still be considered different objects?
v0.12,Initializing a Sentence this way assumes that there is a space after each token
v0.12,get default dictionary
v0.12,init forward LM with 128 hidden states and 1 layer
v0.12,get the example corpus and process at character level in forward direction
v0.12,train the language model
v0.12,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.12,clean up results directory
v0.12,get default dictionary
v0.12,init forward LM with 128 hidden states and 1 layer
v0.12,get the example corpus and process at character level in forward direction
v0.12,train the language model
v0.12,define search space
v0.12,sequence tagger parameter
v0.12,model trainer parameter
v0.12,training parameter
v0.12,find best parameter settings
v0.12,clean up results directory
v0.12,document embeddings parameter
v0.12,training parameter
v0.12,clean up results directory
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,load column dataset with one entry
v0.12,load column dataset with two entries
v0.12,load column dataset with three entries
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,check if Token labels are correct
v0.12,"get training, test and dev data"
v0.12,check if Token labels for frames are correct
v0.12,"get training, test and dev data"
v0.12,"get training, test and dev data"
v0.12,get two corpora as one
v0.12,"get training, test and dev data for full English UD corpus from web"
v0.12,clean up data directory
v0.12,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.12,"""2"","
v0.12,"""0"","
v0.12,"""4"","
v0.12,"""2"","
v0.12,"""2"","
v0.12,"""2"","
v0.12,]
v0.12,"Here, we use the default token annotation fields."
v0.12,"We have manually checked, that these numbers are correct:"
v0.12,"+1 offset, because of missing EOS marker at EOD"
v0.12,Test data for v2.1 release
v0.12,--- Embeddings that are shared by both models --- #
v0.12,--- Task 1: Sentiment Analysis (5-class) --- #
v0.12,Define corpus and model
v0.12,-- Task 2: Binary Sentiment Analysis on Customer Reviews -- #
v0.12,Define corpus and model
v0.12,-- Define mapping (which tagger should train on which model) -- #
v0.12,-- Create model trainer and train -- #
v0.12,load dataset
v0.12,tagger without CRF
v0.12,train
v0.12,check if loaded model can predict
v0.12,check if loaded model successfully fit the training data
v0.12,load dataset
v0.12,tagger without CRF
v0.12,train
v0.12,check if loaded model can predict
v0.12,check if loaded model successfully fit the training data
v0.12,load dataset
v0.12,tagger without CRF
v0.12,train
v0.12,check if loaded model can predict
v0.12,check if loaded model successfully fit the training data
v0.12,load dataset
v0.12,tagger without CRF
v0.12,train
v0.12,check if loaded model can predict
v0.12,check if loaded model successfully fit the training data
v0.12,check if model can predict
v0.12,load model
v0.12,chcek if model predicts correct label
v0.12,check if loaded model successfully fit the training data
v0.12,check if model can predict
v0.12,load model
v0.12,chcek if model predicts correct label
v0.12,check if loaded model successfully fit the training data
v0.12,check if model can predict
v0.12,load model
v0.12,chcek if model predicts correct label
v0.12,check if loaded model successfully fit the training data
v0.12,clean up file
v0.12,no need for label_dict
v0.12,check if right number of classes
v0.12,switch to task with only one label
v0.12,check if right number of classes
v0.12,switch to task with three labels provided as list
v0.12,check if right number of classes
v0.12,switch to task with four labels provided as set
v0.12,check if right number of classes
v0.12,switch to task with two labels provided as Dictionary
v0.12,check if right number of classes
v0.12,Intel ----founded_by---> Gordon Moore
v0.12,Intel ----founded_by---> Robert Noyce
v0.12,Check sentence masking and relation label annotation on
v0.12,"training, validation and test dataset (in this test the splits are the same)"
v0.12,"Entity pair permutations of: ""Larry Page and Sergey Brin founded Google ."""
v0.12,"Entity pair permutations of: ""Microsoft was founded by Bill Gates ."""
v0.12,"Entity pair permutations of: ""Konrad Zuse was born in Berlin on 22 June 1910 ."""
v0.12,"Entity pair permutations of: ""Joseph Weizenbaum , a professor at MIT , was born in Berlin , Germany."""
v0.12,This sentence is only included if we transform the corpus with cross augmentation
v0.12,Ensure this is an example that predicts no classes in multilabel
v0.12,check if right number of classes
v0.12,switch to task with only one label
v0.12,check if right number of classes
v0.12,switch to task with three labels provided as list
v0.12,check if right number of classes
v0.12,switch to task with four labels provided as set
v0.12,check if right number of classes
v0.12,switch to task with two labels provided as Dictionary
v0.12,check if right number of classes
v0.12,ensure that the prepared tensors is what we expect
v0.12,use a SequenceTagger to save and reload the embedding in the manner it is supposed to work
v0.12,previous and next sentence as context
v0.12,test expansion for sentence without context
v0.12,test expansion for with previous and next as context
v0.12,test expansion if first sentence is document boundary
v0.12,test expansion if we don't use context
v0.12,"apparently the precision is not that high on cuda, hence the absolute tolerance needs to be higher."
v0.12,dummy model with embeddings
v0.12,save the dummy and load it again
v0.12,check that context_length and use_context_separator is the same for both
v0.11,mmap seems to be much more memory efficient
v0.11,Remove quotes from etag
v0.11,"If there is an etag, it's everything after the first period"
v0.11,"Otherwise, use None"
v0.11,"URL, so get it from the cache (downloading if necessary)"
v0.11,"File, and it exists."
v0.11,"File, but it doesn't exist."
v0.11,Something unknown
v0.11,Extract all the contents of zip file in current directory
v0.11,Extract all the contents of zip file in current directory
v0.11,get cache path to put the file
v0.11,"Download to temporary file, then copy to cache dir once finished."
v0.11,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.11,GET file object
v0.11,TODO(joelgrus): do we want to do checksums or anything like that?
v0.11,get cache path to put the file
v0.11,make HEAD request to check ETag
v0.11,add ETag to filename if it exists
v0.11,"etag = response.headers.get(""ETag"")"
v0.11,"Download to temporary file, then copy to cache dir once finished."
v0.11,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.11,GET file object
v0.11,These defaults are the same as the argument defaults in tqdm.
v0.11,first determine the distribution of classes in the dataset
v0.11,weight for each sample
v0.11,Create blocks
v0.11,shuffle the blocks
v0.11,concatenate the shuffled blocks
v0.11,Create blocks
v0.11,shuffle the blocks
v0.11,concatenate the shuffled blocks
v0.11,increment for last token in sentence if not followed by whitespace
v0.11,this is the default init size of a lmdb database for embeddings
v0.11,get db filename from embedding name
v0.11,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.11,SequenceTagger
v0.11,TextClassifier
v0.11,get db filename from embedding name
v0.11,if embedding database already exists
v0.11,"otherwise, push embedding to database"
v0.11,if embedding database already exists
v0.11,open the database in read mode
v0.11,we need to set self.k
v0.11,create and load the database in write mode
v0.11,"no idea why, but we need to close and reopen the environment to avoid"
v0.11,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.11,when opening new transaction !
v0.11,init dictionaries
v0.11,"in order to deal with unknown tokens, add <unk>"
v0.11,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.11,set 'add_unk' depending on whether <unk> is a key
v0.11,TODO: does it make sense to exclude labels? Two data points of identical text (but different labels)
v0.11,would be equal now.
v0.11,labels also need to be deleted at Sentence object
v0.11,delete labels at object itself
v0.11,private field for all known spans
v0.11,the tokenizer used for this sentence
v0.11,"if text is passed, instantiate sentence with tokens (words)"
v0.11,determine token positions and whitespace_after flag
v0.11,the last token has no whitespace after
v0.11,log a warning if the dataset is empty
v0.11,some sentences represent a document boundary (but most do not)
v0.11,internal variables to denote position inside dataset
v0.11,data with zero-width characters cannot be handled
v0.11,set token idx and sentence
v0.11,append token to sentence
v0.11,register token annotations on sentence
v0.11,move sentence embeddings to device
v0.11,also move token embeddings to device
v0.11,clear token embeddings
v0.11,infer whitespace after field
v0.11,No character at the corresponding code point: remove it
v0.11,"if no label if specified, return all labels"
v0.11,"if the label type exists in the Sentence, return it"
v0.11,return empty list if none of the above
v0.11,labels also need to be deleted at all tokens
v0.11,labels also need to be deleted at all known spans
v0.11,remove spans without labels
v0.11,delete labels at object itself
v0.11,set name
v0.11,abort if no data is provided
v0.11,sample test data from train if none is provided
v0.11,sample dev data from train if none is provided
v0.11,set train dev and test data
v0.11,find out empty sentence indices
v0.11,create subset of non-empty sentence indices
v0.11,find out empty sentence indices
v0.11,create subset of non-empty sentence indices
v0.11,count all label types per sentence
v0.11,go through all labels of label_type and count values
v0.11,check if there are any span labels
v0.11,"if an unk threshold is set, UNK all label values below this threshold"
v0.11,Make the tag dictionary
v0.11,global variable: cache_root
v0.11,global variable: device
v0.11,global variable: version
v0.11,global variable: arrow symbol
v0.11,dummy return to fulfill trainer.train() needs
v0.11,print(vec)
v0.11,Attach optimizer
v0.11,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.11,if memory mode option 'none' delete everything
v0.11,"if dynamic embedding keys not passed, identify them automatically"
v0.11,always delete dynamic embeddings
v0.11,"if storage mode is ""cpu"", send everything to CPU (pin to memory if we train on GPU)"
v0.11,optional metric space decoder if prototypes have different length than embedding
v0.11,create initial prototypes for all classes (all initial prototypes are a vector of all 1s)
v0.11,"if set, create initial prototypes from normal distribution"
v0.11,"if set, use a radius"
v0.11,all parameters will be pushed internally to the specified device
v0.11,decode embeddings into prototype space
v0.11,"if unlabeled distance is set, mask out loss to unlabeled class prototype"
v0.11,Monkey-patching is problematic for mypy (https://github.com/python/mypy/issues/2427)
v0.11,gradients are not required for prototype computation
v0.11,reset prototypes for all classes
v0.11,decode embeddings into prototype space
v0.11,embeddings need to be removed so that memory doesn't fill up
v0.11,TODO: changes required
v0.11,"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
v0.11,"write out a ""model card"" if one is set"
v0.11,special handling for optimizer:
v0.11,remember optimizer class and state dictionary
v0.11,save model
v0.11,restore optimizer and scheduler to model card if set
v0.11,load_big_file is a workaround byhttps://github.com/highway11git
v0.11,to load models on some Mac/Windows setups
v0.11,see https://github.com/zalandoresearch/flair/issues/351
v0.11,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.11,loss calculation
v0.11,variables for printing
v0.11,variables for computing scores
v0.11,remove any previously predicted labels
v0.11,predict for batch
v0.11,get the gold labels
v0.11,add to all_predicted_values
v0.11,make printout lines
v0.11,convert true and predicted values to two span-aligned lists
v0.11,delete exluded labels if exclude_labels is given
v0.11,"if after excluding labels, no label is left, ignore the datapoint"
v0.11,write all_predicted_values to out_file if set
v0.11,make the evaluation dictionary
v0.11,check if this is a multi-label problem
v0.11,compute numbers by formatting true and predicted such that Scikit-Learn can use them
v0.11,multi-label problems require a multi-hot vector for each true and predicted label
v0.11,single-label problems can do with a single index for each true and predicted label
v0.11,"now, calculate evaluation numbers"
v0.11,there is at least one gold label or one prediction (default)
v0.11,"if there is only one label, then ""micro avg"" = ""macro avg"""
v0.11,"micro average is only computed if zero-label exists (for instance ""O"")"
v0.11,if no zero-label exists (such as in POS tagging) micro average is equal to accuracy
v0.11,same for the main score
v0.11,issue error and default all evaluation numbers to 0.
v0.11,line for log file
v0.11,check if there is a label mismatch
v0.11,print info
v0.11,initialize the label dictionary
v0.11,initialize the decoder
v0.11,set up multi-label logic
v0.11,init dropouts
v0.11,loss weights and loss function
v0.11,Initialize the weight tensor
v0.11,make a forward pass to produce embedded data points and labels
v0.11,no loss can be calculated if there are no labels
v0.11,use dropout
v0.11,push embedded_data_points through decoder to get the scores
v0.11,calculate the loss
v0.11,filter empty sentences
v0.11,reverse sort all sequences by their length
v0.11,progress bar for verbosity
v0.11,stop if all sentences are empty
v0.11,if anything could possibly be predicted
v0.11,remove previously predicted labels of this type
v0.11,add DefaultClassifier arguments
v0.11,add variables of DefaultClassifier
v0.11,Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py#L23
v0.11,Get projected 1st dimension
v0.11,Compute bilinear form
v0.11,Arcosh
v0.11,Project the input data to n+1 dimensions
v0.11,"The first dimension, is recomputed in the distance module"
v0.11,header for 'weights.txt'
v0.11,"determine the column index of loss, f-score and accuracy for"
v0.11,"train, dev and test split"
v0.11,then get all relevant values from the tsv
v0.11,then get all relevant values from the tsv
v0.11,plot i
v0.11,save plots
v0.11,save plots
v0.11,plt.show()
v0.11,save plot
v0.11,take the average over the last three scores of training
v0.11,take average over the scores from the different training runs
v0.11,auto-spawn on GPU if available
v0.11,progress bar for verbosity
v0.11,stop if all sentences are empty
v0.11,clearing token embeddings to save memory
v0.11,"read Dataset into data loader, if list of sentences passed, make Dataset first"
v0.11,TODO: not saving lines yet
v0.11,== similarity measures ==
v0.11,helper class for ModelSimilarity
v0.11,-- works with binary cross entropy loss --
v0.11,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.11,-- works with ranking/triplet loss --
v0.11,normalize the embeddings
v0.11,== similarity losses ==
v0.11,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.11,TODO: this assumes eye matrix
v0.11,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.11,== similarity learner ==
v0.11,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.11,assumes that for each data pair there's at least one embedding per modality
v0.11,pre-compute embeddings for all targets in evaluation dataset
v0.11,compute the similarity
v0.11,sort the similarity matrix across modality 1
v0.11,"get the ranks, so +1 to start counting ranks from 1"
v0.11,The conversion from old model's constructor interface
v0.11,auto-spawn on GPU if available
v0.11,pad strings with whitespaces to longest sentence
v0.11,cut up the input into chunks of max charlength = chunk_size
v0.11,push each chunk through the RNN language model
v0.11,concatenate all chunks to make final output
v0.11,initial hidden state
v0.11,get predicted weights
v0.11,divide by temperature
v0.11,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.11,this makes a vector in which the largest value is 0
v0.11,compute word weights with exponential function
v0.11,try sampling multinomial distribution for next character
v0.11,print(word_idx)
v0.11,input ids
v0.11,push list of character IDs through model
v0.11,the target is always the next character
v0.11,use cross entropy loss to compare output of forward pass with targets
v0.11,exponentiate cross-entropy loss to calculate perplexity
v0.11,"""document_delimiter"" property may be missing in some older pre-trained models"
v0.11,serialize the language models and the constructor arguments (but nothing else)
v0.11,special handling for deserializing language models
v0.11,re-initialize language model with constructor arguments
v0.11,copy over state dictionary to self
v0.11,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.11,"in their ""self.train()"" method)"
v0.11,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.11,"check if this is the case and if so, set it"
v0.11,Transform input data into TARS format
v0.11,"if there are no labels, return a random sample as negatives"
v0.11,"otherwise, go through all labels"
v0.11,make sure the probabilities always sum up to 1
v0.11,get and embed all labels by making a Sentence object that contains only the label text
v0.11,get each label embedding and scale between 0 and 1
v0.11,compute similarity matrix
v0.11,"the higher the similarity, the greater the chance that a label is"
v0.11,sampled as negative example
v0.11,make label dictionary if no Dictionary object is passed
v0.11,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.11,check if candidate_label_set is empty
v0.11,make list if only one candidate label is passed
v0.11,create label dictionary
v0.11,note current task
v0.11,create a temporary task
v0.11,make zero shot predictions
v0.11,switch to the pre-existing task
v0.11,prepare TARS dictionary
v0.11,initialize a bare-bones sequence tagger
v0.11,transformer separator
v0.11,Store task specific labels since TARS can handle multiple tasks
v0.11,make a tars sentence where all labels are O by default
v0.11,init new TARS classifier
v0.11,set all task information
v0.11,return
v0.11,with torch.no_grad():
v0.11,progress bar for verbosity
v0.11,stop if all sentences are empty
v0.11,go through each sentence in the batch
v0.11,always remove tags first
v0.11,get the span and its label
v0.11,"label = span.get_labels(""tars_temp_label"")[0].value"
v0.11,determine whether tokens in this span already have a label
v0.11,only add if all tokens have no label
v0.11,clearing token embeddings to save memory
v0.11,"all labels default to ""O"""
v0.11,set gold token-level
v0.11,set predicted token-level
v0.11,now print labels in CoNLL format
v0.11,prepare TARS dictionary
v0.11,initialize a bare-bones sequence tagger
v0.11,transformer separator
v0.11,Store task specific labels since TARS can handle multiple tasks
v0.11,init new TARS classifier
v0.11,set all task information
v0.11,with torch.no_grad():
v0.11,set context if not set already
v0.11,progress bar for verbosity
v0.11,stop if all sentences are empty
v0.11,go through each sentence in the batch
v0.11,always remove tags first
v0.11,add all labels that according to TARS match the text and are above threshold
v0.11,do not add labels below confidence threshold
v0.11,only use label with highest confidence if enforcing single-label predictions
v0.11,get all label scores and do an argmax to get the best label
v0.11,remove previously added labels and only add the best label
v0.11,clearing token embeddings to save memory
v0.11,set separator to concatenate two sentences
v0.11,auto-spawn on GPU if available
v0.11,pooling operation to get embeddings for entites
v0.11,set embeddings
v0.11,set relation and entity label types
v0.11,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.11,super lame: make dictionary to find relation annotations for a given entity pair
v0.11,get all entity spans
v0.11,"go through cross product of entities, for each pair concat embeddings"
v0.11,filter entity pairs according to their tags if set
v0.11,get gold label for this relation (if one exists)
v0.11,"if there is no gold label for this entity pair, set to 'O' (no relation)"
v0.11,"if predicting, also remember sentences and label candidates"
v0.11,if there's at least one entity pair in the sentence
v0.11,embed sentences and get embeddings for each entity pair
v0.11,get embeddings
v0.11,whether to encode characters and whether to use attention (attention can only be used if chars are encoded)
v0.11,character dictionary for decoding and encoding
v0.11,make sure <unk> is in dictionary for handling of unknown characters
v0.11,add special symbols to dictionary if necessary and save respective indices
v0.11,---- ENCODER ----
v0.11,encoder character embeddings
v0.11,encoder pre-trained embeddings
v0.11,encoder RNN
v0.11,additional encoder linear layer if bidirectional encoding
v0.11,---- DECODER ----
v0.11,decoder: linear layers to transform vectors to and from alphabet_size
v0.11,when using attention we concatenate attention outcome and decoder hidden states
v0.11,decoder RNN
v0.11,loss and softmax
v0.11,self.unreduced_loss = nn.CrossEntropyLoss(reduction='none')  # for prediction
v0.11,add additional columns for special symbols if necessary
v0.11,initialize with dummy symbols
v0.11,encode inputs
v0.11,get labels (we assume each token has a lemma label)
v0.11,get char indices for labels of sentence
v0.11,"(batch_size, max_sequence_length) batch_size = #words in sentence,"
v0.11,max_sequence_length = length of longest label of sentence + 1
v0.11,get char embeddings
v0.11,"(batch_size,max_sequence_length,input_size), i.e. replaces char indices with vectors of length input_size"
v0.11,take decoder input and initial hidden and pass through RNN
v0.11,"if all encoder outputs are provided, use attention"
v0.11,take convex combinations of encoder hidden states as new output using the computed attention coefficients
v0.11,"transform output to vectors of size len(char_dict) -> (batch_size, max_sequence_length, alphabet_size)"
v0.11,get all tokens
v0.11,variable to store initial hidden states for decoder
v0.11,encode input characters by sending them through RNN
v0.11,get one-hots for characters and add special symbols / padding
v0.11,determine length of each token
v0.11,embed character one-hots
v0.11,test packing and padding
v0.11,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.11,concatenate the final hidden states of the encoder. These will be projected to hidden_size of
v0.11,decoder later with self.emb_to_hidden
v0.11,mask out vectors that correspond to a dummy symbol (TODO: check attention masking)
v0.11,use token embedding as initial hidden state for decoder
v0.11,embed sentences
v0.11,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.11,concatenate everything together and project to appropriate size for decoder
v0.11,variable to store initial hidden states for decoder
v0.11,encode input characters by sending them through RNN
v0.11,note that we do not need to fill up with dummy symbols since we process each token seperately
v0.11,embed character one-hots
v0.11,send through encoder RNN (produces initial hidden for decoder)
v0.11,since bidirectional rnn is only used in encoding we need to project outputs to hidden_size of decoder
v0.11,project 2*hidden_size to hidden_size
v0.11,concatenate the final hidden states of the encoder. These will be projected to hidden_size of decoder
v0.11,later with self.emb_to_hidden
v0.11,use token embedding as initial hidden state for decoder
v0.11,"create initial hidden state tensor for batch (num_layers, batch_size, hidden_size)"
v0.11,concatenate everything together and project to appropriate size for decoder
v0.11,"score vector has to have a certain format for (2d-)loss fct (batch_size, alphabet_size, 1, max_seq_length)"
v0.11,"create target vector (batch_size, max_label_seq_length + 1)"
v0.11,filter empty sentences
v0.11,max length of the predicted sequences
v0.11,for printing
v0.11,stop if all sentences are empty
v0.11,remove previously predicted labels of this type
v0.11,create list of tokens in batch
v0.11,encode inputs
v0.11,"create input for first pass (batch_size, 1, input_size), first letter is special character <S>"
v0.11,sequence length is always set to one in prediction
v0.11,option 1: greedy decoding
v0.11,predictions
v0.11,decode next character
v0.11,pick top beam size many outputs with highest probabilities
v0.11,option 2: beam search
v0.11,out_probs = self.softmax(output_vectors).squeeze(1)
v0.11,make sure no dummy symbol <> or start symbol <S> is predicted
v0.11,pick top beam size many outputs with highest probabilities
v0.11,"probabilities, leading_indices = out_probs.topk(self.beam_size, 1)  # max prob along dimension 1"
v0.11,"leading_indices and probabilities have size (batch_size, beam_size)"
v0.11,keep scores of beam_size many hypothesis for each token in the batch
v0.11,stack all leading indices of all hypothesis and corresponding hidden states in two tensors
v0.11,save sequences so far
v0.11,keep track of how many hypothesis were completed for each token
v0.11,"if all_encoder_outputs returned, expand them to beam size (otherwise keep this as None)"
v0.11,decode with log softmax
v0.11,make sure no dummy symbol <> or start symbol <S> is predicted
v0.11,"check if an end symbol <E> has been predicted and, in that case, set hypothesis aside"
v0.11,"if the sequence is already ended, do not record as candidate"
v0.11,index of token in in list tokens_in_batch
v0.11,print(token_number)
v0.11,hypothesis score
v0.11,TODO: remove token if number of completed hypothesis exceeds given value
v0.11,set score of corresponding entry to -inf so it will not be expanded
v0.11,get leading_indices for next expansion
v0.11,find highest scoring hypothesis among beam_size*beam_size possible ones for each token
v0.11,take beam_size many copies of scores vector and add scores of possible new extensions
v0.11,"size (beam_size*batch_size, beam_size)"
v0.11,print(hypothesis_scores)
v0.11,"reshape to vector of size (batch_size, beam_size*beam_size),"
v0.11,each row contains beam_size*beam_size scores of the new possible hypothesis
v0.11,print(hypothesis_scores_per_token)
v0.11,"choose beam_size best for each token - size (batch_size, beam_size)"
v0.11,out of indices_per_token we now need to recompute the original indices of the hypothesis in
v0.11,a list of length beam_size*batch_size
v0.11,"where the first three inidices belong to the first token, the next three to the second token,"
v0.11,and so on
v0.11,with these indices we can compute the tensors for the next iteration
v0.11,expand sequences with corresponding index
v0.11,add log-probabilities to the scores
v0.11,save new leading indices
v0.11,save corresponding hidden states
v0.11,it may happen that no end symbol <E> is predicted for a token in all of the max_length iterations
v0.11,in that case we append one of the final seuqences without end symbol to the final_candidates
v0.11,get best final hypothesis for each token
v0.11,get characters from index sequences and add predicted label to token
v0.11,embeddings
v0.11,dictionaries
v0.11,all parameters will be pushed internally to the specified device
v0.11,get all tokens in this mini-batch
v0.11,now print labels in CoNLL format
v0.11,filter sentences with no candidates (no candidates means nothing can be linked anyway)
v0.11,fields to return
v0.11,embed sentences and send through prediction head
v0.11,embed all tokens
v0.11,get the embeddings of the entity mentions
v0.11,get the label of the entity
v0.11,if there is no RNN
v0.11,embed sentences
v0.11,"Main model implementation drops words and tags (independently), instead, we use word dropout!"
v0.11,apply MLPs for arc and relations to the BiLSTM output states
v0.11,get scores from the biaffine attentions
v0.11,"[batch_size, seq_len, seq_len]"
v0.11,"[batch_size, seq_len, seq_len, n_rels]"
v0.11,append both to file for evaluation
v0.11,"if head AND deprel correct, augment correct_rels score"
v0.11,----- Create the internal tag dictionary -----
v0.11,span-labels need special encoding (BIO or BIOES)
v0.11,the big question is whether the label dictionary should contain an UNK or not
v0.11,"without UNK, we cannot evaluate on data that contains labels not seen in test"
v0.11,"with UNK, the model learns less well if there are no UNK examples"
v0.11,is this a span prediction problem?
v0.11,----- Embeddings -----
v0.11,----- Initial loss weights parameters -----
v0.11,----- RNN specific parameters -----
v0.11,----- Conditional Random Field parameters -----
v0.11,"Previously trained models have been trained without an explicit CRF, thus it is required to check"
v0.11,whether we are loading a model from state dict in order to skip or add START and STOP token
v0.11,----- Dropout parameters -----
v0.11,dropouts
v0.11,----- Model layers -----
v0.11,----- RNN layer -----
v0.11,"If shared RNN provided, else create one for model"
v0.11,Whether to train initial hidden state
v0.11,final linear map to tag space
v0.11,"the loss function is Viterbi if using CRF, else regular Cross Entropy Loss"
v0.11,"if using CRF, we also require a CRF and a Viterbi decoder"
v0.11,"if there are no sentences, there is no loss"
v0.11,forward pass to get scores
v0.11,calculate loss given scores and labels
v0.11,make a zero-padded tensor for the whole sentence
v0.11,sort tensor in decreasing order based on lengths of sentences in batch
v0.11,----- Forward Propagation -----
v0.11,linear map to tag space
v0.11,"Depending on whether we are using CRF or a linear layer, scores is either:"
v0.11,"-- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF"
v0.11,"-- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer"
v0.11,get the gold labels
v0.11,spans need to be encoded as token-level predictions
v0.11,all others are regular labels for each token
v0.11,make sure its a list
v0.11,filter empty sentences
v0.11,reverse sort all sequences by their length
v0.11,progress bar for verbosity
v0.11,stop if all sentences are empty
v0.11,get features from forward propagation
v0.11,remove previously predicted labels of this type
v0.11,"if return_loss, get loss value"
v0.11,Sort batch in same way as forward propagation
v0.11,make predictions
v0.11,add predictions to Sentence
v0.11,BIOES-labels need to be converted to spans
v0.11,"token-labels can be added directly (""O"" and legacy ""_"" predictions are skipped)"
v0.11,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.11,core Flair models on Huggingface ModelHub
v0.11,"Large NER models,"
v0.11,Multilingual NER models
v0.11,English POS models
v0.11,Multilingual POS models
v0.11,English SRL models
v0.11,English chunking models
v0.11,Language-specific NER models
v0.11,English NER models
v0.11,Multilingual NER models
v0.11,English POS models
v0.11,Multilingual POS models
v0.11,English SRL models
v0.11,English chunking models
v0.11,Danish models
v0.11,German models
v0.11,French models
v0.11,Dutch models
v0.11,Malayalam models
v0.11,Portuguese models
v0.11,Keyphase models
v0.11,Biomedical models
v0.11,check if model name is a valid local file
v0.11,"check if model key is remapped to HF key - if so, print out information"
v0.11,get mapped name
v0.11,use mapped name instead
v0.11,"if not, check if model key is remapped to direct download location. If so, download model"
v0.11,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.11,"for all other cases (not local file or special download location), use HF model hub"
v0.11,"if not a local file, get from model hub"
v0.11,use model name as subfolder
v0.11,Lazy import
v0.11,output information
v0.11,"log.error(f"" - Error message: {e}"")"
v0.11,"all labels default to ""O"""
v0.11,set gold token-level
v0.11,set predicted token-level
v0.11,now print labels in CoNLL format
v0.11,print labels in CoNLL format
v0.11,clear embeddings after predicting
v0.11,load each model
v0.11,check if the same embeddings were already loaded previously
v0.11,"if the model uses StackedEmbedding, make a new stack with previous objects"
v0.11,sort embeddings by key alphabetically
v0.11,check previous embeddings and add if found
v0.11,only re-use static embeddings
v0.11,"if not found, use existing embedding"
v0.11,initialize new stack
v0.11,"of the model uses regular embedding, re-load if previous version found"
v0.11,auto-spawn on GPU if available
v0.11,embed sentences
v0.11,make tensor for all embedded sentences in batch
v0.11,English sentiment models
v0.11,Communicative Functions Model
v0.11,"scores_at_targets[range(features.shape[0]), lengths.values -1]"
v0.11,Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices
v0.11,"Initially, get scores from <start> tag to all other tags"
v0.11,"We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp"
v0.11,"Remember, the cur_tag of the previous timestep is the prev_tag of this timestep"
v0.11,Create a tensor to hold accumulated sequence scores at each current tag
v0.11,Create a tensor to hold back-pointers
v0.11,"i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag"
v0.11,"Let pads be the <end> tag index, since that was the last tag in the decoded sequence"
v0.11,"We add scores at current timestep to scores accumulated up to previous timestep, and"
v0.11,choose the previous timestep that corresponds to the max. accumulated score for each current timestep
v0.11,"If sentence is over, add transition to STOP-tag"
v0.11,Decode/trace best path backwards
v0.11,Sanity check
v0.11,remove start-tag and backscore to stop-tag
v0.11,Max + Softmax to get confidence score for predicted label and append label to each token
v0.11,"add a dummy ""O"" to close final prediction"
v0.11,return complex list
v0.11,internal variables
v0.11,non-set tags are OUT tags
v0.11,anything that is not OUT is IN
v0.11,does this prediction start a new span?
v0.11,begin and single tags start new spans
v0.11,"in IOB format, an I tag starts a span if it follows an O or is a different span"
v0.11,single tags that change prediction start new spans
v0.11,if an existing span is ended (either by reaching O or starting a new span)
v0.11,determine score and value
v0.11,append to result list
v0.11,reset for-loop variables for new span
v0.11,remember previous tag
v0.11,"Transitions are used in the following way: transitions[to, from]."
v0.11,"If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag"
v0.11,to START-tag and from STOP-tag to any other tag to -10000.
v0.11,weights for loss function
v0.11,iput size is two times wordembedding size since we use pair of words as input
v0.11,"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
v0.11,regression
v0.11,input size is two times word embedding size since we use pair of words as input
v0.11,the output size is 1
v0.11,auto-spawn on GPU if available
v0.11,forward allows only a single sentcence!!
v0.11,embed words of sentence
v0.11,go through all pairs of words with a maximum number of max_distance in between
v0.11,go through all pairs
v0.11,2-dim matrix whose rows are the embeddings of word pairs of the sentence
v0.11,So far only one sentence allowed
v0.11,If list of sentences is handed the function works with the first sentence of the list
v0.11,Assume data_points is a single sentence!!!
v0.11,scores are the predictions for each word pair
v0.11,"classification needs labels to be integers, regression needs labels to be float"
v0.11,this is due to the different loss functions
v0.11,only single sentences as input
v0.11,gold labels
v0.11,for output text file
v0.11,for buckets
v0.11,for average prediction
v0.11,add some statistics to the output
v0.11,use scikit-learn to evaluate
v0.11,"we iterate over each sentence, instead of batches"
v0.11,get single labels from scores
v0.11,gold labels
v0.11,for output text file
v0.11,hot one vector of true value
v0.11,hot one vector of predicted value
v0.11,"speichert embeddings, falls embedding_storage!= 'None'"
v0.11,"make ""classification report"""
v0.11,get scores
v0.11,"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.11,"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.11,line for log file
v0.11,create a model card for this model with Flair and PyTorch version
v0.11,also record Transformers version if library is loaded
v0.11,remember all parameters used in train() call
v0.11,add model card to model
v0.11,"if optimizer is class, trainer will create a single parameter group"
v0.11,"determine what splits (train, dev, test) to evaluate and log"
v0.11,prepare loss logging file and set up header
v0.11,"if optimizer class is passed, instantiate:"
v0.11,"from here on, use list of learning rates"
v0.11,load existing optimizer state dictionary if it exists
v0.11,"minimize training loss if training with dev data, else maximize dev score"
v0.11,"if scheduler is passed as a class, instantiate"
v0.11,"if we load a checkpoint, we have already trained for epoch"
v0.11,load existing scheduler state dictionary if it exists
v0.11,update optimizer and scheduler in model card
v0.11,"if training also uses dev/train data, include in training set"
v0.11,initialize sampler if provided
v0.11,init with default values if only class is provided
v0.11,set dataset to sample from
v0.11,this field stores the names of all dynamic embeddings in the model (determined after first forward pass)
v0.11,At any point you can hit Ctrl + C to break out of training early.
v0.11,update epoch in model card
v0.11,get new learning rate
v0.11,reload last best model if annealing with restarts is enabled
v0.11,stop training if learning rate becomes too small
v0.11,process mini-batches
v0.11,zero the gradients on the model and optimizer
v0.11,"if necessary, make batch_steps"
v0.11,forward and backward for batch
v0.11,forward pass
v0.11,Backward
v0.11,identify dynamic embeddings (always deleted) on first sentence
v0.11,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.11,do the optimizer step
v0.11,do the scheduler step if one-cycle or linear decay
v0.11,get new learning rate
v0.11,evaluate on train / dev / test split depending on training settings
v0.11,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.11,calculate scores using dev data if available
v0.11,append dev score to score history
v0.11,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.11,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.11,determine if this is the best model or if we need to anneal
v0.11,default mode: anneal against dev score
v0.11,alternative: anneal against dev loss
v0.11,alternative: anneal against train loss
v0.11,determine bad epoch number
v0.11,lr unchanged
v0.11,log bad epochs
v0.11,output log file
v0.11,make headers on first epoch
v0.11,"if checkpoint is enabled, save model at each epoch"
v0.11,Check whether to save best model
v0.11,"if we do not use dev data for model selection, save final model"
v0.11,test best model if test data is present
v0.11,recover all arguments that were used to train this model
v0.11,you can overwrite params with your own
v0.11,surface nested arguments
v0.11,resume training with these parameters
v0.11,"if we are training over multiple datasets, do evaluation for each"
v0.11,get and return the final test score of best model
v0.11,cast string to Path
v0.11,forward pass
v0.11,update optimizer and scheduler
v0.11,"TextDataset returns a list. valid and test are only one file,"
v0.11,so return the first element
v0.11,cast string to Path
v0.11,error message if the validation dataset is too small
v0.11,Shuffle training files randomly after serially iterating
v0.11,through corpus one
v0.11,"iterate through training data, starting at"
v0.11,self.split (for checkpointing)
v0.11,off by one for printing
v0.11,go into train mode
v0.11,reset variables
v0.11,not really sure what this does
v0.11,do the forward pass in the model
v0.11,try to predict the targets
v0.11,Backward
v0.11,`clip_grad_norm` helps prevent the exploding gradient
v0.11,problem in RNNs / LSTMs.
v0.11,We detach the hidden state from how it was
v0.11,previously produced.
v0.11,"If we didn't, the model would try backpropagating"
v0.11,all the way to start of the dataset.
v0.11,explicitly remove loss to clear up memory
v0.11,#########################################################
v0.11,Save the model if the validation loss is the best we've
v0.11,seen so far.
v0.11,#########################################################
v0.11,print info
v0.11,#########################################################
v0.11,##############################################################################
v0.11,final testing
v0.11,##############################################################################
v0.11,Turn on evaluation mode which disables dropout.
v0.11,Work out how cleanly we can divide the dataset into bsz parts.
v0.11,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.11,Evenly divide the data across the bsz batches.
v0.11,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.11,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.11,news-english-forward
v0.11,news-english-backward
v0.11,news-english-forward
v0.11,news-english-backward
v0.11,mix-english-forward
v0.11,mix-english-backward
v0.11,mix-german-forward
v0.11,mix-german-backward
v0.11,common crawl Polish forward
v0.11,common crawl Polish backward
v0.11,Slovenian forward
v0.11,Slovenian backward
v0.11,Bulgarian forward
v0.11,Bulgarian backward
v0.11,Dutch forward
v0.11,Dutch backward
v0.11,Swedish forward
v0.11,Swedish backward
v0.11,French forward
v0.11,French backward
v0.11,Czech forward
v0.11,Czech backward
v0.11,Portuguese forward
v0.11,Portuguese backward
v0.11,initialize cache if use_cache set
v0.11,embed a dummy sentence to determine embedding_length
v0.11,set to eval mode
v0.11,Copy the object's state from self.__dict__ which contains
v0.11,all our instance attributes. Always use the dict.copy()
v0.11,method to avoid modifying the original state.
v0.11,Remove the unpicklable entries.
v0.11,"if cache is used, try setting embeddings from cache first"
v0.11,try populating embeddings from cache
v0.11,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.11,get hidden states from language model
v0.11,take first or last hidden states from language model as word representation
v0.11,if self.tokenized_lm or token.whitespace_after:
v0.11,1-camembert-base -> camembert-base
v0.11,1-xlm-roberta-large -> xlm-roberta-large
v0.11,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.11,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.11,tokens are attended to.
v0.11,Zero-pad up to the sequence length.
v0.11,"first, find longest sentence in batch"
v0.11,prepare id maps for BERT model
v0.11,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.11,get aggregated embeddings for each BERT-subtoken in sentence
v0.11,get the current sentence object
v0.11,add concatenated embedding to sentence
v0.11,use first subword embedding if pooling operation is 'first'
v0.11,"otherwise, do a mean over all subwords in token"
v0.11,"if only one sentence is passed, convert to list of sentence"
v0.11,bidirectional LSTM on top of embedding layer
v0.11,dropouts
v0.11,"first, sort sentences by number of tokens"
v0.11,go through each sentence in batch
v0.11,PADDING: pad shorter sentences out
v0.11,ADD TO SENTENCE LIST: add the representation
v0.11,--------------------------------------------------------------------
v0.11,GET REPRESENTATION FOR ENTIRE BATCH
v0.11,--------------------------------------------------------------------
v0.11,--------------------------------------------------------------------
v0.11,FF PART
v0.11,--------------------------------------------------------------------
v0.11,use word dropout if set
v0.11,--------------------------------------------------------------------
v0.11,EXTRACT EMBEDDINGS FROM LSTM
v0.11,--------------------------------------------------------------------
v0.11,embed a dummy sentence to determine embedding_length
v0.11,Avoid conflicts with flair's Token class
v0.11,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.11,add positional encodings
v0.11,reshape the pixels into the sequence
v0.11,layer norm after convolution and positional encodings
v0.11,add <cls> token
v0.11,"transformer requires input in the shape [h*w+1, b, d]"
v0.11,the output is an embedding of <cls> token
v0.11,this parameter is fixed
v0.11,optional fine-tuning on top of embedding layer
v0.11,"if only one sentence is passed, convert to list of sentence"
v0.11,"if only one sentence is passed, convert to list of sentence"
v0.11,bidirectional RNN on top of embedding layer
v0.11,dropouts
v0.11,TODO: remove in future versions
v0.11,embed words in the sentence
v0.11,before-RNN dropout
v0.11,reproject if set
v0.11,push through RNN
v0.11,after-RNN dropout
v0.11,extract embeddings from RNN
v0.11,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.11,"check if this is the case and if so, set it"
v0.11,serialize the language models and the constructor arguments (but nothing else)
v0.11,special handling for deserializing language models
v0.11,re-initialize language model with constructor arguments
v0.11,copy over state dictionary to self
v0.11,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.11,"in their ""self.train()"" method)"
v0.11,IMPORTANT: add embeddings as torch modules
v0.11,iterate over sentences
v0.11,"if its a forward LM, take last state"
v0.11,"convert to plain strings, embedded in a list for the encode function"
v0.11,CNN
v0.11,dropouts
v0.11,TODO: remove in future versions
v0.11,embed words in the sentence
v0.11,before-RNN dropout
v0.11,reproject if set
v0.11,push CNN
v0.11,after-CNN dropout
v0.11,extract embeddings from CNN
v0.11,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.11,"if only one sentence is passed, convert to list of sentence"
v0.11,temporary fix to disable tokenizer parallelism warning
v0.11,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.11,do not print transformer warnings as these are confusing in this case
v0.11,load tokenizer and transformer model
v0.11,load tokenizer from inmemory zip-file
v0.11,model name
v0.11,embedding parameters
v0.11,send mini-token through to check how many layers the model has
v0.11,return length
v0.11,check if special tokens exist to circumvent error message
v0.11,"most models have an initial BOS token, except for XLNet, T5 and GPT2"
v0.11,"when initializing, embeddings are in eval mode by default"
v0.11,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.11,in case of doubt: token embedding has higher priority than document embedding
v0.11,in case of doubt: token embedding has higher priority than document embedding
v0.11,"transformers returns the ""added_tokens.json"" even if it doesn't create it"
v0.11,remove special markup
v0.11,legacy TransformerDocumentEmbedding
v0.11,legacy TransformerTokenEmbedding
v0.11,legacy Flair <= 0.7
v0.11,legacy TransformerTokenEmbedding
v0.11,Legacy TransformerDocumentEmbedding
v0.11,legacy TransformerTokenEmbedding
v0.11,legacy TransformerDocumentEmbedding
v0.11,copy values from new embedding
v0.11,iterate over subtokens and reconstruct tokens
v0.11,remove special markup
v0.11,TODO check if this is necessary is this method is called before prepare_for_model
v0.11,check if reconstructed token is special begin token ([CLS] or similar)
v0.11,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.11,append subtoken to reconstruct token
v0.11,check if reconstructed token is the same as current token
v0.11,"if so, add subtoken count"
v0.11,reset subtoken count and reconstructed token
v0.11,break from loop if all tokens are accounted for
v0.11,if tokens are unaccounted for
v0.11,check if all tokens were matched to subtokens
v0.11,subtokenize the sentence
v0.11,transformer specific tokenization
v0.11,set zero embeddings for empty sentences and exclude
v0.11,determine into how many subtokens each token is split
v0.11,remember tokenized sentences and their subtokenization
v0.11,Models such as FNet do not have an attention_mask
v0.11,set language IDs for XLM-style transformers
v0.11,cls first pooling can be done without recreating sentence hidden states
v0.11,encode inputs
v0.11,make the tuple a tensor; makes working with it easier.
v0.11,only use layers that will be outputted
v0.11,remove padding tokens
v0.11,set context if not set already
v0.11,create expanded sentence and remember context offsets
v0.11,move embeddings from context back to original sentence (if using context)
v0.11,Expose base classses
v0.11,Expose document embedding classes
v0.11,Expose image embedding classes
v0.11,Expose legacy embedding classes
v0.11,Expose token embedding classes
v0.11,IMPORTANT: add embeddings as torch modules
v0.11,"if only one sentence is passed, convert to list of sentence"
v0.11,GLOVE embeddings
v0.11,TURIAN embeddings
v0.11,KOMNINOS embeddings
v0.11,pubmed embeddings
v0.11,FT-CRAWL embeddings
v0.11,FT-CRAWL embeddings
v0.11,twitter embeddings
v0.11,two-letter language code wiki embeddings
v0.11,two-letter language code wiki embeddings
v0.11,two-letter language code crawl embeddings
v0.11,gensim version 4
v0.11,gensim version 3
v0.11,fix serialized models
v0.11,"this is required to force the module on the cpu,"
v0.11,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.11,self.to(..) actually sets the device properly
v0.11,this ignores the get_cached_vec method when loading older versions
v0.11,it is needed for compatibility reasons
v0.11,gensim version 4
v0.11,gensim version 3
v0.11,use list of common characters if none provided
v0.11,translate words in sentence into ints using dictionary
v0.11,"sort words by length, for batching and masking"
v0.11,chars for rnn processing
v0.11,multilingual models
v0.11,English models
v0.11,Arabic
v0.11,Bulgarian
v0.11,Czech
v0.11,Danish
v0.11,German
v0.11,Spanish
v0.11,Basque
v0.11,Persian
v0.11,Finnish
v0.11,French
v0.11,Hebrew
v0.11,Hindi
v0.11,Croatian
v0.11,Indonesian
v0.11,Italian
v0.11,Japanese
v0.11,Malayalam
v0.11,Dutch
v0.11,Norwegian
v0.11,Polish
v0.11,Portuguese
v0.11,Pubmed
v0.11,Slovenian
v0.11,Swedish
v0.11,Tamil
v0.11,Spanish clinical
v0.11,CLEF HIPE Shared task
v0.11,Amharic
v0.11,load model if in pretrained model map
v0.11,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.11,CLEF HIPE models are lowercased
v0.11,embeddings are static if we don't do finetuning
v0.11,embed a dummy sentence to determine embedding_length
v0.11,set to eval mode
v0.11,make compatible with serialized models (TODO: remove)
v0.11,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.11,make compatible with serialized models (TODO: remove)
v0.11,gradients are enable if fine-tuning is enabled
v0.11,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.11,get hidden states from language model
v0.11,take first or last hidden states from language model as word representation
v0.11,offset mode that extracts at whitespace after last character
v0.11,offset mode that extracts at last character
v0.11,use the character language model embeddings as basis
v0.11,length is twice the original character LM embedding length
v0.11,these fields are for the embedding memory
v0.11,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.11,we re-compute embeddings dynamically at each epoch
v0.11,set the memory method
v0.11,memory is wiped each time we do a training run
v0.11,"if we keep a pooling, it needs to be updated continuously"
v0.11,update embedding
v0.11,check token.text is empty or not
v0.11,set aggregation operation
v0.11,add embeddings after updating
v0.11,this parameter is fixed
v0.11,model architecture
v0.11,model architecture
v0.11,"""pl"","
v0.11,download if necessary
v0.11,load the model
v0.11,"TODO: keep for backwards compatibility, but remove in future"
v0.11,save the sentence piece model as binary file (not as path which may change)
v0.11,write out the binary sentence piece model into the expected directory
v0.11,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.11,"otherwise, use normal process and potentially trigger another download"
v0.11,"once the modes if there, load it with sentence piece"
v0.11,empty words get no embedding
v0.11,all other words get embedded
v0.11,"the default model for ELMo is the 'original' model, which is very large"
v0.11,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.11,put on Cuda if available
v0.11,embed a dummy sentence to determine embedding_length
v0.11,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.11,GLOVE embeddings
v0.11,read in test file if exists
v0.11,read in dev file if exists
v0.11,"find train, dev and test files if not specified"
v0.11,Add tags for each annotated span
v0.11,Tag all other token as Outer (O)
v0.11,Remove leading and trailing whitespaces from annotated spans
v0.11,Search start and end token index for current span
v0.11,If end index is not found set to last token
v0.11,Throw error if indices are not valid
v0.11,Add IOB tags
v0.11,get train data
v0.11,read in test file if exists
v0.11,read in dev file if exists
v0.11,"find train, dev and test files if not specified"
v0.11,special key for space after
v0.11,special key for feature columns
v0.11,special key for dependency head id
v0.11,"store either Sentence objects in memory, or only file offsets"
v0.11,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.11,determine encoding of text file
v0.11,identify which columns are spans and which are word-level
v0.11,now load all sentences
v0.11,skip first line if to selected
v0.11,option 1: keep Sentence objects in memory
v0.11,pointer to previous
v0.11,parse next sentence
v0.11,quit if last sentence reached
v0.11,skip banned sentences
v0.11,set previous and next sentence for context
v0.11,append parsed sentence to list in memory
v0.11,option 2: keep source data in memory
v0.11,"read lines for next sentence, but don't parse"
v0.11,quit if last sentence reached
v0.11,append raw lines for each sentence
v0.11,we make a distinction between word-level tags and span-level tags
v0.11,read first sentence to determine which columns are span-labels
v0.11,skip first line if to selected
v0.11,"sentence_2 = self._convert_lines_to_sentence(self._read_next_sentence(file),"
v0.11,word_level_tag_columns=column_name_map)
v0.11,go through all annotations
v0.11,the space after key is always word-levels
v0.11,for column in self.word_level_tag_columns:
v0.11,"log.info(f""Column {column} ({self.word_level_tag_columns[column]}) is a word-level column."")"
v0.11,"if sentence ends, break"
v0.11,parse comments if possible
v0.11,"otherwise, this line is a token. parse and add to sentence"
v0.11,check if this sentence is a document boundary
v0.11,add span labels
v0.11,parse relations if they are set
v0.11,head and tail span indices are 1-indexed and end index is inclusive
v0.11,get fields from line
v0.11,get head_id if exists (only in dependency parses)
v0.11,initialize token
v0.11,go through all columns
v0.11,'feats' and 'misc' column should be split into different fields
v0.11,special handling for whitespace after
v0.11,add each other feature as label-value pair
v0.11,get the task name (e.g. 'ner')
v0.11,get the label value
v0.11,add label
v0.11,remap regular tag names
v0.11,"if in memory, retrieve parsed sentence"
v0.11,else skip to position in file where sentence begins
v0.11,set sentence context using partials TODO: pointer to dataset is really inefficient
v0.11,this dataset name
v0.11,check if data there
v0.11,column format
v0.11,this dataset name
v0.11,check if data there
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,download files if not present locally
v0.11,we need to slightly modify the original files by adding some new lines after document separators
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,Remove CoNLL-U meta information in the last column
v0.11,column format
v0.11,dataset name
v0.11,data folder: default dataset folder is the cache root
v0.11,download data if necessary
v0.11,column format
v0.11,dataset name
v0.11,data folder: default dataset folder is the cache root
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,entity_mapping
v0.11,this dataset name
v0.11,download data if necessary
v0.11,data validation
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download files if not present locallys
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,# download zip
v0.11,merge the files in one as the zip is containing multiples files
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,check if data there
v0.11,create folder
v0.11,download dataset
v0.11,column format
v0.11,this dataset name
v0.11,download and parse data if necessary
v0.11,create train test dev if not exist
v0.11,column format
v0.11,this dataset name
v0.11,If the extracted corpus file is not yet present in dir
v0.11,download zip if necessary
v0.11,"extracted corpus is not present , so unpacking it."
v0.11,column format
v0.11,this dataset name
v0.11,download zip
v0.11,unpacking the zip
v0.11,merge the files in one as the zip is containing multiples files
v0.11,column format
v0.11,this dataset name
v0.11,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.11,download files if not present locally
v0.11,we need to modify the original files by adding new lines after after the end of each sentence
v0.11,if only one language is given
v0.11,column format
v0.11,this dataset name
v0.11,"use all languages if explicitly set to ""all"""
v0.11,download data if necessary
v0.11,initialize comlumncorpus and add it to list
v0.11,column format
v0.11,this dataset name
v0.11,check if data there
v0.11,code-switch uses the same training data than multi but provides a different test set.
v0.11,"as the test set is not published, those two tasks are the same."
v0.11,column format
v0.11,this dataset name
v0.11,"For each language in languages, the file is downloaded if not existent"
v0.11,Then a comlumncorpus of that data is created and saved in a list
v0.11,this list is handed to the multicorpus
v0.11,list that contains the columncopora
v0.11,download data if necessary
v0.11,"if language not downloaded yet, download it"
v0.11,create folder
v0.11,get google drive id from list
v0.11,download from google drive
v0.11,unzip
v0.11,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.11,transform data into required format
v0.11,"the processed dataset has the additional ending ""_new"""
v0.11,remove the unprocessed dataset
v0.11,initialize comlumncorpus and add it to list
v0.11,if no languages are given as argument all languages used in XTREME will be loaded
v0.11,if only one language is given
v0.11,column format
v0.11,this dataset name
v0.11,"For each language in languages, the file is downloaded if not existent"
v0.11,Then a comlumncorpus of that data is created and saved in a list
v0.11,This list is handed to the multicorpus
v0.11,list that contains the columncopora
v0.11,download data if necessary
v0.11,"if language not downloaded yet, download it"
v0.11,create folder
v0.11,download from HU Server
v0.11,unzip
v0.11,transform data into required format
v0.11,initialize comlumncorpus and add it to list
v0.11,if only one language is given
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,initialize comlumncorpus and add it to list
v0.11,download data if necessary
v0.11,unpack and write out in CoNLL column-like format
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,data is not in IOB2 format. Thus we transform it to IOB2
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,rename according to train - test - dev - convention
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,column format
v0.11,this dataset name
v0.11,download data if necessary
v0.11,Add missing newline after header
v0.11,Workaround for empty tokens
v0.11,"Add ""real"" document marker"
v0.11,Dataset split mapping
v0.11,v2.0 only adds new language and splits for AJMC dataset
v0.11,Special document marker for sample splits in AJMC dataset
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download and parse data if necessary
v0.11,iterate over all html files
v0.11,"get rid of html syntax, we only need the text"
v0.11,between all documents we write a separator symbol
v0.11,skip empty strings
v0.11,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.11,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.11,sentence splitting and tokenization
v0.11,iterate through all annotations and add to corresponding tokens
v0.11,find sentence to which annotation belongs
v0.11,position within corresponding sentence
v0.11,set annotation for tokens of entity mention
v0.11,write to out-file in column format
v0.11,"in case something goes wrong, delete the dataset and raise error"
v0.11,this dataset name
v0.11,download and parse data if necessary
v0.11,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.11,generate qid wikiname dictionaries
v0.11,merge dictionaries
v0.11,ignore first line
v0.11,commented and empty lines
v0.11,read all Q-IDs
v0.11,ignore first line
v0.11,request
v0.11,this dataset name
v0.11,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.11,like this we can quickly check if the corresponding page exists
v0.11,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.11,delete unprocessed file
v0.11,collect all wikiids
v0.11,create the dictionary
v0.11,request
v0.11,this dataset name
v0.11,names of raw text documents
v0.11,open output_file
v0.11,iterate through all documents
v0.11,split sentences and tokenize
v0.11,iterate through all annotations and add to corresponding tokens
v0.11,find sentence to which annotation belongs
v0.11,position within corresponding sentence
v0.11,set annotation for tokens of entity mention
v0.11,write to out file
v0.11,this dataset name
v0.11,download and parse data if necessary
v0.11,this dataset name
v0.11,download and parse data if necessary
v0.11,First parse the post titles
v0.11,Keep track of how many and which entity mentions does a given post title have
v0.11,Check if the current post title has an entity link and parse accordingly
v0.11,Post titles with entity mentions (if any) are handled via this function
v0.11,Then parse the comments
v0.11,"Iterate over the comments.tsv file, until the end is reached"
v0.11,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.11,Each comment thread is handled as one 'document'.
v0.11,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.11,This if-condition is needed to handle this problem.
v0.11,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.11,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.11,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.11,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.11,and not just single letters into single rows.
v0.11,If there are annotated entity mentions for given post title or a comment thread
v0.11,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.11,Write the token with a corresponding tag to file
v0.11,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.11,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.11,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.11,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.11,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.11,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.11,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.11,Check if further annotations belong to the current post title or comment thread as well
v0.11,Stop when the end of an annotation file is reached
v0.11,Check if further annotations belong to the current sentence as well
v0.11,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.11,Docstart
v0.11,if there is more than one word in the chunk we write each in a separate line
v0.11,print(chunks)
v0.11,empty line after each sentence
v0.11,convert the file to CoNLL
v0.11,this dataset name
v0.11,"check if data there, if not, download the data"
v0.11,create folder
v0.11,download data
v0.11,transform data into column format if necessary
v0.11,if no filenames are specified we use all the data
v0.11,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.11,also we remove 'raganato_ALL' from filenames in case its in the list
v0.11,generate the test file
v0.11,make column file and save to data_folder
v0.11,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.11,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.11,create folder
v0.11,download data
v0.11,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.11,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.11,create folder
v0.11,download data
v0.11,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.11,generate the test file
v0.11,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.11,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.11,create folder
v0.11,download data
v0.11,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.11,generate the test file
v0.11,default dataset folder is the cache root
v0.11,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.11,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.11,create folder
v0.11,download data
v0.11,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.11,generate the test file
v0.11,default dataset folder is the cache root
v0.11,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.11,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.11,create folder
v0.11,download data
v0.11,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.11,generate the test file
v0.11,default dataset folder is the cache root
v0.11,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.11,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.11,create folder
v0.11,download data
v0.11,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.11,generate the test file
v0.11,this dataset name
v0.11,download data if necessary
v0.11,if True:
v0.11,write CoNLL-U Plus header
v0.11,"Some special cases (e.g., missing spaces before entity marker)"
v0.11,necessary if text should be whitespace tokenizeable
v0.11,Handle case where tail may occur before the head
v0.11,this dataset name
v0.11,write CoNLL-U Plus header
v0.11,this dataset name
v0.11,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.11,download data if necessary
v0.11,write CoNLL-U Plus header
v0.11,The span has ended.
v0.11,We are entering a new span; reset indices
v0.11,and active tag to new span.
v0.11,We're inside a span.
v0.11,Last token might have been a part of a valid span.
v0.11,this dataset name
v0.11,write CoNLL-U Plus header
v0.11,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.11,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.11,target_file_path = Path(data_folder) / target_filename
v0.11,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.11,# write CoNLL-U Plus header
v0.11,"target_file.write(""# global.columns = id form ner\n"")"
v0.11,for example in json.load(source_file):
v0.11,token_list = self._tacred_example_to_token_list(example)
v0.11,target_file.write(token_list.serialize())
v0.11,check if first tag row is already occupied
v0.11,"if first tag row is occupied, use second tag row"
v0.11,hardcoded mapping TODO: perhaps find nicer solution
v0.11,"find train, dev and test files if not specified"
v0.11,use test_file to create test split if available
v0.11,use dev_file to create test split if available
v0.11,"if data point contains black-listed label, do not use"
v0.11,first check if valid sentence
v0.11,"if so, add to indices"
v0.11,"find train, dev and test files if not specified"
v0.11,variables
v0.11,different handling of in_memory data than streaming data
v0.11,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.11,test if format is OK
v0.11,test if at least one label given
v0.11,make sentence from text (and filter for length)
v0.11,"if a pair column is defined, make a sentence pair object"
v0.11,noinspection PyDefaultArgument
v0.11,dataset name includes the split size
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download each of the 28 splits
v0.11,create dataset directory if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,handle labels file
v0.11,handle data file
v0.11,Create flair compatible labels
v0.11,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,create dataset directory if necessary
v0.11,create train.txt file from CSV
v0.11,create test.txt file from CSV
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,create dataset directory if necessary
v0.11,create train.txt file by iterating over pos and neg file
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,create dataset directory if necessary
v0.11,create train.txt file by iterating over pos and neg file
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,create dataset directory if necessary
v0.11,create train.txt file by iterating over pos and neg file
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,create dataset directory if necessary
v0.11,create train.txt file by iterating over pos and neg file
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,create train dev and test files in fasttext format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download senteval datasets if necessary und unzip
v0.11,convert to FastText format
v0.11,download data if necessary
v0.11,"if data is not downloaded yet, download it"
v0.11,get the zip file
v0.11,move original .tsv files to another folder
v0.11,create train and dev splits in fasttext format
v0.11,create eval_dataset file with no labels
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,download datasets if necessary
v0.11,create dataset directory if necessary
v0.11,create correctly formated txt files
v0.11,multiple labels are possible
v0.11,this dataset name
v0.11,download data if necessary
v0.11,Create flair compatible labels
v0.11,TREC-6 : NUM:dist -> __label__NUM
v0.11,TREC-50: NUM:dist -> __label__NUM:dist
v0.11,this dataset name
v0.11,download data if necessary
v0.11,Create flair compatible labels
v0.11,TREC-6 : NUM:dist -> __label__NUM
v0.11,TREC-50: NUM:dist -> __label__NUM:dist
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,create a separate directory for different tasks
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,check if dataset is supported
v0.11,set file names
v0.11,set file names
v0.11,download and unzip in file structure if necessary
v0.11,instantiate corpus
v0.11,"find train, dev and test files if not specified"
v0.11,"create DataPairDataset for train, test and dev file, if they are given"
v0.11,stop if file does not exist
v0.11,create a DataPair object from strings
v0.11,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.11,"if data is not downloaded yet, download it"
v0.11,get the zip file
v0.11,"rename test file to eval_dataset, since it has no labels"
v0.11,"if data is not downloaded yet, download it"
v0.11,get the zip file
v0.11,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.11,dev sets include 5 different annotations but we will only keep the gold label
v0.11,"rename test file to eval_dataset, since it has no labels"
v0.11,"if data is not downloaded yet, download it"
v0.11,get test and dev sets
v0.11,"if data is not downloaded yet, download it"
v0.11,get the zip file
v0.11,"rename test file to eval_dataset, since it has no labels"
v0.11,"if data is not downloaded yet, download it"
v0.11,get the zip file
v0.11,"rename test file to eval_dataset, since it has no labels"
v0.11,"if data is not downloaded yet, download it"
v0.11,get the zip file
v0.11,"rename test file to eval_dataset, since it has no labels"
v0.11,"if data not downloaded yet, download it"
v0.11,get the zip file
v0.11,"the downloaded files have json format, we transform them to tsv"
v0.11,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.11,remove json file
v0.11,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.11,with sum of all entity lengths as secondary key
v0.11,calculate offset without current text
v0.11,because we stick all passages of a document together
v0.11,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.11,Try to fix incorrect annotations
v0.11,print(
v0.11,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.11,)
v0.11,Ignore empty lines or relation annotations
v0.11,FIX annotation of whitespaces (necessary for PDR)
v0.11,One token may contain multiple entities -> deque all of them
v0.11,column format
v0.11,this dataset name
v0.11,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.11,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,Edge case: last token starts a new entity
v0.11,Last document in file
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,In the huner split files there is no information whether a given id originates
v0.11,from the train or test file of the original corpus - so we have to adapt corpus
v0.11,splitting here
v0.11,In the huner split files there is no information whether a given id originates
v0.11,from the train or test file of the original corpus - so we have to adapt corpus
v0.11,splitting here
v0.11,In the huner split files there is no information whether a given id originates
v0.11,from the train or test file of the original corpus - so we have to adapt corpus
v0.11,splitting here
v0.11,Edge case: last token starts a new entity
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,Read texts
v0.11,Read annotations
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,We need to apply a patch to correct the original training file
v0.11,Articles title
v0.11,Article abstract
v0.11,Entity annotations
v0.11,column format
v0.11,this dataset name
v0.11,Edge case: last token starts a new entity
v0.11,Map all entities to chemicals
v0.11,Map all entities to disease
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,Incomplete article
v0.11,Invalid XML syntax
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,if len(mid) != 3:
v0.11,continue
v0.11,Try to fix entity offsets
v0.11,column format
v0.11,this dataset name
v0.11,There is still one illegal annotation in the file ..
v0.11,column format
v0.11,this dataset name
v0.11,"Abstract first, title second to prevent issues with sentence splitting"
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,"Filter for specific entity types, by default no entities will be filtered"
v0.11,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.11,train and dev split of V2 will be train in V4
v0.11,test split of V2 will be dev in V4
v0.11,New documents in V4 will become test documents
v0.11,column format
v0.11,this dataset name
v0.11,column format
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,cache Feidegger config file
v0.11,cache Feidegger images
v0.11,replace image URL with local cached file
v0.11,append Sentence-Image data point
v0.11,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.11,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.11,"if so, num_workers is set to 0 for faster processing"
v0.11,cast to list if necessary
v0.11,cast to list if necessary
v0.11,"first, check if pymongo is installed"
v0.11,automatically identify train / test / dev files
v0.11,"if no test file is found, take any file with 'test' in name"
v0.11,Expose base classses
v0.11,Expose all biomedical data sets used for the evaluation of BioBERT
v0.11,-
v0.11,-
v0.11,-
v0.11,-
v0.11,Expose all biomedical data sets using the HUNER splits
v0.11,Expose all biomedical data sets
v0.11,Expose all document classification datasets
v0.11,word sense disambiguation
v0.11,Expose all entity linking datasets
v0.11,Expose all relation extraction datasets
v0.11,universal proposition banks
v0.11,keyphrase detection datasets
v0.11,other NER datasets
v0.11,standard NER datasets
v0.11,Expose all sequence labeling datasets
v0.11,Expose all text-image datasets
v0.11,Expose all text-text datasets
v0.11,Expose all treebanks
v0.11,"find train, dev and test files if not specified"
v0.11,get train data
v0.11,get test data
v0.11,get dev data
v0.11,option 1: read only sentence boundaries as offset positions
v0.11,option 2: keep everything in memory
v0.11,"if in memory, retrieve parsed sentence"
v0.11,else skip to position in file where sentence begins
v0.11,current token ID
v0.11,handling for the awful UD multiword format
v0.11,end of sentence
v0.11,comments
v0.11,ellipsis
v0.11,if token is a multi-word
v0.11,normal single-word tokens
v0.11,"if we don't split multiwords, skip over component words"
v0.11,add token
v0.11,add morphological tags
v0.11,derive whitespace logic for multiwords
v0.11,print(token)
v0.11,print(current_multiword_last_token)
v0.11,print(current_multiword_first_token)
v0.11,"if multi-word equals component tokens, there should be no whitespace"
v0.11,go through all tokens in subword and set whitespace_after information
v0.11,print(i)
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,this dataset name
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,download data if necessary
v0.11,this dataset name
v0.11,default dataset folder is the cache root
v0.11,download data if necessary
v0.11,"finally, print model card for information"
v0.11,test corpus
v0.11,create a TARS classifier
v0.11,check if right number of classes
v0.11,switch to task with only one label
v0.11,check if right number of classes
v0.11,switch to task with three labels provided as list
v0.11,check if right number of classes
v0.11,switch to task with four labels provided as set
v0.11,check if right number of classes
v0.11,switch to task with two labels provided as Dictionary
v0.11,check if right number of classes
v0.11,test corpus
v0.11,create a TARS classifier
v0.11,switch to a new task (TARS can do multiple tasks so you must define one)
v0.11,initialize the text classifier trainer
v0.11,start the training
v0.11,"With end symbol, without start symbol, padding in front"
v0.11,"Without end symbol, with start symbol, padding in back"
v0.11,"Without end symbol, without start symbol, padding in front"
v0.11,increment for last token in sentence if not followed by whitespace
v0.11,clean up directory
v0.11,clean up directory
v0.11,initialize trainer
v0.11,clean up results directory
v0.11,initialize trainer
v0.11,initialize trainer
v0.11,initialize trainer
v0.11,initialize trainer
v0.11,initialize trainer
v0.11,initialize trainer
v0.11,train model for 2 epochs
v0.11,load the checkpoint model and train until epoch 4
v0.11,clean up results directory
v0.11,initialize trainer
v0.11,from flair.trainers.trainer_regression import RegressorTrainer
v0.11,def test_trainer_evaluation(tasks_base_path):
v0.11,"corpus, model, trainer = init(tasks_base_path)"
v0.11,
v0.11,expected = model.evaluate(corpus.dev)
v0.11,
v0.11,assert expected is not None
v0.11,def test_trainer_results(tasks_base_path):
v0.11,"corpus, model, trainer = init(tasks_base_path)"
v0.11,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.11,"assert results[""test_score""] > 0"
v0.11,"assert len(results[""dev_loss_history""]) == 1"
v0.11,"assert len(results[""dev_score_history""]) == 1"
v0.11,"assert len(results[""train_loss_history""]) == 1"
v0.11,example sentence
v0.11,set 4 labels for 2 tokens ('love' is tagged twice)
v0.11,check if there are three POS labels with correct text and values
v0.11,check if there are is one SENTIMENT label with correct text and values
v0.11,check if all tokens are correctly labeled
v0.11,remove the pos label from the last word
v0.11,there should be 2 POS labels left
v0.11,now remove all pos tags
v0.11,set 3 labels for 2 spans (HU is tagged twice)
v0.11,check if there are three labels with correct text and values
v0.11,check if there are two spans with correct text and values
v0.11,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.11,should be only one NER label left
v0.11,and only one NER span
v0.11,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.11,check if there are three labels with correct text and values
v0.11,check if there are two spans with correct text and values
v0.11,"now delete the NER tags of ""Humboldt-Universitt zu Berlin"""
v0.11,should be only one NER label left
v0.11,and only one NER span
v0.11,but there is also one orgtype span and label
v0.11,and only one NER span
v0.11,let's add the NER tag back
v0.11,check if there are three labels with correct text and values
v0.11,check if there are two spans with correct text and values
v0.11,now remove all NER tags
v0.11,set 3 labels for 2 spans (HU is tagged twice with different tags)
v0.11,create two relation label
v0.11,there should be two relation labels
v0.11,there should be one syntactic labels
v0.11,"there should be two relations, one with two and one with one label"
v0.11,example sentence
v0.11,add another topic label
v0.11,example sentence
v0.11,has sentiment value
v0.11,has 4 part of speech tags
v0.11,has 1 NER tag
v0.11,should be in total 6 labels
v0.11,example sentence
v0.11,add two NER labels
v0.11,get the four labels
v0.11,check that only two of the respective data points are equal
v0.11,make a sentence and some right context
v0.11,TODO: is this desirable? Or should two sentences with same text still be considered different objects?
v0.11,@pytest.mark.integration
v0.11,initialize trainer
v0.11,get default dictionary
v0.11,init forward LM with 128 hidden states and 1 layer
v0.11,get the example corpus and process at character level in forward direction
v0.11,train the language model
v0.11,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.11,clean up results directory
v0.11,get default dictionary
v0.11,init forward LM with 128 hidden states and 1 layer
v0.11,get the example corpus and process at character level in forward direction
v0.11,train the language model
v0.11,define search space
v0.11,sequence tagger parameter
v0.11,model trainer parameter
v0.11,training parameter
v0.11,find best parameter settings
v0.11,clean up results directory
v0.11,document embeddings parameter
v0.11,training parameter
v0.11,clean up results directory
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,"get training, test and dev data"
v0.11,get two corpora as one
v0.11,"get training, test and dev data for full English UD corpus from web"
v0.11,clean up data directory
v0.11,"assert [token.get_tag(""head"").value for token in sent1.tokens] == ["
v0.11,"""2"","
v0.11,"""0"","
v0.11,"""4"","
v0.11,"""2"","
v0.11,"""2"","
v0.11,"""2"","
v0.11,]
v0.11,"Here, we use the default token annotation fields."
v0.11,"We have manually checked, that these numbers are correct:"
v0.11,"+1 offset, because of missing EOS marker at EOD"
v0.11,load dataset
v0.11,tagger without CRF
v0.11,train
v0.11,check if loaded model can predict
v0.11,check if loaded model successfully fit the training data
v0.11,load dataset
v0.11,tagger without CRF
v0.11,train
v0.11,check if loaded model can predict
v0.11,check if loaded model successfully fit the training data
v0.11,load dataset
v0.11,tagger without CRF
v0.11,train
v0.11,check if loaded model can predict
v0.11,check if loaded model successfully fit the training data
v0.11,load dataset
v0.11,tagger without CRF
v0.11,train
v0.11,check if loaded model can predict
v0.11,check if loaded model successfully fit the training data
v0.11,check if model can predict
v0.11,load model
v0.11,chcek if model predicts correct label
v0.11,check if loaded model successfully fit the training data
v0.11,check if model can predict
v0.11,load model
v0.11,chcek if model predicts correct label
v0.11,check if loaded model successfully fit the training data
v0.11,check if model can predict
v0.11,load model
v0.11,chcek if model predicts correct label
v0.11,check if loaded model successfully fit the training data
v0.11,clean up file
v0.11,train model for 2 epochs
v0.11,load the checkpoint model and train until epoch 4
v0.10,from allennlp.common.tqdm import Tqdm
v0.10,mmap seems to be much more memory efficient
v0.10,Remove quotes from etag
v0.10,"If there is an etag, it's everything after the first period"
v0.10,"Otherwise, use None"
v0.10,"URL, so get it from the cache (downloading if necessary)"
v0.10,"File, and it exists."
v0.10,"File, but it doesn't exist."
v0.10,Something unknown
v0.10,Extract all the contents of zip file in current directory
v0.10,Extract all the contents of zip file in current directory
v0.10,get cache path to put the file
v0.10,"Download to temporary file, then copy to cache dir once finished."
v0.10,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.10,GET file object
v0.10,TODO(joelgrus): do we want to do checksums or anything like that?
v0.10,get cache path to put the file
v0.10,make HEAD request to check ETag
v0.10,add ETag to filename if it exists
v0.10,"etag = response.headers.get(""ETag"")"
v0.10,"Download to temporary file, then copy to cache dir once finished."
v0.10,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.10,GET file object
v0.10,These defaults are the same as the argument defaults in tqdm.
v0.10,first determine the distribution of classes in the dataset
v0.10,weight for each sample
v0.10,Create blocks
v0.10,shuffle the blocks
v0.10,concatenate the shuffled blocks
v0.10,Create blocks
v0.10,shuffle the blocks
v0.10,concatenate the shuffled blocks
v0.10,determine offsets for whitespace_after field
v0.10,increment for last token in sentence if not followed by whitespace
v0.10,determine offsets for whitespace_after field
v0.10,conll 2000 column format
v0.10,conll 03 NER column format
v0.10,WNUT-17
v0.10,-- WikiNER datasets
v0.10,-- Universal Dependencies
v0.10,Germanic
v0.10,Romance
v0.10,West-Slavic
v0.10,South-Slavic
v0.10,East-Slavic
v0.10,Scandinavian
v0.10,Asian
v0.10,Language isolates
v0.10,recent Universal Dependencies
v0.10,other datasets
v0.10,text classification format
v0.10,text regression format
v0.10,"first, try to fetch dataset online"
v0.10,default dataset folder is the cache root
v0.10,get string value if enum is passed
v0.10,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.10,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.10,the CoNLL 03 task for German has an additional lemma column
v0.10,the CoNLL 03 task for Dutch has no NP column
v0.10,the CoNLL 03 task for Spanish only has two columns
v0.10,the GERMEVAL task only has two columns: text and ner
v0.10,WSD tasks may be put into this column format
v0.10,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.10,"for text classifiers, we use our own special format"
v0.10,NER corpus for Basque
v0.10,automatically identify train / test / dev files
v0.10,"if no test file is found, take any file with 'test' in name"
v0.10,get train and test data
v0.10,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.10,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.10,convert tag scheme to iobes
v0.10,automatically identify train / test / dev files
v0.10,automatically identify train / test / dev files
v0.10,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.10,conll 2000 chunking task
v0.10,Support both TREC-6 and TREC-50
v0.10,Create flair compatible labels
v0.10,TREC-6 : NUM:dist -> __label__NUM
v0.10,TREC-50: NUM:dist -> __label__NUM:dist
v0.10,Wikiner NER task
v0.10,unpack and write out in CoNLL column-like format
v0.10,CoNLL 02/03 NER
v0.10,universal dependencies
v0.10,--- UD Germanic
v0.10,--- UD Romance
v0.10,--- UD West-Slavic
v0.10,--- UD Scandinavian
v0.10,--- UD South-Slavic
v0.10,--- UD Asian
v0.10,this is the default init size of a lmdb database for embeddings
v0.10,some non-used parameter to allow print
v0.10,get db filename from embedding name
v0.10,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.10,SequenceTagger
v0.10,TextClassifier
v0.10,get db filename from embedding name
v0.10,if embedding database already exists
v0.10,"otherwise, push embedding to database"
v0.10,if embedding database already exists
v0.10,open the database in read mode
v0.10,we need to set self.k
v0.10,create and load the database in write mode
v0.10,"no idea why, but we need to close and reopen the environment to avoid"
v0.10,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.10,when opening new transaction !
v0.10,init dictionaries
v0.10,"in order to deal with unknown tokens, add <unk>"
v0.10,set 'add_unk' if the dictionary was created with a version of Flair older than 0.9
v0.10,set 'add_unk' depending on whether <unk> is a key
v0.10,"if text is passed, instantiate sentence with tokens (words)"
v0.10,log a warning if the dataset is empty
v0.10,some sentences represent a document boundary (but most do not)
v0.10,data with zero-width characters cannot be handled
v0.10,set token idx if not set
v0.10,non-set tags are OUT tags
v0.10,anything that is not a BIOES tag is a SINGLE tag
v0.10,anything that is not OUT is IN
v0.10,single and begin tags start a new span
v0.10,remember previous tag
v0.10,"if label type is explicitly specified, get spans for this label type"
v0.10,else determine all label types in sentence and get all spans
v0.10,move sentence embeddings to device
v0.10,move token embeddings to device
v0.10,clear sentence embeddings
v0.10,clear token embeddings
v0.10,infer whitespace after field
v0.10,add Sentence labels to output if they exist
v0.10,add Token labels to output if they exist
v0.10,add Sentence labels to output if they exist
v0.10,add Token labels to output if they exist
v0.10,No character at the corresponding code point: remove it
v0.10,TODO: crude hack - replace with something better
v0.10,set name
v0.10,abort if no data is provided
v0.10,sample test data from train if none is provided
v0.10,sample dev data from train if none is provided
v0.10,set train dev and test data
v0.10,find out empty sentence indices
v0.10,create subset of non-empty sentence indices
v0.10,find out empty sentence indices
v0.10,create subset of non-empty sentence indices
v0.10,"if there are token labels of provided type, use these. Otherwise use sentence labels"
v0.10,check for labels of words
v0.10,if we are looking for sentence-level labels
v0.10,check if sentence itself has labels
v0.10,"if this is not a token-level prediction problem, add sentence-level labels to dictionary"
v0.10,Make the tag dictionary
v0.10,global variable: cache_root
v0.10,global variable: device
v0.10,global variable: embedding_storage_mode
v0.10,# dummy return to fulfill trainer.train() needs
v0.10,print(vec)
v0.10,Attach optimizer
v0.10,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.10,if memory mode option 'none' delete everything
v0.10,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.10,find out which ones are dynamic embeddings
v0.10,find out which ones are dynamic embeddings
v0.10,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.10,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.10,"in Flair <0.9.1, optimizer and scheduler used to train model are not saved"
v0.10,"write out a ""model card"" if one is set"
v0.10,special handling for optimizer: remember optimizer class and state dictionary
v0.10,save model
v0.10,restore optimizer and scheduler to model card if set
v0.10,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.10,see https://github.com/zalandoresearch/flair/issues/351
v0.10,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.10,loss calculation
v0.10,variables for printing
v0.10,variables for computing scores
v0.10,remove any previously predicted labels
v0.10,predict for batch
v0.10,get the gold labels
v0.10,add to all_predicted_values
v0.10,make printout lines
v0.10,"if the model is span-level, transfer to word-level annotations for printout"
v0.10,"all labels default to ""O"""
v0.10,set gold token-level
v0.10,set predicted token-level
v0.10,now print labels in CoNLL format
v0.10,check if there is a label mismatch
v0.10,print info
v0.10,write all_predicted_values to out_file if set
v0.10,make the evaluation dictionary
v0.10,"finally, compute numbers"
v0.10,"now, calculate evaluation numbers"
v0.10,there is at least one gold label or one prediction (default)
v0.10,issue error and default all evaluation numbers to 0.
v0.10,line for log file
v0.10,initialize the label dictionary
v0.10,set up multi-label logic
v0.10,loss weights and loss function
v0.10,Initialize the weight tensor
v0.10,filter empty sentences
v0.10,reverse sort all sequences by their length
v0.10,progress bar for verbosity
v0.10,stop if all sentences are empty
v0.10,remove previously predicted labels of this type
v0.10,if anything could possibly be predicted
v0.10,header for 'weights.txt'
v0.10,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.10,then get all relevant values from the tsv
v0.10,then get all relevant values from the tsv
v0.10,plot i
v0.10,save plots
v0.10,save plots
v0.10,plt.show()
v0.10,save plot
v0.10,auto-spawn on GPU if available
v0.10,remove previous embeddings
v0.10,clearing token embeddings to save memory
v0.10,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.10,#TODO: not saving lines yet
v0.10,== similarity measures ==
v0.10,helper class for ModelSimilarity
v0.10,-- works with binary cross entropy loss --
v0.10,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.10,-- works with ranking/triplet loss --
v0.10,normalize the embeddings
v0.10,== similarity losses ==
v0.10,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.10,TODO: this assumes eye matrix
v0.10,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.10,== similarity learner ==
v0.10,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.10,assumes that for each data pair there's at least one embedding per modality
v0.10,pre-compute embeddings for all targets in evaluation dataset
v0.10,compute the similarity
v0.10,sort the similarity matrix across modality 1
v0.10,"get the ranks, so +1 to start counting ranks from 1"
v0.10,The conversion from old model's constructor interface
v0.10,auto-spawn on GPU if available
v0.10,pad strings with whitespaces to longest sentence
v0.10,cut up the input into chunks of max charlength = chunk_size
v0.10,push each chunk through the RNN language model
v0.10,concatenate all chunks to make final output
v0.10,initial hidden state
v0.10,get predicted weights
v0.10,divide by temperature
v0.10,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.10,this makes a vector in which the largest value is 0
v0.10,compute word weights with exponential function
v0.10,try sampling multinomial distribution for next character
v0.10,print(word_idx)
v0.10,input ids
v0.10,push list of character IDs through model
v0.10,the target is always the next character
v0.10,use cross entropy loss to compare output of forward pass with targets
v0.10,exponentiate cross-entropy loss to calculate perplexity
v0.10,serialize the language models and the constructor arguments (but nothing else)
v0.10,special handling for deserializing language models
v0.10,re-initialize language model with constructor arguments
v0.10,copy over state dictionary to self
v0.10,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.10,"in their ""self.train()"" method)"
v0.10,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.10,"check if this is the case and if so, set it"
v0.10,Transform input data into TARS format
v0.10,print(all_labels)
v0.10,"if there are no labels, return a random sample as negatives"
v0.10,print(sample)
v0.10,"otherwise, go through all labels"
v0.10,make sure the probabilities always sum up to 1
v0.10,get and embed all labels by making a Sentence object that contains only the label text
v0.10,get each label embedding and scale between 0 and 1
v0.10,compute similarity matrix
v0.10,"the higher the similarity, the greater the chance that a label is"
v0.10,sampled as negative example
v0.10,make label dictionary if no Dictionary object is passed
v0.10,prepare dictionary of tags (without B- I- prefixes and without UNK)
v0.10,check if candidate_label_set is empty
v0.10,make list if only one candidate label is passed
v0.10,create label dictionary
v0.10,note current task
v0.10,create a temporary task
v0.10,make zero shot predictions
v0.10,switch to the pre-existing task
v0.10,prepare TARS dictionary
v0.10,initialize a bare-bones sequence tagger
v0.10,transformer separator
v0.10,Store task specific labels since TARS can handle multiple tasks
v0.10,make a tars sentence where all labels are O by default
v0.10,overwrite O labels with tags
v0.10,init new TARS classifier
v0.10,set all task information
v0.10,linear layers of internal classifier
v0.10,return
v0.10,with torch.no_grad():
v0.10,reverse sort all sequences by their length
v0.10,progress bar for verbosity
v0.10,stop if all sentences are empty
v0.10,go through each sentence in the batch
v0.10,always remove tags first
v0.10,get the span and its label
v0.10,determine whether tokens in this span already have a label
v0.10,only add if all tokens have no label
v0.10,clearing token embeddings to save memory
v0.10,prepare TARS dictionary
v0.10,initialize a bare-bones sequence tagger
v0.10,transformer separator
v0.10,Store task specific labels since TARS can handle multiple tasks
v0.10,init new TARS classifier
v0.10,set all task information
v0.10,linear layers of internal classifier
v0.10,with torch.no_grad():
v0.10,set context if not set already
v0.10,reverse sort all sequences by their length
v0.10,progress bar for verbosity
v0.10,stop if all sentences are empty
v0.10,go through each sentence in the batch
v0.10,always remove tags first
v0.10,add all labels that according to TARS match the text and are above threshold
v0.10,do not add labels below confidence threshold
v0.10,only use label with highest confidence if enforcing single-label predictions
v0.10,get all label scores and do an argmax to get the best label
v0.10,remove previously added labels and only add the best label
v0.10,clearing token embeddings to save memory
v0.10,if embed_separately == True the linear layer needs twice the length of the embeddings as input size
v0.10,since we concatenate the embeddings of the two DataPoints in the DataPairs
v0.10,representation for both sentences
v0.10,set separator to concatenate two sentences
v0.10,auto-spawn on GPU if available
v0.10,linear layer
v0.10,minimal return is scores and labels
v0.10,set embeddings
v0.10,set relation and entity label types
v0.10,"whether to use gold entity pairs, and whether to filter entity pairs by type"
v0.10,init dropouts
v0.10,pooling operation to get embeddings for entites
v0.10,"entity pairs could also be no relation at all, add default value for this case to dictionary"
v0.10,decoder can be linear or nonlinear
v0.10,super lame: make dictionary to find relation annotations for a given entity pair
v0.10,get all entity spans
v0.10,"go through cross product of entities, for each pair concat embeddings"
v0.10,filter entity pairs according to their tags if set
v0.10,get gold label for this relation (if one exists)
v0.10,"if there is no gold label for this entity pair, set to 'O' (no relation)"
v0.10,"if predicting, also remember sentences and label candidates"
v0.10,if there's at least one entity pair in the sentence
v0.10,embed sentences and get embeddings for each entity pair
v0.10,get embeddings
v0.10,stack and drop out (squeeze and unsqueeze)
v0.10,send through decoder
v0.10,"return either scores and gold labels (for loss calculation), or include label candidates for prediction"
v0.10,if we concatenate the embeddings we need double input size in our linear layer
v0.10,filter sentences with no candidates (no candidates means nothing can be linked anyway)
v0.10,fields to return
v0.10,"if the entire batch has no sentence with candidates, return empty"
v0.10,"otherwise, embed sentence and send through prediction head"
v0.10,embed all tokens
v0.10,get the embeddings of the entity mentions
v0.10,minimal return is scores and labels
v0.10,set the dictionaries
v0.10,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.10,Initialize the weight tensor
v0.10,initialize the network architecture
v0.10,dropouts
v0.10,optional reprojection layer on top of word embeddings
v0.10,bidirectional LSTM on top of embedding layer
v0.10,Create initial hidden state and initialize it
v0.10,TODO: Decide how to initialize the hidden state variables
v0.10,self.hs_initializer(self.lstm_init_h)
v0.10,self.hs_initializer(self.lstm_init_c)
v0.10,final linear map to tag space
v0.10,reverse sort all sequences by their length
v0.10,progress bar for verbosity
v0.10,stop if all sentences are empty
v0.10,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.10,clearing token embeddings to save memory
v0.10,--------------------------------------------------------------------
v0.10,FF PART
v0.10,--------------------------------------------------------------------
v0.10,"if initial hidden state is trainable, use this state"
v0.10,word dropout only before LSTM - TODO: more experimentation needed
v0.10,if self.use_word_dropout > 0.0:
v0.10,sentence_tensor = self.word_dropout(sentence_tensor)
v0.10,get the tags in this sentence
v0.10,add tags as tensor
v0.10,pad tags if using batch-CRF decoder
v0.10,reduce raw values to avoid NaN during exp
v0.10,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.10,default value
v0.10,core Flair models on Huggingface ModelHub
v0.10,"Large NER models,"
v0.10,Multilingual NER models
v0.10,English POS models
v0.10,Multilingual POS models
v0.10,English SRL models
v0.10,English chunking models
v0.10,Language-specific NER models
v0.10,English NER models
v0.10,Multilingual NER models
v0.10,English POS models
v0.10,Multilingual POS models
v0.10,English SRL models
v0.10,English chunking models
v0.10,Danish models
v0.10,German models
v0.10,French models
v0.10,Dutch models
v0.10,Malayalam models
v0.10,Portuguese models
v0.10,Keyphase models
v0.10,Biomedical models
v0.10,check if model name is a valid local file
v0.10,"check if model key is remapped to HF key - if so, print out information"
v0.10,get mapped name
v0.10,output information
v0.10,use mapped name instead
v0.10,"if not, check if model key is remapped to direct download location. If so, download model"
v0.10,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.10,"for all other cases (not local file or special download location), use HF model hub"
v0.10,"if not a local file, get from model hub"
v0.10,use model name as subfolder
v0.10,Lazy import
v0.10,output information
v0.10,"log.error(f"" - Error message: {e}"")"
v0.10,"all_tag_prob=all_tag_prob,"
v0.10,clear embeddings after predicting
v0.10,load each model
v0.10,check if the same embeddings were already loaded previously
v0.10,"if the model uses StackedEmbedding, make a new stack with previous objects"
v0.10,sort embeddings by key alphabetically
v0.10,check previous embeddings and add if found
v0.10,only re-use static embeddings
v0.10,"if not found, use existing embedding"
v0.10,initialize new stack
v0.10,"of the model uses regular embedding, re-load if previous version found"
v0.10,auto-spawn on GPU if available
v0.10,embed sentences
v0.10,make tensor for all embedded sentences in batch
v0.10,send through decoder to get logits
v0.10,minimal return is scores and labels
v0.10,English sentiment models
v0.10,Communicative Functions Model
v0.10,embeddings
v0.10,dictionaries
v0.10,linear layer
v0.10,all parameters will be pushed internally to the specified device
v0.10,get all tokens in this mini-batch
v0.10,minimal return is scores and labels
v0.10,weights for loss function
v0.10,iput size is two times wordembedding size since we use pair of words as input
v0.10,"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
v0.10,regression
v0.10,input size is two times word embedding size since we use pair of words as input
v0.10,the output size is 1
v0.10,auto-spawn on GPU if available
v0.10,all input should be tensors
v0.10,forward allows only a single sentcence!!
v0.10,embed words of sentence
v0.10,go through all pairs of words with a maximum number of max_distance in between
v0.10,go through all pairs
v0.10,2-dim matrix whose rows are the embeddings of word pairs of the sentence
v0.10,So far only one sentence allowed
v0.10,If list of sentences is handed the function works with the first sentence of the list
v0.10,Assume data_points is a single sentence!!!
v0.10,scores are the predictions for each word pair
v0.10,"classification needs labels to be integers, regression needs labels to be float"
v0.10,this is due to the different loss functions
v0.10,only single sentences as input
v0.10,gold labels
v0.10,for output text file
v0.10,for buckets
v0.10,for average prediction
v0.10,add some statistics to the output
v0.10,use scikit-learn to evaluate
v0.10,"we iterate over each sentence, instead of batches"
v0.10,get single labels from scores
v0.10,gold labels
v0.10,for output text file
v0.10,hot one vector of true value
v0.10,hot one vector of predicted value
v0.10,"speichert embeddings, falls embedding_storage!= 'None'"
v0.10,"make ""classification report"""
v0.10,get scores
v0.10,"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.10,"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.10,line for log file
v0.10,create a model card for this model with Flair and PyTorch version
v0.10,also record Transformers version if library is loaded
v0.10,remember all parameters used in train() call
v0.10,add model card to model
v0.10,cast string to Path
v0.10,check for previously saved best models in the current training folder and delete them
v0.10,"determine what splits (train, dev, test) to evaluate and log"
v0.10,prepare loss logging file and set up header
v0.10,"if optimizer class is passed, instantiate:"
v0.10,load existing optimizer state dictionary if it exists
v0.10,"minimize training loss if training with dev data, else maximize dev score"
v0.10,"if scheduler is passed as a class, instantiate"
v0.10,"if we load a checkpoint, we have already trained for epoch"
v0.10,load existing scheduler state dictionary if it exists
v0.10,update optimizer and scheduler in model card
v0.10,"if training also uses dev/train data, include in training set"
v0.10,initialize sampler if provided
v0.10,init with default values if only class is provided
v0.10,set dataset to sample from
v0.10,At any point you can hit Ctrl + C to break out of training early.
v0.10,update epoch in model card
v0.10,get new learning rate
v0.10,reload last best model if annealing with restarts is enabled
v0.10,stop training if learning rate becomes too small
v0.10,process mini-batches
v0.10,zero the gradients on the model and optimizer
v0.10,"if necessary, make batch_steps"
v0.10,forward and backward for batch
v0.10,forward pass
v0.10,Backward
v0.10,do the optimizer step
v0.10,do the scheduler step if one-cycle or linear decay
v0.10,get new learning rate
v0.10,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.10,evaluate on train / dev / test split depending on training settings
v0.10,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.10,calculate scores using dev data if available
v0.10,append dev score to score history
v0.10,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.10,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.10,determine if this is the best model or if we need to anneal
v0.10,default mode: anneal against dev score
v0.10,alternative: anneal against dev loss
v0.10,alternative: anneal against train loss
v0.10,determine bad epoch number
v0.10,log bad epochs
v0.10,output log file
v0.10,make headers on first epoch
v0.10,"if checkpoint is enabled, save model at each epoch"
v0.10,Check whether to save best model
v0.10,"if we do not use dev data for model selection, save final model"
v0.10,test best model if test data is present
v0.10,recover all arguments that were used to train this model
v0.10,you can overwrite params with your own
v0.10,surface nested arguments
v0.10,resume training with these parameters
v0.10,"if we are training over multiple datasets, do evaluation for each"
v0.10,get and return the final test score of best model
v0.10,cast string to Path
v0.10,forward pass
v0.10,update optimizer and scheduler
v0.10,append current loss to list of losses for all iterations
v0.10,compute averaged loss
v0.10,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.10,cast string to Path
v0.10,error message if the validation dataset is too small
v0.10,Shuffle training files randomly after serially iterating through corpus one
v0.10,"iterate through training data, starting at self.split (for checkpointing)"
v0.10,off by one for printing
v0.10,go into train mode
v0.10,reset variables
v0.10,not really sure what this does
v0.10,do the forward pass in the model
v0.10,try to predict the targets
v0.10,Backward
v0.10,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.10,We detach the hidden state from how it was previously produced.
v0.10,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.10,explicitly remove loss to clear up memory
v0.10,##############################################################################
v0.10,Save the model if the validation loss is the best we've seen so far.
v0.10,##############################################################################
v0.10,print info
v0.10,##############################################################################
v0.10,##############################################################################
v0.10,final testing
v0.10,##############################################################################
v0.10,Turn on evaluation mode which disables dropout.
v0.10,Work out how cleanly we can divide the dataset into bsz parts.
v0.10,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.10,Evenly divide the data across the bsz batches.
v0.10,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.10,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.10,news-english-forward
v0.10,news-english-backward
v0.10,news-english-forward
v0.10,news-english-backward
v0.10,mix-english-forward
v0.10,mix-english-backward
v0.10,mix-german-forward
v0.10,mix-german-backward
v0.10,common crawl Polish forward
v0.10,common crawl Polish backward
v0.10,Slovenian forward
v0.10,Slovenian backward
v0.10,Bulgarian forward
v0.10,Bulgarian backward
v0.10,Dutch forward
v0.10,Dutch backward
v0.10,Swedish forward
v0.10,Swedish backward
v0.10,French forward
v0.10,French backward
v0.10,Czech forward
v0.10,Czech backward
v0.10,Portuguese forward
v0.10,Portuguese backward
v0.10,initialize cache if use_cache set
v0.10,embed a dummy sentence to determine embedding_length
v0.10,set to eval mode
v0.10,Copy the object's state from self.__dict__ which contains
v0.10,all our instance attributes. Always use the dict.copy()
v0.10,method to avoid modifying the original state.
v0.10,Remove the unpicklable entries.
v0.10,"if cache is used, try setting embeddings from cache first"
v0.10,try populating embeddings from cache
v0.10,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.10,get hidden states from language model
v0.10,take first or last hidden states from language model as word representation
v0.10,if self.tokenized_lm or token.whitespace_after:
v0.10,1-camembert-base -> camembert-base
v0.10,1-xlm-roberta-large -> xlm-roberta-large
v0.10,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.10,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.10,tokens are attended to.
v0.10,Zero-pad up to the sequence length.
v0.10,"first, find longest sentence in batch"
v0.10,prepare id maps for BERT model
v0.10,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.10,get aggregated embeddings for each BERT-subtoken in sentence
v0.10,get the current sentence object
v0.10,add concatenated embedding to sentence
v0.10,use first subword embedding if pooling operation is 'first'
v0.10,"otherwise, do a mean over all subwords in token"
v0.10,"if only one sentence is passed, convert to list of sentence"
v0.10,bidirectional LSTM on top of embedding layer
v0.10,dropouts
v0.10,"first, sort sentences by number of tokens"
v0.10,go through each sentence in batch
v0.10,PADDING: pad shorter sentences out
v0.10,ADD TO SENTENCE LIST: add the representation
v0.10,--------------------------------------------------------------------
v0.10,GET REPRESENTATION FOR ENTIRE BATCH
v0.10,--------------------------------------------------------------------
v0.10,--------------------------------------------------------------------
v0.10,FF PART
v0.10,--------------------------------------------------------------------
v0.10,use word dropout if set
v0.10,--------------------------------------------------------------------
v0.10,EXTRACT EMBEDDINGS FROM LSTM
v0.10,--------------------------------------------------------------------
v0.10,embed a dummy sentence to determine embedding_length
v0.10,Avoid conflicts with flair's Token class
v0.10,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.10,add positional encodings
v0.10,reshape the pixels into the sequence
v0.10,layer norm after convolution and positional encodings
v0.10,add <cls> token
v0.10,"transformer requires input in the shape [h*w+1, b, d]"
v0.10,the output is an embedding of <cls> token
v0.10,temporary fix to disable tokenizer parallelism warning
v0.10,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.10,do not print transformer warnings as these are confusing in this case
v0.10,load tokenizer and transformer model
v0.10,model name
v0.10,"when initializing, embeddings are in eval mode by default"
v0.10,embedding parameters
v0.10,send mini-token through to check how many layers the model has
v0.10,check whether CLS is at beginning or end
v0.10,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.10,gradients are enabled if fine-tuning is enabled
v0.10,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.10,subtokenize sentences
v0.10,tokenize and truncate to max subtokens (TODO: check better truncation strategies)
v0.10,find longest sentence in batch
v0.10,initialize batch tensors and mask
v0.10,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.10,iterate over all subtokenized sentences
v0.10,use scalar mix of embeddings if so selected
v0.10,set the extracted embedding for the token
v0.10,special handling for serializing transformer models
v0.10,serialize the transformer models and the constructor arguments (but nothing else)
v0.10,necessary for reverse compatibility with Flair <= 0.7
v0.10,special handling for deserializing transformer models
v0.10,load transformer model
v0.10,constructor arguments
v0.10,re-initialize transformer word embeddings with constructor arguments
v0.10,for backward compatibility with previous models
v0.10,"I have no idea why this is necessary, but otherwise it doesn't work"
v0.10,reload tokenizer to get around serialization issues
v0.10,optional fine-tuning on top of embedding layer
v0.10,"if only one sentence is passed, convert to list of sentence"
v0.10,"if only one sentence is passed, convert to list of sentence"
v0.10,bidirectional RNN on top of embedding layer
v0.10,dropouts
v0.10,TODO: remove in future versions
v0.10,embed words in the sentence
v0.10,before-RNN dropout
v0.10,reproject if set
v0.10,push through RNN
v0.10,after-RNN dropout
v0.10,extract embeddings from RNN
v0.10,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.10,"check if this is the case and if so, set it"
v0.10,serialize the language models and the constructor arguments (but nothing else)
v0.10,special handling for deserializing language models
v0.10,re-initialize language model with constructor arguments
v0.10,copy over state dictionary to self
v0.10,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.10,"in their ""self.train()"" method)"
v0.10,IMPORTANT: add embeddings as torch modules
v0.10,iterate over sentences
v0.10,"if its a forward LM, take last state"
v0.10,"convert to plain strings, embedded in a list for the encode function"
v0.10,CNN
v0.10,dropouts
v0.10,TODO: remove in future versions
v0.10,embed words in the sentence
v0.10,before-RNN dropout
v0.10,reproject if set
v0.10,push CNN
v0.10,after-CNN dropout
v0.10,extract embeddings from CNN
v0.10,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.10,"if only one sentence is passed, convert to list of sentence"
v0.10,Expose base classses
v0.10,Expose token embedding classes
v0.10,Expose document embedding classes
v0.10,Expose image embedding classes
v0.10,Expose legacy embedding classes
v0.10,IMPORTANT: add embeddings as torch modules
v0.10,"if only one sentence is passed, convert to list of sentence"
v0.10,GLOVE embeddings
v0.10,TURIAN embeddings
v0.10,KOMNINOS embeddings
v0.10,pubmed embeddings
v0.10,FT-CRAWL embeddings
v0.10,FT-CRAWL embeddings
v0.10,twitter embeddings
v0.10,two-letter language code wiki embeddings
v0.10,two-letter language code wiki embeddings
v0.10,two-letter language code crawl embeddings
v0.10,gensim version 4
v0.10,gensim version 3
v0.10,fix serialized models
v0.10,"this is required to force the module on the cpu,"
v0.10,"if a parent module is put to gpu, the _apply is called to each sub_module"
v0.10,self.to(..) actually sets the device properly
v0.10,this ignores the get_cached_vec method when loading older versions
v0.10,it is needed for compatibility reasons
v0.10,gensim version 4
v0.10,gensim version 3
v0.10,use list of common characters if none provided
v0.10,translate words in sentence into ints using dictionary
v0.10,"sort words by length, for batching and masking"
v0.10,chars for rnn processing
v0.10,multilingual models
v0.10,English models
v0.10,Arabic
v0.10,Bulgarian
v0.10,Czech
v0.10,Danish
v0.10,German
v0.10,Spanish
v0.10,Basque
v0.10,Persian
v0.10,Finnish
v0.10,French
v0.10,Hebrew
v0.10,Hindi
v0.10,Croatian
v0.10,Indonesian
v0.10,Italian
v0.10,Japanese
v0.10,Malayalam
v0.10,Dutch
v0.10,Norwegian
v0.10,Polish
v0.10,Portuguese
v0.10,Pubmed
v0.10,Slovenian
v0.10,Swedish
v0.10,Tamil
v0.10,Spanish clinical
v0.10,CLEF HIPE Shared task
v0.10,Amharic
v0.10,load model if in pretrained model map
v0.10,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.10,CLEF HIPE models are lowercased
v0.10,embeddings are static if we don't do finetuning
v0.10,embed a dummy sentence to determine embedding_length
v0.10,set to eval mode
v0.10,make compatible with serialized models (TODO: remove)
v0.10,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.10,make compatible with serialized models (TODO: remove)
v0.10,gradients are enable if fine-tuning is enabled
v0.10,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.10,get hidden states from language model
v0.10,take first or last hidden states from language model as word representation
v0.10,offset mode that extracts at whitespace after last character
v0.10,offset mode that extracts at last character
v0.10,only clone if optimization mode is 'gpu'
v0.10,use the character language model embeddings as basis
v0.10,length is twice the original character LM embedding length
v0.10,these fields are for the embedding memory
v0.10,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.10,we re-compute embeddings dynamically at each epoch
v0.10,set the memory method
v0.10,memory is wiped each time we do a training run
v0.10,"if we keep a pooling, it needs to be updated continuously"
v0.10,update embedding
v0.10,check token.text is empty or not
v0.10,set aggregation operation
v0.10,add embeddings after updating
v0.10,temporary fix to disable tokenizer parallelism warning
v0.10,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.10,do not print transformer warnings as these are confusing in this case
v0.10,load tokenizer and transformer model
v0.10,"in the end, these models don't need this configuration"
v0.10,model name
v0.10,whether to detach gradients on overlong sentences
v0.10,store whether to use context (and how much)
v0.10,dropout contexts
v0.10,"if using context, can we cross document boundaries?"
v0.10,send self to flair-device
v0.10,embedding parameters
v0.10,send mini-token through to check how many layers the model has
v0.10,calculate embedding length
v0.10,return length
v0.10,check if special tokens exist to circumvent error message
v0.10,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.10,"when initializing, embeddings are in eval mode by default"
v0.10,remove special markup
v0.10,"we require encoded subtokenized sentences, the mapping to original tokens and the number of"
v0.10,parts that each sentence produces
v0.10,"if we also use context, first expand sentence to include context"
v0.10,set context if not set already
v0.10,"in case of contextualization, we must remember non-expanded sentence"
v0.10,create expanded sentence and remember context offsets
v0.10,overwrite sentence with expanded sentence
v0.10,subtokenize the sentence
v0.10,transformer specific tokenization
v0.10,set zero embeddings for empty sentences and exclude
v0.10,determine into how many subtokens each token is split
v0.10,remember tokenized sentences and their subtokenization
v0.10,encode inputs
v0.10,Models such as FNet do not have an attention_mask
v0.10,determine which sentence was split into how many parts
v0.10,set language IDs for XLM-style transformers
v0.10,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.10,make the tuple a tensor; makes working with it easier.
v0.10,gradients are enabled if fine-tuning is enabled
v0.10,iterate over all subtokenized sentences
v0.10,"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
v0.10,in order to get some context into the embeddings of these words.
v0.10,also don't include the embedding of the extra [CLS] and [SEP] tokens.
v0.10,"for each token, get embedding"
v0.10,some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
v0.10,"get states from all selected layers, aggregate with pooling operation"
v0.10,use layer mean of embeddings if so selected
v0.10,set the extracted embedding for the token
v0.10,move embeddings from context back to original sentence (if using context)
v0.10,remember original sentence
v0.10,get left context
v0.10,get right context
v0.10,empty contexts should not introduce whitespace tokens
v0.10,make expanded sentence
v0.10,iterate over subtokens and reconstruct tokens
v0.10,remove special markup
v0.10,TODO check if this is necessary is this method is called before prepare_for_model
v0.10,check if reconstructed token is special begin token ([CLS] or similar)
v0.10,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.10,append subtoken to reconstruct token
v0.10,check if reconstructed token is the same as current token
v0.10,"if so, add subtoken count"
v0.10,reset subtoken count and reconstructed token
v0.10,break from loop if all tokens are accounted for
v0.10,if tokens are unaccounted for
v0.10,check if all tokens were matched to subtokens
v0.10,"""""""Returns the length of the embedding vector."""""""
v0.10,special handling for serializing transformer models
v0.10,serialize the transformer models and the constructor arguments (but nothing else)
v0.10,necessary for reverse compatibility with Flair <= 0.7
v0.10,special handling for deserializing transformer models
v0.10,load transformer model
v0.10,constructor arguments
v0.10,re-initialize transformer word embeddings with constructor arguments
v0.10,"I have no idea why this is necessary, but otherwise it doesn't work"
v0.10,reload tokenizer to get around serialization issues
v0.10,model architecture
v0.10,model architecture
v0.10,"""pl"","
v0.10,download if necessary
v0.10,load the model
v0.10,"TODO: keep for backwards compatibility, but remove in future"
v0.10,save the sentence piece model as binary file (not as path which may change)
v0.10,write out the binary sentence piece model into the expected directory
v0.10,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.10,"otherwise, use normal process and potentially trigger another download"
v0.10,"once the modes if there, load it with sentence piece"
v0.10,empty words get no embedding
v0.10,all other words get embedded
v0.10,"the default model for ELMo is the 'original' model, which is very large"
v0.10,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.10,put on Cuda if available
v0.10,embed a dummy sentence to determine embedding_length
v0.10,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.10,GLOVE embeddings
v0.10,get train data
v0.10,read in test file if exists
v0.10,read in dev file if exists
v0.10,"find train, dev and test files if not specified"
v0.10,special key for space after
v0.10,"store either Sentence objects in memory, or only file offsets"
v0.10,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.10,determine encoding of text file
v0.10,skip first line if to selected
v0.10,option 1: read only sentence boundaries as offset positions
v0.10,option 2: keep everything in memory
v0.10,pointer to previous
v0.10,"if sentence ends, break"
v0.10,skip comments
v0.10,"if sentence ends, convert and return"
v0.10,check if this sentence is a document boundary
v0.10,"otherwise, this line is a token. parse and add to sentence"
v0.10,check if this sentence is a document boundary
v0.10,"for example, transforming 'B-OBJ' to 'B-part-of-speech-object'"
v0.10,"if in memory, retrieve parsed sentence"
v0.10,else skip to position in file where sentence begins
v0.10,set sentence context using partials
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,check if data there
v0.10,code-switch uses the same training data than multi but provides a different test set.
v0.10,"as the test set is not published, those two tasks are the same."
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,check if data there
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,check if data there
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download files if not present locally
v0.10,we need to slightly modify the original files by adding some new lines after document separators
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,Remove CoNLL-U meta information in the last column
v0.10,column format
v0.10,dataset name
v0.10,data folder: default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,dataset name
v0.10,data folder: default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,entity_mapping
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,data validation
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download files if not present locallys
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,# download zip
v0.10,merge the files in one as the zip is containing multiples files
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,check if data there
v0.10,create folder
v0.10,download dataset
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download and parse data if necessary
v0.10,create train test dev if not exist
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,If the extracted corpus file is not yet present in dir
v0.10,download zip if necessary
v0.10,"extracted corpus is not present , so unpacking it."
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download zip
v0.10,unpacking the zip
v0.10,merge the files in one as the zip is containing multiples files
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.10,download files if not present locally
v0.10,we need to modify the original files by adding new lines after after the end of each sentence
v0.10,if only one language is given
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,"use all languages if explicitly set to ""all"""
v0.10,download data if necessary
v0.10,initialize comlumncorpus and add it to list
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,"For each language in languages, the file is downloaded if not existent"
v0.10,Then a comlumncorpus of that data is created and saved in a list
v0.10,this list is handed to the multicorpus
v0.10,list that contains the columncopora
v0.10,download data if necessary
v0.10,"if language not downloaded yet, download it"
v0.10,create folder
v0.10,get google drive id from list
v0.10,download from google drive
v0.10,unzip
v0.10,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.10,transform data into required format
v0.10,"the processed dataset has the additional ending ""_new"""
v0.10,remove the unprocessed dataset
v0.10,initialize comlumncorpus and add it to list
v0.10,if no languages are given as argument all languages used in XTREME will be loaded
v0.10,if only one language is given
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,"For each language in languages, the file is downloaded if not existent"
v0.10,Then a comlumncorpus of that data is created and saved in a list
v0.10,This list is handed to the multicorpus
v0.10,list that contains the columncopora
v0.10,download data if necessary
v0.10,"if language not downloaded yet, download it"
v0.10,create folder
v0.10,download from HU Server
v0.10,unzip
v0.10,transform data into required format
v0.10,initialize comlumncorpus and add it to list
v0.10,if only one language is given
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,initialize comlumncorpus and add it to list
v0.10,download data if necessary
v0.10,unpack and write out in CoNLL column-like format
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,data is not in IOB2 format. Thus we transform it to IOB2
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,rename according to train - test - dev - convention
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,"TODO: Add a routine, that checks annotations for some widespread errors/inconsistencies??? (e.g. in AQUAINT corpus Iran-Iraq_War vs. Iran-Iraq_war)"
v0.10,Create the annotation dictionary
v0.10,this fct removes every second unknown label
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download and parse data if necessary
v0.10,iterate over all html files
v0.10,"get rid of html syntax, we only need the text"
v0.10,between all documents we write a separator symbol
v0.10,skip empty strings
v0.10,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.10,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.10,sentence splitting and tokenization
v0.10,iterate through all annotations and add to corresponding tokens
v0.10,find sentence to which annotation belongs
v0.10,position within corresponding sentence
v0.10,set annotation for tokens of entity mention
v0.10,write to out-file in column format
v0.10,"in case something goes wrong, delete the dataset and raise error"
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download and parse data if necessary
v0.10,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.10,generate qid wikiname dictionaries
v0.10,merge dictionaries
v0.10,ignore first line
v0.10,commented and empty lines
v0.10,read all Q-IDs
v0.10,ignore first line
v0.10,request
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.10,like this we can quickly check if the corresponding page exists
v0.10,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.10,delete unprocessed file
v0.10,collect all wikiids
v0.10,create the dictionary
v0.10,request
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,names of raw text documents
v0.10,open output_file
v0.10,iterate through all documents
v0.10,split sentences and tokenize
v0.10,iterate through all annotations and add to corresponding tokens
v0.10,find sentence to which annotation belongs
v0.10,position within corresponding sentence
v0.10,set annotation for tokens of entity mention
v0.10,write to out file
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download and parse data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download and parse data if necessary
v0.10,First parse the post titles
v0.10,Keep track of how many and which entity mentions does a given post title have
v0.10,Check if the current post title has an entity link and parse accordingly
v0.10,Post titles with entity mentions (if any) are handled via this function
v0.10,Then parse the comments
v0.10,"Iterate over the comments.tsv file, until the end is reached"
v0.10,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.10,Each comment thread is handled as one 'document'.
v0.10,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.10,This if-condition is needed to handle this problem.
v0.10,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.10,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.10,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.10,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.10,and not just single letters into single rows.
v0.10,If there are annotated entity mentions for given post title or a comment thread
v0.10,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.10,Write the token with a corresponding tag to file
v0.10,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.10,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.10,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.10,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.10,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.10,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.10,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.10,Check if further annotations belong to the current post title or comment thread as well
v0.10,Stop when the end of an annotation file is reached
v0.10,Check if further annotations belong to the current sentence as well
v0.10,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.10,Docstart
v0.10,if there is more than one word in the chunk we write each in a separate line
v0.10,print(chunks)
v0.10,empty line after each sentence
v0.10,convert the file to CoNLL
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,"check if data there, if not, download the data"
v0.10,create folder
v0.10,download data
v0.10,transform data into column format if necessary
v0.10,if no filenames are specified we use all the data
v0.10,"in this case no test data should be generated by sampling from train data. But if the sample arguments are set to true, the dev set will be sampled"
v0.10,also we remove 'raganato_ALL' from filenames in case its in the list
v0.10,generate the test file
v0.10,make column file and save to data_folder
v0.10,default dataset folder is the cache root
v0.10,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.10,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.10,create folder
v0.10,download data
v0.10,default dataset folder is the cache root
v0.10,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.10,Note that this downloads more datasets than just SemCor. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.10,create folder
v0.10,download data
v0.10,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.10,generate the test file
v0.10,default dataset folder is the cache root
v0.10,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.10,Note that this downloads more datasets than just WordNet Gloss Tagged. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.10,create folder
v0.10,download data
v0.10,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.10,generate the test file
v0.10,default dataset folder is the cache root
v0.10,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.10,Note that this downloads more datasets than just MASC. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.10,create folder
v0.10,download data
v0.10,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.10,generate the test file
v0.10,default dataset folder is the cache root
v0.10,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.10,Note that this downloads more datasets than just OMSTI. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.10,create folder
v0.10,download data
v0.10,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.10,generate the test file
v0.10,default dataset folder is the cache root
v0.10,"We check if the the UFSAC data has already been downloaded. If not, we download it."
v0.10,Note that this downloads more datasets than just Train-O-Matic. But the size of the download is only around 190 Mb (around 4.5 Gb unpacked)
v0.10,create folder
v0.10,download data
v0.10,"in this case no test data should be generated by sampling from train data. But if sample_missing_splits is true, the dev set will be sampled."
v0.10,generate the test file
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,if True:
v0.10,write CoNLL-U Plus header
v0.10,"Some special cases (e.g., missing spaces before entity marker)"
v0.10,necessary if text should be whitespace tokenizeable
v0.10,Handle case where tail may occur before the head
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,write CoNLL-U Plus header
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.10,download data if necessary
v0.10,write CoNLL-U Plus header
v0.10,The span has ended.
v0.10,We are entering a new span; reset indices
v0.10,and active tag to new span.
v0.10,We're inside a span.
v0.10,Last token might have been a part of a valid span.
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,write CoNLL-U Plus header
v0.10,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.10,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.10,target_file_path = Path(data_folder) / target_filename
v0.10,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.10,# write CoNLL-U Plus header
v0.10,"target_file.write(""# global.columns = id form ner\n"")"
v0.10,for example in json.load(source_file):
v0.10,token_list = self._tacred_example_to_token_list(example)
v0.10,target_file.write(token_list.serialize())
v0.10,check if first tag row is already occupied
v0.10,"if first tag row is occupied, use second tag row"
v0.10,hardcoded mapping TODO: perhaps find nicer solution
v0.10,"find train, dev and test files if not specified"
v0.10,use test_file to create test split if available
v0.10,use dev_file to create test split if available
v0.10,"if data point contains black-listed label, do not use"
v0.10,first check if valid sentence
v0.10,"if so, add to indices"
v0.10,"find train, dev and test files if not specified"
v0.10,variables
v0.10,different handling of in_memory data than streaming data
v0.10,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.10,test if format is OK
v0.10,test if at least one label given
v0.10,make sentence from text (and filter for length)
v0.10,"if a pair column is defined, make a sentence pair object"
v0.10,noinspection PyDefaultArgument
v0.10,dataset name includes the split size
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download each of the 28 splits
v0.10,create dataset directory if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,create dataset directory if necessary
v0.10,create train.txt file from CSV
v0.10,create test.txt file from CSV
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,create dataset directory if necessary
v0.10,create train.txt file by iterating over pos and neg file
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,create dataset directory if necessary
v0.10,create train.txt file by iterating over pos and neg file
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,create dataset directory if necessary
v0.10,create train.txt file by iterating over pos and neg file
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,create dataset directory if necessary
v0.10,create train.txt file by iterating over pos and neg file
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,create train dev and test files in fasttext format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download senteval datasets if necessary und unzip
v0.10,convert to FastText format
v0.10,if no base_path provided take cache root
v0.10,download data if necessary
v0.10,"if data is not downloaded yet, download it"
v0.10,get the zip file
v0.10,move original .tsv files to another folder
v0.10,create train and dev splits in fasttext format
v0.10,create eval_dataset file with no labels
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,download datasets if necessary
v0.10,create dataset directory if necessary
v0.10,create correctly formated txt files
v0.10,multiple labels are possible
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,Create flair compatible labels
v0.10,TREC-6 : NUM:dist -> __label__NUM
v0.10,TREC-50: NUM:dist -> __label__NUM:dist
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,Create flair compatible labels
v0.10,TREC-6 : NUM:dist -> __label__NUM
v0.10,TREC-50: NUM:dist -> __label__NUM:dist
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,create a separate directory for different tasks
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,check if dataset is supported
v0.10,set file names
v0.10,set file names
v0.10,download and unzip in file structure if necessary
v0.10,instantiate corpus
v0.10,"find train, dev and test files if not specified"
v0.10,"create DataPairDataset for train, test and dev file, if they are given"
v0.10,stop if file does not exist
v0.10,create a DataPair object from strings
v0.10,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.10,if no base_path provided take cache root
v0.10,"if data is not downloaded yet, download it"
v0.10,get the zip file
v0.10,"rename test file to eval_dataset, since it has no labels"
v0.10,if no base_path provided take cache root
v0.10,"if data is not downloaded yet, download it"
v0.10,get the zip file
v0.10,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.10,dev sets include 5 different annotations but we will only keep the gold label
v0.10,"rename test file to eval_dataset, since it has no labels"
v0.10,if no base_path provided take cache root
v0.10,"if data is not downloaded yet, download it"
v0.10,get test and dev sets
v0.10,if no base_path provided take cache root
v0.10,"if data is not downloaded yet, download it"
v0.10,get the zip file
v0.10,"rename test file to eval_dataset, since it has no labels"
v0.10,if no base_path provided take cache root
v0.10,"if data is not downloaded yet, download it"
v0.10,get the zip file
v0.10,"rename test file to eval_dataset, since it has no labels"
v0.10,if no base_path provided take cache root
v0.10,"if data is not downloaded yet, download it"
v0.10,get the zip file
v0.10,"rename test file to eval_dataset, since it has no labels"
v0.10,if no base_path provided take cache root
v0.10,"if data not downloaded yet, download it"
v0.10,get the zip file
v0.10,"the downloaded files have json format, we transform them to tsv"
v0.10,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.10,remove json file
v0.10,noinspection PyProtectedMember
v0.10,noinspection PyProtectedMember
v0.10,"find train, dev and test files if not specified"
v0.10,get train data
v0.10,get test data
v0.10,get dev data
v0.10,noinspection PyProtectedMember
v0.10,"if no fields specified, check if the file is CoNLL plus formatted and get fields"
v0.10,Validate fields and token_annotation_fields
v0.10,noinspection PyProtectedMember
v0.10,option 1: read only sentence boundaries as offset positions
v0.10,option 2: keep everything in memory
v0.10,pointer to previous
v0.10,"if in memory, retrieve parsed sentence"
v0.10,else skip to position in file where sentence begins
v0.10,Build the sentence tokens and add the annotations.
v0.10,"For fields that contain key-value annotations,"
v0.10,we add the key as label type-name and the value as the label value.
v0.10,head and tail span indices are 1-indexed and end index is inclusive
v0.10,determine all NER label types in sentence and add all NER spans as sentence-level labels
v0.10,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.10,with sum of all entity lengths as secondary key
v0.10,calculate offset without current text
v0.10,because we stick all passages of a document together
v0.10,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.10,Try to fix incorrect annotations
v0.10,print(
v0.10,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.10,)
v0.10,Ignore empty lines or relation annotations
v0.10,FIX annotation of whitespaces (necessary for PDR)
v0.10,One token may contain multiple entities -> deque all of them
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.10,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,Edge case: last token starts a new entity
v0.10,Last document in file
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,In the huner split files there is no information whether a given id originates
v0.10,from the train or test file of the original corpus - so we have to adapt corpus
v0.10,splitting here
v0.10,In the huner split files there is no information whether a given id originates
v0.10,from the train or test file of the original corpus - so we have to adapt corpus
v0.10,splitting here
v0.10,In the huner split files there is no information whether a given id originates
v0.10,from the train or test file of the original corpus - so we have to adapt corpus
v0.10,splitting here
v0.10,Edge case: last token starts a new entity
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download file is huge => make default_dir visible so that derivative
v0.10,corpora can all use the same download file
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,Read texts
v0.10,Read annotations
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,We need to apply a patch to correct the original training file
v0.10,Articles title
v0.10,Article abstract
v0.10,Entity annotations
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,Edge case: last token starts a new entity
v0.10,Map all entities to chemicals
v0.10,Map all entities to disease
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,Incomplete article
v0.10,Invalid XML syntax
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,if len(mid) != 3:
v0.10,continue
v0.10,Try to fix entity offsets
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,There is still one illegal annotation in the file ..
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,"Abstract first, title second to prevent issues with sentence splitting"
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,"Filter for specific entity types, by default no entities will be filtered"
v0.10,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.10,train and dev split of V2 will be train in V4
v0.10,test split of V2 will be dev in V4
v0.10,New documents in V4 will become test documents
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,column format
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,cache Feidegger config file
v0.10,cache Feidegger images
v0.10,replace image URL with local cached file
v0.10,append Sentence-Image data point
v0.10,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.10,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.10,"if so, num_workers is set to 0 for faster processing"
v0.10,cast to list if necessary
v0.10,cast to list if necessary
v0.10,"first, check if pymongo is installed"
v0.10,automatically identify train / test / dev files
v0.10,"if no test file is found, take any file with 'test' in name"
v0.10,Expose base classses
v0.10,Expose all sequence labeling datasets
v0.10,standard NER datasets
v0.10,other NER datasets
v0.10,keyphrase detection datasets
v0.10,universal proposition banks
v0.10,Expose all entity linking datasets
v0.10,word sense disambiguation
v0.10,Expose all document classification datasets
v0.10,Expose all treebanks
v0.10,Expose all text-text datasets
v0.10,Expose all text-image datasets
v0.10,Expose all biomedical data sets
v0.10,Expose all biomedical data sets using the HUNER splits
v0.10,-
v0.10,-
v0.10,-
v0.10,-
v0.10,Expose all biomedical data sets used for the evaluation of BioBERT
v0.10,Expose all relation extraction datasets
v0.10,"find train, dev and test files if not specified"
v0.10,get train data
v0.10,get test data
v0.10,get dev data
v0.10,option 1: read only sentence boundaries as offset positions
v0.10,option 2: keep everything in memory
v0.10,"if in memory, retrieve parsed sentence"
v0.10,else skip to position in file where sentence begins
v0.10,current token ID
v0.10,handling for the awful UD multiword format
v0.10,end of sentence
v0.10,comments
v0.10,ellipsis
v0.10,if token is a multi-word
v0.10,normal single-word tokens
v0.10,"if we don't split multiwords, skip over component words"
v0.10,add token
v0.10,add morphological tags
v0.10,derive whitespace logic for multiwords
v0.10,print(token)
v0.10,print(current_multiword_last_token)
v0.10,print(current_multiword_first_token)
v0.10,"if multi-word equals component tokens, there should be no whitespace"
v0.10,go through all tokens in subword and set whitespace_after information
v0.10,print(i)
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,this dataset name
v0.10,default dataset folder is the cache root
v0.10,download data if necessary
v0.10,"finally, print model card for information"
v0.10,test corpus
v0.10,create a TARS classifier
v0.10,check if right number of classes
v0.10,switch to task with only one label
v0.10,check if right number of classes
v0.10,switch to task with three labels provided as list
v0.10,check if right number of classes
v0.10,switch to task with four labels provided as set
v0.10,check if right number of classes
v0.10,switch to task with two labels provided as Dictionary
v0.10,check if right number of classes
v0.10,test corpus
v0.10,create a TARS classifier
v0.10,switch to a new task (TARS can do multiple tasks so you must define one)
v0.10,initialize the text classifier trainer
v0.10,start the training
v0.10,"mini_batch_chunk_size=4,  # optionally set this if transformer is too much for your machine"
v0.10,clean up file
v0.10,bioes tags
v0.10,bio tags
v0.10,broken tags
v0.10,all tags
v0.10,all weird tags
v0.10,tags with confidence
v0.10,bioes tags
v0.10,bioes tags
v0.10,"city single-token, person and company multi-token"
v0.10,increment for last token in sentence if not followed by whitespace
v0.10,clean up directory
v0.10,clean up directory
v0.10,initialize trainer
v0.10,clean up results directory
v0.10,initialize trainer
v0.10,clean up results directory
v0.10,initialize trainer
v0.10,initialize trainer
v0.10,clean up results directory
v0.10,initialize trainer
v0.10,clean up results directory
v0.10,initialize trainer
v0.10,clean up results directory
v0.10,initialize trainer
v0.10,clean up results directory
v0.10,train model for 2 epochs
v0.10,load the checkpoint model and train until epoch 4
v0.10,clean up results directory
v0.10,initialize trainer
v0.10,clean up results directory
v0.10,from flair.trainers.trainer_regression import RegressorTrainer
v0.10,def test_trainer_evaluation(tasks_base_path):
v0.10,"corpus, model, trainer = init(tasks_base_path)"
v0.10,
v0.10,expected = model.evaluate(corpus.dev)
v0.10,
v0.10,assert expected is not None
v0.10,def test_trainer_results(tasks_base_path):
v0.10,"corpus, model, trainer = init(tasks_base_path)"
v0.10,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.10,"assert results[""test_score""] > 0"
v0.10,"assert len(results[""dev_loss_history""]) == 1"
v0.10,"assert len(results[""dev_score_history""]) == 1"
v0.10,"assert len(results[""train_loss_history""]) == 1"
v0.10,@pytest.mark.integration
v0.10,initialize trainer
v0.10,"loaded_model.predict([sentence, sentence_empty])"
v0.10,loaded_model.predict([sentence_empty])
v0.10,clean up results directory
v0.10,get default dictionary
v0.10,init forward LM with 128 hidden states and 1 layer
v0.10,get the example corpus and process at character level in forward direction
v0.10,train the language model
v0.10,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.10,clean up results directory
v0.10,get default dictionary
v0.10,init forward LM with 128 hidden states and 1 layer
v0.10,get the example corpus and process at character level in forward direction
v0.10,train the language model
v0.10,clean up results directory
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,"get training, test and dev data"
v0.10,get two corpora as one
v0.10,"get training, test and dev data for full English UD corpus from web"
v0.10,clean up data directory
v0.10,"Here, we use the default token annotation fields."
v0.10,load dataset
v0.10,tagger without CRF
v0.10,train
v0.10,check if loaded model can predict
v0.10,check if loaded model successfully fit the training data
v0.10,clean up results directory
v0.10,load dataset
v0.10,tagger without CRF
v0.10,train
v0.10,check if loaded model can predict
v0.10,check if loaded model successfully fit the training data
v0.10,clean up results directory
v0.10,load dataset
v0.10,tagger without CRF
v0.10,train
v0.10,check if loaded model can predict
v0.10,check if loaded model successfully fit the training data
v0.10,clean up results directory
v0.10,load dataset
v0.10,tagger without CRF
v0.10,train
v0.10,check if loaded model can predict
v0.10,check if loaded model successfully fit the training data
v0.10,clean up results directory
v0.10,check if model can predict
v0.10,load model
v0.10,chcek if model predicts correct label
v0.10,check if loaded model successfully fit the training data
v0.10,clean up results directory
v0.10,check if model can predict
v0.10,load model
v0.10,chcek if model predicts correct label
v0.10,check if loaded model successfully fit the training data
v0.10,clean up results directory
v0.10,check if model can predict
v0.10,load model
v0.10,chcek if model predicts correct label
v0.10,check if loaded model successfully fit the training data
v0.10,clean up results directory
v0.10,clean up results directory
v0.10,clean up results directory
v0.10,clean up results directory
v0.10,clean up results directory
v0.10,clean up results directory
v0.10,train model for 2 epochs
v0.10,load the checkpoint model and train until epoch 4
v0.10,clean up results directory
v0.9,from allennlp.common.tqdm import Tqdm
v0.9,mmap seems to be much more memory efficient
v0.9,Remove quotes from etag
v0.9,"If there is an etag, it's everything after the first period"
v0.9,"Otherwise, use None"
v0.9,"URL, so get it from the cache (downloading if necessary)"
v0.9,"File, and it exists."
v0.9,"File, but it doesn't exist."
v0.9,Something unknown
v0.9,Extract all the contents of zip file in current directory
v0.9,Extract all the contents of zip file in current directory
v0.9,get cache path to put the file
v0.9,"Download to temporary file, then copy to cache dir once finished."
v0.9,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.9,GET file object
v0.9,TODO(joelgrus): do we want to do checksums or anything like that?
v0.9,get cache path to put the file
v0.9,make HEAD request to check ETag
v0.9,add ETag to filename if it exists
v0.9,"etag = response.headers.get(""ETag"")"
v0.9,"Download to temporary file, then copy to cache dir once finished."
v0.9,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.9,GET file object
v0.9,These defaults are the same as the argument defaults in tqdm.
v0.9,first determine the distribution of classes in the dataset
v0.9,weight for each sample
v0.9,Create blocks
v0.9,shuffle the blocks
v0.9,concatenate the shuffled blocks
v0.9,Create blocks
v0.9,shuffle the blocks
v0.9,concatenate the shuffled blocks
v0.9,State initialization
v0.9,Exponential moving average of gradient values
v0.9,Exponential moving average of squared gradient values
v0.9,Maintains max of all exp. moving avg. of sq. grad. values
v0.9,Decay the first and second moment running average coefficient
v0.9,Maintains the maximum of all 2nd moment running avg. till now
v0.9,Use the max. for normalizing running avg. of gradient
v0.9,determine offsets for whitespace_after field
v0.9,increment for last token in sentence if not followed by whitespace
v0.9,determine offsets for whitespace_after field
v0.9,conll 2000 column format
v0.9,conll 03 NER column format
v0.9,WNUT-17
v0.9,-- WikiNER datasets
v0.9,-- Universal Dependencies
v0.9,Germanic
v0.9,Romance
v0.9,West-Slavic
v0.9,South-Slavic
v0.9,East-Slavic
v0.9,Scandinavian
v0.9,Asian
v0.9,Language isolates
v0.9,recent Universal Dependencies
v0.9,other datasets
v0.9,text classification format
v0.9,text regression format
v0.9,"first, try to fetch dataset online"
v0.9,default dataset folder is the cache root
v0.9,get string value if enum is passed
v0.9,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.9,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.9,the CoNLL 03 task for German has an additional lemma column
v0.9,the CoNLL 03 task for Dutch has no NP column
v0.9,the CoNLL 03 task for Spanish only has two columns
v0.9,the GERMEVAL task only has two columns: text and ner
v0.9,WSD tasks may be put into this column format
v0.9,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.9,"for text classifiers, we use our own special format"
v0.9,NER corpus for Basque
v0.9,automatically identify train / test / dev files
v0.9,"if no test file is found, take any file with 'test' in name"
v0.9,get train and test data
v0.9,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.9,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.9,convert tag scheme to iobes
v0.9,automatically identify train / test / dev files
v0.9,automatically identify train / test / dev files
v0.9,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.9,conll 2000 chunking task
v0.9,Support both TREC-6 and TREC-50
v0.9,Create flair compatible labels
v0.9,TREC-6 : NUM:dist -> __label__NUM
v0.9,TREC-50: NUM:dist -> __label__NUM:dist
v0.9,Wikiner NER task
v0.9,unpack and write out in CoNLL column-like format
v0.9,CoNLL 02/03 NER
v0.9,universal dependencies
v0.9,--- UD Germanic
v0.9,--- UD Romance
v0.9,--- UD West-Slavic
v0.9,--- UD Scandinavian
v0.9,--- UD South-Slavic
v0.9,--- UD Asian
v0.9,this is the default init size of a lmdb database for embeddings
v0.9,some non-used parameter to allow print
v0.9,get db filename from embedding name
v0.9,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.9,SequenceTagger
v0.9,TextClassifier
v0.9,get db filename from embedding name
v0.9,if embedding database already exists
v0.9,"otherwise, push embedding to database"
v0.9,if embedding database already exists
v0.9,open the database in read mode
v0.9,we need to set self.k
v0.9,create and load the database in write mode
v0.9,"no idea why, but we need to close and reopen the environment to avoid"
v0.9,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.9,when opening new transaction !
v0.9,init dictionaries
v0.9,"in order to deal with unknown tokens, add <unk>"
v0.9,"if text is passed, instantiate sentence with tokens (words)"
v0.9,log a warning if the dataset is empty
v0.9,some sentences represent a document boundary (but most do not)
v0.9,data with zero-width characters cannot be handled
v0.9,set token idx if not set
v0.9,non-set tags are OUT tags
v0.9,anything that is not a BIOES tag is a SINGLE tag
v0.9,anything that is not OUT is IN
v0.9,single and begin tags start a new span
v0.9,remember previous tag
v0.9,"if label type is explicitly specified, get spans for this label type"
v0.9,else determine all label types in sentence and get all spans
v0.9,move sentence embeddings to device
v0.9,move token embeddings to device
v0.9,clear sentence embeddings
v0.9,clear token embeddings
v0.9,infer whitespace after field
v0.9,add Sentence labels to output if they exist
v0.9,add Token labels to output if they exist
v0.9,add Sentence labels to output if they exist
v0.9,add Token labels to output if they exist
v0.9,No character at the corresponding code point: remove it
v0.9,TODO: crude hack - replace with something better
v0.9,set name
v0.9,sample test data if none is provided
v0.9,sample dev data if none is provided
v0.9,set train dev and test data
v0.9,find out empty sentence indices
v0.9,create subset of non-empty sentence indices
v0.9,find out empty sentence indices
v0.9,create subset of non-empty sentence indices
v0.9,"if there are token labels of provided type, use these. Otherwise use sentence labels"
v0.9,if we are looking for sentence-level labels
v0.9,check if sentence itself has labels
v0.9,check for labels of words
v0.9,"if this is not a token-level prediction problem, add sentence-level labels to dictionary"
v0.9,Make the tag dictionary
v0.9,global variable: cache_root
v0.9,global variable: device
v0.9,global variable: embedding_storage_mode
v0.9,# dummy return to fulfill trainer.train() needs
v0.9,print(vec)
v0.9,Attach optimizer
v0.9,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.9,if memory mode option 'none' delete everything
v0.9,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.9,find out which ones are dynamic embeddings
v0.9,find out which ones are dynamic embeddings
v0.9,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.9,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.9,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.9,see https://github.com/zalandoresearch/flair/issues/351
v0.9,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.9,loss calculation
v0.9,variables for printing
v0.9,variables for computing scores
v0.9,remove any previously predicted labels
v0.9,predict for batch
v0.9,get the gold labels
v0.9,add to all_predicted_values
v0.9,make printout lines
v0.9,"if the model is span-level, transfer to word-level annotations for printout"
v0.9,"all labels default to ""O"""
v0.9,set gold token-level
v0.9,set predicted token-level
v0.9,now print labels in CoNLL format
v0.9,check if there is a label mismatch
v0.9,print info
v0.9,write all_predicted_values to out_file if set
v0.9,make the evaluation dictionary
v0.9,"finally, compute numbers"
v0.9,"now, calculate evaluation numbers"
v0.9,there is at least one gold label or one prediction (default)
v0.9,issue error and default all evaluation numbers to 0.
v0.9,line for log file
v0.9,initialize the label dictionary
v0.9,self.label_dictionary.add_item('O')
v0.9,set up multi-label logic
v0.9,loss weights and loss function
v0.9,Initialize the weight tensor
v0.9,filter empty sentences
v0.9,reverse sort all sequences by their length
v0.9,progress bar for verbosity
v0.9,stop if all sentences are empty
v0.9,remove previously predicted labels of this type
v0.9,if anything could possibly be predicted
v0.9,header for 'weights.txt'
v0.9,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.9,then get all relevant values from the tsv
v0.9,then get all relevant values from the tsv
v0.9,plot i
v0.9,save plots
v0.9,save plots
v0.9,plt.show()
v0.9,save plot
v0.9,take the average over the last three scores of training
v0.9,take average over the scores from the different training runs
v0.9,remove previous embeddings
v0.9,clearing token embeddings to save memory
v0.9,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.9,#TODO: not saving lines yet
v0.9,== similarity measures ==
v0.9,helper class for ModelSimilarity
v0.9,-- works with binary cross entropy loss --
v0.9,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.9,-- works with ranking/triplet loss --
v0.9,normalize the embeddings
v0.9,== similarity losses ==
v0.9,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.9,TODO: this assumes eye matrix
v0.9,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.9,== similarity learner ==
v0.9,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.9,assumes that for each data pair there's at least one embedding per modality
v0.9,pre-compute embeddings for all targets in evaluation dataset
v0.9,compute the similarity
v0.9,sort the similarity matrix across modality 1
v0.9,"get the ranks, so +1 to start counting ranks from 1"
v0.9,The conversion from old model's constructor interface
v0.9,auto-spawn on GPU if available
v0.9,pad strings with whitespaces to longest sentence
v0.9,cut up the input into chunks of max charlength = chunk_size
v0.9,push each chunk through the RNN language model
v0.9,concatenate all chunks to make final output
v0.9,initial hidden state
v0.9,get predicted weights
v0.9,divide by temperature
v0.9,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.9,this makes a vector in which the largest value is 0
v0.9,compute word weights with exponential function
v0.9,try sampling multinomial distribution for next character
v0.9,print(word_idx)
v0.9,input ids
v0.9,push list of character IDs through model
v0.9,the target is always the next character
v0.9,use cross entropy loss to compare output of forward pass with targets
v0.9,exponentiate cross-entropy loss to calculate perplexity
v0.9,serialize the language models and the constructor arguments (but nothing else)
v0.9,special handling for deserializing language models
v0.9,re-initialize language model with constructor arguments
v0.9,copy over state dictionary to self
v0.9,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.9,"in their ""self.train()"" method)"
v0.9,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.9,"check if this is the case and if so, set it"
v0.9,Transform input data into TARS format
v0.9,print(all_labels)
v0.9,"if there are no labels, return a random sample as negatives"
v0.9,print(sample)
v0.9,"otherwise, go through all labels"
v0.9,make sure the probabilities always sum up to 1
v0.9,get and embed all labels by making a Sentence object that contains only the label text
v0.9,get each label embedding and scale between 0 and 1
v0.9,compute similarity matrix
v0.9,"the higher the similarity, the greater the chance that a label is"
v0.9,sampled as negative example
v0.9,make label dictionary if no Dictionary object is passed
v0.9,prepare dictionary of tags (without B- I- prefixes)
v0.9,check if candidate_label_set is empty
v0.9,make list if only one candidate label is passed
v0.9,"if list is passed, convert to set"
v0.9,note current task
v0.9,create a temporary task
v0.9,make zero shot predictions
v0.9,switch to the pre-existing task
v0.9,prepare TARS dictionary
v0.9,initialize a bare-bones sequence tagger
v0.9,transformer separator
v0.9,Store task specific labels since TARS can handle multiple tasks
v0.9,make a tars sentence where all labels are O by default
v0.9,overwrite O labels with tags
v0.9,init new TARS classifier
v0.9,set all task information
v0.9,linear layers of internal classifier
v0.9,return
v0.9,with torch.no_grad():
v0.9,reverse sort all sequences by their length
v0.9,progress bar for verbosity
v0.9,stop if all sentences are empty
v0.9,go through each sentence in the batch
v0.9,always remove tags first
v0.9,get the span and its label
v0.9,determine whether tokens in this span already have a label
v0.9,only add if all tokens have no label
v0.9,clearing token embeddings to save memory
v0.9,prepare TARS dictionary
v0.9,initialize a bare-bones sequence tagger
v0.9,transformer separator
v0.9,Store task specific labels since TARS can handle multiple tasks
v0.9,init new TARS classifier
v0.9,set all task information
v0.9,linear layers of internal classifier
v0.9,with torch.no_grad():
v0.9,set context if not set already
v0.9,reverse sort all sequences by their length
v0.9,progress bar for verbosity
v0.9,stop if all sentences are empty
v0.9,go through each sentence in the batch
v0.9,always remove tags first
v0.9,clearing token embeddings to save memory
v0.9,if embed_separately == True the linear layer needs twice the length of the embeddings as input size
v0.9,since we concatenate the embeddings of the two DataPoints in the DataPairs
v0.9,representation for both sentences
v0.9,set separator to concatenate two sentences
v0.9,auto-spawn on GPU if available
v0.9,linear layer
v0.9,minimal return is scores and labels
v0.9,"entity pairs could also be no relation at all, add default value for this case to dictionary"
v0.9,entity_pairs = []
v0.9,super lame: make dictionary to find relation annotations for a given entity pair
v0.9,get all entity spans
v0.9,get embedding for each entity
v0.9,"go through cross product of entities, for each pair concat embeddings"
v0.9,get gold label for this relation (if one exists)
v0.9,"if using gold spans only, skip all entity pairs that are not in gold data"
v0.9,"if no gold label exists, and all spans are used, label defaults to 'O' (no relation)"
v0.9,"if predicting, also remember sentences and label candidates"
v0.9,"return either scores and gold labels (for loss calculation), or include label candidates for prediction"
v0.9,if we concatenate the embeddings we need double input size in our linear layer
v0.9,filter sentences with no candidates (no candidates means nothing can be linked anyway)
v0.9,fields to return
v0.9,"if the entire batch has no sentence with candidates, return empty"
v0.9,"otherwise, embed sentence and send through prediction head"
v0.9,embed all tokens
v0.9,get the embeddings of the entity mentions
v0.9,minimal return is scores and labels
v0.9,set the dictionaries
v0.9,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.9,Initialize the weight tensor
v0.9,initialize the network architecture
v0.9,dropouts
v0.9,optional reprojection layer on top of word embeddings
v0.9,bidirectional LSTM on top of embedding layer
v0.9,Create initial hidden state and initialize it
v0.9,TODO: Decide how to initialize the hidden state variables
v0.9,self.hs_initializer(self.lstm_init_h)
v0.9,self.hs_initializer(self.lstm_init_c)
v0.9,final linear map to tag space
v0.9,reverse sort all sequences by their length
v0.9,progress bar for verbosity
v0.9,stop if all sentences are empty
v0.9,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.9,clearing token embeddings to save memory
v0.9,--------------------------------------------------------------------
v0.9,FF PART
v0.9,--------------------------------------------------------------------
v0.9,"if initial hidden state is trainable, use this state"
v0.9,word dropout only before LSTM - TODO: more experimentation needed
v0.9,if self.use_word_dropout > 0.0:
v0.9,sentence_tensor = self.word_dropout(sentence_tensor)
v0.9,get the tags in this sentence
v0.9,add tags as tensor
v0.9,pad tags if using batch-CRF decoder
v0.9,reduce raw values to avoid NaN during exp
v0.9,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.9,default value
v0.9,core Flair models on Huggingface ModelHub
v0.9,"Large NER models,"
v0.9,Multilingual NER models
v0.9,English POS models
v0.9,Multilingual POS models
v0.9,English SRL models
v0.9,English chunking models
v0.9,Language-specific NER models
v0.9,English NER models
v0.9,Multilingual NER models
v0.9,English POS models
v0.9,Multilingual POS models
v0.9,English SRL models
v0.9,English chunking models
v0.9,Danish models
v0.9,German models
v0.9,French models
v0.9,Dutch models
v0.9,Malayalam models
v0.9,Portuguese models
v0.9,Keyphase models
v0.9,Biomedical models
v0.9,check if model name is a valid local file
v0.9,"check if model key is remapped to HF key - if so, print out information"
v0.9,get mapped name
v0.9,output information
v0.9,use mapped name instead
v0.9,"if not, check if model key is remapped to direct download location. If so, download model"
v0.9,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.9,"for all other cases (not local file or special download location), use HF model hub"
v0.9,"if not a local file, get from model hub"
v0.9,use model name as subfolder
v0.9,Lazy import
v0.9,output information
v0.9,"log.error(f"" - Error message: {e}"")"
v0.9,"all_tag_prob=all_tag_prob,"
v0.9,clear embeddings after predicting
v0.9,load each model
v0.9,check if the same embeddings were already loaded previously
v0.9,"if the model uses StackedEmbedding, make a new stack with previous objects"
v0.9,sort embeddings by key alphabetically
v0.9,check previous embeddings and add if found
v0.9,only re-use static embeddings
v0.9,"if not found, use existing embedding"
v0.9,initialize new stack
v0.9,"of the model uses regular embedding, re-load if previous version found"
v0.9,auto-spawn on GPU if available
v0.9,embed sentences
v0.9,make tensor for all embedded sentences in batch
v0.9,send through decoder to get logits
v0.9,minimal return is scores and labels
v0.9,English sentiment models
v0.9,Communicative Functions Model
v0.9,embeddings
v0.9,dictionaries
v0.9,linear layer
v0.9,all parameters will be pushed internally to the specified device
v0.9,get all tokens in this mini-batch
v0.9,minimal return is scores and labels
v0.9,weights for loss function
v0.9,iput size is two times wordembedding size since we use pair of words as input
v0.9,"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
v0.9,regression
v0.9,input size is two times word embedding size since we use pair of words as input
v0.9,the output size is 1
v0.9,auto-spawn on GPU if available
v0.9,all input should be tensors
v0.9,forward allows only a single sentcence!!
v0.9,embed words of sentence
v0.9,go through all pairs of words with a maximum number of max_distance in between
v0.9,go through all pairs
v0.9,2-dim matrix whose rows are the embeddings of word pairs of the sentence
v0.9,So far only one sentence allowed
v0.9,If list of sentences is handed the function works with the first sentence of the list
v0.9,Assume data_points is a single sentence!!!
v0.9,scores are the predictions for each word pair
v0.9,"classification needs labels to be integers, regression needs labels to be float"
v0.9,this is due to the different loss functions
v0.9,only single sentences as input
v0.9,gold labels
v0.9,for output text file
v0.9,for buckets
v0.9,for average prediction
v0.9,add some statistics to the output
v0.9,use scikit-learn to evaluate
v0.9,"we iterate over each sentence, instead of batches"
v0.9,get single labels from scores
v0.9,gold labels
v0.9,for output text file
v0.9,hot one vector of true value
v0.9,hot one vector of predicted value
v0.9,"speichert embeddings, falls embedding_storage!= 'None'"
v0.9,"make ""classification report"""
v0.9,get scores
v0.9,"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.9,"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.9,line for log file
v0.9,cast string to Path
v0.9,check for previously saved best models in the current training folder and delete them
v0.9,"determine what splits (train, dev, test) to evaluate and log"
v0.9,prepare loss logging file and set up header
v0.9,"minimize training loss if training with dev data, else maximize dev score"
v0.9,"if we load a checkpoint, we have already trained for self.epoch"
v0.9,"if training also uses dev/train data, include in training set"
v0.9,initialize sampler if provided
v0.9,init with default values if only class is provided
v0.9,set dataset to sample from
v0.9,At any point you can hit Ctrl + C to break out of training early.
v0.9,get new learning rate
v0.9,reload last best model if annealing with restarts is enabled
v0.9,stop training if learning rate becomes too small
v0.9,process mini-batches
v0.9,zero the gradients on the model and optimizer
v0.9,"if necessary, make batch_steps"
v0.9,forward and backward for batch
v0.9,forward pass
v0.9,Backward
v0.9,do the optimizer step
v0.9,do the scheduler step if one-cycle
v0.9,get new learning rate
v0.9,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.9,evaluate on train / dev / test split depending on training settings
v0.9,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.9,calculate scores using dev data if available
v0.9,append dev score to score history
v0.9,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.9,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.9,determine if this is the best model or if we need to anneal
v0.9,default mode: anneal against dev score
v0.9,alternative: anneal against dev loss
v0.9,alternative: anneal against train loss
v0.9,determine bad epoch number
v0.9,log bad epochs
v0.9,output log file
v0.9,make headers on first epoch
v0.9,"if checkpoint is enabled, save model at each epoch"
v0.9,Check whether to save best model
v0.9,"if we do not use dev data for model selection, save final model"
v0.9,test best model if test data is present
v0.9,"if we are training over multiple datasets, do evaluation for each"
v0.9,get and return the final test score of best model
v0.9,cast string to Path
v0.9,forward pass
v0.9,update optimizer and scheduler
v0.9,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.9,cast string to Path
v0.9,error message if the validation dataset is too small
v0.9,Shuffle training files randomly after serially iterating through corpus one
v0.9,"iterate through training data, starting at self.split (for checkpointing)"
v0.9,off by one for printing
v0.9,go into train mode
v0.9,reset variables
v0.9,not really sure what this does
v0.9,do the forward pass in the model
v0.9,try to predict the targets
v0.9,Backward
v0.9,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.9,We detach the hidden state from how it was previously produced.
v0.9,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.9,explicitly remove loss to clear up memory
v0.9,##############################################################################
v0.9,Save the model if the validation loss is the best we've seen so far.
v0.9,##############################################################################
v0.9,print info
v0.9,##############################################################################
v0.9,##############################################################################
v0.9,final testing
v0.9,##############################################################################
v0.9,Turn on evaluation mode which disables dropout.
v0.9,Work out how cleanly we can divide the dataset into bsz parts.
v0.9,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.9,Evenly divide the data across the bsz batches.
v0.9,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.9,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.9,news-english-forward
v0.9,news-english-backward
v0.9,news-english-forward
v0.9,news-english-backward
v0.9,mix-english-forward
v0.9,mix-english-backward
v0.9,mix-german-forward
v0.9,mix-german-backward
v0.9,common crawl Polish forward
v0.9,common crawl Polish backward
v0.9,Slovenian forward
v0.9,Slovenian backward
v0.9,Bulgarian forward
v0.9,Bulgarian backward
v0.9,Dutch forward
v0.9,Dutch backward
v0.9,Swedish forward
v0.9,Swedish backward
v0.9,French forward
v0.9,French backward
v0.9,Czech forward
v0.9,Czech backward
v0.9,Portuguese forward
v0.9,Portuguese backward
v0.9,initialize cache if use_cache set
v0.9,embed a dummy sentence to determine embedding_length
v0.9,set to eval mode
v0.9,Copy the object's state from self.__dict__ which contains
v0.9,all our instance attributes. Always use the dict.copy()
v0.9,method to avoid modifying the original state.
v0.9,Remove the unpicklable entries.
v0.9,"if cache is used, try setting embeddings from cache first"
v0.9,try populating embeddings from cache
v0.9,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.9,get hidden states from language model
v0.9,take first or last hidden states from language model as word representation
v0.9,if self.tokenized_lm or token.whitespace_after:
v0.9,1-camembert-base -> camembert-base
v0.9,1-xlm-roberta-large -> xlm-roberta-large
v0.9,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.9,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.9,tokens are attended to.
v0.9,Zero-pad up to the sequence length.
v0.9,"first, find longest sentence in batch"
v0.9,prepare id maps for BERT model
v0.9,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.9,get aggregated embeddings for each BERT-subtoken in sentence
v0.9,get the current sentence object
v0.9,add concatenated embedding to sentence
v0.9,use first subword embedding if pooling operation is 'first'
v0.9,"otherwise, do a mean over all subwords in token"
v0.9,"if only one sentence is passed, convert to list of sentence"
v0.9,bidirectional LSTM on top of embedding layer
v0.9,dropouts
v0.9,"first, sort sentences by number of tokens"
v0.9,go through each sentence in batch
v0.9,PADDING: pad shorter sentences out
v0.9,ADD TO SENTENCE LIST: add the representation
v0.9,--------------------------------------------------------------------
v0.9,GET REPRESENTATION FOR ENTIRE BATCH
v0.9,--------------------------------------------------------------------
v0.9,--------------------------------------------------------------------
v0.9,FF PART
v0.9,--------------------------------------------------------------------
v0.9,use word dropout if set
v0.9,--------------------------------------------------------------------
v0.9,EXTRACT EMBEDDINGS FROM LSTM
v0.9,--------------------------------------------------------------------
v0.9,embed a dummy sentence to determine embedding_length
v0.9,Avoid conflicts with flair's Token class
v0.9,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.9,add positional encodings
v0.9,reshape the pixels into the sequence
v0.9,layer norm after convolution and positional encodings
v0.9,add <cls> token
v0.9,"transformer requires input in the shape [h*w+1, b, d]"
v0.9,the output is an embedding of <cls> token
v0.9,temporary fix to disable tokenizer parallelism warning
v0.9,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.9,do not print transformer warnings as these are confusing in this case
v0.9,load tokenizer and transformer model
v0.9,model name
v0.9,"when initializing, embeddings are in eval mode by default"
v0.9,embedding parameters
v0.9,send mini-token through to check how many layers the model has
v0.9,check whether CLS is at beginning or end
v0.9,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.9,gradients are enabled if fine-tuning is enabled
v0.9,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.9,subtokenize sentences
v0.9,tokenize and truncate to max subtokens (TODO: check better truncation strategies)
v0.9,find longest sentence in batch
v0.9,initialize batch tensors and mask
v0.9,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.9,iterate over all subtokenized sentences
v0.9,use scalar mix of embeddings if so selected
v0.9,set the extracted embedding for the token
v0.9,special handling for serializing transformer models
v0.9,serialize the transformer models and the constructor arguments (but nothing else)
v0.9,necessary for reverse compatibility with Flair <= 0.7
v0.9,special handling for deserializing transformer models
v0.9,load transformer model
v0.9,constructor arguments
v0.9,re-initialize transformer word embeddings with constructor arguments
v0.9,for backward compatibility with previous models
v0.9,"I have no idea why this is necessary, but otherwise it doesn't work"
v0.9,reload tokenizer to get around serialization issues
v0.9,optional fine-tuning on top of embedding layer
v0.9,"if only one sentence is passed, convert to list of sentence"
v0.9,"if only one sentence is passed, convert to list of sentence"
v0.9,bidirectional RNN on top of embedding layer
v0.9,dropouts
v0.9,TODO: remove in future versions
v0.9,embed words in the sentence
v0.9,before-RNN dropout
v0.9,reproject if set
v0.9,push through RNN
v0.9,after-RNN dropout
v0.9,extract embeddings from RNN
v0.9,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.9,"check if this is the case and if so, set it"
v0.9,serialize the language models and the constructor arguments (but nothing else)
v0.9,special handling for deserializing language models
v0.9,re-initialize language model with constructor arguments
v0.9,copy over state dictionary to self
v0.9,"set the language model to eval() by default (this is necessary since FlairEmbeddings ""protect"" the LM"
v0.9,"in their ""self.train()"" method)"
v0.9,IMPORTANT: add embeddings as torch modules
v0.9,iterate over sentences
v0.9,"if its a forward LM, take last state"
v0.9,"convert to plain strings, embedded in a list for the encode function"
v0.9,CNN
v0.9,dropouts
v0.9,TODO: remove in future versions
v0.9,embed words in the sentence
v0.9,before-RNN dropout
v0.9,reproject if set
v0.9,push CNN
v0.9,after-CNN dropout
v0.9,extract embeddings from CNN
v0.9,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.9,"if only one sentence is passed, convert to list of sentence"
v0.9,Expose base classses
v0.9,Expose token embedding classes
v0.9,Expose document embedding classes
v0.9,Expose image embedding classes
v0.9,Expose legacy embedding classes
v0.9,IMPORTANT: add embeddings as torch modules
v0.9,"if only one sentence is passed, convert to list of sentence"
v0.9,GLOVE embeddings
v0.9,TURIAN embeddings
v0.9,KOMNINOS embeddings
v0.9,pubmed embeddings
v0.9,FT-CRAWL embeddings
v0.9,FT-CRAWL embeddings
v0.9,twitter embeddings
v0.9,two-letter language code wiki embeddings
v0.9,two-letter language code wiki embeddings
v0.9,two-letter language code crawl embeddings
v0.9,fix serialized models
v0.9,use list of common characters if none provided
v0.9,translate words in sentence into ints using dictionary
v0.9,"sort words by length, for batching and masking"
v0.9,chars for rnn processing
v0.9,multilingual models
v0.9,English models
v0.9,Arabic
v0.9,Bulgarian
v0.9,Czech
v0.9,Danish
v0.9,German
v0.9,Spanish
v0.9,Basque
v0.9,Persian
v0.9,Finnish
v0.9,French
v0.9,Hebrew
v0.9,Hindi
v0.9,Croatian
v0.9,Indonesian
v0.9,Italian
v0.9,Japanese
v0.9,Malayalam
v0.9,Dutch
v0.9,Norwegian
v0.9,Polish
v0.9,Portuguese
v0.9,Pubmed
v0.9,Slovenian
v0.9,Swedish
v0.9,Tamil
v0.9,Spanish clinical
v0.9,CLEF HIPE Shared task
v0.9,load model if in pretrained model map
v0.9,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.9,CLEF HIPE models are lowercased
v0.9,embeddings are static if we don't do finetuning
v0.9,embed a dummy sentence to determine embedding_length
v0.9,set to eval mode
v0.9,make compatible with serialized models (TODO: remove)
v0.9,"unless fine-tuning is set, do not set language model to train() in order to disallow language model dropout"
v0.9,make compatible with serialized models (TODO: remove)
v0.9,gradients are enable if fine-tuning is enabled
v0.9,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.9,get hidden states from language model
v0.9,take first or last hidden states from language model as word representation
v0.9,offset mode that extracts at whitespace after last character
v0.9,offset mode that extracts at last character
v0.9,only clone if optimization mode is 'gpu'
v0.9,use the character language model embeddings as basis
v0.9,length is twice the original character LM embedding length
v0.9,these fields are for the embedding memory
v0.9,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.9,we re-compute embeddings dynamically at each epoch
v0.9,set the memory method
v0.9,memory is wiped each time we do a training run
v0.9,"if we keep a pooling, it needs to be updated continuously"
v0.9,update embedding
v0.9,check token.text is empty or not
v0.9,set aggregation operation
v0.9,add embeddings after updating
v0.9,temporary fix to disable tokenizer parallelism warning
v0.9,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.9,do not print transformer warnings as these are confusing in this case
v0.9,load tokenizer and transformer model
v0.9,"in the end, these models don't need this configuration"
v0.9,model name
v0.9,whether to detach gradients on overlong sentences
v0.9,store whether to use context (and how much)
v0.9,dropout contexts
v0.9,"if using context, can we cross document boundaries?"
v0.9,send self to flair-device
v0.9,embedding parameters
v0.9,send mini-token through to check how many layers the model has
v0.9,calculate embedding length
v0.9,return length
v0.9,check if special tokens exist to circumvent error message
v0.9,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.9,"when initializing, embeddings are in eval mode by default"
v0.9,remove special markup
v0.9,"we require encoded subtokenized sentences, the mapping to original tokens and the number of"
v0.9,parts that each sentence produces
v0.9,"if we also use context, first expand sentence to include context"
v0.9,set context if not set already
v0.9,"in case of contextualization, we must remember non-expanded sentence"
v0.9,create expanded sentence and remember context offsets
v0.9,overwrite sentence with expanded sentence
v0.9,subtokenize the sentence
v0.9,transformer specific tokenization
v0.9,set zero embeddings for empty sentences and return
v0.9,determine into how many subtokens each token is split
v0.9,encode inputs
v0.9,overlong sentences are handled as multiple splits
v0.9,find longest sentence in batch
v0.9,initialize batch tensors and mask
v0.9,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.9,make the tuple a tensor; makes working with it easier.
v0.9,gradients are enabled if fine-tuning is enabled
v0.9,iterate over all subtokenized sentences
v0.9,"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
v0.9,in order to get some context into the embeddings of these words.
v0.9,also don't include the embedding of the extra [CLS] and [SEP] tokens.
v0.9,"for each token, get embedding"
v0.9,some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
v0.9,"get states from all selected layers, aggregate with pooling operation"
v0.9,use layer mean of embeddings if so selected
v0.9,set the extracted embedding for the token
v0.9,move embeddings from context back to original sentence (if using context)
v0.9,remember original sentence
v0.9,get left context
v0.9,get right context
v0.9,empty contexts should not introduce whitespace tokens
v0.9,make expanded sentence
v0.9,iterate over subtokens and reconstruct tokens
v0.9,remove special markup
v0.9,TODO check if this is necessary is this method is called before prepare_for_model
v0.9,check if reconstructed token is special begin token ([CLS] or similar)
v0.9,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.9,append subtoken to reconstruct token
v0.9,check if reconstructed token is the same as current token
v0.9,"if so, add subtoken count"
v0.9,reset subtoken count and reconstructed token
v0.9,break from loop if all tokens are accounted for
v0.9,if tokens are unaccounted for
v0.9,check if all tokens were matched to subtokens
v0.9,"""""""Returns the length of the embedding vector."""""""
v0.9,special handling for serializing transformer models
v0.9,serialize the transformer models and the constructor arguments (but nothing else)
v0.9,necessary for reverse compatibility with Flair <= 0.7
v0.9,special handling for deserializing transformer models
v0.9,load transformer model
v0.9,constructor arguments
v0.9,re-initialize transformer word embeddings with constructor arguments
v0.9,"I have no idea why this is necessary, but otherwise it doesn't work"
v0.9,reload tokenizer to get around serialization issues
v0.9,max_tokens = 500
v0.9,model architecture
v0.9,model architecture
v0.9,download if necessary
v0.9,load the model
v0.9,"TODO: keep for backwards compatibility, but remove in future"
v0.9,save the sentence piece model as binary file (not as path which may change)
v0.9,write out the binary sentence piece model into the expected directory
v0.9,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.9,"otherwise, use normal process and potentially trigger another download"
v0.9,"once the modes if there, load it with sentence piece"
v0.9,empty words get no embedding
v0.9,all other words get embedded
v0.9,"the default model for ELMo is the 'original' model, which is very large"
v0.9,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.9,put on Cuda if available
v0.9,embed a dummy sentence to determine embedding_length
v0.9,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.9,GLOVE embeddings
v0.9,"find train, dev and test files if not specified"
v0.9,get train data
v0.9,read in test file if exists
v0.9,read in dev file if exists
v0.9,special key for space after
v0.9,"store either Sentence objects in memory, or only file offsets"
v0.9,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.9,determine encoding of text file
v0.9,skip first line if to selected
v0.9,option 1: read only sentence boundaries as offset positions
v0.9,option 2: keep everything in memory
v0.9,pointer to previous
v0.9,"if sentence ends, break"
v0.9,skip comments
v0.9,"if sentence ends, convert and return"
v0.9,check if this sentence is a document boundary
v0.9,"otherwise, this line is a token. parse and add to sentence"
v0.9,check if this sentence is a document boundary
v0.9,"for example, transforming 'B-OBJ' to 'B-part-of-speech-object'"
v0.9,"if in memory, retrieve parsed sentence"
v0.9,else skip to position in file where sentence begins
v0.9,set sentence context using partials
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,check if data there
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,check if data there
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download files if not present locally
v0.9,we need to slightly modify the original files by adding some new lines after document separators
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,Remove CoNLL-U meta information in the last column
v0.9,column format
v0.9,dataset name
v0.9,data folder: default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,dataset name
v0.9,data folder: default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,entity_mapping
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,data validation
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download files if not present locallys
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,# download zip
v0.9,merge the files in one as the zip is containing multiples files
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,check if data there
v0.9,create folder
v0.9,download dataset
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download and parse data if necessary
v0.9,create train test dev if not exist
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,If the extracted corpus file is not yet present in dir
v0.9,download zip if necessary
v0.9,"extracted corpus is not present , so unpacking it."
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download zip
v0.9,unpacking the zip
v0.9,merge the files in one as the zip is containing multiples files
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,"download data from github if necessary (hironsan.txt, ja.wikipedia.conll)"
v0.9,download files if not present locally
v0.9,we need to modify the original files by adding new lines after after the end of each sentence
v0.9,if only one language is given
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,"use all languages if explicitly set to ""all"""
v0.9,download data if necessary
v0.9,initialize comlumncorpus and add it to list
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,"For each language in languages, the file is downloaded if not existent"
v0.9,Then a comlumncorpus of that data is created and saved in a list
v0.9,this list is handed to the multicorpus
v0.9,list that contains the columncopora
v0.9,download data if necessary
v0.9,"if language not downloaded yet, download it"
v0.9,create folder
v0.9,get google drive id from list
v0.9,download from google drive
v0.9,unzip
v0.9,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.9,transform data into required format
v0.9,"the processed dataset has the additional ending ""_new"""
v0.9,remove the unprocessed dataset
v0.9,initialize comlumncorpus and add it to list
v0.9,if no languages are given as argument all languages used in XTREME will be loaded
v0.9,if only one language is given
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,"For each language in languages, the file is downloaded if not existent"
v0.9,Then a comlumncorpus of that data is created and saved in a list
v0.9,This list is handed to the multicorpus
v0.9,list that contains the columncopora
v0.9,download data if necessary
v0.9,"if language not downloaded yet, download it"
v0.9,create folder
v0.9,download from HU Server
v0.9,unzip
v0.9,transform data into required format
v0.9,initialize comlumncorpus and add it to list
v0.9,if only one language is given
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,initialize comlumncorpus and add it to list
v0.9,download data if necessary
v0.9,unpack and write out in CoNLL column-like format
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,data is not in IOB2 format. Thus we transform it to IOB2
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,rename according to train - test - dev - convention
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,
v0.9,since only the WordNet 3.0 version for senses is consistently available for all provided datasets we will
v0.9,only consider this version
v0.9,
v0.9,also we ignore the id annotation used in datasets that were originally created for evaluation tasks
v0.9,
v0.9,if the other annotations should be needed simply add the columns in correct order according
v0.9,to the chosen datasets here and respectively change the values of the blacklist array and
v0.9,the range value of the else case in the token for loop in the from_ufsac_to_conll function
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,check if data there
v0.9,determine correct CoNLL files
v0.9,tokens to ignore (edit here for variation)
v0.9,counter to keep track how many tags have been found in line
v0.9,variable to count of how many words a chunk consists
v0.9,indicates if surface form is chunk or not
v0.9,array to save tags temporarily for handling chunks
v0.9,cut token to get chunk
v0.9,save single words of chunk
v0.9,handle first word of chunk
v0.9,edit here for variation
v0.9,check if converted file exists
v0.9,convert the file to CoNLL
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,"TODO: Add a routine, that checks annotations for some widespread errors/inconsistencies??? (e.g. in AQUAINT corpus Iran-Iraq_War vs. Iran-Iraq_war)"
v0.9,Create the annotation dictionary
v0.9,this fct removes every second unknown label
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download and parse data if necessary
v0.9,iterate over all html files
v0.9,"get rid of html syntax, we only need the text"
v0.9,between all documents we write a separator symbol
v0.9,skip empty strings
v0.9,"process the annotation format in the text and collect triples (begin_mention, length_mention, wikiname)"
v0.9,"replace [[wikiname|surface_form|score]] by surface_form and save index, length and wikiname of mention"
v0.9,sentence splitting and tokenization
v0.9,iterate through all annotations and add to corresponding tokens
v0.9,find sentence to which annotation belongs
v0.9,position within corresponding sentence
v0.9,set annotation for tokens of entity mention
v0.9,write to out-file in column format
v0.9,"in case something goes wrong, delete the dataset and raise error"
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download and parse data if necessary
v0.9,from qwikidata.linked_data_interface import get_entity_dict_from_api
v0.9,generate qid wikiname dictionaries
v0.9,merge dictionaries
v0.9,ignore first line
v0.9,commented and empty lines
v0.9,read all Q-IDs
v0.9,ignore first line
v0.9,request
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,we use the wikiids in the data instead of directly utilizing the wikipedia urls.
v0.9,like this we can quickly check if the corresponding page exists
v0.9,if there is a bad wikiid we can check if the given url in the data exists using wikipediaapi
v0.9,delete unprocessed file
v0.9,collect all wikiids
v0.9,create the dictionary
v0.9,request
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,names of raw text documents
v0.9,open output_file
v0.9,iterate through all documents
v0.9,split sentences and tokenize
v0.9,iterate through all annotations and add to corresponding tokens
v0.9,find sentence to which annotation belongs
v0.9,position within corresponding sentence
v0.9,set annotation for tokens of entity mention
v0.9,write to out file
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download and parse data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download and parse data if necessary
v0.9,First parse the post titles
v0.9,Keep track of how many and which entity mentions does a given post title have
v0.9,Check if the current post title has an entity link and parse accordingly
v0.9,Post titles with entity mentions (if any) are handled via this function
v0.9,Then parse the comments
v0.9,"Iterate over the comments.tsv file, until the end is reached"
v0.9,"Keep track of the current comment thread and its corresponding key, on which the annotations are matched."
v0.9,Each comment thread is handled as one 'document'.
v0.9,Python's csv package for some reason fails to correctly parse a handful of rows inside the comments.tsv file.
v0.9,This if-condition is needed to handle this problem.
v0.9,"In case we are dealing with properly parsed rows, proceed with a regular parsing procedure"
v0.9,"Check if the current comment thread has an entity link and parse accordingly, same as with post titles above"
v0.9,"In two of the comment thread a case of capital letter spacing occurs, which the SegtokTokenizer cannot properly handle."
v0.9,"The following if-elif condition handles these two cases and as result writes full capitalized words in each corresponding row,"
v0.9,and not just single letters into single rows.
v0.9,If there are annotated entity mentions for given post title or a comment thread
v0.9,"Keep track which is the correct corresponding entity link, in cases where there is >1 link in a sentence"
v0.9,Write the token with a corresponding tag to file
v0.9,"IndexError is raised in cases when there is exactly one link in a sentence, therefore can be dismissed"
v0.9,"If a comment thread or a post title has no entity link, all tokens are assigned the O tag"
v0.9,Prevent writing empty lines if e.g. a quote comes after a dot or initials are tokenized
v0.9,"incorrectly, in order to keep the desired format (empty line as a sentence separator)."
v0.9,"Thrown when the second check above happens, but the last token of a sentence is reached."
v0.9,"Indicates that the EOS punctuaion mark is present, therefore an empty line needs to be written below."
v0.9,"If there is no punctuation mark indicating EOS, an empty line is still needed after the EOS"
v0.9,Check if further annotations belong to the current post title or comment thread as well
v0.9,Stop when the end of an annotation file is reached
v0.9,Check if further annotations belong to the current sentence as well
v0.9,"'else ""  ""' is needed to keep the proper token positions (for accordance with annotations)"
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,if True:
v0.9,write CoNLL-U Plus header
v0.9,"Some special cases (e.g., missing spaces before entity marker)"
v0.9,necessary if text should be whitespace tokenizeable
v0.9,Handle case where tail may occur before the head
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,write CoNLL-U Plus header
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,TODO: change data source to original CoNLL04 -- this dataset has span formatting errors
v0.9,download data if necessary
v0.9,write CoNLL-U Plus header
v0.9,The span has ended.
v0.9,We are entering a new span; reset indices
v0.9,and active tag to new span.
v0.9,We're inside a span.
v0.9,Last token might have been a part of a valid span.
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,write CoNLL-U Plus header
v0.9,"for source_file_path, target_filename in zip(source_file_paths, target_filenames):"
v0.9,"with zip_file.open(source_file_path, mode=""r"") as source_file:"
v0.9,target_file_path = Path(data_folder) / target_filename
v0.9,"with open(target_file_path, mode=""w"", encoding=""utf-8"") as target_file:"
v0.9,# write CoNLL-U Plus header
v0.9,"target_file.write(""# global.columns = id form ner\n"")"
v0.9,for example in json.load(source_file):
v0.9,token_list = self._tacred_example_to_token_list(example)
v0.9,target_file.write(token_list.serialize())
v0.9,hardcoded mapping TODO: perhaps find nicer solution
v0.9,"find train, dev and test files if not specified"
v0.9,use test_file to create test split if available
v0.9,use dev_file to create test split if available
v0.9,"if data point contains black-listed label, do not use"
v0.9,first check if valid sentence
v0.9,"if so, add to indices"
v0.9,"find train, dev and test files if not specified"
v0.9,variables
v0.9,different handling of in_memory data than streaming data
v0.9,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.9,test if format is OK
v0.9,test if at least one label given
v0.9,make sentence from text (and filter for length)
v0.9,"if a pair column is defined, make a sentence pair object"
v0.9,noinspection PyDefaultArgument
v0.9,dataset name includes the split size
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download each of the 28 splits
v0.9,create dataset directory if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,create dataset directory if necessary
v0.9,create train.txt file from CSV
v0.9,create test.txt file from CSV
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,create dataset directory if necessary
v0.9,create train.txt file by iterating over pos and neg file
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,create dataset directory if necessary
v0.9,create train.txt file by iterating over pos and neg file
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,create dataset directory if necessary
v0.9,create train.txt file by iterating over pos and neg file
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,create dataset directory if necessary
v0.9,create train.txt file by iterating over pos and neg file
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,create train dev and test files in fasttext format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download senteval datasets if necessary und unzip
v0.9,convert to FastText format
v0.9,if no base_path provided take cache root
v0.9,download data if necessary
v0.9,"if data is not downloaded yet, download it"
v0.9,get the zip file
v0.9,move original .tsv files to another folder
v0.9,create train and dev splits in fasttext format
v0.9,create eval_dataset file with no labels
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,download datasets if necessary
v0.9,create dataset directory if necessary
v0.9,create correctly formated txt files
v0.9,multiple labels are possible
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,Create flair compatible labels
v0.9,TREC-6 : NUM:dist -> __label__NUM
v0.9,TREC-50: NUM:dist -> __label__NUM:dist
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,Create flair compatible labels
v0.9,TREC-6 : NUM:dist -> __label__NUM
v0.9,TREC-50: NUM:dist -> __label__NUM:dist
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,create a separate directory for different tasks
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,check if dataset is supported
v0.9,set file names
v0.9,set file names
v0.9,download and unzip in file structure if necessary
v0.9,instantiate corpus
v0.9,"find train, dev and test files if not specified"
v0.9,"create DataPairDataset for train, test and dev file, if they are given"
v0.9,stop if file does not exist
v0.9,create a DataPair object from strings
v0.9,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.9,if no base_path provided take cache root
v0.9,"if data is not downloaded yet, download it"
v0.9,get the zip file
v0.9,"rename test file to eval_dataset, since it has no labels"
v0.9,if no base_path provided take cache root
v0.9,"if data is not downloaded yet, download it"
v0.9,get the zip file
v0.9,"reorder dev datasets to have same columns as in train set: 8, 9, and 11"
v0.9,dev sets include 5 different annotations but we will only keep the gold label
v0.9,"rename test file to eval_dataset, since it has no labels"
v0.9,if no base_path provided take cache root
v0.9,"if data is not downloaded yet, download it"
v0.9,get test and dev sets
v0.9,if no base_path provided take cache root
v0.9,"if data is not downloaded yet, download it"
v0.9,get the zip file
v0.9,"rename test file to eval_dataset, since it has no labels"
v0.9,if no base_path provided take cache root
v0.9,"if data is not downloaded yet, download it"
v0.9,get the zip file
v0.9,"rename test file to eval_dataset, since it has no labels"
v0.9,if no base_path provided take cache root
v0.9,"if data is not downloaded yet, download it"
v0.9,get the zip file
v0.9,"rename test file to eval_dataset, since it has no labels"
v0.9,if no base_path provided take cache root
v0.9,"if data not downloaded yet, download it"
v0.9,get the zip file
v0.9,"the downloaded files have json format, we transform them to tsv"
v0.9,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.9,remove json file
v0.9,"find train, dev and test files if not specified"
v0.9,get train data
v0.9,get test data
v0.9,get dev data
v0.9,"if no fields specified, check if the file is CoNLL plus formatted and get fields"
v0.9,option 1: read only sentence boundaries as offset positions
v0.9,option 2: keep everything in memory
v0.9,"if in memory, retrieve parsed sentence"
v0.9,else skip to position in file where sentence begins
v0.9,current token ID
v0.9,relations: List[Relation] = []
v0.9,head and tail span indices are 1-indexed and end index is inclusive
v0.9,determine all NER label types in sentence and add all NER spans as sentence-level labels
v0.9,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.9,with sum of all entity lengths as secondary key
v0.9,calculate offset without current text
v0.9,because we stick all passages of a document together
v0.9,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.9,Try to fix incorrect annotations
v0.9,print(
v0.9,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.9,)
v0.9,Ignore empty lines or relation annotations
v0.9,FIX annotation of whitespaces (necessary for PDR)
v0.9,One token may contain multiple entities -> deque all of them
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.9,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,Edge case: last token starts a new entity
v0.9,Last document in file
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,In the huner split files there is no information whether a given id originates
v0.9,from the train or test file of the original corpus - so we have to adapt corpus
v0.9,splitting here
v0.9,In the huner split files there is no information whether a given id originates
v0.9,from the train or test file of the original corpus - so we have to adapt corpus
v0.9,splitting here
v0.9,In the huner split files there is no information whether a given id originates
v0.9,from the train or test file of the original corpus - so we have to adapt corpus
v0.9,splitting here
v0.9,Edge case: last token starts a new entity
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download file is huge => make default_dir visible so that derivative
v0.9,corpora can all use the same download file
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,Read texts
v0.9,Read annotations
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,We need to apply a patch to correct the original training file
v0.9,Articles title
v0.9,Article abstract
v0.9,Entity annotations
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,Edge case: last token starts a new entity
v0.9,Map all entities to chemicals
v0.9,Map all entities to disease
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,Incomplete article
v0.9,Invalid XML syntax
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,if len(mid) != 3:
v0.9,continue
v0.9,Try to fix entity offsets
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,There is still one illegal annotation in the file ..
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,"Abstract first, title second to prevent issues with sentence splitting"
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,"Filter for specific entity types, by default no entities will be filtered"
v0.9,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.9,train and dev split of V2 will be train in V4
v0.9,test split of V2 will be dev in V4
v0.9,New documents in V4 will become test documents
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,column format
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,cache Feidegger config file
v0.9,cache Feidegger images
v0.9,replace image URL with local cached file
v0.9,append Sentence-Image data point
v0.9,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.9,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.9,"if so, num_workers is set to 0 for faster processing"
v0.9,cast to list if necessary
v0.9,cast to list if necessary
v0.9,"first, check if pymongo is installed"
v0.9,automatically identify train / test / dev files
v0.9,"if no test file is found, take any file with 'test' in name"
v0.9,Expose base classses
v0.9,Expose all sequence labeling datasets
v0.9,standard NER datasets
v0.9,other NER datasets
v0.9,keyphrase detection datasets
v0.9,word sense disambiugation
v0.9,universal proposition banks
v0.9,Expose all entity linking datasets
v0.9,Expose all document classification datasets
v0.9,Expose all treebanks
v0.9,Expose all text-text datasets
v0.9,Expose all text-image datasets
v0.9,Expose all biomedical data sets
v0.9,Expose all biomedical data sets using the HUNER splits
v0.9,-
v0.9,-
v0.9,-
v0.9,-
v0.9,Expose all biomedical data sets used for the evaluation of BioBERT
v0.9,Expose all relation extraction datasets
v0.9,"find train, dev and test files if not specified"
v0.9,get train data
v0.9,get test data
v0.9,get dev data
v0.9,option 1: read only sentence boundaries as offset positions
v0.9,option 2: keep everything in memory
v0.9,"if in memory, retrieve parsed sentence"
v0.9,else skip to position in file where sentence begins
v0.9,current token ID
v0.9,handling for the awful UD multiword format
v0.9,end of sentence
v0.9,comments
v0.9,ellipsis
v0.9,if token is a multi-word
v0.9,normal single-word tokens
v0.9,"if we don't split multiwords, skip over component words"
v0.9,add token
v0.9,add morphological tags
v0.9,derive whitespace logic for multiwords
v0.9,print(token)
v0.9,print(current_multiword_last_token)
v0.9,print(current_multiword_first_token)
v0.9,"if multi-word equals component tokens, there should be no whitespace"
v0.9,go through all tokens in subword and set whitespace_after information
v0.9,print(i)
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,this dataset name
v0.9,default dataset folder is the cache root
v0.9,download data if necessary
v0.9,test corpus
v0.9,create a TARS classifier
v0.9,check if right number of classes
v0.9,switch to task with only one label
v0.9,check if right number of classes
v0.9,switch to task with three labels provided as list
v0.9,check if right number of classes
v0.9,switch to task with four labels provided as set
v0.9,check if right number of classes
v0.9,switch to task with two labels provided as Dictionary
v0.9,check if right number of classes
v0.9,clean up file
v0.9,bioes tags
v0.9,bio tags
v0.9,broken tags
v0.9,all tags
v0.9,all weird tags
v0.9,tags with confidence
v0.9,bioes tags
v0.9,bioes tags
v0.9,"city single-token, person and company multi-token"
v0.9,increment for last token in sentence if not followed by whitespace
v0.9,clean up directory
v0.9,clean up directory
v0.9,initialize trainer
v0.9,clean up results directory
v0.9,initialize trainer
v0.9,clean up results directory
v0.9,initialize trainer
v0.9,clean up results directory
v0.9,initialize trainer
v0.9,clean up results directory
v0.9,initialize trainer
v0.9,clean up results directory
v0.9,initialize trainer
v0.9,clean up results directory
v0.9,clean up results directory
v0.9,initialize trainer
v0.9,clean up results directory
v0.9,from flair.trainers.trainer_regression import RegressorTrainer
v0.9,def test_trainer_evaluation(tasks_base_path):
v0.9,"corpus, model, trainer = init(tasks_base_path)"
v0.9,
v0.9,expected = model.evaluate(corpus.dev)
v0.9,
v0.9,assert expected is not None
v0.9,def test_trainer_results(tasks_base_path):
v0.9,"corpus, model, trainer = init(tasks_base_path)"
v0.9,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.9,"assert results[""test_score""] > 0"
v0.9,"assert len(results[""dev_loss_history""]) == 1"
v0.9,"assert len(results[""dev_score_history""]) == 1"
v0.9,"assert len(results[""train_loss_history""]) == 1"
v0.9,@pytest.mark.integration
v0.9,initialize trainer
v0.9,"loaded_model.predict([sentence, sentence_empty])"
v0.9,loaded_model.predict([sentence_empty])
v0.9,clean up results directory
v0.9,get default dictionary
v0.9,init forward LM with 128 hidden states and 1 layer
v0.9,get the example corpus and process at character level in forward direction
v0.9,train the language model
v0.9,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.9,clean up results directory
v0.9,get default dictionary
v0.9,init forward LM with 128 hidden states and 1 layer
v0.9,get the example corpus and process at character level in forward direction
v0.9,train the language model
v0.9,clean up results directory
v0.9,define search space
v0.9,sequence tagger parameter
v0.9,model trainer parameter
v0.9,training parameter
v0.9,find best parameter settings
v0.9,clean up results directory
v0.9,document embeddings parameter
v0.9,training parameter
v0.9,clean up results directory
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,"get training, test and dev data"
v0.9,get two corpora as one
v0.9,"get training, test and dev data for full English UD corpus from web"
v0.9,clean up data directory
v0.9,load dataset
v0.9,tagger without CRF
v0.9,train
v0.9,check if loaded model can predict
v0.9,check if loaded model successfully fit the training data
v0.9,clean up results directory
v0.9,load dataset
v0.9,tagger without CRF
v0.9,train
v0.9,check if loaded model can predict
v0.9,check if loaded model successfully fit the training data
v0.9,clean up results directory
v0.9,load dataset
v0.9,tagger without CRF
v0.9,train
v0.9,check if loaded model can predict
v0.9,check if loaded model successfully fit the training data
v0.9,clean up results directory
v0.9,load dataset
v0.9,tagger without CRF
v0.9,train
v0.9,check if loaded model can predict
v0.9,check if loaded model successfully fit the training data
v0.9,clean up results directory
v0.9,check if model can predict
v0.9,load model
v0.9,chcek if model predicts correct label
v0.9,check if loaded model successfully fit the training data
v0.9,clean up results directory
v0.9,check if model can predict
v0.9,load model
v0.9,chcek if model predicts correct label
v0.9,check if loaded model successfully fit the training data
v0.9,clean up results directory
v0.9,check if model can predict
v0.9,load model
v0.9,chcek if model predicts correct label
v0.9,check if loaded model successfully fit the training data
v0.9,clean up results directory
v0.9,clean up results directory
v0.9,clean up results directory
v0.9,clean up results directory
v0.9,clean up results directory
v0.9,clean up results directory
v0.9,clean up results directory
v0.9,def test_labels_to_indices(tasks_base_path):
v0.9,"corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"", label_type=""topic"")"
v0.9,label_dict = corpus.make_label_dictionary()
v0.9,"model = TextClassifier(document_embeddings,"
v0.9,"label_dictionary=label_dict,"
v0.9,"label_type=""topic"","
v0.9,multi_label=False)
v0.9,
v0.9,result = model._labels_to_indices(corpus.train)
v0.9,
v0.9,for i in range(len(corpus.train)):
v0.9,expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value)
v0.9,actual = result[i].item()
v0.9,
v0.9,assert expected == actual
v0.9,
v0.9,
v0.9,def test_labels_to_one_hot(tasks_base_path):
v0.9,"corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"", label_type=""topic"")"
v0.9,label_dict = corpus.make_label_dictionary()
v0.9,"model = TextClassifier(document_embeddings,"
v0.9,"label_dictionary=label_dict,"
v0.9,"label_type=""topic"","
v0.9,multi_label=False)
v0.9,
v0.9,result = model._labels_to_one_hot(corpus.train)
v0.9,
v0.9,for i in range(len(corpus.train)):
v0.9,expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value)
v0.9,actual = result[i]
v0.9,
v0.9,for idx in range(len(label_dict)):
v0.9,if idx == expected:
v0.9,assert actual[idx] == 1
v0.9,else:
v0.9,assert actual[idx] == 0
v0.8,1. get the corpus
v0.8,2. what tag do we want to predict?
v0.8,3. make the tag dictionary from the corpus
v0.8,initialize embeddings
v0.8,comment in this line to use character embeddings
v0.8,"CharacterEmbeddings(),"
v0.8,comment in these lines to use contextual string embeddings
v0.8,
v0.8,"FlairEmbeddings('news-forward'),"
v0.8,
v0.8,"FlairEmbeddings('news-backward'),"
v0.8,initialize sequence tagger
v0.8,initialize trainer
v0.8,from allennlp.common.tqdm import Tqdm
v0.8,mmap seems to be much more memory efficient
v0.8,Remove quotes from etag
v0.8,"If there is an etag, it's everything after the first period"
v0.8,"Otherwise, use None"
v0.8,"URL, so get it from the cache (downloading if necessary)"
v0.8,"File, and it exists."
v0.8,"File, but it doesn't exist."
v0.8,Something unknown
v0.8,Extract all the contents of zip file in current directory
v0.8,Extract all the contents of zip file in current directory
v0.8,get cache path to put the file
v0.8,"Download to temporary file, then copy to cache dir once finished."
v0.8,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.8,GET file object
v0.8,TODO(joelgrus): do we want to do checksums or anything like that?
v0.8,get cache path to put the file
v0.8,make HEAD request to check ETag
v0.8,add ETag to filename if it exists
v0.8,"etag = response.headers.get(""ETag"")"
v0.8,"Download to temporary file, then copy to cache dir once finished."
v0.8,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.8,GET file object
v0.8,These defaults are the same as the argument defaults in tqdm.
v0.8,first determine the distribution of classes in the dataset
v0.8,weight for each sample
v0.8,Create blocks
v0.8,shuffle the blocks
v0.8,concatenate the shuffled blocks
v0.8,Create blocks
v0.8,shuffle the blocks
v0.8,concatenate the shuffled blocks
v0.8,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.8,see https://github.com/zalandoresearch/flair/issues/351
v0.8,State initialization
v0.8,Exponential moving average of gradient values
v0.8,Exponential moving average of squared gradient values
v0.8,Maintains max of all exp. moving avg. of sq. grad. values
v0.8,Decay the first and second moment running average coefficient
v0.8,Maintains the maximum of all 2nd moment running avg. till now
v0.8,Use the max. for normalizing running avg. of gradient
v0.8,determine offsets for whitespace_after field
v0.8,increment for last token in sentence if not followed by whitespace
v0.8,determine offsets for whitespace_after field
v0.8,conll 2000 column format
v0.8,conll 03 NER column format
v0.8,WNUT-17
v0.8,-- WikiNER datasets
v0.8,-- Universal Dependencies
v0.8,Germanic
v0.8,Romance
v0.8,West-Slavic
v0.8,South-Slavic
v0.8,East-Slavic
v0.8,Scandinavian
v0.8,Asian
v0.8,Language isolates
v0.8,recent Universal Dependencies
v0.8,other datasets
v0.8,text classification format
v0.8,text regression format
v0.8,"first, try to fetch dataset online"
v0.8,default dataset folder is the cache root
v0.8,get string value if enum is passed
v0.8,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.8,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.8,the CoNLL 03 task for German has an additional lemma column
v0.8,the CoNLL 03 task for Dutch has no NP column
v0.8,the CoNLL 03 task for Spanish only has two columns
v0.8,the GERMEVAL task only has two columns: text and ner
v0.8,WSD tasks may be put into this column format
v0.8,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.8,"for text classifiers, we use our own special format"
v0.8,NER corpus for Basque
v0.8,automatically identify train / test / dev files
v0.8,"if no test file is found, take any file with 'test' in name"
v0.8,get train and test data
v0.8,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.8,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.8,convert tag scheme to iobes
v0.8,automatically identify train / test / dev files
v0.8,automatically identify train / test / dev files
v0.8,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.8,conll 2000 chunking task
v0.8,Support both TREC-6 and TREC-50
v0.8,Create flair compatible labels
v0.8,TREC-6 : NUM:dist -> __label__NUM
v0.8,TREC-50: NUM:dist -> __label__NUM:dist
v0.8,Wikiner NER task
v0.8,unpack and write out in CoNLL column-like format
v0.8,CoNLL 02/03 NER
v0.8,universal dependencies
v0.8,--- UD Germanic
v0.8,--- UD Romance
v0.8,--- UD West-Slavic
v0.8,--- UD Scandinavian
v0.8,--- UD South-Slavic
v0.8,--- UD Asian
v0.8,this is the default init size of a lmdb database for embeddings
v0.8,some non-used parameter to allow print
v0.8,get db filename from embedding name
v0.8,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.8,SequenceTagger
v0.8,TextClassifier
v0.8,get db filename from embedding name
v0.8,if embedding database already exists
v0.8,"otherwise, push embedding to database"
v0.8,if embedding database already exists
v0.8,open the database in read mode
v0.8,we need to set self.k
v0.8,create and load the database in write mode
v0.8,"no idea why, but we need to close and reopen the environment to avoid"
v0.8,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.8,when opening new transaction !
v0.8,init dictionaries
v0.8,"in order to deal with unknown tokens, add <unk>"
v0.8,"We don't want to create a SpaceTokenizer object each time this function is called,"
v0.8,so delegate the call directly to the static run_tokenize method
v0.8,"We don't want to create a SegtokTokenizer object each time this function is called,"
v0.8,so delegate the call directly to the static run_tokenize method
v0.8,"if text is passed, instantiate sentence with tokens (words)"
v0.8,log a warning if the dataset is empty
v0.8,some sentences represent a document boundary (but most do not)
v0.8,data with zero-width characters cannot be handled
v0.8,set token idx if not set
v0.8,non-set tags are OUT tags
v0.8,anything that is not a BIOES tag is a SINGLE tag
v0.8,anything that is not OUT is IN
v0.8,single and begin tags start a new span
v0.8,remember previous tag
v0.8,"if label type is explicitly specified, get spans for this label type"
v0.8,else determine all label types in sentence and get all spans
v0.8,move sentence embeddings to device
v0.8,move token embeddings to device
v0.8,clear sentence embeddings
v0.8,clear token embeddings
v0.8,infer whitespace after field
v0.8,add Sentence labels to output if they exist
v0.8,add Token labels to output if they exist
v0.8,add Sentence labels to output if they exist
v0.8,add Token labels to output if they exist
v0.8,No character at the corresponding code point: remove it
v0.8,set name
v0.8,sample test data if none is provided
v0.8,sample dev data if none is provided
v0.8,set train dev and test data
v0.8,find out empty sentence indices
v0.8,create subset of non-empty sentence indices
v0.8,find out empty sentence indices
v0.8,create subset of non-empty sentence indices
v0.8,check if sentence itself has labels
v0.8,check for labels of words
v0.8,Make the tag dictionary
v0.8,global variable: cache_root
v0.8,global variable: device
v0.8,global variable: embedding_storage_mode
v0.8,# dummy return to fulfill trainer.train() needs
v0.8,print(vec)
v0.8,Attach optimizer
v0.8,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.8,if memory mode option 'none' delete everything
v0.8,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.8,find out which ones are dynamic embeddings
v0.8,find out which ones are dynamic embeddings
v0.8,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.8,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.8,header for 'weights.txt'
v0.8,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.8,then get all relevant values from the tsv
v0.8,then get all relevant values from the tsv
v0.8,plot i
v0.8,save plots
v0.8,save plots
v0.8,plt.show()
v0.8,save plot
v0.8,take the average over the last three scores of training
v0.8,take average over the scores from the different training runs
v0.8,remove previous embeddings
v0.8,clearing token embeddings to save memory
v0.8,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.8,#TODO: not saving lines yet
v0.8,== similarity measures ==
v0.8,helper class for ModelSimilarity
v0.8,-- works with binary cross entropy loss --
v0.8,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.8,-- works with ranking/triplet loss --
v0.8,normalize the embeddings
v0.8,== similarity losses ==
v0.8,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.8,TODO: this assumes eye matrix
v0.8,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.8,== similarity learner ==
v0.8,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.8,assumes that for each data pair there's at least one embedding per modality
v0.8,pre-compute embeddings for all targets in evaluation dataset
v0.8,compute the similarity
v0.8,sort the similarity matrix across modality 1
v0.8,"get the ranks, so +1 to start counting ranks from 1"
v0.8,The conversion from old model's constructor interface
v0.8,auto-spawn on GPU if available
v0.8,pad strings with whitespaces to longest sentence
v0.8,cut up the input into chunks of max charlength = chunk_size
v0.8,push each chunk through the RNN language model
v0.8,concatenate all chunks to make final output
v0.8,initial hidden state
v0.8,get predicted weights
v0.8,divide by temperature
v0.8,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.8,this makes a vector in which the largest value is 0
v0.8,compute word weights with exponential function
v0.8,try sampling multinomial distribution for next character
v0.8,print(word_idx)
v0.8,input ids
v0.8,push list of character IDs through model
v0.8,the target is always the next character
v0.8,use cross entropy loss to compare output of forward pass with targets
v0.8,exponentiate cross-entropy loss to calculate perplexity
v0.8,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.8,"check if this is the case and if so, set it"
v0.8,set the dictionaries
v0.8,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.8,Initialize the weight tensor
v0.8,initialize the network architecture
v0.8,dropouts
v0.8,optional reprojection layer on top of word embeddings
v0.8,bidirectional LSTM on top of embedding layer
v0.8,Create initial hidden state and initialize it
v0.8,TODO: Decide how to initialize the hidden state variables
v0.8,self.hs_initializer(self.lstm_init_h)
v0.8,self.hs_initializer(self.lstm_init_c)
v0.8,final linear map to tag space
v0.8,set context if not set already
v0.8,reverse sort all sequences by their length
v0.8,progress bar for verbosity
v0.8,stop if all sentences are empty
v0.8,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.8,clearing token embeddings to save memory
v0.8,predict for batch
v0.8,make list of gold tags
v0.8,make list of predicted tags
v0.8,"check for true positives, false positives and false negatives"
v0.8,also write to file in BIO format to use old conlleval script
v0.8,check if in gold spans
v0.8,check if in predicted spans
v0.8,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.8,"if span F1 needs to be used, use separate eval method"
v0.8,"else, use scikit-learn to evaluate"
v0.8,predict for batch
v0.8,add gold tag
v0.8,add predicted tag
v0.8,for file output
v0.8,use sklearn
v0.8,"make ""classification report"""
v0.8,report over all in case there are no labels
v0.8,get scores
v0.8,line for log file
v0.8,--------------------------------------------------------------------
v0.8,FF PART
v0.8,--------------------------------------------------------------------
v0.8,"if initial hidden state is trainable, use this state"
v0.8,word dropout only before LSTM - TODO: more experimentation needed
v0.8,if self.use_word_dropout > 0.0:
v0.8,sentence_tensor = self.word_dropout(sentence_tensor)
v0.8,get the tags in this sentence
v0.8,add tags as tensor
v0.8,pad tags if using batch-CRF decoder
v0.8,reduce raw values to avoid NaN during exp
v0.8,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.8,default value
v0.8,core Flair models on Huggingface ModelHub
v0.8,"Large NER models,"
v0.8,Multilingual NER models
v0.8,English POS models
v0.8,Multilingual POS models
v0.8,English SRL models
v0.8,English chunking models
v0.8,Language-specific NER models
v0.8,English NER models
v0.8,Multilingual NER models
v0.8,English POS models
v0.8,Multilingual POS models
v0.8,English SRL models
v0.8,English chunking models
v0.8,Danish models
v0.8,German models
v0.8,French models
v0.8,Dutch models
v0.8,Malayalam models
v0.8,Portuguese models
v0.8,Keyphase models
v0.8,Biomedical models
v0.8,check if model name is a valid local file
v0.8,"check if model key is remapped to HF key - if so, print out information"
v0.8,get mapped name
v0.8,output information
v0.8,use mapped name instead
v0.8,"if not, check if model key is remapped to direct download location. If so, download model"
v0.8,special handling for the taggers by the @redewiegergabe project (TODO: move to model hub)
v0.8,"for all other cases (not local file or special download location), use HF model hub"
v0.8,"if not a local file, get from model hub"
v0.8,use model name as subfolder
v0.8,Lazy import
v0.8,output information
v0.8,"log.error(f"" - Error message: {e}"")"
v0.8,clear embeddings after predicting
v0.8,load each model
v0.8,check if the same embeddings were already loaded previously
v0.8,"if the model uses StackedEmbedding, make a new stack with previous objects"
v0.8,sort embeddings by key alphabetically
v0.8,check previous embeddings and add if found
v0.8,only re-use static embeddings
v0.8,"if not found, use existing embedding"
v0.8,initialize new stack
v0.8,"of the model uses regular embedding, re-load if previous version found"
v0.8,Initialize the weight tensor
v0.8,auto-spawn on GPU if available
v0.8,filter empty sentences
v0.8,reverse sort all sequences by their length
v0.8,progress bar for verbosity
v0.8,stop if all sentences are empty
v0.8,clearing token embeddings to save memory
v0.8,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.8,use scikit-learn to evaluate
v0.8,remove previously predicted labels
v0.8,get the gold labels
v0.8,predict for batch
v0.8,get the predicted labels
v0.8,remove predicted labels
v0.8,"make ""classification report"""
v0.8,get scores
v0.8,line for log file
v0.8,English sentiment models
v0.8,Communicative Functions Model
v0.8,Initialize TextClassifier
v0.8,if bi_mode == True the linear layer needs twice the length of the embeddings as input size
v0.8,since we concatenate the embeddings of the two DataPoints in the DataPairs
v0.8,TODO: Transformers use special separator symbols in the beginning and between elements
v0.8,of datapair. Here should be a case dinstintion between the different transformers.
v0.8,linear layer
v0.8,Drop unnecessary attributes from Parent class
v0.8,prepare binary label dictionary
v0.8,Store task specific labels since TARS can handle multiple tasks
v0.8,make label dictionary if no Dictionary object is passed
v0.8,get and embed all labels by making a Sentence object that contains only the label text
v0.8,get each label embedding and scale between 0 and 1
v0.8,compute similarity matrix
v0.8,"the higher the similarity, the greater the chance that a label is"
v0.8,sampled as negative example
v0.8,make sure the probabilities always sum up to 1
v0.8,init new TARS classifier
v0.8,set all task information
v0.8,linear layers of internal classifier
v0.8,Transform input data into TARS format
v0.8,"M: num_classes in task, N: num_samples"
v0.8,reshape scores MN x 2 -> N x M x 2
v0.8,import torch
v0.8,a = torch.arange(30)
v0.8,"b = torch.reshape(-1, 3, 2)"
v0.8,"c = b[:,:,1]"
v0.8,target shape N x M
v0.8,Transform label_scores
v0.8,Transform label_scores into current task's desired format
v0.8,"TARS does not do a softmax, so confidence of the best predicted class might be very low."
v0.8,Therefore enforce a min confidence of 0.5 for a match.
v0.8,make list if only one candidate label is passed
v0.8,"if list is passed, convert to set"
v0.8,check if candidate_label_set is empty
v0.8,note current task
v0.8,create a temporary task
v0.8,make zero shot predictions
v0.8,switch to the pre-existing task
v0.8,remember current task
v0.8,predict with each task model
v0.8,switch to the pre-existing task
v0.8,embeddings
v0.8,dictionaries
v0.8,linear layer
v0.8,F-beta score
v0.8,all parameters will be pushed internally to the specified device
v0.8,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.8,"if span F1 needs to be used, use separate eval method"
v0.8,"else, use scikit-learn to evaluate"
v0.8,predict for batch
v0.8,add gold tag
v0.8,add predicted tag
v0.8,for file output
v0.8,use sklearn
v0.8,"make ""classification report"""
v0.8,report over all in case there are no labels
v0.8,get scores
v0.8,line for log file
v0.8,reverse sort all sequences by their length
v0.8,progress bar for verbosity
v0.8,stop if all sentences are empty
v0.8,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.8,clearing token embeddings to save memory
v0.8,get the tags in this sentence
v0.8,add tags as tensor
v0.8,predict for batch
v0.8,make list of gold tags
v0.8,make list of predicted tags
v0.8,"check for true positives, false positives and false negatives"
v0.8,also write to file in BIO format to use old conlleval script
v0.8,check if in gold spans
v0.8,check if in predicted spans
v0.8,weights for loss function
v0.8,iput size is two times wordembedding size since we use pair of words as input
v0.8,"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
v0.8,regression
v0.8,input size is two times word embedding size since we use pair of words as input
v0.8,the output size is 1
v0.8,auto-spawn on GPU if available
v0.8,all input should be tensors
v0.8,forward allows only a single sentcence!!
v0.8,embed words of sentence
v0.8,go through all pairs of words with a maximum number of max_distance in between
v0.8,go through all pairs
v0.8,2-dim matrix whose rows are the embeddings of word pairs of the sentence
v0.8,So far only one sentence allowed
v0.8,If list of sentences is handed the function works with the first sentence of the list
v0.8,Assume data_points is a single sentence!!!
v0.8,scores are the predictions for each word pair
v0.8,"classification needs labels to be integers, regression needs labels to be float"
v0.8,this is due to the different loss functions
v0.8,only single sentences as input
v0.8,gold labels
v0.8,for output text file
v0.8,for buckets
v0.8,for average prediction
v0.8,add some statistics to the output
v0.8,use scikit-learn to evaluate
v0.8,"we iterate over each sentence, instead of batches"
v0.8,get single labels from scores
v0.8,gold labels
v0.8,for output text file
v0.8,hot one vector of true value
v0.8,hot one vector of predicted value
v0.8,"speichert embeddings, falls embedding_storage!= 'None'"
v0.8,"make ""classification report"""
v0.8,get scores
v0.8,"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.8,"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.8,line for log file
v0.8,cast string to Path
v0.8,"determine what splits (train, dev, test) to evaluate and log"
v0.8,prepare loss logging file and set up header
v0.8,"minimize training loss if training with dev data, else maximize dev score"
v0.8,"if training also uses dev/train data, include in training set"
v0.8,initialize sampler if provided
v0.8,init with default values if only class is provided
v0.8,set dataset to sample from
v0.8,At any point you can hit Ctrl + C to break out of training early.
v0.8,get new learning rate
v0.8,reload last best model if annealing with restarts is enabled
v0.8,stop training if learning rate becomes too small
v0.8,process mini-batches
v0.8,zero the gradients on the model and optimizer
v0.8,"if necessary, make batch_steps"
v0.8,forward and backward for batch
v0.8,forward pass
v0.8,Backward
v0.8,do the optimizer step
v0.8,do the scheduler step if one-cycle
v0.8,get new learning rate
v0.8,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.8,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.8,evaluate on train / dev / test split depending on training settings
v0.8,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.8,calculate scores using dev data if available
v0.8,append dev score to score history
v0.8,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.8,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.8,determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
v0.8,determine bad epoch number
v0.8,log bad epochs
v0.8,output log file
v0.8,make headers on first epoch
v0.8,"if checkpoint is enabled, save model at each epoch"
v0.8,"if we use dev data, remember best model based on dev evaluation score"
v0.8,"if we do not use dev data for model selection, save final model"
v0.8,test best model if test data is present
v0.8,"if we are training over multiple datasets, do evaluation for each"
v0.8,get and return the final test score of best model
v0.8,cast string to Path
v0.8,forward pass
v0.8,update optimizer and scheduler
v0.8,Add chars to the dictionary
v0.8,charsplit file content
v0.8,charsplit file content
v0.8,Add words to the dictionary
v0.8,Tokenize file content
v0.8,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.8,cast string to Path
v0.8,error message if the validation dataset is too small
v0.8,Shuffle training files randomly after serially iterating through corpus one
v0.8,"iterate through training data, starting at self.split (for checkpointing)"
v0.8,off by one for printing
v0.8,go into train mode
v0.8,reset variables
v0.8,not really sure what this does
v0.8,do the forward pass in the model
v0.8,try to predict the targets
v0.8,Backward
v0.8,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.8,We detach the hidden state from how it was previously produced.
v0.8,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.8,explicitly remove loss to clear up memory
v0.8,##############################################################################
v0.8,Save the model if the validation loss is the best we've seen so far.
v0.8,##############################################################################
v0.8,print info
v0.8,##############################################################################
v0.8,##############################################################################
v0.8,final testing
v0.8,##############################################################################
v0.8,Turn on evaluation mode which disables dropout.
v0.8,Work out how cleanly we can divide the dataset into bsz parts.
v0.8,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.8,Evenly divide the data across the bsz batches.
v0.8,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.8,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.8,news-english-forward
v0.8,news-english-backward
v0.8,news-english-forward
v0.8,news-english-backward
v0.8,mix-english-forward
v0.8,mix-english-backward
v0.8,mix-german-forward
v0.8,mix-german-backward
v0.8,common crawl Polish forward
v0.8,common crawl Polish backward
v0.8,Slovenian forward
v0.8,Slovenian backward
v0.8,Bulgarian forward
v0.8,Bulgarian backward
v0.8,Dutch forward
v0.8,Dutch backward
v0.8,Swedish forward
v0.8,Swedish backward
v0.8,French forward
v0.8,French backward
v0.8,Czech forward
v0.8,Czech backward
v0.8,Portuguese forward
v0.8,Portuguese backward
v0.8,initialize cache if use_cache set
v0.8,embed a dummy sentence to determine embedding_length
v0.8,set to eval mode
v0.8,Copy the object's state from self.__dict__ which contains
v0.8,all our instance attributes. Always use the dict.copy()
v0.8,method to avoid modifying the original state.
v0.8,Remove the unpicklable entries.
v0.8,"if cache is used, try setting embeddings from cache first"
v0.8,try populating embeddings from cache
v0.8,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.8,get hidden states from language model
v0.8,take first or last hidden states from language model as word representation
v0.8,if self.tokenized_lm or token.whitespace_after:
v0.8,1-camembert-base -> camembert-base
v0.8,1-xlm-roberta-large -> xlm-roberta-large
v0.8,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.8,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.8,tokens are attended to.
v0.8,Zero-pad up to the sequence length.
v0.8,"first, find longest sentence in batch"
v0.8,prepare id maps for BERT model
v0.8,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.8,get aggregated embeddings for each BERT-subtoken in sentence
v0.8,get the current sentence object
v0.8,add concatenated embedding to sentence
v0.8,use first subword embedding if pooling operation is 'first'
v0.8,"otherwise, do a mean over all subwords in token"
v0.8,"if only one sentence is passed, convert to list of sentence"
v0.8,bidirectional LSTM on top of embedding layer
v0.8,dropouts
v0.8,"first, sort sentences by number of tokens"
v0.8,go through each sentence in batch
v0.8,PADDING: pad shorter sentences out
v0.8,ADD TO SENTENCE LIST: add the representation
v0.8,--------------------------------------------------------------------
v0.8,GET REPRESENTATION FOR ENTIRE BATCH
v0.8,--------------------------------------------------------------------
v0.8,--------------------------------------------------------------------
v0.8,FF PART
v0.8,--------------------------------------------------------------------
v0.8,use word dropout if set
v0.8,--------------------------------------------------------------------
v0.8,EXTRACT EMBEDDINGS FROM LSTM
v0.8,--------------------------------------------------------------------
v0.8,embed a dummy sentence to determine embedding_length
v0.8,Avoid conflicts with flair's Token class
v0.8,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.8,add positional encodings
v0.8,reshape the pixels into the sequence
v0.8,layer norm after convolution and positional encodings
v0.8,add <cls> token
v0.8,"transformer requires input in the shape [h*w+1, b, d]"
v0.8,the output is an embedding of <cls> token
v0.8,temporary fix to disable tokenizer parallelism warning
v0.8,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.8,load tokenizer and transformer model
v0.8,model name
v0.8,"when initializing, embeddings are in eval mode by default"
v0.8,embedding parameters
v0.8,send mini-token through to check how many layers the model has
v0.8,check whether CLS is at beginning or end
v0.8,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.8,using list comprehension
v0.8,gradients are enabled if fine-tuning is enabled
v0.8,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.8,subtokenize sentences
v0.8,tokenize and truncate to max subtokens (TODO: check better truncation strategies)
v0.8,find longest sentence in batch
v0.8,initialize batch tensors and mask
v0.8,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.8,iterate over all subtokenized sentences
v0.8,use scalar mix of embeddings if so selected
v0.8,set the extracted embedding for the token
v0.8,special handling for serializing transformer models
v0.8,serialize the transformer models and the constructor arguments (but nothing else)
v0.8,necessary for reverse compatibility with Flair <= 0.7
v0.8,special handling for deserializing transformer models
v0.8,load transformer model
v0.8,constructor arguments
v0.8,re-initialize transformer word embeddings with constructor arguments
v0.8,"I have no idea why this is necessary, but otherwise it doesn't work"
v0.8,reload tokenizer to get around serialization issues
v0.8,optional fine-tuning on top of embedding layer
v0.8,"if only one sentence is passed, convert to list of sentence"
v0.8,"if only one sentence is passed, convert to list of sentence"
v0.8,bidirectional RNN on top of embedding layer
v0.8,dropouts
v0.8,TODO: remove in future versions
v0.8,embed words in the sentence
v0.8,before-RNN dropout
v0.8,reproject if set
v0.8,push through RNN
v0.8,after-RNN dropout
v0.8,extract embeddings from RNN
v0.8,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.8,"check if this is the case and if so, set it"
v0.8,IMPORTANT: add embeddings as torch modules
v0.8,iterate over sentences
v0.8,"if its a forward LM, take last state"
v0.8,"convert to plain strings, embedded in a list for the encode function"
v0.8,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.8,"if only one sentence is passed, convert to list of sentence"
v0.8,Expose base classses
v0.8,Expose token embedding classes
v0.8,Expose document embedding classes
v0.8,Expose image embedding classes
v0.8,Expose legacy embedding classes
v0.8,IMPORTANT: add embeddings as torch modules
v0.8,"if only one sentence is passed, convert to list of sentence"
v0.8,GLOVE embeddings
v0.8,TURIAN embeddings
v0.8,KOMNINOS embeddings
v0.8,pubmed embeddings
v0.8,FT-CRAWL embeddings
v0.8,FT-CRAWL embeddings
v0.8,twitter embeddings
v0.8,two-letter language code wiki embeddings
v0.8,two-letter language code wiki embeddings
v0.8,two-letter language code crawl embeddings
v0.8,fix serialized models
v0.8,use list of common characters if none provided
v0.8,translate words in sentence into ints using dictionary
v0.8,"sort words by length, for batching and masking"
v0.8,chars for rnn processing
v0.8,multilingual models
v0.8,English models
v0.8,Arabic
v0.8,Bulgarian
v0.8,Czech
v0.8,Danish
v0.8,German
v0.8,Spanish
v0.8,Basque
v0.8,Persian
v0.8,Finnish
v0.8,French
v0.8,Hebrew
v0.8,Hindi
v0.8,Croatian
v0.8,Indonesian
v0.8,Italian
v0.8,Japanese
v0.8,Malayalam
v0.8,Dutch
v0.8,Norwegian
v0.8,Polish
v0.8,Portuguese
v0.8,Pubmed
v0.8,Slovenian
v0.8,Swedish
v0.8,Tamil
v0.8,CLEF HIPE Shared task
v0.8,load model if in pretrained model map
v0.8,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.8,embeddings are static if we don't do finetuning
v0.8,embed a dummy sentence to determine embedding_length
v0.8,set to eval mode
v0.8,make compatible with serialized models (TODO: remove)
v0.8,make compatible with serialized models (TODO: remove)
v0.8,gradients are enable if fine-tuning is enabled
v0.8,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.8,get hidden states from language model
v0.8,take first or last hidden states from language model as word representation
v0.8,offset mode that extracts at whitespace after last character
v0.8,offset mode that extracts at last character
v0.8,only clone if optimization mode is 'gpu'
v0.8,use the character language model embeddings as basis
v0.8,length is twice the original character LM embedding length
v0.8,these fields are for the embedding memory
v0.8,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.8,we re-compute embeddings dynamically at each epoch
v0.8,set the memory method
v0.8,memory is wiped each time we do a training run
v0.8,"if we keep a pooling, it needs to be updated continuously"
v0.8,update embedding
v0.8,check token.text is empty or not
v0.8,set aggregation operation
v0.8,add embeddings after updating
v0.8,temporary fix to disable tokenizer parallelism warning
v0.8,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.8,load tokenizer and transformer model
v0.8,model name
v0.8,whether to detach gradients on overlong sentences
v0.8,store whether to use context (and how much)
v0.8,dropout contexts
v0.8,"if using context, can we cross document boundaries?"
v0.8,"when initializing, embeddings are in eval mode by default"
v0.8,embedding parameters
v0.8,send mini-token through to check how many layers the model has
v0.8,calculate embedding length
v0.8,return length
v0.8,check if special tokens exist to circumvent error message
v0.8,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.8,remove special markup
v0.8,embed each sentence separately
v0.8,"TODO: keep for backwards compatibility, but remove in future"
v0.8,"some pretrained models do not have this property, applying default settings now."
v0.8,can be set manually after loading the model.
v0.8,"if we also use context, first expand sentence to include context"
v0.8,"in case of contextualization, we must remember non-expanded sentence"
v0.8,create expanded sentence and remember context offsets
v0.8,overwrite sentence with expanded sentence
v0.8,subtokenize the sentence
v0.8,method 1: subtokenize sentence
v0.8,"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
v0.8,method 2:
v0.8,transformer specific tokenization
v0.8,set zero embeddings for empty sentences and return
v0.8,determine into how many subtokens each token is split
v0.8,"if sentence is too long, will be split into multiple parts"
v0.8,check if transformer version 3 is used - in this case use old handling
v0.8,get sentence as list of subtoken ids
v0.8,"else if current transformer is used, use default handling"
v0.8,overlong sentences are handled as multiple splits
v0.8,embed each sentence split
v0.8,initialize batch tensors and mask
v0.8,propagate gradients if fine-tuning and only during training
v0.8,increase memory effectiveness by skipping all but last sentence split
v0.8,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.8,get hidden states as single tensor
v0.8,put splits back together into one tensor using overlapping strides
v0.8,"for each token, get embedding"
v0.8,some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
v0.8,"get states from all selected layers, aggregate with pooling operation"
v0.8,use layer mean of embeddings if so selected
v0.8,set the extracted embedding for the token
v0.8,move embeddings from context back to original sentence (if using context)
v0.8,remember original sentence
v0.8,get left context
v0.8,get right context
v0.8,make expanded sentence
v0.8,iterate over subtokens and reconstruct tokens
v0.8,remove special markup
v0.8,TODO check if this is necessary is this method is called before prepare_for_model
v0.8,check if reconstructed token is special begin token ([CLS] or similar)
v0.8,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.8,append subtoken to reconstruct token
v0.8,check if reconstructed token is the same as current token
v0.8,"if so, add subtoken count"
v0.8,reset subtoken count and reconstructed token
v0.8,break from loop if all tokens are accounted for
v0.8,if tokens are unaccounted for
v0.8,check if all tokens were matched to subtokens
v0.8,"""""""Returns the length of the embedding vector."""""""
v0.8,special handling for serializing transformer models
v0.8,serialize the transformer models and the constructor arguments (but nothing else)
v0.8,necessary for reverse compatibility with Flair <= 0.7
v0.8,special handling for deserializing transformer models
v0.8,load transformer model
v0.8,constructor arguments
v0.8,re-initialize transformer word embeddings with constructor arguments
v0.8,"I have no idea why this is necessary, but otherwise it doesn't work"
v0.8,reload tokenizer to get around serialization issues
v0.8,max_tokens = 500
v0.8,model architecture
v0.8,model architecture
v0.8,download if necessary
v0.8,load the model
v0.8,"TODO: keep for backwards compatibility, but remove in future"
v0.8,save the sentence piece model as binary file (not as path which may change)
v0.8,write out the binary sentence piece model into the expected directory
v0.8,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.8,"otherwise, use normal process and potentially trigger another download"
v0.8,"once the modes if there, load it with sentence piece"
v0.8,empty words get no embedding
v0.8,all other words get embedded
v0.8,"the default model for ELMo is the 'original' model, which is very large"
v0.8,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.8,put on Cuda if available
v0.8,embed a dummy sentence to determine embedding_length
v0.8,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.8,GLOVE embeddings
v0.8,"find train, dev and test files if not specified"
v0.8,get train data
v0.8,read in test file if exists
v0.8,read in dev file if exists
v0.8,special key for space after
v0.8,"store either Sentence objects in memory, or only file offsets"
v0.8,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.8,determine encoding of text file
v0.8,skip first line if to selected
v0.8,option 1: read only sentence boundaries as offset positions
v0.8,option 2: keep everything in memory
v0.8,pointer to previous
v0.8,"if sentence ends, break"
v0.8,skip comments
v0.8,"if sentence ends, convert and return"
v0.8,check if this sentence is a document boundary
v0.8,"otherwise, this line is a token. parse and add to sentence"
v0.8,check if this sentence is a document boundary
v0.8,"if in memory, retrieve parsed sentence"
v0.8,else skip to position in file where sentence begins
v0.8,set sentence context using partials
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.8,"tag_to_bioes=tag_to_bioes,"
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,check if data there
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,check if data there
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download files if not present locally
v0.8,we need to slightly modify the original files by adding some new lines after document separators
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,"cached_path(f""{STACKOVERFLOW_NER_path}train_merged_labels.txt"", Path(""datasets"") / dataset_name) # TODO: what is this?"
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,If the extracted corpus file is not yet present in dir
v0.8,download zip if necessary
v0.8,"extracted corpus is not present , so unpacking it."
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,Remove CoNLL-U meta information in the last column
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,check if data there
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,rename according to train - test - dev - convention
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,dataset name
v0.8,data folder: default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,dataset name
v0.8,data folder: default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,data is not in IOB2 format. Thus we transform it to IOB2
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,tokens to ignore (edit here for variation)
v0.8,counter to keep track how many tags have been found in line
v0.8,variable to count of how many words a chunk consists
v0.8,indicates if surface form is chunk or not
v0.8,array to save tags temporarily for handling chunks
v0.8,cut token to get chunk
v0.8,save single words of chunk
v0.8,handle first word of chunk
v0.8,edit here for variation
v0.8,check if converted file exists
v0.8,convert the file to CoNLL
v0.8,column format
v0.8,
v0.8,since only the WordNet 3.0 version for senses is consistently available for all provided datasets we will
v0.8,only consider this version
v0.8,
v0.8,also we ignore the id annotation used in datasets that were originally created for evaluation tasks
v0.8,
v0.8,if the other annotations should be needed simply add the columns in correct order according
v0.8,to the chosen datasets here and respectively change the values of the blacklist array and
v0.8,the range value of the else case in the token for loop in the from_ufsac_to_conll function
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,check if data there
v0.8,determine correct CoNLL files
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,"For each language in languages, the file is downloaded if not existent"
v0.8,Then a comlumncorpus of that data is created and saved in a list
v0.8,this list is handed to the multicorpus
v0.8,list that contains the columncopora
v0.8,download data if necessary
v0.8,"if language not downloaded yet, download it"
v0.8,create folder
v0.8,get google drive id from list
v0.8,download from google drive
v0.8,unzip
v0.8,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.8,transform data into required format
v0.8,"the processed dataset has the additional ending ""_new"""
v0.8,remove the unprocessed dataset
v0.8,initialize comlumncorpus and add it to list
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.8,download data if necessary
v0.8,unpack and write out in CoNLL column-like format
v0.8,if no languages are given as argument all languages used in XTREME will be loaded
v0.8,if only one language is given
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,"For each language in languages, the file is downloaded if not existent"
v0.8,Then a comlumncorpus of that data is created and saved in a list
v0.8,This list is handed to the multicorpus
v0.8,list that contains the columncopora
v0.8,download data if necessary
v0.8,"if language not downloaded yet, download it"
v0.8,create folder
v0.8,download from HU Server
v0.8,unzip
v0.8,transform data into required format
v0.8,initialize comlumncorpus and add it to list
v0.8,"find train, dev and test files if not specified"
v0.8,use test_file to create test split if available
v0.8,use dev_file to create test split if available
v0.8,"if data point contains black-listed label, do not use"
v0.8,first check if valid sentence
v0.8,"if so, add to indices"
v0.8,"find train, dev and test files if not specified"
v0.8,variables
v0.8,different handling of in_memory data than streaming data
v0.8,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.8,test if format is OK
v0.8,test if at least one label given
v0.8,noinspection PyDefaultArgument
v0.8,dataset name includes the split size
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download each of the 28 splits
v0.8,create dataset directory if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,create dataset directory if necessary
v0.8,create train.txt file from CSV
v0.8,create test.txt file from CSV
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,create dataset directory if necessary
v0.8,create train.txt file by iterating over pos and neg file
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,create dataset directory if necessary
v0.8,create train.txt file by iterating over pos and neg file
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,create dataset directory if necessary
v0.8,create train.txt file by iterating over pos and neg file
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,create dataset directory if necessary
v0.8,create train.txt file by iterating over pos and neg file
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,create train.txt file by iterating over pos and neg file
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download senteval datasets if necessary und unzip
v0.8,convert to FastText format
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,download datasets if necessary
v0.8,create dataset directory if necessary
v0.8,create correctly formated txt files
v0.8,multiple labels are possible
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,Create flair compatible labels
v0.8,TREC-6 : NUM:dist -> __label__NUM
v0.8,TREC-50: NUM:dist -> __label__NUM:dist
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,Create flair compatible labels
v0.8,TREC-6 : NUM:dist -> __label__NUM
v0.8,TREC-50: NUM:dist -> __label__NUM:dist
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,create a separate directory for different tasks
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,check if dataset is supported
v0.8,set file names
v0.8,download and unzip in file structure if necessary
v0.8,instantiate corpus
v0.8,"find train, dev and test files if not specified"
v0.8,"create DataPairDataset for train, test and dev file, if they are given"
v0.8,stop if file does not exist
v0.8,create a DataPair object from strings
v0.8,"if in_memory is True we return a datapair, otherwise we create one from the lists of strings"
v0.8,if no base_path provided take cache root
v0.8,"if data is not downloaded yet, download it"
v0.8,get the zip file
v0.8,"rename test file to eval_dataset, since it has no labels"
v0.8,if no base_path provided take cache root
v0.8,"if data not downloaded yet, download it"
v0.8,get the zip file
v0.8,"the downloaded files have json format, we transform them to tsv"
v0.8,Function to transform JSON file to tsv for Recognizing Textual Entailment Data
v0.8,remove json file
v0.8,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.8,with sum of all entity lengths as secondary key
v0.8,calculate offset without current text
v0.8,because we stick all passages of a document together
v0.8,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.8,Try to fix incorrect annotations
v0.8,print(
v0.8,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.8,)
v0.8,Ignore empty lines or relation annotations
v0.8,FIX annotation of whitespaces (necessary for PDR)
v0.8,One token may contain multiple entities -> deque all of them
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.8,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,Edge case: last token starts a new entity
v0.8,Last document in file
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,In the huner split files there is no information whether a given id originates
v0.8,from the train or test file of the original corpus - so we have to adapt corpus
v0.8,splitting here
v0.8,In the huner split files there is no information whether a given id originates
v0.8,from the train or test file of the original corpus - so we have to adapt corpus
v0.8,splitting here
v0.8,In the huner split files there is no information whether a given id originates
v0.8,from the train or test file of the original corpus - so we have to adapt corpus
v0.8,splitting here
v0.8,Edge case: last token starts a new entity
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download file is huge => make default_dir visible so that derivative
v0.8,corpora can all use the same download file
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,Read texts
v0.8,Read annotations
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,We need to apply a patch to correct the original training file
v0.8,Articles title
v0.8,Article abstract
v0.8,Entity annotations
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,Edge case: last token starts a new entity
v0.8,Map all entities to chemicals
v0.8,Map all entities to disease
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,Incomplete article
v0.8,Invalid XML syntax
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,if len(mid) != 3:
v0.8,continue
v0.8,Try to fix entity offsets
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,There is still one illegal annotation in the file ..
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,"Abstract first, title second to prevent issues with sentence splitting"
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,"Filter for specific entity types, by default no entities will be filtered"
v0.8,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.8,train and dev split of V2 will be train in V4
v0.8,test split of V2 will be dev in V4
v0.8,New documents in V4 will become test documents
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,column format
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,cache Feidegger config file
v0.8,cache Feidegger images
v0.8,replace image URL with local cached file
v0.8,append Sentence-Image data point
v0.8,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.8,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.8,"if so, num_workers is set to 0 for faster processing"
v0.8,cast to list if necessary
v0.8,cast to list if necessary
v0.8,"first, check if pymongo is installed"
v0.8,automatically identify train / test / dev files
v0.8,"if no test file is found, take any file with 'test' in name"
v0.8,Expose base classses
v0.8,Expose all sequence labeling datasets
v0.8,Expose all document classification datasets
v0.8,Expose all treebanks
v0.8,Expose all text-text datasets
v0.8,Expose all text-image datasets
v0.8,Expose all biomedical data sets
v0.8,Expose all biomedical data sets using the HUNER splits
v0.8,-
v0.8,-
v0.8,-
v0.8,-
v0.8,Expose all biomedical data sets used for the evaluation of BioBERT
v0.8,"find train, dev and test files if not specified"
v0.8,get train data
v0.8,get test data
v0.8,get dev data
v0.8,option 1: read only sentence boundaries as offset positions
v0.8,option 2: keep everything in memory
v0.8,"if in memory, retrieve parsed sentence"
v0.8,else skip to position in file where sentence begins
v0.8,current token ID
v0.8,handling for the awful UD multiword format
v0.8,end of sentence
v0.8,comments
v0.8,ellipsis
v0.8,if token is a multi-word
v0.8,normal single-word tokens
v0.8,"if we don't split multiwords, skip over component words"
v0.8,add token
v0.8,add morphological tags
v0.8,derive whitespace logic for multiwords
v0.8,print(token)
v0.8,print(current_multiword_last_token)
v0.8,print(current_multiword_first_token)
v0.8,"if multi-word equals component tokens, there should be no whitespace"
v0.8,go through all tokens in subword and set whitespace_after information
v0.8,print(i)
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,download data if necessary
v0.8,this dataset name
v0.8,default dataset folder is the cache root
v0.8,test corpus
v0.8,create a TARS classifier
v0.8,check if right number of classes
v0.8,switch to task with only one label
v0.8,check if right number of classes
v0.8,switch to task with three labels provided as list
v0.8,check if right number of classes
v0.8,switch to task with four labels provided as set
v0.8,check if right number of classes
v0.8,switch to task with two labels provided as Dictionary
v0.8,check if right number of classes
v0.8,clean up file
v0.8,bioes tags
v0.8,bio tags
v0.8,broken tags
v0.8,all tags
v0.8,all weird tags
v0.8,tags with confidence
v0.8,bioes tags
v0.8,bioes tags
v0.8,increment for last token in sentence if not followed by whitespace
v0.8,clean up directory
v0.8,clean up directory
v0.8,initialize trainer
v0.8,clean up results directory
v0.8,initialize trainer
v0.8,clean up results directory
v0.8,initialize trainer
v0.8,clean up results directory
v0.8,initialize trainer
v0.8,clean up results directory
v0.8,initialize trainer
v0.8,clean up results directory
v0.8,clean up results directory
v0.8,initialize trainer
v0.8,clean up results directory
v0.8,def test_multiclass_metrics():
v0.8,
v0.8,"metric = Metric(""Test"")"
v0.8,"available_labels = [""A"", ""B"", ""C""]"
v0.8,
v0.8,"predictions = [""A"", ""B""]"
v0.8,"true_values = [""A""]"
v0.8,TextClassifier._evaluate_sentence_for_text_classification(
v0.8,"metric, available_labels, predictions, true_values"
v0.8,)
v0.8,
v0.8,"predictions = [""C"", ""B""]"
v0.8,"true_values = [""A"", ""B""]"
v0.8,TextClassifier._evaluate_sentence_for_text_classification(
v0.8,"metric, available_labels, predictions, true_values"
v0.8,)
v0.8,
v0.8,print(metric)
v0.8,from flair.trainers.trainer_regression import RegressorTrainer
v0.8,def test_trainer_results(tasks_base_path):
v0.8,"corpus, model, trainer = init(tasks_base_path)"
v0.8,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.8,"assert results[""test_score""] > 0"
v0.8,"assert len(results[""dev_loss_history""]) == 1"
v0.8,"assert len(results[""dev_score_history""]) == 1"
v0.8,"assert len(results[""train_loss_history""]) == 1"
v0.8,get default dictionary
v0.8,init forward LM with 128 hidden states and 1 layer
v0.8,get the example corpus and process at character level in forward direction
v0.8,train the language model
v0.8,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.8,clean up results directory
v0.8,get default dictionary
v0.8,init forward LM with 128 hidden states and 1 layer
v0.8,get the example corpus and process at character level in forward direction
v0.8,train the language model
v0.8,clean up results directory
v0.8,define search space
v0.8,sequence tagger parameter
v0.8,model trainer parameter
v0.8,training parameter
v0.8,find best parameter settings
v0.8,clean up results directory
v0.8,document embeddings parameter
v0.8,training parameter
v0.8,clean up results directory
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,"get training, test and dev data"
v0.8,get two corpora as one
v0.8,"get training, test and dev data for full English UD corpus from web"
v0.8,clean up data directory
v0.8,clean up results directory
v0.8,clean up results directory
v0.8,clean up results directory
v0.8,clean up results directory
v0.8,clean up results directory
v0.8,clean up results directory
v0.7,1. get the corpus
v0.7,2. what tag do we want to predict?
v0.7,3. make the tag dictionary from the corpus
v0.7,initialize embeddings
v0.7,comment in this line to use character embeddings
v0.7,"CharacterEmbeddings(),"
v0.7,comment in these lines to use contextual string embeddings
v0.7,
v0.7,"FlairEmbeddings('news-forward'),"
v0.7,
v0.7,"FlairEmbeddings('news-backward'),"
v0.7,initialize sequence tagger
v0.7,initialize trainer
v0.7,from allennlp.common.tqdm import Tqdm
v0.7,mmap seems to be much more memory efficient
v0.7,Remove quotes from etag
v0.7,"If there is an etag, it's everything after the first period"
v0.7,"Otherwise, use None"
v0.7,"URL, so get it from the cache (downloading if necessary)"
v0.7,"File, and it exists."
v0.7,"File, but it doesn't exist."
v0.7,Something unknown
v0.7,Extract all the contents of zip file in current directory
v0.7,Extract all the contents of zip file in current directory
v0.7,get cache path to put the file
v0.7,"Download to temporary file, then copy to cache dir once finished."
v0.7,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.7,GET file object
v0.7,TODO(joelgrus): do we want to do checksums or anything like that?
v0.7,get cache path to put the file
v0.7,make HEAD request to check ETag
v0.7,add ETag to filename if it exists
v0.7,"etag = response.headers.get(""ETag"")"
v0.7,"Download to temporary file, then copy to cache dir once finished."
v0.7,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.7,GET file object
v0.7,These defaults are the same as the argument defaults in tqdm.
v0.7,first determine the distribution of classes in the dataset
v0.7,weight for each sample
v0.7,Create blocks
v0.7,shuffle the blocks
v0.7,concatenate the shuffled blocks
v0.7,Create blocks
v0.7,shuffle the blocks
v0.7,concatenate the shuffled blocks
v0.7,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.7,see https://github.com/zalandoresearch/flair/issues/351
v0.7,State initialization
v0.7,Exponential moving average of gradient values
v0.7,Exponential moving average of squared gradient values
v0.7,Maintains max of all exp. moving avg. of sq. grad. values
v0.7,Decay the first and second moment running average coefficient
v0.7,Maintains the maximum of all 2nd moment running avg. till now
v0.7,Use the max. for normalizing running avg. of gradient
v0.7,determine offsets for whitespace_after field
v0.7,increment for last token in sentence if not followed by whitespace
v0.7,determine offsets for whitespace_after field
v0.7,conll 2000 column format
v0.7,conll 03 NER column format
v0.7,WNUT-17
v0.7,-- WikiNER datasets
v0.7,-- Universal Dependencies
v0.7,Germanic
v0.7,Romance
v0.7,West-Slavic
v0.7,South-Slavic
v0.7,East-Slavic
v0.7,Scandinavian
v0.7,Asian
v0.7,Language isolates
v0.7,recent Universal Dependencies
v0.7,other datasets
v0.7,text classification format
v0.7,text regression format
v0.7,"first, try to fetch dataset online"
v0.7,default dataset folder is the cache root
v0.7,get string value if enum is passed
v0.7,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.7,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.7,the CoNLL 03 task for German has an additional lemma column
v0.7,the CoNLL 03 task for Dutch has no NP column
v0.7,the CoNLL 03 task for Spanish only has two columns
v0.7,the GERMEVAL task only has two columns: text and ner
v0.7,WSD tasks may be put into this column format
v0.7,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.7,"for text classifiers, we use our own special format"
v0.7,NER corpus for Basque
v0.7,automatically identify train / test / dev files
v0.7,"if no test file is found, take any file with 'test' in name"
v0.7,get train and test data
v0.7,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.7,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.7,convert tag scheme to iobes
v0.7,automatically identify train / test / dev files
v0.7,automatically identify train / test / dev files
v0.7,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.7,conll 2000 chunking task
v0.7,Support both TREC-6 and TREC-50
v0.7,Create flair compatible labels
v0.7,TREC-6 : NUM:dist -> __label__NUM
v0.7,TREC-50: NUM:dist -> __label__NUM:dist
v0.7,Wikiner NER task
v0.7,unpack and write out in CoNLL column-like format
v0.7,CoNLL 02/03 NER
v0.7,universal dependencies
v0.7,--- UD Germanic
v0.7,--- UD Romance
v0.7,--- UD West-Slavic
v0.7,--- UD Scandinavian
v0.7,--- UD South-Slavic
v0.7,--- UD Asian
v0.7,this is the default init size of a lmdb database for embeddings
v0.7,some non-used parameter to allow print
v0.7,get db filename from embedding name
v0.7,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.7,SequenceTagger
v0.7,TextClassifier
v0.7,get db filename from embedding name
v0.7,if embedding database already exists
v0.7,"otherwise, push embedding to database"
v0.7,if embedding database already exists
v0.7,open the database in read mode
v0.7,we need to set self.k
v0.7,create and load the database in write mode
v0.7,"no idea why, but we need to close and reopen the environment to avoid"
v0.7,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.7,when opening new transaction !
v0.7,init dictionaries
v0.7,"in order to deal with unknown tokens, add <unk>"
v0.7,"We don't want to create a SpaceTokenizer object each time this function is called,"
v0.7,so delegate the call directly to the static run_tokenize method
v0.7,"We don't want to create a SegtokTokenizer object each time this function is called,"
v0.7,so delegate the call directly to the static run_tokenize method
v0.7,"if text is passed, instantiate sentence with tokens (words)"
v0.7,log a warning if the dataset is empty
v0.7,data with zero-width characters cannot be handled
v0.7,set token idx if not set
v0.7,non-set tags are OUT tags
v0.7,anything that is not a BIOES tag is a SINGLE tag
v0.7,anything that is not OUT is IN
v0.7,single and begin tags start a new span
v0.7,remember previous tag
v0.7,"if label type is explicitly specified, get spans for this label type"
v0.7,else determine all label types in sentence and get all spans
v0.7,move sentence embeddings to device
v0.7,move token embeddings to device
v0.7,clear sentence embeddings
v0.7,clear token embeddings
v0.7,infer whitespace after field
v0.7,add Sentence labels to output if they exist
v0.7,add Token labels to output if they exist
v0.7,add Sentence labels to output if they exist
v0.7,add Token labels to output if they exist
v0.7,No character at the corresponding code point: remove it
v0.7,set name
v0.7,sample test data if none is provided
v0.7,sample dev data if none is provided
v0.7,set train dev and test data
v0.7,find out empty sentence indices
v0.7,create subset of non-empty sentence indices
v0.7,find out empty sentence indices
v0.7,create subset of non-empty sentence indices
v0.7,check if sentence itself has labels
v0.7,check for labels of words
v0.7,Make the tag dictionary
v0.7,global variable: cache_root
v0.7,global variable: device
v0.7,global variable: embedding_storage_mode
v0.7,# dummy return to fulfill trainer.train() needs
v0.7,print(vec)
v0.7,Attach optimizer
v0.7,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.7,if memory mode option 'none' delete everything
v0.7,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.7,find out which ones are dynamic embeddings
v0.7,find out which ones are dynamic embeddings
v0.7,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.7,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.7,header for 'weights.txt'
v0.7,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.7,then get all relevant values from the tsv
v0.7,then get all relevant values from the tsv
v0.7,plot i
v0.7,save plots
v0.7,save plots
v0.7,plt.show()
v0.7,save plot
v0.7,take the average over the last three scores of training
v0.7,take average over the scores from the different training runs
v0.7,remove previous embeddings
v0.7,clearing token embeddings to save memory
v0.7,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.7,#TODO: not saving lines yet
v0.7,== similarity measures ==
v0.7,helper class for ModelSimilarity
v0.7,-- works with binary cross entropy loss --
v0.7,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.7,-- works with ranking/triplet loss --
v0.7,normalize the embeddings
v0.7,== similarity losses ==
v0.7,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.7,TODO: this assumes eye matrix
v0.7,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.7,== similarity learner ==
v0.7,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.7,assumes that for each data pair there's at least one embedding per modality
v0.7,pre-compute embeddings for all targets in evaluation dataset
v0.7,compute the similarity
v0.7,sort the similarity matrix across modality 1
v0.7,"get the ranks, so +1 to start counting ranks from 1"
v0.7,The conversion from old model's constructor interface
v0.7,auto-spawn on GPU if available
v0.7,pad strings with whitespaces to longest sentence
v0.7,cut up the input into chunks of max charlength = chunk_size
v0.7,push each chunk through the RNN language model
v0.7,concatenate all chunks to make final output
v0.7,initial hidden state
v0.7,get predicted weights
v0.7,divide by temperature
v0.7,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.7,this makes a vector in which the largest value is 0
v0.7,compute word weights with exponential function
v0.7,try sampling multinomial distribution for next character
v0.7,print(word_idx)
v0.7,input ids
v0.7,push list of character IDs through model
v0.7,the target is always the next character
v0.7,use cross entropy loss to compare output of forward pass with targets
v0.7,exponentiate cross-entropy loss to calculate perplexity
v0.7,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.7,"check if this is the case and if so, set it"
v0.7,set the dictionaries
v0.7,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.7,Initialize the weight tensor
v0.7,initialize the network architecture
v0.7,dropouts
v0.7,optional reprojection layer on top of word embeddings
v0.7,bidirectional LSTM on top of embedding layer
v0.7,Create initial hidden state and initialize it
v0.7,TODO: Decide how to initialize the hidden state variables
v0.7,self.hs_initializer(self.lstm_init_h)
v0.7,self.hs_initializer(self.lstm_init_c)
v0.7,final linear map to tag space
v0.7,reverse sort all sequences by their length
v0.7,progress bar for verbosity
v0.7,stop if all sentences are empty
v0.7,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.7,clearing token embeddings to save memory
v0.7,predict for batch
v0.7,make list of gold tags
v0.7,make list of predicted tags
v0.7,"check for true positives, false positives and false negatives"
v0.7,also write to file in BIO format to use old conlleval script
v0.7,check if in gold spans
v0.7,check if in predicted spans
v0.7,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.7,"if span F1 needs to be used, use separate eval method"
v0.7,"else, use scikit-learn to evaluate"
v0.7,predict for batch
v0.7,add gold tag
v0.7,add predicted tag
v0.7,for file output
v0.7,use sklearn
v0.7,"make ""classification report"""
v0.7,report over all in case there are no labels
v0.7,get scores
v0.7,line for log file
v0.7,--------------------------------------------------------------------
v0.7,FF PART
v0.7,--------------------------------------------------------------------
v0.7,"if initial hidden state is trainable, use this state"
v0.7,word dropout only before LSTM - TODO: more experimentation needed
v0.7,if self.use_word_dropout > 0.0:
v0.7,sentence_tensor = self.word_dropout(sentence_tensor)
v0.7,get the tags in this sentence
v0.7,add tags as tensor
v0.7,pad tags if using batch-CRF decoder
v0.7,reduce raw values to avoid NaN during exp
v0.7,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.7,default value
v0.7,English NER models
v0.7,Multilingual NER models
v0.7,English POS models
v0.7,Multilingual POS models
v0.7,English SRL models
v0.7,English chunking models
v0.7,Danish models
v0.7,German models
v0.7,French models
v0.7,Dutch models
v0.7,Malayalam models
v0.7,Portuguese models
v0.7,Keyphase models
v0.7,Biomedical models
v0.7,the historical German taggers by the @redewiegergabe project
v0.7,Fallback to Hugging Face model hub
v0.7,e.g. stefan-it/flair-ner-conll03 is a valid namespace
v0.7,and  stefan-it/flair-ner-conll03@main supports specifying a commit/branch name
v0.7,Lazy import
v0.7,clear embeddings after predicting
v0.7,load each model
v0.7,check if the same embeddings were already loaded previously
v0.7,"if the model uses StackedEmbedding, make a new stack with previous objects"
v0.7,sort embeddings by key alphabetically
v0.7,check previous embeddings and add if found
v0.7,only re-use static embeddings
v0.7,"if not found, use existing embedding"
v0.7,initialize new stack
v0.7,"of the model uses regular embedding, re-load if previous version found"
v0.7,Initialize the weight tensor
v0.7,auto-spawn on GPU if available
v0.7,filter empty sentences
v0.7,reverse sort all sequences by their length
v0.7,progress bar for verbosity
v0.7,stop if all sentences are empty
v0.7,clearing token embeddings to save memory
v0.7,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.7,use scikit-learn to evaluate
v0.7,remove previously predicted labels
v0.7,get the gold labels
v0.7,predict for batch
v0.7,get the predicted labels
v0.7,remove predicted labels
v0.7,"make ""classification report"""
v0.7,get scores
v0.7,line for log file
v0.7,English sentiment models
v0.7,Communicative Functions Model
v0.7,Drop unnecessary attributes from Parent class
v0.7,prepare binary label dictionary
v0.7,Store task specific labels since TARS can handle multiple tasks
v0.7,make label dictionary if no Dictionary object is passed
v0.7,get and embed all labels by making a Sentence object that contains only the label text
v0.7,get each label embedding and scale between 0 and 1
v0.7,compute similarity matrix
v0.7,"the higher the similarity, the greater the chance that a label is"
v0.7,sampled as negative example
v0.7,make sure the probabilities always sum up to 1
v0.7,Transform input data into TARS format
v0.7,"M: num_classes in task, N: num_samples"
v0.7,reshape scores MN x 2 -> N x M x 2
v0.7,import torch
v0.7,a = torch.arange(30)
v0.7,"b = torch.reshape(-1, 3, 2)"
v0.7,"c = b[:,:,1]"
v0.7,target shape N x M
v0.7,Transform label_scores
v0.7,Transform label_scores into current task's desired format
v0.7,"TARS does not do a softmax, so confidence of the best predicted class might be very low."
v0.7,Therefore enforce a min confidence of 0.5 for a match.
v0.7,make list if only one candidate label is passed
v0.7,"if list is passed, convert to set"
v0.7,check if candidate_label_set is empty
v0.7,note current task
v0.7,create a temporary task
v0.7,make zero shot predictions
v0.7,switch to the pre-existing task
v0.7,remember current task
v0.7,predict with each task model
v0.7,switch to the pre-existing task
v0.7,embeddings
v0.7,dictionaries
v0.7,linear layer
v0.7,F-beta score
v0.7,all parameters will be pushed internally to the specified device
v0.7,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.7,"if span F1 needs to be used, use separate eval method"
v0.7,"else, use scikit-learn to evaluate"
v0.7,predict for batch
v0.7,add gold tag
v0.7,add predicted tag
v0.7,for file output
v0.7,use sklearn
v0.7,"make ""classification report"""
v0.7,report over all in case there are no labels
v0.7,get scores
v0.7,line for log file
v0.7,reverse sort all sequences by their length
v0.7,progress bar for verbosity
v0.7,stop if all sentences are empty
v0.7,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.7,clearing token embeddings to save memory
v0.7,get the tags in this sentence
v0.7,add tags as tensor
v0.7,predict for batch
v0.7,make list of gold tags
v0.7,make list of predicted tags
v0.7,"check for true positives, false positives and false negatives"
v0.7,also write to file in BIO format to use old conlleval script
v0.7,check if in gold spans
v0.7,check if in predicted spans
v0.7,weights for loss function
v0.7,iput size is two times wordembedding size since we use pair of words as input
v0.7,"the output size is max_distance + 1, i.e. we allow 0,1,...,max_distance words between pairs"
v0.7,regression
v0.7,input size is two times word embedding size since we use pair of words as input
v0.7,the output size is 1
v0.7,auto-spawn on GPU if available
v0.7,all input should be tensors
v0.7,forward allows only a single sentcence!!
v0.7,embed words of sentence
v0.7,go through all pairs of words with a maximum number of max_distance in between
v0.7,go through all pairs
v0.7,2-dim matrix whose rows are the embeddings of word pairs of the sentence
v0.7,So far only one sentence allowed
v0.7,If list of sentences is handed the function works with the first sentence of the list
v0.7,Assume data_points is a single sentence!!!
v0.7,scores are the predictions for each word pair
v0.7,"classification needs labels to be integers, regression needs labels to be float"
v0.7,this is due to the different loss functions
v0.7,only single sentences as input
v0.7,gold labels
v0.7,for output text file
v0.7,for buckets
v0.7,for average prediction
v0.7,add some statistics to the output
v0.7,use scikit-learn to evaluate
v0.7,"we iterate over each sentence, instead of batches"
v0.7,get single labels from scores
v0.7,gold labels
v0.7,for output text file
v0.7,hot one vector of true value
v0.7,hot one vector of predicted value
v0.7,"speichert embeddings, falls embedding_storage!= 'None'"
v0.7,"make ""classification report"""
v0.7,get scores
v0.7,"precision_score = round(metrics.precision_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.7,"recall_score = round(metrics.recall_score(y_true, y_pred, average='macro', zero_division=0), 4)"
v0.7,line for log file
v0.7,cast string to Path
v0.7,"determine what splits (train, dev, test) to evaluate and log"
v0.7,prepare loss logging file and set up header
v0.7,"minimize training loss if training with dev data, else maximize dev score"
v0.7,"if training also uses dev data, include in training set"
v0.7,initialize sampler if provided
v0.7,init with default values if only class is provided
v0.7,set dataset to sample from
v0.7,At any point you can hit Ctrl + C to break out of training early.
v0.7,get new learning rate
v0.7,reload last best model if annealing with restarts is enabled
v0.7,stop training if learning rate becomes too small
v0.7,process mini-batches
v0.7,zero the gradients on the model and optimizer
v0.7,"if necessary, make batch_steps"
v0.7,forward and backward for batch
v0.7,forward pass
v0.7,Backward
v0.7,do the optimizer step
v0.7,do the scheduler step if one-cycle
v0.7,get new learning rate
v0.7,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.7,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.7,evaluate on train / dev / test split depending on training settings
v0.7,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.7,calculate scores using dev data if available
v0.7,append dev score to score history
v0.7,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.7,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.7,determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
v0.7,determine bad epoch number
v0.7,log bad epochs
v0.7,output log file
v0.7,make headers on first epoch
v0.7,"if checkpoint is enabled, save model at each epoch"
v0.7,"if we use dev data, remember best model based on dev evaluation score"
v0.7,"if we do not use dev data for model selection, save final model"
v0.7,test best model if test data is present
v0.7,"if we are training over multiple datasets, do evaluation for each"
v0.7,get and return the final test score of best model
v0.7,cast string to Path
v0.7,forward pass
v0.7,update optimizer and scheduler
v0.7,Add chars to the dictionary
v0.7,charsplit file content
v0.7,charsplit file content
v0.7,Add words to the dictionary
v0.7,Tokenize file content
v0.7,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.7,cast string to Path
v0.7,error message if the validation dataset is too small
v0.7,Shuffle training files randomly after serially iterating through corpus one
v0.7,"iterate through training data, starting at self.split (for checkpointing)"
v0.7,off by one for printing
v0.7,go into train mode
v0.7,reset variables
v0.7,not really sure what this does
v0.7,do the forward pass in the model
v0.7,try to predict the targets
v0.7,Backward
v0.7,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.7,We detach the hidden state from how it was previously produced.
v0.7,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.7,explicitly remove loss to clear up memory
v0.7,##############################################################################
v0.7,Save the model if the validation loss is the best we've seen so far.
v0.7,##############################################################################
v0.7,print info
v0.7,##############################################################################
v0.7,##############################################################################
v0.7,final testing
v0.7,##############################################################################
v0.7,Turn on evaluation mode which disables dropout.
v0.7,Work out how cleanly we can divide the dataset into bsz parts.
v0.7,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.7,Evenly divide the data across the bsz batches.
v0.7,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.7,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.7,news-english-forward
v0.7,news-english-backward
v0.7,news-english-forward
v0.7,news-english-backward
v0.7,mix-english-forward
v0.7,mix-english-backward
v0.7,mix-german-forward
v0.7,mix-german-backward
v0.7,common crawl Polish forward
v0.7,common crawl Polish backward
v0.7,Slovenian forward
v0.7,Slovenian backward
v0.7,Bulgarian forward
v0.7,Bulgarian backward
v0.7,Dutch forward
v0.7,Dutch backward
v0.7,Swedish forward
v0.7,Swedish backward
v0.7,French forward
v0.7,French backward
v0.7,Czech forward
v0.7,Czech backward
v0.7,Portuguese forward
v0.7,Portuguese backward
v0.7,initialize cache if use_cache set
v0.7,embed a dummy sentence to determine embedding_length
v0.7,set to eval mode
v0.7,Copy the object's state from self.__dict__ which contains
v0.7,all our instance attributes. Always use the dict.copy()
v0.7,method to avoid modifying the original state.
v0.7,Remove the unpicklable entries.
v0.7,"if cache is used, try setting embeddings from cache first"
v0.7,try populating embeddings from cache
v0.7,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.7,get hidden states from language model
v0.7,take first or last hidden states from language model as word representation
v0.7,if self.tokenized_lm or token.whitespace_after:
v0.7,1-camembert-base -> camembert-base
v0.7,1-xlm-roberta-large -> xlm-roberta-large
v0.7,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.7,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.7,tokens are attended to.
v0.7,Zero-pad up to the sequence length.
v0.7,"first, find longest sentence in batch"
v0.7,prepare id maps for BERT model
v0.7,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.7,get aggregated embeddings for each BERT-subtoken in sentence
v0.7,get the current sentence object
v0.7,add concatenated embedding to sentence
v0.7,use first subword embedding if pooling operation is 'first'
v0.7,"otherwise, do a mean over all subwords in token"
v0.7,"if only one sentence is passed, convert to list of sentence"
v0.7,bidirectional LSTM on top of embedding layer
v0.7,dropouts
v0.7,"first, sort sentences by number of tokens"
v0.7,go through each sentence in batch
v0.7,PADDING: pad shorter sentences out
v0.7,ADD TO SENTENCE LIST: add the representation
v0.7,--------------------------------------------------------------------
v0.7,GET REPRESENTATION FOR ENTIRE BATCH
v0.7,--------------------------------------------------------------------
v0.7,--------------------------------------------------------------------
v0.7,FF PART
v0.7,--------------------------------------------------------------------
v0.7,use word dropout if set
v0.7,--------------------------------------------------------------------
v0.7,EXTRACT EMBEDDINGS FROM LSTM
v0.7,--------------------------------------------------------------------
v0.7,embed a dummy sentence to determine embedding_length
v0.7,Avoid conflicts with flair's Token class
v0.7,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.7,add positional encodings
v0.7,reshape the pixels into the sequence
v0.7,layer norm after convolution and positional encodings
v0.7,add <cls> token
v0.7,"transformer requires input in the shape [h*w+1, b, d]"
v0.7,the output is an embedding of <cls> token
v0.7,temporary fix to disable tokenizer parallelism warning
v0.7,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.7,load tokenizer and transformer model
v0.7,model name
v0.7,"when initializing, embeddings are in eval mode by default"
v0.7,embedding parameters
v0.7,send mini-token through to check how many layers the model has
v0.7,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.7,using list comprehension
v0.7,gradients are enabled if fine-tuning is enabled
v0.7,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.7,subtokenize sentences
v0.7,tokenize and truncate to max subtokens (TODO: check better truncation strategies)
v0.7,find longest sentence in batch
v0.7,initialize batch tensors and mask
v0.7,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.7,iterate over all subtokenized sentences
v0.7,use scalar mix of embeddings if so selected
v0.7,set the extracted embedding for the token
v0.7,reload tokenizer to get around serialization issues
v0.7,optional fine-tuning on top of embedding layer
v0.7,"if only one sentence is passed, convert to list of sentence"
v0.7,bidirectional RNN on top of embedding layer
v0.7,dropouts
v0.7,TODO: remove in future versions
v0.7,embed words in the sentence
v0.7,before-RNN dropout
v0.7,reproject if set
v0.7,push through RNN
v0.7,after-RNN dropout
v0.7,extract embeddings from RNN
v0.7,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.7,"check if this is the case and if so, set it"
v0.7,IMPORTANT: add embeddings as torch modules
v0.7,iterate over sentences
v0.7,"if its a forward LM, take last state"
v0.7,"convert to plain strings, embedded in a list for the encode function"
v0.7,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.7,"if only one sentence is passed, convert to list of sentence"
v0.7,Expose base classses
v0.7,Expose token embedding classes
v0.7,Expose document embedding classes
v0.7,Expose image embedding classes
v0.7,Expose legacy embedding classes
v0.7,IMPORTANT: add embeddings as torch modules
v0.7,"if only one sentence is passed, convert to list of sentence"
v0.7,GLOVE embeddings
v0.7,TURIAN embeddings
v0.7,KOMNINOS embeddings
v0.7,pubmed embeddings
v0.7,FT-CRAWL embeddings
v0.7,FT-CRAWL embeddings
v0.7,twitter embeddings
v0.7,two-letter language code wiki embeddings
v0.7,two-letter language code wiki embeddings
v0.7,two-letter language code crawl embeddings
v0.7,fix serialized models
v0.7,use list of common characters if none provided
v0.7,translate words in sentence into ints using dictionary
v0.7,"sort words by length, for batching and masking"
v0.7,chars for rnn processing
v0.7,multilingual models
v0.7,English models
v0.7,Arabic
v0.7,Bulgarian
v0.7,Czech
v0.7,Danish
v0.7,German
v0.7,Spanish
v0.7,Basque
v0.7,Persian
v0.7,Finnish
v0.7,French
v0.7,Hebrew
v0.7,Hindi
v0.7,Croatian
v0.7,Indonesian
v0.7,Italian
v0.7,Japanese
v0.7,Malayalam
v0.7,Dutch
v0.7,Norwegian
v0.7,Polish
v0.7,Portuguese
v0.7,Pubmed
v0.7,Slovenian
v0.7,Swedish
v0.7,Tamil
v0.7,CLEF HIPE Shared task
v0.7,load model if in pretrained model map
v0.7,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.7,embeddings are static if we don't do finetuning
v0.7,embed a dummy sentence to determine embedding_length
v0.7,set to eval mode
v0.7,make compatible with serialized models (TODO: remove)
v0.7,make compatible with serialized models (TODO: remove)
v0.7,gradients are enable if fine-tuning is enabled
v0.7,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.7,get hidden states from language model
v0.7,take first or last hidden states from language model as word representation
v0.7,offset mode that extracts at whitespace after last character
v0.7,offset mode that extracts at last character
v0.7,only clone if optimization mode is 'gpu'
v0.7,use the character language model embeddings as basis
v0.7,length is twice the original character LM embedding length
v0.7,these fields are for the embedding memory
v0.7,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.7,we re-compute embeddings dynamically at each epoch
v0.7,set the memory method
v0.7,memory is wiped each time we do a training run
v0.7,"if we keep a pooling, it needs to be updated continuously"
v0.7,update embedding
v0.7,check token.text is empty or not
v0.7,set aggregation operation
v0.7,add embeddings after updating
v0.7,temporary fix to disable tokenizer parallelism warning
v0.7,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.7,load tokenizer and transformer model
v0.7,model name
v0.7,"when initializing, embeddings are in eval mode by default"
v0.7,embedding parameters
v0.7,send mini-token through to check how many layers the model has
v0.7,"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
v0.7,check if special tokens exist to circumvent error message
v0.7,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.7,split into micro batches of size self.batch_size before pushing through transformer
v0.7,embed each micro-batch
v0.7,remove special markup
v0.7,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.7,"TODO: keep for backwards compatibility, but remove in future"
v0.7,"some pretrained models do not have this property, applying default settings now."
v0.7,can be set manually after loading the model.
v0.7,method 1: subtokenize sentence
v0.7,"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
v0.7,method 2:
v0.7,transformer specific tokenization
v0.7,empty sentences get zero embeddings
v0.7,only embed non-empty sentences and if there is at least one
v0.7,find longest sentence in batch
v0.7,initialize batch tensors and mask
v0.7,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.7,make the tuple a tensor; makes working with it easier.
v0.7,gradients are enabled if fine-tuning is enabled
v0.7,iterate over all subtokenized sentences
v0.7,"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
v0.7,in order to get some context into the embeddings of these words.
v0.7,also don't include the embedding of the extra [CLS] and [SEP] tokens.
v0.7,"for each token, get embedding"
v0.7,some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
v0.7,"get states from all selected layers, aggregate with pooling operation"
v0.7,use scalar mix of embeddings if so selected
v0.7,sm_embeddings = self.mix(subtoken_embeddings)
v0.7,set the extracted embedding for the token
v0.7,iterate over subtokens and reconstruct tokens
v0.7,remove special markup
v0.7,TODO check if this is necessary is this method is called before prepare_for_model
v0.7,check if reconstructed token is special begin token ([CLS] or similar)
v0.7,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.7,append subtoken to reconstruct token
v0.7,check if reconstructed token is the same as current token
v0.7,"if so, add subtoken count"
v0.7,reset subtoken count and reconstructed token
v0.7,break from loop if all tokens are accounted for
v0.7,if tokens are unaccounted for
v0.7,check if all tokens were matched to subtokens
v0.7,"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
v0.7,module should never be in training mode
v0.7,reload tokenizer to get around serialization issues
v0.7,max_tokens = 500
v0.7,model architecture
v0.7,model architecture
v0.7,download if necessary
v0.7,load the model
v0.7,"TODO: keep for backwards compatibility, but remove in future"
v0.7,save the sentence piece model as binary file (not as path which may change)
v0.7,write out the binary sentence piece model into the expected directory
v0.7,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.7,"otherwise, use normal process and potentially trigger another download"
v0.7,"once the modes if there, load it with sentence piece"
v0.7,empty words get no embedding
v0.7,all other words get embedded
v0.7,"the default model for ELMo is the 'original' model, which is very large"
v0.7,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.7,put on Cuda if available
v0.7,embed a dummy sentence to determine embedding_length
v0.7,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.7,GLOVE embeddings
v0.7,"find train, dev and test files if not specified"
v0.7,get train data
v0.7,read in test file if exists
v0.7,read in dev file if exists
v0.7,special key for space after
v0.7,"store either Sentence objects in memory, or only file offsets"
v0.7,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.7,determine encoding of text file
v0.7,skip first line if to selected
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,"cached_path(f""{anercorp_path}test.txt"", Path(""datasets"") / dataset_name)"
v0.7,"tag_to_bioes=tag_to_bioes,"
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,check if data there
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,check if data there
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,Remove CoNLL-U meta information in the last column
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,check if data there
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,rename according to train - test - dev - convention
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,dataset name
v0.7,data folder: default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,dataset name
v0.7,data folder: default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,data is not in IOB2 format. Thus we transform it to IOB2
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,"For each language in languages, the file is downloaded if not existent"
v0.7,Then a comlumncorpus of that data is created and saved in a list
v0.7,this list is handed to the multicorpus
v0.7,list that contains the columncopora
v0.7,download data if necessary
v0.7,"if language not downloaded yet, download it"
v0.7,create folder
v0.7,get google drive id from list
v0.7,download from google drive
v0.7,unzip
v0.7,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.7,transform data into required format
v0.7,"the processed dataset has the additional ending ""_new"""
v0.7,remove the unprocessed dataset
v0.7,initialize comlumncorpus and add it to list
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,"unzip the downloaded repo and merge the train, dev and test datasets"
v0.7,download data if necessary
v0.7,unpack and write out in CoNLL column-like format
v0.7,if no languages are given as argument all languages used in XTREME will be loaded
v0.7,if only one language is given
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,"For each language in languages, the file is downloaded if not existent"
v0.7,Then a comlumncorpus of that data is created and saved in a list
v0.7,This list is handed to the multicorpus
v0.7,list that contains the columncopora
v0.7,download data if necessary
v0.7,"if language not downloaded yet, download it"
v0.7,create folder
v0.7,download from HU Server
v0.7,unzip
v0.7,transform data into required format
v0.7,initialize comlumncorpus and add it to list
v0.7,"find train, dev and test files if not specified"
v0.7,use test_file to create test split if available
v0.7,use dev_file to create test split if available
v0.7,"if data point contains black-listed label, do not use"
v0.7,first check if valid sentence
v0.7,"if so, add to indices"
v0.7,"find train, dev and test files if not specified"
v0.7,variables
v0.7,different handling of in_memory data than streaming data
v0.7,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.7,test if format is OK
v0.7,test if at least one label given
v0.7,noinspection PyDefaultArgument
v0.7,dataset name includes the split size
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download each of the 28 splits
v0.7,create dataset directory if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,create dataset directory if necessary
v0.7,create train.txt file from CSV
v0.7,create test.txt file from CSV
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,create dataset directory if necessary
v0.7,create train.txt file by iterating over pos and neg file
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,create dataset directory if necessary
v0.7,create train.txt file by iterating over pos and neg file
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,create dataset directory if necessary
v0.7,create train.txt file by iterating over pos and neg file
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,create dataset directory if necessary
v0.7,create train.txt file by iterating over pos and neg file
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,create train.txt file by iterating over pos and neg file
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download senteval datasets if necessary und unzip
v0.7,convert to FastText format
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,download datasets if necessary
v0.7,create dataset directory if necessary
v0.7,create correctly formated txt files
v0.7,multiple labels are possible
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,Create flair compatible labels
v0.7,TREC-6 : NUM:dist -> __label__NUM
v0.7,TREC-50: NUM:dist -> __label__NUM:dist
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,Create flair compatible labels
v0.7,TREC-6 : NUM:dist -> __label__NUM
v0.7,TREC-50: NUM:dist -> __label__NUM:dist
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,check if dataset is supported
v0.7,set file names
v0.7,download and unzip in file structure if necessary
v0.7,instantiate corpus
v0.7,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.7,with sum of all entity lengths as secondary key
v0.7,calculate offset without current text
v0.7,because we stick all passages of a document together
v0.7,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.7,Try to fix incorrect annotations
v0.7,print(
v0.7,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.7,)
v0.7,Ignore empty lines or relation annotations
v0.7,FIX annotation of whitespaces (necessary for PDR)
v0.7,One token may contain multiple entities -> deque all of them
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.7,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,Edge case: last token starts a new entity
v0.7,Last document in file
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,In the huner split files there is no information whether a given id originates
v0.7,from the train or test file of the original corpus - so we have to adapt corpus
v0.7,splitting here
v0.7,In the huner split files there is no information whether a given id originates
v0.7,from the train or test file of the original corpus - so we have to adapt corpus
v0.7,splitting here
v0.7,In the huner split files there is no information whether a given id originates
v0.7,from the train or test file of the original corpus - so we have to adapt corpus
v0.7,splitting here
v0.7,Edge case: last token starts a new entity
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download file is huge => make default_dir visible so that derivative
v0.7,corpora can all use the same download file
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,Read texts
v0.7,Read annotations
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,We need to apply a patch to correct the original training file
v0.7,Articles title
v0.7,Article abstract
v0.7,Entity annotations
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,Edge case: last token starts a new entity
v0.7,Map all entities to chemicals
v0.7,Map all entities to disease
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,Incomplete article
v0.7,Invalid XML syntax
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,if len(mid) != 3:
v0.7,continue
v0.7,Try to fix entity offsets
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,There is still one illegal annotation in the file ..
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,"Abstract first, title second to prevent issues with sentence splitting"
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,"Filter for specific entity types, by default no entities will be filtered"
v0.7,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.7,train and dev split of V2 will be train in V4
v0.7,test split of V2 will be dev in V4
v0.7,New documents in V4 will become test documents
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,column format
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,cache Feidegger config file
v0.7,cache Feidegger images
v0.7,replace image URL with local cached file
v0.7,append Sentence-Image data point
v0.7,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.7,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.7,"if so, num_workers is set to 0 for faster processing"
v0.7,cast to list if necessary
v0.7,cast to list if necessary
v0.7,"first, check if pymongo is installed"
v0.7,automatically identify train / test / dev files
v0.7,"if no test file is found, take any file with 'test' in name"
v0.7,Expose base classses
v0.7,Expose all sequence labeling datasets
v0.7,Expose all document classification datasets
v0.7,Expose all treebanks
v0.7,Expose all text-text datasets
v0.7,Expose all text-image datasets
v0.7,Expose all biomedical data sets
v0.7,Expose all biomedical data sets using the HUNER splits
v0.7,-
v0.7,-
v0.7,-
v0.7,-
v0.7,Expose all biomedical data sets used for the evaluation of BioBERT
v0.7,"find train, dev and test files if not specified"
v0.7,get train data
v0.7,get test data
v0.7,get dev data
v0.7,option 1: read only sentence boundaries as offset positions
v0.7,option 2: keep everything in memory
v0.7,"if in memory, retrieve parsed sentence"
v0.7,else skip to position in file where sentence begins
v0.7,current token ID
v0.7,handling for the awful UD multiword format
v0.7,end of sentence
v0.7,comments
v0.7,ellipsis
v0.7,if token is a multi-word
v0.7,normal single-word tokens
v0.7,"if we don't split multiwords, skip over component words"
v0.7,add token
v0.7,add morphological tags
v0.7,derive whitespace logic for multiwords
v0.7,print(token)
v0.7,print(current_multiword_last_token)
v0.7,print(current_multiword_first_token)
v0.7,"if multi-word equals component tokens, there should be no whitespace"
v0.7,go through all tokens in subword and set whitespace_after information
v0.7,print(i)
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,download data if necessary
v0.7,this dataset name
v0.7,default dataset folder is the cache root
v0.7,test corpus
v0.7,create a TARS classifier
v0.7,check if right number of classes
v0.7,switch to task with only one label
v0.7,check if right number of classes
v0.7,switch to task with three labels provided as list
v0.7,check if right number of classes
v0.7,switch to task with four labels provided as set
v0.7,check if right number of classes
v0.7,switch to task with two labels provided as Dictionary
v0.7,check if right number of classes
v0.7,clean up file
v0.7,bioes tags
v0.7,bio tags
v0.7,broken tags
v0.7,all tags
v0.7,all weird tags
v0.7,tags with confidence
v0.7,bioes tags
v0.7,bioes tags
v0.7,increment for last token in sentence if not followed by whitespace
v0.7,clean up directory
v0.7,clean up directory
v0.7,initialize trainer
v0.7,clean up results directory
v0.7,initialize trainer
v0.7,clean up results directory
v0.7,initialize trainer
v0.7,clean up results directory
v0.7,initialize trainer
v0.7,clean up results directory
v0.7,initialize trainer
v0.7,clean up results directory
v0.7,clean up results directory
v0.7,initialize trainer
v0.7,clean up results directory
v0.7,def test_multiclass_metrics():
v0.7,
v0.7,"metric = Metric(""Test"")"
v0.7,"available_labels = [""A"", ""B"", ""C""]"
v0.7,
v0.7,"predictions = [""A"", ""B""]"
v0.7,"true_values = [""A""]"
v0.7,TextClassifier._evaluate_sentence_for_text_classification(
v0.7,"metric, available_labels, predictions, true_values"
v0.7,)
v0.7,
v0.7,"predictions = [""C"", ""B""]"
v0.7,"true_values = [""A"", ""B""]"
v0.7,TextClassifier._evaluate_sentence_for_text_classification(
v0.7,"metric, available_labels, predictions, true_values"
v0.7,)
v0.7,
v0.7,print(metric)
v0.7,from flair.trainers.trainer_regression import RegressorTrainer
v0.7,def test_trainer_results(tasks_base_path):
v0.7,"corpus, model, trainer = init(tasks_base_path)"
v0.7,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.7,"assert results[""test_score""] > 0"
v0.7,"assert len(results[""dev_loss_history""]) == 1"
v0.7,"assert len(results[""dev_score_history""]) == 1"
v0.7,"assert len(results[""train_loss_history""]) == 1"
v0.7,get default dictionary
v0.7,init forward LM with 128 hidden states and 1 layer
v0.7,get the example corpus and process at character level in forward direction
v0.7,train the language model
v0.7,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.7,clean up results directory
v0.7,get default dictionary
v0.7,init forward LM with 128 hidden states and 1 layer
v0.7,get the example corpus and process at character level in forward direction
v0.7,train the language model
v0.7,clean up results directory
v0.7,define search space
v0.7,sequence tagger parameter
v0.7,model trainer parameter
v0.7,training parameter
v0.7,find best parameter settings
v0.7,clean up results directory
v0.7,document embeddings parameter
v0.7,training parameter
v0.7,clean up results directory
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,"get training, test and dev data"
v0.7,get two corpora as one
v0.7,"get training, test and dev data for full English UD corpus from web"
v0.7,clean up data directory
v0.7,clean up results directory
v0.7,clean up results directory
v0.7,clean up results directory
v0.7,clean up results directory
v0.7,clean up results directory
v0.7,clean up results directory
v0.6.1,1. get the corpus
v0.6.1,2. what tag do we want to predict?
v0.6.1,3. make the tag dictionary from the corpus
v0.6.1,initialize embeddings
v0.6.1,comment in this line to use character embeddings
v0.6.1,"CharacterEmbeddings(),"
v0.6.1,comment in these lines to use contextual string embeddings
v0.6.1,
v0.6.1,"FlairEmbeddings('news-forward'),"
v0.6.1,
v0.6.1,"FlairEmbeddings('news-backward'),"
v0.6.1,initialize sequence tagger
v0.6.1,initialize trainer
v0.6.1,from allennlp.common.tqdm import Tqdm
v0.6.1,mmap seems to be much more memory efficient
v0.6.1,Remove quotes from etag
v0.6.1,"If there is an etag, it's everything after the first period"
v0.6.1,"Otherwise, use None"
v0.6.1,"URL, so get it from the cache (downloading if necessary)"
v0.6.1,"File, and it exists."
v0.6.1,"File, but it doesn't exist."
v0.6.1,Something unknown
v0.6.1,Extract all the contents of zip file in current directory
v0.6.1,Extract all the contents of zip file in current directory
v0.6.1,get cache path to put the file
v0.6.1,"Download to temporary file, then copy to cache dir once finished."
v0.6.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.6.1,GET file object
v0.6.1,TODO(joelgrus): do we want to do checksums or anything like that?
v0.6.1,get cache path to put the file
v0.6.1,make HEAD request to check ETag
v0.6.1,add ETag to filename if it exists
v0.6.1,"etag = response.headers.get(""ETag"")"
v0.6.1,"Download to temporary file, then copy to cache dir once finished."
v0.6.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.6.1,GET file object
v0.6.1,These defaults are the same as the argument defaults in tqdm.
v0.6.1,first determine the distribution of classes in the dataset
v0.6.1,weight for each sample
v0.6.1,Create blocks
v0.6.1,shuffle the blocks
v0.6.1,concatenate the shuffled blocks
v0.6.1,Create blocks
v0.6.1,shuffle the blocks
v0.6.1,concatenate the shuffled blocks
v0.6.1,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.6.1,see https://github.com/zalandoresearch/flair/issues/351
v0.6.1,State initialization
v0.6.1,Exponential moving average of gradient values
v0.6.1,Exponential moving average of squared gradient values
v0.6.1,Maintains max of all exp. moving avg. of sq. grad. values
v0.6.1,Decay the first and second moment running average coefficient
v0.6.1,Maintains the maximum of all 2nd moment running avg. till now
v0.6.1,Use the max. for normalizing running avg. of gradient
v0.6.1,determine offsets for whitespace_after field
v0.6.1,increment for last token in sentence if not followed by whitespace
v0.6.1,determine offsets for whitespace_after field
v0.6.1,conll 2000 column format
v0.6.1,conll 03 NER column format
v0.6.1,WNUT-17
v0.6.1,-- WikiNER datasets
v0.6.1,-- Universal Dependencies
v0.6.1,Germanic
v0.6.1,Romance
v0.6.1,West-Slavic
v0.6.1,South-Slavic
v0.6.1,East-Slavic
v0.6.1,Scandinavian
v0.6.1,Asian
v0.6.1,Language isolates
v0.6.1,recent Universal Dependencies
v0.6.1,other datasets
v0.6.1,text classification format
v0.6.1,text regression format
v0.6.1,"first, try to fetch dataset online"
v0.6.1,default dataset folder is the cache root
v0.6.1,get string value if enum is passed
v0.6.1,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.6.1,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.6.1,the CoNLL 03 task for German has an additional lemma column
v0.6.1,the CoNLL 03 task for Dutch has no NP column
v0.6.1,the CoNLL 03 task for Spanish only has two columns
v0.6.1,the GERMEVAL task only has two columns: text and ner
v0.6.1,WSD tasks may be put into this column format
v0.6.1,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.6.1,"for text classifiers, we use our own special format"
v0.6.1,NER corpus for Basque
v0.6.1,automatically identify train / test / dev files
v0.6.1,"if no test file is found, take any file with 'test' in name"
v0.6.1,get train and test data
v0.6.1,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.6.1,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.6.1,convert tag scheme to iobes
v0.6.1,automatically identify train / test / dev files
v0.6.1,automatically identify train / test / dev files
v0.6.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.6.1,conll 2000 chunking task
v0.6.1,Support both TREC-6 and TREC-50
v0.6.1,Create flair compatible labels
v0.6.1,TREC-6 : NUM:dist -> __label__NUM
v0.6.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.6.1,Wikiner NER task
v0.6.1,unpack and write out in CoNLL column-like format
v0.6.1,CoNLL 02/03 NER
v0.6.1,universal dependencies
v0.6.1,--- UD Germanic
v0.6.1,--- UD Romance
v0.6.1,--- UD West-Slavic
v0.6.1,--- UD Scandinavian
v0.6.1,--- UD South-Slavic
v0.6.1,--- UD Asian
v0.6.1,this is the default init size of a lmdb database for embeddings
v0.6.1,some non-used parameter to allow print
v0.6.1,get db filename from embedding name
v0.6.1,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.6.1,SequenceTagger
v0.6.1,TextClassifier
v0.6.1,get db filename from embedding name
v0.6.1,if embedding database already exists
v0.6.1,"otherwise, push embedding to database"
v0.6.1,if embedding database already exists
v0.6.1,open the database in read mode
v0.6.1,we need to set self.k
v0.6.1,create and load the database in write mode
v0.6.1,"no idea why, but we need to close and reopen the environment to avoid"
v0.6.1,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.6.1,when opening new transaction !
v0.6.1,init dictionaries
v0.6.1,"in order to deal with unknown tokens, add <unk>"
v0.6.1,"We don't want to create a SpaceTokenizer object each time this function is called,"
v0.6.1,so delegate the call directly to the static run_tokenize method
v0.6.1,"We don't want to create a SegtokTokenizer object each time this function is called,"
v0.6.1,so delegate the call directly to the static run_tokenize method
v0.6.1,"if text is passed, instantiate sentence with tokens (words)"
v0.6.1,log a warning if the dataset is empty
v0.6.1,data with zero-width characters cannot be handled
v0.6.1,set token idx if not set
v0.6.1,non-set tags are OUT tags
v0.6.1,anything that is not a BIOES tag is a SINGLE tag
v0.6.1,anything that is not OUT is IN
v0.6.1,single and begin tags start a new span
v0.6.1,remember previous tag
v0.6.1,"if label type is explicitly specified, get spans for this label type"
v0.6.1,else determine all label types in sentence and get all spans
v0.6.1,move sentence embeddings to device
v0.6.1,move token embeddings to device
v0.6.1,clear sentence embeddings
v0.6.1,clear token embeddings
v0.6.1,infer whitespace after field
v0.6.1,add Sentence labels to output if they exist
v0.6.1,add Token labels to output if they exist
v0.6.1,add Sentence labels to output if they exist
v0.6.1,add Token labels to output if they exist
v0.6.1,No character at the corresponding code point: remove it
v0.6.1,set name
v0.6.1,sample test data if none is provided
v0.6.1,sample dev data if none is provided
v0.6.1,set train dev and test data
v0.6.1,find out empty sentence indices
v0.6.1,create subset of non-empty sentence indices
v0.6.1,find out empty sentence indices
v0.6.1,create subset of non-empty sentence indices
v0.6.1,check if sentence itself has labels
v0.6.1,check for labels of words
v0.6.1,Make the tag dictionary
v0.6.1,global variable: cache_root
v0.6.1,global variable: device
v0.6.1,global variable: embedding_storage_mode
v0.6.1,# dummy return to fulfill trainer.train() needs
v0.6.1,print(vec)
v0.6.1,Attach optimizer
v0.6.1,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.6.1,if memory mode option 'none' delete everything
v0.6.1,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.6.1,find out which ones are dynamic embeddings
v0.6.1,find out which ones are dynamic embeddings
v0.6.1,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.6.1,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.6.1,header for 'weights.txt'
v0.6.1,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.6.1,then get all relevant values from the tsv
v0.6.1,then get all relevant values from the tsv
v0.6.1,plot i
v0.6.1,save plots
v0.6.1,save plots
v0.6.1,plt.show()
v0.6.1,save plot
v0.6.1,take the average over the last three scores of training
v0.6.1,take average over the scores from the different training runs
v0.6.1,remove previous embeddings
v0.6.1,clearing token embeddings to save memory
v0.6.1,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.6.1,#TODO: not saving lines yet
v0.6.1,== similarity measures ==
v0.6.1,helper class for ModelSimilarity
v0.6.1,-- works with binary cross entropy loss --
v0.6.1,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.6.1,-- works with ranking/triplet loss --
v0.6.1,normalize the embeddings
v0.6.1,== similarity losses ==
v0.6.1,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.6.1,TODO: this assumes eye matrix
v0.6.1,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.6.1,== similarity learner ==
v0.6.1,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.6.1,assumes that for each data pair there's at least one embedding per modality
v0.6.1,pre-compute embeddings for all targets in evaluation dataset
v0.6.1,compute the similarity
v0.6.1,sort the similarity matrix across modality 1
v0.6.1,"get the ranks, so +1 to start counting ranks from 1"
v0.6.1,The conversion from old model's constructor interface
v0.6.1,auto-spawn on GPU if available
v0.6.1,pad strings with whitespaces to longest sentence
v0.6.1,cut up the input into chunks of max charlength = chunk_size
v0.6.1,push each chunk through the RNN language model
v0.6.1,concatenate all chunks to make final output
v0.6.1,initial hidden state
v0.6.1,get predicted weights
v0.6.1,divide by temperature
v0.6.1,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.6.1,this makes a vector in which the largest value is 0
v0.6.1,compute word weights with exponential function
v0.6.1,try sampling multinomial distribution for next character
v0.6.1,print(word_idx)
v0.6.1,input ids
v0.6.1,push list of character IDs through model
v0.6.1,the target is always the next character
v0.6.1,use cross entropy loss to compare output of forward pass with targets
v0.6.1,exponentiate cross-entropy loss to calculate perplexity
v0.6.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.6.1,"check if this is the case and if so, set it"
v0.6.1,set the dictionaries
v0.6.1,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.6.1,Initialize the weight tensor
v0.6.1,initialize the network architecture
v0.6.1,dropouts
v0.6.1,optional reprojection layer on top of word embeddings
v0.6.1,bidirectional LSTM on top of embedding layer
v0.6.1,Create initial hidden state and initialize it
v0.6.1,TODO: Decide how to initialize the hidden state variables
v0.6.1,self.hs_initializer(self.lstm_init_h)
v0.6.1,self.hs_initializer(self.lstm_init_c)
v0.6.1,final linear map to tag space
v0.6.1,reverse sort all sequences by their length
v0.6.1,progress bar for verbosity
v0.6.1,stop if all sentences are empty
v0.6.1,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.6.1,clearing token embeddings to save memory
v0.6.1,predict for batch
v0.6.1,make list of gold tags
v0.6.1,make list of predicted tags
v0.6.1,"check for true positives, false positives and false negatives"
v0.6.1,also write to file in BIO format to use old conlleval script
v0.6.1,check if in gold spans
v0.6.1,check if in predicted spans
v0.6.1,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.6.1,"if span F1 needs to be used, use separate eval method"
v0.6.1,"else, use scikit-learn to evaluate"
v0.6.1,predict for batch
v0.6.1,add gold tag
v0.6.1,add predicted tag
v0.6.1,for file output
v0.6.1,use sklearn
v0.6.1,"make ""classification report"""
v0.6.1,get scores
v0.6.1,line for log file
v0.6.1,--------------------------------------------------------------------
v0.6.1,FF PART
v0.6.1,--------------------------------------------------------------------
v0.6.1,"if initial hidden state is trainable, use this state"
v0.6.1,word dropout only before LSTM - TODO: more experimentation needed
v0.6.1,if self.use_word_dropout > 0.0:
v0.6.1,sentence_tensor = self.word_dropout(sentence_tensor)
v0.6.1,get the tags in this sentence
v0.6.1,add tags as tensor
v0.6.1,pad tags if using batch-CRF decoder
v0.6.1,reduce raw values to avoid NaN during exp
v0.6.1,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.6.1,default value
v0.6.1,English NER models
v0.6.1,Multilingual NER models
v0.6.1,English POS models
v0.6.1,Multilingual POS models
v0.6.1,English SRL models
v0.6.1,English chunking models
v0.6.1,Danish models
v0.6.1,German models
v0.6.1,French models
v0.6.1,Dutch models
v0.6.1,Malayalam models
v0.6.1,Portuguese models
v0.6.1,Keyphase models
v0.6.1,Biomedical models
v0.6.1,the historical German taggers by the @redewiegergabe project
v0.6.1,clear embeddings after predicting
v0.6.1,load each model
v0.6.1,check if the same embeddings were already loaded previously
v0.6.1,"if the model uses StackedEmbedding, make a new stack with previous objects"
v0.6.1,sort embeddings by key alphabetically
v0.6.1,check previous embeddings and add if found
v0.6.1,only re-use static embeddings
v0.6.1,"if not found, use existing embedding"
v0.6.1,initialize new stack
v0.6.1,"of the model uses regular embedding, re-load if previous version found"
v0.6.1,Initialize the weight tensor
v0.6.1,auto-spawn on GPU if available
v0.6.1,filter empty sentences
v0.6.1,reverse sort all sequences by their length
v0.6.1,progress bar for verbosity
v0.6.1,stop if all sentences are empty
v0.6.1,clearing token embeddings to save memory
v0.6.1,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.6.1,use scikit-learn to evaluate
v0.6.1,remove previously predicted labels
v0.6.1,get the gold labels
v0.6.1,predict for batch
v0.6.1,get the predicted labels
v0.6.1,remove predicted labels
v0.6.1,"make ""classification report"""
v0.6.1,get scores
v0.6.1,line for log file
v0.6.1,English sentiment models
v0.6.1,Communicative Functions Model
v0.6.1,cast string to Path
v0.6.1,"determine what splits (train, dev, test) to evaluate and log"
v0.6.1,prepare loss logging file and set up header
v0.6.1,"minimize training loss if training with dev data, else maximize dev score"
v0.6.1,"if training also uses dev data, include in training set"
v0.6.1,initialize sampler if provided
v0.6.1,init with default values if only class is provided
v0.6.1,set dataset to sample from
v0.6.1,At any point you can hit Ctrl + C to break out of training early.
v0.6.1,get new learning rate
v0.6.1,reload last best model if annealing with restarts is enabled
v0.6.1,stop training if learning rate becomes too small
v0.6.1,process mini-batches
v0.6.1,zero the gradients on the model and optimizer
v0.6.1,"if necessary, make batch_steps"
v0.6.1,forward and backward for batch
v0.6.1,forward pass
v0.6.1,Backward
v0.6.1,do the optimizer step
v0.6.1,do the scheduler step if one-cycle
v0.6.1,get new learning rate
v0.6.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6.1,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.6.1,evaluate on train / dev / test split depending on training settings
v0.6.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6.1,calculate scores using dev data if available
v0.6.1,append dev score to score history
v0.6.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6.1,determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
v0.6.1,determine bad epoch number
v0.6.1,log bad epochs
v0.6.1,output log file
v0.6.1,make headers on first epoch
v0.6.1,"if checkpoint is enabled, save model at each epoch"
v0.6.1,"if we use dev data, remember best model based on dev evaluation score"
v0.6.1,"if we do not use dev data for model selection, save final model"
v0.6.1,test best model if test data is present
v0.6.1,"if we are training over multiple datasets, do evaluation for each"
v0.6.1,get and return the final test score of best model
v0.6.1,cast string to Path
v0.6.1,forward pass
v0.6.1,update optimizer and scheduler
v0.6.1,Add chars to the dictionary
v0.6.1,charsplit file content
v0.6.1,charsplit file content
v0.6.1,Add words to the dictionary
v0.6.1,Tokenize file content
v0.6.1,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.6.1,cast string to Path
v0.6.1,error message if the validation dataset is too small
v0.6.1,Shuffle training files randomly after serially iterating through corpus one
v0.6.1,"iterate through training data, starting at self.split (for checkpointing)"
v0.6.1,off by one for printing
v0.6.1,go into train mode
v0.6.1,reset variables
v0.6.1,not really sure what this does
v0.6.1,do the forward pass in the model
v0.6.1,try to predict the targets
v0.6.1,Backward
v0.6.1,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.6.1,We detach the hidden state from how it was previously produced.
v0.6.1,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.6.1,explicitly remove loss to clear up memory
v0.6.1,##############################################################################
v0.6.1,Save the model if the validation loss is the best we've seen so far.
v0.6.1,##############################################################################
v0.6.1,print info
v0.6.1,##############################################################################
v0.6.1,##############################################################################
v0.6.1,final testing
v0.6.1,##############################################################################
v0.6.1,Turn on evaluation mode which disables dropout.
v0.6.1,Work out how cleanly we can divide the dataset into bsz parts.
v0.6.1,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.6.1,Evenly divide the data across the bsz batches.
v0.6.1,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.6.1,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.6.1,news-english-forward
v0.6.1,news-english-backward
v0.6.1,news-english-forward
v0.6.1,news-english-backward
v0.6.1,mix-english-forward
v0.6.1,mix-english-backward
v0.6.1,mix-german-forward
v0.6.1,mix-german-backward
v0.6.1,common crawl Polish forward
v0.6.1,common crawl Polish backward
v0.6.1,Slovenian forward
v0.6.1,Slovenian backward
v0.6.1,Bulgarian forward
v0.6.1,Bulgarian backward
v0.6.1,Dutch forward
v0.6.1,Dutch backward
v0.6.1,Swedish forward
v0.6.1,Swedish backward
v0.6.1,French forward
v0.6.1,French backward
v0.6.1,Czech forward
v0.6.1,Czech backward
v0.6.1,Portuguese forward
v0.6.1,Portuguese backward
v0.6.1,initialize cache if use_cache set
v0.6.1,embed a dummy sentence to determine embedding_length
v0.6.1,set to eval mode
v0.6.1,Copy the object's state from self.__dict__ which contains
v0.6.1,all our instance attributes. Always use the dict.copy()
v0.6.1,method to avoid modifying the original state.
v0.6.1,Remove the unpicklable entries.
v0.6.1,"if cache is used, try setting embeddings from cache first"
v0.6.1,try populating embeddings from cache
v0.6.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.6.1,get hidden states from language model
v0.6.1,take first or last hidden states from language model as word representation
v0.6.1,if self.tokenized_lm or token.whitespace_after:
v0.6.1,1-camembert-base -> camembert-base
v0.6.1,1-xlm-roberta-large -> xlm-roberta-large
v0.6.1,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.6.1,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.6.1,tokens are attended to.
v0.6.1,Zero-pad up to the sequence length.
v0.6.1,"first, find longest sentence in batch"
v0.6.1,prepare id maps for BERT model
v0.6.1,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.6.1,get aggregated embeddings for each BERT-subtoken in sentence
v0.6.1,get the current sentence object
v0.6.1,add concatenated embedding to sentence
v0.6.1,use first subword embedding if pooling operation is 'first'
v0.6.1,"otherwise, do a mean over all subwords in token"
v0.6.1,"if only one sentence is passed, convert to list of sentence"
v0.6.1,bidirectional LSTM on top of embedding layer
v0.6.1,dropouts
v0.6.1,"first, sort sentences by number of tokens"
v0.6.1,go through each sentence in batch
v0.6.1,PADDING: pad shorter sentences out
v0.6.1,ADD TO SENTENCE LIST: add the representation
v0.6.1,--------------------------------------------------------------------
v0.6.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.6.1,--------------------------------------------------------------------
v0.6.1,--------------------------------------------------------------------
v0.6.1,FF PART
v0.6.1,--------------------------------------------------------------------
v0.6.1,use word dropout if set
v0.6.1,--------------------------------------------------------------------
v0.6.1,EXTRACT EMBEDDINGS FROM LSTM
v0.6.1,--------------------------------------------------------------------
v0.6.1,embed a dummy sentence to determine embedding_length
v0.6.1,Avoid conflicts with flair's Token class
v0.6.1,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.6.1,add positional encodings
v0.6.1,reshape the pixels into the sequence
v0.6.1,layer norm after convolution and positional encodings
v0.6.1,add <cls> token
v0.6.1,"transformer requires input in the shape [h*w+1, b, d]"
v0.6.1,the output is an embedding of <cls> token
v0.6.1,temporary fix to disable tokenizer parallelism warning
v0.6.1,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.6.1,load tokenizer and transformer model
v0.6.1,model name
v0.6.1,"when initializing, embeddings are in eval mode by default"
v0.6.1,embedding parameters
v0.6.1,send mini-token through to check how many layers the model has
v0.6.1,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.6.1,using list comprehension
v0.6.1,gradients are enabled if fine-tuning is enabled
v0.6.1,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.6.1,subtokenize sentences
v0.6.1,tokenize and truncate to max subtokens (TODO: check better truncation strategies)
v0.6.1,find longest sentence in batch
v0.6.1,initialize batch tensors and mask
v0.6.1,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.6.1,iterate over all subtokenized sentences
v0.6.1,use scalar mix of embeddings if so selected
v0.6.1,set the extracted embedding for the token
v0.6.1,reload tokenizer to get around serialization issues
v0.6.1,optional fine-tuning on top of embedding layer
v0.6.1,"if only one sentence is passed, convert to list of sentence"
v0.6.1,bidirectional RNN on top of embedding layer
v0.6.1,dropouts
v0.6.1,TODO: remove in future versions
v0.6.1,embed words in the sentence
v0.6.1,before-RNN dropout
v0.6.1,reproject if set
v0.6.1,push through RNN
v0.6.1,after-RNN dropout
v0.6.1,extract embeddings from RNN
v0.6.1,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.6.1,"check if this is the case and if so, set it"
v0.6.1,IMPORTANT: add embeddings as torch modules
v0.6.1,iterate over sentences
v0.6.1,"if its a forward LM, take last state"
v0.6.1,"convert to plain strings, embedded in a list for the encode function"
v0.6.1,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.6.1,"if only one sentence is passed, convert to list of sentence"
v0.6.1,Expose base classses
v0.6.1,Expose token embedding classes
v0.6.1,Expose document embedding classes
v0.6.1,Expose image embedding classes
v0.6.1,Expose legacy embedding classes
v0.6.1,IMPORTANT: add embeddings as torch modules
v0.6.1,"if only one sentence is passed, convert to list of sentence"
v0.6.1,GLOVE embeddings
v0.6.1,TURIAN embeddings
v0.6.1,KOMNINOS embeddings
v0.6.1,pubmed embeddings
v0.6.1,FT-CRAWL embeddings
v0.6.1,FT-CRAWL embeddings
v0.6.1,twitter embeddings
v0.6.1,two-letter language code wiki embeddings
v0.6.1,two-letter language code wiki embeddings
v0.6.1,two-letter language code crawl embeddings
v0.6.1,fix serialized models
v0.6.1,use list of common characters if none provided
v0.6.1,translate words in sentence into ints using dictionary
v0.6.1,"sort words by length, for batching and masking"
v0.6.1,chars for rnn processing
v0.6.1,multilingual models
v0.6.1,English models
v0.6.1,Arabic
v0.6.1,Bulgarian
v0.6.1,Czech
v0.6.1,Danish
v0.6.1,German
v0.6.1,Spanish
v0.6.1,Basque
v0.6.1,Persian
v0.6.1,Finnish
v0.6.1,French
v0.6.1,Hebrew
v0.6.1,Hindi
v0.6.1,Croatian
v0.6.1,Indonesian
v0.6.1,Italian
v0.6.1,Japanese
v0.6.1,Malayalam
v0.6.1,Dutch
v0.6.1,Norwegian
v0.6.1,Polish
v0.6.1,Portuguese
v0.6.1,Pubmed
v0.6.1,Slovenian
v0.6.1,Swedish
v0.6.1,Tamil
v0.6.1,CLEF HIPE Shared task
v0.6.1,load model if in pretrained model map
v0.6.1,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.6.1,embeddings are static if we don't do finetuning
v0.6.1,embed a dummy sentence to determine embedding_length
v0.6.1,set to eval mode
v0.6.1,make compatible with serialized models (TODO: remove)
v0.6.1,make compatible with serialized models (TODO: remove)
v0.6.1,gradients are enable if fine-tuning is enabled
v0.6.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.6.1,get hidden states from language model
v0.6.1,take first or last hidden states from language model as word representation
v0.6.1,offset mode that extracts at whitespace after last character
v0.6.1,offset mode that extracts at last character
v0.6.1,only clone if optimization mode is 'gpu'
v0.6.1,use the character language model embeddings as basis
v0.6.1,length is twice the original character LM embedding length
v0.6.1,these fields are for the embedding memory
v0.6.1,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.6.1,we re-compute embeddings dynamically at each epoch
v0.6.1,set the memory method
v0.6.1,memory is wiped each time we do a training run
v0.6.1,"if we keep a pooling, it needs to be updated continuously"
v0.6.1,update embedding
v0.6.1,check token.text is empty or not
v0.6.1,set aggregation operation
v0.6.1,add embeddings after updating
v0.6.1,temporary fix to disable tokenizer parallelism warning
v0.6.1,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.6.1,load tokenizer and transformer model
v0.6.1,model name
v0.6.1,"when initializing, embeddings are in eval mode by default"
v0.6.1,embedding parameters
v0.6.1,send mini-token through to check how many layers the model has
v0.6.1,"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
v0.6.1,check if special tokens exist to circumvent error message
v0.6.1,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.6.1,split into micro batches of size self.batch_size before pushing through transformer
v0.6.1,embed each micro-batch
v0.6.1,remove special markup
v0.6.1,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.6.1,"TODO: keep for backwards compatibility, but remove in future"
v0.6.1,"some pretrained models do not have this property, applying default settings now."
v0.6.1,can be set manually after loading the model.
v0.6.1,method 1: subtokenize sentence
v0.6.1,"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
v0.6.1,method 2:
v0.6.1,transformer specific tokenization
v0.6.1,empty sentences get zero embeddings
v0.6.1,only embed non-empty sentences and if there is at least one
v0.6.1,find longest sentence in batch
v0.6.1,initialize batch tensors and mask
v0.6.1,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.6.1,make the tuple a tensor; makes working with it easier.
v0.6.1,gradients are enabled if fine-tuning is enabled
v0.6.1,iterate over all subtokenized sentences
v0.6.1,"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
v0.6.1,in order to get some context into the embeddings of these words.
v0.6.1,also don't include the embedding of the extra [CLS] and [SEP] tokens.
v0.6.1,"for each token, get embedding"
v0.6.1,some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
v0.6.1,"get states from all selected layers, aggregate with pooling operation"
v0.6.1,use scalar mix of embeddings if so selected
v0.6.1,sm_embeddings = self.mix(subtoken_embeddings)
v0.6.1,set the extracted embedding for the token
v0.6.1,iterate over subtokens and reconstruct tokens
v0.6.1,remove special markup
v0.6.1,TODO check if this is necessary is this method is called before prepare_for_model
v0.6.1,check if reconstructed token is special begin token ([CLS] or similar)
v0.6.1,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.6.1,append subtoken to reconstruct token
v0.6.1,check if reconstructed token is the same as current token
v0.6.1,"if so, add subtoken count"
v0.6.1,reset subtoken count and reconstructed token
v0.6.1,break from loop if all tokens are accounted for
v0.6.1,if tokens are unaccounted for
v0.6.1,check if all tokens were matched to subtokens
v0.6.1,"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
v0.6.1,module should never be in training mode
v0.6.1,reload tokenizer to get around serialization issues
v0.6.1,max_tokens = 500
v0.6.1,model architecture
v0.6.1,model architecture
v0.6.1,download if necessary
v0.6.1,load the model
v0.6.1,"TODO: keep for backwards compatibility, but remove in future"
v0.6.1,save the sentence piece model as binary file (not as path which may change)
v0.6.1,write out the binary sentence piece model into the expected directory
v0.6.1,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.6.1,"otherwise, use normal process and potentially trigger another download"
v0.6.1,"once the modes if there, load it with sentence piece"
v0.6.1,empty words get no embedding
v0.6.1,all other words get embedded
v0.6.1,"the default model for ELMo is the 'original' model, which is very large"
v0.6.1,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.6.1,put on Cuda if available
v0.6.1,embed a dummy sentence to determine embedding_length
v0.6.1,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.6.1,GLOVE embeddings
v0.6.1,"find train, dev and test files if not specified"
v0.6.1,get train data
v0.6.1,read in test file if exists
v0.6.1,read in dev file if exists
v0.6.1,special key for space after
v0.6.1,"store either Sentence objects in memory, or only file offsets"
v0.6.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.6.1,determine encoding of text file
v0.6.1,skip first line if to selected
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,check if data there
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,check if data there
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,if no languages are given as argument all languages used in XTREME will be loaded
v0.6.1,if only one language is given
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,"For each language in languages, the file is downloaded if not existent"
v0.6.1,Then a comlumncorpus of that data is created and saved in a list
v0.6.1,This list is handed to the multicorpus
v0.6.1,list that contains the columncopora
v0.6.1,download data if necessary
v0.6.1,"if language not downloaded yet, download it"
v0.6.1,create folder
v0.6.1,download from HU Server
v0.6.1,unzip
v0.6.1,transform data into required format
v0.6.1,initialize comlumncorpus and add it to list
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,"For each language in languages, the file is downloaded if not existent"
v0.6.1,Then a comlumncorpus of that data is created and saved in a list
v0.6.1,this list is handed to the multicorpus
v0.6.1,list that contains the columncopora
v0.6.1,download data if necessary
v0.6.1,"if language not downloaded yet, download it"
v0.6.1,create folder
v0.6.1,get google drive id from list
v0.6.1,download from google drive
v0.6.1,unzip
v0.6.1,"tar.extractall(language_folder,members=[tar.getmember(file_name)])"
v0.6.1,transform data into required format
v0.6.1,"the processed dataset has the additional ending ""_new"""
v0.6.1,remove the unprocessed dataset
v0.6.1,initialize comlumncorpus and add it to list
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,Remove CoNLL-U meta information in the last column
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,check if data there
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,rename according to train - test - dev - convention
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,data is not in IOB2 format. Thus we transform it to IOB2
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download data if necessary
v0.6.1,unpack and write out in CoNLL column-like format
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,"find train, dev and test files if not specified"
v0.6.1,use test_file to create test split if available
v0.6.1,use dev_file to create test split if available
v0.6.1,"if data point contains black-listed label, do not use"
v0.6.1,first check if valid sentence
v0.6.1,"if so, add to indices"
v0.6.1,"find train, dev and test files if not specified"
v0.6.1,variables
v0.6.1,different handling of in_memory data than streaming data
v0.6.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.6.1,test if format is OK
v0.6.1,test if at least one label given
v0.6.1,noinspection PyDefaultArgument
v0.6.1,dataset name includes the split size
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download each of the 28 splits
v0.6.1,create dataset directory if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,create dataset directory if necessary
v0.6.1,create train.txt file from CSV
v0.6.1,create test.txt file from CSV
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,create dataset directory if necessary
v0.6.1,create train.txt file by iterating over pos and neg file
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,create dataset directory if necessary
v0.6.1,create train.txt file by iterating over pos and neg file
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,create dataset directory if necessary
v0.6.1,create train.txt file by iterating over pos and neg file
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,create dataset directory if necessary
v0.6.1,create train.txt file by iterating over pos and neg file
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,create train.txt file by iterating over pos and neg file
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,download senteval datasets if necessary und unzip
v0.6.1,convert to FastText format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,Create flair compatible labels
v0.6.1,TREC-6 : NUM:dist -> __label__NUM
v0.6.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,Create flair compatible labels
v0.6.1,TREC-6 : NUM:dist -> __label__NUM
v0.6.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,check if dataset is supported
v0.6.1,set file names
v0.6.1,download and unzip in file structure if necessary
v0.6.1,instantiate corpus
v0.6.1,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.6.1,with sum of all entity lengths as secondary key
v0.6.1,calculate offset without current text
v0.6.1,because we stick all passages of a document together
v0.6.1,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.6.1,Try to fix incorrect annotations
v0.6.1,print(
v0.6.1,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.6.1,)
v0.6.1,Ignore empty lines or relation annotations
v0.6.1,FIX annotation of whitespaces (necessary for PDR)
v0.6.1,One token may contain multiple entities -> deque all of them
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.6.1,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,Edge case: last token starts a new entity
v0.6.1,Last document in file
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,In the huner split files there is no information whether a given id originates
v0.6.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.6.1,splitting here
v0.6.1,In the huner split files there is no information whether a given id originates
v0.6.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.6.1,splitting here
v0.6.1,In the huner split files there is no information whether a given id originates
v0.6.1,from the train or test file of the original corpus - so we have to adapt corpus
v0.6.1,splitting here
v0.6.1,Edge case: last token starts a new entity
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download file is huge => make default_dir visible so that derivative
v0.6.1,corpora can all use the same download file
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,Read texts
v0.6.1,Read annotations
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,We need to apply a patch to correct the original training file
v0.6.1,Articles title
v0.6.1,Article abstract
v0.6.1,Entity annotations
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,Edge case: last token starts a new entity
v0.6.1,Map all entities to chemicals
v0.6.1,Map all entities to disease
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,Incomplete article
v0.6.1,Invalid XML syntax
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,if len(mid) != 3:
v0.6.1,continue
v0.6.1,Try to fix entity offsets
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,There is still one illegal annotation in the file ..
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,"Abstract first, title second to prevent issues with sentence splitting"
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,"Filter for specific entity types, by default no entities will be filtered"
v0.6.1,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.6.1,train and dev split of V2 will be train in V4
v0.6.1,test split of V2 will be dev in V4
v0.6.1,New documents in V4 will become test documents
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,column format
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,cache Feidegger config file
v0.6.1,cache Feidegger images
v0.6.1,replace image URL with local cached file
v0.6.1,append Sentence-Image data point
v0.6.1,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.6.1,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.6.1,"if so, num_workers is set to 0 for faster processing"
v0.6.1,cast to list if necessary
v0.6.1,cast to list if necessary
v0.6.1,"first, check if pymongo is installed"
v0.6.1,automatically identify train / test / dev files
v0.6.1,"if no test file is found, take any file with 'test' in name"
v0.6.1,Expose base classses
v0.6.1,Expose all sequence labeling datasets
v0.6.1,Expose all document classification datasets
v0.6.1,Expose all treebanks
v0.6.1,Expose all text-text datasets
v0.6.1,Expose all text-image datasets
v0.6.1,Expose all biomedical data sets
v0.6.1,Expose all biomedical data sets using the HUNER splits
v0.6.1,-
v0.6.1,-
v0.6.1,-
v0.6.1,-
v0.6.1,Expose all biomedical data sets used for the evaluation of BioBERT
v0.6.1,"find train, dev and test files if not specified"
v0.6.1,get train data
v0.6.1,get test data
v0.6.1,get dev data
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,this dataset name
v0.6.1,default dataset folder is the cache root
v0.6.1,download data if necessary
v0.6.1,clean up file
v0.6.1,bioes tags
v0.6.1,bio tags
v0.6.1,broken tags
v0.6.1,all tags
v0.6.1,all weird tags
v0.6.1,tags with confidence
v0.6.1,bioes tags
v0.6.1,bioes tags
v0.6.1,increment for last token in sentence if not followed by whitespace
v0.6.1,clean up directory
v0.6.1,clean up directory
v0.6.1,initialize trainer
v0.6.1,clean up results directory
v0.6.1,initialize trainer
v0.6.1,clean up results directory
v0.6.1,initialize trainer
v0.6.1,clean up results directory
v0.6.1,initialize trainer
v0.6.1,clean up results directory
v0.6.1,initialize trainer
v0.6.1,clean up results directory
v0.6.1,clean up results directory
v0.6.1,initialize trainer
v0.6.1,clean up results directory
v0.6.1,def test_multiclass_metrics():
v0.6.1,
v0.6.1,"metric = Metric(""Test"")"
v0.6.1,"available_labels = [""A"", ""B"", ""C""]"
v0.6.1,
v0.6.1,"predictions = [""A"", ""B""]"
v0.6.1,"true_values = [""A""]"
v0.6.1,TextClassifier._evaluate_sentence_for_text_classification(
v0.6.1,"metric, available_labels, predictions, true_values"
v0.6.1,)
v0.6.1,
v0.6.1,"predictions = [""C"", ""B""]"
v0.6.1,"true_values = [""A"", ""B""]"
v0.6.1,TextClassifier._evaluate_sentence_for_text_classification(
v0.6.1,"metric, available_labels, predictions, true_values"
v0.6.1,)
v0.6.1,
v0.6.1,print(metric)
v0.6.1,from flair.trainers.trainer_regression import RegressorTrainer
v0.6.1,def test_trainer_results(tasks_base_path):
v0.6.1,"corpus, model, trainer = init(tasks_base_path)"
v0.6.1,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.6.1,"assert results[""test_score""] > 0"
v0.6.1,"assert len(results[""dev_loss_history""]) == 1"
v0.6.1,"assert len(results[""dev_score_history""]) == 1"
v0.6.1,"assert len(results[""train_loss_history""]) == 1"
v0.6.1,get default dictionary
v0.6.1,init forward LM with 128 hidden states and 1 layer
v0.6.1,get the example corpus and process at character level in forward direction
v0.6.1,train the language model
v0.6.1,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.6.1,clean up results directory
v0.6.1,get default dictionary
v0.6.1,init forward LM with 128 hidden states and 1 layer
v0.6.1,get the example corpus and process at character level in forward direction
v0.6.1,train the language model
v0.6.1,clean up results directory
v0.6.1,define search space
v0.6.1,sequence tagger parameter
v0.6.1,model trainer parameter
v0.6.1,training parameter
v0.6.1,find best parameter settings
v0.6.1,clean up results directory
v0.6.1,document embeddings parameter
v0.6.1,training parameter
v0.6.1,clean up results directory
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,"get training, test and dev data"
v0.6.1,get two corpora as one
v0.6.1,"get training, test and dev data for full English UD corpus from web"
v0.6.1,clean up data directory
v0.6.1,clean up results directory
v0.6.1,clean up results directory
v0.6.1,clean up results directory
v0.6.1,clean up results directory
v0.6.1,clean up results directory
v0.6.1,clean up results directory
v0.6,1. get the corpus
v0.6,2. what tag do we want to predict?
v0.6,3. make the tag dictionary from the corpus
v0.6,initialize embeddings
v0.6,comment in this line to use character embeddings
v0.6,"CharacterEmbeddings(),"
v0.6,comment in these lines to use contextual string embeddings
v0.6,
v0.6,"FlairEmbeddings('news-forward'),"
v0.6,
v0.6,"FlairEmbeddings('news-backward'),"
v0.6,initialize sequence tagger
v0.6,initialize trainer
v0.6,from allennlp.common.tqdm import Tqdm
v0.6,mmap seems to be much more memory efficient
v0.6,Remove quotes from etag
v0.6,"If there is an etag, it's everything after the first period"
v0.6,"Otherwise, use None"
v0.6,"URL, so get it from the cache (downloading if necessary)"
v0.6,"File, and it exists."
v0.6,"File, but it doesn't exist."
v0.6,Something unknown
v0.6,Extract all the contents of zip file in current directory
v0.6,Extract all the contents of zip file in current directory
v0.6,get cache path to put the file
v0.6,"Download to temporary file, then copy to cache dir once finished."
v0.6,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.6,GET file object
v0.6,TODO(joelgrus): do we want to do checksums or anything like that?
v0.6,get cache path to put the file
v0.6,make HEAD request to check ETag
v0.6,add ETag to filename if it exists
v0.6,"etag = response.headers.get(""ETag"")"
v0.6,"Download to temporary file, then copy to cache dir once finished."
v0.6,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.6,GET file object
v0.6,These defaults are the same as the argument defaults in tqdm.
v0.6,first determine the distribution of classes in the dataset
v0.6,weight for each sample
v0.6,Create blocks
v0.6,shuffle the blocks
v0.6,concatenate the shuffled blocks
v0.6,Create blocks
v0.6,shuffle the blocks
v0.6,concatenate the shuffled blocks
v0.6,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.6,see https://github.com/zalandoresearch/flair/issues/351
v0.6,State initialization
v0.6,Exponential moving average of gradient values
v0.6,Exponential moving average of squared gradient values
v0.6,Maintains max of all exp. moving avg. of sq. grad. values
v0.6,Decay the first and second moment running average coefficient
v0.6,Maintains the maximum of all 2nd moment running avg. till now
v0.6,Use the max. for normalizing running avg. of gradient
v0.6,determine offsets for whitespace_after field
v0.6,increment for last token in sentence if not followed by whitespace
v0.6,determine offsets for whitespace_after field
v0.6,conll 2000 column format
v0.6,conll 03 NER column format
v0.6,WNUT-17
v0.6,-- WikiNER datasets
v0.6,-- Universal Dependencies
v0.6,Germanic
v0.6,Romance
v0.6,West-Slavic
v0.6,South-Slavic
v0.6,East-Slavic
v0.6,Scandinavian
v0.6,Asian
v0.6,Language isolates
v0.6,recent Universal Dependencies
v0.6,other datasets
v0.6,text classification format
v0.6,text regression format
v0.6,"first, try to fetch dataset online"
v0.6,default dataset folder is the cache root
v0.6,get string value if enum is passed
v0.6,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.6,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.6,the CoNLL 03 task for German has an additional lemma column
v0.6,the CoNLL 03 task for Dutch has no NP column
v0.6,the CoNLL 03 task for Spanish only has two columns
v0.6,the GERMEVAL task only has two columns: text and ner
v0.6,WSD tasks may be put into this column format
v0.6,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.6,"for text classifiers, we use our own special format"
v0.6,NER corpus for Basque
v0.6,automatically identify train / test / dev files
v0.6,"if no test file is found, take any file with 'test' in name"
v0.6,get train and test data
v0.6,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.6,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.6,convert tag scheme to iobes
v0.6,automatically identify train / test / dev files
v0.6,automatically identify train / test / dev files
v0.6,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.6,conll 2000 chunking task
v0.6,Support both TREC-6 and TREC-50
v0.6,Create flair compatible labels
v0.6,TREC-6 : NUM:dist -> __label__NUM
v0.6,TREC-50: NUM:dist -> __label__NUM:dist
v0.6,Wikiner NER task
v0.6,unpack and write out in CoNLL column-like format
v0.6,CoNLL 02/03 NER
v0.6,universal dependencies
v0.6,--- UD Germanic
v0.6,--- UD Romance
v0.6,--- UD West-Slavic
v0.6,--- UD Scandinavian
v0.6,--- UD South-Slavic
v0.6,--- UD Asian
v0.6,this is the default init size of a lmdb database for embeddings
v0.6,some non-used parameter to allow print
v0.6,get db filename from embedding name
v0.6,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.6,SequenceTagger
v0.6,TextClassifier
v0.6,get db filename from embedding name
v0.6,if embedding database already exists
v0.6,"otherwise, push embedding to database"
v0.6,if embedding database already exists
v0.6,open the database in read mode
v0.6,we need to set self.k
v0.6,create and load the database in write mode
v0.6,"no idea why, but we need to close and reopen the environment to avoid"
v0.6,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.6,when opening new transaction !
v0.6,init dictionaries
v0.6,"in order to deal with unknown tokens, add <unk>"
v0.6,"We don't want to create a SpaceTokenizer object each time this function is called,"
v0.6,so delegate the call directly to the static run_tokenize method
v0.6,"We don't want to create a SegtokTokenizer object each time this function is called,"
v0.6,so delegate the call directly to the static run_tokenize method
v0.6,"if text is passed, instantiate sentence with tokens (words)"
v0.6,log a warning if the dataset is empty
v0.6,data with zero-width characters cannot be handled
v0.6,set token idx if not set
v0.6,non-set tags are OUT tags
v0.6,anything that is not a BIOES tag is a SINGLE tag
v0.6,anything that is not OUT is IN
v0.6,single and begin tags start a new span
v0.6,remember previous tag
v0.6,"if label type is explicitly specified, get spans for this label type"
v0.6,else determine all label types in sentence and get all spans
v0.6,move sentence embeddings to device
v0.6,move token embeddings to device
v0.6,clear sentence embeddings
v0.6,clear token embeddings
v0.6,infer whitespace after field
v0.6,add Sentence labels to output if they exist
v0.6,add Token labels to output if they exist
v0.6,add Sentence labels to output if they exist
v0.6,add Token labels to output if they exist
v0.6,No character at the corresponding code point: remove it
v0.6,set name
v0.6,sample test data if none is provided
v0.6,sample dev data if none is provided
v0.6,set train dev and test data
v0.6,find out empty sentence indices
v0.6,create subset of non-empty sentence indices
v0.6,find out empty sentence indices
v0.6,create subset of non-empty sentence indices
v0.6,check if sentence itself has labels
v0.6,check for labels of words
v0.6,Make the tag dictionary
v0.6,global variable: cache_root
v0.6,global variable: device
v0.6,global variable: embedding_storage_mode
v0.6,# dummy return to fulfill trainer.train() needs
v0.6,print(vec)
v0.6,Attach optimizer
v0.6,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.6,if memory mode option 'none' delete everything
v0.6,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.6,find out which ones are dynamic embeddings
v0.6,find out which ones are dynamic embeddings
v0.6,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.6,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.6,header for 'weights.txt'
v0.6,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.6,then get all relevant values from the tsv
v0.6,then get all relevant values from the tsv
v0.6,plot i
v0.6,save plots
v0.6,save plots
v0.6,plt.show()
v0.6,save plot
v0.6,take the average over the last three scores of training
v0.6,take average over the scores from the different training runs
v0.6,remove previous embeddings
v0.6,clearing token embeddings to save memory
v0.6,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.6,#TODO: not saving lines yet
v0.6,== similarity measures ==
v0.6,helper class for ModelSimilarity
v0.6,-- works with binary cross entropy loss --
v0.6,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.6,-- works with ranking/triplet loss --
v0.6,normalize the embeddings
v0.6,== similarity losses ==
v0.6,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.6,TODO: this assumes eye matrix
v0.6,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.6,== similarity learner ==
v0.6,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.6,assumes that for each data pair there's at least one embedding per modality
v0.6,pre-compute embeddings for all targets in evaluation dataset
v0.6,compute the similarity
v0.6,sort the similarity matrix across modality 1
v0.6,"get the ranks, so +1 to start counting ranks from 1"
v0.6,The conversion from old model's constructor interface
v0.6,auto-spawn on GPU if available
v0.6,pad strings with whitespaces to longest sentence
v0.6,cut up the input into chunks of max charlength = chunk_size
v0.6,push each chunk through the RNN language model
v0.6,concatenate all chunks to make final output
v0.6,initial hidden state
v0.6,get predicted weights
v0.6,divide by temperature
v0.6,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.6,this makes a vector in which the largest value is 0
v0.6,compute word weights with exponential function
v0.6,try sampling multinomial distribution for next character
v0.6,print(word_idx)
v0.6,input ids
v0.6,push list of character IDs through model
v0.6,the target is always the next character
v0.6,use cross entropy loss to compare output of forward pass with targets
v0.6,exponentiate cross-entropy loss to calculate perplexity
v0.6,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.6,"check if this is the case and if so, set it"
v0.6,set the dictionaries
v0.6,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.6,Initialize the weight tensor
v0.6,initialize the network architecture
v0.6,dropouts
v0.6,optional reprojection layer on top of word embeddings
v0.6,bidirectional LSTM on top of embedding layer
v0.6,Create initial hidden state and initialize it
v0.6,TODO: Decide how to initialize the hidden state variables
v0.6,self.hs_initializer(self.lstm_init_h)
v0.6,self.hs_initializer(self.lstm_init_c)
v0.6,final linear map to tag space
v0.6,reverse sort all sequences by their length
v0.6,progress bar for verbosity
v0.6,stop if all sentences are empty
v0.6,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.6,clearing token embeddings to save memory
v0.6,predict for batch
v0.6,make list of gold tags
v0.6,make list of predicted tags
v0.6,"check for true positives, false positives and false negatives"
v0.6,also write to file in BIO format to use old conlleval script
v0.6,check if in gold spans
v0.6,check if in predicted spans
v0.6,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.6,"if span F1 needs to be used, use separate eval method"
v0.6,"else, use scikit-learn to evaluate"
v0.6,predict for batch
v0.6,add gold tag
v0.6,add predicted tag
v0.6,for file output
v0.6,use sklearn
v0.6,"make ""classification report"""
v0.6,get scores
v0.6,line for log file
v0.6,--------------------------------------------------------------------
v0.6,FF PART
v0.6,--------------------------------------------------------------------
v0.6,"if initial hidden state is trainable, use this state"
v0.6,word dropout only before LSTM - TODO: more experimentation needed
v0.6,if self.use_word_dropout > 0.0:
v0.6,sentence_tensor = self.word_dropout(sentence_tensor)
v0.6,get the tags in this sentence
v0.6,add tags as tensor
v0.6,pad tags if using batch-CRF decoder
v0.6,reduce raw values to avoid NaN during exp
v0.6,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.6,default value
v0.6,the historical German taggers by the @redewiegergabe project
v0.6,clear embeddings after predicting
v0.6,load each model
v0.6,check if the same embeddings were already loaded previously
v0.6,"if the model uses StackedEmbedding, make a new stack with previous objects"
v0.6,sort embeddings by key alphabetically
v0.6,check previous embeddings and add if found
v0.6,only re-use static embeddings
v0.6,"if not found, use existing embedding"
v0.6,initialize new stack
v0.6,"of the model uses regular embedding, re-load if previous version found"
v0.6,Initialize the weight tensor
v0.6,auto-spawn on GPU if available
v0.6,filter empty sentences
v0.6,reverse sort all sequences by their length
v0.6,progress bar for verbosity
v0.6,stop if all sentences are empty
v0.6,clearing token embeddings to save memory
v0.6,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.6,use scikit-learn to evaluate
v0.6,remove previously predicted labels
v0.6,get the gold labels
v0.6,predict for batch
v0.6,get the predicted labels
v0.6,remove predicted labels
v0.6,"make ""classification report"""
v0.6,get scores
v0.6,line for log file
v0.6,English sentiment models
v0.6,Communicative Functions Model
v0.6,cast string to Path
v0.6,"determine what splits (train, dev, test) to evaluate and log"
v0.6,prepare loss logging file and set up header
v0.6,"minimize training loss if training with dev data, else maximize dev score"
v0.6,"if training also uses dev data, include in training set"
v0.6,initialize sampler if provided
v0.6,init with default values if only class is provided
v0.6,set dataset to sample from
v0.6,At any point you can hit Ctrl + C to break out of training early.
v0.6,get new learning rate
v0.6,reload last best model if annealing with restarts is enabled
v0.6,stop training if learning rate becomes too small
v0.6,process mini-batches
v0.6,zero the gradients on the model and optimizer
v0.6,"if necessary, make batch_steps"
v0.6,forward and backward for batch
v0.6,forward pass
v0.6,Backward
v0.6,do the optimizer step
v0.6,do the scheduler step if one-cycle
v0.6,get new learning rate
v0.6,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.6,evaluate on train / dev / test split depending on training settings
v0.6,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6,calculate scores using dev data if available
v0.6,append dev score to score history
v0.6,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.6,determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
v0.6,determine bad epoch number
v0.6,log bad epochs
v0.6,output log file
v0.6,make headers on first epoch
v0.6,"if checkpoint is enabled, save model at each epoch"
v0.6,"if we use dev data, remember best model based on dev evaluation score"
v0.6,"if we do not use dev data for model selection, save final model"
v0.6,test best model if test data is present
v0.6,"if we are training over multiple datasets, do evaluation for each"
v0.6,get and return the final test score of best model
v0.6,cast string to Path
v0.6,forward pass
v0.6,update optimizer and scheduler
v0.6,Add chars to the dictionary
v0.6,charsplit file content
v0.6,charsplit file content
v0.6,Add words to the dictionary
v0.6,Tokenize file content
v0.6,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.6,cast string to Path
v0.6,error message if the validation dataset is too small
v0.6,Shuffle training files randomly after serially iterating through corpus one
v0.6,"iterate through training data, starting at self.split (for checkpointing)"
v0.6,off by one for printing
v0.6,go into train mode
v0.6,reset variables
v0.6,not really sure what this does
v0.6,do the forward pass in the model
v0.6,try to predict the targets
v0.6,Backward
v0.6,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.6,We detach the hidden state from how it was previously produced.
v0.6,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.6,explicitly remove loss to clear up memory
v0.6,##############################################################################
v0.6,Save the model if the validation loss is the best we've seen so far.
v0.6,##############################################################################
v0.6,print info
v0.6,##############################################################################
v0.6,##############################################################################
v0.6,final testing
v0.6,##############################################################################
v0.6,Turn on evaluation mode which disables dropout.
v0.6,Work out how cleanly we can divide the dataset into bsz parts.
v0.6,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.6,Evenly divide the data across the bsz batches.
v0.6,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.6,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.6,news-english-forward
v0.6,news-english-backward
v0.6,news-english-forward
v0.6,news-english-backward
v0.6,mix-english-forward
v0.6,mix-english-backward
v0.6,mix-german-forward
v0.6,mix-german-backward
v0.6,common crawl Polish forward
v0.6,common crawl Polish backward
v0.6,Slovenian forward
v0.6,Slovenian backward
v0.6,Bulgarian forward
v0.6,Bulgarian backward
v0.6,Dutch forward
v0.6,Dutch backward
v0.6,Swedish forward
v0.6,Swedish backward
v0.6,French forward
v0.6,French backward
v0.6,Czech forward
v0.6,Czech backward
v0.6,Portuguese forward
v0.6,Portuguese backward
v0.6,initialize cache if use_cache set
v0.6,embed a dummy sentence to determine embedding_length
v0.6,set to eval mode
v0.6,Copy the object's state from self.__dict__ which contains
v0.6,all our instance attributes. Always use the dict.copy()
v0.6,method to avoid modifying the original state.
v0.6,Remove the unpicklable entries.
v0.6,"if cache is used, try setting embeddings from cache first"
v0.6,try populating embeddings from cache
v0.6,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.6,get hidden states from language model
v0.6,take first or last hidden states from language model as word representation
v0.6,if self.tokenized_lm or token.whitespace_after:
v0.6,1-camembert-base -> camembert-base
v0.6,1-xlm-roberta-large -> xlm-roberta-large
v0.6,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.6,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.6,tokens are attended to.
v0.6,Zero-pad up to the sequence length.
v0.6,"first, find longest sentence in batch"
v0.6,prepare id maps for BERT model
v0.6,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.6,get aggregated embeddings for each BERT-subtoken in sentence
v0.6,get the current sentence object
v0.6,add concatenated embedding to sentence
v0.6,use first subword embedding if pooling operation is 'first'
v0.6,"otherwise, do a mean over all subwords in token"
v0.6,"if only one sentence is passed, convert to list of sentence"
v0.6,bidirectional LSTM on top of embedding layer
v0.6,dropouts
v0.6,"first, sort sentences by number of tokens"
v0.6,go through each sentence in batch
v0.6,PADDING: pad shorter sentences out
v0.6,ADD TO SENTENCE LIST: add the representation
v0.6,--------------------------------------------------------------------
v0.6,GET REPRESENTATION FOR ENTIRE BATCH
v0.6,--------------------------------------------------------------------
v0.6,--------------------------------------------------------------------
v0.6,FF PART
v0.6,--------------------------------------------------------------------
v0.6,use word dropout if set
v0.6,--------------------------------------------------------------------
v0.6,EXTRACT EMBEDDINGS FROM LSTM
v0.6,--------------------------------------------------------------------
v0.6,embed a dummy sentence to determine embedding_length
v0.6,Avoid conflicts with flair's Token class
v0.6,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.6,add positional encodings
v0.6,reshape the pixels into the sequence
v0.6,layer norm after convolution and positional encodings
v0.6,add <cls> token
v0.6,"transformer requires input in the shape [h*w+1, b, d]"
v0.6,the output is an embedding of <cls> token
v0.6,temporary fix to disable tokenizer parallelism warning
v0.6,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.6,load tokenizer and transformer model
v0.6,model name
v0.6,"when initializing, embeddings are in eval mode by default"
v0.6,embedding parameters
v0.6,send mini-token through to check how many layers the model has
v0.6,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.6,using list comprehension
v0.6,gradients are enabled if fine-tuning is enabled
v0.6,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.6,subtokenize sentences
v0.6,tokenize and truncate to max subtokens (TODO: check better truncation strategies)
v0.6,find longest sentence in batch
v0.6,initialize batch tensors and mask
v0.6,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.6,iterate over all subtokenized sentences
v0.6,use scalar mix of embeddings if so selected
v0.6,set the extracted embedding for the token
v0.6,reload tokenizer to get around serialization issues
v0.6,optional fine-tuning on top of embedding layer
v0.6,"if only one sentence is passed, convert to list of sentence"
v0.6,bidirectional RNN on top of embedding layer
v0.6,dropouts
v0.6,TODO: remove in future versions
v0.6,embed words in the sentence
v0.6,before-RNN dropout
v0.6,reproject if set
v0.6,push through RNN
v0.6,after-RNN dropout
v0.6,extract embeddings from RNN
v0.6,models that were serialized using torch versions older than 1.4.0 lack the _flat_weights_names attribute
v0.6,"check if this is the case and if so, set it"
v0.6,IMPORTANT: add embeddings as torch modules
v0.6,iterate over sentences
v0.6,"if its a forward LM, take last state"
v0.6,"convert to plain strings, embedded in a list for the encode function"
v0.6,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.6,"if only one sentence is passed, convert to list of sentence"
v0.6,Expose base classses
v0.6,Expose token embedding classes
v0.6,Expose document embedding classes
v0.6,Expose image embedding classes
v0.6,Expose legacy embedding classes
v0.6,IMPORTANT: add embeddings as torch modules
v0.6,"if only one sentence is passed, convert to list of sentence"
v0.6,GLOVE embeddings
v0.6,TURIAN embeddings
v0.6,KOMNINOS embeddings
v0.6,pubmed embeddings
v0.6,FT-CRAWL embeddings
v0.6,FT-CRAWL embeddings
v0.6,twitter embeddings
v0.6,two-letter language code wiki embeddings
v0.6,two-letter language code wiki embeddings
v0.6,two-letter language code crawl embeddings
v0.6,fix serialized models
v0.6,use list of common characters if none provided
v0.6,translate words in sentence into ints using dictionary
v0.6,"sort words by length, for batching and masking"
v0.6,chars for rnn processing
v0.6,multilingual models
v0.6,English models
v0.6,Arabic
v0.6,Bulgarian
v0.6,Czech
v0.6,Danish
v0.6,German
v0.6,Spanish
v0.6,Basque
v0.6,Persian
v0.6,Finnish
v0.6,French
v0.6,Hebrew
v0.6,Hindi
v0.6,Croatian
v0.6,Indonesian
v0.6,Italian
v0.6,Japanese
v0.6,Malayalam
v0.6,Dutch
v0.6,Norwegian
v0.6,Polish
v0.6,Portuguese
v0.6,Pubmed
v0.6,Slovenian
v0.6,Swedish
v0.6,Tamil
v0.6,CLEF HIPE Shared task
v0.6,load model if in pretrained model map
v0.6,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.6,embeddings are static if we don't do finetuning
v0.6,embed a dummy sentence to determine embedding_length
v0.6,set to eval mode
v0.6,make compatible with serialized models (TODO: remove)
v0.6,make compatible with serialized models (TODO: remove)
v0.6,gradients are enable if fine-tuning is enabled
v0.6,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.6,get hidden states from language model
v0.6,take first or last hidden states from language model as word representation
v0.6,offset mode that extracts at whitespace after last character
v0.6,offset mode that extracts at last character
v0.6,only clone if optimization mode is 'gpu'
v0.6,use the character language model embeddings as basis
v0.6,length is twice the original character LM embedding length
v0.6,these fields are for the embedding memory
v0.6,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.6,we re-compute embeddings dynamically at each epoch
v0.6,set the memory method
v0.6,memory is wiped each time we do a training run
v0.6,"if we keep a pooling, it needs to be updated continuously"
v0.6,update embedding
v0.6,check token.text is empty or not
v0.6,set aggregation operation
v0.6,add embeddings after updating
v0.6,temporary fix to disable tokenizer parallelism warning
v0.6,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.6,load tokenizer and transformer model
v0.6,model name
v0.6,"when initializing, embeddings are in eval mode by default"
v0.6,embedding parameters
v0.6,send mini-token through to check how many layers the model has
v0.6,"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
v0.6,check if special tokens exist to circumvent error message
v0.6,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.6,split into micro batches of size self.batch_size before pushing through transformer
v0.6,embed each micro-batch
v0.6,remove special markup
v0.6,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.6,"TODO: keep for backwards compatibility, but remove in future"
v0.6,"some pretrained models do not have this property, applying default settings now."
v0.6,can be set manually after loading the model.
v0.6,method 1: subtokenize sentence
v0.6,"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
v0.6,method 2:
v0.6,transformer specific tokenization
v0.6,empty sentences get zero embeddings
v0.6,only embed non-empty sentences and if there is at least one
v0.6,find longest sentence in batch
v0.6,initialize batch tensors and mask
v0.6,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.6,make the tuple a tensor; makes working with it easier.
v0.6,gradients are enabled if fine-tuning is enabled
v0.6,iterate over all subtokenized sentences
v0.6,"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
v0.6,in order to get some context into the embeddings of these words.
v0.6,also don't include the embedding of the extra [CLS] and [SEP] tokens.
v0.6,"for each token, get embedding"
v0.6,some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
v0.6,"get states from all selected layers, aggregate with pooling operation"
v0.6,use scalar mix of embeddings if so selected
v0.6,sm_embeddings = self.mix(subtoken_embeddings)
v0.6,set the extracted embedding for the token
v0.6,iterate over subtokens and reconstruct tokens
v0.6,remove special markup
v0.6,TODO check if this is necessary is this method is called before prepare_for_model
v0.6,check if reconstructed token is special begin token ([CLS] or similar)
v0.6,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.6,append subtoken to reconstruct token
v0.6,check if reconstructed token is the same as current token
v0.6,"if so, add subtoken count"
v0.6,reset subtoken count and reconstructed token
v0.6,break from loop if all tokens are accounted for
v0.6,if tokens are unaccounted for
v0.6,check if all tokens were matched to subtokens
v0.6,"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
v0.6,module should never be in training mode
v0.6,reload tokenizer to get around serialization issues
v0.6,max_tokens = 500
v0.6,model architecture
v0.6,model architecture
v0.6,download if necessary
v0.6,load the model
v0.6,"TODO: keep for backwards compatibility, but remove in future"
v0.6,save the sentence piece model as binary file (not as path which may change)
v0.6,write out the binary sentence piece model into the expected directory
v0.6,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.6,"otherwise, use normal process and potentially trigger another download"
v0.6,"once the modes if there, load it with sentence piece"
v0.6,empty words get no embedding
v0.6,all other words get embedded
v0.6,"the default model for ELMo is the 'original' model, which is very large"
v0.6,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.6,put on Cuda if available
v0.6,embed a dummy sentence to determine embedding_length
v0.6,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.6,GLOVE embeddings
v0.6,"find train, dev and test files if not specified"
v0.6,get train data
v0.6,read in test file if exists
v0.6,read in dev file if exists
v0.6,special key for space after
v0.6,"store either Sentence objects in memory, or only file offsets"
v0.6,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.6,determine encoding of text file
v0.6,skip first line if to selected
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,check if data there
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,check if data there
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,Remove CoNLL-U meta information in the last column
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,check if data there
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,rename according to train - test - dev - convention
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,data is not in IOB2 format. Thus we transform it to IOB2
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download data if necessary
v0.6,unpack and write out in CoNLL column-like format
v0.6,"find train, dev and test files if not specified"
v0.6,use test_file to create test split if available
v0.6,use dev_file to create test split if available
v0.6,"if data point contains black-listed label, do not use"
v0.6,first check if valid sentence
v0.6,"if so, add to indices"
v0.6,"find train, dev and test files if not specified"
v0.6,variables
v0.6,different handling of in_memory data than streaming data
v0.6,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.6,test if format is OK
v0.6,test if at least one label given
v0.6,noinspection PyDefaultArgument
v0.6,dataset name includes the split size
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download each of the 28 splits
v0.6,create dataset directory if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,create dataset directory if necessary
v0.6,create train.txt file from CSV
v0.6,create test.txt file from CSV
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,create dataset directory if necessary
v0.6,create train.txt file by iterating over pos and neg file
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,create dataset directory if necessary
v0.6,create train.txt file by iterating over pos and neg file
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,create dataset directory if necessary
v0.6,create train.txt file by iterating over pos and neg file
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,create dataset directory if necessary
v0.6,create train.txt file by iterating over pos and neg file
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,create train.txt file by iterating over pos and neg file
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,download senteval datasets if necessary und unzip
v0.6,convert to FastText format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,Create flair compatible labels
v0.6,TREC-6 : NUM:dist -> __label__NUM
v0.6,TREC-50: NUM:dist -> __label__NUM:dist
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,Create flair compatible labels
v0.6,TREC-6 : NUM:dist -> __label__NUM
v0.6,TREC-50: NUM:dist -> __label__NUM:dist
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,check if dataset is supported
v0.6,set file names
v0.6,download and unzip in file structure if necessary
v0.6,instantiate corpus
v0.6,Uses dynamic programming approach to calculate maximum independent set in interval graph
v0.6,with sum of all entity lengths as secondary key
v0.6,calculate offset without current text
v0.6,because we stick all passages of a document together
v0.6,TODO For split entities we also annotate everything inbetween which might be a bad idea?
v0.6,Try to fix incorrect annotations
v0.6,print(
v0.6,"f""Found {non_matching} non-matching entities ({non_matching/all_entities}%) in {bioc_file}"""
v0.6,)
v0.6,Ignore empty lines or relation annotations
v0.6,FIX annotation of whitespaces (necessary for PDR)
v0.6,One token may contain multiple entities -> deque all of them
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,Create tokenization-dependent CONLL files. This is necessary to prevent
v0.6,from caching issues (e.g. loading the same corpus with different sentence splitters)
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,Edge case: last token starts a new entity
v0.6,Last document in file
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,In the huner split files there is no information whether a given id originates
v0.6,from the train or test file of the original corpus - so we have to adapt corpus
v0.6,splitting here
v0.6,In the huner split files there is no information whether a given id originates
v0.6,from the train or test file of the original corpus - so we have to adapt corpus
v0.6,splitting here
v0.6,In the huner split files there is no information whether a given id originates
v0.6,from the train or test file of the original corpus - so we have to adapt corpus
v0.6,splitting here
v0.6,Edge case: last token starts a new entity
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download file is huge => make default_dir visible so that derivative
v0.6,corpora can all use the same download file
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,Read texts
v0.6,Read annotations
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,We need to apply a patch to correct the original training file
v0.6,Articles title
v0.6,Article abstract
v0.6,Entity annotations
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,Edge case: last token starts a new entity
v0.6,Map all entities to chemicals
v0.6,Map all entities to disease
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,Incomplete article
v0.6,Invalid XML syntax
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,if len(mid) != 3:
v0.6,continue
v0.6,Try to fix entity offsets
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,There is still one illegal annotation in the file ..
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,"Abstract first, title second to prevent issues with sentence splitting"
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,"Filter for specific entity types, by default no entities will be filtered"
v0.6,Get original HUNER splits to retrieve a list of all document ids contained in V2
v0.6,train and dev split of V2 will be train in V4
v0.6,test split of V2 will be dev in V4
v0.6,New documents in V4 will become test documents
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,column format
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,cache Feidegger config file
v0.6,cache Feidegger images
v0.6,replace image URL with local cached file
v0.6,append Sentence-Image data point
v0.6,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.6,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.6,"if so, num_workers is set to 0 for faster processing"
v0.6,cast to list if necessary
v0.6,cast to list if necessary
v0.6,"first, check if pymongo is installed"
v0.6,automatically identify train / test / dev files
v0.6,"if no test file is found, take any file with 'test' in name"
v0.6,Expose base classses
v0.6,Expose all sequence labeling datasets
v0.6,Expose all document classification datasets
v0.6,Expose all treebanks
v0.6,Expose all text-text datasets
v0.6,Expose all text-image datasets
v0.6,Expose all biomedical data sets
v0.6,Expose all biomedical data sets using the HUNER splits
v0.6,-
v0.6,-
v0.6,-
v0.6,-
v0.6,Expose all biomedical data sets used for the evaluation of BioBERT
v0.6,"find train, dev and test files if not specified"
v0.6,get train data
v0.6,get test data
v0.6,get dev data
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,this dataset name
v0.6,default dataset folder is the cache root
v0.6,download data if necessary
v0.6,clean up file
v0.6,bioes tags
v0.6,bio tags
v0.6,broken tags
v0.6,all tags
v0.6,all weird tags
v0.6,tags with confidence
v0.6,bioes tags
v0.6,bioes tags
v0.6,increment for last token in sentence if not followed by whitespace
v0.6,clean up directory
v0.6,clean up directory
v0.6,initialize trainer
v0.6,clean up results directory
v0.6,initialize trainer
v0.6,clean up results directory
v0.6,initialize trainer
v0.6,clean up results directory
v0.6,initialize trainer
v0.6,clean up results directory
v0.6,initialize trainer
v0.6,clean up results directory
v0.6,clean up results directory
v0.6,initialize trainer
v0.6,clean up results directory
v0.6,def test_multiclass_metrics():
v0.6,
v0.6,"metric = Metric(""Test"")"
v0.6,"available_labels = [""A"", ""B"", ""C""]"
v0.6,
v0.6,"predictions = [""A"", ""B""]"
v0.6,"true_values = [""A""]"
v0.6,TextClassifier._evaluate_sentence_for_text_classification(
v0.6,"metric, available_labels, predictions, true_values"
v0.6,)
v0.6,
v0.6,"predictions = [""C"", ""B""]"
v0.6,"true_values = [""A"", ""B""]"
v0.6,TextClassifier._evaluate_sentence_for_text_classification(
v0.6,"metric, available_labels, predictions, true_values"
v0.6,)
v0.6,
v0.6,print(metric)
v0.6,from flair.trainers.trainer_regression import RegressorTrainer
v0.6,def test_trainer_results(tasks_base_path):
v0.6,"corpus, model, trainer = init(tasks_base_path)"
v0.6,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.6,"assert results[""test_score""] > 0"
v0.6,"assert len(results[""dev_loss_history""]) == 1"
v0.6,"assert len(results[""dev_score_history""]) == 1"
v0.6,"assert len(results[""train_loss_history""]) == 1"
v0.6,get default dictionary
v0.6,init forward LM with 128 hidden states and 1 layer
v0.6,get the example corpus and process at character level in forward direction
v0.6,train the language model
v0.6,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.6,clean up results directory
v0.6,get default dictionary
v0.6,init forward LM with 128 hidden states and 1 layer
v0.6,get the example corpus and process at character level in forward direction
v0.6,train the language model
v0.6,clean up results directory
v0.6,define search space
v0.6,sequence tagger parameter
v0.6,model trainer parameter
v0.6,training parameter
v0.6,find best parameter settings
v0.6,clean up results directory
v0.6,document embeddings parameter
v0.6,training parameter
v0.6,clean up results directory
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,"get training, test and dev data"
v0.6,get two corpora as one
v0.6,"get training, test and dev data for full English UD corpus from web"
v0.6,clean up data directory
v0.6,clean up results directory
v0.6,clean up results directory
v0.6,clean up results directory
v0.6,clean up results directory
v0.6,clean up results directory
v0.6,clean up results directory
v0.5.1,1. get the corpus
v0.5.1,2. what tag do we want to predict?
v0.5.1,3. make the tag dictionary from the corpus
v0.5.1,initialize embeddings
v0.5.1,comment in this line to use character embeddings
v0.5.1,"CharacterEmbeddings(),"
v0.5.1,comment in these lines to use contextual string embeddings
v0.5.1,
v0.5.1,"FlairEmbeddings('news-forward'),"
v0.5.1,
v0.5.1,"FlairEmbeddings('news-backward'),"
v0.5.1,initialize sequence tagger
v0.5.1,initialize trainer
v0.5.1,from allennlp.common.tqdm import Tqdm
v0.5.1,mmap seems to be much more memory efficient
v0.5.1,Remove quotes from etag
v0.5.1,"If there is an etag, it's everything after the first period"
v0.5.1,"Otherwise, use None"
v0.5.1,"URL, so get it from the cache (downloading if necessary)"
v0.5.1,"File, and it exists."
v0.5.1,"File, but it doesn't exist."
v0.5.1,Something unknown
v0.5.1,Extract all the contents of zip file in current directory
v0.5.1,get cache path to put the file
v0.5.1,"Download to temporary file, then copy to cache dir once finished."
v0.5.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.5.1,GET file object
v0.5.1,TODO(joelgrus): do we want to do checksums or anything like that?
v0.5.1,get cache path to put the file
v0.5.1,make HEAD request to check ETag
v0.5.1,add ETag to filename if it exists
v0.5.1,"etag = response.headers.get(""ETag"")"
v0.5.1,"Download to temporary file, then copy to cache dir once finished."
v0.5.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.5.1,GET file object
v0.5.1,These defaults are the same as the argument defaults in tqdm.
v0.5.1,first determine the distribution of classes in the dataset
v0.5.1,weight for each sample
v0.5.1,Create blocks
v0.5.1,shuffle the blocks
v0.5.1,concatenate the shuffled blocks
v0.5.1,Create blocks
v0.5.1,shuffle the blocks
v0.5.1,concatenate the shuffled blocks
v0.5.1,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.5.1,see https://github.com/zalandoresearch/flair/issues/351
v0.5.1,State initialization
v0.5.1,Exponential moving average of gradient values
v0.5.1,Exponential moving average of squared gradient values
v0.5.1,Maintains max of all exp. moving avg. of sq. grad. values
v0.5.1,Decay the first and second moment running average coefficient
v0.5.1,Maintains the maximum of all 2nd moment running avg. till now
v0.5.1,Use the max. for normalizing running avg. of gradient
v0.5.1,determine offsets for whitespace_after field
v0.5.1,increment for last token in sentence if not followed by whitespace
v0.5.1,determine offsets for whitespace_after field
v0.5.1,conll 2000 column format
v0.5.1,conll 03 NER column format
v0.5.1,WNUT-17
v0.5.1,-- WikiNER datasets
v0.5.1,-- Universal Dependencies
v0.5.1,Germanic
v0.5.1,Romance
v0.5.1,West-Slavic
v0.5.1,South-Slavic
v0.5.1,East-Slavic
v0.5.1,Scandinavian
v0.5.1,Asian
v0.5.1,Language isolates
v0.5.1,recent Universal Dependencies
v0.5.1,other datasets
v0.5.1,text classification format
v0.5.1,text regression format
v0.5.1,"first, try to fetch dataset online"
v0.5.1,default dataset folder is the cache root
v0.5.1,get string value if enum is passed
v0.5.1,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.5.1,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.5.1,the CoNLL 03 task for German has an additional lemma column
v0.5.1,the CoNLL 03 task for Dutch has no NP column
v0.5.1,the CoNLL 03 task for Spanish only has two columns
v0.5.1,the GERMEVAL task only has two columns: text and ner
v0.5.1,WSD tasks may be put into this column format
v0.5.1,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.5.1,"for text classifiers, we use our own special format"
v0.5.1,NER corpus for Basque
v0.5.1,automatically identify train / test / dev files
v0.5.1,"if no test file is found, take any file with 'test' in name"
v0.5.1,get train and test data
v0.5.1,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.5.1,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.5.1,convert tag scheme to iobes
v0.5.1,automatically identify train / test / dev files
v0.5.1,automatically identify train / test / dev files
v0.5.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.5.1,conll 2000 chunking task
v0.5.1,Support both TREC-6 and TREC-50
v0.5.1,Create flair compatible labels
v0.5.1,TREC-6 : NUM:dist -> __label__NUM
v0.5.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.5.1,Wikiner NER task
v0.5.1,unpack and write out in CoNLL column-like format
v0.5.1,CoNLL 02/03 NER
v0.5.1,universal dependencies
v0.5.1,--- UD Germanic
v0.5.1,--- UD Romance
v0.5.1,--- UD West-Slavic
v0.5.1,--- UD Scandinavian
v0.5.1,--- UD South-Slavic
v0.5.1,--- UD Asian
v0.5.1,this is the default init size of a lmdb database for embeddings
v0.5.1,some non-used parameter to allow print
v0.5.1,get db filename from embedding name
v0.5.1,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.5.1,SequenceTagger
v0.5.1,TextClassifier
v0.5.1,get db filename from embedding name
v0.5.1,if embedding database already exists
v0.5.1,"otherwise, push embedding to database"
v0.5.1,if embedding database already exists
v0.5.1,open the database in read mode
v0.5.1,we need to set self.k
v0.5.1,create and load the database in write mode
v0.5.1,"no idea why, but we need to close and reopen the environment to avoid"
v0.5.1,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.5.1,when opening new transaction !
v0.5.1,init dictionaries
v0.5.1,"in order to deal with unknown tokens, add <unk>"
v0.5.1,"We don't want to create a SpaceTokenizer object each time this function is called,"
v0.5.1,so delegate the call directly to the static run_tokenize method
v0.5.1,"We don't want to create a SegtokTokenizer object each time this function is called,"
v0.5.1,so delegate the call directly to the static run_tokenize method
v0.5.1,"if text is passed, instantiate sentence with tokens (words)"
v0.5.1,log a warning if the dataset is empty
v0.5.1,data with zero-width characters cannot be handled
v0.5.1,set token idx if not set
v0.5.1,non-set tags are OUT tags
v0.5.1,anything that is not a BIOES tag is a SINGLE tag
v0.5.1,anything that is not OUT is IN
v0.5.1,single and begin tags start a new span
v0.5.1,remember previous tag
v0.5.1,move sentence embeddings to device
v0.5.1,move token embeddings to device
v0.5.1,clear sentence embeddings
v0.5.1,clear token embeddings
v0.5.1,infer whitespace after field
v0.5.1,add Sentence labels to output if they exist
v0.5.1,add Token labels to output if they exist
v0.5.1,add Sentence labels to output if they exist
v0.5.1,add Token labels to output if they exist
v0.5.1,No character at the corresponding code point: remove it
v0.5.1,set name
v0.5.1,sample test data if none is provided
v0.5.1,sample dev data if none is provided
v0.5.1,set train dev and test data
v0.5.1,find out empty sentence indices
v0.5.1,create subset of non-empty sentence indices
v0.5.1,check if sentence itself has labels
v0.5.1,check for labels of words
v0.5.1,Make the tag dictionary
v0.5.1,global variable: cache_root
v0.5.1,global variable: device
v0.5.1,global variable: embedding_storage_mode
v0.5.1,# dummy return to fulfill trainer.train() needs
v0.5.1,Attach optimizer
v0.5.1,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.5.1,if memory mode option 'none' delete everything
v0.5.1,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.5.1,find out which ones are dynamic embeddings
v0.5.1,find out which ones are dynamic embeddings
v0.5.1,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.5.1,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.5.1,header for 'weights.txt'
v0.5.1,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.5.1,then get all relevant values from the tsv
v0.5.1,then get all relevant values from the tsv
v0.5.1,plot i
v0.5.1,save plots
v0.5.1,save plots
v0.5.1,plt.show()
v0.5.1,save plot
v0.5.1,take the average over the last three scores of training
v0.5.1,take average over the scores from the different training runs
v0.5.1,remove previous embeddings
v0.5.1,clearing token embeddings to save memory
v0.5.1,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.5.1,#TODO: not saving lines yet
v0.5.1,== similarity measures ==
v0.5.1,helper class for ModelSimilarity
v0.5.1,-- works with binary cross entropy loss --
v0.5.1,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.5.1,-- works with ranking/triplet loss --
v0.5.1,normalize the embeddings
v0.5.1,== similarity losses ==
v0.5.1,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.5.1,TODO: this assumes eye matrix
v0.5.1,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.5.1,== similarity learner ==
v0.5.1,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.5.1,assumes that for each data pair there's at least one embedding per modality
v0.5.1,pre-compute embeddings for all targets in evaluation dataset
v0.5.1,compute the similarity
v0.5.1,sort the similarity matrix across modality 1
v0.5.1,"get the ranks, so +1 to start counting ranks from 1"
v0.5.1,The conversion from old model's constructor interface
v0.5.1,auto-spawn on GPU if available
v0.5.1,pad strings with whitespaces to longest sentence
v0.5.1,cut up the input into chunks of max charlength = chunk_size
v0.5.1,push each chunk through the RNN language model
v0.5.1,concatenate all chunks to make final output
v0.5.1,initial hidden state
v0.5.1,get predicted weights
v0.5.1,divide by temperature
v0.5.1,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.5.1,this makes a vector in which the largest value is 0
v0.5.1,compute word weights with exponential function
v0.5.1,try sampling multinomial distribution for next character
v0.5.1,print(word_idx)
v0.5.1,input ids
v0.5.1,push list of character IDs through model
v0.5.1,the target is always the next character
v0.5.1,use cross entropy loss to compare output of forward pass with targets
v0.5.1,exponentiate cross-entropy loss to calculate perplexity
v0.5.1,fixed RNN change format for torch 1.4.0
v0.5.1,set the dictionaries
v0.5.1,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.5.1,Initialize the weight tensor
v0.5.1,initialize the network architecture
v0.5.1,dropouts
v0.5.1,optional reprojection layer on top of word embeddings
v0.5.1,bidirectional LSTM on top of embedding layer
v0.5.1,Create initial hidden state and initialize it
v0.5.1,TODO: Decide how to initialize the hidden state variables
v0.5.1,self.hs_initializer(self.lstm_init_h)
v0.5.1,self.hs_initializer(self.lstm_init_c)
v0.5.1,final linear map to tag space
v0.5.1,reverse sort all sequences by their length
v0.5.1,progress bar for verbosity
v0.5.1,stop if all sentences are empty
v0.5.1,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.5.1,clearing token embeddings to save memory
v0.5.1,predict for batch
v0.5.1,make list of gold tags
v0.5.1,make list of predicted tags
v0.5.1,"check for true positives, false positives and false negatives"
v0.5.1,also write to file in BIO format to use old conlleval script
v0.5.1,check if in gold spans
v0.5.1,check if in predicted spans
v0.5.1,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.5.1,"if span F1 needs to be used, use separate eval method"
v0.5.1,"else, use scikit-learn to evaluate"
v0.5.1,predict for batch
v0.5.1,add gold tag
v0.5.1,add predicted tag
v0.5.1,for file output
v0.5.1,use sklearn
v0.5.1,"make ""classification report"""
v0.5.1,get scores
v0.5.1,line for log file
v0.5.1,--------------------------------------------------------------------
v0.5.1,FF PART
v0.5.1,--------------------------------------------------------------------
v0.5.1,"if initial hidden state is trainable, use this state"
v0.5.1,word dropout only before LSTM - TODO: more experimentation needed
v0.5.1,if self.use_word_dropout > 0.0:
v0.5.1,sentence_tensor = self.word_dropout(sentence_tensor)
v0.5.1,get the tags in this sentence
v0.5.1,add tags as tensor
v0.5.1,pad tags if using batch-CRF decoder
v0.5.1,reduce raw values to avoid NaN during exp
v0.5.1,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.5.1,default value
v0.5.1,the historical German taggers by the @redewiegergabe project
v0.5.1,Initialize the weight tensor
v0.5.1,auto-spawn on GPU if available
v0.5.1,filter empty sentences
v0.5.1,reverse sort all sequences by their length
v0.5.1,progress bar for verbosity
v0.5.1,stop if all sentences are empty
v0.5.1,clearing token embeddings to save memory
v0.5.1,"read Dataset into data loader (if list of sentences passed, make Dataset first)"
v0.5.1,use scikit-learn to evaluate
v0.5.1,predict for batch
v0.5.1,remove predicted labels
v0.5.1,"make ""classification report"""
v0.5.1,get scores
v0.5.1,line for log file
v0.5.1,English sentiment models
v0.5.1,Communicative Functions Model
v0.5.1,cast string to Path
v0.5.1,"determine what splits (train, dev, test) to evaluate and log"
v0.5.1,prepare loss logging file and set up header
v0.5.1,"minimize training loss if training with dev data, else maximize dev score"
v0.5.1,"if training also uses dev data, include in training set"
v0.5.1,initialize sampler if provided
v0.5.1,init with default values if only class is provided
v0.5.1,set dataset to sample from
v0.5.1,At any point you can hit Ctrl + C to break out of training early.
v0.5.1,get new learning rate
v0.5.1,reload last best model if annealing with restarts is enabled
v0.5.1,stop training if learning rate becomes too small
v0.5.1,process mini-batches
v0.5.1,zero the gradients on the model and optimizer
v0.5.1,"if necessary, make batch_steps"
v0.5.1,forward and backward for batch
v0.5.1,forward pass
v0.5.1,Backward
v0.5.1,do the optimizer step
v0.5.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5.1,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.5.1,evaluate on train / dev / test split depending on training settings
v0.5.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5.1,calculate scores using dev data if available
v0.5.1,append dev score to score history
v0.5.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5.1,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5.1,determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
v0.5.1,determine bad epoch number
v0.5.1,log bad epochs
v0.5.1,output log file
v0.5.1,make headers on first epoch
v0.5.1,"if checkpoint is enabled, save model at each epoch"
v0.5.1,"if we use dev data, remember best model based on dev evaluation score"
v0.5.1,"if we do not use dev data for model selection, save final model"
v0.5.1,test best model if test data is present
v0.5.1,"if we are training over multiple datasets, do evaluation for each"
v0.5.1,get and return the final test score of best model
v0.5.1,cast string to Path
v0.5.1,forward pass
v0.5.1,update optimizer and scheduler
v0.5.1,Add chars to the dictionary
v0.5.1,charsplit file content
v0.5.1,charsplit file content
v0.5.1,Add words to the dictionary
v0.5.1,Tokenize file content
v0.5.1,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.5.1,cast string to Path
v0.5.1,error message if the validation dataset is too small
v0.5.1,Shuffle training files randomly after serially iterating through corpus one
v0.5.1,"iterate through training data, starting at self.split (for checkpointing)"
v0.5.1,off by one for printing
v0.5.1,go into train mode
v0.5.1,reset variables
v0.5.1,not really sure what this does
v0.5.1,do the forward pass in the model
v0.5.1,try to predict the targets
v0.5.1,Backward
v0.5.1,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.5.1,We detach the hidden state from how it was previously produced.
v0.5.1,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.5.1,explicitly remove loss to clear up memory
v0.5.1,##############################################################################
v0.5.1,Save the model if the validation loss is the best we've seen so far.
v0.5.1,##############################################################################
v0.5.1,print info
v0.5.1,##############################################################################
v0.5.1,##############################################################################
v0.5.1,final testing
v0.5.1,##############################################################################
v0.5.1,Turn on evaluation mode which disables dropout.
v0.5.1,Work out how cleanly we can divide the dataset into bsz parts.
v0.5.1,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.5.1,Evenly divide the data across the bsz batches.
v0.5.1,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.5.1,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.5.1,news-english-forward
v0.5.1,news-english-backward
v0.5.1,news-english-forward
v0.5.1,news-english-backward
v0.5.1,mix-english-forward
v0.5.1,mix-english-backward
v0.5.1,mix-german-forward
v0.5.1,mix-german-backward
v0.5.1,common crawl Polish forward
v0.5.1,common crawl Polish backward
v0.5.1,Slovenian forward
v0.5.1,Slovenian backward
v0.5.1,Bulgarian forward
v0.5.1,Bulgarian backward
v0.5.1,Dutch forward
v0.5.1,Dutch backward
v0.5.1,Swedish forward
v0.5.1,Swedish backward
v0.5.1,French forward
v0.5.1,French backward
v0.5.1,Czech forward
v0.5.1,Czech backward
v0.5.1,Portuguese forward
v0.5.1,Portuguese backward
v0.5.1,initialize cache if use_cache set
v0.5.1,embed a dummy sentence to determine embedding_length
v0.5.1,set to eval mode
v0.5.1,Copy the object's state from self.__dict__ which contains
v0.5.1,all our instance attributes. Always use the dict.copy()
v0.5.1,method to avoid modifying the original state.
v0.5.1,Remove the unpicklable entries.
v0.5.1,"if cache is used, try setting embeddings from cache first"
v0.5.1,try populating embeddings from cache
v0.5.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.5.1,get hidden states from language model
v0.5.1,take first or last hidden states from language model as word representation
v0.5.1,if self.tokenized_lm or token.whitespace_after:
v0.5.1,1-camembert-base -> camembert-base
v0.5.1,1-xlm-roberta-large -> xlm-roberta-large
v0.5.1,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.5.1,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.5.1,tokens are attended to.
v0.5.1,Zero-pad up to the sequence length.
v0.5.1,"first, find longest sentence in batch"
v0.5.1,prepare id maps for BERT model
v0.5.1,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.5.1,get aggregated embeddings for each BERT-subtoken in sentence
v0.5.1,get the current sentence object
v0.5.1,add concatenated embedding to sentence
v0.5.1,use first subword embedding if pooling operation is 'first'
v0.5.1,"otherwise, do a mean over all subwords in token"
v0.5.1,"if only one sentence is passed, convert to list of sentence"
v0.5.1,bidirectional LSTM on top of embedding layer
v0.5.1,dropouts
v0.5.1,"first, sort sentences by number of tokens"
v0.5.1,go through each sentence in batch
v0.5.1,PADDING: pad shorter sentences out
v0.5.1,ADD TO SENTENCE LIST: add the representation
v0.5.1,--------------------------------------------------------------------
v0.5.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.5.1,--------------------------------------------------------------------
v0.5.1,--------------------------------------------------------------------
v0.5.1,FF PART
v0.5.1,--------------------------------------------------------------------
v0.5.1,use word dropout if set
v0.5.1,--------------------------------------------------------------------
v0.5.1,EXTRACT EMBEDDINGS FROM LSTM
v0.5.1,--------------------------------------------------------------------
v0.5.1,embed a dummy sentence to determine embedding_length
v0.5.1,Avoid conflicts with flair's Token class
v0.5.1,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.5.1,add positional encodings
v0.5.1,reshape the pixels into the sequence
v0.5.1,layer norm after convolution and positional encodings
v0.5.1,add <cls> token
v0.5.1,"transformer requires input in the shape [h*w+1, b, d]"
v0.5.1,the output is an embedding of <cls> token
v0.5.1,temporary fix to disable tokenizer parallelism warning
v0.5.1,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.5.1,load tokenizer and transformer model
v0.5.1,model name
v0.5.1,"when initializing, embeddings are in eval mode by default"
v0.5.1,embedding parameters
v0.5.1,send mini-token through to check how many layers the model has
v0.5.1,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.5.1,using list comprehension
v0.5.1,gradients are enabled if fine-tuning is enabled
v0.5.1,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.5.1,subtokenize sentences
v0.5.1,tokenize and truncate to 512 subtokens (TODO: check better truncation strategies)
v0.5.1,find longest sentence in batch
v0.5.1,initialize batch tensors and mask
v0.5.1,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.5.1,iterate over all subtokenized sentences
v0.5.1,use scalar mix of embeddings if so selected
v0.5.1,set the extracted embedding for the token
v0.5.1,reload tokenizer to get around serialization issues
v0.5.1,optional fine-tuning on top of embedding layer
v0.5.1,"if only one sentence is passed, convert to list of sentence"
v0.5.1,bidirectional RNN on top of embedding layer
v0.5.1,dropouts
v0.5.1,TODO: remove in future versions
v0.5.1,embed words in the sentence
v0.5.1,before-RNN dropout
v0.5.1,reproject if set
v0.5.1,push through RNN
v0.5.1,after-RNN dropout
v0.5.1,extract embeddings from RNN
v0.5.1,fixed RNN change format for torch 1.4.0
v0.5.1,IMPORTANT: add embeddings as torch modules
v0.5.1,iterate over sentences
v0.5.1,"if its a forward LM, take last state"
v0.5.1,"convert to plain strings, embedded in a list for the encode function"
v0.5.1,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.5.1,"if only one sentence is passed, convert to list of sentence"
v0.5.1,Expose base classses
v0.5.1,Expose token embedding classes
v0.5.1,Expose document embedding classes
v0.5.1,Expose image embedding classes
v0.5.1,Expose legacy embedding classes
v0.5.1,IMPORTANT: add embeddings as torch modules
v0.5.1,"if only one sentence is passed, convert to list of sentence"
v0.5.1,GLOVE embeddings
v0.5.1,TURIAN embeddings
v0.5.1,KOMNINOS embeddings
v0.5.1,FT-CRAWL embeddings
v0.5.1,FT-CRAWL embeddings
v0.5.1,twitter embeddings
v0.5.1,two-letter language code wiki embeddings
v0.5.1,two-letter language code wiki embeddings
v0.5.1,two-letter language code crawl embeddings
v0.5.1,fix serialized models
v0.5.1,use list of common characters if none provided
v0.5.1,translate words in sentence into ints using dictionary
v0.5.1,"sort words by length, for batching and masking"
v0.5.1,chars for rnn processing
v0.5.1,multilingual models
v0.5.1,English models
v0.5.1,Arabic
v0.5.1,Bulgarian
v0.5.1,Czech
v0.5.1,Danish
v0.5.1,German
v0.5.1,Spanish
v0.5.1,Basque
v0.5.1,Persian
v0.5.1,Finnish
v0.5.1,French
v0.5.1,Hebrew
v0.5.1,Hindi
v0.5.1,Croatian
v0.5.1,Indonesian
v0.5.1,Italian
v0.5.1,Japanese
v0.5.1,Malayalam
v0.5.1,Dutch
v0.5.1,Norwegian
v0.5.1,Polish
v0.5.1,Portuguese
v0.5.1,Pubmed
v0.5.1,Slovenian
v0.5.1,Swedish
v0.5.1,Tamil
v0.5.1,CLEF HIPE Shared task
v0.5.1,load model if in pretrained model map
v0.5.1,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.5.1,embeddings are static if we don't do finetuning
v0.5.1,embed a dummy sentence to determine embedding_length
v0.5.1,set to eval mode
v0.5.1,make compatible with serialized models (TODO: remove)
v0.5.1,make compatible with serialized models (TODO: remove)
v0.5.1,gradients are enable if fine-tuning is enabled
v0.5.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.5.1,get hidden states from language model
v0.5.1,take first or last hidden states from language model as word representation
v0.5.1,offset mode that extracts at whitespace after last character
v0.5.1,offset mode that extracts at last character
v0.5.1,only clone if optimization mode is 'gpu'
v0.5.1,use the character language model embeddings as basis
v0.5.1,length is twice the original character LM embedding length
v0.5.1,these fields are for the embedding memory
v0.5.1,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.5.1,we re-compute embeddings dynamically at each epoch
v0.5.1,set the memory method
v0.5.1,memory is wiped each time we do a training run
v0.5.1,"if we keep a pooling, it needs to be updated continuously"
v0.5.1,update embedding
v0.5.1,check token.text is empty or not
v0.5.1,set aggregation operation
v0.5.1,add embeddings after updating
v0.5.1,temporary fix to disable tokenizer parallelism warning
v0.5.1,(see https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning)
v0.5.1,load tokenizer and transformer model
v0.5.1,model name
v0.5.1,"when initializing, embeddings are in eval mode by default"
v0.5.1,embedding parameters
v0.5.1,send mini-token through to check how many layers the model has
v0.5.1,"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
v0.5.1,check if special tokens exist to circumvent error message
v0.5.1,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.5.1,split into micro batches of size self.batch_size before pushing through transformer
v0.5.1,embed each micro-batch
v0.5.1,remove special markup
v0.5.1,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.5.1,"TODO: keep for backwards compatibility, but remove in future"
v0.5.1,"some pretrained models do not have this property, applying default settings now."
v0.5.1,can be set manually after loading the model.
v0.5.1,method 1: subtokenize sentence
v0.5.1,"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
v0.5.1,method 2:
v0.5.1,transformer specific tokenization
v0.5.1,empty sentences get zero embeddings
v0.5.1,only embed non-empty sentences and if there is at least one
v0.5.1,find longest sentence in batch
v0.5.1,initialize batch tensors and mask
v0.5.1,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.5.1,make the tuple a tensor; makes working with it easier.
v0.5.1,gradients are enabled if fine-tuning is enabled
v0.5.1,iterate over all subtokenized sentences
v0.5.1,"remove stride_size//2 at end of sentence_hidden_state, and half at beginning of remainder,"
v0.5.1,in order to get some context into the embeddings of these words.
v0.5.1,also don't include the embedding of the extra [CLS] and [SEP] tokens.
v0.5.1,"for each token, get embedding"
v0.5.1,some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector
v0.5.1,"get states from all selected layers, aggregate with pooling operation"
v0.5.1,use scalar mix of embeddings if so selected
v0.5.1,sm_embeddings = self.mix(subtoken_embeddings)
v0.5.1,set the extracted embedding for the token
v0.5.1,iterate over subtokens and reconstruct tokens
v0.5.1,remove special markup
v0.5.1,TODO check if this is necessary is this method is called before prepare_for_model
v0.5.1,check if reconstructed token is special begin token ([CLS] or similar)
v0.5.1,some BERT tokenizers somehow omit words - in such cases skip to next token
v0.5.1,append subtoken to reconstruct token
v0.5.1,check if reconstructed token is the same as current token
v0.5.1,"if so, add subtoken count"
v0.5.1,reset subtoken count and reconstructed token
v0.5.1,break from loop if all tokens are accounted for
v0.5.1,if tokens are unaccounted for
v0.5.1,check if all tokens were matched to subtokens
v0.5.1,"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
v0.5.1,module should never be in training mode
v0.5.1,reload tokenizer to get around serialization issues
v0.5.1,max_tokens = 500
v0.5.1,model architecture
v0.5.1,model architecture
v0.5.1,download if necessary
v0.5.1,load the model
v0.5.1,"TODO: keep for backwards compatibility, but remove in future"
v0.5.1,save the sentence piece model as binary file (not as path which may change)
v0.5.1,write out the binary sentence piece model into the expected directory
v0.5.1,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.5.1,"otherwise, use normal process and potentially trigger another download"
v0.5.1,"once the modes if there, load it with sentence piece"
v0.5.1,empty words get no embedding
v0.5.1,all other words get embedded
v0.5.1,"the default model for ELMo is the 'original' model, which is very large"
v0.5.1,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.5.1,put on Cuda if available
v0.5.1,embed a dummy sentence to determine embedding_length
v0.5.1,ELMoEmbeddings before Release 0.5 did not set self.embedding_mode_fn
v0.5.1,GLOVE embeddings
v0.5.1,"find train, dev and test files if not specified"
v0.5.1,get train data
v0.5.1,read in test file if exists
v0.5.1,read in dev file if exists
v0.5.1,special key for space after
v0.5.1,"store either Sentence objects in memory, or only file offsets"
v0.5.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.5.1,determine encoding of text file
v0.5.1,skip first line if to selected
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,check if data there
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,check if data there
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,Remove CoNLL-U meta information in the last column
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,check if data there
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,rename according to train - test - dev - convention
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,data is not in IOB2 format. Thus we transform it to IOB2
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,column format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download data if necessary
v0.5.1,unpack and write out in CoNLL column-like format
v0.5.1,"find train, dev and test files if not specified"
v0.5.1,use test_file to create test split if available
v0.5.1,use dev_file to create test split if available
v0.5.1,"if data point contains black-listed label, do not use"
v0.5.1,first check if valid sentence
v0.5.1,"if so, add to indices"
v0.5.1,"find train, dev and test files if not specified"
v0.5.1,variables
v0.5.1,different handling of in_memory data than streaming data
v0.5.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.5.1,test if format is OK
v0.5.1,test if at least one label given
v0.5.1,noinspection PyDefaultArgument
v0.5.1,dataset name includes the split size
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download each of the 28 splits
v0.5.1,create dataset directory if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,create dataset directory if necessary
v0.5.1,create train.txt file from CSV
v0.5.1,create test.txt file from CSV
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,create dataset directory if necessary
v0.5.1,create train.txt file by iterating over pos and neg file
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,create dataset directory if necessary
v0.5.1,create train.txt file by iterating over pos and neg file
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,create dataset directory if necessary
v0.5.1,create train.txt file by iterating over pos and neg file
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,create dataset directory if necessary
v0.5.1,create train.txt file by iterating over pos and neg file
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,create train.txt file by iterating over pos and neg file
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,download senteval datasets if necessary und unzip
v0.5.1,convert to FastText format
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,Create flair compatible labels
v0.5.1,TREC-6 : NUM:dist -> __label__NUM
v0.5.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,Create flair compatible labels
v0.5.1,TREC-6 : NUM:dist -> __label__NUM
v0.5.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,check if dataset is supported
v0.5.1,set file names
v0.5.1,download and unzip in file structure if necessary
v0.5.1,instantiate corpus
v0.5.1,cache Feidegger config file
v0.5.1,cache Feidegger images
v0.5.1,replace image URL with local cached file
v0.5.1,append Sentence-Image data point
v0.5.1,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.5.1,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.5.1,"if so, num_workers is set to 0 for faster processing"
v0.5.1,cast to list if necessary
v0.5.1,cast to list if necessary
v0.5.1,"first, check if pymongo is installed"
v0.5.1,automatically identify train / test / dev files
v0.5.1,"if no test file is found, take any file with 'test' in name"
v0.5.1,Expose base classses
v0.5.1,Expose all sequence labeling datasets
v0.5.1,Expose all document classification datasets
v0.5.1,Expose all treebanks
v0.5.1,Expose all text-text datasets
v0.5.1,Expose all text-image datasets
v0.5.1,"find train, dev and test files if not specified"
v0.5.1,get train data
v0.5.1,get test data
v0.5.1,get dev data
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,this dataset name
v0.5.1,default dataset folder is the cache root
v0.5.1,download data if necessary
v0.5.1,skip because it is optional https://github.com/flairNLP/flair/pull/1296
v0.5.1,clean up file
v0.5.1,bioes tags
v0.5.1,bio tags
v0.5.1,broken tags
v0.5.1,all tags
v0.5.1,all weird tags
v0.5.1,tags with confidence
v0.5.1,bioes tags
v0.5.1,bioes tags
v0.5.1,clean up directory
v0.5.1,clean up directory
v0.5.1,initialize trainer
v0.5.1,clean up results directory
v0.5.1,initialize trainer
v0.5.1,clean up results directory
v0.5.1,initialize trainer
v0.5.1,clean up results directory
v0.5.1,initialize trainer
v0.5.1,clean up results directory
v0.5.1,initialize trainer
v0.5.1,clean up results directory
v0.5.1,clean up results directory
v0.5.1,initialize trainer
v0.5.1,clean up results directory
v0.5.1,def test_multiclass_metrics():
v0.5.1,
v0.5.1,"metric = Metric(""Test"")"
v0.5.1,"available_labels = [""A"", ""B"", ""C""]"
v0.5.1,
v0.5.1,"predictions = [""A"", ""B""]"
v0.5.1,"true_values = [""A""]"
v0.5.1,TextClassifier._evaluate_sentence_for_text_classification(
v0.5.1,"metric, available_labels, predictions, true_values"
v0.5.1,)
v0.5.1,
v0.5.1,"predictions = [""C"", ""B""]"
v0.5.1,"true_values = [""A"", ""B""]"
v0.5.1,TextClassifier._evaluate_sentence_for_text_classification(
v0.5.1,"metric, available_labels, predictions, true_values"
v0.5.1,)
v0.5.1,
v0.5.1,print(metric)
v0.5.1,from flair.trainers.trainer_regression import RegressorTrainer
v0.5.1,def test_trainer_results(tasks_base_path):
v0.5.1,"corpus, model, trainer = init(tasks_base_path)"
v0.5.1,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.5.1,"assert results[""test_score""] > 0"
v0.5.1,"assert len(results[""dev_loss_history""]) == 1"
v0.5.1,"assert len(results[""dev_score_history""]) == 1"
v0.5.1,"assert len(results[""train_loss_history""]) == 1"
v0.5.1,get default dictionary
v0.5.1,init forward LM with 128 hidden states and 1 layer
v0.5.1,get the example corpus and process at character level in forward direction
v0.5.1,train the language model
v0.5.1,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.5.1,clean up results directory
v0.5.1,get default dictionary
v0.5.1,init forward LM with 128 hidden states and 1 layer
v0.5.1,get the example corpus and process at character level in forward direction
v0.5.1,train the language model
v0.5.1,clean up results directory
v0.5.1,define search space
v0.5.1,sequence tagger parameter
v0.5.1,model trainer parameter
v0.5.1,training parameter
v0.5.1,find best parameter settings
v0.5.1,clean up results directory
v0.5.1,document embeddings parameter
v0.5.1,training parameter
v0.5.1,clean up results directory
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,"get training, test and dev data"
v0.5.1,get two corpora as one
v0.5.1,"get training, test and dev data for full English UD corpus from web"
v0.5.1,clean up data directory
v0.5.1,clean up results directory
v0.5.1,clean up results directory
v0.5.1,clean up results directory
v0.5.1,clean up results directory
v0.5.1,clean up results directory
v0.5.1,clean up results directory
v0.5,1. get the corpus
v0.5,2. what tag do we want to predict?
v0.5,3. make the tag dictionary from the corpus
v0.5,initialize embeddings
v0.5,comment in this line to use character embeddings
v0.5,"CharacterEmbeddings(),"
v0.5,comment in these lines to use contextual string embeddings
v0.5,
v0.5,"FlairEmbeddings('news-forward'),"
v0.5,
v0.5,"FlairEmbeddings('news-backward'),"
v0.5,initialize sequence tagger
v0.5,initialize trainer
v0.5,from allennlp.common.tqdm import Tqdm
v0.5,mmap seems to be much more memory efficient
v0.5,Remove quotes from etag
v0.5,"If there is an etag, it's everything after the first period"
v0.5,"Otherwise, use None"
v0.5,"URL, so get it from the cache (downloading if necessary)"
v0.5,"File, and it exists."
v0.5,"File, but it doesn't exist."
v0.5,Something unknown
v0.5,Extract all the contents of zip file in current directory
v0.5,get cache path to put the file
v0.5,"Download to temporary file, then copy to cache dir once finished."
v0.5,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.5,GET file object
v0.5,TODO(joelgrus): do we want to do checksums or anything like that?
v0.5,get cache path to put the file
v0.5,make HEAD request to check ETag
v0.5,add ETag to filename if it exists
v0.5,"etag = response.headers.get(""ETag"")"
v0.5,"Download to temporary file, then copy to cache dir once finished."
v0.5,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.5,GET file object
v0.5,These defaults are the same as the argument defaults in tqdm.
v0.5,first determine the distribution of classes in the dataset
v0.5,weight for each sample
v0.5,Create blocks
v0.5,shuffle the blocks
v0.5,concatenate the shuffled blocks
v0.5,Create blocks
v0.5,shuffle the blocks
v0.5,concatenate the shuffled blocks
v0.5,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.5,see https://github.com/zalandoresearch/flair/issues/351
v0.5,State initialization
v0.5,Exponential moving average of gradient values
v0.5,Exponential moving average of squared gradient values
v0.5,Maintains max of all exp. moving avg. of sq. grad. values
v0.5,Decay the first and second moment running average coefficient
v0.5,Maintains the maximum of all 2nd moment running avg. till now
v0.5,Use the max. for normalizing running avg. of gradient
v0.5,conll 2000 column format
v0.5,conll 03 NER column format
v0.5,WNUT-17
v0.5,-- WikiNER datasets
v0.5,-- Universal Dependencies
v0.5,Germanic
v0.5,Romance
v0.5,West-Slavic
v0.5,South-Slavic
v0.5,East-Slavic
v0.5,Scandinavian
v0.5,Asian
v0.5,Language isolates
v0.5,recent Universal Dependencies
v0.5,other datasets
v0.5,text classification format
v0.5,text regression format
v0.5,"first, try to fetch dataset online"
v0.5,default dataset folder is the cache root
v0.5,get string value if enum is passed
v0.5,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.5,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.5,the CoNLL 03 task for German has an additional lemma column
v0.5,the CoNLL 03 task for Dutch has no NP column
v0.5,the CoNLL 03 task for Spanish only has two columns
v0.5,the GERMEVAL task only has two columns: text and ner
v0.5,WSD tasks may be put into this column format
v0.5,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.5,"for text classifiers, we use our own special format"
v0.5,NER corpus for Basque
v0.5,automatically identify train / test / dev files
v0.5,"if no test file is found, take any file with 'test' in name"
v0.5,get train and test data
v0.5,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.5,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.5,convert tag scheme to iobes
v0.5,automatically identify train / test / dev files
v0.5,automatically identify train / test / dev files
v0.5,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.5,conll 2000 chunking task
v0.5,Support both TREC-6 and TREC-50
v0.5,Create flair compatible labels
v0.5,TREC-6 : NUM:dist -> __label__NUM
v0.5,TREC-50: NUM:dist -> __label__NUM:dist
v0.5,Wikiner NER task
v0.5,unpack and write out in CoNLL column-like format
v0.5,CoNLL 02/03 NER
v0.5,universal dependencies
v0.5,--- UD Germanic
v0.5,--- UD Romance
v0.5,--- UD West-Slavic
v0.5,--- UD Scandinavian
v0.5,--- UD South-Slavic
v0.5,--- UD Asian
v0.5,this is the default init size of a lmdb database for embeddings
v0.5,some non-used parameter to allow print
v0.5,get db filename from embedding name
v0.5,"In case initialization of cached version failed, just fallback to the original WordEmbeddings"
v0.5,SequenceTagger
v0.5,TextClassifier
v0.5,get db filename from embedding name
v0.5,if embedding database already exists
v0.5,"otherwise, push embedding to database"
v0.5,if embedding database already exists
v0.5,open the database in read mode
v0.5,we need to set self.k
v0.5,create and load the database in write mode
v0.5,"no idea why, but we need to close and reopen the environment to avoid"
v0.5,mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
v0.5,when opening new transaction !
v0.5,init dictionaries
v0.5,"in order to deal with unknown tokens, add <unk>"
v0.5,"if text is passed, instantiate sentence with tokens (words)"
v0.5,log a warning if the dataset is empty
v0.5,data with zero-width characters cannot be handled
v0.5,set token idx if not set
v0.5,non-set tags are OUT tags
v0.5,anything that is not a BIOES tag is a SINGLE tag
v0.5,anything that is not OUT is IN
v0.5,single and begin tags start a new span
v0.5,remember previous tag
v0.5,move sentence embeddings to device
v0.5,move token embeddings to device
v0.5,clear sentence embeddings
v0.5,clear token embeddings
v0.5,infer whitespace after field
v0.5,add Sentence labels to output if they exist
v0.5,add Token labels to output if they exist
v0.5,add Sentence labels to output if they exist
v0.5,add Token labels to output if they exist
v0.5,No character at the corresponding code point: remove it
v0.5,set name
v0.5,sample test data if none is provided
v0.5,sample dev data if none is provided
v0.5,set train dev and test data
v0.5,find out empty sentence indices
v0.5,create subset of non-empty sentence indices
v0.5,check if sentence itself has labels
v0.5,check for labels of words
v0.5,Make the tag dictionary
v0.5,increment for last token in sentence if not followed by whitespace
v0.5,determine offsets for whitespace_after field
v0.5,determine offsets for whitespace_after field
v0.5,global variable: cache_root
v0.5,global variable: device
v0.5,global variable: embedding_storage_mode
v0.5,# dummy return to fulfill trainer.train() needs
v0.5,Attach optimizer
v0.5,"convert `metrics` to float, in case it's a zero-dim Tensor"
v0.5,if memory mode option 'none' delete everything
v0.5,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.5,find out which ones are dynamic embeddings
v0.5,find out which ones are dynamic embeddings
v0.5,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.5,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.5,header for 'weights.txt'
v0.5,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.5,then get all relevant values from the tsv
v0.5,then get all relevant values from the tsv
v0.5,print(rows)
v0.5,"figsize = (16, 16)"
v0.5,plot i
v0.5,save plots
v0.5,save plots
v0.5,plt.show()
v0.5,save plot
v0.5,take the average over the last three scores of training
v0.5,take average over the scores from the different training runs
v0.5,remove previous embeddings
v0.5,clearing token embeddings to save memory
v0.5,#TODO: not saving lines yet
v0.5,== similarity measures ==
v0.5,helper class for ModelSimilarity
v0.5,-- works with binary cross entropy loss --
v0.5,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.5,-- works with ranking/triplet loss --
v0.5,normalize the embeddings
v0.5,== similarity losses ==
v0.5,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.5,TODO: this assumes eye matrix
v0.5,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.5,== similarity learner ==
v0.5,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.5,assumes that for each data pair there's at least one embedding per modality
v0.5,pre-compute embeddings for all targets in evaluation dataset
v0.5,compute the similarity
v0.5,sort the similarity matrix across modality 1
v0.5,"get the ranks, so +1 to start counting ranks from 1"
v0.5,The conversion from old model's constructor interface
v0.5,auto-spawn on GPU if available
v0.5,pad strings with whitespaces to longest sentence
v0.5,cut up the input into chunks of max charlength = chunk_size
v0.5,push each chunk through the RNN language model
v0.5,concatenate all chunks to make final output
v0.5,initial hidden state
v0.5,get predicted weights
v0.5,divide by temperature
v0.5,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.5,this makes a vector in which the largest value is 0
v0.5,compute word weights with exponential function
v0.5,try sampling multinomial distribution for next character
v0.5,print(word_idx)
v0.5,input ids
v0.5,push list of character IDs through model
v0.5,the target is always the next character
v0.5,use cross entropy loss to compare output of forward pass with targets
v0.5,exponentiate cross-entropy loss to calculate perplexity
v0.5,fixed RNN change format for torch 1.4.0
v0.5,set the dictionaries
v0.5,"if we use a CRF, we must add special START and STOP tags to the dictionary"
v0.5,Initialize the weight tensor
v0.5,initialize the network architecture
v0.5,dropouts
v0.5,"if no dimensionality for reprojection layer is set, reproject to equal dimension"
v0.5,bidirectional LSTM on top of embedding layer
v0.5,Create initial hidden state and initialize it
v0.5,TODO: Decide how to initialize the hidden state variables
v0.5,self.hs_initializer(self.lstm_init_h)
v0.5,self.hs_initializer(self.lstm_init_c)
v0.5,final linear map to tag space
v0.5,reverse sort all sequences by their length
v0.5,remove previous embeddings
v0.5,progress bar for verbosity
v0.5,stop if all sentences are empty
v0.5,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.5,clearing token embeddings to save memory
v0.5,append both to file for evaluation
v0.5,make list of gold tags
v0.5,make list of predicted tags
v0.5,"check for true positives, false positives and false negatives"
v0.5,--------------------------------------------------------------------
v0.5,FF PART
v0.5,--------------------------------------------------------------------
v0.5,"if initial hidden state is trainable, use this state"
v0.5,word dropout only before LSTM - TODO: more experimentation needed
v0.5,if self.use_word_dropout > 0.0:
v0.5,sentence_tensor = self.word_dropout(sentence_tensor)
v0.5,get the tags in this sentence
v0.5,add tags as tensor
v0.5,pad tags if using batch-CRF decoder
v0.5,reduce raw values to avoid NaN during exp
v0.5,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.5,default value
v0.5,the historical German taggers by the @redewiegergabe project
v0.5,Initialize the weight tensor
v0.5,auto-spawn on GPU if available
v0.5,filter empty sentences
v0.5,reverse sort all sequences by their length
v0.5,remove previous embeddings
v0.5,progress bar for verbosity
v0.5,stop if all sentences are empty
v0.5,clearing token embeddings to save memory
v0.5,English sentiment models
v0.5,cast string to Path
v0.5,"determine what splits (train, dev, test) to evaluate and log"
v0.5,prepare loss logging file and set up header
v0.5,"minimize training loss if training with dev data, else maximize dev score"
v0.5,"if training also uses dev data, include in training set"
v0.5,initialize sampler if provided
v0.5,init with default values if only class is provided
v0.5,set dataset to sample from
v0.5,At any point you can hit Ctrl + C to break out of training early.
v0.5,get new learning rate
v0.5,reload last best model if annealing with restarts is enabled
v0.5,stop training if learning rate becomes too small
v0.5,process mini-batches
v0.5,zero the gradients on the model and optimizer
v0.5,"if necessary, make batch_steps"
v0.5,forward and backward for batch
v0.5,forward pass
v0.5,Backward
v0.5,do the optimizer step
v0.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.5,evaluate on train / dev / test split depending on training settings
v0.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5,calculate scores using dev data if available
v0.5,append dev score to score history
v0.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.5,determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau
v0.5,determine bad epoch number
v0.5,log bad epochs
v0.5,output log file
v0.5,make headers on first epoch
v0.5,"if checkpoint is enabled, save model at each epoch"
v0.5,"if we use dev data, remember best model based on dev evaluation score"
v0.5,"if we do not use dev data for model selection, save final model"
v0.5,test best model if test data is present
v0.5,"if we are training over multiple datasets, do evaluation for each"
v0.5,get and return the final test score of best model
v0.5,cast string to Path
v0.5,forward pass
v0.5,update optimizer and scheduler
v0.5,Add chars to the dictionary
v0.5,charsplit file content
v0.5,charsplit file content
v0.5,Add words to the dictionary
v0.5,Tokenize file content
v0.5,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.5,cast string to Path
v0.5,error message if the validation dataset is too small
v0.5,Shuffle training files randomly after serially iterating through corpus one
v0.5,"iterate through training data, starting at self.split (for checkpointing)"
v0.5,off by one for printing
v0.5,go into train mode
v0.5,reset variables
v0.5,not really sure what this does
v0.5,do the forward pass in the model
v0.5,try to predict the targets
v0.5,Backward
v0.5,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.5,We detach the hidden state from how it was previously produced.
v0.5,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.5,explicitly remove loss to clear up memory
v0.5,##############################################################################
v0.5,Save the model if the validation loss is the best we've seen so far.
v0.5,##############################################################################
v0.5,print info
v0.5,##############################################################################
v0.5,##############################################################################
v0.5,final testing
v0.5,##############################################################################
v0.5,Turn on evaluation mode which disables dropout.
v0.5,Work out how cleanly we can divide the dataset into bsz parts.
v0.5,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.5,Evenly divide the data across the bsz batches.
v0.5,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.5,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.5,news-english-forward
v0.5,news-english-backward
v0.5,news-english-forward
v0.5,news-english-backward
v0.5,mix-english-forward
v0.5,mix-english-backward
v0.5,mix-german-forward
v0.5,mix-german-backward
v0.5,common crawl Polish forward
v0.5,common crawl Polish backward
v0.5,Slovenian forward
v0.5,Slovenian backward
v0.5,Bulgarian forward
v0.5,Bulgarian backward
v0.5,Dutch forward
v0.5,Dutch backward
v0.5,Swedish forward
v0.5,Swedish backward
v0.5,French forward
v0.5,French backward
v0.5,Czech forward
v0.5,Czech backward
v0.5,Portuguese forward
v0.5,Portuguese backward
v0.5,initialize cache if use_cache set
v0.5,embed a dummy sentence to determine embedding_length
v0.5,set to eval mode
v0.5,Copy the object's state from self.__dict__ which contains
v0.5,all our instance attributes. Always use the dict.copy()
v0.5,method to avoid modifying the original state.
v0.5,Remove the unpicklable entries.
v0.5,"if cache is used, try setting embeddings from cache first"
v0.5,try populating embeddings from cache
v0.5,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.5,get hidden states from language model
v0.5,take first or last hidden states from language model as word representation
v0.5,if self.tokenized_lm or token.whitespace_after:
v0.5,1-camembert-base -> camembert-base
v0.5,1-xlm-roberta-large -> xlm-roberta-large
v0.5,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.5,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.5,tokens are attended to.
v0.5,Zero-pad up to the sequence length.
v0.5,"first, find longest sentence in batch"
v0.5,prepare id maps for BERT model
v0.5,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.5,get aggregated embeddings for each BERT-subtoken in sentence
v0.5,get the current sentence object
v0.5,add concatenated embedding to sentence
v0.5,use first subword embedding if pooling operation is 'first'
v0.5,"otherwise, do a mean over all subwords in token"
v0.5,"if only one sentence is passed, convert to list of sentence"
v0.5,bidirectional LSTM on top of embedding layer
v0.5,dropouts
v0.5,"first, sort sentences by number of tokens"
v0.5,go through each sentence in batch
v0.5,PADDING: pad shorter sentences out
v0.5,ADD TO SENTENCE LIST: add the representation
v0.5,--------------------------------------------------------------------
v0.5,GET REPRESENTATION FOR ENTIRE BATCH
v0.5,--------------------------------------------------------------------
v0.5,--------------------------------------------------------------------
v0.5,FF PART
v0.5,--------------------------------------------------------------------
v0.5,use word dropout if set
v0.5,--------------------------------------------------------------------
v0.5,EXTRACT EMBEDDINGS FROM LSTM
v0.5,--------------------------------------------------------------------
v0.5,embed a dummy sentence to determine embedding_length
v0.5,Avoid conflicts with flair's Token class
v0.5,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.5,add positional encodings
v0.5,reshape the pixels into the sequence
v0.5,layer norm after convolution and positional encodings
v0.5,add <cls> token
v0.5,"transformer requires input in the shape [h*w+1, b, d]"
v0.5,the output is an embedding of <cls> token
v0.5,load tokenizer and transformer model
v0.5,model name
v0.5,"when initializing, embeddings are in eval mode by default"
v0.5,embedding parameters
v0.5,send mini-token through to check how many layers the model has
v0.5,"most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial"
v0.5,using list comprehension
v0.5,gradients are enabled if fine-tuning is enabled
v0.5,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.5,subtokenize sentences
v0.5,tokenize and truncate to 512 subtokens (TODO: check better truncation strategies)
v0.5,find longest sentence in batch
v0.5,initialize batch tensors and mask
v0.5,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.5,iterate over all subtokenized sentences
v0.5,use scalar mix of embeddings if so selected
v0.5,set the extracted embedding for the token
v0.5,reload tokenizer to get around serialization issues
v0.5,optional fine-tuning on top of embedding layer
v0.5,"if only one sentence is passed, convert to list of sentence"
v0.5,bidirectional RNN on top of embedding layer
v0.5,dropouts
v0.5,TODO: remove in future versions
v0.5,embed words in the sentence
v0.5,before-RNN dropout
v0.5,reproject if set
v0.5,push through RNN
v0.5,after-RNN dropout
v0.5,extract embeddings from RNN
v0.5,fixed RNN change format for torch 1.4.0
v0.5,IMPORTANT: add embeddings as torch modules
v0.5,iterate over sentences
v0.5,"if its a forward LM, take last state"
v0.5,"if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency"
v0.5,"if only one sentence is passed, convert to list of sentence"
v0.5,Expose base classses
v0.5,Expose token embedding classes
v0.5,Expose document embedding classes
v0.5,Expose image embedding classes
v0.5,Expose legacy embedding classes
v0.5,IMPORTANT: add embeddings as torch modules
v0.5,"if only one sentence is passed, convert to list of sentence"
v0.5,GLOVE embeddings
v0.5,TURIAN embeddings
v0.5,KOMNINOS embeddings
v0.5,FT-CRAWL embeddings
v0.5,FT-CRAWL embeddings
v0.5,twitter embeddings
v0.5,two-letter language code wiki embeddings
v0.5,two-letter language code wiki embeddings
v0.5,two-letter language code crawl embeddings
v0.5,fix serialized models
v0.5,use list of common characters if none provided
v0.5,translate words in sentence into ints using dictionary
v0.5,"sort words by length, for batching and masking"
v0.5,chars for rnn processing
v0.5,multilingual models
v0.5,English models
v0.5,Arabic
v0.5,Bulgarian
v0.5,Czech
v0.5,Danish
v0.5,German
v0.5,Spanish
v0.5,Basque
v0.5,Persian
v0.5,Finnish
v0.5,French
v0.5,Hebrew
v0.5,Hindi
v0.5,Croatian
v0.5,Indonesian
v0.5,Italian
v0.5,Japanese
v0.5,Malayalam
v0.5,Dutch
v0.5,Norwegian
v0.5,Polish
v0.5,Portuguese
v0.5,Pubmed
v0.5,Slovenian
v0.5,Swedish
v0.5,Tamil
v0.5,CLEF HIPE Shared task
v0.5,load model if in pretrained model map
v0.5,Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)
v0.5,embeddings are static if we don't do finetuning
v0.5,embed a dummy sentence to determine embedding_length
v0.5,set to eval mode
v0.5,make compatible with serialized models (TODO: remove)
v0.5,make compatible with serialized models (TODO: remove)
v0.5,gradients are enable if fine-tuning is enabled
v0.5,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.5,get hidden states from language model
v0.5,take first or last hidden states from language model as word representation
v0.5,offset mode that extracts at whitespace after last character
v0.5,offset mode that extracts at last character
v0.5,only clone if optimization mode is 'gpu'
v0.5,use the character language model embeddings as basis
v0.5,length is twice the original character LM embedding length
v0.5,these fields are for the embedding memory
v0.5,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.5,we re-compute embeddings dynamically at each epoch
v0.5,set the memory method
v0.5,memory is wiped each time we do a training run
v0.5,"if we keep a pooling, it needs to be updated continuously"
v0.5,update embedding
v0.5,check token.text is empty or not
v0.5,add embeddings after updating
v0.5,load tokenizer and transformer model
v0.5,model name
v0.5,"when initializing, embeddings are in eval mode by default"
v0.5,embedding parameters
v0.5,send mini-token through to check how many layers the model has
v0.5,"self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)"
v0.5,check if special tokens exist to circumvent error message
v0.5,"most models have an intial BOS token, except for XLNet, T5 and GPT2"
v0.5,split into micro batches of size self.batch_size before pushing through transformer
v0.5,embed each micro-batch
v0.5,remove special markup
v0.5,"first, subtokenize each sentence and find out into how many subtokens each token was divided"
v0.5,method 1: subtokenize sentence
v0.5,"subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)"
v0.5,method 2:
v0.5,print(subtokens)
v0.5,iterate over subtokens and reconstruct tokens
v0.5,remove special markup
v0.5,append subtoken to reconstruct token
v0.5,check if reconstructed token is special begin token ([CLS] or similar)
v0.5,check if reconstructed token is the same as current token
v0.5,"if so, add subtoken count"
v0.5,reset subtoken count and reconstructed token
v0.5,break from loop if all tokens are accounted for
v0.5,check if all tokens were matched to subtokens
v0.5,find longest sentence in batch
v0.5,initialize batch tensors and mask
v0.5,put encoded batch through transformer model to get all hidden states of all encoder layers
v0.5,gradients are enabled if fine-tuning is enabled
v0.5,iterate over all subtokenized sentences
v0.5,"for each token, get embedding"
v0.5,"get states from all selected layers, aggregate with pooling operation"
v0.5,use scalar mix of embeddings if so selected
v0.5,sm_embeddings = self.mix(subtoken_embeddings)
v0.5,set the extracted embedding for the token
v0.5,"if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this"
v0.5,module should never be in training mode
v0.5,reload tokenizer to get around serialization issues
v0.5,max_tokens = 500
v0.5,model architecture
v0.5,model architecture
v0.5,download if necessary
v0.5,load the model
v0.5,"TODO: keep for backwards compatibility, but remove in future"
v0.5,save the sentence piece model as binary file (not as path which may change)
v0.5,write out the binary sentence piece model into the expected directory
v0.5,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.5,"otherwise, use normal process and potentially trigger another download"
v0.5,"once the modes if there, load it with sentence piece"
v0.5,empty words get no embedding
v0.5,all other words get embedded
v0.5,"the default model for ELMo is the 'original' model, which is very large"
v0.5,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.5,put on Cuda if available
v0.5,embed a dummy sentence to determine embedding_length
v0.5,GLOVE embeddings
v0.5,"find train, dev and test files if not specified"
v0.5,get train data
v0.5,read in test file if exists
v0.5,read in dev file if exists
v0.5,special key for space after
v0.5,"store either Sentence objects in memory, or only file offsets"
v0.5,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.5,determine encoding of text file
v0.5,skip first line if to selected
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,check if data there
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,check if data there
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,Remove CoNLL-U meta information in the last column
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,check if data there
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,column format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download data if necessary
v0.5,unpack and write out in CoNLL column-like format
v0.5,"find train, dev and test files if not specified"
v0.5,use test_file to create test split if available
v0.5,use dev_file to create test split if available
v0.5,"if data point contains black-listed label, do not use"
v0.5,first check if valid sentence
v0.5,"if so, add to indices"
v0.5,"find train, dev and test files if not specified"
v0.5,variables
v0.5,different handling of in_memory data than streaming data
v0.5,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.5,test if format is OK
v0.5,test if at least one label given
v0.5,noinspection PyDefaultArgument
v0.5,dataset name includes the split size
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download each of the 28 splits
v0.5,create dataset directory if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,"by defaut, map point score to POSITIVE / NEGATIVE values"
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,create dataset directory if necessary
v0.5,create train.txt file from CSV
v0.5,create test.txt file from CSV
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,create dataset directory if necessary
v0.5,create train.txt file by iterating over pos and neg file
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,create dataset directory if necessary
v0.5,create train.txt file by iterating over pos and neg file
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,create dataset directory if necessary
v0.5,create train.txt file by iterating over pos and neg file
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,create dataset directory if necessary
v0.5,create train.txt file by iterating over pos and neg file
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,create train.txt file by iterating over pos and neg file
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,download senteval datasets if necessary und unzip
v0.5,convert to FastText format
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,Create flair compatible labels
v0.5,TREC-6 : NUM:dist -> __label__NUM
v0.5,TREC-50: NUM:dist -> __label__NUM:dist
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,Create flair compatible labels
v0.5,TREC-6 : NUM:dist -> __label__NUM
v0.5,TREC-50: NUM:dist -> __label__NUM:dist
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,check if dataset is supported
v0.5,set file names
v0.5,download and unzip in file structure if necessary
v0.5,instantiate corpus
v0.5,cache Feidegger config file
v0.5,cache Feidegger images
v0.5,replace image URL with local cached file
v0.5,append Sentence-Image data point
v0.5,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.5,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.5,"if so, num_workers is set to 0 for faster processing"
v0.5,cast to list if necessary
v0.5,cast to list if necessary
v0.5,"first, check if pymongo is installed"
v0.5,automatically identify train / test / dev files
v0.5,"if no test file is found, take any file with 'test' in name"
v0.5,Expose base classses
v0.5,Expose all sequence labeling datasets
v0.5,Expose all document classification datasets
v0.5,Expose all treebanks
v0.5,Expose all text-text datasets
v0.5,Expose all text-image datasets
v0.5,"find train, dev and test files if not specified"
v0.5,get train data
v0.5,get test data
v0.5,get dev data
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,this dataset name
v0.5,default dataset folder is the cache root
v0.5,download data if necessary
v0.5,skip because it is optional https://github.com/flairNLP/flair/pull/1296
v0.5,def test_create_sentence_using_japanese_tokenizer():
v0.5,"sentence: Sentence = Sentence("""", use_tokenizer=build_japanese_tokenizer())"
v0.5,
v0.5,assert 5 == len(sentence.tokens)
v0.5,"assert """" == sentence.tokens[0].text"
v0.5,"assert """" == sentence.tokens[1].text"
v0.5,"assert """" == sentence.tokens[2].text"
v0.5,"assert """" == sentence.tokens[3].text"
v0.5,"assert """" == sentence.tokens[4].text"
v0.5,clean up file
v0.5,bioes tags
v0.5,bio tags
v0.5,broken tags
v0.5,all tags
v0.5,all weird tags
v0.5,tags with confidence
v0.5,bioes tags
v0.5,bioes tags
v0.5,clean up directory
v0.5,clean up directory
v0.5,initialize trainer
v0.5,clean up results directory
v0.5,initialize trainer
v0.5,clean up results directory
v0.5,initialize trainer
v0.5,clean up results directory
v0.5,initialize trainer
v0.5,clean up results directory
v0.5,initialize trainer
v0.5,clean up results directory
v0.5,clean up results directory
v0.5,initialize trainer
v0.5,clean up results directory
v0.5,def test_multiclass_metrics():
v0.5,
v0.5,"metric = Metric(""Test"")"
v0.5,"available_labels = [""A"", ""B"", ""C""]"
v0.5,
v0.5,"predictions = [""A"", ""B""]"
v0.5,"true_values = [""A""]"
v0.5,TextClassifier._evaluate_sentence_for_text_classification(
v0.5,"metric, available_labels, predictions, true_values"
v0.5,)
v0.5,
v0.5,"predictions = [""C"", ""B""]"
v0.5,"true_values = [""A"", ""B""]"
v0.5,TextClassifier._evaluate_sentence_for_text_classification(
v0.5,"metric, available_labels, predictions, true_values"
v0.5,)
v0.5,
v0.5,print(metric)
v0.5,from flair.trainers.trainer_regression import RegressorTrainer
v0.5,def test_trainer_results(tasks_base_path):
v0.5,"corpus, model, trainer = init(tasks_base_path)"
v0.5,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.5,"assert results[""test_score""] > 0"
v0.5,"assert len(results[""dev_loss_history""]) == 1"
v0.5,"assert len(results[""dev_score_history""]) == 1"
v0.5,"assert len(results[""train_loss_history""]) == 1"
v0.5,get default dictionary
v0.5,init forward LM with 128 hidden states and 1 layer
v0.5,get the example corpus and process at character level in forward direction
v0.5,train the language model
v0.5,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.5,clean up results directory
v0.5,get default dictionary
v0.5,init forward LM with 128 hidden states and 1 layer
v0.5,get the example corpus and process at character level in forward direction
v0.5,train the language model
v0.5,clean up results directory
v0.5,define search space
v0.5,sequence tagger parameter
v0.5,model trainer parameter
v0.5,training parameter
v0.5,find best parameter settings
v0.5,clean up results directory
v0.5,document embeddings parameter
v0.5,training parameter
v0.5,clean up results directory
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,"get training, test and dev data"
v0.5,get two corpora as one
v0.5,"get training, test and dev data for full English UD corpus from web"
v0.5,clean up data directory
v0.5,clean up results directory
v0.5,clean up results directory
v0.5,clean up results directory
v0.5,clean up results directory
v0.5,clean up results directory
v0.5,clean up results directory
v0.4.5,1. get the corpus
v0.4.5,2. what tag do we want to predict?
v0.4.5,3. make the tag dictionary from the corpus
v0.4.5,initialize embeddings
v0.4.5,comment in this line to use character embeddings
v0.4.5,"CharacterEmbeddings(),"
v0.4.5,comment in these lines to use contextual string embeddings
v0.4.5,
v0.4.5,"FlairEmbeddings('news-forward'),"
v0.4.5,
v0.4.5,"FlairEmbeddings('news-backward'),"
v0.4.5,initialize sequence tagger
v0.4.5,initialize trainer
v0.4.5,"if only one sentence is passed, convert to list of sentence"
v0.4.5,IMPORTANT: add embeddings as torch modules
v0.4.5,"if only one sentence is passed, convert to list of sentence"
v0.4.5,GLOVE embeddings
v0.4.5,TURIAN embeddings
v0.4.5,KOMNINOS embeddings
v0.4.5,FT-CRAWL embeddings
v0.4.5,FT-CRAWL embeddings
v0.4.5,twitter embeddings
v0.4.5,two-letter language code wiki embeddings
v0.4.5,two-letter language code wiki embeddings
v0.4.5,two-letter language code crawl embeddings
v0.4.5,fix serialized models
v0.4.5,max_tokens = 500
v0.4.5,model architecture
v0.4.5,model architecture
v0.4.5,download if necessary
v0.4.5,load the model
v0.4.5,empty words get no embedding
v0.4.5,all other words get embedded
v0.4.5,"the default model for ELMo is the 'original' model, which is very large"
v0.4.5,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.4.5,put on Cuda if available
v0.4.5,embed a dummy sentence to determine embedding_length
v0.4.5,embed a dummy sentence to determine embedding_length
v0.4.5,Avoid conflicts with flair's Token class
v0.4.5,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.4.5,1-camembert-base -> camembert-base
v0.4.5,1-xlm-roberta-large -> xlm-roberta-large
v0.4.5,use list of common characters if none provided
v0.4.5,translate words in sentence into ints using dictionary
v0.4.5,"sort words by length, for batching and masking"
v0.4.5,chars for rnn processing
v0.4.5,multilingual models
v0.4.5,English models
v0.4.5,Arabic
v0.4.5,Bulgarian
v0.4.5,Czech
v0.4.5,Danish
v0.4.5,German
v0.4.5,Spanish
v0.4.5,Basque
v0.4.5,Persian
v0.4.5,Finnish
v0.4.5,French
v0.4.5,Hebrew
v0.4.5,Hindi
v0.4.5,Croatian
v0.4.5,Indonesian
v0.4.5,Italian
v0.4.5,Japanese
v0.4.5,Dutch
v0.4.5,Norwegian
v0.4.5,Polish
v0.4.5,Portuguese
v0.4.5,Pubmed
v0.4.5,Slovenian
v0.4.5,Swedish
v0.4.5,Tamil
v0.4.5,load model if in pretrained model map
v0.4.5,embeddings are static if we don't do finetuning
v0.4.5,embed a dummy sentence to determine embedding_length
v0.4.5,set to eval mode
v0.4.5,make compatible with serialized models (TODO: remove)
v0.4.5,gradients are enable if fine-tuning is enabled
v0.4.5,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.5,get hidden states from language model
v0.4.5,take first or last hidden states from language model as word representation
v0.4.5,if self.tokenized_lm or token.whitespace_after:
v0.4.5,only clone if optimization mode is 'gpu'
v0.4.5,use the character language model embeddings as basis
v0.4.5,length is twice the original character LM embedding length
v0.4.5,these fields are for the embedding memory
v0.4.5,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.4.5,we re-compute embeddings dynamically at each epoch
v0.4.5,set the memory method
v0.4.5,memory is wiped each time we do a training run
v0.4.5,"if we keep a pooling, it needs to be updated continuously"
v0.4.5,update embedding
v0.4.5,check token.text is empty or not
v0.4.5,add embeddings after updating
v0.4.5,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.4.5,tokens are attended to.
v0.4.5,Zero-pad up to the sequence length.
v0.4.5,"first, find longest sentence in batch"
v0.4.5,prepare id maps for BERT model
v0.4.5,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.4.5,get aggregated embeddings for each BERT-subtoken in sentence
v0.4.5,get the current sentence object
v0.4.5,add concatenated embedding to sentence
v0.4.5,use first subword embedding if pooling operation is 'first'
v0.4.5,"otherwise, do a mean over all subwords in token"
v0.4.5,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.5,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.5,news-english-forward
v0.4.5,news-english-backward
v0.4.5,news-english-forward
v0.4.5,news-english-backward
v0.4.5,mix-english-forward
v0.4.5,mix-english-backward
v0.4.5,mix-german-forward
v0.4.5,mix-german-backward
v0.4.5,common crawl Polish forward
v0.4.5,common crawl Polish backward
v0.4.5,Slovenian forward
v0.4.5,Slovenian backward
v0.4.5,Bulgarian forward
v0.4.5,Bulgarian backward
v0.4.5,Dutch forward
v0.4.5,Dutch backward
v0.4.5,Swedish forward
v0.4.5,Swedish backward
v0.4.5,French forward
v0.4.5,French backward
v0.4.5,Czech forward
v0.4.5,Czech backward
v0.4.5,Portuguese forward
v0.4.5,Portuguese backward
v0.4.5,initialize cache if use_cache set
v0.4.5,embed a dummy sentence to determine embedding_length
v0.4.5,set to eval mode
v0.4.5,Copy the object's state from self.__dict__ which contains
v0.4.5,all our instance attributes. Always use the dict.copy()
v0.4.5,method to avoid modifying the original state.
v0.4.5,Remove the unpicklable entries.
v0.4.5,"if cache is used, try setting embeddings from cache first"
v0.4.5,try populating embeddings from cache
v0.4.5,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.5,get hidden states from language model
v0.4.5,take first or last hidden states from language model as word representation
v0.4.5,if self.tokenized_lm or token.whitespace_after:
v0.4.5,"if only one sentence is passed, convert to list of sentence"
v0.4.5,optional fine-tuning on top of embedding layer
v0.4.5,"if only one sentence is passed, convert to list of sentence"
v0.4.5,bidirectional RNN on top of embedding layer
v0.4.5,dropouts
v0.4.5,TODO: remove in future versions
v0.4.5,embed words in the sentence
v0.4.5,before-RNN dropout
v0.4.5,reproject if set
v0.4.5,push through RNN
v0.4.5,after-RNN dropout
v0.4.5,extract embeddings from RNN
v0.4.5,fixed RNN change format for torch 1.4.0
v0.4.5,bidirectional LSTM on top of embedding layer
v0.4.5,dropouts
v0.4.5,"first, sort sentences by number of tokens"
v0.4.5,go through each sentence in batch
v0.4.5,PADDING: pad shorter sentences out
v0.4.5,ADD TO SENTENCE LIST: add the representation
v0.4.5,--------------------------------------------------------------------
v0.4.5,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.5,--------------------------------------------------------------------
v0.4.5,--------------------------------------------------------------------
v0.4.5,FF PART
v0.4.5,--------------------------------------------------------------------
v0.4.5,use word dropout if set
v0.4.5,--------------------------------------------------------------------
v0.4.5,EXTRACT EMBEDDINGS FROM LSTM
v0.4.5,--------------------------------------------------------------------
v0.4.5,IMPORTANT: add embeddings as torch modules
v0.4.5,iterate over sentences
v0.4.5,"if its a forward LM, take last state"
v0.4.5,GLOVE embeddings
v0.4.5,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.4.5,add positional encodings
v0.4.5,reshape the pixels into the sequence
v0.4.5,layer norm after convolution and positional encodings
v0.4.5,add <cls> token
v0.4.5,"transformer requires input in the shape [h*w+1, b, d]"
v0.4.5,the output is an embedding of <cls> token
v0.4.5,"TODO: keep for backwards compatibility, but remove in future"
v0.4.5,save the sentence piece model as binary file (not as path which may change)
v0.4.5,write out the binary sentence piece model into the expected directory
v0.4.5,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.4.5,"otherwise, use normal process and potentially trigger another download"
v0.4.5,"once the modes if there, load it with sentence piece"
v0.4.5,from allennlp.common.tqdm import Tqdm
v0.4.5,mmap seems to be much more memory efficient
v0.4.5,Remove quotes from etag
v0.4.5,"If there is an etag, it's everything after the first period"
v0.4.5,"Otherwise, use None"
v0.4.5,"URL, so get it from the cache (downloading if necessary)"
v0.4.5,"File, and it exists."
v0.4.5,"File, but it doesn't exist."
v0.4.5,Something unknown
v0.4.5,Extract all the contents of zip file in current directory
v0.4.5,get cache path to put the file
v0.4.5,"Download to temporary file, then copy to cache dir once finished."
v0.4.5,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.5,GET file object
v0.4.5,TODO(joelgrus): do we want to do checksums or anything like that?
v0.4.5,get cache path to put the file
v0.4.5,make HEAD request to check ETag
v0.4.5,add ETag to filename if it exists
v0.4.5,"etag = response.headers.get(""ETag"")"
v0.4.5,"Download to temporary file, then copy to cache dir once finished."
v0.4.5,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.5,GET file object
v0.4.5,These defaults are the same as the argument defaults in tqdm.
v0.4.5,first determine the distribution of classes in the dataset
v0.4.5,weight for each sample
v0.4.5,Create blocks
v0.4.5,shuffle the blocks
v0.4.5,concatenate the shuffled blocks
v0.4.5,Create blocks
v0.4.5,shuffle the blocks
v0.4.5,concatenate the shuffled blocks
v0.4.5,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.5,see https://github.com/zalandoresearch/flair/issues/351
v0.4.5,State initialization
v0.4.5,Exponential moving average of gradient values
v0.4.5,Exponential moving average of squared gradient values
v0.4.5,Maintains max of all exp. moving avg. of sq. grad. values
v0.4.5,Decay the first and second moment running average coefficient
v0.4.5,Maintains the maximum of all 2nd moment running avg. till now
v0.4.5,Use the max. for normalizing running avg. of gradient
v0.4.5,conll 2000 column format
v0.4.5,conll 03 NER column format
v0.4.5,WNUT-17
v0.4.5,-- WikiNER datasets
v0.4.5,-- Universal Dependencies
v0.4.5,Germanic
v0.4.5,Romance
v0.4.5,West-Slavic
v0.4.5,South-Slavic
v0.4.5,East-Slavic
v0.4.5,Scandinavian
v0.4.5,Asian
v0.4.5,Language isolates
v0.4.5,recent Universal Dependencies
v0.4.5,other datasets
v0.4.5,text classification format
v0.4.5,text regression format
v0.4.5,"first, try to fetch dataset online"
v0.4.5,default dataset folder is the cache root
v0.4.5,get string value if enum is passed
v0.4.5,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.4.5,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.4.5,the CoNLL 03 task for German has an additional lemma column
v0.4.5,the CoNLL 03 task for Dutch has no NP column
v0.4.5,the CoNLL 03 task for Spanish only has two columns
v0.4.5,the GERMEVAL task only has two columns: text and ner
v0.4.5,WSD tasks may be put into this column format
v0.4.5,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.4.5,"for text classifiers, we use our own special format"
v0.4.5,NER corpus for Basque
v0.4.5,automatically identify train / test / dev files
v0.4.5,"if no test file is found, take any file with 'test' in name"
v0.4.5,get train and test data
v0.4.5,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.5,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.5,convert tag scheme to iobes
v0.4.5,automatically identify train / test / dev files
v0.4.5,automatically identify train / test / dev files
v0.4.5,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.5,conll 2000 chunking task
v0.4.5,Support both TREC-6 and TREC-50
v0.4.5,Create flair compatible labels
v0.4.5,TREC-6 : NUM:dist -> __label__NUM
v0.4.5,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.5,Wikiner NER task
v0.4.5,unpack and write out in CoNLL column-like format
v0.4.5,CoNLL 02/03 NER
v0.4.5,universal dependencies
v0.4.5,--- UD Germanic
v0.4.5,--- UD Romance
v0.4.5,--- UD West-Slavic
v0.4.5,--- UD Scandinavian
v0.4.5,--- UD South-Slavic
v0.4.5,--- UD Asian
v0.4.5,some non-used parameter to allow print
v0.4.5,get db filename from embedding name
v0.4.5,if embedding database already exists
v0.4.5,"otherwise, push embedding to database"
v0.4.5,init dictionaries
v0.4.5,"in order to deal with unknown tokens, add <unk>"
v0.4.5,increment for last token in sentence if not followed by whitespace
v0.4.5,determine offsets for whitespace_after field
v0.4.5,determine offsets for whitespace_after field
v0.4.5,"if text is passed, instantiate sentence with tokens (words)"
v0.4.5,log a warning if the dataset is empty
v0.4.5,set token idx if not set
v0.4.5,non-set tags are OUT tags
v0.4.5,anything that is not a BIOES tag is a SINGLE tag
v0.4.5,anything that is not OUT is IN
v0.4.5,single and begin tags start a new span
v0.4.5,remember previous tag
v0.4.5,move sentence embeddings to device
v0.4.5,move token embeddings to device
v0.4.5,clear sentence embeddings
v0.4.5,clear token embeddings
v0.4.5,infer whitespace after field
v0.4.5,No character at the corresponding code point: remove it
v0.4.5,find out empty sentence indices
v0.4.5,create subset of non-empty sentence indices
v0.4.5,Make the tag dictionary
v0.4.5,automatically identify train / test / dev files
v0.4.5,"if no test file is found, take any file with 'test' in name"
v0.4.5,get train data
v0.4.5,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.5,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.5,automatically identify train / test / dev files
v0.4.5,get train data
v0.4.5,get test data
v0.4.5,get dev data
v0.4.5,automatically identify train / test / dev files
v0.4.5,use test_file to create test split if available
v0.4.5,"otherwise, sample test data from train data"
v0.4.5,use dev_file to create test split if available
v0.4.5,"otherwise, sample dev data from dev data"
v0.4.5,cache Feidegger config file
v0.4.5,cache Feidegger images
v0.4.5,replace image URL with local cached file
v0.4.5,automatically identify train / test / dev files
v0.4.5,check if dataset is supported
v0.4.5,set file names
v0.4.5,download and unzip in file structure if necessary
v0.4.5,instantiate corpus
v0.4.5,cast to list if necessary
v0.4.5,cast to list if necessary
v0.4.5,"store either Sentence objects in memory, or only file offsets"
v0.4.5,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.5,determine encoding of text file
v0.4.5,variables
v0.4.5,different handling of in_memory data than streaming data
v0.4.5,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.5,test if format is OK
v0.4.5,test if at least one label given
v0.4.5,"first, check if pymongo is installed"
v0.4.5,append Sentence-Image data point
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,check if data there
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,check if data there
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,check if data there
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,Create flair compatible labels
v0.4.5,TREC-6 : NUM:dist -> __label__NUM
v0.4.5,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,Create flair compatible labels
v0.4.5,TREC-6 : NUM:dist -> __label__NUM
v0.4.5,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,download data if necessary
v0.4.5,unpack and write out in CoNLL column-like format
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,column format
v0.4.5,this dataset name
v0.4.5,default dataset folder is the cache root
v0.4.5,download data if necessary
v0.4.5,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.4.5,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.4.5,"if so, num_workers is set to 0 for faster processing"
v0.4.5,global variable: cache_root
v0.4.5,global variable: device
v0.4.5,global variable: embedding_storage_mode
v0.4.5,# dummy return to fulfill trainer.train() needs
v0.4.5,if memory mode option 'none' delete everything
v0.4.5,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.4.5,find out which ones are dynamic embeddings
v0.4.5,find out which ones are dynamic embeddings
v0.4.5,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.4.5,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.4.5,header for 'weights.txt'
v0.4.5,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.4.5,then get all relevant values from the tsv
v0.4.5,then get all relevant values from the tsv
v0.4.5,print(rows)
v0.4.5,"figsize = (16, 16)"
v0.4.5,plot i
v0.4.5,save plots
v0.4.5,save plots
v0.4.5,plt.show()
v0.4.5,save plot
v0.4.5,take the average over the last three scores of training
v0.4.5,take average over the scores from the different training runs
v0.4.5,remove previous embeddings
v0.4.5,clearing token embeddings to save memory
v0.4.5,#TODO: not saving lines yet
v0.4.5,== similarity measures ==
v0.4.5,helper class for ModelSimilarity
v0.4.5,-- works with binary cross entropy loss --
v0.4.5,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.4.5,-- works with ranking/triplet loss --
v0.4.5,normalize the embeddings
v0.4.5,== similarity losses ==
v0.4.5,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.4.5,TODO: this assumes eye matrix
v0.4.5,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.4.5,== similarity learner ==
v0.4.5,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.4.5,assumes that for each data pair there's at least one embedding per modality
v0.4.5,pre-compute embeddings for all targets in evaluation dataset
v0.4.5,compute the similarity
v0.4.5,sort the similarity matrix across modality 1
v0.4.5,"get the ranks, so +1 to start counting ranks from 1"
v0.4.5,The conversion from old model's constructor interface
v0.4.5,auto-spawn on GPU if available
v0.4.5,pad strings with whitespaces to longest sentence
v0.4.5,cut up the input into chunks of max charlength = chunk_size
v0.4.5,push each chunk through the RNN language model
v0.4.5,concatenate all chunks to make final output
v0.4.5,initial hidden state
v0.4.5,get predicted weights
v0.4.5,divide by temperature
v0.4.5,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.4.5,this makes a vector in which the largest value is 0
v0.4.5,compute word weights with exponential function
v0.4.5,try sampling multinomial distribution for next character
v0.4.5,print(word_idx)
v0.4.5,input ids
v0.4.5,push list of character IDs through model
v0.4.5,the target is always the next character
v0.4.5,use cross entropy loss to compare output of forward pass with targets
v0.4.5,exponentiate cross-entropy loss to calculate perplexity
v0.4.5,fixed RNN change format for torch 1.4.0
v0.4.5,set the dictionaries
v0.4.5,Initialize the weight tensor
v0.4.5,initialize the network architecture
v0.4.5,dropouts
v0.4.5,bidirectional LSTM on top of embedding layer
v0.4.5,Create initial hidden state and initialize it
v0.4.5,TODO: Decide how to initialize the hidden state variables
v0.4.5,self.hs_initializer(self.lstm_init_h)
v0.4.5,self.hs_initializer(self.lstm_init_c)
v0.4.5,final linear map to tag space
v0.4.5,reverse sort all sequences by their length
v0.4.5,remove previous embeddings
v0.4.5,progress bar for verbosity
v0.4.5,stop if all sentences are empty
v0.4.5,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.4.5,clearing token embeddings to save memory
v0.4.5,append both to file for evaluation
v0.4.5,make list of gold tags
v0.4.5,make list of predicted tags
v0.4.5,"check for true positives, false positives and false negatives"
v0.4.5,--------------------------------------------------------------------
v0.4.5,FF PART
v0.4.5,--------------------------------------------------------------------
v0.4.5,"if initial hidden state is trainable, use this state"
v0.4.5,word dropout only before LSTM - TODO: more experimentation needed
v0.4.5,if self.use_word_dropout > 0.0:
v0.4.5,sentence_tensor = self.word_dropout(sentence_tensor)
v0.4.5,get the tags in this sentence
v0.4.5,add tags as tensor
v0.4.5,pad tags if using batch-CRF decoder
v0.4.5,reduce raw values to avoid NaN during exp
v0.4.5,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.4.5,default value
v0.4.5,Initialize the weight tensor
v0.4.5,auto-spawn on GPU if available
v0.4.5,reverse sort all sequences by their length
v0.4.5,remove previous embeddings
v0.4.5,progress bar for verbosity
v0.4.5,stop if all sentences are empty
v0.4.5,clearing token embeddings to save memory
v0.4.5,cast string to Path
v0.4.5,"determine what splits (train, dev, test) to evaluate and log"
v0.4.5,prepare loss logging file and set up header
v0.4.5,"minimize training loss if training with dev data, else maximize dev score"
v0.4.5,"if training also uses dev data, include in training set"
v0.4.5,initialize sampler if provided
v0.4.5,init with default values if only class is provided
v0.4.5,set dataset to sample from
v0.4.5,At any point you can hit Ctrl + C to break out of training early.
v0.4.5,get new learning rate
v0.4.5,reload last best model if annealing with restarts is enabled
v0.4.5,stop training if learning rate becomes too small
v0.4.5,process mini-batches
v0.4.5,zero the gradients on the model and optimizer
v0.4.5,"if necessary, make batch_steps"
v0.4.5,forward and backward for batch
v0.4.5,forward pass
v0.4.5,Backward
v0.4.5,do the optimizer step
v0.4.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.5,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.4.5,evaluate on train / dev / test split depending on training settings
v0.4.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.5,calculate scores using dev data if available
v0.4.5,append dev score to score history
v0.4.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.5,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.5,determine learning rate annealing through scheduler
v0.4.5,determine bad epoch number
v0.4.5,log bad epochs
v0.4.5,output log file
v0.4.5,make headers on first epoch
v0.4.5,"if checkpoint is enabled, save model at each epoch"
v0.4.5,"if we use dev data, remember best model based on dev evaluation score"
v0.4.5,"if we do not use dev data for model selection, save final model"
v0.4.5,test best model if test data is present
v0.4.5,"if we are training over multiple datasets, do evaluation for each"
v0.4.5,get and return the final test score of best model
v0.4.5,cast string to Path
v0.4.5,forward pass
v0.4.5,update optimizer and scheduler
v0.4.5,Add chars to the dictionary
v0.4.5,charsplit file content
v0.4.5,charsplit file content
v0.4.5,Add words to the dictionary
v0.4.5,Tokenize file content
v0.4.5,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.4.5,cast string to Path
v0.4.5,error message if the validation dataset is too small
v0.4.5,Shuffle training files randomly after serially iterating through corpus one
v0.4.5,"iterate through training data, starting at self.split (for checkpointing)"
v0.4.5,off by one for printing
v0.4.5,go into train mode
v0.4.5,reset variables
v0.4.5,not really sure what this does
v0.4.5,do the forward pass in the model
v0.4.5,try to predict the targets
v0.4.5,Backward
v0.4.5,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.4.5,We detach the hidden state from how it was previously produced.
v0.4.5,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.4.5,explicitly remove loss to clear up memory
v0.4.5,##############################################################################
v0.4.5,Save the model if the validation loss is the best we've seen so far.
v0.4.5,##############################################################################
v0.4.5,print info
v0.4.5,##############################################################################
v0.4.5,##############################################################################
v0.4.5,final testing
v0.4.5,##############################################################################
v0.4.5,Turn on evaluation mode which disables dropout.
v0.4.5,Work out how cleanly we can divide the dataset into bsz parts.
v0.4.5,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.4.5,Evenly divide the data across the bsz batches.
v0.4.5,skip because it is optional https://github.com/flairNLP/flair/pull/1296
v0.4.5,def test_create_sentence_using_japanese_tokenizer():
v0.4.5,"sentence: Sentence = Sentence("""", use_tokenizer=build_japanese_tokenizer())"
v0.4.5,
v0.4.5,assert 5 == len(sentence.tokens)
v0.4.5,"assert """" == sentence.tokens[0].text"
v0.4.5,"assert """" == sentence.tokens[1].text"
v0.4.5,"assert """" == sentence.tokens[2].text"
v0.4.5,"assert """" == sentence.tokens[3].text"
v0.4.5,"assert """" == sentence.tokens[4].text"
v0.4.5,clean up file
v0.4.5,bioes tags
v0.4.5,bio tags
v0.4.5,broken tags
v0.4.5,all tags
v0.4.5,all weird tags
v0.4.5,tags with confidence
v0.4.5,bioes tags
v0.4.5,bioes tags
v0.4.5,clean up directory
v0.4.5,clean up directory
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,"get training, test and dev data"
v0.4.5,get two corpora as one
v0.4.5,"get training, test and dev data for full English UD corpus from web"
v0.4.5,clean up data directory
v0.4.5,def test_multiclass_metrics():
v0.4.5,
v0.4.5,"metric = Metric(""Test"")"
v0.4.5,"available_labels = [""A"", ""B"", ""C""]"
v0.4.5,
v0.4.5,"predictions = [""A"", ""B""]"
v0.4.5,"true_values = [""A""]"
v0.4.5,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.5,"metric, available_labels, predictions, true_values"
v0.4.5,)
v0.4.5,
v0.4.5,"predictions = [""C"", ""B""]"
v0.4.5,"true_values = [""A"", ""B""]"
v0.4.5,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.5,"metric, available_labels, predictions, true_values"
v0.4.5,)
v0.4.5,
v0.4.5,print(metric)
v0.4.5,from flair.trainers.trainer_regression import RegressorTrainer
v0.4.5,def test_trainer_results(tasks_base_path):
v0.4.5,"corpus, model, trainer = init(tasks_base_path)"
v0.4.5,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.4.5,"assert results[""test_score""] > 0"
v0.4.5,"assert len(results[""dev_loss_history""]) == 1"
v0.4.5,"assert len(results[""dev_score_history""]) == 1"
v0.4.5,"assert len(results[""train_loss_history""]) == 1"
v0.4.5,initialize trainer
v0.4.5,clean up results directory
v0.4.5,initialize trainer
v0.4.5,clean up results directory
v0.4.5,initialize trainer
v0.4.5,clean up results directory
v0.4.5,initialize trainer
v0.4.5,clean up results directory
v0.4.5,initialize trainer
v0.4.5,clean up results directory
v0.4.5,initialize trainer
v0.4.5,clean up results directory
v0.4.5,clean up results directory
v0.4.5,clean up results directory
v0.4.5,clean up results directory
v0.4.5,clean up results directory
v0.4.5,clean up results directory
v0.4.5,get default dictionary
v0.4.5,init forward LM with 128 hidden states and 1 layer
v0.4.5,get the example corpus and process at character level in forward direction
v0.4.5,train the language model
v0.4.5,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.4.5,clean up results directory
v0.4.5,initialize trainer
v0.4.5,clean up results directory
v0.4.5,document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(
v0.4.5,"[flair_embeddings], 128, 1, False"
v0.4.5,)
v0.4.5,clean up results directory
v0.4.5,clean up results directory
v0.4.5,get default dictionary
v0.4.5,init forward LM with 128 hidden states and 1 layer
v0.4.5,get the example corpus and process at character level in forward direction
v0.4.5,train the language model
v0.4.5,clean up results directory
v0.4.5,get default dictionary
v0.4.5,get the example corpus and process at character level in forward direction
v0.4.5,define search space
v0.4.5,sequence tagger parameter
v0.4.5,model trainer parameter
v0.4.5,training parameter
v0.4.5,find best parameter settings
v0.4.5,clean up results directory
v0.4.5,document embeddings parameter
v0.4.5,training parameter
v0.4.5,clean up results directory
v0.4.5,0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15
v0.4.5,
v0.4.5,"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'"
v0.4.5,\     /       |        |         |       |      |      |         \      |      /     |      |      |
v0.4.5,Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
v0.4.5,
v0.4.5,0          1        2         3       4      5       6               7             8     9      10
v0.4.5,First subword embedding
v0.4.5,Last subword embedding
v0.4.5,First token is splitted into two subwords.
v0.4.5,"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
v0.4.5,First and last subword embedding
v0.4.5,Mean of all subword embeddings
v0.4.5,Check embedding dimension when using multiple layers
v0.4.5,Check embedding dimension when using multiple layers and scalar mix
v0.4.5,0             1           2            3          4         5         6        7       8       9        10        11         12
v0.4.5,
v0.4.5,"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'"
v0.4.5,|             |           |            |          |         |         |         \      |      /          |         |          |
v0.4.5,Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
v0.4.5,
v0.4.5,0             1           2            3          4         5         6                7                  8        9          10
v0.4.5,First subword embedding
v0.4.5,Last subword embedding
v0.4.5,First and last subword embedding
v0.4.5,Mean of all subword embeddings
v0.4.5,Check embedding dimension when using multiple layers
v0.4.5,Check embedding dimension when using multiple layers and scalar mix
v0.4.5,0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15
v0.4.5,
v0.4.5,"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'"
v0.4.5,\     /       |        |         |       |      |      |         \      |      /     |      |      |
v0.4.5,Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
v0.4.5,
v0.4.5,0          1        2         3       4      5       6               7             8     9      10
v0.4.5,First subword embedding
v0.4.5,Last subword embedding
v0.4.5,First token is splitted into two subwords.
v0.4.5,"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
v0.4.5,First and last subword embedding
v0.4.5,Mean of all subword embeddings
v0.4.5,Check embedding dimension when using multiple layers
v0.4.5,Check embedding dimension when using multiple layers and scalar mix
v0.4.5,0        1         2         3         4      5      6      7        8        9      10      11   12   13     14
v0.4.5,
v0.4.5,"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'"
v0.4.5,|          |         |         |      |      |      |         \      /       |       |     \    /
v0.4.5,Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .
v0.4.5,
v0.4.5,0          1         2         3      4      5       6           7           8       9        10
v0.4.5,First subword embedding
v0.4.5,Last subword embedding
v0.4.5,First and last subword embedding
v0.4.5,Mean of all subword embeddings
v0.4.5,Check embedding dimension when using multiple layers
v0.4.5,Check embedding dimension when using multiple layers and scalar mix
v0.4.5,0       1        2        3     4     5      6        7        8      9     10     11
v0.4.5,
v0.4.5,"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'"
v0.4.5,|       |        |        |     |     |      |        |        |      |     |
v0.4.5,Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .
v0.4.5,
v0.4.5,0       1        2        3     4     5      6        7        8      9     10
v0.4.5,Check embedding dimension when using multiple layers
v0.4.5,Check embedding dimension when using multiple layers and scalar mix
v0.4.5,0      1             2           3            4          5         6         7         8       9      10        11       12         13        14
v0.4.5,
v0.4.5,"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>"
v0.4.5,|             |           |            |          |         |         |         \      |      /          |         |          |
v0.4.5,Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
v0.4.5,
v0.4.5,0             1           2            3          4         5          6               7                  8        9          10
v0.4.5,First subword embedding
v0.4.5,Last subword embedding
v0.4.5,First and last subword embedding
v0.4.5,Mean of all subword embeddings
v0.4.5,Check embedding dimension when using multiple layers
v0.4.5,Check embedding dimension when using multiple layers and scalar mix
v0.4.5,0       1          2         3         4      5       6       7    8      9
v0.4.5,
v0.4.5,"'<s>',   'J',      ""'"",     'aime',   'le', 'ca', 'member', 't', '!', '</s>'"
v0.4.5,\          |         /         |      \       |        /   |
v0.4.5,J'aime                le         camembert        !
v0.4.5,
v0.4.5,0                   1              2            3
v0.4.5,First subword embedding
v0.4.5,Last subword embedding
v0.4.5,First and last subword embedding
v0.4.5,Mean of all subword embeddings
v0.4.5,Check embedding dimension when using multiple layers
v0.4.5,Check embedding dimension when using multiple layers and scalar mix
v0.4.5,"get training, test and dev data"
v0.4.4,1. get the corpus
v0.4.4,2. what tag do we want to predict?
v0.4.4,3. make the tag dictionary from the corpus
v0.4.4,initialize embeddings
v0.4.4,comment in this line to use character embeddings
v0.4.4,"CharacterEmbeddings(),"
v0.4.4,comment in these lines to use contextual string embeddings
v0.4.4,
v0.4.4,"FlairEmbeddings('news-forward'),"
v0.4.4,
v0.4.4,"FlairEmbeddings('news-backward'),"
v0.4.4,initialize sequence tagger
v0.4.4,initialize trainer
v0.4.4,"if only one sentence is passed, convert to list of sentence"
v0.4.4,IMPORTANT: add embeddings as torch modules
v0.4.4,"if only one sentence is passed, convert to list of sentence"
v0.4.4,GLOVE embeddings
v0.4.4,TURIAN embeddings
v0.4.4,KOMNINOS embeddings
v0.4.4,FT-CRAWL embeddings
v0.4.4,FT-CRAWL embeddings
v0.4.4,twitter embeddings
v0.4.4,two-letter language code wiki embeddings
v0.4.4,two-letter language code wiki embeddings
v0.4.4,two-letter language code crawl embeddings
v0.4.4,fix serialized models
v0.4.4,max_tokens = 500
v0.4.4,model architecture
v0.4.4,download if necessary
v0.4.4,load the model
v0.4.4,empty words get no embedding
v0.4.4,all other words get embedded
v0.4.4,"the default model for ELMo is the 'original' model, which is very large"
v0.4.4,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.4.4,put on Cuda if available
v0.4.4,embed a dummy sentence to determine embedding_length
v0.4.4,embed a dummy sentence to determine embedding_length
v0.4.4,Avoid conflicts with flair's Token class
v0.4.4,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.4.4,use list of common characters if none provided
v0.4.4,translate words in sentence into ints using dictionary
v0.4.4,"sort words by length, for batching and masking"
v0.4.4,chars for rnn processing
v0.4.4,multilingual models
v0.4.4,English models
v0.4.4,Arabic
v0.4.4,Bulgarian
v0.4.4,Czech
v0.4.4,Danish
v0.4.4,German
v0.4.4,Spanish
v0.4.4,Basque
v0.4.4,Persian
v0.4.4,Finnish
v0.4.4,French
v0.4.4,Hebrew
v0.4.4,Hindi
v0.4.4,Croatian
v0.4.4,Indonesian
v0.4.4,Italian
v0.4.4,Japanese
v0.4.4,Dutch
v0.4.4,Norwegian
v0.4.4,Polish
v0.4.4,Portuguese
v0.4.4,Pubmed
v0.4.4,Slovenian
v0.4.4,Swedish
v0.4.4,Tamil
v0.4.4,load model if in pretrained model map
v0.4.4,embeddings are static if we don't do finetuning
v0.4.4,embed a dummy sentence to determine embedding_length
v0.4.4,set to eval mode
v0.4.4,make compatible with serialized models (TODO: remove)
v0.4.4,gradients are enable if fine-tuning is enabled
v0.4.4,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.4,get hidden states from language model
v0.4.4,take first or last hidden states from language model as word representation
v0.4.4,if self.tokenized_lm or token.whitespace_after:
v0.4.4,only clone if optimization mode is 'gpu'
v0.4.4,use the character language model embeddings as basis
v0.4.4,length is twice the original character LM embedding length
v0.4.4,these fields are for the embedding memory
v0.4.4,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.4.4,we re-compute embeddings dynamically at each epoch
v0.4.4,set the memory method
v0.4.4,memory is wiped each time we do a training run
v0.4.4,"if we keep a pooling, it needs to be updated continuously"
v0.4.4,update embedding
v0.4.4,check token.text is empty or not
v0.4.4,add embeddings after updating
v0.4.4,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.4.4,tokens are attended to.
v0.4.4,Zero-pad up to the sequence length.
v0.4.4,"first, find longest sentence in batch"
v0.4.4,prepare id maps for BERT model
v0.4.4,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.4.4,get aggregated embeddings for each BERT-subtoken in sentence
v0.4.4,get the current sentence object
v0.4.4,add concatenated embedding to sentence
v0.4.4,use first subword embedding if pooling operation is 'first'
v0.4.4,"otherwise, do a mean over all subwords in token"
v0.4.4,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.4,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.4,news-english-forward
v0.4.4,news-english-backward
v0.4.4,news-english-forward
v0.4.4,news-english-backward
v0.4.4,mix-english-forward
v0.4.4,mix-english-backward
v0.4.4,mix-german-forward
v0.4.4,mix-german-backward
v0.4.4,common crawl Polish forward
v0.4.4,common crawl Polish backward
v0.4.4,Slovenian forward
v0.4.4,Slovenian backward
v0.4.4,Bulgarian forward
v0.4.4,Bulgarian backward
v0.4.4,Dutch forward
v0.4.4,Dutch backward
v0.4.4,Swedish forward
v0.4.4,Swedish backward
v0.4.4,French forward
v0.4.4,French backward
v0.4.4,Czech forward
v0.4.4,Czech backward
v0.4.4,Portuguese forward
v0.4.4,Portuguese backward
v0.4.4,initialize cache if use_cache set
v0.4.4,embed a dummy sentence to determine embedding_length
v0.4.4,set to eval mode
v0.4.4,Copy the object's state from self.__dict__ which contains
v0.4.4,all our instance attributes. Always use the dict.copy()
v0.4.4,method to avoid modifying the original state.
v0.4.4,Remove the unpicklable entries.
v0.4.4,"if cache is used, try setting embeddings from cache first"
v0.4.4,try populating embeddings from cache
v0.4.4,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.4,get hidden states from language model
v0.4.4,take first or last hidden states from language model as word representation
v0.4.4,if self.tokenized_lm or token.whitespace_after:
v0.4.4,"if only one sentence is passed, convert to list of sentence"
v0.4.4,optional fine-tuning on top of embedding layer
v0.4.4,"if only one sentence is passed, convert to list of sentence"
v0.4.4,bidirectional RNN on top of embedding layer
v0.4.4,dropouts
v0.4.4,TODO: remove in future versions
v0.4.4,embed words in the sentence
v0.4.4,initialize zero-padded word embeddings tensor
v0.4.4,sentence_tensor = torch.zeros(
v0.4.4,[
v0.4.4,"len(sentences),"
v0.4.4,"longest_token_sequence_in_batch,"
v0.4.4,"self.embeddings.embedding_length,"
v0.4.4,"],"
v0.4.4,"dtype=torch.float,"
v0.4.4,"device=flair.device,"
v0.4.4,)
v0.4.4,
v0.4.4,"for s_id, sentence in enumerate(sentences):"
v0.4.4,# fill values with word embeddings
v0.4.4,all_embs = list()
v0.4.4,
v0.4.4,"for index_token, token in enumerate(sentence):"
v0.4.4,embs = token.get_each_embedding()
v0.4.4,if not all_embs:
v0.4.4,all_embs = [list() for _ in range(len(embs))]
v0.4.4,"for index_emb, emb in enumerate(embs):"
v0.4.4,all_embs[index_emb].append(emb)
v0.4.4,
v0.4.4,concat_word_emb = [torch.stack(embs) for embs in all_embs]
v0.4.4,"concat_sentence_emb = torch.cat(concat_word_emb, dim=1)"
v0.4.4,sentence_tensor[s_id][: len(sentence)] = concat_sentence_emb
v0.4.4,before-RNN dropout
v0.4.4,reproject if set
v0.4.4,push through RNN
v0.4.4,after-RNN dropout
v0.4.4,extract embeddings from RNN
v0.4.4,bidirectional LSTM on top of embedding layer
v0.4.4,dropouts
v0.4.4,"first, sort sentences by number of tokens"
v0.4.4,go through each sentence in batch
v0.4.4,PADDING: pad shorter sentences out
v0.4.4,ADD TO SENTENCE LIST: add the representation
v0.4.4,--------------------------------------------------------------------
v0.4.4,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.4,--------------------------------------------------------------------
v0.4.4,--------------------------------------------------------------------
v0.4.4,FF PART
v0.4.4,--------------------------------------------------------------------
v0.4.4,use word dropout if set
v0.4.4,--------------------------------------------------------------------
v0.4.4,EXTRACT EMBEDDINGS FROM LSTM
v0.4.4,--------------------------------------------------------------------
v0.4.4,IMPORTANT: add embeddings as torch modules
v0.4.4,iterate over sentences
v0.4.4,"if its a forward LM, take last state"
v0.4.4,GLOVE embeddings
v0.4.4,"<cls> token initially set to 1/D, so it attends to all image features equally"
v0.4.4,add positional encodings
v0.4.4,reshape the pixels into the sequence
v0.4.4,layer norm after convolution and positional encodings
v0.4.4,add <cls> token
v0.4.4,"transformer requires input in the shape [h*w+1, b, d]"
v0.4.4,the output is an embedding of <cls> token
v0.4.4,"TODO: keep for backwards compatibility, but remove in future"
v0.4.4,save the sentence piece model as binary file (not as path which may change)
v0.4.4,write out the binary sentence piece model into the expected directory
v0.4.4,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.4.4,"otherwise, use normal process and potentially trigger another download"
v0.4.4,"once the modes if there, load it with sentence piece"
v0.4.4,from allennlp.common.tqdm import Tqdm
v0.4.4,mmap seems to be much more memory efficient
v0.4.4,Remove quotes from etag
v0.4.4,"If there is an etag, it's everything after the first period"
v0.4.4,"Otherwise, use None"
v0.4.4,"URL, so get it from the cache (downloading if necessary)"
v0.4.4,"File, and it exists."
v0.4.4,"File, but it doesn't exist."
v0.4.4,Something unknown
v0.4.4,Extract all the contents of zip file in current directory
v0.4.4,get cache path to put the file
v0.4.4,"Download to temporary file, then copy to cache dir once finished."
v0.4.4,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.4,GET file object
v0.4.4,TODO(joelgrus): do we want to do checksums or anything like that?
v0.4.4,get cache path to put the file
v0.4.4,make HEAD request to check ETag
v0.4.4,add ETag to filename if it exists
v0.4.4,"etag = response.headers.get(""ETag"")"
v0.4.4,"Download to temporary file, then copy to cache dir once finished."
v0.4.4,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.4,GET file object
v0.4.4,These defaults are the same as the argument defaults in tqdm.
v0.4.4,first determine the distribution of classes in the dataset
v0.4.4,weight for each sample
v0.4.4,Create blocks
v0.4.4,shuffle the blocks
v0.4.4,concatenate the shuffled blocks
v0.4.4,Create blocks
v0.4.4,shuffle the blocks
v0.4.4,concatenate the shuffled blocks
v0.4.4,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.4,see https://github.com/zalandoresearch/flair/issues/351
v0.4.4,State initialization
v0.4.4,Exponential moving average of gradient values
v0.4.4,Exponential moving average of squared gradient values
v0.4.4,Maintains max of all exp. moving avg. of sq. grad. values
v0.4.4,Decay the first and second moment running average coefficient
v0.4.4,Maintains the maximum of all 2nd moment running avg. till now
v0.4.4,Use the max. for normalizing running avg. of gradient
v0.4.4,conll 2000 column format
v0.4.4,conll 03 NER column format
v0.4.4,WNUT-17
v0.4.4,-- WikiNER datasets
v0.4.4,-- Universal Dependencies
v0.4.4,Germanic
v0.4.4,Romance
v0.4.4,West-Slavic
v0.4.4,South-Slavic
v0.4.4,East-Slavic
v0.4.4,Scandinavian
v0.4.4,Asian
v0.4.4,Language isolates
v0.4.4,recent Universal Dependencies
v0.4.4,other datasets
v0.4.4,text classification format
v0.4.4,text regression format
v0.4.4,"first, try to fetch dataset online"
v0.4.4,default dataset folder is the cache root
v0.4.4,get string value if enum is passed
v0.4.4,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.4.4,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.4.4,the CoNLL 03 task for German has an additional lemma column
v0.4.4,the CoNLL 03 task for Dutch has no NP column
v0.4.4,the CoNLL 03 task for Spanish only has two columns
v0.4.4,the GERMEVAL task only has two columns: text and ner
v0.4.4,WSD tasks may be put into this column format
v0.4.4,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.4.4,"for text classifiers, we use our own special format"
v0.4.4,NER corpus for Basque
v0.4.4,automatically identify train / test / dev files
v0.4.4,"if no test file is found, take any file with 'test' in name"
v0.4.4,get train and test data
v0.4.4,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.4,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.4,convert tag scheme to iobes
v0.4.4,automatically identify train / test / dev files
v0.4.4,automatically identify train / test / dev files
v0.4.4,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.4,conll 2000 chunking task
v0.4.4,Support both TREC-6 and TREC-50
v0.4.4,Create flair compatible labels
v0.4.4,TREC-6 : NUM:dist -> __label__NUM
v0.4.4,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.4,Wikiner NER task
v0.4.4,unpack and write out in CoNLL column-like format
v0.4.4,CoNLL 02/03 NER
v0.4.4,universal dependencies
v0.4.4,--- UD Germanic
v0.4.4,--- UD Romance
v0.4.4,--- UD West-Slavic
v0.4.4,--- UD Scandinavian
v0.4.4,--- UD South-Slavic
v0.4.4,--- UD Asian
v0.4.4,init dictionaries
v0.4.4,"in order to deal with unknown tokens, add <unk>"
v0.4.4,increment for last token in sentence if not followed by whitespace
v0.4.4,determine offsets for whitespace_after field
v0.4.4,determine offsets for whitespace_after field
v0.4.4,"if text is passed, instantiate sentence with tokens (words)"
v0.4.4,log a warning if the dataset is empty
v0.4.4,set token idx if not set
v0.4.4,non-set tags are OUT tags
v0.4.4,anything that is not a BIOES tag is a SINGLE tag
v0.4.4,anything that is not OUT is IN
v0.4.4,single and begin tags start a new span
v0.4.4,remember previous tag
v0.4.4,move sentence embeddings to device
v0.4.4,move token embeddings to device
v0.4.4,clear sentence embeddings
v0.4.4,clear token embeddings
v0.4.4,infer whitespace after field
v0.4.4,No character at the corresponding code point: remove it
v0.4.4,find out empty sentence indices
v0.4.4,create subset of non-empty sentence indices
v0.4.4,Make the tag dictionary
v0.4.4,automatically identify train / test / dev files
v0.4.4,"if no test file is found, take any file with 'test' in name"
v0.4.4,get train data
v0.4.4,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.4,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.4,automatically identify train / test / dev files
v0.4.4,get train data
v0.4.4,get test data
v0.4.4,get dev data
v0.4.4,automatically identify train / test / dev files
v0.4.4,use test_file to create test split if available
v0.4.4,"otherwise, sample test data from train data"
v0.4.4,use dev_file to create test split if available
v0.4.4,"otherwise, sample dev data from dev data"
v0.4.4,cache Feidegger config file
v0.4.4,cache Feidegger images
v0.4.4,replace image URL with local cached file
v0.4.4,automatically identify train / test / dev files
v0.4.4,check if dataset is supported
v0.4.4,set file names
v0.4.4,download and unzip in file structure if necessary
v0.4.4,instantiate corpus
v0.4.4,cast to list if necessary
v0.4.4,cast to list if necessary
v0.4.4,"store either Sentence objects in memory, or only file offsets"
v0.4.4,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.4,determine encoding of text file
v0.4.4,variables
v0.4.4,different handling of in_memory data than streaming data
v0.4.4,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.4,test if format is OK
v0.4.4,test if at least one label given
v0.4.4,append Sentence-Image data point
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,check if data there
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,check if data there
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,check if data there
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,Create flair compatible labels
v0.4.4,TREC-6 : NUM:dist -> __label__NUM
v0.4.4,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,Create flair compatible labels
v0.4.4,TREC-6 : NUM:dist -> __label__NUM
v0.4.4,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,download data if necessary
v0.4.4,unpack and write out in CoNLL column-like format
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,column format
v0.4.4,this dataset name
v0.4.4,default dataset folder is the cache root
v0.4.4,download data if necessary
v0.4.4,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.4.4,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.4.4,"if so, num_workers is set to 0 for faster processing"
v0.4.4,global variable: cache_root
v0.4.4,global variable: device
v0.4.4,global variable: embedding_storage_mode
v0.4.4,# dummy return to fulfill trainer.train() needs
v0.4.4,if memory mode option 'none' delete everything
v0.4.4,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.4.4,find out which ones are dynamic embeddings
v0.4.4,find out which ones are dynamic embeddings
v0.4.4,memory management - option 1: send everything to CPU (pin to memory if we train on GPU)
v0.4.4,record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)
v0.4.4,to enable %matplotlib inline if running in ipynb
v0.4.4,change from Agg to TkAgg for interative mode
v0.4.4,change from Agg to TkAgg for interative mode
v0.4.4,header for 'weights.txt'
v0.4.4,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.4.4,then get all relevant values from the tsv
v0.4.4,then get all relevant values from the tsv
v0.4.4,print(rows)
v0.4.4,"figsize = (16, 16)"
v0.4.4,plot i
v0.4.4,save plots
v0.4.4,save plots
v0.4.4,plt.show()
v0.4.4,save plot
v0.4.4,take the average over the last three scores of training
v0.4.4,take average over the scores from the different training runs
v0.4.4,remove previous embeddings
v0.4.4,clearing token embeddings to save memory
v0.4.4,#TODO: not saving lines yet
v0.4.4,== similarity measures ==
v0.4.4,helper class for ModelSimilarity
v0.4.4,-- works with binary cross entropy loss --
v0.4.4,"model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}"
v0.4.4,-- works with ranking/triplet loss --
v0.4.4,normalize the embeddings
v0.4.4,== similarity losses ==
v0.4.4,"we want that logits for corresponding pairs are high, and for non-corresponding low"
v0.4.4,TODO: this assumes eye matrix
v0.4.4,"loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa"
v0.4.4,== similarity learner ==
v0.4.4,"1/3 only source branch of model, 1/3 only target branch of model, 1/3 both"
v0.4.4,assumes that for each data pair there's at least one embedding per modality
v0.4.4,pre-compute embeddings for all targets in evaluation dataset
v0.4.4,compute the similarity
v0.4.4,sort the similarity matrix across modality 1
v0.4.4,"get the ranks, so +1 to start counting ranks from 1"
v0.4.4,The conversion from old model's constructor interface
v0.4.4,auto-spawn on GPU if available
v0.4.4,pad strings with whitespaces to longest sentence
v0.4.4,cut up the input into chunks of max charlength = chunk_size
v0.4.4,push each chunk through the RNN language model
v0.4.4,concatenate all chunks to make final output
v0.4.4,initial hidden state
v0.4.4,get predicted weights
v0.4.4,divide by temperature
v0.4.4,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.4.4,this makes a vector in which the largest value is 0
v0.4.4,compute word weights with exponential function
v0.4.4,try sampling multinomial distribution for next character
v0.4.4,print(word_idx)
v0.4.4,input ids
v0.4.4,push list of character IDs through model
v0.4.4,the target is always the next character
v0.4.4,use cross entropy loss to compare output of forward pass with targets
v0.4.4,exponentiate cross-entropy loss to calculate perplexity
v0.4.4,set the dictionaries
v0.4.4,initialize the network architecture
v0.4.4,dropouts
v0.4.4,bidirectional LSTM on top of embedding layer
v0.4.4,Create initial hidden state and initialize it
v0.4.4,TODO: Decide how to initialize the hidden state variables
v0.4.4,self.hs_initializer(self.lstm_init_h)
v0.4.4,self.hs_initializer(self.lstm_init_c)
v0.4.4,final linear map to tag space
v0.4.4,reverse sort all sequences by their length
v0.4.4,remove previous embeddings
v0.4.4,progress bar for verbosity
v0.4.4,stop if all sentences are empty
v0.4.4,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.4.4,clearing token embeddings to save memory
v0.4.4,append both to file for evaluation
v0.4.4,make list of gold tags
v0.4.4,make list of predicted tags
v0.4.4,"check for true positives, false positives and false negatives"
v0.4.4,--------------------------------------------------------------------
v0.4.4,FF PART
v0.4.4,--------------------------------------------------------------------
v0.4.4,"if initial hidden state is trainable, use this state"
v0.4.4,word dropout only before LSTM - TODO: more experimentation needed
v0.4.4,if self.use_word_dropout > 0.0:
v0.4.4,sentence_tensor = self.word_dropout(sentence_tensor)
v0.4.4,get the tags in this sentence
v0.4.4,add tags as tensor
v0.4.4,pad tags if using batch-CRF decoder
v0.4.4,reduce raw values to avoid NaN during exp
v0.4.4,broadcasting will do the job of reshaping and is more efficient than calling repeat
v0.4.4,default value
v0.4.4,auto-spawn on GPU if available
v0.4.4,reverse sort all sequences by their length
v0.4.4,remove previous embeddings
v0.4.4,progress bar for verbosity
v0.4.4,stop if all sentences are empty
v0.4.4,clearing token embeddings to save memory
v0.4.4,cast string to Path
v0.4.4,"determine what splits (train, dev, test) to evaluate and log"
v0.4.4,prepare loss logging file and set up header
v0.4.4,"minimize training loss if training with dev data, else maximize dev score"
v0.4.4,"if training also uses dev data, include in training set"
v0.4.4,initialize sampler if provided
v0.4.4,init with default values if only class is provided
v0.4.4,set dataset to sample from
v0.4.4,At any point you can hit Ctrl + C to break out of training early.
v0.4.4,get new learning rate
v0.4.4,reload last best model if annealing with restarts is enabled
v0.4.4,stop training if learning rate becomes too small
v0.4.4,process mini-batches
v0.4.4,zero the gradients on the model and optimizer
v0.4.4,"if necessary, make batch_steps"
v0.4.4,forward and backward for batch
v0.4.4,forward pass
v0.4.4,Backward
v0.4.4,do the optimizer step
v0.4.4,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.4,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.4.4,evaluate on train / dev / test split depending on training settings
v0.4.4,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.4,calculate scores using dev data if available
v0.4.4,append dev score to score history
v0.4.4,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.4,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.4,determine learning rate annealing through scheduler
v0.4.4,determine bad epoch number
v0.4.4,log bad epochs
v0.4.4,output log file
v0.4.4,make headers on first epoch
v0.4.4,"if checkpoint is enabled, save model at each epoch"
v0.4.4,"if we use dev data, remember best model based on dev evaluation score"
v0.4.4,"if we do not use dev data for model selection, save final model"
v0.4.4,test best model if test data is present
v0.4.4,"if we are training over multiple datasets, do evaluation for each"
v0.4.4,get and return the final test score of best model
v0.4.4,cast string to Path
v0.4.4,forward pass
v0.4.4,update optimizer and scheduler
v0.4.4,Add chars to the dictionary
v0.4.4,charsplit file content
v0.4.4,charsplit file content
v0.4.4,Add words to the dictionary
v0.4.4,Tokenize file content
v0.4.4,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.4.4,cast string to Path
v0.4.4,Shuffle training files randomly after serially iterating through corpus one
v0.4.4,"iterate through training data, starting at self.split (for checkpointing)"
v0.4.4,off by one for printing
v0.4.4,go into train mode
v0.4.4,reset variables
v0.4.4,not really sure what this does
v0.4.4,do the forward pass in the model
v0.4.4,try to predict the targets
v0.4.4,Backward
v0.4.4,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.4.4,We detach the hidden state from how it was previously produced.
v0.4.4,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.4.4,explicitly remove loss to clear up memory
v0.4.4,##############################################################################
v0.4.4,Save the model if the validation loss is the best we've seen so far.
v0.4.4,##############################################################################
v0.4.4,print info
v0.4.4,##############################################################################
v0.4.4,##############################################################################
v0.4.4,final testing
v0.4.4,##############################################################################
v0.4.4,Turn on evaluation mode which disables dropout.
v0.4.4,Work out how cleanly we can divide the dataset into bsz parts.
v0.4.4,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.4.4,Evenly divide the data across the bsz batches.
v0.4.4,clean up file
v0.4.4,bioes tags
v0.4.4,bio tags
v0.4.4,broken tags
v0.4.4,all tags
v0.4.4,all weird tags
v0.4.4,tags with confidence
v0.4.4,bioes tags
v0.4.4,bioes tags
v0.4.4,clean up directory
v0.4.4,clean up directory
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,"get training, test and dev data"
v0.4.4,get two corpora as one
v0.4.4,"get training, test and dev data for full English UD corpus from web"
v0.4.4,clean up data directory
v0.4.4,def test_multiclass_metrics():
v0.4.4,
v0.4.4,"metric = Metric(""Test"")"
v0.4.4,"available_labels = [""A"", ""B"", ""C""]"
v0.4.4,
v0.4.4,"predictions = [""A"", ""B""]"
v0.4.4,"true_values = [""A""]"
v0.4.4,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.4,"metric, available_labels, predictions, true_values"
v0.4.4,)
v0.4.4,
v0.4.4,"predictions = [""C"", ""B""]"
v0.4.4,"true_values = [""A"", ""B""]"
v0.4.4,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.4,"metric, available_labels, predictions, true_values"
v0.4.4,)
v0.4.4,
v0.4.4,print(metric)
v0.4.4,from flair.trainers.trainer_regression import RegressorTrainer
v0.4.4,def test_trainer_results(tasks_base_path):
v0.4.4,"corpus, model, trainer = init(tasks_base_path)"
v0.4.4,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.4.4,"assert results[""test_score""] > 0"
v0.4.4,"assert len(results[""dev_loss_history""]) == 1"
v0.4.4,"assert len(results[""dev_score_history""]) == 1"
v0.4.4,"assert len(results[""train_loss_history""]) == 1"
v0.4.4,initialize trainer
v0.4.4,clean up results directory
v0.4.4,initialize trainer
v0.4.4,clean up results directory
v0.4.4,initialize trainer
v0.4.4,clean up results directory
v0.4.4,initialize trainer
v0.4.4,clean up results directory
v0.4.4,initialize trainer
v0.4.4,clean up results directory
v0.4.4,initialize trainer
v0.4.4,clean up results directory
v0.4.4,clean up results directory
v0.4.4,clean up results directory
v0.4.4,clean up results directory
v0.4.4,clean up results directory
v0.4.4,clean up results directory
v0.4.4,get default dictionary
v0.4.4,init forward LM with 128 hidden states and 1 layer
v0.4.4,get the example corpus and process at character level in forward direction
v0.4.4,train the language model
v0.4.4,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.4.4,clean up results directory
v0.4.4,initialize trainer
v0.4.4,clean up results directory
v0.4.4,clean up results directory
v0.4.4,clean up results directory
v0.4.4,get default dictionary
v0.4.4,init forward LM with 128 hidden states and 1 layer
v0.4.4,get the example corpus and process at character level in forward direction
v0.4.4,train the language model
v0.4.4,clean up results directory
v0.4.4,get default dictionary
v0.4.4,get the example corpus and process at character level in forward direction
v0.4.4,define search space
v0.4.4,sequence tagger parameter
v0.4.4,model trainer parameter
v0.4.4,training parameter
v0.4.4,find best parameter settings
v0.4.4,clean up results directory
v0.4.4,document embeddings parameter
v0.4.4,training parameter
v0.4.4,clean up results directory
v0.4.4,0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15
v0.4.4,
v0.4.4,"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'"
v0.4.4,\     /       |        |         |       |      |      |         \      |      /     |      |      |
v0.4.4,Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
v0.4.4,
v0.4.4,0          1        2         3       4      5       6               7             8     9      10
v0.4.4,First subword embedding
v0.4.4,Last subword embedding
v0.4.4,First token is splitted into two subwords.
v0.4.4,"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
v0.4.4,First and last subword embedding
v0.4.4,Mean of all subword embeddings
v0.4.4,Check embedding dimension when using multiple layers
v0.4.4,Check embedding dimension when using multiple layers and scalar mix
v0.4.4,0             1           2            3          4         5         6        7       8       9        10        11         12
v0.4.4,
v0.4.4,"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'"
v0.4.4,|             |           |            |          |         |         |         \      |      /          |         |          |
v0.4.4,Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
v0.4.4,
v0.4.4,0             1           2            3          4         5         6                7                  8        9          10
v0.4.4,First subword embedding
v0.4.4,Last subword embedding
v0.4.4,First and last subword embedding
v0.4.4,Mean of all subword embeddings
v0.4.4,Check embedding dimension when using multiple layers
v0.4.4,Check embedding dimension when using multiple layers and scalar mix
v0.4.4,0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15
v0.4.4,
v0.4.4,"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'"
v0.4.4,\     /       |        |         |       |      |      |         \      |      /     |      |      |
v0.4.4,Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
v0.4.4,
v0.4.4,0          1        2         3       4      5       6               7             8     9      10
v0.4.4,First subword embedding
v0.4.4,Last subword embedding
v0.4.4,First token is splitted into two subwords.
v0.4.4,"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
v0.4.4,First and last subword embedding
v0.4.4,Mean of all subword embeddings
v0.4.4,Check embedding dimension when using multiple layers
v0.4.4,Check embedding dimension when using multiple layers and scalar mix
v0.4.4,0        1         2         3         4      5      6      7        8        9      10      11   12   13     14
v0.4.4,
v0.4.4,"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'"
v0.4.4,|          |         |         |      |      |      |         \      /       |       |     \    /
v0.4.4,Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .
v0.4.4,
v0.4.4,0          1         2         3      4      5       6           7           8       9        10
v0.4.4,First subword embedding
v0.4.4,Last subword embedding
v0.4.4,First and last subword embedding
v0.4.4,Mean of all subword embeddings
v0.4.4,Check embedding dimension when using multiple layers
v0.4.4,Check embedding dimension when using multiple layers and scalar mix
v0.4.4,0       1        2        3     4     5      6        7        8      9     10     11
v0.4.4,
v0.4.4,"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'"
v0.4.4,|       |        |        |     |     |      |        |        |      |     |
v0.4.4,Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .
v0.4.4,
v0.4.4,0       1        2        3     4     5      6        7        8      9     10
v0.4.4,Check embedding dimension when using multiple layers
v0.4.4,Check embedding dimension when using multiple layers and scalar mix
v0.4.4,0      1             2           3            4          5         6         7         8       9      10        11       12         13        14
v0.4.4,
v0.4.4,"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>"
v0.4.4,|             |           |            |          |         |         |         \      |      /          |         |          |
v0.4.4,Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
v0.4.4,
v0.4.4,0             1           2            3          4         5          6               7                  8        9          10
v0.4.4,First subword embedding
v0.4.4,Last subword embedding
v0.4.4,First and last subword embedding
v0.4.4,Mean of all subword embeddings
v0.4.4,Check embedding dimension when using multiple layers
v0.4.4,Check embedding dimension when using multiple layers and scalar mix
v0.4.4,"get training, test and dev data"
v0.4.3,1. get the corpus
v0.4.3,2. what tag do we want to predict?
v0.4.3,3. make the tag dictionary from the corpus
v0.4.3,initialize embeddings
v0.4.3,comment in this line to use character embeddings
v0.4.3,"CharacterEmbeddings(),"
v0.4.3,comment in these lines to use contextual string embeddings
v0.4.3,
v0.4.3,"FlairEmbeddings('news-forward'),"
v0.4.3,
v0.4.3,"FlairEmbeddings('news-backward'),"
v0.4.3,initialize sequence tagger
v0.4.3,initialize trainer
v0.4.3,"if only one sentence is passed, convert to list of sentence"
v0.4.3,IMPORTANT: add embeddings as torch modules
v0.4.3,"if only one sentence is passed, convert to list of sentence"
v0.4.3,GLOVE embeddings
v0.4.3,TURIAN embeddings
v0.4.3,KOMNINOS embeddings
v0.4.3,FT-CRAWL embeddings
v0.4.3,FT-CRAWL embeddings
v0.4.3,twitter embeddings
v0.4.3,two-letter language code wiki embeddings
v0.4.3,two-letter language code wiki embeddings
v0.4.3,two-letter language code crawl embeddings
v0.4.3,fix serialized models
v0.4.3,max_tokens = 500
v0.4.3,model architecture
v0.4.3,save the sentence piece model as binary file (not as path which may change)
v0.4.3,write out the binary sentence piece model into the expected directory
v0.4.3,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.4.3,"otherwise, use normal process and potentially trigger another download"
v0.4.3,"once the modes if there, load it with sentence piece"
v0.4.3,download if necessary
v0.4.3,load the model
v0.4.3,empty words get no embedding
v0.4.3,all other words get embedded
v0.4.3,"the default model for ELMo is the 'original' model, which is very large"
v0.4.3,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.4.3,put on Cuda if available
v0.4.3,embed a dummy sentence to determine embedding_length
v0.4.3,embed a dummy sentence to determine embedding_length
v0.4.3,Avoid conflicts with flair's Token class
v0.4.3,Dummy token is needed to get the actually token tokenized correctly with special ```` symbol
v0.4.3,use list of common characters if none provided
v0.4.3,translate words in sentence into ints using dictionary
v0.4.3,"sort words by length, for batching and masking"
v0.4.3,chars for rnn processing
v0.4.3,multilingual models
v0.4.3,English models
v0.4.3,Arabic
v0.4.3,Bulgarian
v0.4.3,Czech
v0.4.3,Danish
v0.4.3,German
v0.4.3,Spanish
v0.4.3,Basque
v0.4.3,Persian
v0.4.3,Finnish
v0.4.3,French
v0.4.3,Hebrew
v0.4.3,Hindi
v0.4.3,Croatian
v0.4.3,Indonesian
v0.4.3,Italian
v0.4.3,Japanese
v0.4.3,Dutch
v0.4.3,Norwegian
v0.4.3,Polish
v0.4.3,Portuguese
v0.4.3,Pubmed
v0.4.3,Slovenian
v0.4.3,Swedish
v0.4.3,Tamil
v0.4.3,load model if in pretrained model map
v0.4.3,embeddings are static if we don't do finetuning
v0.4.3,embed a dummy sentence to determine embedding_length
v0.4.3,set to eval mode
v0.4.3,make compatible with serialized models (TODO: remove)
v0.4.3,gradients are enable if fine-tuning is enabled
v0.4.3,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.3,pad strings with whitespaces to longest sentence
v0.4.3,get hidden states from language model
v0.4.3,take first or last hidden states from language model as word representation
v0.4.3,if self.tokenized_lm or token.whitespace_after:
v0.4.3,use the character language model embeddings as basis
v0.4.3,length is twice the original character LM embedding length
v0.4.3,these fields are for the embedding memory
v0.4.3,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.4.3,we re-compute embeddings dynamically at each epoch
v0.4.3,set the memory method
v0.4.3,memory is wiped each time we do a training run
v0.4.3,"if we keep a pooling, it needs to be updated continuously"
v0.4.3,update embedding
v0.4.3,add embeddings after updating
v0.4.3,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.4.3,tokens are attended to.
v0.4.3,Zero-pad up to the sequence length.
v0.4.3,"first, find longest sentence in batch"
v0.4.3,prepare id maps for BERT model
v0.4.3,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.4.3,get aggregated embeddings for each BERT-subtoken in sentence
v0.4.3,get the current sentence object
v0.4.3,add concatenated embedding to sentence
v0.4.3,use first subword embedding if pooling operation is 'first'
v0.4.3,"otherwise, do a mean over all subwords in token"
v0.4.3,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.3,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.3,news-english-forward
v0.4.3,news-english-backward
v0.4.3,news-english-forward
v0.4.3,news-english-backward
v0.4.3,mix-english-forward
v0.4.3,mix-english-backward
v0.4.3,mix-german-forward
v0.4.3,mix-german-backward
v0.4.3,common crawl Polish forward
v0.4.3,common crawl Polish backward
v0.4.3,Slovenian forward
v0.4.3,Slovenian backward
v0.4.3,Bulgarian forward
v0.4.3,Bulgarian backward
v0.4.3,Dutch forward
v0.4.3,Dutch backward
v0.4.3,Swedish forward
v0.4.3,Swedish backward
v0.4.3,French forward
v0.4.3,French backward
v0.4.3,Czech forward
v0.4.3,Czech backward
v0.4.3,Portuguese forward
v0.4.3,Portuguese backward
v0.4.3,initialize cache if use_cache set
v0.4.3,embed a dummy sentence to determine embedding_length
v0.4.3,set to eval mode
v0.4.3,Copy the object's state from self.__dict__ which contains
v0.4.3,all our instance attributes. Always use the dict.copy()
v0.4.3,method to avoid modifying the original state.
v0.4.3,Remove the unpicklable entries.
v0.4.3,"if cache is used, try setting embeddings from cache first"
v0.4.3,try populating embeddings from cache
v0.4.3,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.3,pad strings with whitespaces to longest sentence
v0.4.3,get hidden states from language model
v0.4.3,take first or last hidden states from language model as word representation
v0.4.3,if self.tokenized_lm or token.whitespace_after:
v0.4.3,"if only one sentence is passed, convert to list of sentence"
v0.4.3,optional fine-tuning on top of embedding layer
v0.4.3,"if only one sentence is passed, convert to list of sentence"
v0.4.3,bidirectional RNN on top of embedding layer
v0.4.3,dropouts
v0.4.3,"the permutation that sorts the sentences by length, descending"
v0.4.3,the inverse permutation that restores the input order; it's an index tensor therefore LongTensor
v0.4.3,sort sentences by number of tokens
v0.4.3,all_sentence_tensors = []
v0.4.3,initialize zero-padded word embeddings tensor
v0.4.3,fill values with word embeddings
v0.4.3,TODO: this can only be removed once the implementations of word_dropout and locked_dropout have a batch_first mode
v0.4.3,--------------------------------------------------------------------
v0.4.3,FF PART
v0.4.3,--------------------------------------------------------------------
v0.4.3,use word dropout if set
v0.4.3,--------------------------------------------------------------------
v0.4.3,EXTRACT EMBEDDINGS FROM RNN
v0.4.3,--------------------------------------------------------------------
v0.4.3,restore original order of sentences in the batch
v0.4.3,bidirectional LSTM on top of embedding layer
v0.4.3,dropouts
v0.4.3,"first, sort sentences by number of tokens"
v0.4.3,go through each sentence in batch
v0.4.3,PADDING: pad shorter sentences out
v0.4.3,ADD TO SENTENCE LIST: add the representation
v0.4.3,--------------------------------------------------------------------
v0.4.3,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.3,--------------------------------------------------------------------
v0.4.3,--------------------------------------------------------------------
v0.4.3,FF PART
v0.4.3,--------------------------------------------------------------------
v0.4.3,use word dropout if set
v0.4.3,--------------------------------------------------------------------
v0.4.3,EXTRACT EMBEDDINGS FROM LSTM
v0.4.3,--------------------------------------------------------------------
v0.4.3,IMPORTANT: add embeddings as torch modules
v0.4.3,iterate over sentences
v0.4.3,"if its a forward LM, take last state"
v0.4.3,GLOVE embeddings
v0.4.3,from allennlp.common.tqdm import Tqdm
v0.4.3,mmap seems to be much more memory efficient
v0.4.3,Remove quotes from etag
v0.4.3,"If there is an etag, it's everything after the first period"
v0.4.3,"Otherwise, use None"
v0.4.3,"URL, so get it from the cache (downloading if necessary)"
v0.4.3,"File, and it exists."
v0.4.3,"File, but it doesn't exist."
v0.4.3,Something unknown
v0.4.3,unpack and write out in CoNLL column-like format
v0.4.3,Extract all the contents of zip file in current directory
v0.4.3,get cache path to put the file
v0.4.3,"Download to temporary file, then copy to cache dir once finished."
v0.4.3,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.3,GET file object
v0.4.3,TODO(joelgrus): do we want to do checksums or anything like that?
v0.4.3,get cache path to put the file
v0.4.3,make HEAD request to check ETag
v0.4.3,add ETag to filename if it exists
v0.4.3,"etag = response.headers.get(""ETag"")"
v0.4.3,"Download to temporary file, then copy to cache dir once finished."
v0.4.3,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.3,GET file object
v0.4.3,These defaults are the same as the argument defaults in tqdm.
v0.4.3,first determine the distribution of classes in the dataset
v0.4.3,weight for each sample
v0.4.3,Create blocks
v0.4.3,shuffle the blocks
v0.4.3,concatenate the shuffled blocks
v0.4.3,Create blocks
v0.4.3,shuffle the blocks
v0.4.3,concatenate the shuffled blocks
v0.4.3,additional fields for model checkpointing
v0.4.3,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.3,see https://github.com/zalandoresearch/flair/issues/351
v0.4.3,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.3,see https://github.com/zalandoresearch/flair/issues/351
v0.4.3,State initialization
v0.4.3,Exponential moving average of gradient values
v0.4.3,Exponential moving average of squared gradient values
v0.4.3,Maintains max of all exp. moving avg. of sq. grad. values
v0.4.3,Decay the first and second moment running average coefficient
v0.4.3,Maintains the maximum of all 2nd moment running avg. till now
v0.4.3,Use the max. for normalizing running avg. of gradient
v0.4.3,conll 2000 column format
v0.4.3,conll 03 NER column format
v0.4.3,WNUT-17
v0.4.3,-- WikiNER datasets
v0.4.3,-- Universal Dependencies
v0.4.3,Germanic
v0.4.3,Romance
v0.4.3,West-Slavic
v0.4.3,South-Slavic
v0.4.3,East-Slavic
v0.4.3,Scandinavian
v0.4.3,Asian
v0.4.3,Language isolates
v0.4.3,recent Universal Dependencies
v0.4.3,other datasets
v0.4.3,text classification format
v0.4.3,text regression format
v0.4.3,"first, try to fetch dataset online"
v0.4.3,default dataset folder is the cache root
v0.4.3,get string value if enum is passed
v0.4.3,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.4.3,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.4.3,the CoNLL 03 task for German has an additional lemma column
v0.4.3,the CoNLL 03 task for Dutch has no NP column
v0.4.3,the CoNLL 03 task for Spanish only has two columns
v0.4.3,the GERMEVAL task only has two columns: text and ner
v0.4.3,WSD tasks may be put into this column format
v0.4.3,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.4.3,"for text classifiers, we use our own special format"
v0.4.3,NER corpus for Basque
v0.4.3,automatically identify train / test / dev files
v0.4.3,"if no test file is found, take any file with 'test' in name"
v0.4.3,get train and test data
v0.4.3,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.3,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.3,convert tag scheme to iobes
v0.4.3,automatically identify train / test / dev files
v0.4.3,automatically identify train / test / dev files
v0.4.3,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.3,conll 2000 chunking task
v0.4.3,Support both TREC-6 and TREC-50
v0.4.3,Create flair compatible labels
v0.4.3,TREC-6 : NUM:dist -> __label__NUM
v0.4.3,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.3,Wikiner NER task
v0.4.3,unpack and write out in CoNLL column-like format
v0.4.3,CoNLL 02/03 NER
v0.4.3,universal dependencies
v0.4.3,--- UD Germanic
v0.4.3,--- UD Romance
v0.4.3,--- UD West-Slavic
v0.4.3,--- UD Scandinavian
v0.4.3,--- UD South-Slavic
v0.4.3,--- UD Asian
v0.4.3,init dictionaries
v0.4.3,"in order to deal with unknown tokens, add <unk>"
v0.4.3,"if text is passed, instantiate sentence with tokens (words)"
v0.4.3,tokenize the text first if option selected
v0.4.3,use segtok for tokenization
v0.4.3,determine offsets for whitespace_after field
v0.4.3,otherwise assumes whitespace tokenized text
v0.4.3,add each word in tokenized string as Token object to Sentence
v0.4.3,increment for last token in sentence if not followed by whtespace
v0.4.3,log a warning if the dataset is empty
v0.4.3,set token idx if not set
v0.4.3,non-set tags are OUT tags
v0.4.3,anything that is not a BIOES tag is a SINGLE tag
v0.4.3,anything that is not OUT is IN
v0.4.3,single and begin tags start a new span
v0.4.3,remember previous tag
v0.4.3,move sentence embeddings to device
v0.4.3,move token embeddings to device
v0.4.3,clear sentence embeddings
v0.4.3,clear token embeddings
v0.4.3,infer whitespace after field
v0.4.3,find out empty sentence indices
v0.4.3,create subset of non-empty sentence indices
v0.4.3,Make the tag dictionary
v0.4.3,automatically identify train / test / dev files
v0.4.3,"if no test file is found, take any file with 'test' in name"
v0.4.3,get train data
v0.4.3,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.3,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.3,automatically identify train / test / dev files
v0.4.3,get train data
v0.4.3,get test data
v0.4.3,get dev data
v0.4.3,automatically identify train / test / dev files
v0.4.3,automatically identify train / test / dev files
v0.4.3,cast to list if necessary
v0.4.3,"store either Sentence objects in memory, or only file offsets"
v0.4.3,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.3,determine encoding of text file
v0.4.3,variables
v0.4.3,different handling of in_memory data than streaming data
v0.4.3,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.3,test if format is OK
v0.4.3,test if at least one label given
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,check if data there
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,check if data there
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,check if data there
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,Create flair compatible labels
v0.4.3,TREC-6 : NUM:dist -> __label__NUM
v0.4.3,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,Create flair compatible labels
v0.4.3,TREC-6 : NUM:dist -> __label__NUM
v0.4.3,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,download data if necessary
v0.4.3,unpack and write out in CoNLL column-like format
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,column format
v0.4.3,this dataset name
v0.4.3,default dataset folder is the cache root
v0.4.3,download data if necessary
v0.4.3,"in certain cases, multi-CPU data loading makes no sense and slows"
v0.4.3,"everything down. For this reason, we detect if a dataset is in-memory:"
v0.4.3,"if so, num_workers is set to 0 for faster processing"
v0.4.3,global variable: cache_root
v0.4.3,global variable: device
v0.4.3,# dummy return to fulfill trainer.train() needs
v0.4.3,if memory mode option 'none' delete everything
v0.4.3,else delete only dynamic embeddings (otherwise autograd will keep everything in memory)
v0.4.3,find out which ones are dynamic embeddings
v0.4.3,find out which ones are dynamic embeddings
v0.4.3,memory management - option 1: send everything to CPU
v0.4.3,to enable %matplotlib inline if running in ipynb
v0.4.3,change from Agg to TkAgg for interative mode
v0.4.3,change from Agg to TkAgg for interative mode
v0.4.3,header for 'weights.txt'
v0.4.3,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.4.3,then get all relevant values from the tsv
v0.4.3,then get all relevant values from the tsv
v0.4.3,print(rows)
v0.4.3,"figsize = (16, 16)"
v0.4.3,plot i
v0.4.3,save plots
v0.4.3,save plots
v0.4.3,plt.show()
v0.4.3,save plot
v0.4.3,take the average over the last three scores of training
v0.4.3,take average over the scores from the different training runs
v0.4.3,remove previous embeddings
v0.4.3,clearing token embeddings to save memory
v0.4.3,#TODO: not saving lines yet
v0.4.3,auto-spawn on GPU if available
v0.4.3,cut up the input into chunks of max charlength = chunk_size
v0.4.3,push each chunk through the RNN language model
v0.4.3,concatenate all chunks to make final output
v0.4.3,initial hidden state
v0.4.3,get predicted weights
v0.4.3,divide by temperature
v0.4.3,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.4.3,this makes a vector in which the largest value is 0
v0.4.3,compute word weights with exponential function
v0.4.3,try sampling multinomial distribution for next character
v0.4.3,print(word_idx)
v0.4.3,input ids
v0.4.3,push list of character IDs through model
v0.4.3,the target is always the next character
v0.4.3,use cross entropy loss to compare output of forward pass with targets
v0.4.3,exponentiate cross-entropy loss to calculate perplexity
v0.4.3,set the dictionaries
v0.4.3,initialize the network architecture
v0.4.3,dropouts
v0.4.3,bidirectional LSTM on top of embedding layer
v0.4.3,Create initial hidden state and initialize it
v0.4.3,TODO: Decide how to initialize the hidden state variables
v0.4.3,self.hs_initializer(self.lstm_init_h)
v0.4.3,self.hs_initializer(self.lstm_init_c)
v0.4.3,final linear map to tag space
v0.4.3,append both to file for evaluation
v0.4.3,make list of gold tags
v0.4.3,make list of predicted tags
v0.4.3,"check for true positives, false positives and false negatives"
v0.4.3,remove previous embeddings
v0.4.3,reverse sort all sequences by their length
v0.4.3,make mini-batches
v0.4.3,progress bar for verbosity
v0.4.3,"all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided"
v0.4.3,clearing token embeddings to save memory
v0.4.3,initialize zero-padded word embeddings tensor
v0.4.3,fill values with word embeddings
v0.4.3,TODO: this can only be removed once the implementations of word_dropout and locked_dropout have a batch_first mode
v0.4.3,--------------------------------------------------------------------
v0.4.3,FF PART
v0.4.3,--------------------------------------------------------------------
v0.4.3,"if initial hidden state is trainable, use this state"
v0.4.3,word dropout only before LSTM - TODO: more experimentation needed
v0.4.3,if self.use_word_dropout > 0.0:
v0.4.3,sentence_tensor = self.word_dropout(sentence_tensor)
v0.4.3,transpose to batch_first mode
v0.4.3,get the tags in this sentence
v0.4.3,add tags as tensor
v0.4.3,pad tags if using batch-CRF decoder
v0.4.3,return all scores if so selected
v0.4.3,auto-spawn on GPU if available
v0.4.3,remove previous embeddings
v0.4.3,clearing token embeddings to save memory
v0.4.3,cast string to Path
v0.4.3,"determine what splits (train, dev, test) to evaluate and log"
v0.4.3,prepare loss logging file and set up header
v0.4.3,"minimize training loss if training with dev data, else maximize dev score"
v0.4.3,"if training also uses dev data, include in training set"
v0.4.3,At any point you can hit Ctrl + C to break out of training early.
v0.4.3,get new learning rate
v0.4.3,reload last best model if annealing with restarts is enabled
v0.4.3,stop training if learning rate becomes too small
v0.4.3,process mini-batches
v0.4.3,Backward
v0.4.3,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.3,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.4.3,evaluate on train / dev / test split depending on training settings
v0.4.3,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.3,calculate scores using dev data if available
v0.4.3,append dev score to score history
v0.4.3,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.3,"depending on memory mode, embeddings are moved to CPU, GPU or deleted"
v0.4.3,determine learning rate annealing through scheduler
v0.4.3,determine bad epoch number
v0.4.3,log bad epochs
v0.4.3,output log file
v0.4.3,make headers on first epoch
v0.4.3,"if checkpoint is enable, save model at each epoch"
v0.4.3,"if we use dev data, remember best model based on dev evaluation score"
v0.4.3,"if we do not use dev data for model selection, save final model"
v0.4.3,test best model if test data is present
v0.4.3,"if we are training over multiple datasets, do evaluation for each"
v0.4.3,get and return the final test score of best model
v0.4.3,cast string to Path
v0.4.3,Add chars to the dictionary
v0.4.3,charsplit file content
v0.4.3,charsplit file content
v0.4.3,Add words to the dictionary
v0.4.3,Tokenize file content
v0.4.3,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.4.3,cast string to Path
v0.4.3,Shuffle training files randomly after serially iterating through corpus one
v0.4.3,"iterate through training data, starting at self.split (for checkpointing)"
v0.4.3,off by one for printing
v0.4.3,go into train mode
v0.4.3,reset variables
v0.4.3,not really sure what this does
v0.4.3,do the forward pass in the model
v0.4.3,try to predict the targets
v0.4.3,Backward
v0.4.3,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.4.3,We detach the hidden state from how it was previously produced.
v0.4.3,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.4.3,explicitly remove loss to clear up memory
v0.4.3,##############################################################################
v0.4.3,Save the model if the validation loss is the best we've seen so far.
v0.4.3,##############################################################################
v0.4.3,print info
v0.4.3,##############################################################################
v0.4.3,##############################################################################
v0.4.3,final testing
v0.4.3,##############################################################################
v0.4.3,Turn on evaluation mode which disables dropout.
v0.4.3,Work out how cleanly we can divide the dataset into bsz parts.
v0.4.3,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.4.3,Evenly divide the data across the bsz batches.
v0.4.3,clean up file
v0.4.3,bioes tags
v0.4.3,bio tags
v0.4.3,broken tags
v0.4.3,all tags
v0.4.3,all weird tags
v0.4.3,tags with confidence
v0.4.3,bioes tags
v0.4.3,bioes tags
v0.4.3,clean up directory
v0.4.3,clean up directory
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,"get training, test and dev data"
v0.4.3,get two corpora as one
v0.4.3,"get training, test and dev data for full English UD corpus from web"
v0.4.3,clean up data directory
v0.4.3,def test_multiclass_metrics():
v0.4.3,
v0.4.3,"metric = Metric(""Test"")"
v0.4.3,"available_labels = [""A"", ""B"", ""C""]"
v0.4.3,
v0.4.3,"predictions = [""A"", ""B""]"
v0.4.3,"true_values = [""A""]"
v0.4.3,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.3,"metric, available_labels, predictions, true_values"
v0.4.3,)
v0.4.3,
v0.4.3,"predictions = [""C"", ""B""]"
v0.4.3,"true_values = [""A"", ""B""]"
v0.4.3,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.3,"metric, available_labels, predictions, true_values"
v0.4.3,)
v0.4.3,
v0.4.3,print(metric)
v0.4.3,from flair.trainers.trainer_regression import RegressorTrainer
v0.4.3,def test_trainer_results(tasks_base_path):
v0.4.3,"corpus, model, trainer = init(tasks_base_path)"
v0.4.3,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.4.3,"assert results[""test_score""] > 0"
v0.4.3,"assert len(results[""dev_loss_history""]) == 1"
v0.4.3,"assert len(results[""dev_score_history""]) == 1"
v0.4.3,"assert len(results[""train_loss_history""]) == 1"
v0.4.3,initialize trainer
v0.4.3,clean up results directory
v0.4.3,initialize trainer
v0.4.3,clean up results directory
v0.4.3,initialize trainer
v0.4.3,clean up results directory
v0.4.3,initialize trainer
v0.4.3,clean up results directory
v0.4.3,initialize trainer
v0.4.3,clean up results directory
v0.4.3,initialize trainer
v0.4.3,clean up results directory
v0.4.3,clean up results directory
v0.4.3,clean up results directory
v0.4.3,clean up results directory
v0.4.3,clean up results directory
v0.4.3,clean up results directory
v0.4.3,get default dictionary
v0.4.3,init forward LM with 128 hidden states and 1 layer
v0.4.3,get the example corpus and process at character level in forward direction
v0.4.3,train the language model
v0.4.3,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.4.3,clean up results directory
v0.4.3,initialize trainer
v0.4.3,clean up results directory
v0.4.3,clean up results directory
v0.4.3,clean up results directory
v0.4.3,get default dictionary
v0.4.3,init forward LM with 128 hidden states and 1 layer
v0.4.3,get the example corpus and process at character level in forward direction
v0.4.3,train the language model
v0.4.3,clean up results directory
v0.4.3,get default dictionary
v0.4.3,get the example corpus and process at character level in forward direction
v0.4.3,define search space
v0.4.3,sequence tagger parameter
v0.4.3,model trainer parameter
v0.4.3,training parameter
v0.4.3,find best parameter settings
v0.4.3,clean up results directory
v0.4.3,document embeddings parameter
v0.4.3,training parameter
v0.4.3,clean up results directory
v0.4.3,0           1      2       3        4         5       6      7      8       9      10     11     12     13    14      15
v0.4.3,
v0.4.3,"'<s>',      'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.',  '</s>'"
v0.4.3,\     /       |        |         |       |      |      |         \      |      /     |      |      |
v0.4.3,Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
v0.4.3,
v0.4.3,0          1        2         3       4      5       6               7             8     9      10
v0.4.3,First subword embedding
v0.4.3,Last subword embedding
v0.4.3,First token is splitted into two subwords.
v0.4.3,"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
v0.4.3,First and last subword embedding
v0.4.3,Mean of all subword embeddings
v0.4.3,Check embedding dimension when using multiple layers
v0.4.3,Check embedding dimension when using multiple layers and scalar mix
v0.4.3,0             1           2            3          4         5         6        7       8       9        10        11         12
v0.4.3,
v0.4.3,"'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>'"
v0.4.3,|             |           |            |          |         |         |         \      |      /          |         |          |
v0.4.3,Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
v0.4.3,
v0.4.3,0             1           2            3          4         5         6                7                  8        9          10
v0.4.3,First subword embedding
v0.4.3,Last subword embedding
v0.4.3,First and last subword embedding
v0.4.3,Mean of all subword embeddings
v0.4.3,Check embedding dimension when using multiple layers
v0.4.3,Check embedding dimension when using multiple layers and scalar mix
v0.4.3,0           1      2       3        4         5       6      7      8       9      10     11     12     13    14          15
v0.4.3,
v0.4.3,"'<|endoftext|>', 'Ber', 'lin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'pupp', 'ete', 'er', 'to', 'see', '.', '<|endoftext|>'"
v0.4.3,\     /       |        |         |       |      |      |         \      |      /     |      |      |
v0.4.3,Berlin      and    Munich     have      a     lot     of           puppeteer        to    see     .
v0.4.3,
v0.4.3,0          1        2         3       4      5       6               7             8     9      10
v0.4.3,First subword embedding
v0.4.3,Last subword embedding
v0.4.3,First token is splitted into two subwords.
v0.4.3,"As we use ""last"" as pooling operation, we consider the last subword as ""first token"" here"
v0.4.3,First and last subword embedding
v0.4.3,Mean of all subword embeddings
v0.4.3,Check embedding dimension when using multiple layers
v0.4.3,Check embedding dimension when using multiple layers and scalar mix
v0.4.3,0        1         2         3         4      5      6      7        8        9      10      11   12   13     14
v0.4.3,
v0.4.3,"'<s>', 'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppet', 'eer', 'to', 'see', '', '.', '</s>'"
v0.4.3,|          |         |         |      |      |      |         \      /       |       |     \    /
v0.4.3,Berlin      and     Munich     have     a     lot     of       puppeteer       to     see       .
v0.4.3,
v0.4.3,0          1         2         3      4      5       6           7           8       9        10
v0.4.3,First subword embedding
v0.4.3,Last subword embedding
v0.4.3,First and last subword embedding
v0.4.3,Mean of all subword embeddings
v0.4.3,Check embedding dimension when using multiple layers
v0.4.3,Check embedding dimension when using multiple layers and scalar mix
v0.4.3,0       1        2        3     4     5      6        7        8      9     10     11
v0.4.3,
v0.4.3,"'Berlin', 'and', 'Munich', 'have', 'a', 'lot', 'of', 'puppeteer', 'to', 'see', '.', '<eos>'"
v0.4.3,|       |        |        |     |     |      |        |        |      |     |
v0.4.3,Berlin    and    Munich    have    a    lot    of    puppeteer    to    see    .
v0.4.3,
v0.4.3,0       1        2        3     4     5      6        7        8      9     10
v0.4.3,Check embedding dimension when using multiple layers
v0.4.3,Check embedding dimension when using multiple layers and scalar mix
v0.4.3,0      1             2           3            4          5         6         7         8       9      10        11       12         13        14
v0.4.3,
v0.4.3,"<s>  'berlin</w>', 'and</w>', 'munich</w>', 'have</w>', 'a</w>', 'lot</w>', 'of</w>', 'pupp', 'ete', 'er</w>', 'to</w>', 'see</w>', '.</w>', '</s>"
v0.4.3,|             |           |            |          |         |         |         \      |      /          |         |          |
v0.4.3,Berlin         and        Munich        have        a        lot        of           puppeteer             to       see         .
v0.4.3,
v0.4.3,0             1           2            3          4         5          6               7                  8        9          10
v0.4.3,First subword embedding
v0.4.3,Last subword embedding
v0.4.3,First and last subword embedding
v0.4.3,Mean of all subword embeddings
v0.4.3,Check embedding dimension when using multiple layers
v0.4.3,Check embedding dimension when using multiple layers and scalar mix
v0.4.3,"get training, test and dev data"
v0.4.2,1. get the corpus
v0.4.2,2. what tag do we want to predict?
v0.4.2,3. make the tag dictionary from the corpus
v0.4.2,initialize embeddings
v0.4.2,comment in this line to use character embeddings
v0.4.2,"CharacterEmbeddings(),"
v0.4.2,comment in these lines to use contextual string embeddings
v0.4.2,
v0.4.2,"FlairEmbeddings('news-forward'),"
v0.4.2,
v0.4.2,"FlairEmbeddings('news-backward'),"
v0.4.2,initialize sequence tagger
v0.4.2,initialize trainer
v0.4.2,"if only one sentence is passed, convert to list of sentence"
v0.4.2,IMPORTANT: add embeddings as torch modules
v0.4.2,"if only one sentence is passed, convert to list of sentence"
v0.4.2,GLOVE embeddings
v0.4.2,TURIAN embeddings
v0.4.2,KOMNINOS embeddings
v0.4.2,FT-CRAWL embeddings
v0.4.2,FT-CRAWL embeddings
v0.4.2,twitter embeddings
v0.4.2,two-letter language code wiki embeddings
v0.4.2,two-letter language code wiki embeddings
v0.4.2,two-letter language code crawl embeddings
v0.4.2,max_tokens = 500
v0.4.2,model architecture
v0.4.2,save the sentence piece model as binary file (not as path which may change)
v0.4.2,write out the binary sentence piece model into the expected directory
v0.4.2,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.4.2,"otherwise, use normal process and potentially trigger another download"
v0.4.2,"once the modes if there, load it with sentence piece"
v0.4.2,empty words get no embedding
v0.4.2,all other words get embedded
v0.4.2,"the default model for ELMo is the 'original' model, which is very large"
v0.4.2,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.4.2,put on Cuda if available
v0.4.2,embed a dummy sentence to determine embedding_length
v0.4.2,embed a dummy sentence to determine embedding_length
v0.4.2,Avoid conflicts with flair's Token class
v0.4.2,Use embedding of first subword
v0.4.2,Use embedding of first and last subword
v0.4.2,"Otherwise, use mean over all subwords in token"
v0.4.2,use list of common characters if none provided
v0.4.2,translate words in sentence into ints using dictionary
v0.4.2,"sort words by length, for batching and masking"
v0.4.2,chars for rnn processing
v0.4.2,multilingual models
v0.4.2,English models
v0.4.2,Arabic
v0.4.2,Bulgarian
v0.4.2,Czech
v0.4.2,Danish
v0.4.2,German
v0.4.2,Spanish
v0.4.2,Basque
v0.4.2,Farsi
v0.4.2,Finnish
v0.4.2,French
v0.4.2,Hebrew
v0.4.2,Hindi
v0.4.2,Croatian
v0.4.2,Indonesian
v0.4.2,Italian
v0.4.2,Japanese
v0.4.2,Dutch
v0.4.2,Norwegian
v0.4.2,Polish
v0.4.2,Portuguese
v0.4.2,Pubmed
v0.4.2,Slovenian
v0.4.2,Swedish
v0.4.2,load model if in pretrained model map
v0.4.2,initialize cache if use_cache set
v0.4.2,embed a dummy sentence to determine embedding_length
v0.4.2,set to eval mode
v0.4.2,Copy the object's state from self.__dict__ which contains
v0.4.2,all our instance attributes. Always use the dict.copy()
v0.4.2,method to avoid modifying the original state.
v0.4.2,Remove the unpicklable entries.
v0.4.2,make compatible with serialized models
v0.4.2,"if cache is used, try setting embeddings from cache first"
v0.4.2,try populating embeddings from cache
v0.4.2,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.2,pad strings with whitespaces to longest sentence
v0.4.2,get hidden states from language model
v0.4.2,take first or last hidden states from language model as word representation
v0.4.2,if self.tokenized_lm or token.whitespace_after:
v0.4.2,use the character language model embeddings as basis
v0.4.2,length is twice the original character LM embedding length
v0.4.2,these fields are for the embedding memory
v0.4.2,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.4.2,we re-compute embeddings dynamically at each epoch
v0.4.2,set the memory method
v0.4.2,memory is wiped each time we do a training run
v0.4.2,"if we keep a pooling, it needs to be updated continuously"
v0.4.2,update embedding
v0.4.2,add embeddings after updating
v0.4.2,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.4.2,tokens are attended to.
v0.4.2,Zero-pad up to the sequence length.
v0.4.2,"first, find longest sentence in batch"
v0.4.2,prepare id maps for BERT model
v0.4.2,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.4.2,get aggregated embeddings for each BERT-subtoken in sentence
v0.4.2,get the current sentence object
v0.4.2,add concatenated embedding to sentence
v0.4.2,use first subword embedding if pooling operation is 'first'
v0.4.2,"otherwise, do a mean over all subwords in token"
v0.4.2,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.2,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.2,news-english-forward
v0.4.2,news-english-backward
v0.4.2,news-english-forward
v0.4.2,news-english-backward
v0.4.2,mix-english-forward
v0.4.2,mix-english-backward
v0.4.2,mix-german-forward
v0.4.2,mix-german-backward
v0.4.2,common crawl Polish forward
v0.4.2,common crawl Polish backward
v0.4.2,Slovenian forward
v0.4.2,Slovenian backward
v0.4.2,Bulgarian forward
v0.4.2,Bulgarian backward
v0.4.2,Dutch forward
v0.4.2,Dutch backward
v0.4.2,Swedish forward
v0.4.2,Swedish backward
v0.4.2,French forward
v0.4.2,French backward
v0.4.2,Czech forward
v0.4.2,Czech backward
v0.4.2,Portuguese forward
v0.4.2,Portuguese backward
v0.4.2,initialize cache if use_cache set
v0.4.2,embed a dummy sentence to determine embedding_length
v0.4.2,set to eval mode
v0.4.2,Copy the object's state from self.__dict__ which contains
v0.4.2,all our instance attributes. Always use the dict.copy()
v0.4.2,method to avoid modifying the original state.
v0.4.2,Remove the unpicklable entries.
v0.4.2,"if cache is used, try setting embeddings from cache first"
v0.4.2,try populating embeddings from cache
v0.4.2,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.2,pad strings with whitespaces to longest sentence
v0.4.2,get hidden states from language model
v0.4.2,take first or last hidden states from language model as word representation
v0.4.2,if self.tokenized_lm or token.whitespace_after:
v0.4.2,"if only one sentence is passed, convert to list of sentence"
v0.4.2,optional fine-tuning on top of embedding layer
v0.4.2,"if only one sentence is passed, convert to list of sentence"
v0.4.2,bidirectional RNN on top of embedding layer
v0.4.2,dropouts
v0.4.2,"first, sort sentences by number of tokens"
v0.4.2,go through each sentence in batch
v0.4.2,PADDING: pad shorter sentences out
v0.4.2,ADD TO SENTENCE LIST: add the representation
v0.4.2,--------------------------------------------------------------------
v0.4.2,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.2,--------------------------------------------------------------------
v0.4.2,--------------------------------------------------------------------
v0.4.2,FF PART
v0.4.2,--------------------------------------------------------------------
v0.4.2,use word dropout if set
v0.4.2,--------------------------------------------------------------------
v0.4.2,EXTRACT EMBEDDINGS FROM RNN
v0.4.2,--------------------------------------------------------------------
v0.4.2,bidirectional LSTM on top of embedding layer
v0.4.2,dropouts
v0.4.2,"first, sort sentences by number of tokens"
v0.4.2,go through each sentence in batch
v0.4.2,PADDING: pad shorter sentences out
v0.4.2,ADD TO SENTENCE LIST: add the representation
v0.4.2,--------------------------------------------------------------------
v0.4.2,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.2,--------------------------------------------------------------------
v0.4.2,--------------------------------------------------------------------
v0.4.2,FF PART
v0.4.2,--------------------------------------------------------------------
v0.4.2,use word dropout if set
v0.4.2,--------------------------------------------------------------------
v0.4.2,EXTRACT EMBEDDINGS FROM LSTM
v0.4.2,--------------------------------------------------------------------
v0.4.2,iterate over sentences
v0.4.2,"if its a forward LM, take last state"
v0.4.2,GLOVE embeddings
v0.4.2,from allennlp.common.tqdm import Tqdm
v0.4.2,mmap seems to be much more memory efficient
v0.4.2,Remove quotes from etag
v0.4.2,"If there is an etag, it's everything after the first period"
v0.4.2,"Otherwise, use None"
v0.4.2,"URL, so get it from the cache (downloading if necessary)"
v0.4.2,"File, and it exists."
v0.4.2,"File, but it doesn't exist."
v0.4.2,Something unknown
v0.4.2,unpack and write out in CoNLL column-like format
v0.4.2,Extract all the contents of zip file in current directory
v0.4.2,get cache path to put the file
v0.4.2,"Download to temporary file, then copy to cache dir once finished."
v0.4.2,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.2,GET file object
v0.4.2,TODO(joelgrus): do we want to do checksums or anything like that?
v0.4.2,get cache path to put the file
v0.4.2,make HEAD request to check ETag
v0.4.2,add ETag to filename if it exists
v0.4.2,"etag = response.headers.get(""ETag"")"
v0.4.2,"Download to temporary file, then copy to cache dir once finished."
v0.4.2,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.2,GET file object
v0.4.2,These defaults are the same as the argument defaults in tqdm.
v0.4.2,additional fields for model checkpointing
v0.4.2,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.2,see https://github.com/zalandoresearch/flair/issues/351
v0.4.2,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.2,see https://github.com/zalandoresearch/flair/issues/351
v0.4.2,State initialization
v0.4.2,Exponential moving average of gradient values
v0.4.2,Exponential moving average of squared gradient values
v0.4.2,Maintains max of all exp. moving avg. of sq. grad. values
v0.4.2,Decay the first and second moment running average coefficient
v0.4.2,Maintains the maximum of all 2nd moment running avg. till now
v0.4.2,Use the max. for normalizing running avg. of gradient
v0.4.2,conll 2000 column format
v0.4.2,conll 03 NER column format
v0.4.2,WNUT-17
v0.4.2,-- WikiNER datasets
v0.4.2,-- Universal Dependencies
v0.4.2,Germanic
v0.4.2,Romance
v0.4.2,West-Slavic
v0.4.2,South-Slavic
v0.4.2,East-Slavic
v0.4.2,Scandinavian
v0.4.2,Asian
v0.4.2,Language isolates
v0.4.2,recent Universal Dependencies
v0.4.2,other datasets
v0.4.2,text classification format
v0.4.2,text regression format
v0.4.2,"first, try to fetch dataset online"
v0.4.2,default dataset folder is the cache root
v0.4.2,get string value if enum is passed
v0.4.2,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.4.2,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.4.2,the CoNLL 03 task for German has an additional lemma column
v0.4.2,the CoNLL 03 task for Dutch has no NP column
v0.4.2,the CoNLL 03 task for Spanish only has two columns
v0.4.2,the GERMEVAL task only has two columns: text and ner
v0.4.2,WSD tasks may be put into this column format
v0.4.2,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.4.2,"for text classifiers, we use our own special format"
v0.4.2,NER corpus for Basque
v0.4.2,automatically identify train / test / dev files
v0.4.2,"if no test file is found, take any file with 'test' in name"
v0.4.2,get train and test data
v0.4.2,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.2,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.2,convert tag scheme to iobes
v0.4.2,automatically identify train / test / dev files
v0.4.2,automatically identify train / test / dev files
v0.4.2,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.2,conll 2000 chunking task
v0.4.2,Support both TREC-6 and TREC-50
v0.4.2,Create flair compatible labels
v0.4.2,TREC-6 : NUM:dist -> __label__NUM
v0.4.2,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.2,Wikiner NER task
v0.4.2,unpack and write out in CoNLL column-like format
v0.4.2,CoNLL 02/03 NER
v0.4.2,universal dependencies
v0.4.2,--- UD Germanic
v0.4.2,--- UD Romance
v0.4.2,--- UD West-Slavic
v0.4.2,--- UD Scandinavian
v0.4.2,--- UD South-Slavic
v0.4.2,--- UD Asian
v0.4.2,init dictionaries
v0.4.2,"in order to deal with unknown tokens, add <unk>"
v0.4.2,"if text is passed, instantiate sentence with tokens (words)"
v0.4.2,tokenize the text first if option selected
v0.4.2,use segtok for tokenization
v0.4.2,determine offsets for whitespace_after field
v0.4.2,otherwise assumes whitespace tokenized text
v0.4.2,add each word in tokenized string as Token object to Sentence
v0.4.2,increment for last token in sentence if not followed by whtespace
v0.4.2,log a warning if the dataset is empty
v0.4.2,set token idx if not set
v0.4.2,non-set tags are OUT tags
v0.4.2,anything that is not a BIOES tag is a SINGLE tag
v0.4.2,anything that is not OUT is IN
v0.4.2,single and begin tags start a new span
v0.4.2,remember previous tag
v0.4.2,infer whitespace after field
v0.4.2,Make the tag dictionary
v0.4.2,automatically identify train / test / dev files
v0.4.2,"if no test file is found, take any file with 'test' in name"
v0.4.2,get train data
v0.4.2,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.2,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.2,automatically identify train / test / dev files
v0.4.2,get train data
v0.4.2,get test data
v0.4.2,get dev data
v0.4.2,automatically identify train / test / dev files
v0.4.2,"store either Sentence objects in memory, or only file offsets"
v0.4.2,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.2,determine encoding of text file
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,check if data there
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,check if data there
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,check if data there
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,Create flair compatible labels
v0.4.2,TREC-6 : NUM:dist -> __label__NUM
v0.4.2,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,Create flair compatible labels
v0.4.2,TREC-6 : NUM:dist -> __label__NUM
v0.4.2,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,download data if necessary
v0.4.2,unpack and write out in CoNLL column-like format
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,column format
v0.4.2,this dataset name
v0.4.2,default dataset folder is the cache root
v0.4.2,download data if necessary
v0.4.2,global variable: cache_root
v0.4.2,global variable: device
v0.4.2,# dummy return to fulfill trainer.train() needs
v0.4.2,header for 'weights.txt'
v0.4.2,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.4.2,then get all relevant values from the tsv
v0.4.2,then get all relevant values from the tsv
v0.4.2,plot i
v0.4.2,save plots
v0.4.2,save plots
v0.4.2,save plot
v0.4.2,take the average over the last three scores of training
v0.4.2,take average over the scores from the different training runs
v0.4.2,#TODO: not saving lines yet
v0.4.2,auto-spawn on GPU if available
v0.4.2,cut up the input into chunks of max charlength = chunk_size
v0.4.2,push each chunk through the RNN language model
v0.4.2,concatenate all chunks to make final output
v0.4.2,initial hidden state
v0.4.2,get predicted weights
v0.4.2,divide by temperature
v0.4.2,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.4.2,this makes a vector in which the largest value is 0
v0.4.2,compute word weights with exponential function
v0.4.2,try sampling multinomial distribution for next character
v0.4.2,print(word_idx)
v0.4.2,input ids
v0.4.2,push list of character IDs through model
v0.4.2,the target is always the next character
v0.4.2,use cross entropy loss to compare output of forward pass with targets
v0.4.2,exponentiate cross-entropy loss to calculate perplexity
v0.4.2,set the dictionaries
v0.4.2,initialize the network architecture
v0.4.2,dropouts
v0.4.2,bidirectional LSTM on top of embedding layer
v0.4.2,final linear map to tag space
v0.4.2,append both to file for evaluation
v0.4.2,make list of gold tags
v0.4.2,make list of predicted tags
v0.4.2,"check for true positives, false positives and false negatives"
v0.4.2,remove previous embeddings
v0.4.2,revere sort all sequences by their length
v0.4.2,make mini-batches
v0.4.2,progress bar for verbosity
v0.4.2,clearing token embeddings to save memory
v0.4.2,initialize zero-padded word embeddings tensor
v0.4.2,fill values with word embeddings
v0.4.2,get the tags in this sentence
v0.4.2,add tags as tensor
v0.4.2,--------------------------------------------------------------------
v0.4.2,FF PART
v0.4.2,--------------------------------------------------------------------
v0.4.2,word dropout only before LSTM - TODO: more experimentation needed
v0.4.2,if self.use_word_dropout > 0.0:
v0.4.2,sentence_tensor = self.word_dropout(sentence_tensor)
v0.4.2,get the tags in this sentence
v0.4.2,add tags as tensor
v0.4.2,pad tags if using batch-CRF decoder
v0.4.2,auto-spawn on GPU if available
v0.4.2,cast string to Path
v0.4.2,"determine what splits (train, dev, test) to evaluate and log"
v0.4.2,"minimize training loss if training with dev data, else maximize dev score"
v0.4.2,"if training also uses dev data, include in training set"
v0.4.2,At any point you can hit Ctrl + C to break out of training early.
v0.4.2,reload last best model if annealing with restarts is enabled
v0.4.2,stop training if learning rate becomes too small
v0.4.2,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.4.2,calculate scores using dev data if available
v0.4.2,append dev score to score history
v0.4.2,"if checkpoint is enable, save model at each epoch"
v0.4.2,"if we use dev data, remember best model based on dev evaluation score"
v0.4.2,"if we do not use dev data for model selection, save final model"
v0.4.2,test best model if test data is present
v0.4.2,"if we are training over multiple datasets, do evaluation for each"
v0.4.2,get and return the final test score of best model
v0.4.2,cast string to Path
v0.4.2,Add chars to the dictionary
v0.4.2,charsplit file content
v0.4.2,charsplit file content
v0.4.2,Add words to the dictionary
v0.4.2,Tokenize file content
v0.4.2,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.4.2,cast string to Path
v0.4.2,Shuffle training files randomly after serially iterating through corpus one
v0.4.2,"iterate through training data, starting at self.split (for checkpointing)"
v0.4.2,off by one for printing
v0.4.2,go into train mode
v0.4.2,reset variables
v0.4.2,not really sure what this does
v0.4.2,do the forward pass in the model
v0.4.2,try to predict the targets
v0.4.2,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.4.2,We detach the hidden state from how it was previously produced.
v0.4.2,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.4.2,explicitly remove loss to clear up memory
v0.4.2,##############################################################################
v0.4.2,Save the model if the validation loss is the best we've seen so far.
v0.4.2,##############################################################################
v0.4.2,print info
v0.4.2,##############################################################################
v0.4.2,##############################################################################
v0.4.2,final testing
v0.4.2,##############################################################################
v0.4.2,Turn on evaluation mode which disables dropout.
v0.4.2,Work out how cleanly we can divide the dataset into bsz parts.
v0.4.2,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.4.2,Evenly divide the data across the bsz batches.
v0.4.2,clean up file
v0.4.2,bioes tags
v0.4.2,bio tags
v0.4.2,broken tags
v0.4.2,all tags
v0.4.2,all weird tags
v0.4.2,tags with confidence
v0.4.2,bioes tags
v0.4.2,bioes tags
v0.4.2,clean up directory
v0.4.2,clean up directory
v0.4.2,"get training, test and dev data"
v0.4.2,"get training, test and dev data"
v0.4.2,"get training, test and dev data"
v0.4.2,"get training, test and dev data"
v0.4.2,"get training, test and dev data"
v0.4.2,"get training, test and dev data"
v0.4.2,"get training, test and dev data"
v0.4.2,"get training, test and dev data"
v0.4.2,get two corpora as one
v0.4.2,"get training, test and dev data for full English UD corpus from web"
v0.4.2,clean up data directory
v0.4.2,def test_multiclass_metrics():
v0.4.2,
v0.4.2,"metric = Metric(""Test"")"
v0.4.2,"available_labels = [""A"", ""B"", ""C""]"
v0.4.2,
v0.4.2,"predictions = [""A"", ""B""]"
v0.4.2,"true_values = [""A""]"
v0.4.2,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.2,"metric, available_labels, predictions, true_values"
v0.4.2,)
v0.4.2,
v0.4.2,"predictions = [""C"", ""B""]"
v0.4.2,"true_values = [""A"", ""B""]"
v0.4.2,TextClassifier._evaluate_sentence_for_text_classification(
v0.4.2,"metric, available_labels, predictions, true_values"
v0.4.2,)
v0.4.2,
v0.4.2,print(metric)
v0.4.2,from flair.trainers.trainer_regression import RegressorTrainer
v0.4.2,def test_trainer_results(tasks_base_path):
v0.4.2,"corpus, model, trainer = init(tasks_base_path)"
v0.4.2,"results = trainer.train(""regression_train/"", max_epochs=1)"
v0.4.2,"assert results[""test_score""] > 0"
v0.4.2,"assert len(results[""dev_loss_history""]) == 1"
v0.4.2,"assert len(results[""dev_score_history""]) == 1"
v0.4.2,"assert len(results[""train_loss_history""]) == 1"
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,make a temporary cache directory that we remove afterwards
v0.4.2,initialize trainer
v0.4.2,remove the cache directory
v0.4.2,clean up results directory
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,clean up results directory
v0.4.2,clean up results directory
v0.4.2,clean up results directory
v0.4.2,clean up results directory
v0.4.2,clean up results directory
v0.4.2,get default dictionary
v0.4.2,init forward LM with 128 hidden states and 1 layer
v0.4.2,get the example corpus and process at character level in forward direction
v0.4.2,train the language model
v0.4.2,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.4.2,clean up results directory
v0.4.2,initialize trainer
v0.4.2,clean up results directory
v0.4.2,clean up results directory
v0.4.2,clean up results directory
v0.4.2,get default dictionary
v0.4.2,init forward LM with 128 hidden states and 1 layer
v0.4.2,get the example corpus and process at character level in forward direction
v0.4.2,train the language model
v0.4.2,clean up results directory
v0.4.2,get default dictionary
v0.4.2,get the example corpus and process at character level in forward direction
v0.4.2,define search space
v0.4.2,sequence tagger parameter
v0.4.2,model trainer parameter
v0.4.2,training parameter
v0.4.2,find best parameter settings
v0.4.2,clean up results directory
v0.4.2,document embeddings parameter
v0.4.2,training parameter
v0.4.2,clean up results directory
v0.4.2,"get training, test and dev data"
v0.4.1,1. get the corpus
v0.4.1,2. what tag do we want to predict?
v0.4.1,3. make the tag dictionary from the corpus
v0.4.1,initialize embeddings
v0.4.1,comment in this line to use character embeddings
v0.4.1,"CharacterEmbeddings(),"
v0.4.1,comment in these lines to use contextual string embeddings
v0.4.1,
v0.4.1,"CharLMEmbeddings('news-forward'),"
v0.4.1,
v0.4.1,"CharLMEmbeddings('news-backward'),"
v0.4.1,initialize sequence tagger
v0.4.1,initialize trainer
v0.4.1,"if only one sentence is passed, convert to list of sentence"
v0.4.1,IMPORTANT: add embeddings as torch modules
v0.4.1,"if only one sentence is passed, convert to list of sentence"
v0.4.1,GLOVE embeddings
v0.4.1,TURIAN embeddings
v0.4.1,KOMNIOS embeddings
v0.4.1,FT-CRAWL embeddings
v0.4.1,FT-CRAWL embeddings
v0.4.1,twitter embeddings
v0.4.1,two-letter language code wiki embeddings
v0.4.1,two-letter language code wiki embeddings
v0.4.1,two-letter language code crawl embeddings
v0.4.1,save the sentence piece model as binary file (not as path which may change)
v0.4.1,write out the binary sentence piece model into the expected directory
v0.4.1,"if the model was saved as binary and it is not found on disk, write to appropriate path"
v0.4.1,"otherwise, use normal process and potentially trigger another download"
v0.4.1,"once the modes if there, load it with sentence piece"
v0.4.1,empty words get no embedding
v0.4.1,all other words get embedded
v0.4.1,"the default model for ELMo is the 'original' model, which is very large"
v0.4.1,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.4.1,put on Cuda if available
v0.4.1,embed a dummy sentence to determine embedding_length
v0.4.1,embed a dummy sentence to determine embedding_length
v0.4.1,Avoid conflicts with flair's Token class
v0.4.1,use list of common characters if none provided
v0.4.1,translate words in sentence into ints using dictionary
v0.4.1,"sort words by length, for batching and masking"
v0.4.1,chars for rnn processing
v0.4.1,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.1,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.1,"multilingual forward fast (English, German, French, Italian, Dutch, Polish)"
v0.4.1,"multilingual backward fast (English, German, French, Italian, Dutch, Polish)"
v0.4.1,news-english-forward
v0.4.1,news-english-backward
v0.4.1,news-english-forward
v0.4.1,news-english-backward
v0.4.1,mix-english-forward
v0.4.1,mix-english-backward
v0.4.1,mix-german-forward
v0.4.1,mix-german-backward
v0.4.1,common crawl Polish forward
v0.4.1,common crawl Polish backward
v0.4.1,Slovenian forward
v0.4.1,Slovenian backward
v0.4.1,Bulgarian forward
v0.4.1,Bulgarian backward
v0.4.1,Dutch forward
v0.4.1,Dutch backward
v0.4.1,Swedish forward
v0.4.1,Swedish backward
v0.4.1,French forward
v0.4.1,French backward
v0.4.1,Czech forward
v0.4.1,Czech backward
v0.4.1,Portuguese forward
v0.4.1,Portuguese backward
v0.4.1,Basque forward
v0.4.1,Basque backward
v0.4.1,Spanish forward fast
v0.4.1,Spanish backward fast
v0.4.1,Spanish forward
v0.4.1,Spanish backward
v0.4.1,Pubmed forward
v0.4.1,Pubmed backward
v0.4.1,Japanese forward
v0.4.1,Japanese backward
v0.4.1,initialize cache if use_cache set
v0.4.1,embed a dummy sentence to determine embedding_length
v0.4.1,set to eval mode
v0.4.1,Copy the object's state from self.__dict__ which contains
v0.4.1,all our instance attributes. Always use the dict.copy()
v0.4.1,method to avoid modifying the original state.
v0.4.1,Remove the unpicklable entries.
v0.4.1,make compatible with serialized models
v0.4.1,"if cache is used, try setting embeddings from cache first"
v0.4.1,try populating embeddings from cache
v0.4.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.1,pad strings with whitespaces to longest sentence
v0.4.1,get hidden states from language model
v0.4.1,take first or last hidden states from language model as word representation
v0.4.1,if self.tokenized_lm or token.whitespace_after:
v0.4.1,use the character language model embeddings as basis
v0.4.1,length is twice the original character LM embedding length
v0.4.1,these fields are for the embedding memory
v0.4.1,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.4.1,we re-compute embeddings dynamically at each epoch
v0.4.1,set the memory method
v0.4.1,memory is wiped each time we do a training run
v0.4.1,"if we keep a pooling, it needs to be updated continuously"
v0.4.1,update embedding
v0.4.1,add embeddings after updating
v0.4.1,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.4.1,tokens are attended to.
v0.4.1,Zero-pad up to the sequence length.
v0.4.1,"first, find longest sentence in batch"
v0.4.1,prepare id maps for BERT model
v0.4.1,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.4.1,get aggregated embeddings for each BERT-subtoken in sentence
v0.4.1,get the current sentence object
v0.4.1,add concatenated embedding to sentence
v0.4.1,use first subword embedding if pooling operation is 'first'
v0.4.1,"otherwise, do a mean over all subwords in token"
v0.4.1,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.1,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.1,news-english-forward
v0.4.1,news-english-backward
v0.4.1,news-english-forward
v0.4.1,news-english-backward
v0.4.1,mix-english-forward
v0.4.1,mix-english-backward
v0.4.1,mix-german-forward
v0.4.1,mix-german-backward
v0.4.1,common crawl Polish forward
v0.4.1,common crawl Polish backward
v0.4.1,Slovenian forward
v0.4.1,Slovenian backward
v0.4.1,Bulgarian forward
v0.4.1,Bulgarian backward
v0.4.1,Dutch forward
v0.4.1,Dutch backward
v0.4.1,Swedish forward
v0.4.1,Swedish backward
v0.4.1,French forward
v0.4.1,French backward
v0.4.1,Czech forward
v0.4.1,Czech backward
v0.4.1,Portuguese forward
v0.4.1,Portuguese backward
v0.4.1,initialize cache if use_cache set
v0.4.1,embed a dummy sentence to determine embedding_length
v0.4.1,set to eval mode
v0.4.1,Copy the object's state from self.__dict__ which contains
v0.4.1,all our instance attributes. Always use the dict.copy()
v0.4.1,method to avoid modifying the original state.
v0.4.1,Remove the unpicklable entries.
v0.4.1,"if cache is used, try setting embeddings from cache first"
v0.4.1,try populating embeddings from cache
v0.4.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.1,pad strings with whitespaces to longest sentence
v0.4.1,get hidden states from language model
v0.4.1,take first or last hidden states from language model as word representation
v0.4.1,if self.tokenized_lm or token.whitespace_after:
v0.4.1,"if only one sentence is passed, convert to list of sentence"
v0.4.1,"if only one sentence is passed, convert to list of sentence"
v0.4.1,bidirectional RNN on top of embedding layer
v0.4.1,dropouts
v0.4.1,"first, sort sentences by number of tokens"
v0.4.1,go through each sentence in batch
v0.4.1,PADDING: pad shorter sentences out
v0.4.1,ADD TO SENTENCE LIST: add the representation
v0.4.1,--------------------------------------------------------------------
v0.4.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.1,--------------------------------------------------------------------
v0.4.1,--------------------------------------------------------------------
v0.4.1,FF PART
v0.4.1,--------------------------------------------------------------------
v0.4.1,use word dropout if set
v0.4.1,--------------------------------------------------------------------
v0.4.1,EXTRACT EMBEDDINGS FROM RNN
v0.4.1,--------------------------------------------------------------------
v0.4.1,bidirectional LSTM on top of embedding layer
v0.4.1,dropouts
v0.4.1,"first, sort sentences by number of tokens"
v0.4.1,go through each sentence in batch
v0.4.1,PADDING: pad shorter sentences out
v0.4.1,ADD TO SENTENCE LIST: add the representation
v0.4.1,--------------------------------------------------------------------
v0.4.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.1,--------------------------------------------------------------------
v0.4.1,--------------------------------------------------------------------
v0.4.1,FF PART
v0.4.1,--------------------------------------------------------------------
v0.4.1,use word dropout if set
v0.4.1,--------------------------------------------------------------------
v0.4.1,EXTRACT EMBEDDINGS FROM LSTM
v0.4.1,--------------------------------------------------------------------
v0.4.1,iterate over sentences
v0.4.1,"if its a forward LM, take last state"
v0.4.1,from allennlp.common.tqdm import Tqdm
v0.4.1,mmap seems to be much more memory efficient
v0.4.1,Remove quotes from etag
v0.4.1,"If there is an etag, it's everything after the first period"
v0.4.1,"Otherwise, use None"
v0.4.1,"URL, so get it from the cache (downloading if necessary)"
v0.4.1,"File, and it exists."
v0.4.1,"File, but it doesn't exist."
v0.4.1,Something unknown
v0.4.1,TODO(joelgrus): do we want to do checksums or anything like that?
v0.4.1,get cache path to put the file
v0.4.1,make HEAD request to check ETag
v0.4.1,add ETag to filename if it exists
v0.4.1,"etag = response.headers.get(""ETag"")"
v0.4.1,"Download to temporary file, then copy to cache dir once finished."
v0.4.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.1,GET file object
v0.4.1,These defaults are the same as the argument defaults in tqdm.
v0.4.1,State initialization
v0.4.1,Exponential moving average of gradient values
v0.4.1,Exponential moving average of squared gradient values
v0.4.1,Maintains max of all exp. moving avg. of sq. grad. values
v0.4.1,Decay the first and second moment running average coefficient
v0.4.1,Maintains the maximum of all 2nd moment running avg. till now
v0.4.1,Use the max. for normalizing running avg. of gradient
v0.4.1,conll 2000 column format
v0.4.1,conll 03 NER column format
v0.4.1,WNUT-17
v0.4.1,-- WikiNER datasets
v0.4.1,-- Universal Dependencies
v0.4.1,Germanic
v0.4.1,Romance
v0.4.1,West-Slavic
v0.4.1,South-Slavic
v0.4.1,East-Slavic
v0.4.1,Scandinavian
v0.4.1,Asian
v0.4.1,Language isolates
v0.4.1,other datasets
v0.4.1,text classification format
v0.4.1,"first, try to fetch dataset online"
v0.4.1,default dataset folder is the cache root
v0.4.1,get string value if enum is passed
v0.4.1,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.4.1,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.4.1,the CoNLL 03 task for German has an additional lemma column
v0.4.1,the CoNLL 03 task for Dutch has no NP column
v0.4.1,the CoNLL 03 task for Spanish only has two columns
v0.4.1,the GERMEVAL task only has two columns: text and ner
v0.4.1,WSD tasks may be put into this column format
v0.4.1,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.4.1,"for text classifiers, we use our own special format"
v0.4.1,NER corpus for Basque
v0.4.1,automatically identify train / test / dev files
v0.4.1,"if no test file is found, take any file with 'test' in name"
v0.4.1,get train and test data
v0.4.1,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.1,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.1,convert tag scheme to iobes
v0.4.1,automatically identify train / test / dev files
v0.4.1,automatically identify train / test / dev files
v0.4.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.1,conll 2000 chunking task
v0.4.1,Support both TREC-6 and TREC-50
v0.4.1,Create flair compatible labels
v0.4.1,TREC-6 : NUM:dist -> __label__NUM
v0.4.1,TREC-50: NUM:dist -> __label__NUM:dist
v0.4.1,Wikiner NER task
v0.4.1,unpack and write out in CoNLL column-like format
v0.4.1,CoNLL 02/03 NER
v0.4.1,universal dependencies
v0.4.1,--- UD Germanic
v0.4.1,--- UD Romance
v0.4.1,--- UD West-Slavic
v0.4.1,--- UD Scandinavian
v0.4.1,--- UD South-Slavic
v0.4.1,--- UD Asian
v0.4.1,init dictionaries
v0.4.1,"in order to deal with unknown tokens, add <unk>"
v0.4.1,"if text is passed, instantiate sentence with tokens (words)"
v0.4.1,tokenize the text first if option selected
v0.4.1,use segtok for tokenization
v0.4.1,determine offsets for whitespace_after field
v0.4.1,otherwise assumes whitespace tokenized text
v0.4.1,add each word in tokenized string as Token object to Sentence
v0.4.1,increment for last token in sentence if not followed by whtespace
v0.4.1,set token idx if not set
v0.4.1,non-set tags are OUT tags
v0.4.1,anything that is not a BIOES tag is a SINGLE tag
v0.4.1,anything that is not OUT is IN
v0.4.1,single and begin tags start a new span
v0.4.1,remember previous tag
v0.4.1,infer whitespace after field
v0.4.1,Make the tag dictionary
v0.4.1,Make the tag dictionary
v0.4.1,header for 'weights.txt'
v0.4.1,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.4.1,then get all relevant values from the tsv
v0.4.1,then get all relevant values from the tsv
v0.4.1,plot i
v0.4.1,save plots
v0.4.1,plot 1
v0.4.1,plot 2
v0.4.1,plot 3
v0.4.1,save plots
v0.4.1,save plot
v0.4.1,take the average over the last three scores of training
v0.4.1,take average over the scores from the different training runs
v0.4.1,auto-spawn on GPU if available
v0.4.1,cut up the input into chunks of max charlength = chunk_size
v0.4.1,push each chunk through the RNN language model
v0.4.1,concatenate all chunks to make final output
v0.4.1,initial hidden state
v0.4.1,get predicted weights
v0.4.1,divide by temperature
v0.4.1,"to prevent overflow problem with small temperature values, substract largest value from all"
v0.4.1,this makes a vector in which the largest value is 0
v0.4.1,compute word weights with exponential function
v0.4.1,try sampling multinomial distribution for next character
v0.4.1,print(word_idx)
v0.4.1,input ids
v0.4.1,push list of character IDs through model
v0.4.1,the target is always the next character
v0.4.1,use cross entropy loss to compare output of forward pass with targets
v0.4.1,exponentiate cross-entropy loss to calculate perplexity
v0.4.1,set the dictionaries
v0.4.1,initialize the network architecture
v0.4.1,dropouts
v0.4.1,bidirectional LSTM on top of embedding layer
v0.4.1,final linear map to tag space
v0.4.1,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.4.1,serialization of torch objects
v0.4.1,https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
v0.4.1,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.1,see https://github.com/zalandoresearch/flair/issues/351
v0.4.1,remove previous embeddings
v0.4.1,revere sort all sequences by their length
v0.4.1,make mini-batches
v0.4.1,progress bar for verbosity
v0.4.1,clearing token embeddings to save memory
v0.4.1,"if sorting is enabled, sort sentences by number of tokens"
v0.4.1,initialize zero-padded word embeddings tensor
v0.4.1,fill values with word embeddings
v0.4.1,get the tags in this sentence
v0.4.1,add tags as tensor
v0.4.1,--------------------------------------------------------------------
v0.4.1,FF PART
v0.4.1,--------------------------------------------------------------------
v0.4.1,word dropout only before LSTM - TODO: more experimentation needed
v0.4.1,if self.use_word_dropout > 0.0:
v0.4.1,sentence_tensor = self.word_dropout(sentence_tensor)
v0.4.1,pad tags if using batch-CRF decoder
v0.4.1,auto-spawn on GPU if available
v0.4.1,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.4.1,serialization of torch objects
v0.4.1,https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
v0.4.1,load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups
v0.4.1,see https://github.com/zalandoresearch/flair/issues/351
v0.4.1,cast string to Path
v0.4.1,annealing scheduler
v0.4.1,"if training also uses dev data, include in training set"
v0.4.1,At any point you can hit Ctrl + C to break out of training early.
v0.4.1,reload last best model if annealing with restarts is enabled
v0.4.1,stop training if learning rate becomes too small
v0.4.1,calculate scores using dev data if available
v0.4.1,append dev score to score history
v0.4.1,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.4.1,"if checkpoint is enable, save model at each epoch"
v0.4.1,"if we use dev data, remember best model based on dev evaluation score"
v0.4.1,"if we do not use dev data for model selection, save final model"
v0.4.1,test best model if test data is present
v0.4.1,"if we are training over multiple datasets, do evaluation for each"
v0.4.1,get and return the final test score of best model
v0.4.1,append both to file for evaluation
v0.4.1,make list of gold tags
v0.4.1,make list of predicted tags
v0.4.1,"check for true positives, false positives and false negatives"
v0.4.1,cast string to Path
v0.4.1,Add chars to the dictionary
v0.4.1,charsplit file content
v0.4.1,charsplit file content
v0.4.1,Add words to the dictionary
v0.4.1,Tokenize file content
v0.4.1,"TextDataset returns a list. valid and test are only one file, so return the first element"
v0.4.1,cast string to Path
v0.4.1,Shuffle training files randomly after serially iterating through corpus one
v0.4.1,"iterate through training data, starting at self.split (for checkpointing)"
v0.4.1,off by one for printing
v0.4.1,go into train mode
v0.4.1,reset variables
v0.4.1,not really sure what this does
v0.4.1,do the forward pass in the model
v0.4.1,try to predict the targets
v0.4.1,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.4.1,We detach the hidden state from how it was previously produced.
v0.4.1,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.4.1,explicitly remove loss to clear up memory
v0.4.1,##############################################################################
v0.4.1,Save the model if the validation loss is the best we've seen so far.
v0.4.1,##############################################################################
v0.4.1,print info
v0.4.1,##############################################################################
v0.4.1,##############################################################################
v0.4.1,final testing
v0.4.1,##############################################################################
v0.4.1,Turn on evaluation mode which disables dropout.
v0.4.1,Work out how cleanly we can divide the dataset into bsz parts.
v0.4.1,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.4.1,Evenly divide the data across the bsz batches.
v0.4.1,clean up file
v0.4.1,bioes tags
v0.4.1,bio tags
v0.4.1,broken tags
v0.4.1,all tags
v0.4.1,all weird tags
v0.4.1,tags with confidence
v0.4.1,bioes tags
v0.4.1,bioes tags
v0.4.1,clean up directory
v0.4.1,clean up directory
v0.4.1,"get training, test and dev data"
v0.4.1,"get training, test and dev data"
v0.4.1,"get training, test and dev data"
v0.4.1,"get training, test and dev data"
v0.4.1,"get training, test and dev data"
v0.4.1,"get training, test and dev data"
v0.4.1,"get training, test and dev data"
v0.4.1,get two corpora as one
v0.4.1,"get training, test and dev data for full English UD corpus from web"
v0.4.1,clean up data directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,make a temporary cache directory that we remove afterwards
v0.4.1,initialize trainer
v0.4.1,remove the cache directory
v0.4.1,clean up results directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,clean up results directory
v0.4.1,"corpus = NLPTaskDataFetcher.load_corpus('multi_class', base_path=tasks_base_path)"
v0.4.1,clean up results directory
v0.4.1,clean up results directory
v0.4.1,clean up results directory
v0.4.1,get default dictionary
v0.4.1,init forward LM with 128 hidden states and 1 layer
v0.4.1,get the example corpus and process at character level in forward direction
v0.4.1,train the language model
v0.4.1,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.4.1,clean up results directory
v0.4.1,initialize trainer
v0.4.1,clean up results directory
v0.4.1,clean up results directory
v0.4.1,clean up results directory
v0.4.1,get default dictionary
v0.4.1,init forward LM with 128 hidden states and 1 layer
v0.4.1,get the example corpus and process at character level in forward direction
v0.4.1,train the language model
v0.4.1,clean up results directory
v0.4.1,get default dictionary
v0.4.1,get the example corpus and process at character level in forward direction
v0.4.1,define search space
v0.4.1,sequence tagger parameter
v0.4.1,model trainer parameter
v0.4.1,training parameter
v0.4.1,find best parameter settings
v0.4.1,clean up results directory
v0.4.1,document embeddings parameter
v0.4.1,training parameter
v0.4.1,clean up results directory
v0.4.0,1. get the corpus
v0.4.0,2. what tag do we want to predict?
v0.4.0,3. make the tag dictionary from the corpus
v0.4.0,initialize embeddings
v0.4.0,comment in this line to use character embeddings
v0.4.0,"CharacterEmbeddings(),"
v0.4.0,comment in these lines to use contextual string embeddings
v0.4.0,
v0.4.0,"CharLMEmbeddings('news-forward'),"
v0.4.0,
v0.4.0,"CharLMEmbeddings('news-backward'),"
v0.4.0,initialize sequence tagger
v0.4.0,initialize trainer
v0.4.0,"if only one sentence is passed, convert to list of sentence"
v0.4.0,IMPORTANT: add embeddings as torch modules
v0.4.0,"if only one sentence is passed, convert to list of sentence"
v0.4.0,GLOVE embeddings
v0.4.0,KOMNIOS embeddings
v0.4.0,FT-CRAWL embeddings
v0.4.0,FT-CRAWL embeddings
v0.4.0,twitter embeddings
v0.4.0,two-letter language code wiki embeddings
v0.4.0,two-letter language code wiki embeddings
v0.4.0,two-letter language code crawl embeddings
v0.4.0,"the default model for ELMo is the 'original' model, which is very large"
v0.4.0,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.4.0,put on Cuda if available
v0.4.0,embed a dummy sentence to determine embedding_length
v0.4.0,use list of common characters if none provided
v0.4.0,translate words in sentence into ints using dictionary
v0.4.0,"sort words by length, for batching and masking"
v0.4.0,chars for rnn processing
v0.4.0,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.0,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.0,"multilingual forward fast (English, German, French, Italian, Dutch, Polish)"
v0.4.0,"multilingual backward fast (English, German, French, Italian, Dutch, Polish)"
v0.4.0,news-english-forward
v0.4.0,news-english-backward
v0.4.0,news-english-forward
v0.4.0,news-english-backward
v0.4.0,mix-english-forward
v0.4.0,mix-english-backward
v0.4.0,mix-german-forward
v0.4.0,mix-german-backward
v0.4.0,common crawl Polish forward
v0.4.0,common crawl Polish backward
v0.4.0,Slovenian forward
v0.4.0,Slovenian backward
v0.4.0,Bulgarian forward
v0.4.0,Bulgarian backward
v0.4.0,Dutch forward
v0.4.0,Dutch backward
v0.4.0,Swedish forward
v0.4.0,Swedish backward
v0.4.0,French forward
v0.4.0,French backward
v0.4.0,Czech forward
v0.4.0,Czech backward
v0.4.0,Portuguese forward
v0.4.0,Portuguese backward
v0.4.0,initialize cache if use_cache set
v0.4.0,embed a dummy sentence to determine embedding_length
v0.4.0,set to eval mode
v0.4.0,Copy the object's state from self.__dict__ which contains
v0.4.0,all our instance attributes. Always use the dict.copy()
v0.4.0,method to avoid modifying the original state.
v0.4.0,Remove the unpicklable entries.
v0.4.0,"if cache is used, try setting embeddings from cache first"
v0.4.0,try populating embeddings from cache
v0.4.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.0,pad strings with whitespaces to longest sentence
v0.4.0,get hidden states from language model
v0.4.0,take first or last hidden states from language model as word representation
v0.4.0,if self.tokenized_lm or token.whitespace_after:
v0.4.0,use the character language model embeddings as basis
v0.4.0,length is twice the original character LM embedding length
v0.4.0,these fields are for the embedding memory
v0.4.0,whether to add only capitalized words to memory (faster runtime and lower memory consumption)
v0.4.0,we re-compute embeddings dynamically at each epoch
v0.4.0,set the memory method
v0.4.0,memory is wiped each time we do a training run
v0.4.0,"if we keep a pooling, it needs to be updated continuously"
v0.4.0,update embedding
v0.4.0,add embeddings after updating
v0.4.0,"the default model for ELMo is the 'original' model, which is very large"
v0.4.0,"alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name"
v0.4.0,put on Cuda if available
v0.4.0,embed a dummy sentence to determine embedding_length
v0.4.0,The mask has 1 for real tokens and 0 for padding tokens. Only real
v0.4.0,tokens are attended to.
v0.4.0,Zero-pad up to the sequence length.
v0.4.0,"first, find longest sentence in batch"
v0.4.0,prepare id maps for BERT model
v0.4.0,put encoded batch through BERT model to get all hidden states of all encoder layers
v0.4.0,get aggregated embeddings for each BERT-subtoken in sentence
v0.4.0,get the current sentence object
v0.4.0,add concatenated embedding to sentence
v0.4.0,use first subword embedding if pooling operation is 'first'
v0.4.0,"otherwise, do a mean over all subwords in token"
v0.4.0,"multilingual forward (English, German, French, Italian, Dutch, Polish)"
v0.4.0,"multilingual backward  (English, German, French, Italian, Dutch, Polish)"
v0.4.0,news-english-forward
v0.4.0,news-english-backward
v0.4.0,news-english-forward
v0.4.0,news-english-backward
v0.4.0,mix-english-forward
v0.4.0,mix-english-backward
v0.4.0,mix-german-forward
v0.4.0,mix-german-backward
v0.4.0,common crawl Polish forward
v0.4.0,common crawl Polish backward
v0.4.0,Slovenian forward
v0.4.0,Slovenian backward
v0.4.0,Bulgarian forward
v0.4.0,Bulgarian backward
v0.4.0,Dutch forward
v0.4.0,Dutch backward
v0.4.0,Swedish forward
v0.4.0,Swedish backward
v0.4.0,French forward
v0.4.0,French backward
v0.4.0,Czech forward
v0.4.0,Czech backward
v0.4.0,Portuguese forward
v0.4.0,Portuguese backward
v0.4.0,initialize cache if use_cache set
v0.4.0,embed a dummy sentence to determine embedding_length
v0.4.0,set to eval mode
v0.4.0,Copy the object's state from self.__dict__ which contains
v0.4.0,all our instance attributes. Always use the dict.copy()
v0.4.0,method to avoid modifying the original state.
v0.4.0,Remove the unpicklable entries.
v0.4.0,"if cache is used, try setting embeddings from cache first"
v0.4.0,try populating embeddings from cache
v0.4.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.4.0,pad strings with whitespaces to longest sentence
v0.4.0,get hidden states from language model
v0.4.0,take first or last hidden states from language model as word representation
v0.4.0,if self.tokenized_lm or token.whitespace_after:
v0.4.0,"if only one sentence is passed, convert to list of sentence"
v0.4.0,"if only one sentence is passed, convert to list of sentence"
v0.4.0,bidirectional LSTM on top of embedding layer
v0.4.0,dropouts
v0.4.0,"first, sort sentences by number of tokens"
v0.4.0,go through each sentence in batch
v0.4.0,PADDING: pad shorter sentences out
v0.4.0,ADD TO SENTENCE LIST: add the representation
v0.4.0,--------------------------------------------------------------------
v0.4.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.4.0,--------------------------------------------------------------------
v0.4.0,--------------------------------------------------------------------
v0.4.0,FF PART
v0.4.0,--------------------------------------------------------------------
v0.4.0,use word dropout if set
v0.4.0,--------------------------------------------------------------------
v0.4.0,EXTRACT EMBEDDINGS FROM LSTM
v0.4.0,--------------------------------------------------------------------
v0.4.0,iterate over sentences
v0.4.0,"if its a forward LM, take last state"
v0.4.0,from allennlp.common.tqdm import Tqdm
v0.4.0,Remove quotes from etag
v0.4.0,"If there is an etag, it's everything after the first period"
v0.4.0,"Otherwise, use None"
v0.4.0,"URL, so get it from the cache (downloading if necessary)"
v0.4.0,"File, and it exists."
v0.4.0,"File, but it doesn't exist."
v0.4.0,Something unknown
v0.4.0,TODO(joelgrus): do we want to do checksums or anything like that?
v0.4.0,get cache path to put the file
v0.4.0,make HEAD request to check ETag
v0.4.0,add ETag to filename if it exists
v0.4.0,"etag = response.headers.get(""ETag"")"
v0.4.0,"Download to temporary file, then copy to cache dir once finished."
v0.4.0,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.4.0,GET file object
v0.4.0,These defaults are the same as the argument defaults in tqdm.
v0.4.0,State initialization
v0.4.0,Exponential moving average of gradient values
v0.4.0,Exponential moving average of squared gradient values
v0.4.0,Maintains max of all exp. moving avg. of sq. grad. values
v0.4.0,Decay the first and second moment running average coefficient
v0.4.0,Maintains the maximum of all 2nd moment running avg. till now
v0.4.0,Use the max. for normalizing running avg. of gradient
v0.4.0,conll 2000 column format
v0.4.0,conll 03 NER column format
v0.4.0,WNUT-17
v0.4.0,-- WikiNER datasets
v0.4.0,-- Universal Dependencies
v0.4.0,Germanic
v0.4.0,Romance
v0.4.0,West-Slavic
v0.4.0,South-Slavic
v0.4.0,East-Slavic
v0.4.0,Scandinavian
v0.4.0,Asian
v0.4.0,other datasets
v0.4.0,text classification format
v0.4.0,"first, try to fetch dataset online"
v0.4.0,default dataset folder is the cache root
v0.4.0,get string value if enum is passed
v0.4.0,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.4.0,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.4.0,the CoNLL 03 task for German has an additional lemma column
v0.4.0,the CoNLL 03 task for Dutch has no NP column
v0.4.0,the CoNLL 03 task for Spanish only has two columns
v0.4.0,the GERMEVAL task only has two columns: text and ner
v0.4.0,WSD tasks may be put into this column format
v0.4.0,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.4.0,"for text classifiers, we use our own special format"
v0.4.0,automatically identify train / test / dev files
v0.4.0,"if no test file is found, take any file with 'test' in name"
v0.4.0,get train and test data
v0.4.0,"read in test file if exists, otherwise sample 10% of train data as test dataset"
v0.4.0,"read in dev file if exists, otherwise sample 10% of train data as dev dataset"
v0.4.0,convert tag scheme to iobes
v0.4.0,automatically identify train / test / dev files
v0.4.0,automatically identify train / test / dev files
v0.4.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.4.0,conll 2000 chunking task
v0.4.0,Wikiner NER task
v0.4.0,unpack and write out in CoNLL column-like format
v0.4.0,CoNLL 02/03 NER
v0.4.0,universal dependencies
v0.4.0,--- UD Germanic
v0.4.0,--- UD Romance
v0.4.0,--- UD West-Slavic
v0.4.0,--- UD Scandinavian
v0.4.0,--- UD South-Slavic
v0.4.0,--- UD Asian
v0.4.0,init dictionaries
v0.4.0,"in order to deal with unknown tokens, add <unk>"
v0.4.0,"if text is passed, instantiate sentence with tokens (words)"
v0.4.0,tokenize the text first if option selected
v0.4.0,use segtok for tokenization
v0.4.0,determine offsets for whitespace_after field
v0.4.0,otherwise assumes whitespace tokenized text
v0.4.0,add each word in tokenized string as Token object to Sentence
v0.4.0,increment for last token in sentence if not followed by whtespace
v0.4.0,set token idx if not set
v0.4.0,non-set tags are OUT tags
v0.4.0,anything that is not a BIOES tag is a SINGLE tag
v0.4.0,anything that is not OUT is IN
v0.4.0,single and begin tags start a new span
v0.4.0,remember previous tag
v0.4.0,infer whitespace after field
v0.4.0,Make the tag dictionary
v0.4.0,Make the tag dictionary
v0.4.0,header for 'weights.txt'
v0.4.0,"determine the column index of loss, f-score and accuracy for train, dev and test split"
v0.4.0,then get all relevant values from the tsv
v0.4.0,then get all relevant values from the tsv
v0.4.0,plot i
v0.4.0,save plots
v0.4.0,plot 1
v0.4.0,plot 2
v0.4.0,plot 3
v0.4.0,save plots
v0.4.0,save plot
v0.4.0,take the average over the last three scores of training
v0.4.0,take average over the scores from the different training runs
v0.4.0,auto-spawn on GPU if available
v0.4.0,set the dictionaries
v0.4.0,initialize the network architecture
v0.4.0,dropouts
v0.4.0,bidirectional LSTM on top of embedding layer
v0.4.0,final linear map to tag space
v0.4.0,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.4.0,serialization of torch objects
v0.4.0,https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
v0.4.0,remove previous embeddings
v0.4.0,make mini-batches
v0.4.0,"first, sort sentences by number of tokens"
v0.4.0,initialize zero-padded word embeddings tensor
v0.4.0,fill values with word embeddings
v0.4.0,get the tags in this sentence
v0.4.0,add tags as tensor
v0.4.0,--------------------------------------------------------------------
v0.4.0,FF PART
v0.4.0,--------------------------------------------------------------------
v0.4.0,word dropout only before LSTM - TODO: more experimentation needed
v0.4.0,if self.use_word_dropout > 0.0:
v0.4.0,sentence_tensor = self.word_dropout(sentence_tensor)
v0.4.0,pad tags if using batch-CRF decoder
v0.4.0,auto-spawn on GPU if available
v0.4.0,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.4.0,serialization of torch objects
v0.4.0,https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
v0.4.0,cast string to Path
v0.4.0,annealing scheduler
v0.4.0,"if training also uses dev data, include in training set"
v0.4.0,At any point you can hit Ctrl + C to break out of training early.
v0.4.0,reload last best model if annealing with restarts is enabled
v0.4.0,stop training if learning rate becomes too small
v0.4.0,calculate scores using dev data if available
v0.4.0,append dev score to score history
v0.4.0,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.4.0,"if checkpoint is enable, save model at each epoch"
v0.4.0,"if we use dev data, remember best model based on dev evaluation score"
v0.4.0,"if we do not use dev data for model selection, save final model"
v0.4.0,test best model on test data
v0.4.0,"if we are training over multiple datasets, do evaluation for each"
v0.4.0,get and return the final test score of best model
v0.4.0,append both to file for evaluation
v0.4.0,make list of gold tags
v0.4.0,make list of predicted tags
v0.4.0,"check for true positives, false positives and false negatives"
v0.4.0,cast string to Path
v0.4.0,
v0.4.0,Add chars to the dictionary
v0.4.0,charsplit file content
v0.4.0,charsplit file content
v0.4.0,Add words to the dictionary
v0.4.0,Tokenize file content
v0.4.0,cast string to Path
v0.4.0,"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
v0.4.0,"after pass over all splits, increment epoch count"
v0.4.0,go into train mode
v0.4.0,reset variables
v0.4.0,not really sure what this does
v0.4.0,do batches
v0.4.0,"Starting each batch, we detach the hidden state from how it was previously produced."
v0.4.0,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.4.0,do the forward pass in the model
v0.4.0,try to predict the targets
v0.4.0,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.4.0,##############################################################################
v0.4.0,TEST
v0.4.0,##############################################################################
v0.4.0,Save the model if the validation loss is the best we've seen so far.
v0.4.0,##############################################################################
v0.4.0,print info
v0.4.0,##############################################################################
v0.4.0,##############################################################################
v0.4.0,final testing
v0.4.0,##############################################################################
v0.4.0,Turn on evaluation mode which disables dropout.
v0.4.0,Work out how cleanly we can divide the dataset into bsz parts.
v0.4.0,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.4.0,Evenly divide the data across the bsz batches.
v0.4.0,clean up file
v0.4.0,bioes tags
v0.4.0,bio tags
v0.4.0,broken tags
v0.4.0,all tags
v0.4.0,all weird tags
v0.4.0,tags with confidence
v0.4.0,bioes tags
v0.4.0,bioes tags
v0.4.0,clean up directory
v0.4.0,clean up directory
v0.4.0,clean up directory
v0.4.0,clean up directory
v0.4.0,clean up directory
v0.4.0,"get training, test and dev data"
v0.4.0,"get training, test and dev data"
v0.4.0,"get training, test and dev data"
v0.4.0,"get training, test and dev data"
v0.4.0,"get training, test and dev data"
v0.4.0,"get training, test and dev data"
v0.4.0,"get training, test and dev data"
v0.4.0,get two corpora as one
v0.4.0,"get training, test and dev data for full English UD corpus from web"
v0.4.0,clean up data directory
v0.4.0,initialize trainer
v0.4.0,clean up results directory
v0.4.0,initialize trainer
v0.4.0,clean up results directory
v0.4.0,make a temporary cache directory that we remove afterwards
v0.4.0,initialize trainer
v0.4.0,remove the cache directory
v0.4.0,clean up results directory
v0.4.0,initialize trainer
v0.4.0,clean up results directory
v0.4.0,initialize trainer
v0.4.0,clean up results directory
v0.4.0,initialize trainer
v0.4.0,clean up results directory
v0.4.0,initialize trainer
v0.4.0,clean up results directory
v0.4.0,clean up results directory
v0.4.0,clean up results directory
v0.4.0,clean up results directory
v0.4.0,get default dictionary
v0.4.0,init forward LM with 128 hidden states and 1 layer
v0.4.0,get the example corpus and process at character level in forward direction
v0.4.0,train the language model
v0.4.0,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.4.0,clean up results directory
v0.4.0,initialize trainer
v0.4.0,clean up results directory
v0.4.0,clean up results directory
v0.4.0,clean up results directory
v0.4.0,get default dictionary
v0.4.0,init forward LM with 128 hidden states and 1 layer
v0.4.0,get the example corpus and process at character level in forward direction
v0.4.0,train the language model
v0.4.0,clean up results directory
v0.4.0,get default dictionary
v0.4.0,get the example corpus and process at character level in forward direction
v0.4.0,define search space
v0.4.0,sequence tagger parameter
v0.4.0,model trainer parameter
v0.4.0,training parameter
v0.4.0,find best parameter settings
v0.4.0,clean up results directory
v0.4.0,document embeddings parameter
v0.4.0,training parameter
v0.4.0,clean up results directory
v0.3.2,1. get the corpus
v0.3.2,2. what tag do we want to predict?
v0.3.2,3. make the tag dictionary from the corpus
v0.3.2,initialize embeddings
v0.3.2,comment in this line to use character embeddings
v0.3.2,"CharacterEmbeddings(),"
v0.3.2,comment in these lines to use contextual string embeddings
v0.3.2,
v0.3.2,"CharLMEmbeddings('news-forward'),"
v0.3.2,
v0.3.2,"CharLMEmbeddings('news-backward'),"
v0.3.2,initialize sequence tagger
v0.3.2,initialize trainer
v0.3.2,"if only one sentence is passed, convert to list of sentence"
v0.3.2,IMPORTANT: add embeddings as torch modules
v0.3.2,"if only one sentence is passed, convert to list of sentence"
v0.3.2,GLOVE embeddings
v0.3.2,KOMNIOS embeddings
v0.3.2,FT-CRAWL embeddings
v0.3.2,FT-CRAWL embeddings
v0.3.2,other language fasttext embeddings
v0.3.2,add label if in training mode
v0.3.2,use list of common characters if none provided
v0.3.2,translate words in sentence into ints using dictionary
v0.3.2,"sort words by length, for batching and masking"
v0.3.2,chars for rnn processing
v0.3.2,news-english-forward
v0.3.2,news-english-backward
v0.3.2,news-english-forward
v0.3.2,news-english-backward
v0.3.2,mix-english-forward
v0.3.2,mix-english-backward
v0.3.2,mix-german-forward
v0.3.2,mix-german-backward
v0.3.2,common crawl Polish forward
v0.3.2,common crawl Polish backward
v0.3.2,Slovenian forward
v0.3.2,Slovenian backward
v0.3.2,Bulgarian forward
v0.3.2,Bulgarian backward
v0.3.2,caching variables
v0.3.2,set to eval mode
v0.3.2,Copy the object's state from self.__dict__ which contains
v0.3.2,all our instance attributes. Always use the dict.copy()
v0.3.2,method to avoid modifying the original state.
v0.3.2,Remove the unpicklable entries.
v0.3.2,this whole block is for compatibility with older serialized models  TODO: remove in version 0.4
v0.3.2,"if cache is used, try setting embeddings from cache first"
v0.3.2,lazy initialization of cache
v0.3.2,try populating embeddings from cache
v0.3.2,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.3.2,pad strings with whitespaces to longest sentence
v0.3.2,get hidden states from language model
v0.3.2,take first or last hidden states from language model as word representation
v0.3.2,if self.tokenized_lm or token.whitespace_after:
v0.3.2,"if only one sentence is passed, convert to list of sentence"
v0.3.2,"if only one sentence is passed, convert to list of sentence"
v0.3.2,bidirectional LSTM on top of embedding layer
v0.3.2,dropouts
v0.3.2,"first, sort sentences by number of tokens"
v0.3.2,go through each sentence in batch
v0.3.2,PADDING: pad shorter sentences out
v0.3.2,ADD TO SENTENCE LIST: add the representation
v0.3.2,--------------------------------------------------------------------
v0.3.2,GET REPRESENTATION FOR ENTIRE BATCH
v0.3.2,--------------------------------------------------------------------
v0.3.2,--------------------------------------------------------------------
v0.3.2,FF PART
v0.3.2,--------------------------------------------------------------------
v0.3.2,use word dropout if set
v0.3.2,--------------------------------------------------------------------
v0.3.2,EXTRACT EMBEDDINGS FROM LSTM
v0.3.2,--------------------------------------------------------------------
v0.3.2,iterate over sentences
v0.3.2,"if its a forward LM, take last state"
v0.3.2,from allennlp.common.tqdm import Tqdm
v0.3.2,Remove quotes from etag
v0.3.2,"If there is an etag, it's everything after the first period"
v0.3.2,"Otherwise, use None"
v0.3.2,"URL, so get it from the cache (downloading if necessary)"
v0.3.2,"File, and it exists."
v0.3.2,"File, but it doesn't exist."
v0.3.2,Something unknown
v0.3.2,TODO(joelgrus): do we want to do checksums or anything like that?
v0.3.2,get cache path to put the file
v0.3.2,make HEAD request to check ETag
v0.3.2,add ETag to filename if it exists
v0.3.2,"etag = response.headers.get(""ETag"")"
v0.3.2,"Download to temporary file, then copy to cache dir once finished."
v0.3.2,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.3.2,GET file object
v0.3.2,These defaults are the same as the argument defaults in tqdm.
v0.3.2,conll column format
v0.3.2,conll-u format
v0.3.2,text classification format
v0.3.2,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.3.2,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.3.2,the CoNLL 03 task for German has an additional lemma column
v0.3.2,the GERMEVAL task only has two columns: text and ner
v0.3.2,WSD tasks may be put into this column format
v0.3.2,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.3.2,"get train, test and dev data"
v0.3.2,"get train, test and dev data"
v0.3.2,"get train, test and dev data"
v0.3.2,"get train, test and dev data"
v0.3.2,"for text classifiers, we use our own special format"
v0.3.2,"for text classifiers, we use our own special format"
v0.3.2,"TODO: move all paths to use pathlib.Path, for now convert to str for compatibility"
v0.3.2,get train and test data
v0.3.2,sample dev data from train
v0.3.2,convert tag scheme to iobes
v0.3.2,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.3.2,init dictionaries
v0.3.2,"in order to deal with unknown tokens, add <unk>"
v0.3.2,"if text is passed, instantiate sentence with tokens (words)"
v0.3.2,tokenize the text first if option selected
v0.3.2,use segtok for tokenization
v0.3.2,determine offsets for whitespace_after field
v0.3.2,otherwise assumes whitespace tokenized text
v0.3.2,add each word in tokenized string as Token object to Sentence
v0.3.2,set token idx if not set
v0.3.2,non-set tags are OUT tags
v0.3.2,anything that is not a BIOES tag is a SINGLE tag
v0.3.2,anything that is not OUT is IN
v0.3.2,single and begin tags start a new span
v0.3.2,remember previous tag
v0.3.2,infer whitespace after field
v0.3.2,Make the tag dictionary
v0.3.2,print(tag)
v0.3.2,header for 'loss.tsv'
v0.3.2,header for 'weights.txt'
v0.3.2,plot i
v0.3.2,save plots
v0.3.2,plot 1
v0.3.2,plot 2
v0.3.2,plot 3
v0.3.2,save plots
v0.3.2,auto-spawn on GPU if available
v0.3.2,initial hidden state
v0.3.2,set the dictionaries
v0.3.2,initialize the network architecture
v0.3.2,dropouts
v0.3.2,bidirectional LSTM on top of embedding layer
v0.3.2,final linear map to tag space
v0.3.2,suppress torch warnings:
v0.3.2,https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
v0.3.2,"first, sort sentences by number of tokens"
v0.3.2,initialize zero-padded word embeddings tensor
v0.3.2,fill values with word embeddings
v0.3.2,get the tags in this sentence
v0.3.2,add tags as tensor
v0.3.2,--------------------------------------------------------------------
v0.3.2,FF PART
v0.3.2,--------------------------------------------------------------------
v0.3.2,word dropout only before LSTM - TODO: more experimentation needed
v0.3.2,if self.use_word_dropout > 0.0:
v0.3.2,sentence_tensor = self.word_dropout(sentence_tensor)
v0.3.2,pad tags if using batch-CRF decoder
v0.3.2,remove previous embeddings
v0.3.2,make mini-batches
v0.3.2,get the predicted tag
v0.3.2,auto-spawn on GPU if available
v0.3.2,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.3.2,serialization of torch objects
v0.3.2,https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings
v0.3.2,annealing scheduler
v0.3.2,"if training also uses dev data, include in training set"
v0.3.2,At any point you can hit Ctrl + C to break out of training early.
v0.3.2,reload last best model if annealing with restarts is enabled
v0.3.2,stop training if learning rate becomes too small
v0.3.2,"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
v0.3.2,switch to eval mode
v0.3.2,"if checkpointing is enable, save model at each epoch"
v0.3.2,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.3.2,logging info
v0.3.2,"if we use dev data, remember best model based on dev evaluation score"
v0.3.2,"if we do not use dev data for model selection, save final model"
v0.3.2,get the predicted tag
v0.3.2,add predicted tags
v0.3.2,append both to file for evaluation
v0.3.2,make list of gold tags
v0.3.2,make list of predicted tags
v0.3.2,"check for true positives, false positives and false negatives"
v0.3.2,
v0.3.2,Add chars to the dictionary
v0.3.2,charsplit file content
v0.3.2,charsplit file content
v0.3.2,Add words to the dictionary
v0.3.2,Tokenize file content
v0.3.2,"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
v0.3.2,"after pass over all splits, increment epoch count"
v0.3.2,go into train mode
v0.3.2,reset variables
v0.3.2,not really sure what this does
v0.3.2,do batches
v0.3.2,"Starting each batch, we detach the hidden state from how it was previously produced."
v0.3.2,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.3.2,do the forward pass in the model
v0.3.2,try to predict the targets
v0.3.2,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.3.2,##############################################################################
v0.3.2,TEST
v0.3.2,##############################################################################
v0.3.2,Save the model if the validation loss is the best we've seen so far.
v0.3.2,##############################################################################
v0.3.2,print info
v0.3.2,##############################################################################
v0.3.2,##############################################################################
v0.3.2,final testing
v0.3.2,##############################################################################
v0.3.2,Turn on evaluation mode which disables dropout.
v0.3.2,Work out how cleanly we can divide the dataset into bsz parts.
v0.3.2,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.3.2,Evenly divide the data across the bsz batches.
v0.3.2,"if training also uses dev data, include in training set"
v0.3.2,At any point you can hit Ctrl + C to break out of training early.
v0.3.2,reload last best model if annealing with restarts is enabled
v0.3.2,stop training if learning rate becomes too small
v0.3.2,"if checkpoint is enable, save model at each epoch"
v0.3.2,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.3.2,"if we use dev data, remember best model based on dev evaluation score"
v0.3.2,clean up file
v0.3.2,bioes tags
v0.3.2,bio tags
v0.3.2,broken tags
v0.3.2,all tags
v0.3.2,all weird tags
v0.3.2,tags with confidence
v0.3.2,bioes tags
v0.3.2,bioes tags
v0.3.2,clean up directory
v0.3.2,clean up directory
v0.3.2,clean up directory
v0.3.2,clean up directory
v0.3.2,clean up directory
v0.3.2,"get training, test and dev data"
v0.3.2,"get training, test and dev data"
v0.3.2,"get training, test and dev data"
v0.3.2,"get training, test and dev data"
v0.3.2,"get training, test and dev data"
v0.3.2,initialize trainer
v0.3.2,clean up results directory
v0.3.2,initialize trainer
v0.3.2,clean up results directory
v0.3.2,make a temporary cache directory that we remove afterwards
v0.3.2,initialize trainer
v0.3.2,remove the cache directory
v0.3.2,clean up results directory
v0.3.2,initialize trainer
v0.3.2,clean up results directory
v0.3.2,clean up results directory
v0.3.2,clean up results directory
v0.3.2,clean up results directory
v0.3.2,get default dictionary
v0.3.2,init forward LM with 128 hidden states and 1 layer
v0.3.2,get the example corpus and process at character level in forward direction
v0.3.2,train the language model
v0.3.2,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.3.2,clean up results directory
v0.3.1,1. get the corpus
v0.3.1,2. what tag do we want to predict?
v0.3.1,3. make the tag dictionary from the corpus
v0.3.1,initialize embeddings
v0.3.1,comment in this line to use character embeddings
v0.3.1,"CharacterEmbeddings(),"
v0.3.1,comment in these lines to use contextual string embeddings
v0.3.1,
v0.3.1,"CharLMEmbeddings('news-forward'),"
v0.3.1,
v0.3.1,"CharLMEmbeddings('news-backward'),"
v0.3.1,initialize sequence tagger
v0.3.1,initialize trainer
v0.3.1,"if only one sentence is passed, convert to list of sentence"
v0.3.1,IMPORTANT: add embeddings as torch modules
v0.3.1,"if only one sentence is passed, convert to list of sentence"
v0.3.1,GLOVE embeddings
v0.3.1,twitter embeddings
v0.3.1,KOMNIOS embeddings
v0.3.1,NUMBERBATCH embeddings
v0.3.1,FT-CRAWL embeddings
v0.3.1,FT-CRAWL embeddings
v0.3.1,GERMAN FASTTEXT embeddings
v0.3.1,NUMBERBATCH embeddings
v0.3.1,SWEDISCH FASTTEXT embeddings
v0.3.1,add label if in training mode
v0.3.1,use list of common characters if none provided
v0.3.1,translate words in sentence into ints using dictionary
v0.3.1,"sort words by length, for batching and masking"
v0.3.1,chars for rnn processing
v0.3.1,news-english-forward
v0.3.1,news-english-backward
v0.3.1,news-english-forward
v0.3.1,news-english-backward
v0.3.1,mix-english-forward
v0.3.1,mix-english-backward
v0.3.1,mix-german-forward
v0.3.1,mix-german-backward
v0.3.1,common crawl Polish forward
v0.3.1,common crawl Polish backward
v0.3.1,caching variables
v0.3.1,Copy the object's state from self.__dict__ which contains
v0.3.1,all our instance attributes. Always use the dict.copy()
v0.3.1,method to avoid modifying the original state.
v0.3.1,Remove the unpicklable entries.
v0.3.1,this whole block is for compatibility with older serialized models  TODO: remove in version 0.4
v0.3.1,"if cache is used, try setting embeddings from cache first"
v0.3.1,lazy initialization of cache
v0.3.1,try populating embeddings from cache
v0.3.1,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.3.1,pad strings with whitespaces to longest sentence
v0.3.1,get hidden states from language model
v0.3.1,take first or last hidden states from language model as word representation
v0.3.1,if self.tokenized_lm or token.whitespace_after:
v0.3.1,"if only one sentence is passed, convert to list of sentence"
v0.3.1,bidirectional LSTM on top of embedding layer
v0.3.1,dropouts
v0.3.1,"first, sort sentences by number of tokens"
v0.3.1,go through each sentence in batch
v0.3.1,PADDING: pad shorter sentences out
v0.3.1,ADD TO SENTENCE LIST: add the representation
v0.3.1,--------------------------------------------------------------------
v0.3.1,GET REPRESENTATION FOR ENTIRE BATCH
v0.3.1,--------------------------------------------------------------------
v0.3.1,--------------------------------------------------------------------
v0.3.1,FF PART
v0.3.1,--------------------------------------------------------------------
v0.3.1,use word dropout if set
v0.3.1,--------------------------------------------------------------------
v0.3.1,EXTRACT EMBEDDINGS FROM LSTM
v0.3.1,--------------------------------------------------------------------
v0.3.1,iterate over sentences
v0.3.1,"if its a forward LM, take last state"
v0.3.1,from allennlp.common.tqdm import Tqdm
v0.3.1,Remove quotes from etag
v0.3.1,"If there is an etag, it's everything after the first period"
v0.3.1,"Otherwise, use None"
v0.3.1,"URL, so get it from the cache (downloading if necessary)"
v0.3.1,"File, and it exists."
v0.3.1,"File, but it doesn't exist."
v0.3.1,Something unknown
v0.3.1,TODO(joelgrus): do we want to do checksums or anything like that?
v0.3.1,get cache path to put the file
v0.3.1,make HEAD request to check ETag
v0.3.1,add ETag to filename if it exists
v0.3.1,"etag = response.headers.get(""ETag"")"
v0.3.1,"Download to temporary file, then copy to cache dir once finished."
v0.3.1,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.3.1,GET file object
v0.3.1,These defaults are the same as the argument defaults in tqdm.
v0.3.1,conll column format
v0.3.1,conll-u format
v0.3.1,text classification format
v0.3.1,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.3.1,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.3.1,the CoNLL 03 task for German has an additional lemma column
v0.3.1,the GERMEVAL task only has two columns: text and ner
v0.3.1,WSD tasks may be put into this column format
v0.3.1,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.3.1,"get train, test and dev data"
v0.3.1,"get train, test and dev data"
v0.3.1,"get train, test and dev data"
v0.3.1,"get train, test and dev data"
v0.3.1,"for text classifiers, we use our own special format"
v0.3.1,"for text classifiers, we use our own special format"
v0.3.1,get train and test data
v0.3.1,sample dev data from train
v0.3.1,convert tag scheme to iobes
v0.3.1,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.3.1,init dictionaries
v0.3.1,"in order to deal with unknown tokens, add <unk>"
v0.3.1,"if text is passed, instantiate sentence with tokens (words)"
v0.3.1,tokenize the text first if option selected
v0.3.1,use segtok for tokenization
v0.3.1,determine offsets for whitespace_after field
v0.3.1,otherwise assumes whitespace tokenized text
v0.3.1,add each word in tokenized string as Token object to Sentence
v0.3.1,set token idx if not set
v0.3.1,non-set tags are OUT tags
v0.3.1,anything that is not a BIOES tag is a SINGLE tag
v0.3.1,anything that is not OUT is IN
v0.3.1,single and begin tags start a new span
v0.3.1,remember previous tag
v0.3.1,infer whitespace after field
v0.3.1,Make the tag dictionary
v0.3.1,print(tag)
v0.3.1,header for 'loss.tsv'
v0.3.1,header for 'weights.txt'
v0.3.1,plot i
v0.3.1,save plots
v0.3.1,plot 1
v0.3.1,plot 2
v0.3.1,plot 3
v0.3.1,save plots
v0.3.1,auto-spawn on GPU if available
v0.3.1,set the dictionaries
v0.3.1,initialize the network architecture
v0.3.1,dropouts
v0.3.1,bidirectional LSTM on top of embedding layer
v0.3.1,final linear map to tag space
v0.3.1,"first, sort sentences by number of tokens"
v0.3.1,initialize zero-padded word embeddings tensor
v0.3.1,fill values with word embeddings
v0.3.1,get the tags in this sentence
v0.3.1,add tags as tensor
v0.3.1,--------------------------------------------------------------------
v0.3.1,FF PART
v0.3.1,--------------------------------------------------------------------
v0.3.1,use word dropout if set
v0.3.1,pad tags if using batch-CRF decoder
v0.3.1,remove previous embeddings
v0.3.1,make mini-batches
v0.3.1,get the predicted tag
v0.3.1,auto-spawn on GPU if available
v0.3.1,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.3.1,serialization of torch objects
v0.3.1,annealing scheduler
v0.3.1,"if training also uses dev data, include in training set"
v0.3.1,At any point you can hit Ctrl + C to break out of training early.
v0.3.1,reload last best model if annealing with restarts is enabled
v0.3.1,stop training if learning rate becomes too small
v0.3.1,"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
v0.3.1,switch to eval mode
v0.3.1,"if checkpointing is enable, save model at each epoch"
v0.3.1,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.3.1,logging info
v0.3.1,"if we use dev data, remember best model based on dev evaluation score"
v0.3.1,"if we do not use dev data for model selection, save final model"
v0.3.1,get the predicted tag
v0.3.1,add predicted tags
v0.3.1,append both to file for evaluation
v0.3.1,make list of gold tags
v0.3.1,make list of predicted tags
v0.3.1,"check for true positives, false positives and false negatives"
v0.3.1,
v0.3.1,Add chars to the dictionary
v0.3.1,charsplit file content
v0.3.1,charsplit file content
v0.3.1,Add words to the dictionary
v0.3.1,Tokenize file content
v0.3.1,"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
v0.3.1,"after pass over all splits, increment epoch count"
v0.3.1,go into train mode
v0.3.1,reset variables
v0.3.1,not really sure what this does
v0.3.1,do batches
v0.3.1,"Starting each batch, we detach the hidden state from how it was previously produced."
v0.3.1,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.3.1,do the forward pass in the model
v0.3.1,try to predict the targets
v0.3.1,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.3.1,##############################################################################
v0.3.1,TEST
v0.3.1,##############################################################################
v0.3.1,Save the model if the validation loss is the best we've seen so far.
v0.3.1,##############################################################################
v0.3.1,print info
v0.3.1,##############################################################################
v0.3.1,Turn on evaluation mode which disables dropout.
v0.3.1,Work out how cleanly we can divide the dataset into bsz parts.
v0.3.1,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.3.1,Evenly divide the data across the bsz batches.
v0.3.1,"if training also uses dev data, include in training set"
v0.3.1,At any point you can hit Ctrl + C to break out of training early.
v0.3.1,reload last best model if annealing with restarts is enabled
v0.3.1,stop training if learning rate becomes too small
v0.3.1,"if checkpoint is enable, save model at each epoch"
v0.3.1,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.3.1,"if we use dev data, remember best model based on dev evaluation score"
v0.3.1,clean up file
v0.3.1,with pytest.raises(ValueError):
v0.3.1,label.name = ''
v0.3.1,bioes tags
v0.3.1,bio tags
v0.3.1,broken tags
v0.3.1,all tags
v0.3.1,all weird tags
v0.3.1,tags with confidence
v0.3.1,bioes tags
v0.3.1,bioes tags
v0.3.1,get default dictionary
v0.3.1,init forward LM with 128 hidden states and 1 layer
v0.3.1,get the example corpus and process at character level in forward direction
v0.3.1,train the language model
v0.3.1,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.3.1,clean up results directory
v0.3.1,clean up directory
v0.3.1,clean up directory
v0.3.1,clean up directory
v0.3.1,clean up directory
v0.3.1,clean up directory
v0.3.1,clean up results directory
v0.3.1,clean up results directory
v0.3.1,test tagging
v0.3.1,test re-tagging
v0.3.1,"get training, test and dev data"
v0.3.1,"get training, test and dev data"
v0.3.1,"get training, test and dev data"
v0.3.1,"get training, test and dev data"
v0.3.1,"get training, test and dev data"
v0.3.1,initialize trainer
v0.3.1,clean up results directory
v0.3.1,initialize trainer
v0.3.1,clean up results directory
v0.3.1,make a temporary cache directory that we remove afterwards
v0.3.1,initialize trainer
v0.3.1,remove the cache directory
v0.3.1,clean up results directory
v0.3.1,initialize trainer
v0.3.1,clean up results directory
v0.3.1,clean up results directory
v0.3.1,clean up results directory
v0.3.1,clean up results directory
v0.3.1,initialize trainer
v0.3.1,clean up results directory
v0.3.0,1. get the corpus
v0.3.0,2. what tag do we want to predict?
v0.3.0,3. make the tag dictionary from the corpus
v0.3.0,initialize embeddings
v0.3.0,comment in this line to use character embeddings
v0.3.0,"CharacterEmbeddings(),"
v0.3.0,comment in these lines to use contextual string embeddings
v0.3.0,
v0.3.0,"CharLMEmbeddings('news-forward'),"
v0.3.0,
v0.3.0,"CharLMEmbeddings('news-backward'),"
v0.3.0,initialize sequence tagger
v0.3.0,initialize trainer
v0.3.0,"if only one sentence is passed, convert to list of sentence"
v0.3.0,IMPORTANT: add embeddings as torch modules
v0.3.0,"if only one sentence is passed, convert to list of sentence"
v0.3.0,GLOVE embeddings
v0.3.0,twitter embeddings
v0.3.0,KOMNIOS embeddings
v0.3.0,NUMBERBATCH embeddings
v0.3.0,FT-CRAWL embeddings
v0.3.0,FT-CRAWL embeddings
v0.3.0,GERMAN FASTTEXT embeddings
v0.3.0,NUMBERBATCH embeddings
v0.3.0,SWEDISCH FASTTEXT embeddings
v0.3.0,add label if in training mode
v0.3.0,use list of common characters if none provided
v0.3.0,translate words in sentence into ints using dictionary
v0.3.0,"sort words by length, for batching and masking"
v0.3.0,chars for rnn processing
v0.3.0,news-english-forward
v0.3.0,news-english-backward
v0.3.0,news-english-forward
v0.3.0,news-english-backward
v0.3.0,mix-english-forward
v0.3.0,mix-english-backward
v0.3.0,mix-german-forward
v0.3.0,mix-german-backward
v0.3.0,common crawl Polish forward
v0.3.0,common crawl Polish backward
v0.3.0,caching variables
v0.3.0,Copy the object's state from self.__dict__ which contains
v0.3.0,all our instance attributes. Always use the dict.copy()
v0.3.0,method to avoid modifying the original state.
v0.3.0,Remove the unpicklable entries.
v0.3.0,"by default, use_cache is false (for older pre-trained models TODO: remove in version 0.4)"
v0.3.0,"if cache is used, try setting embeddings from cache first"
v0.3.0,lazy initialization of cache
v0.3.0,try populating embeddings from cache
v0.3.0,"if this is not possible, use LM to generate embedding. First, get text sentences"
v0.3.0,pad strings with whitespaces to longest sentence
v0.3.0,get hidden states from language model
v0.3.0,take first or last hidden states from language model as word representation
v0.3.0,if self.tokenized_lm or token.whitespace_after:
v0.3.0,"if only one sentence is passed, convert to list of sentence"
v0.3.0,bidirectional LSTM on top of embedding layer
v0.3.0,dropouts
v0.3.0,"first, sort sentences by number of tokens"
v0.3.0,go through each sentence in batch
v0.3.0,PADDING: pad shorter sentences out
v0.3.0,ADD TO SENTENCE LIST: add the representation
v0.3.0,--------------------------------------------------------------------
v0.3.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.3.0,--------------------------------------------------------------------
v0.3.0,--------------------------------------------------------------------
v0.3.0,FF PART
v0.3.0,--------------------------------------------------------------------
v0.3.0,use word dropout if set
v0.3.0,--------------------------------------------------------------------
v0.3.0,EXTRACT EMBEDDINGS FROM LSTM
v0.3.0,--------------------------------------------------------------------
v0.3.0,iterate over sentences
v0.3.0,"if its a forward LM, take last state"
v0.3.0,from allennlp.common.tqdm import Tqdm
v0.3.0,Remove quotes from etag
v0.3.0,"If there is an etag, it's everything after the first period"
v0.3.0,"Otherwise, use None"
v0.3.0,"URL, so get it from the cache (downloading if necessary)"
v0.3.0,"File, and it exists."
v0.3.0,"File, but it doesn't exist."
v0.3.0,Something unknown
v0.3.0,TODO(joelgrus): do we want to do checksums or anything like that?
v0.3.0,get cache path to put the file
v0.3.0,make HEAD request to check ETag
v0.3.0,add ETag to filename if it exists
v0.3.0,"etag = response.headers.get(""ETag"")"
v0.3.0,"Download to temporary file, then copy to cache dir once finished."
v0.3.0,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.3.0,GET file object
v0.3.0,These defaults are the same as the argument defaults in tqdm.
v0.3.0,conll column format
v0.3.0,conll-u format
v0.3.0,text classification format
v0.3.0,"the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)"
v0.3.0,"many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag"
v0.3.0,the CoNLL 03 task for German has an additional lemma column
v0.3.0,the GERMEVAL task only has two columns: text and ner
v0.3.0,WSD tasks may be put into this column format
v0.3.0,"the UD corpora follow the CoNLL-U format, for which we have a special reader"
v0.3.0,"get train, test and dev data"
v0.3.0,"get train, test and dev data"
v0.3.0,"get train, test and dev data"
v0.3.0,"get train, test and dev data"
v0.3.0,"for text classifiers, we use our own special format"
v0.3.0,"for text classifiers, we use our own special format"
v0.3.0,get train and test data
v0.3.0,sample dev data from train
v0.3.0,convert tag scheme to iobes
v0.3.0,"most data sets have the token text in the first column, if not, pass 'text' as column"
v0.3.0,init dictionaries
v0.3.0,"in order to deal with unknown tokens, add <unk>"
v0.3.0,"if text is passed, instantiate sentence with tokens (words)"
v0.3.0,tokenize the text first if option selected
v0.3.0,use segtok for tokenization
v0.3.0,determine offsets for whitespace_after field
v0.3.0,otherwise assumes whitespace tokenized text
v0.3.0,add each word in tokenized string as Token object to Sentence
v0.3.0,set token idx if not set
v0.3.0,non-set tags are OUT tags
v0.3.0,anything that is not a BIOES tag is a SINGLE tag
v0.3.0,anything that is not OUT is IN
v0.3.0,single and begin tags start a new span
v0.3.0,remember previous tag
v0.3.0,infer whitespace after field
v0.3.0,Make the tag dictionary
v0.3.0,print(tag)
v0.3.0,header for 'loss.tsv'
v0.3.0,header for 'weights.txt'
v0.3.0,plot i
v0.3.0,save plots
v0.3.0,plot 1
v0.3.0,plot 2
v0.3.0,plot 3
v0.3.0,save plots
v0.3.0,auto-spawn on GPU if available
v0.3.0,set the dictionaries
v0.3.0,initialize the network architecture
v0.3.0,dropouts
v0.3.0,bidirectional LSTM on top of embedding layer
v0.3.0,final linear map to tag space
v0.3.0,"first, sort sentences by number of tokens"
v0.3.0,initialize zero-padded word embeddings tensor
v0.3.0,fill values with word embeddings
v0.3.0,get the tags in this sentence
v0.3.0,add tags as tensor
v0.3.0,--------------------------------------------------------------------
v0.3.0,FF PART
v0.3.0,--------------------------------------------------------------------
v0.3.0,use word dropout if set
v0.3.0,pad tags if using batch-CRF decoder
v0.3.0,remove previous embeddings
v0.3.0,make mini-batches
v0.3.0,get the predicted tag
v0.3.0,auto-spawn on GPU if available
v0.3.0,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.3.0,serialization of torch objects
v0.3.0,annealing scheduler
v0.3.0,"if training also uses dev data, include in training set"
v0.3.0,At any point you can hit Ctrl + C to break out of training early.
v0.3.0,reload last best model if annealing with restarts is enabled
v0.3.0,stop training if learning rate becomes too small
v0.3.0,"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
v0.3.0,switch to eval mode
v0.3.0,"if checkpointing is enable, save model at each epoch"
v0.3.0,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.3.0,logging info
v0.3.0,"if we use dev data, remember best model based on dev evaluation score"
v0.3.0,"if we do not use dev data for model selection, save final model"
v0.3.0,get the predicted tag
v0.3.0,add predicted tags
v0.3.0,append both to file for evaluation
v0.3.0,make list of gold tags
v0.3.0,make list of predicted tags
v0.3.0,"check for true positives, false positives and false negatives"
v0.3.0,
v0.3.0,Add chars to the dictionary
v0.3.0,charsplit file content
v0.3.0,charsplit file content
v0.3.0,Add words to the dictionary
v0.3.0,Tokenize file content
v0.3.0,"an epoch has a number, so calculate total max splits bby multiplying max_epochs with number_of_splits"
v0.3.0,"after pass over all splits, increment epoch count"
v0.3.0,go into train mode
v0.3.0,reset variables
v0.3.0,not really sure what this does
v0.3.0,do batches
v0.3.0,"Starting each batch, we detach the hidden state from how it was previously produced."
v0.3.0,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.3.0,do the forward pass in the model
v0.3.0,try to predict the targets
v0.3.0,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.3.0,##############################################################################
v0.3.0,TEST
v0.3.0,##############################################################################
v0.3.0,Save the model if the validation loss is the best we've seen so far.
v0.3.0,##############################################################################
v0.3.0,print info
v0.3.0,##############################################################################
v0.3.0,Turn on evaluation mode which disables dropout.
v0.3.0,Work out how cleanly we can divide the dataset into bsz parts.
v0.3.0,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.3.0,Evenly divide the data across the bsz batches.
v0.3.0,"if training also uses dev data, include in training set"
v0.3.0,At any point you can hit Ctrl + C to break out of training early.
v0.3.0,record overall best dev scores and best loss
v0.3.0,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.3.0,clean up file
v0.3.0,with pytest.raises(ValueError):
v0.3.0,label.name = ''
v0.3.0,bioes tags
v0.3.0,bio tags
v0.3.0,broken tags
v0.3.0,all tags
v0.3.0,all weird tags
v0.3.0,tags with confidence
v0.3.0,bioes tags
v0.3.0,bioes tags
v0.3.0,get default dictionary
v0.3.0,init forward LM with 128 hidden states and 1 layer
v0.3.0,get the example corpus and process at character level in forward direction
v0.3.0,train the language model
v0.3.0,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.3.0,clean up results directory
v0.3.0,clean up directory
v0.3.0,clean up directory
v0.3.0,clean up directory
v0.3.0,clean up directory
v0.3.0,clean up directory
v0.3.0,clean up results directory
v0.3.0,clean up results directory
v0.3.0,test tagging
v0.3.0,test re-tagging
v0.3.0,"get training, test and dev data"
v0.3.0,"get training, test and dev data"
v0.3.0,"get training, test and dev data"
v0.3.0,"get training, test and dev data"
v0.3.0,"get training, test and dev data"
v0.3.0,initialize trainer
v0.3.0,clean up results directory
v0.2.0,1. get the corpus
v0.2.0,2. what tag do we want to predict?
v0.2.0,3. make the tag dictionary from the corpus
v0.2.0,initialize embeddings
v0.2.0,comment in this line to use character embeddings
v0.2.0,"CharacterEmbeddings(),"
v0.2.0,comment in these lines to use contextual string embeddings
v0.2.0,
v0.2.0,"CharLMEmbeddings('news-forward'),"
v0.2.0,
v0.2.0,"CharLMEmbeddings('news-backward'),"
v0.2.0,initialize sequence tagger
v0.2.0,initialize trainer
v0.2.0,"if only one sentence is passed, convert to list of sentence"
v0.2.0,IMPORTANT: add embeddings as torch modules
v0.2.0,"if only one sentence is passed, convert to list of sentence"
v0.2.0,GLOVE embeddings
v0.2.0,twitter embeddings
v0.2.0,KOMNIOS embeddings
v0.2.0,NUMBERBATCH embeddings
v0.2.0,FT-CRAWL embeddings
v0.2.0,FT-CRAWL embeddings
v0.2.0,GERMAN FASTTEXT embeddings
v0.2.0,NUMBERBATCH embeddings
v0.2.0,SWEDISCH FASTTEXT embeddings
v0.2.0,use list of common characters if none provided
v0.2.0,translate words in sentence into ints using dictionary
v0.2.0,"sort words by length, for batching and masking"
v0.2.0,chars for rnn processing
v0.2.0,news-english-forward
v0.2.0,news-english-backward
v0.2.0,news-english-forward
v0.2.0,news-english-backward
v0.2.0,mix-english-forward
v0.2.0,mix-english-backward
v0.2.0,mix-english-forward
v0.2.0,mix-english-backward
v0.2.0,find longest sentence by characters
v0.2.0,get states from LM
v0.2.0,"if only one sentence is passed, convert to list of sentence"
v0.2.0,bidirectional LSTM on top of embedding layer
v0.2.0,"first, sort sentences by number of tokens"
v0.2.0,go through each sentence in batch
v0.2.0,PADDING: pad shorter sentences out
v0.2.0,ADD TO SENTENCE LIST: add the representation
v0.2.0,--------------------------------------------------------------------
v0.2.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.2.0,--------------------------------------------------------------------
v0.2.0,--------------------------------------------------------------------
v0.2.0,FF PART
v0.2.0,--------------------------------------------------------------------
v0.2.0,--------------------------------------------------------------------
v0.2.0,EXTRACT EMBEDDINGS FROM LSTM
v0.2.0,--------------------------------------------------------------------
v0.2.0,iterate over sentences
v0.2.0,"if its a forward LM, take last state"
v0.2.0,from allennlp.common.tqdm import Tqdm
v0.2.0,Remove quotes from etag
v0.2.0,"If there is an etag, it's everything after the first period"
v0.2.0,"Otherwise, use None"
v0.2.0,"URL, so get it from the cache (downloading if necessary)"
v0.2.0,"File, and it exists."
v0.2.0,"File, but it doesn't exist."
v0.2.0,Something unknown
v0.2.0,TODO(joelgrus): do we want to do checksums or anything like that?
v0.2.0,get cache path to put the file
v0.2.0,make HEAD request to check ETag
v0.2.0,add ETag to filename if it exists
v0.2.0,"etag = response.headers.get(""ETag"")"
v0.2.0,"Download to temporary file, then copy to cache dir once finished."
v0.2.0,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.2.0,GET file object
v0.2.0,These defaults are the same as the argument defaults in tqdm.
v0.2.0,print(line)
v0.2.0,print(line)
v0.2.0,init dictionaries
v0.2.0,"in order to deal with unknown tokens, add <unk>"
v0.2.0,"optionally, directly instantiate with sentence tokens"
v0.2.0,"tokenize the text first if option selected, otherwise assumes whitespace tokenized text"
v0.2.0,add each word in tokenized string as Token object to Sentence
v0.2.0,set token idx if not set
v0.2.0,"def to_tag_string(self, tag_type: str = 'tag') -> str:"
v0.2.0,
v0.2.0,list = []
v0.2.0,for token in self.tokens:
v0.2.0,list.append(token.text)
v0.2.0,if token.get_tag(tag_type) == '' or token.get_tag(tag_type) == 'O': continue
v0.2.0,list.append('<' + token.get_tag(tag_type) + '>')
v0.2.0,return ' '.join(list)
v0.2.0,
v0.2.0,def to_ner_string(self) -> str:
v0.2.0,list = []
v0.2.0,for token in self.tokens:
v0.2.0,if token.get_tag('ner') == 'O' or token.get_tag('ner') == '':
v0.2.0,list.append(token.text)
v0.2.0,else:
v0.2.0,list.append(token.text)
v0.2.0,list.append('<' + token.get_tag('ner') + '>')
v0.2.0,return ' '.join(list)
v0.2.0,Make the tag dictionary
v0.2.0,auto-spawn on GPU if available
v0.2.0,vec 2D: 1 * tagset_size
v0.2.0,set the dictionaries
v0.2.0,initialize the network architecture
v0.2.0,self.dropout = nn.Dropout(0.5)
v0.2.0,bidirectional LSTM on top of embedding layer
v0.2.0,final linear map to tag space
v0.2.0,"trans is also a score tensor, not a probability: THIS THING NEEDS TO GO!!!!"
v0.2.0,ACHTUNG: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.2.0,serialization of torch objects
v0.2.0,"first, sort sentences by number of tokens"
v0.2.0,get the tags in this sentence
v0.2.0,get the tag
v0.2.0,get the word embeddings
v0.2.0,pad shorter sentences out
v0.2.0,padded tensor for entire batch
v0.2.0,--------------------------------------------------------------------
v0.2.0,FF PART
v0.2.0,--------------------------------------------------------------------
v0.2.0,sentence_tensor = self.nonlinearity(sentence_tensor)
v0.2.0,"tags is ground_truth, a list of ints, length is len(sentence)"
v0.2.0,"feats is a 2D tensor, len(sentence) * tagset_size"
v0.2.0,analogous to forward
v0.2.0,"calculate the score of the ground_truth, in CRF"
v0.2.0,calculate in log domain
v0.2.0,feats is len(sentence) * tagset_size
v0.2.0,initialize alpha with a Tensor with values all equal to -10000.
v0.2.0,Z(x)
v0.2.0,viterbi to get tag_seq
v0.2.0,get the predicted tag
v0.2.0,remove previous embeddings
v0.2.0,make mini-batches
v0.2.0,get the predicted tag
v0.2.0,viterbi to get tag_seq
v0.2.0,overall_score += score
v0.2.0,auto-spawn on GPU if available
v0.2.0,ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.2.0,serialization of torch objects
v0.2.0,"if training also uses dev data, include in training set"
v0.2.0,At any point you can hit Ctrl + C to break out of training early.
v0.2.0,"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
v0.2.0,switch to eval mode
v0.2.0,switch back to train mode
v0.2.0,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.2.0,save if model is current best and we use dev data for model selection
v0.2.0,"if we do not use dev data for model selection, save final model"
v0.2.0,Step 3. Run our forward pass.
v0.2.0,Step 5. Compute predictions
v0.2.0,get the predicted tag
v0.2.0,get the gold tag
v0.2.0,append both to file for evaluation
v0.2.0,positives
v0.2.0,true positives
v0.2.0,false positive
v0.2.0,negatives
v0.2.0,true negative
v0.2.0,false negative
v0.2.0,get the eval script
v0.2.0,parse the result file
v0.2.0,
v0.2.0,print(chars)
v0.2.0,Add chars to the dictionary
v0.2.0,charsplit file content
v0.2.0,charsplit file content
v0.2.0,Add words to the dictionary
v0.2.0,Tokenize file content
v0.2.0,go into train mode
v0.2.0,reset variables
v0.2.0,not really sure what this does
v0.2.0,do batches
v0.2.0,"Starting each batch, we detach the hidden state from how it was previously produced."
v0.2.0,"If we didn't, the model would try backpropagating all the way to start of the dataset."
v0.2.0,do the forward pass in the model
v0.2.0,try to predict the targets
v0.2.0,`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
v0.2.0,##############################################################################
v0.2.0,TEST
v0.2.0,##############################################################################
v0.2.0,Save the model if the validation loss is the best we've seen so far.
v0.2.0,##############################################################################
v0.2.0,print info
v0.2.0,##############################################################################
v0.2.0,Turn on evaluation mode which disables dropout.
v0.2.0,Work out how cleanly we can divide the dataset into bsz parts.
v0.2.0,Trim off any extra elements that wouldn't cleanly fit (remainders).
v0.2.0,Evenly divide the data across the bsz batches.
v0.2.0,"if training also uses dev data, include in training set"
v0.2.0,At any point you can hit Ctrl + C to break out of training early.
v0.2.0,record overall best dev scores and best loss
v0.2.0,IMPORTANT: Switch to eval mode
v0.2.0,IMPORTANT: Switch back to train mode
v0.2.0,"anneal against train loss if training with dev, otherwise anneal against dev score"
v0.2.0,clean up file
v0.2.0,get default dictionary
v0.2.0,init forward LM with 128 hidden states and 1 layer
v0.2.0,get the example corpus and process at character level in forward direction
v0.2.0,train the language model
v0.2.0,use the character LM as embeddings to embed the example sentence 'I love Berlin'
v0.2.0,clean up results directory
v0.2.0,clean up results directory
v0.2.0,clean up results directory
v0.2.0,test tagging
v0.2.0,test re-tagging
v0.2.0,"get training, test and dev data"
v0.2.0,"get training, test and dev data"
v0.2.0,"get training, test and dev data"
v0.2.0,initialize trainer
v0.2.0,clean up results directory
v0.1.0,1. get the corpus
v0.1.0,2. what tag do we want to predict?
v0.1.0,3. make the tag dictionary from the corpus
v0.1.0,initialize embeddings
v0.1.0,comment in this line to use character embeddings
v0.1.0,comment in these lines to use contextual string embeddings
v0.1.0,initialize sequence tagger
v0.1.0,initialize trainer
v0.1.0,"if only one sentence is passed, convert to list of sentence"
v0.1.0,IMPORTANT: add embeddings as torch modules
v0.1.0,GLOVE embeddings
v0.1.0,KOMNIOS embeddings
v0.1.0,NUMBERBATCH embeddings
v0.1.0,FT-CRAWL embeddings
v0.1.0,FT-CRAWL embeddings
v0.1.0,GERMAN FASTTEXT embeddings
v0.1.0,NUMBERBATCH embeddings
v0.1.0,SWEDISCH FASTTEXT embeddings
v0.1.0,get list of common characters if none provided
v0.1.0,load dictionary
v0.1.0,print(self.char_dictionary.item2idx)
v0.1.0,translate words in sentence into ints using dictionary
v0.1.0,print(token)
v0.1.0,"sort words by length, for batching and masking"
v0.1.0,chars for rnn processing
v0.1.0,news-english-forward
v0.1.0,news-english-backward
v0.1.0,mix-english-forward
v0.1.0,mix-english-backward
v0.1.0,mix-english-forward
v0.1.0,mix-english-backward
v0.1.0,find longest sentence by characters
v0.1.0,print(sentences_padded)
v0.1.0,get states from LM
v0.1.0,if not torch.cuda.is_available():
v0.1.0,embedding = embedding.cpu()
v0.1.0,lines: List[str] = []
v0.1.0,lines.append(vec)
v0.1.0,"if only one sentence is passed, convert to list of sentence"
v0.1.0,mean_embedding /= len(paragraph.tokens)
v0.1.0,self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=word_embeddings)
v0.1.0,self.__embedding_length: int = hidden_states
v0.1.0,bidirectional LSTM on top of embedding layer
v0.1.0,"first, sort sentences by number of tokens"
v0.1.0,go through each sentence in batch
v0.1.0,PADDING: pad shorter sentences out
v0.1.0,ADD TO SENTENCE LIST: add the representation
v0.1.0,--------------------------------------------------------------------
v0.1.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.1.0,--------------------------------------------------------------------
v0.1.0,--------------------------------------------------------------------
v0.1.0,FF PART
v0.1.0,--------------------------------------------------------------------
v0.1.0,iterate over sentences
v0.1.0,"if its a forward LM, take last state"
v0.1.0,from allennlp.common.tqdm import Tqdm
v0.1.0,Remove quotes from etag
v0.1.0,"If there is an etag, it's everything after the first period"
v0.1.0,"Otherwise, use None"
v0.1.0,"URL, so get it from the cache (downloading if necessary)"
v0.1.0,"File, and it exists."
v0.1.0,"File, but it doesn't exist."
v0.1.0,Something unknown
v0.1.0,TODO(joelgrus): do we want to do checksums or anything like that?
v0.1.0,get cache path to put the file
v0.1.0,make HEAD request to check ETag
v0.1.0,add ETag to filename if it exists
v0.1.0,"etag = response.headers.get(""ETag"")"
v0.1.0,"Download to temporary file, then copy to cache dir once finished."
v0.1.0,Otherwise you get corrupt cache entries if the download gets interrupted.
v0.1.0,GET file object
v0.1.0,These defaults are the same as the argument defaults in tqdm.
v0.1.0,init dictionaries
v0.1.0,"in order to deal with unknown tokens, add <unk>"
v0.1.0,"optionally, directly instantiate with sentence tokens"
v0.1.0,"tokenize the text first if option selected, otherwise assumes whitespace tokenized text"
v0.1.0,add each word in tokenized string as Token object to Sentence
v0.1.0,set token idx if not set
v0.1.0,"def to_tag_string(self, tag_type: str = 'tag') -> str:"
v0.1.0,
v0.1.0,list = []
v0.1.0,for token in self.tokens:
v0.1.0,list.append(token.text)
v0.1.0,if token.get_tag(tag_type) == '' or token.get_tag(tag_type) == 'O': continue
v0.1.0,list.append('<' + token.get_tag(tag_type) + '>')
v0.1.0,return ' '.join(list)
v0.1.0,
v0.1.0,def to_ner_string(self) -> str:
v0.1.0,list = []
v0.1.0,for token in self.tokens:
v0.1.0,if token.get_tag('ner') == 'O' or token.get_tag('ner') == '':
v0.1.0,list.append(token.text)
v0.1.0,else:
v0.1.0,list.append(token.text)
v0.1.0,list.append('<' + token.get_tag('ner') + '>')
v0.1.0,return ' '.join(list)
v0.1.0,Make the tag dictionary
v0.1.0,print(line)
v0.1.0,print(line)
v0.1.0,
v0.1.0,print(chars)
v0.1.0,Add chars to the dictionary
v0.1.0,charsplit file content
v0.1.0,charsplit file content
v0.1.0,Add words to the dictionary
v0.1.0,Tokenize file content
v0.1.0,"if training also uses dev data, include in training set"
v0.1.0,At any point you can hit Ctrl + C to break out of training early.
v0.1.0,record overall best dev scores and best loss
v0.1.0,best_dev_score = 0
v0.1.0,best_loss: float = 10000
v0.1.0,this variable is used for annealing schemes
v0.1.0,"Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()"
v0.1.0,IMPORTANT: Switch to eval mode
v0.1.0,IMPORTANT: Switch back to train mode
v0.1.0,checkpoint model
v0.1.0,is this the best model so far?
v0.1.0,"if dev data is used for model selection, use dev F1 score to determine best model"
v0.1.0,"if dev data is used for training, use training loss to determine best model"
v0.1.0,save model
v0.1.0,anneal after 3 epochs of no improvement if anneal mode
v0.1.0,print info
v0.1.0,Step 3. Run our forward pass.
v0.1.0,Step 5. Compute predictions
v0.1.0,print(token)
v0.1.0,get the predicted tag
v0.1.0,get the gold tag
v0.1.0,append both to file for evaluation
v0.1.0,parse the result file
v0.1.0,vec 2D: 1 * tagset_size
v0.1.0,set the dictionaries
v0.1.0,initialize the network architecture
v0.1.0,self.dropout = nn.Dropout(0.5)
v0.1.0,bidirectional LSTM on top of embedding layer
v0.1.0,final linear map to tag space
v0.1.0,"trans is also a score tensor, not a probability: THIS THING NEEDS TO GO!!!!"
v0.1.0,ACHTUNG: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive
v0.1.0,serialization of torch objects
v0.1.0,"first, sort sentences by number of tokens"
v0.1.0,print(sent)
v0.1.0,print(sent.tokens[0].get_embedding()[0:7])
v0.1.0,go through each sentence in batch
v0.1.0,get the tags in this sentence
v0.1.0,get the tag
v0.1.0,PADDING: pad shorter sentences out
v0.1.0,ADD TO SENTENCE LIST: add the representation
v0.1.0,--------------------------------------------------------------------
v0.1.0,GET REPRESENTATION FOR ENTIRE BATCH
v0.1.0,--------------------------------------------------------------------
v0.1.0,--------------------------------------------------------------------
v0.1.0,FF PART
v0.1.0,--------------------------------------------------------------------
v0.1.0,print(tags)
v0.1.0,"tags is ground_truth, a list of ints, length is len(sentence)"
v0.1.0,"feats is a 2D tensor, len(sentence) * tagset_size"
v0.1.0,analogous to forward
v0.1.0,"sentence, tags is a list of ints"
v0.1.0,"features is a 2D tensor, len(sentence) * self.tagset_size"
v0.1.0,for sentence in sentences:
v0.1.0,print(sentence)
v0.1.0,"calculate the score of the ground_truth, in CRF"
v0.1.0,calculate in log domain
v0.1.0,feats is len(sentence) * tagset_size
v0.1.0,initialize alpha with a Tensor with values all equal to -10000.
v0.1.0,Z(x)
v0.1.0,viterbi to get tag_seq
v0.1.0,get the predicted tag
