Version,Commit Message
v4.6.0,coding: utf-8
v4.6.0,"now that the relevant information has been pulled out of params, it's safe to overwrite it"
v4.6.0,with the content that should be used for training (i.e. with aliases resolved)
v4.6.0,"if there were not multiple boosting rounds configurations provided in params,"
v4.6.0,then by definition they cannot have conflicting values... no need to warn
v4.6.0,"if all the aliases have the same value, no need to warn"
v4.6.0,"if this line is reached, lightgbm should warn"
v4.6.0,create predictor first
v4.6.0,setting early stopping via global params should be possible
v4.6.0,reduce cost for prediction training data
v4.6.0,process callbacks
v4.6.0,construct booster
v4.6.0,start training
v4.6.0,check evaluation result.
v4.6.0,"ranking task, split according to groups"
v4.6.0,run preprocessing on the data set if needed
v4.6.0,"build up 2 maps, of the form:"
v4.6.0,
v4.6.0,OrderedDict{
v4.6.0,"(<dataset_name>, <metric_name>): <is_higher_better>"
v4.6.0,}
v4.6.0,
v4.6.0,OrderedDict{
v4.6.0,"(<dataset_name>, <metric_name>): list[<metric_value>]"
v4.6.0,}
v4.6.0,
v4.6.0,turn that into a list of tuples of the form:
v4.6.0,
v4.6.0,[
v4.6.0,"(<dataset_name>, <metric_name>, mean(<values>), <is_higher_better>, std_dev(<values>))"
v4.6.0,]
v4.6.0,setting early stopping via global params should be possible
v4.6.0,setup callbacks
v4.6.0,coding: utf-8
v4.6.0,"scikit-learn is intentionally imported first here,"
v4.6.0,see https://github.com/microsoft/LightGBM/issues/6509
v4.6.0,dummy function to support older version of scikit-learn
v4.6.0,"validate_data() was added in scikit-learn 1.6, this function roughly imitates it for older versions."
v4.6.0,It can be removed when lightgbm's minimum scikit-learn version is at least 1.6.
v4.6.0,'force_all_finite' was renamed to 'ensure_all_finite' in scikit-learn 1.6
v4.6.0,"trap other keyword arguments that only work on scikit-learn >=1.6, like 'reset'"
v4.6.0,it's safe to import _num_features unconditionally because:
v4.6.0,
v4.6.0,* it was first added in scikit-learn 0.24.2
v4.6.0,* lightgbm cannot be used with scikit-learn versions older than that
v4.6.0,* this validate_data() re-implementation will not be called in scikit-learn>=1.6
v4.6.0,
v4.6.0,_num_features() raises a TypeError on 1-dimensional input. That's a problem
v4.6.0,because scikit-learn's 'check_fit1d' estimator check sets that expectation that
v4.6.0,estimators must raise a ValueError when a 1-dimensional input is passed to fit().
v4.6.0,
v4.6.0,"So here, lightgbm avoids calling _num_features() on 1-dimensional inputs."
v4.6.0,"NOTE: check_X_y() calls check_array() internally, so only need to call one or the other of them here"
v4.6.0,this only needs to be updated at fit() time
v4.6.0,raise the same error that scikit-learn's `validate_data()` does on scikit-learn>=1.6
v4.6.0,additional scikit-learn imports only for type hints
v4.6.0,sklearn.utils.Tags can be imported unconditionally once
v4.6.0,lightgbm's minimum scikit-learn version is 1.6 or higher
v4.6.0,catching 'ValueError' here because of this:
v4.6.0,https://github.com/microsoft/LightGBM/issues/6365#issuecomment-2002330003
v4.6.0,
v4.6.0,"That's potentially risky as dask does some significant import-time processing,"
v4.6.0,"like loading configuration from environment variables and files, and catching"
v4.6.0,ValueError here might hide issues with that config-loading.
v4.6.0,
v4.6.0,"But in exchange, it's less likely that 'import lightgbm' will fail for"
v4.6.0,"dask-related reasons, which is beneficial for any workloads that are using"
v4.6.0,lightgbm but not its Dask functionality.
v4.6.0,coding: utf-8
v4.6.0,"f(labels, preds)"
v4.6.0,"f(labels, preds, weights)"
v4.6.0,"f(labels, preds, weights, group)"
v4.6.0,"f(labels, preds)"
v4.6.0,"f(labels, preds, weights)"
v4.6.0,"f(labels, preds, weights, group)"
v4.6.0,documentation templates for LGBMModel methods are shared between the classes in
v4.6.0,this module and those in the ``dask`` module
v4.6.0,"It's possible, for example, to pass 3 eval sets through `eval_set`,"
v4.6.0,but only 1 init_score through `eval_init_score`.
v4.6.0,
v4.6.0,This if-else accounts for that possibility.
v4.6.0,scikit-learn 1.6 introduced an __sklearn__tags() method intended to replace _more_tags().
v4.6.0,_more_tags() can be removed whenever lightgbm's minimum supported scikit-learn version
v4.6.0,is >=1.6.
v4.6.0,ref: https://github.com/microsoft/LightGBM/pull/6651
v4.6.0,"""check_sample_weight_equivalence"" can be removed when lightgbm's"
v4.6.0,minimum supported scikit-learn version is at least 1.6
v4.6.0,ref: https://github.com/scikit-learn/scikit-learn/pull/30137
v4.6.0,"_LGBMModelBase.__sklearn_tags__() cannot be called unconditionally,"
v4.6.0,because that method isn't defined for scikit-learn<1.6
v4.6.0,"take whatever tags are provided by BaseEstimator, then modify"
v4.6.0,them with LightGBM-specific values
v4.6.0,Based on: https://github.com/dmlc/xgboost/blob/bd92b1c9c0db3e75ec3dfa513e1435d518bb535d/python-package/xgboost/sklearn.py#L941
v4.6.0,which was based on: https://stackoverflow.com/questions/59248211
v4.6.0,
v4.6.0,`get_params()` flows like this:
v4.6.0,
v4.6.0,"0. Get parameters in subclass (self.__class__) first, by using inspect."
v4.6.0,1. Get parameters in all parent classes (especially `LGBMModel`).
v4.6.0,2. Get whatever was passed via `**kwargs`.
v4.6.0,3. Merge them.
v4.6.0,
v4.6.0,This needs to accommodate being called recursively in the following
v4.6.0,inheritance graphs (and similar for classification and ranking):
v4.6.0,
v4.6.0,DaskLGBMRegressor -> LGBMRegressor     -> LGBMModel -> BaseEstimator
v4.6.0,(custom subclass) -> LGBMRegressor     -> LGBMModel -> BaseEstimator
v4.6.0,LGBMRegressor     -> LGBMModel -> BaseEstimator
v4.6.0,(custom subclass) -> LGBMModel -> BaseEstimator
v4.6.0,LGBMModel -> BaseEstimator
v4.6.0,
v4.6.0,"If the immediate parent defines get_params(), use that."
v4.6.0,"Otherwise, skip it and assume the next class will have it."
v4.6.0,This is here primarily for cases where the first class in MRO is a scikit-learn mixin.
v4.6.0,register default metric for consistency with callable eval_metric case
v4.6.0,try to deduce from class instance
v4.6.0,overwrite default metric by explicitly set metric
v4.6.0,"use joblib conventions for negative n_jobs, just like scikit-learn"
v4.6.0,"at predict time, this is handled later due to the order of parameter updates"
v4.6.0,Do not modify original args in fit function
v4.6.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v4.6.0,Separate built-in from callable evaluation metrics
v4.6.0,concatenate metric from params (or default if not provided in params) and eval_metric
v4.6.0,"allow any input type (this validation is done further down, in lgb.Dataset())"
v4.6.0,do not raise an error if Inf of NaN values are found (LightGBM handles these internally)
v4.6.0,raise an error on 0-row and 1-row inputs
v4.6.0,"for other data types, setting n_features_in_ is handled by _LGBMValidateData() in the branch above"
v4.6.0,reduce cost for prediction training data
v4.6.0,"This populates the property self.n_features_, the number of features in the fitted model,"
v4.6.0,and so should only be set after fitting.
v4.6.0,
v4.6.0,"The related property self._n_features_in, which populates self.n_features_in_,"
v4.6.0,is set BEFORE fitting.
v4.6.0,free dataset
v4.6.0,'y' being omitted = run scikit-learn's check_array() instead of check_X_y()
v4.6.0,
v4.6.0,Prevent scikit-learn from deleting or modifying attributes like 'feature_names_in_' and 'n_features_in_'.
v4.6.0,These shouldn't be changed at predict() time.
v4.6.0,"allow any input type (this validation is done further down, in lgb.Dataset())"
v4.6.0,do not raise an error if Inf of NaN values are found (LightGBM handles these internally)
v4.6.0,raise an error on 0-row inputs
v4.6.0,retrieve original params that possibly can be used in both training and prediction
v4.6.0,and then overwrite them (considering aliases) with params that were passed directly in prediction
v4.6.0,number of threads can have values with special meaning which is only applied
v4.6.0,"in the scikit-learn interface, these should not reach the c++ side as-is"
v4.6.0,NOTE: all args from LGBMModel.__init__() are intentionally repeated here for
v4.6.0,"docs, help(), and tab completion."
v4.6.0,handle the case where RegressorMixin possibly provides _more_tags()
v4.6.0,override those with LightGBM-specific preferences
v4.6.0,NOTE: all args from LGBMModel.__init__() are intentionally repeated here for
v4.6.0,"docs, help(), and tab completion."
v4.6.0,handle the case where ClassifierMixin possibly provides _more_tags()
v4.6.0,override those with LightGBM-specific preferences
v4.6.0,adjust eval metrics to match whether binary or multiclass
v4.6.0,classification is being performed
v4.6.0,"do not modify args, as it causes errors in model selection tools"
v4.6.0,NOTE: all args from LGBMModel.__init__() are intentionally repeated here for
v4.6.0,"docs, help(), and tab completion."
v4.6.0,check group data
v4.6.0,coding: utf-8
v4.6.0,we don't need lib_lightgbm while building docs
v4.6.0,coding: utf-8
v4.6.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v4.6.0,coding: utf-8
v4.6.0,".basic is intentionally loaded as early as possible, to dlopen() lib_lightgbm.{dll,dylib,so}"
v4.6.0,and its dependencies as early as possible
v4.6.0,coding: utf-8
v4.6.0,"This import causes lib_lightgbm.{dll,dylib,so} to be loaded."
v4.6.0,"It's intentionally done here, as early as possible, to avoid issues like"
v4.6.0,"""libgomp.so.1: cannot allocate memory in static TLS block"" on aarch64 Linux."
v4.6.0,
v4.6.0,"For details, see the ""cannot allocate memory in static TLS block"" entry in docs/FAQ.rst."
v4.6.0,typing.TypeGuard was only introduced in Python 3.10
v4.6.0,"ensure dtype and order, copies if either do not match"
v4.6.0,flatten array without copying
v4.6.0,connect the Python logger to logging in lib_lightgbm
v4.6.0,TypeError: obj is not a string or a number
v4.6.0,ValueError: invalid literal
v4.6.0,Obtain objects to export
v4.6.0,Prepare export
v4.6.0,Export all objects
v4.6.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v4.6.0,ref: https://peps.python.org/pep-0565/#additional-use-case-for-futurewarning
v4.6.0,"lazy evaluation to allow import without dynamic library, e.g., for docs generation"
v4.6.0,"if buffer length is not long enough, re-allocate a buffer"
v4.6.0,avoid side effects on passed-in parameters
v4.6.0,"if main_param_name was provided, keep that value and remove all aliases"
v4.6.0,"if main param name was not found, search for an alias"
v4.6.0,"neither of main_param_name, aliases were found"
v4.6.0,most common case (no nullable dtypes)
v4.6.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.6.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.6.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.6.0,take shallow copy in case we modify categorical columns
v4.6.0,whole column modifications don't change the original df
v4.6.0,determine feature names
v4.6.0,determine categorical features
v4.6.0,use cat cols from DataFrame
v4.6.0,so that the target dtype considers floats
v4.6.0,Get total row number.
v4.6.0,Random access by row index. Used for data sampling.
v4.6.0,Range data access. Used to read data in batch when constructing Dataset.
v4.6.0,Optionally specify batch_size to control range data read size.
v4.6.0,Only required if using ``Dataset.subset()``.
v4.6.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.6.0,avoid memory consumption by arrays concatenation operations
v4.6.0,create numpy array from output arrays
v4.6.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v4.6.0,for CSC there is extra column added
v4.6.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v4.6.0,same shape as input csr or csc matrix except extra column for expected value
v4.6.0,note: make sure we copy data as it will be deallocated next
v4.6.0,"free the temporary native indptr, indices, and data"
v4.6.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.6.0,avoid memory consumption by arrays concatenation operations
v4.6.0,Check that the input is valid: we only handle numbers (for now)
v4.6.0,Prepare prediction output array
v4.6.0,Export Arrow table to C and run prediction
v4.6.0,c type: double**
v4.6.0,each double* element points to start of each column of sample data.
v4.6.0,c type int**
v4.6.0,each int* points to start of indices for each column
v4.6.0,"no min_data, nthreads and verbose in this function"
v4.6.0,check data has header or not
v4.6.0,need to regroup init_score
v4.6.0,process for args
v4.6.0,get categorical features
v4.6.0,"If the params[cat_alias] is equal to categorical_indices, do not report the warning."
v4.6.0,process for reference dataset
v4.6.0,start construct data
v4.6.0,set feature names
v4.6.0,"Select sampled rows, transpose to column order."
v4.6.0,create validation dataset from ref_dataset
v4.6.0,Check that the input is valid: we only handle numbers (for now)
v4.6.0,Export Arrow table to C
v4.6.0,create valid
v4.6.0,construct subset
v4.6.0,create train
v4.6.0,could be updated if data is not freed
v4.6.0,set to None
v4.6.0,"If the data is a arrow data, we can just pass it to C"
v4.6.0,"If a table is being passed, we concatenate the columns. This is only valid for"
v4.6.0,'init_score'.
v4.6.0,we're done if self and reference share a common upstream reference
v4.6.0,Check if the weight contains values other than one
v4.6.0,Set field
v4.6.0,original values can be modified at cpp side
v4.6.0,"if buffer length is not long enough, reallocate buffers"
v4.6.0,"group data from LightGBM is boundaries data, need to convert to group size"
v4.6.0,Training task
v4.6.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v4.6.0,construct booster object
v4.6.0,copy the parameters from train_set
v4.6.0,save reference to data
v4.6.0,buffer for inner predict
v4.6.0,Prediction task
v4.6.0,"if buffer length is not long enough, re-allocate a buffer"
v4.6.0,if a single node tree it won't have `leaf_index` so return 0
v4.6.0,"Create the node record, and populate universal data members"
v4.6.0,Update values to reflect node type (leaf or split)
v4.6.0,traverse the next level of the tree
v4.6.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v4.6.0,and we add node to the list.
v4.6.0,need reset training data
v4.6.0,need to push new valid data
v4.6.0,ensure that existing Booster is freed before replacing it
v4.6.0,with a new one createdfrom file
v4.6.0,"if buffer length is not long enough, re-allocate a buffer"
v4.6.0,"if buffer length is not long enough, reallocate a buffer"
v4.6.0,Copy models
v4.6.0,Get name of features
v4.6.0,"if buffer length is not long enough, reallocate buffers"
v4.6.0,avoid to predict many time in one iteration
v4.6.0,Get num of inner evals
v4.6.0,Get name of eval metrics
v4.6.0,"if buffer length is not long enough, reallocate buffers"
v4.6.0,coding: utf-8
v4.6.0,Callback environment used by callbacks
v4.6.0,this import is here to avoid a circular import
v4.6.0,"tuples from cv() sometimes have a 5th item, with standard deviation of"
v4.6.0,the evaluation metric (taken over all cross-validation folds)
v4.6.0,"for cv(), 'metric_value' is actually a mean of metric values over all CV folds"
v4.6.0,train()
v4.6.0,cv()
v4.6.0,"CVBooster holds a list of Booster objects, each needs to be updated"
v4.6.0,"for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set"
v4.6.0,and those metrics are considered for early stopping
v4.6.0,"for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name"
v4.6.0,get details of the first dataset
v4.6.0,validation sets are guaranteed to not be identical to the training data in cv()
v4.6.0,self.best_score_list is initialized to an empty list
v4.6.0,coding: utf-8
v4.6.0,Acquire port in worker
v4.6.0,schedule futures to retrieve each element of the tuple
v4.6.0,retrieve ports
v4.6.0,Concatenate many parts into one
v4.6.0,construct local eval_set data.
v4.6.0,store indices of eval_set components that were not contained within local parts.
v4.6.0,consolidate parts of each individual eval component.
v4.6.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v4.6.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v4.6.0,filter padding from eval parts then _concat each eval_set component.
v4.6.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v4.6.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v4.6.0,capture whether local_listen_port or its aliases were provided
v4.6.0,capture whether machines or its aliases were provided
v4.6.0,Some passed-in parameters can be removed:
v4.6.0,* 'num_machines': set automatically from Dask worker list
v4.6.0,* 'num_threads': overridden to match nthreads on each Dask process
v4.6.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v4.6.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v4.6.0,X and y are each delayed sub-lists of original eval dask Collections.
v4.6.0,find maximum number of parts in an individual eval set so that we can
v4.6.0,pad eval sets when they come in different sizes.
v4.6.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v4.6.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v4.6.0,first time a chunk of this eval set is added to this part.
v4.6.0,append additional chunks of this eval set to this part.
v4.6.0,ensure that all evaluation parts map uniquely to one part.
v4.6.0,assign sub-eval_set components to worker parts.
v4.6.0,Start computation in the background
v4.6.0,trigger error locally
v4.6.0,Find locations of all parts and map them to particular Dask workers
v4.6.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v4.6.0,data artifacts may not be populated depending on worker returning final estimator.
v4.6.0,assign general validation set settings to fit kwargs.
v4.6.0,resolve aliases for network parameters and pop the result off params.
v4.6.0,these values are added back in calls to `_train_part()`
v4.6.0,figure out network params
v4.6.0,Tell each worker to train on the parts that it has locally
v4.6.0,
v4.6.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v4.6.0,1. there is randomness in the training process unless parameters ``seed``
v4.6.0,and ``deterministic`` are set
v4.6.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v4.6.0,relies on global state (it and all the other LightGBM training processes
v4.6.0,coordinate with each other)
v4.6.0,"if network parameters were changed during training, remove them from the"
v4.6.0,returned model so that they're generated dynamically on every run based
v4.6.0,on the Dask cluster you're connected to and which workers have pieces of
v4.6.0,the training data
v4.6.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v4.6.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v4.6.0,are returned as a list of sparse matrices (one per class)
v4.6.0,"pred_contrib output will have one column per feature,"
v4.6.0,plus one more for the base value
v4.6.0,need to tell Dask the expected type and shape of individual preds
v4.6.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v4.6.0,the code below is used instead to ensure that the sparse type is preserved during concatenation
v4.6.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v4.6.0,Concatenate them to return a list of Dask Arrays.
v4.6.0,"DaskLGBMClassifier does not support group, eval_group."
v4.6.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v4.6.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group."
v4.6.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v4.6.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v4.6.0,DaskLGBMRanker support for callbacks and init_model is not tested
v4.6.0,coding: utf-8
v4.6.0,load or create your dataset
v4.6.0,generate feature names
v4.6.0,create dataset for lightgbm
v4.6.0,"if you want to re-use data, remember to set free_raw_data=False"
v4.6.0,specify your configurations as a dict
v4.6.0,feature_name and categorical_feature
v4.6.0,check feature name
v4.6.0,save model to file
v4.6.0,dump model to JSON (and save to file)
v4.6.0,feature names
v4.6.0,feature importances
v4.6.0,load model to predict
v4.6.0,can only predict with the best iteration (or the saving iteration)
v4.6.0,eval with loaded model
v4.6.0,dump model with pickle
v4.6.0,load model with pickle to predict
v4.6.0,can predict with any iteration when loaded in pickle way
v4.6.0,eval with loaded model
v4.6.0,continue training
v4.6.0,init_model accepts:
v4.6.0,1. model file name
v4.6.0,2. Booster()
v4.6.0,decay learning rates
v4.6.0,reset_parameter callback accepts:
v4.6.0,1. list with length = num_boost_round
v4.6.0,2. function(curr_iter)
v4.6.0,change other parameters during training
v4.6.0,self-defined objective function
v4.6.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v4.6.0,log likelihood loss
v4.6.0,self-defined eval metric
v4.6.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.6.0,binary error
v4.6.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.6.0,This may make built-in evaluation metric calculate wrong results
v4.6.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.6.0,Keep this in mind when you use the customization
v4.6.0,Pass custom objective function through params
v4.6.0,another self-defined eval metric
v4.6.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.6.0,accuracy
v4.6.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.6.0,This may make built-in evaluation metric calculate wrong results
v4.6.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.6.0,Keep this in mind when you use the customization
v4.6.0,Pass custom objective function through params
v4.6.0,callback
v4.6.0,coding: utf-8
v4.6.0,load or create your dataset
v4.6.0,train
v4.6.0,predict
v4.6.0,eval
v4.6.0,feature importances
v4.6.0,self-defined eval metric
v4.6.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.6.0,Root Mean Squared Logarithmic Error (RMSLE)
v4.6.0,train
v4.6.0,another self-defined eval metric
v4.6.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.6.0,Relative Absolute Error (RAE)
v4.6.0,train
v4.6.0,predict
v4.6.0,eval
v4.6.0,other scikit-learn modules
v4.6.0,coding: utf-8
v4.6.0,load or create your dataset
v4.6.0,create dataset for lightgbm
v4.6.0,specify your configurations as a dict
v4.6.0,train
v4.6.0,coding: utf-8
v4.6.0,################
v4.6.0,Simulate some binary data with a single categorical and
v4.6.0,single continuous predictor
v4.6.0,################
v4.6.0,Set up a couple of utilities for our experiments
v4.6.0,################
v4.6.0,Observe the behavior of `binary` and `xentropy` objectives
v4.6.0,Trying this throws an error on non-binary values of y:
v4.6.0,"experiment('binary', label_type='probability', DATA)"
v4.6.0,The speed of `binary` is not drastically different than
v4.6.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v4.6.0,there are reasons to suspect that `binary` should run faster when the
v4.6.0,label is an integer instead of a float
v4.6.0,coding: utf-8
v4.6.0,load or create your dataset
v4.6.0,create dataset for lightgbm
v4.6.0,specify your configurations as a dict
v4.6.0,train
v4.6.0,save model to file
v4.6.0,predict
v4.6.0,eval
v4.6.0,We can also open HDF5 file once and get access to
v4.6.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v4.6.0,
v4.6.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v4.6.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v4.6.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v4.6.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v4.6.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v4.6.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v4.6.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v4.6.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v4.6.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v4.6.0,Here we are just creating a chunk size that matches with batch_size.
v4.6.0,
v4.6.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v4.6.0,lightgbm Dataset.
v4.6.0,Save to 2 HDF5 files for demonstration.
v4.6.0,We can store multiple datasets inside a single HDF5 file.
v4.6.0,Separating X and Y for choosing best chunk size for data loading.
v4.6.0,split training data into two partitions
v4.6.0,make this array dense because we're splitting across
v4.6.0,a sparse boundary to partition the data
v4.6.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v4.6.0,predictions and target values back from workers to the client
v4.6.0,
v4.6.0,"for larger datasets, consider the metrics from dask-ml instead"
v4.6.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v4.6.0,!/usr/bin/env python3
v4.6.0,-*- coding: utf-8 -*-
v4.6.0,
v4.6.0,"LightGBM documentation build configuration file, created by"
v4.6.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v4.6.0,
v4.6.0,This file is execfile()d with the current directory set to its
v4.6.0,containing dir.
v4.6.0,
v4.6.0,Note that not all possible configuration values are present in this
v4.6.0,autogenerated file.
v4.6.0,
v4.6.0,All configuration values have a default; values that are commented out
v4.6.0,serve to show the default.
v4.6.0,"If extensions (or modules to document with autodoc) are in another directory,"
v4.6.0,add these directories to sys.path here. If the directory is relative to the
v4.6.0,"documentation root, use os.path.abspath to make it absolute."
v4.6.0,-- General configuration ------------------------------------------------
v4.6.0,"If your documentation needs a minimal Sphinx version, state it here."
v4.6.0,"Add any Sphinx extension module names here, as strings. They can be"
v4.6.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v4.6.0,ones.
v4.6.0,mock out modules
v4.6.0,hide type hints in API docs
v4.6.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v4.6.0,Only the class' docstring is inserted.
v4.6.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v4.6.0,The master toctree document.
v4.6.0,General information about the project.
v4.6.0,The name of an image file (relative to this directory) to place at the top
v4.6.0,of the sidebar.
v4.6.0,The name of an image file (relative to this directory) to use as a favicon of
v4.6.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v4.6.0,pixels large.
v4.6.0,"The version info for the project you're documenting, acts as replacement for"
v4.6.0,"|version| and |release|, also used in various other places throughout the"
v4.6.0,built documents.
v4.6.0,The short X.Y version.
v4.6.0,"The full version, including alpha/beta/rc tags."
v4.6.0,The language for content autogenerated by Sphinx. Refer to documentation
v4.6.0,for a list of supported languages.
v4.6.0,
v4.6.0,This is also used if you do content translation via gettext catalogs.
v4.6.0,"Usually you set ""language"" from the command line for these cases."
v4.6.0,"List of patterns, relative to source directory, that match files and"
v4.6.0,directories to ignore when looking for source files.
v4.6.0,This patterns also effect to html_static_path and html_extra_path
v4.6.0,The name of the Pygments (syntax highlighting) style to use.
v4.6.0,-- Configuration for C API docs generation ------------------------------
v4.6.0,-- Options for HTML output ----------------------------------------------
v4.6.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v4.6.0,a list of builtin themes.
v4.6.0,Theme options are theme-specific and customize the look and feel of a theme
v4.6.0,"further.  For a list of options available for each theme, see the"
v4.6.0,documentation.
v4.6.0,"Add any paths that contain custom static files (such as style sheets) here,"
v4.6.0,"relative to this directory. They are copied after the builtin static files,"
v4.6.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v4.6.0,-- Options for HTMLHelp output ------------------------------------------
v4.6.0,Output file base name for HTML help builder.
v4.6.0,-- Options for LaTeX output ---------------------------------------------
v4.6.0,The name of an image file (relative to this directory) to place at the top of
v4.6.0,the title page.
v4.6.0,intersphinx configuration
v4.6.0,Warning! The following code can cause buffer overflows on RTD.
v4.6.0,Consider suppressing output completely if RTD project silently fails.
v4.6.0,Refer to https://github.com/svenevs/exhale
v4.6.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.6.0,Warning! The following code can cause buffer overflows on RTD.
v4.6.0,Consider suppressing output completely if RTD project silently fails.
v4.6.0,Refer to https://github.com/svenevs/exhale
v4.6.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.6.0,coding: utf-8
v4.6.0,This is a basic test for floating number parsing.
v4.6.0,Most of the test cases come from:
v4.6.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v4.6.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v4.6.0,FLT_MAX
v4.6.0,FLT_MIN
v4.6.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v4.6.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v4.6.0,Near DBL_MIN
v4.6.0,DBL_MIN 2^-1022
v4.6.0,The behavior for parsing -nan depends on implementation.
v4.6.0,Thus we skip binary check for negative nan.
v4.6.0,See comment in test_cases.
v4.6.0,construct sample data first (use all data for convenience and since size is small)
v4.6.0,Load some test data
v4.6.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.6.0,Add some fake initial_scores and groups so we can test streaming them
v4.6.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.6.0,Load some test data
v4.6.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.6.0,Add some fake initial_scores and groups so we can test streaming them
v4.6.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.6.0,This code is copied and adapted from the official Arrow producer examples:
v4.6.0,https://arrow.apache.org/docs/format/CDataInterface.html#exporting-a-struct-float32-utf8-array
v4.6.0,Free children
v4.6.0,Finalize
v4.6.0,Free children
v4.6.0,Free buffers
v4.6.0,Finalize
v4.6.0,NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests
v4.6.0,"By using `calloc` above, we only need to set 'true' values"
v4.6.0,Arithmetic
v4.6.0,Subscripts
v4.6.0,End
v4.6.0,Check for values in first chunk
v4.6.0,Check for some values in second chunk
v4.6.0,Check end
v4.6.0,Load some test data
v4.6.0,Serialize the reference
v4.6.0,Deserialize the reference
v4.6.0,Confirm 1 successful API call
v4.6.0,Free memory
v4.6.0,Load some test data
v4.6.0,Run a single row prediction and compare with regular Mat prediction:
v4.6.0,"Drop the result from the dataset, we only care about checking that prediction results are equal"
v4.6.0,in both cases
v4.6.0,Test LGBM_BoosterPredictForMat in multi-threaded mode
v4.6.0,Now let's run with the single row fast prediction API:
v4.6.0,Free all:
v4.6.0,Test that Data() points to first value written
v4.6.0,Constants
v4.6.0,Start with some content:
v4.6.0,Clear & re-use:
v4.6.0,Output should match new content:
v4.6.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v4.6.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v4.6.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v4.6.0,Compute a new trial address & value & if it is a valid address:
v4.6.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v4.6.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v4.6.0,Check the just-stored value with getitem():
v4.6.0,Also store the just-stored value for future tracking:
v4.6.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v4.6.0,Test in 2 ways that the values are correctly laid out in memory:
v4.6.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.6.0,Use multiple threads to test concurrency
v4.6.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.6.0,Calculate expected boundaries
v4.6.0,Extract a set of rows from the column-based format (still maintaining column based format)
v4.6.0,coding: utf-8
v4.6.0,"at initialization, should be -1"
v4.6.0,updating that value through the C API should work
v4.6.0,resetting to any negative number should set it to -1
v4.6.0,coding: utf-8
v4.6.0,check saved model persistence
v4.6.0,"we need to check the consistency of model file here, so test for exact equal"
v4.6.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v4.6.0,"scores likely to be different, but prediction should still be the same"
v4.6.0,test that shape is checked during prediction
v4.6.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v4.6.0,The following is for demo and testing purpose.
v4.6.0,whole col
v4.6.0,half col
v4.6.0,Create dataset from numpy array directly.
v4.6.0,Create dataset using Sequence.
v4.6.0,Test for validation set.
v4.6.0,Select some random rows as valid data.
v4.6.0,"From Dataset constructor, with dataset from numpy array."
v4.6.0,"From Dataset.create_valid, with dataset from sequence."
v4.6.0,test that method works even with free_raw_data=True
v4.6.0,test that method works but sets raw data to None in case of immergeable data types
v4.6.0,test that method works for different data types
v4.6.0,"Set extremely harsh penalties, so CEGB will block most splits."
v4.6.0,"Compare pairs of penalties, to ensure scaling works as intended"
v4.6.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v4.6.0,"unconstructed, get_* methods should return whatever was provided"
v4.6.0,"before construction, get_field() should raise an exception"
v4.6.0,"constructed, get_* methods should return numpy arrays, even when the provided"
v4.6.0,input was a list of floats or ints
v4.6.0,"get_field(""group"") returns a numpy array with boundaries, instead of size"
v4.6.0,"NOTE: ""position"" is converted to int32 on the C++ side"
v4.6.0,"should resolve duplicate aliases, and prefer the main parameter"
v4.6.0,should choose the highest priority alias and set that value on main param
v4.6.0,if only aliases are used
v4.6.0,should use the default if main param and aliases are missing
v4.6.0,all changes should be made on copies and not modify the original
v4.6.0,preserves None found for main param and still removes aliases
v4.6.0,correctly chooses value when only an alias is provided
v4.6.0,adds None if that's given as the default and param not found
v4.6.0,If callable is found in objective
v4.6.0,Value in params should be preferred to the default_value passed from keyword arguments
v4.6.0,"None of objective or its aliases in params, but default_value is callable."
v4.6.0,"""bad"" = 1 element too many"
v4.6.0,"copy=False is necessary because starting with pandas 3.0, pd.DataFrame() creates"
v4.6.0,a copy of the input numpy array by default
v4.6.0,ref: https://github.com/pandas-dev/pandas/issues/58913
v4.6.0,check that the original data wasn't modified
v4.6.0,check that the built data has the codes
v4.6.0,if all categories were seen during training we just take the codes
v4.6.0,if we only saw 'a' during training we just replace its code
v4.6.0,and leave the rest as nan
v4.6.0,test using defined feature names
v4.6.0,test using default feature names
v4.6.0,check for feature indices outside of range
v4.6.0,"NOTE: this intentionally contains values where num_leaves <, ==, and > (max_depth^2)"
v4.6.0,"NOTE: max_depth < 5 is significant here because the default for num_leaves=31. With max_depth=5,"
v4.6.0,a full depth-wise tree would have 2^5 = 32 leaves.
v4.6.0,makes a copy
v4.6.0,row-major dataset
v4.6.0,col-major dataset
v4.6.0,check datasets are equal
v4.6.0,several matrices
v4.6.0,one matrix
v4.6.0,coding: utf-8
v4.6.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v4.6.0,set_index ensures partitions are based on group id.
v4.6.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v4.6.0,"separate target, weight from features."
v4.6.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v4.6.0,"so that within each partition, sum(g) = n_samples."
v4.6.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v4.6.0,make one categorical feature relevant to the target
v4.6.0,https://github.com/microsoft/LightGBM/issues/4118
v4.6.0,extra predict() parameters should be passed through correctly
v4.6.0,pref_leaf values should have the right shape
v4.6.0,and values that look like valid tree nodes
v4.6.0,"be sure LightGBM actually used at least one categorical column,"
v4.6.0,and that it was correctly treated as a categorical feature
v4.6.0,shape depends on whether it is binary or multiclass classification
v4.6.0,"in the special case of multi-class classification using scipy sparse matrices,"
v4.6.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v4.6.0,
v4.6.0,"since that case is so different than all other cases, check the relevant things here"
v4.6.0,and then return early
v4.6.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v4.6.0,"be sure LightGBM actually used at least one categorical column,"
v4.6.0,and that it was correctly treated as a categorical feature
v4.6.0,* shape depends on whether it is binary or multiclass classification
v4.6.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v4.6.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v4.6.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v4.6.0,that the output has the right shape and base values are in the right position
v4.6.0,"with a custom objective, prediction result is a raw score instead of predicted class"
v4.6.0,function should have been preserved
v4.6.0,should correctly classify every sample
v4.6.0,probability estimates should be similar
v4.6.0,Scores should be the same
v4.6.0,Predictions should be roughly the same.
v4.6.0,pref_leaf values should have the right shape
v4.6.0,and values that look like valid tree nodes
v4.6.0,extra predict() parameters should be passed through correctly
v4.6.0,"be sure LightGBM actually used at least one categorical column,"
v4.6.0,and that it was correctly treated as a categorical feature
v4.6.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v4.6.0,that the output has the right shape and base values are in the right position
v4.6.0,"be sure LightGBM actually used at least one categorical column,"
v4.6.0,and that it was correctly treated as a categorical feature
v4.6.0,Quantiles should be right
v4.6.0,"be sure LightGBM actually used at least one categorical column,"
v4.6.0,and that it was correctly treated as a categorical feature
v4.6.0,function should have been preserved
v4.6.0,Scores should be the same
v4.6.0,local and Dask predictions should be the same
v4.6.0,predictions should be better than random
v4.6.0,rebalance small dask.Array dataset for better performance.
v4.6.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v4.6.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v4.6.0,distributed ranker should be able to rank decently well and should
v4.6.0,have high rank correlation with scores from serial ranker.
v4.6.0,extra predict() parameters should be passed through correctly
v4.6.0,pref_leaf values should have the right shape
v4.6.0,and values that look like valid tree nodes
v4.6.0,"be sure LightGBM actually used at least one categorical column,"
v4.6.0,and that it was correctly treated as a categorical feature
v4.6.0,rebalance small dask.Array dataset for better performance.
v4.6.0,distributed ranker should be able to rank decently well with the least-squares objective
v4.6.0,and should have high rank correlation with scores from serial ranker.
v4.6.0,function should have been preserved
v4.6.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v4.6.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v4.6.0,"test eval_class_weight, eval_init_score on binary-classification task."
v4.6.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v4.6.0,create eval_sets by creating new datasets or copying training data.
v4.6.0,total number of trees scales up for ova classifier.
v4.6.0,check that early stopping was not applied.
v4.6.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v4.6.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v4.6.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v4.6.0,should be able to use the class without specifying a client
v4.6.0,should be able to set client after construction
v4.6.0,data on cluster1
v4.6.0,create identical data on cluster2
v4.6.0,"at this point, the result of default_client() is client2 since it was the most recently"
v4.6.0,created. So setting client to client1 here to test that you can select a non-default client
v4.6.0,"unfitted model should survive pickling round trip, and pickling"
v4.6.0,shouldn't have side effects on the model object
v4.6.0,client will always be None after unpickling
v4.6.0,"fitted model should survive pickling round trip, and pickling"
v4.6.0,shouldn't have side effects on the model object
v4.6.0,client will always be None after unpickling
v4.6.0,rebalance data to be sure that each worker has a piece of the data
v4.6.0,model 1 - no network parameters given
v4.6.0,model 2 - machines given
v4.6.0,model 3 - local_listen_port given
v4.6.0,training should fail because LightGBM will try to use the same
v4.6.0,port for multiple worker processes on the same machine
v4.6.0,rebalance data to be sure that each worker has a piece of the data
v4.6.0,"test that ""machines"" is actually respected by creating a socket that uses"
v4.6.0,"one of the ports mentioned in ""machines"""
v4.6.0,The above error leaves a worker waiting
v4.6.0,"an informative error should be raised if ""machines"" has duplicates"
v4.6.0,should not allow for any varargs
v4.6.0,"the only varkw should be **kwargs,"
v4.6.0,for pass-through to parent classes' __init__()
v4.6.0,"""client"" should be the only different, and the final argument"
v4.6.0,default values for all constructor arguments should be identical
v4.6.0,
v4.6.0,NOTE: if LGBMClassifier / LGBMRanker / LGBMRegressor ever override
v4.6.0,"any of LGBMModel's constructor arguments, this will need to be updated"
v4.6.0,only positional argument should be 'self'
v4.6.0,"get_params() should be identical, except for ""client"""
v4.6.0,check if init score changes predictions
v4.6.0,this test is separate because it takes a not-yet-constructed estimator
v4.6.0,"if init_score was provided, a model of stumps should predict all 0s"
v4.6.0,"if init_score was not provided, prediction for a model of stumps should be"
v4.6.0,"the ""average"" of the labels"
v4.6.0,coding: utf-8
v4.6.0,coding: utf-8
v4.6.0,"build target, group ID vectors."
v4.6.0,build y/target and group-id vectors with user-specified group sizes.
v4.6.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v4.6.0,groups should contain > 1 element for pairwise learning objective.
v4.6.0,"build feature data, X. Transform first few into informative features."
v4.6.0,"doing this here, at import time, to ensure it only runs once_per import"
v4.6.0,instead of once per assertion
v4.6.0,coding: utf-8
v4.6.0,"NOTE: In the AppVeyor CI, importing pyarrow fails due to an old Visual Studio version. Hence,"
v4.6.0,"we conditionally import pyarrow here (and skip tests if it cannot be imported). However, we"
v4.6.0,"don't want these tests to silently be skipped, hence, we only conditionally import when a"
v4.6.0,specific env var is set.
v4.6.0,----------------------------------------------------------------------------------------------- #
v4.6.0,UTILITIES                                            #
v4.6.0,----------------------------------------------------------------------------------------------- #
v4.6.0,Set random nulls
v4.6.0,Split data into <=2 random chunks
v4.6.0,Turn chunks into array
v4.6.0,----------------------------------------------------------------------------------------------- #
v4.6.0,UNIT TESTS                                           #
v4.6.0,----------------------------------------------------------------------------------------------- #
v4.6.0,------------------------------------------- DATASET ------------------------------------------- #
v4.6.0,-------------------------------------------- FIELDS ------------------------------------------- #
v4.6.0,Check for equality
v4.6.0,-------------------------------------------- LABELS ------------------------------------------- #
v4.6.0,------------------------------------------- WEIGHTS ------------------------------------------- #
v4.6.0,-------------------------------------------- GROUPS ------------------------------------------- #
v4.6.0,----------------------------------------- INIT SCORES ----------------------------------------- #
v4.6.0,------------------------------------------ PREDICTION ----------------------------------------- #
v4.6.0,coding: utf-8
v4.6.0,check that really dummy objective was used and estimator didn't learn anything
v4.6.0,prediction result is actually not transformed (is raw) due to custom objective
v4.6.0,original estimator is unaffected
v4.6.0,"new estimator is unfitted, but has the same parameters"
v4.6.0,should not allow for any varargs
v4.6.0,"the only varkw should be **kwargs,"
v4.6.0,default values for all constructor arguments should be identical
v4.6.0,
v4.6.0,NOTE: if LGBMClassifier / LGBMRanker / LGBMRegressor ever override
v4.6.0,"any of LGBMModel's constructor arguments, this will need to be updated"
v4.6.0,only positional argument should be 'self'
v4.6.0,get_params() should be identical
v4.6.0,"Overrides, used to test that passing through **kwargs works as expected."
v4.6.0,
v4.6.0,why these?
v4.6.0,
v4.6.0,- 'n_estimators' directly matches a keyword arg for the scikit-learn estimators
v4.6.0,- 'eta' is a parameter alias for 'learning_rate'
v4.6.0,lightgbm-official classes
v4.6.0,custom sub-classes
v4.6.0,param values to make training deterministic and
v4.6.0,"just train a small, cheap model"
v4.6.0,scikit-learn estimators should remember every parameter passed
v4.6.0,"via keyword arguments in the estimator constructor, but then"
v4.6.0,only pass the correct value down to LightGBM's C++ side
v4.6.0,scikit-learn get_params()
v4.6.0,lgb.Booster's 'params' attribute
v4.6.0,Config in the 'LightGBM::Booster' on the C++ side
v4.6.0,Test if random_state is properly stored
v4.6.0,Test if two random states produce identical models
v4.6.0,Test if subsequent fits sample from random_state object and produce different models
v4.6.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v4.6.0,why fixed seed?
v4.6.0,sometimes there is no difference how cols are treated (cat or not cat)
v4.6.0,With default params
v4.6.0,Tests same probabilities
v4.6.0,Tests same predictions
v4.6.0,Tests same raw scores
v4.6.0,Tests same leaf indices
v4.6.0,Tests same feature contributions
v4.6.0,Tests other parameters for the prediction works
v4.6.0,Tests start_iteration
v4.6.0,"Tests same probabilities, starting from iteration 10"
v4.6.0,"Tests same predictions, starting from iteration 10"
v4.6.0,"Tests same raw scores, starting from iteration 10"
v4.6.0,"Tests same leaf indices, starting from iteration 10"
v4.6.0,"Tests same feature contributions, starting from iteration 10"
v4.6.0,"Tests other parameters for the prediction works, starting from iteration 10"
v4.6.0,Test multiclass binary classification
v4.6.0,test that params passed in predict have higher priority
v4.6.0,"no custom objective, no custom metric"
v4.6.0,default metric
v4.6.0,non-default metric
v4.6.0,no metric
v4.6.0,non-default metric in eval_metric
v4.6.0,non-default metric with non-default metric in eval_metric
v4.6.0,non-default metric with multiple metrics in eval_metric
v4.6.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v4.6.0,default metric for non-default objective
v4.6.0,non-default metric for non-default objective
v4.6.0,no metric
v4.6.0,non-default metric in eval_metric for non-default objective
v4.6.0,non-default metric with non-default metric in eval_metric for non-default objective
v4.6.0,non-default metric with multiple metrics in eval_metric for non-default objective
v4.6.0,"custom objective, no custom metric"
v4.6.0,default regression metric for custom objective
v4.6.0,non-default regression metric for custom objective
v4.6.0,multiple regression metrics for custom objective
v4.6.0,no metric
v4.6.0,default regression metric with non-default metric in eval_metric for custom objective
v4.6.0,non-default regression metric with metric in eval_metric for custom objective
v4.6.0,multiple regression metrics with metric in eval_metric for custom objective
v4.6.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v4.6.0,"no custom objective, custom metric"
v4.6.0,default metric with custom metric
v4.6.0,non-default metric with custom metric
v4.6.0,multiple metrics with custom metric
v4.6.0,custom metric (disable default metric)
v4.6.0,default metric for non-default objective with custom metric
v4.6.0,non-default metric for non-default objective with custom metric
v4.6.0,multiple metrics for non-default objective with custom metric
v4.6.0,custom metric (disable default metric for non-default objective)
v4.6.0,"custom objective, custom metric"
v4.6.0,custom metric for custom objective
v4.6.0,non-default regression metric with custom metric for custom objective
v4.6.0,multiple regression metrics with custom metric for custom objective
v4.6.0,default metric and invalid binary metric is replaced with multiclass alternative
v4.6.0,invalid binary metric is replaced with multiclass alternative
v4.6.0,default metric for non-default multiclass objective
v4.6.0,and invalid binary metric is replaced with multiclass alternative
v4.6.0,default metric and invalid multiclass metric is replaced with binary alternative
v4.6.0,invalid multiclass metric is replaced with binary alternative for custom objective
v4.6.0,the evaluation metric changes to multiclass metric even num classes is 2 for multiclass objective
v4.6.0,the evaluation metric changes to multiclass metric even num classes is 2 for ovr objective
v4.6.0,"Verify that can receive a list of metrics, only callable"
v4.6.0,Verify that can receive a list of custom and built-in metrics
v4.6.0,Verify that works as expected when eval_metric is empty
v4.6.0,"Verify that can receive a list of metrics, only built-in"
v4.6.0,Verify that eval_metric is robust to receiving a list with None
v4.6.0,feval
v4.6.0,single eval_set
v4.6.0,two eval_set
v4.6.0,"no warning: no aliases, all defaults"
v4.6.0,"no warning: no aliases, just n_estimators"
v4.6.0,no warning: 1 alias + n_estimators (both same value)
v4.6.0,no warning: 1 alias + n_estimators (different values... value from params should win)
v4.6.0,no warning: 2 aliases (both same value)
v4.6.0,no warning: 4 aliases (all same value)
v4.6.0,"warning: 2 aliases (different values... ""num_iterations"" wins because it's the main param name)"
v4.6.0,"should not be any other logs (except the warning, intercepted by pytest)"
v4.6.0,warning: 2 aliases (different values... first one in the order from Config::parameter2aliases() wins)
v4.6.0,"should not be any other logs (except the warning, intercepted by pytest)"
v4.6.0,"input is a numpy array, which doesn't have feature names. LightGBM adds"
v4.6.0,"feature names to the fitted model, which is inconsistent with sklearn's behavior"
v4.6.0,"Starting with scikit-learn 1.6 (https://github.com/scikit-learn/scikit-learn/pull/30149),"
v4.6.0,the only API for marking estimator tests as expected to fail is to pass a keyword argument
v4.6.0,to parametrize_with_checks(). That function didn't accept additional arguments in earlier
v4.6.0,versions.
v4.6.0,
v4.6.0,This block defines a patched version of parametrize_with_checks() so lightgbm's tests
v4.6.0,can be compatible with scikit-learn <1.6 and >=1.6.
v4.6.0,
v4.6.0,This should be removed once minimum supported scikit-learn version is at least 1.6.
v4.6.0,the try-except part of this should be removed once lightgbm's
v4.6.0,minimum supported scikit-learn version is at least 1.6
v4.6.0,only the exact error we expected to be raised should be raised
v4.6.0,"if no AttributeError was thrown, we must be using scikit-learn>=1.6,"
v4.6.0,and so the actual effects of __sklearn_tags__() should be tested
v4.6.0,'val_minus_two' here is the expected number of threads for n_jobs=-2
v4.6.0,"Note: according to joblib's formula, a value of n_jobs=-2 means"
v4.6.0,"""use all but one thread"" (formula: n_cpus + 1 + n_jobs)"
v4.6.0,try to predict with a different feature
v4.6.0,check that disabling the check doesn't raise the error
v4.6.0,LightGBM's 'predict_disable_shape_check' mechanism is intentionally not respected by
v4.6.0,"its scikit-learn estimators, for consistency with scikit-learn's own behavior."
v4.6.0,train on the first 3 features
v4.6.0,more cols in X than features: error
v4.6.0,fewer cols in X than features: error
v4.6.0,same number of columns in both: no error
v4.6.0,"make weights and init_score same types as y, just to avoid"
v4.6.0,a huge number of combinations and therefore test cases
v4.6.0,"make weights and init_score same types as y, just to avoid"
v4.6.0,a huge number of combinations and therefore test cases
v4.6.0,coding: utf-8
v4.6.0,we're in a leaf now
v4.6.0,check that the rest of the elements have black color
v4.6.0,check that we got to the expected leaf
v4.6.0,coding: utf-8
v4.6.0,coding: utf-8
v4.6.0,check that default gives same result as k = 1
v4.6.0,check against independent calculation for k = 1
v4.6.0,check against independent calculation for k = 2
v4.6.0,check against independent calculation for k = 10
v4.6.0,check cases where predictions are equal
v4.6.0,should give same result as binary auc for 2 classes
v4.6.0,test the case where all predictions are equal
v4.6.0,test that weighted data gives different auc_mu
v4.6.0,test that equal data weights give same auc_mu as unweighted data
v4.6.0,should give 1 when accuracy = 1
v4.6.0,test loading class weights
v4.6.0,Simulates position bias for a given ranking dataset.
v4.6.0,The output dataset is identical to the input one with the exception for the relevance labels.
v4.6.0,The new labels are generated according to an instance of a cascade user model:
v4.6.0,"for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker"
v4.6.0,"(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)"
v4.6.0,and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;
v4.6.0,"at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,"
v4.6.0,new_label=0. Thus the generated new labels are biased towards the baseline ranker.
v4.6.0,"The positions of the documents in the ranked lists produced by the baseline, are returned."
v4.6.0,a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it
v4.6.0,an instantiation of a cascade model where the user stops with probability 0.2 after observing each document
v4.6.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.6.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.6.0,add extra row to position file
v4.6.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.6.0,test setting positions through Dataset constructor with numpy array
v4.6.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.6.0,test setting positions through Dataset constructor with pandas Series
v4.6.0,test setting positions through set_position
v4.6.0,test get_position works
v4.6.0,no early stopping
v4.6.0,early stopping occurs
v4.6.0,regular early stopping
v4.6.0,positive min_delta
v4.6.0,test custom eval metrics
v4.6.0,"shuffle = False, override metric in params"
v4.6.0,"shuffle = True, callbacks"
v4.6.0,enable display training loss
v4.6.0,self defined folds
v4.6.0,LambdaRank
v4.6.0,... with l2 metric
v4.6.0,... with NDCG (default) metric
v4.6.0,self defined folds with lambdarank
v4.6.0,init_model from an in-memory Booster
v4.6.0,init_model from a text file
v4.6.0,predictions should be identical
v4.6.0,with early stopping
v4.6.0,predict by each fold booster
v4.6.0,check that each booster predicted using the best iteration
v4.6.0,fold averaging
v4.6.0,without early stopping
v4.6.0,test feature_names with whitespaces
v4.6.0,This has non-ascii strings.
v4.6.0,check that passing parameters to the constructor raises warning and ignores them
v4.6.0,check inference isn't affected by unknown parameter
v4.6.0,entries whose values should reflect params passed to lgb.train()
v4.6.0,'l1' was passed in with alias 'mae'
v4.6.0,NOTE: this was passed in with alias 'sub_row'
v4.6.0,entries with default values of params
v4.6.0,add device-specific entries
v4.6.0,
v4.6.0,passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...
v4.6.0,https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377
v4.6.0,check that model text has all expected param entries
v4.6.0,"since Booster.model_to_string() is used when pickling, check that parameters all"
v4.6.0,roundtrip pickling successfully too
v4.6.0,why fixed seed?
v4.6.0,sometimes there is no difference how cols are treated (cat or not cat)
v4.6.0,take subsets and train
v4.6.0,generate CSR sparse dataset
v4.6.0,convert data to dense and get back same contribs
v4.6.0,validate the values are the same
v4.6.0,validate using CSC matrix
v4.6.0,validate the values are the same
v4.6.0,generate CSR sparse dataset
v4.6.0,convert data to dense and get back same contribs
v4.6.0,validate the values are the same
v4.6.0,validate using CSC matrix
v4.6.0,validate the values are the same
v4.6.0,"@pytest.mark.skipif(psutil.virtual_memory().available / 1024 / 1024 / 1024 < 3, reason=""not enough RAM"")"
v4.6.0,def test_int32_max_sparse_contribs(rng):
v4.6.0,"params = {""objective"": ""binary""}"
v4.6.0,"train_features = rng.uniform(size=(100, 1000))"
v4.6.0,train_targets = [0] * 50 + [1] * 50
v4.6.0,"lgb_train = lgb.Dataset(train_features, train_targets)"
v4.6.0,"gbm = lgb.train(params, lgb_train, num_boost_round=2)"
v4.6.0,"csr_input_shape = (3000000, 1000)"
v4.6.0,test_features = csr_matrix(csr_input_shape)
v4.6.0,"for i in range(0, csr_input_shape[0], csr_input_shape[0] // 6):"
v4.6.0,"for j in range(0, 1000, 100):"
v4.6.0,"test_features[i, j] = random.random()"
v4.6.0,"y_pred_csr = gbm.predict(test_features, pred_contrib=True)"
v4.6.0,# Note there is an extra column added to the output for the expected value
v4.6.0,"csr_output_shape = (csr_input_shape[0], csr_input_shape[1] + 1)"
v4.6.0,assert y_pred_csr.shape == csr_output_shape
v4.6.0,"y_pred_csc = gbm.predict(test_features.tocsc(), pred_contrib=True)"
v4.6.0,# Note output CSC shape should be same as CSR output shape
v4.6.0,assert y_pred_csc.shape == csr_output_shape
v4.6.0,test sliced labels
v4.6.0,append some columns
v4.6.0,append some rows
v4.6.0,test sliced 2d matrix
v4.6.0,test sliced CSR
v4.6.0,trees start at position 1.
v4.6.0,split_features are in 4th line.
v4.6.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v4.6.0,The penalization is so high that the first 2 features should not be used here
v4.6.0,Check that a very high penalization is the same as not using the features at all
v4.6.0,check refit accepts dataset_params
v4.6.0,the following checks that dart and rf with mape can predict outside the 0-1 range
v4.6.0,https://github.com/microsoft/LightGBM/issues/1579
v4.6.0,"no custom objective, no feval"
v4.6.0,default metric
v4.6.0,non-default metric in params
v4.6.0,default metric in args
v4.6.0,non-default metric in args
v4.6.0,metric in args overwrites one in params
v4.6.0,metric in args overwrites one in params
v4.6.0,multiple metrics in params
v4.6.0,multiple metrics in args
v4.6.0,remove default metric by 'None' in list
v4.6.0,remove default metric by 'None' aliases
v4.6.0,"custom objective, no feval"
v4.6.0,no default metric
v4.6.0,metric in params
v4.6.0,metric in args
v4.6.0,metric in args overwrites its' alias in params
v4.6.0,multiple metrics in params
v4.6.0,multiple metrics in args
v4.6.0,"no custom objective, feval"
v4.6.0,default metric with custom one
v4.6.0,non-default metric in params with custom one
v4.6.0,default metric in args with custom one
v4.6.0,default metric in args with 1 custom function returning a list of 2 metrics
v4.6.0,non-default metric in args with custom one
v4.6.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.6.0,multiple metrics in params with custom one
v4.6.0,multiple metrics in args with custom one
v4.6.0,custom metric is evaluated despite 'None' is passed
v4.6.0,"custom objective, feval"
v4.6.0,"no default metric, only custom one"
v4.6.0,metric in params with custom one
v4.6.0,metric in args with custom one
v4.6.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.6.0,multiple metrics in params with custom one
v4.6.0,multiple metrics in args with custom one
v4.6.0,custom metric is evaluated despite 'None' is passed
v4.6.0,"no custom objective, no feval"
v4.6.0,default metric
v4.6.0,default metric in params
v4.6.0,non-default metric in params
v4.6.0,multiple metrics in params
v4.6.0,remove default metric by 'None' aliases
v4.6.0,"custom objective, no feval"
v4.6.0,no default metric
v4.6.0,metric in params
v4.6.0,multiple metrics in params
v4.6.0,"no custom objective, feval"
v4.6.0,default metric with custom one
v4.6.0,default metric in params with custom one
v4.6.0,default metric in params with custom function returning a list of 2 metrics
v4.6.0,non-default metric in params with custom one
v4.6.0,multiple metrics in params with custom one
v4.6.0,custom metric is evaluated despite 'None' is passed
v4.6.0,"custom objective, feval"
v4.6.0,"no default metric, only custom one"
v4.6.0,metric in params with custom one
v4.6.0,multiple metrics in params with custom one
v4.6.0,custom metric is evaluated despite 'None' is passed
v4.6.0,Custom objective replaces multiclass
v4.6.0,multiclass default metric
v4.6.0,multiclass default metric with custom one
v4.6.0,multiclass metric alias with custom one for custom objective
v4.6.0,no metric for invalid class_num
v4.6.0,custom metric for invalid class_num
v4.6.0,multiclass metric alias with custom one with invalid class_num
v4.6.0,multiclass default metric without num_class
v4.6.0,multiclass metric alias
v4.6.0,multiclass metric
v4.6.0,non-valid metric for multiclass objective
v4.6.0,non-default num_class for default objective
v4.6.0,no metric with non-default num_class for custom objective
v4.6.0,multiclass metric alias for custom objective
v4.6.0,multiclass metric for custom objective
v4.6.0,binary metric with non-default num_class for custom objective
v4.6.0,Expect three metrics but mean and stdv for each metric
v4.6.0,test XGBoost-style return value
v4.6.0,test numpy-style return value
v4.6.0,test bins string type
v4.6.0,test histogram is disabled for categorical features
v4.6.0,test for lgb.train
v4.6.0,test feval for lgb.train
v4.6.0,test with two valid data for lgb.train
v4.6.0,test for lgb.cv
v4.6.0,test feval for lgb.cv
v4.6.0,test that binning works properly for features with only positive or only negative values
v4.6.0,decreasing without freeing raw data is allowed
v4.6.0,decreasing before lazy init is allowed
v4.6.0,increasing is allowed
v4.6.0,decreasing with disabled filter is allowed
v4.6.0,decreasing with enabled filter is disallowed;
v4.6.0,also changes of other params are disallowed
v4.6.0,check extra trees increases regularization
v4.6.0,check path smoothing increases regularization
v4.6.0,test edge case with one leaf
v4.6.0,check that constraint containing all features is equivalent to no constraint
v4.6.0,check that constraint partitioning the features reduces train accuracy
v4.6.0,check that constraints consisting of single features reduce accuracy further
v4.6.0,test that interaction constraints work when not all features are used
v4.6.0,check that number of threads does not affect result
v4.6.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v4.6.0,test again with nans in data
v4.6.0,test again with bagging
v4.6.0,test with a feature that has only one non-nan value
v4.6.0,test with a categorical feature
v4.6.0,test refit: same results on same data
v4.6.0,test refit with save and load
v4.6.0,test refit: different results training on different data
v4.6.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v4.6.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v4.6.0,"test the case where start_iteration <= 0, and num_iteration is None"
v4.6.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v4.6.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v4.6.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v4.6.0,test for regression
v4.6.0,test both with and without early stopping
v4.6.0,test for multi-class
v4.6.0,test both with and without early stopping
v4.6.0,test for binary
v4.6.0,test both with and without early stopping
v4.6.0,"checking prediction from 1 iteration and the whole model, to prevent bugs"
v4.6.0,"of the form ""a model of n stumps predicts n * initial_score"""
v4.6.0,"if init_score was provided, a model of stumps should predict all 0s"
v4.6.0,"if init_score was not provided, prediction for a model of stumps should be"
v4.6.0,"the ""average"" of the labels"
v4.6.0,1-round model
v4.6.0,2-round model
v4.6.0,1-round model
v4.6.0,2-round model
v4.6.0,1-round model
v4.6.0,2-round model
v4.6.0,test against sklearn average precision metric
v4.6.0,test that average precision is 1 where model predicts perfectly
v4.6.0,data as float64
v4.6.0,test all features were used
v4.6.0,test the score is better than predicting the mean
v4.6.0,test all predictions are equal using different input dtypes
v4.6.0,introduce some missing values
v4.6.0,"in recent versions of pandas, type 'bool' is incompatible with nan values in x4"
v4.6.0,train with regular dtypes
v4.6.0,convert to nullable dtypes
v4.6.0,test training succeeds
v4.6.0,test all features were used
v4.6.0,test the score is better than predicting the mean
v4.6.0,test equal predictions
v4.6.0,test data are taken from bug report
v4.6.0,https://github.com/microsoft/LightGBM/issues/4708
v4.6.0,modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811
v4.6.0,and https://github.com/microsoft/LightGBM/pull/5087
v4.6.0,test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree
v4.6.0,which is done in the fix #5164
v4.6.0,without the fix:
v4.6.0,Check failed: (best_split_info.left_count) > (0)
v4.6.0,"no warning: no aliases, all defaults"
v4.6.0,"no warning: no aliases, just num_boost_round"
v4.6.0,no warning: 1 alias + num_boost_round (both same value)
v4.6.0,no warning: 1 alias + num_boost_round (different values... value from params should win)
v4.6.0,no warning: 2 aliases (both same value)
v4.6.0,no warning: 4 aliases (all same value)
v4.6.0,"warning: 2 aliases (different values... ""num_iterations"" wins because it's the main param name)"
v4.6.0,"should not be any other logs (except the warning, intercepted by pytest)"
v4.6.0,warning: 2 aliases (different values... first one in the order from Config::parameter2aliases() wins)
v4.6.0,"should not be any other logs (except the warning, intercepted by pytest)"
v4.6.0,"no warning: no aliases, all defaults"
v4.6.0,"no warning: no aliases, just num_boost_round"
v4.6.0,no warning: 1 alias + num_boost_round (both same value)
v4.6.0,no warning: 1 alias + num_boost_round (different values... value from params should win)
v4.6.0,no warning: 2 aliases (both same value)
v4.6.0,no warning: 4 aliases (all same value)
v4.6.0,"warning: 2 aliases (different values... ""num_iterations"" wins because it's the main param name)"
v4.6.0,"should not be any other logs (except the warning, intercepted by pytest)"
v4.6.0,warning: 2 aliases (different values... first one in the order from Config::parameter2aliases() wins)
v4.6.0,"should not be any other logs (except the warning, intercepted by pytest)"
v4.6.0,try to predict with a different feature
v4.6.0,check that disabling the check doesn't raise the error
v4.6.0,try to refit with a different feature
v4.6.0,check that disabling the check doesn't raise the error
v4.6.0,coding: utf-8
v4.6.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v4.6.0,Double-precision floats are only supported on x86_64 with PoCL
v4.6.0,coding: utf-8
v4.6.0,coding: utf-8
v4.6.0,coding: utf-8
v4.6.0,coding: utf-8
v4.6.0,alias table
v4.6.0,names
v4.6.0,from strings
v4.6.0,tails
v4.6.0,tails
v4.6.0,the following are stored as comma separated strings but are arrays in the wrappers
v4.6.0,coding: utf-8
v4.6.0,"Note: MSVC has issues with Altrep classes, so they are disabled for it."
v4.6.0,See: https://github.com/microsoft/LightGBM/pull/6213#issuecomment-2111025768
v4.6.0,These are helper functions to allow doing a stack unwind
v4.6.0,"after an R allocation error, which would trigger a long jump."
v4.6.0,convert from one-based to zero-based index
v4.6.0,"if any feature names were larger than allocated size,"
v4.6.0,allow for a larger size and try again
v4.6.0,convert from boundaries to size
v4.6.0,--- start Booster interfaces
v4.6.0,"if any eval names were larger than allocated size,"
v4.6.0,allow for a larger size and try again
v4.6.0,"Note: for some reason, MSVC crashes when an error is thrown here"
v4.6.0,"if the buffer variable is defined as 'std::unique_ptr<std::vector<char>>',"
v4.6.0,but not if it is defined as '<std::vector<char>'.
v4.6.0,"if the model string was larger than the initial buffer, call the function again, writing directly to the R object"
v4.6.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.6.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.6.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.6.0,.Call() calls
v4.6.0,Single row predictor to abstract away caching logic
v4.6.0,Prevent the booster from being modified while we have a predictor relying on it during prediction
v4.6.0,If several threads try to predict at the same time using the same SingleRowPredictor
v4.6.0,"we want them to still provide correct values, so the mutex is necessary due to the shared"
v4.6.0,resources in the predictor.
v4.6.0,"However the recommended approach is to instantiate one SingleRowPredictor per thread,"
v4.6.0,to avoid contention here.
v4.6.0,create boosting
v4.6.0,initialize the boosting
v4.6.0,create objective function
v4.6.0,initialize the objective function
v4.6.0,create training metric
v4.6.0,reset the boosting
v4.6.0,create objective function
v4.6.0,initialize the objective function
v4.6.0,Workaround https://github.com/microsoft/LightGBM/issues/6142 by locking here
v4.6.0,"This is only a workaround because if predictors are initialized differently it may still behave incorrectly,"
v4.6.0,and because multiple racing Predictor initializations through LGBM_BoosterPredictForMat suffers from that same issue of Predictor init writing things in the booster.
v4.6.0,"Once #6142 is fixed (predictor doesn't write in the Booster as should have been the case since 1c35c3b9ede9adab8ccc5fd7b4b2b6af188a79f0), this line can be removed."
v4.6.0,calculate the nonzero data and indices size
v4.6.0,allocate data and indices arrays
v4.6.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.6.0,aggregated per row feature contribution results
v4.6.0,keep track of the row_vector sizes for parallelization
v4.6.0,copy vector results to output for each row
v4.6.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.6.0,aggregated per row feature contribution results
v4.6.0,calculate number of elements per column to construct
v4.6.0,the CSC matrix with random access
v4.6.0,keep track of column counts
v4.6.0,keep track of beginning index for each column
v4.6.0,keep track of beginning index for each matrix
v4.6.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v4.6.0,store the row index
v4.6.0,update column count
v4.6.0,explicitly declare symbols from LightGBM namespace
v4.6.0,some help functions used to convert data
v4.6.0,Row iterator of on column for CSC matrix
v4.6.0,"return value at idx, only can access by ascent order"
v4.6.0,"return next non-zero pair, if index < 0, means no more data"
v4.6.0,start of c_api functions
v4.6.0,This API is to keep python binding's behavior the same with C++ implementation.
v4.6.0,"Sample count, random seed etc. should be provided in parameters."
v4.6.0,convert internal thread id to be unique based on external thread id
v4.6.0,convert internal thread id to be unique based on external thread id
v4.6.0,sample data first
v4.6.0,sample data first
v4.6.0,sample data first
v4.6.0,local buffer to re-use memory
v4.6.0,sample data first
v4.6.0,no more data
v4.6.0,Prepare the Arrow data
v4.6.0,Initialize the dataset
v4.6.0,"If there is no reference dataset, we first sample indices"
v4.6.0,"Then, we obtain sample values by parallelizing across columns"
v4.6.0,Values need to be copied from the record batches.
v4.6.0,The chunks are iterated over in the inner loop as columns can be treated independently.
v4.6.0,"Finally, we initialize a loader from the sampled values"
v4.6.0,"After sampling and properly initializing all bins, we can add our data to the dataset. Here,"
v4.6.0,we parallelize across rows.
v4.6.0,---- start of booster
v4.6.0,"Naming: In future versions of LightGBM, public API named around `FastConfig` should be made named around"
v4.6.0,"`SingleRowPredictor`, because it is specific to single row prediction, and doesn't actually hold only config."
v4.6.0,For now this is kept as `FastConfig` for backwards compatibility.
v4.6.0,"At the same time, one should consider removing the old non-fast single row public API that stores its Predictor"
v4.6.0,"in the Booster, because that will enable removing these Predictors from the Booster, and associated initialization"
v4.6.0,code.
v4.6.0,Single row in row-major format:
v4.6.0,Apply the configuration
v4.6.0,Set up chunked array and iterators for all columns
v4.6.0,Build row function
v4.6.0,Run prediction
v4.6.0,---- start of some help functions
v4.6.0,data is array of pointers to individual rows
v4.6.0,set number of threads for openmp
v4.6.0,read parameters from config file
v4.6.0,"remove str after ""#"""
v4.6.0,de-duplicate params
v4.6.0,prediction is needed if using input initial model(continued train)
v4.6.0,need to continue training
v4.6.0,sync up random seed for data partition
v4.6.0,load Training data
v4.6.0,load data for distributed training
v4.6.0,load data for single machine
v4.6.0,need save binary file
v4.6.0,create training metric
v4.6.0,only when have metrics then need to construct validation data
v4.6.0,"Add validation data, if it exists"
v4.6.0,add
v4.6.0,need save binary file
v4.6.0,add metric for validation data
v4.6.0,output used time on each iteration
v4.6.0,need init network
v4.6.0,create boosting
v4.6.0,create objective function
v4.6.0,load training data
v4.6.0,initialize the objective function
v4.6.0,initialize the boosting
v4.6.0,add validation data into boosting
v4.6.0,convert model to if-else statement code
v4.6.0,create predictor
v4.6.0,Free memory
v4.6.0,create predictor
v4.6.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v4.6.0,counts for all labels
v4.6.0,"start from top label, and accumulate DCG"
v4.6.0,counts for all labels
v4.6.0,calculate k Max DCG by one pass
v4.6.0,get sorted indices by score
v4.6.0,calculate multi dcg by one pass
v4.6.0,wait for all client start up
v4.6.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v4.6.0,Instead we will handle finalize/abort for MPI in main().
v4.6.0,default set to -1
v4.6.0,"distance at k-th communication, distance[k] = 2^k"
v4.6.0,set incoming rank at k-th communication
v4.6.0,set outgoing rank at k-th communication
v4.6.0,default set as -1
v4.6.0,construct all recursive halving map for all machines
v4.6.0,let 1 << k <= num_machines
v4.6.0,distance of each communication
v4.6.0,"if num_machines = 2^k, don't need to group machines"
v4.6.0,"communication direction, %2 == 0 is positive"
v4.6.0,neighbor at k-th communication
v4.6.0,receive data block at k-th communication
v4.6.0,send data block at k-th communication
v4.6.0,"if num_machines != 2^k, need to group machines"
v4.6.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v4.6.0,let left machine as group leader
v4.6.0,"cache block information for groups, group with 2 machines will have double block size"
v4.6.0,convert from group to node leader
v4.6.0,convert from node to group
v4.6.0,meet new group
v4.6.0,add block len for this group
v4.6.0,calculate the group block start
v4.6.0,not need to construct
v4.6.0,get receive block information
v4.6.0,accumulate block len
v4.6.0,get send block information
v4.6.0,accumulate block len
v4.6.0,static member definition
v4.6.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v4.6.0,assign the blocks to every rank.
v4.6.0,do reduce scatter
v4.6.0,do all gather
v4.6.0,assign blocks
v4.6.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v4.6.0,copy back
v4.6.0,assign blocks
v4.6.0,start all gather
v4.6.0,when num_machines is small and data is large
v4.6.0,use output as receive buffer
v4.6.0,get current local block size
v4.6.0,get out rank
v4.6.0,get in rank
v4.6.0,get send information
v4.6.0,get recv information
v4.6.0,send and recv at same time
v4.6.0,rotate in-place
v4.6.0,use output as receive buffer
v4.6.0,get current local block size
v4.6.0,get send information
v4.6.0,get recv information
v4.6.0,send and recv at same time
v4.6.0,use output as receive buffer
v4.6.0,send and recv at same time
v4.6.0,send local data to neighbor first
v4.6.0,receive neighbor data first
v4.6.0,reduce
v4.6.0,get target
v4.6.0,get send information
v4.6.0,get recv information
v4.6.0,send and recv at same time
v4.6.0,reduce
v4.6.0,send result to neighbor
v4.6.0,receive result from neighbor
v4.6.0,copy result
v4.6.0,start up socket
v4.6.0,parse clients from file
v4.6.0,get ip list of local machine
v4.6.0,get local rank
v4.6.0,construct listener
v4.6.0,construct communication topo
v4.6.0,construct linkers
v4.6.0,free listener
v4.6.0,set timeout
v4.6.0,accept incoming socket
v4.6.0,receive rank
v4.6.0,add new socket
v4.6.0,save ranks that need to connect with
v4.6.0,start listener
v4.6.0,start connect
v4.6.0,let smaller rank connect to larger rank
v4.6.0,send local rank
v4.6.0,wait for listener
v4.6.0,print connected linkers
v4.6.0,only need to copy subset
v4.6.0,avoid to copy subset many times
v4.6.0,avoid out of range
v4.6.0,may need to recopy subset
v4.6.0,valid the type
v4.6.0,parser factory implementation.
v4.6.0,customized parser add-on.
v4.6.0,save header to parser config in case needed.
v4.6.0,save label id to parser config in case needed.
v4.6.0,Constructors
v4.6.0,Get type tag
v4.6.0,Comparisons
v4.6.0,"This has to be separate, not in Statics, because Json() accesses"
v4.6.0,statics().null.
v4.6.0,"advance until next line, or end of input"
v4.6.0,advance until closing tokens
v4.6.0,The usual case: non-escaped characters
v4.6.0,Handle escapes
v4.6.0,Extract 4-byte escape sequence
v4.6.0,Explicitly check length of the substring. The following loop
v4.6.0,relies on std::string returning the terminating NUL when
v4.6.0,accessing str[length]. Checking here reduces brittleness.
v4.6.0,JSON specifies that characters outside the BMP shall be encoded as a
v4.6.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v4.6.0,components. Check whether we're in the middle of such a beast: the
v4.6.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v4.6.0,a trail (low) surrogate.
v4.6.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v4.6.0,per the UTF-16 algorithm.
v4.6.0,Integer part
v4.6.0,Decimal part
v4.6.0,Exponent part
v4.6.0,Check for any trailing garbage
v4.6.0,Documented in json11.hpp
v4.6.0,Check for another object
v4.6.0,get column names
v4.6.0,"support to get header from parser config, so could utilize following label name to id mapping logic."
v4.6.0,load label idx first
v4.6.0,"if parser config file exists, feature names may be changed after customized parser applied."
v4.6.0,clear here so could use default filled feature names during dataset construction.
v4.6.0,may improve by saving real feature names defined in parser in the future.
v4.6.0,erase label column name
v4.6.0,load ignore columns
v4.6.0,load weight idx
v4.6.0,load group idx
v4.6.0,don't support query id in data file when using distributed training
v4.6.0,read data to memory
v4.6.0,sample data
v4.6.0,construct feature bin mappers & clear sample data
v4.6.0,initialize label
v4.6.0,extract features
v4.6.0,sample data from file
v4.6.0,construct feature bin mappers & clear sample data
v4.6.0,initialize label
v4.6.0,extract features
v4.6.0,load data from binary file
v4.6.0,checks whether there's a initial score file when loaded from binary data files
v4.6.0,"the initial score file should with suffix "".bin.init"""
v4.6.0,check meta data
v4.6.0,need to check training data
v4.6.0,read data in memory
v4.6.0,initialize label
v4.6.0,extract features
v4.6.0,Get number of lines of data file
v4.6.0,initialize label
v4.6.0,extract features
v4.6.0,load data from binary file
v4.6.0,checks whether there's a initial score file when loaded from binary data files
v4.6.0,"the initial score file should with suffix "".bin.init"""
v4.6.0,not need to check validation data
v4.6.0,check meta data
v4.6.0,check token
v4.6.0,read feature group definitions
v4.6.0,read feature size
v4.6.0,buffer to read binary file
v4.6.0,check token
v4.6.0,read size of header
v4.6.0,re-allocate space if not enough
v4.6.0,read header
v4.6.0,get header
v4.6.0,read size of meta data
v4.6.0,re-allocate space if not enough
v4.6.0,read meta data
v4.6.0,load meta data
v4.6.0,sample local used data if need to partition
v4.6.0,"if not contain query file, minimal sample unit is one record"
v4.6.0,"if contain query file, minimal sample unit is one query"
v4.6.0,if is new query
v4.6.0,read feature data
v4.6.0,read feature size
v4.6.0,re-allocate space if not enough
v4.6.0,raw data
v4.6.0,fill feature_names_ if not header
v4.6.0,get forced split
v4.6.0,"if only one machine, find bin locally"
v4.6.0,"if have multi-machines, need to find bin distributed"
v4.6.0,different machines will find bin for different features
v4.6.0,start and len will store the process feature indices for different machines
v4.6.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.6.0,free
v4.6.0,gather global feature bin mappers
v4.6.0,restore features bins from buffer
v4.6.0,---- private functions ----
v4.6.0,get header
v4.6.0,num_groups
v4.6.0,real_feature_idx_
v4.6.0,feature2group
v4.6.0,feature2subfeature
v4.6.0,group_bin_boundaries
v4.6.0,group_feature_start_
v4.6.0,group_feature_cnt_
v4.6.0,get feature names
v4.6.0,get forced_bin_bounds_
v4.6.0,"if features are ordered, not need to use hist_buf"
v4.6.0,read all lines
v4.6.0,get query data
v4.6.0,"if not contain query data, minimal sample unit is one record"
v4.6.0,"if contain query data, minimal sample unit is one query"
v4.6.0,if is new query
v4.6.0,get query data
v4.6.0,"if not contain query file, minimal sample unit is one record"
v4.6.0,"if contain query file, minimal sample unit is one query"
v4.6.0,if is new query
v4.6.0,parse features
v4.6.0,get forced split
v4.6.0,"check the range of label_idx, weight_idx and group_idx"
v4.6.0,"skip label check if user input parser config file,"
v4.6.0,because label id is got from raw features while dataset features are consistent with customized parser.
v4.6.0,fill feature_names_ if not header
v4.6.0,start find bins
v4.6.0,"if only one machine, find bin locally"
v4.6.0,start and len will store the process feature indices for different machines
v4.6.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.6.0,free
v4.6.0,gather global feature bin mappers
v4.6.0,restore features bins from buffer
v4.6.0,if doesn't need to prediction with initial model
v4.6.0,parser
v4.6.0,set label
v4.6.0,free processed line:
v4.6.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v4.6.0,text_reader_->Lines()[i].shrink_to_fit();
v4.6.0,push data
v4.6.0,if is used feature
v4.6.0,if need to prediction with initial model
v4.6.0,parser
v4.6.0,set initial score
v4.6.0,set label
v4.6.0,free processed line:
v4.6.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v4.6.0,text_reader_->Lines()[i].shrink_to_fit();
v4.6.0,push data
v4.6.0,if is used feature
v4.6.0,metadata_ will manage space of init_score
v4.6.0,text data can be free after loaded feature values
v4.6.0,parser
v4.6.0,set initial score
v4.6.0,set label
v4.6.0,push data
v4.6.0,if is used feature
v4.6.0,only need part of data
v4.6.0,need full data
v4.6.0,metadata_ will manage space of init_score
v4.6.0,read size of token
v4.6.0,remove duplicates
v4.6.0,deep copy function for BinMapper
v4.6.0,mean size for one bin
v4.6.0,need a new bin
v4.6.0,update bin upper bound
v4.6.0,last bin upper bound
v4.6.0,get number of positive and negative distinct values
v4.6.0,include zero bounds and infinity bound
v4.6.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v4.6.0,find remaining bounds
v4.6.0,find distinct_values first
v4.6.0,push zero in the front
v4.6.0,use the large value
v4.6.0,push zero in the back
v4.6.0,convert to int type first
v4.6.0,sort by counts in descending order
v4.6.0,will ignore the categorical of small counts
v4.6.0,Push the dummy bin for NaN
v4.6.0,Use MissingType::None to represent this bin contains all categoricals
v4.6.0,fix count of NaN bin
v4.6.0,check trivial(num_bin_ == 1) feature
v4.6.0,check useless bin
v4.6.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v4.6.0,so use most_freq_bin_ = default_bin_ when there is not so sparse
v4.6.0,calculate max bin of all features to select the int type in MultiValDenseBin
v4.6.0,"for lambdarank, it needs query data for partition data in distributed learning"
v4.6.0,need convert query_id to boundaries
v4.6.0,check weights
v4.6.0,check positions
v4.6.0,check query boundaries
v4.6.0,contain initial score file
v4.6.0,check weights
v4.6.0,get local weights
v4.6.0,check positions
v4.6.0,get local positions
v4.6.0,check query boundaries
v4.6.0,get local query boundaries
v4.6.0,contain initial score file
v4.6.0,get local initial scores
v4.6.0,re-calculate query weight
v4.6.0,Clear init scores on empty input
v4.6.0,"Note that len here is row count, not num_init_score, so we compare against num_data"
v4.6.0,"We need to use source_size here, because len might not equal size (due to a partially loaded dataset)"
v4.6.0,CUDA is handled after all insertions are complete
v4.6.0,CUDA is handled after all insertions are complete
v4.6.0,Clear weights on empty input
v4.6.0,CUDA is handled after all insertions are complete
v4.6.0,Clear query boundaries on empty input
v4.6.0,save to nullptr
v4.6.0,CUDA is handled after all insertions are complete
v4.6.0,default weight file name
v4.6.0,default position file name
v4.6.0,default init_score file name
v4.6.0,use first line to count number class
v4.6.0,default query file name
v4.6.0,root is in the depth 0
v4.6.0,non-leaf
v4.6.0,leaf
v4.6.0,use this for the missing value conversion
v4.6.0,Predict func by Map to ifelse
v4.6.0,use this for the missing value conversion
v4.6.0,non-leaf
v4.6.0,left subtree
v4.6.0,right subtree
v4.6.0,leaf
v4.6.0,non-leaf
v4.6.0,left subtree
v4.6.0,right subtree
v4.6.0,leaf
v4.6.0,recursive computation of SHAP values for a decision tree
v4.6.0,extend the unique path
v4.6.0,leaf node
v4.6.0,internal node
v4.6.0,"see if we have already split on this feature,"
v4.6.0,if so we undo that split so we can redo it for this node
v4.6.0,recursive sparse computation of SHAP values for a decision tree
v4.6.0,extend the unique path
v4.6.0,leaf node
v4.6.0,internal node
v4.6.0,"see if we have already split on this feature,"
v4.6.0,if so we undo that split so we can redo it for this node
v4.6.0,"if ""verbosity"" was found in params, prefer that to any other aliases"
v4.6.0,"if ""verbose"" was found in params and ""verbosity"" was not, use that value"
v4.6.0,"if ""verbosity"" and ""verbose"" were both missing from params, don't modify LightGBM's log level"
v4.6.0,"otherwise, update LightGBM's log level based on the passed-in value"
v4.6.0,add names of objective function if not providing metric
v4.6.0,equal weights for all classes
v4.6.0,generate seeds by seed.
v4.6.0,sort eval_at
v4.6.0,Only push the non-training data
v4.6.0,check for conflicts
v4.6.0,"check if objective, metric, and num_class match"
v4.6.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v4.6.0,"max_depth defaults to -1, so max_depth>0 implies ""you explicitly overrode the default"""
v4.6.0,
v4.6.0,Changing max_depth while leaving num_leaves at its default (31) can lead to 2 undesirable situations:
v4.6.0,
v4.6.0,* (0 <= max_depth <= 4) it's not possible to produce a tree with 31 leaves
v4.6.0,- this block reduces num_leaves to 2^max_depth
v4.6.0,"* (max_depth > 4) 31 leaves is less than a full depth-wise tree, which might lead to underfitting"
v4.6.0,- this block warns about that
v4.6.0,ref: https://github.com/microsoft/LightGBM/issues/2898#issuecomment-1002860601
v4.6.0,"Fits in an int, and is more restrictive than the current num_leaves"
v4.6.0,force col-wise for gpu version
v4.6.0,force row-wise for cuda version
v4.6.0,linear tree learner must be serial type and run on CPU device
v4.6.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v4.6.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v4.6.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v4.6.0,calculated split gain can be positive even with zero gradient and hessian.
v4.6.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v4.6.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v4.6.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v4.6.0,first round: fill the single val group
v4.6.0,always push the last group
v4.6.0,put dense feature first
v4.6.0,sort by non zero cnt
v4.6.0,"sort by non zero cnt, bigger first"
v4.6.0,shuffle groups
v4.6.0,Using std::swap for vector<bool> will cause the wrong result.
v4.6.0,get num_features
v4.6.0,get bin_mappers
v4.6.0,"for sparse multi value bin, we store the feature bin values with offset added"
v4.6.0,"for dense multi value bin, the feature bin values without offsets are used"
v4.6.0,copy feature bin mapper data
v4.6.0,copy feature bin mapper data
v4.6.0,update CUDA storage for column data and metadata
v4.6.0,"if not pass a filename, just append "".bin"" of original file"
v4.6.0,Write the basic header information for the dataset
v4.6.0,get size of meta data
v4.6.0,write meta data
v4.6.0,write feature data
v4.6.0,get size of feature
v4.6.0,write feature
v4.6.0,write raw data; use row-major order so we can read row-by-row
v4.6.0,Calculate approximate size of output and reserve space
v4.6.0,write feature group definitions
v4.6.0,"Give a little extra just in case, to avoid unnecessary resizes"
v4.6.0,"Write token that marks the data as binary reference, and the version"
v4.6.0,Write the basic definition of the overall dataset
v4.6.0,write feature group definitions
v4.6.0,get size of feature
v4.6.0,write feature
v4.6.0,size of feature names and forced bins
v4.6.0,write header
v4.6.0,write feature names
v4.6.0,write forced bins
v4.6.0,"explicitly initialize template methods, for cross module call"
v4.6.0,"explicitly initialize template methods, for cross module call"
v4.6.0,"Only one multi-val group, just simply merge"
v4.6.0,Skip the leading 0 when copying group_bin_boundaries.
v4.6.0,regenerate other fields
v4.6.0,need to iterate bin iterator
v4.6.0,is dense column
v4.6.0,is sparse column
v4.6.0,initialize the subset cuda column data
v4.6.0,"if one column has too many bins, use a separate partition for that column"
v4.6.0,try if adding this column exceed the maximum number per partition
v4.6.0,"if one column has too many bins, use a separate partition for that column"
v4.6.0,try if adding this column exceed the maximum number per partition
v4.6.0,"if LightGBM-specific default has been set, ignore OpenMP-global config"
v4.6.0,"otherwise, default to OpenMP-global config"
v4.6.0,"ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't"
v4.6.0,use more than that many threads
v4.6.0,store the importance first
v4.6.0,PredictRaw
v4.6.0,PredictRawByMap
v4.6.0,Predict
v4.6.0,PredictByMap
v4.6.0,PredictLeafIndex
v4.6.0,PredictLeafIndexByMap
v4.6.0,output model type
v4.6.0,output number of class
v4.6.0,output label index
v4.6.0,output max_feature_idx
v4.6.0,output objective
v4.6.0,output tree models
v4.6.0,store the importance first
v4.6.0,sort the importance
v4.6.0,use serialized string to restore this object
v4.6.0,Use first 128 chars to avoid exceed the message buffer.
v4.6.0,get number of classes
v4.6.0,get index of label
v4.6.0,get max_feature_idx first
v4.6.0,get average_output
v4.6.0,get feature names
v4.6.0,get monotone_constraints
v4.6.0,set zero
v4.6.0,predict all the trees for one iteration
v4.6.0,check early stopping
v4.6.0,set zero
v4.6.0,predict all the trees for one iteration
v4.6.0,check early stopping
v4.6.0,margin_threshold will be captured by value
v4.6.0,copy and sort
v4.6.0,margin_threshold will be captured by value
v4.6.0,Fix for compiler warnings about reaching end of control
v4.6.0,load forced_splits file
v4.6.0,init tree learner
v4.6.0,push training metrics
v4.6.0,get max feature index
v4.6.0,get label index
v4.6.0,get feature names
v4.6.0,get parser config file content
v4.6.0,check that forced splits does not use feature indices larger than dataset size
v4.6.0,"if need bagging, create buffer"
v4.6.0,"for a validation dataset, we need its score and metric"
v4.6.0,update score
v4.6.0,objective function will calculate gradients and hessians
v4.6.0,output used time per iteration
v4.6.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v4.6.0,boosting first
v4.6.0,use customized objective function
v4.6.0,the check below fails unless objective=custom is provided in the parameters on Booster creation
v4.6.0,need to copy customized gradients when using GOSS
v4.6.0,bagging logic
v4.6.0,need to copy gradients for bagging subset.
v4.6.0,shrinkage by learning rate
v4.6.0,update score
v4.6.0,only add default score one-time
v4.6.0,updates scores
v4.6.0,extend init_scores with zeros
v4.6.0,add model
v4.6.0,reset score
v4.6.0,remove model
v4.6.0,print message for metric
v4.6.0,pop last early_stopping_round_ models
v4.6.0,update training score
v4.6.0,we need to predict out-of-bag scores of data for boosting
v4.6.0,update validation score
v4.6.0,print training metric
v4.6.0,print validation metric
v4.6.0,set zero
v4.6.0,predict all the trees for one iteration
v4.6.0,predict all the trees for one iteration
v4.6.0,push training metrics
v4.6.0,"not same training data, need reset score and others"
v4.6.0,create score tracker
v4.6.0,update score
v4.6.0,resize gradient vectors to copy the customized gradients for goss or bagging with subset
v4.6.0,load forced_splits file
v4.6.0,"if exists initial score, will start from it"
v4.6.0,clear host score buffer
v4.6.0,"Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated"
v4.6.0,"with the larger categorical l2, whereas min_split_gain uses the original l2."
v4.6.0,"if data not enough, or sum hessian too small"
v4.6.0,if data not enough
v4.6.0,if sum hessian too small
v4.6.0,current split gain
v4.6.0,gain with split is worse than without split
v4.6.0,mark as able to be split
v4.6.0,better split point
v4.6.0,recover sum of gradient and hessian from the sum of quantized gradient and hessian
v4.6.0,"Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated"
v4.6.0,"with the larger categorical l2, whereas min_split_gain uses the original l2."
v4.6.0,"if data not enough, or sum hessian too small"
v4.6.0,if data not enough
v4.6.0,if sum hessian too small
v4.6.0,current split gain
v4.6.0,gain with split is worse than without split
v4.6.0,mark as able to be split
v4.6.0,better split point
v4.6.0,Get the max size of pool
v4.6.0,at least need 2 leaves
v4.6.0,push split information for all leaves
v4.6.0,initialize splits for leaf
v4.6.0,initialize data partition
v4.6.0,initialize ordered gradients and hessians
v4.6.0,cannot change is_hist_col_wise during training
v4.6.0,initialize splits for leaf
v4.6.0,initialize data partition
v4.6.0,initialize ordered gradients and hessians
v4.6.0,Get the max size of pool
v4.6.0,at least need 2 leaves
v4.6.0,push split information for all leaves
v4.6.0,some initial works before training
v4.6.0,"set the root value by hand, as it is not handled by splits"
v4.6.0,root leaf
v4.6.0,only root leaf can be splitted on first time
v4.6.0,some initial works before finding best split
v4.6.0,find best threshold for every feature
v4.6.0,Get a leaf with max split gain
v4.6.0,Get split information for best leaf
v4.6.0,"cannot split, quit"
v4.6.0,split tree with best leaf
v4.6.0,reset histogram pool
v4.6.0,initialize data partition
v4.6.0,reset the splits for leaves
v4.6.0,Sumup for root
v4.6.0,use all data
v4.6.0,"use bagging, only use part of data"
v4.6.0,check depth of current leaf
v4.6.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v4.6.0,no enough data to continue
v4.6.0,only have root
v4.6.0,put parent(left) leaf's histograms into larger leaf's histograms
v4.6.0,put parent(left) leaf's histograms to larger leaf's histograms
v4.6.0,construct smaller leaf
v4.6.0,construct larger leaf
v4.6.0,find splits
v4.6.0,only has root leaf
v4.6.0,start at root leaf
v4.6.0,Histogram construction require parent features.
v4.6.0,"then, compute own splits"
v4.6.0,split info should exist because searching in bfs fashion - should have added from parent
v4.6.0,update before tree split
v4.6.0,don't need to update this in data-based parallel model
v4.6.0,"split tree, will return right leaf"
v4.6.0,store the true split gain in tree model
v4.6.0,don't need to update this in data-based parallel model
v4.6.0,store the true split gain in tree model
v4.6.0,init the leaves that used on next iteration
v4.6.0,update leave outputs if needed
v4.6.0,bag_mapper[index_mapper[i]]
v4.6.0,it is needed to filter the features after the above code.
v4.6.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v4.6.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v4.6.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v4.6.0,find splits
v4.6.0,identify features containing nans
v4.6.0,preallocate the matrix used to calculate linear model coefficients
v4.6.0,"store only upper triangular half of matrix as an array, in row-major order"
v4.6.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v4.6.0,we add another 8 to ensure cache lines are not shared among processors
v4.6.0,some initial works before training
v4.6.0,root leaf
v4.6.0,only root leaf can be splitted on first time
v4.6.0,some initial works before finding best split
v4.6.0,find best threshold for every feature
v4.6.0,Get a leaf with max split gain
v4.6.0,Get split information for best leaf
v4.6.0,"cannot split, quit"
v4.6.0,split tree with best leaf
v4.6.0,map data to leaf number
v4.6.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v4.6.0,the coefficients vector is given by
v4.6.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v4.6.0,where:
v4.6.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v4.6.0,"H is the diagonal matrix of the hessian,"
v4.6.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v4.6.0,g is the vector of gradients
v4.6.0,the subscript _T denotes the transpose
v4.6.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v4.6.0,clear the coefficient matrices
v4.6.0,aggregate results from different threads
v4.6.0,copy into eigen matrices and solve
v4.6.0,update the tree properties
v4.6.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.6.0,get feature partition
v4.6.0,get local used features
v4.6.0,get best split at smaller leaf
v4.6.0,find local best split for larger leaf
v4.6.0,sync global best info
v4.6.0,update best split
v4.6.0,"instantiate template classes, otherwise linker cannot find the code"
v4.6.0,initialize SerialTreeLearner
v4.6.0,Get local rank and global machine size
v4.6.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.6.0,allocate buffer for communication
v4.6.0,get block start and block len for reduce scatter
v4.6.0,get buffer_write_start_pos
v4.6.0,get buffer_read_start_pos
v4.6.0,generate feature partition for current tree
v4.6.0,get local used feature
v4.6.0,get block start and block len for reduce scatter
v4.6.0,sync global data sumup info
v4.6.0,global sumup reduce
v4.6.0,copy back
v4.6.0,set global sumup info
v4.6.0,init global data count in leaf
v4.6.0,reset hist num bits according to global num data
v4.6.0,sync global data sumup info
v4.6.0,global sumup reduce
v4.6.0,copy back
v4.6.0,set global sumup info
v4.6.0,init global data count in leaf
v4.6.0,clear histogram buffer before synchronizing
v4.6.0,otherwise histogram contents from the previous iteration will be sent
v4.6.0,construct local histograms
v4.6.0,copy to buffer
v4.6.0,Reduce scatter for histogram
v4.6.0,restore global histograms from buffer
v4.6.0,only root leaf
v4.6.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v4.6.0,find local best split for larger leaf
v4.6.0,sync global best info
v4.6.0,set best split
v4.6.0,need update global number of data in leaf
v4.6.0,reset hist num bits according to global num data
v4.6.0,"instantiate template classes, otherwise linker cannot find the code"
v4.6.0,initialize SerialTreeLearner
v4.6.0,some additional variables needed for GPU trainer
v4.6.0,Initialize GPU buffers and kernels
v4.6.0,some functions used for debugging the GPU histogram construction
v4.6.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v4.6.0,goto err;
v4.6.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v4.6.0,also guarantee that there are at least 2K examples per workgroup
v4.6.0,return 0;
v4.6.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v4.6.0,decide the best number of workgroups working on one feature4 tuple
v4.6.0,set work group size based on feature size
v4.6.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v4.6.0,we need to refresh the kernel arguments after reallocating
v4.6.0,The only argument that needs to be changed later is num_data_
v4.6.0,"the GPU kernel will process all features in one call, and each"
v4.6.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v4.6.0,process one feature4 tuple
v4.6.0,"for the root node, indices are not copied"
v4.6.0,"for constant hessian, hessians are not copied except for the root node"
v4.6.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v4.6.0,and we will launch num_feature workgroups for this kernel
v4.6.0,will launch threads for all features
v4.6.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v4.6.0,copy the results asynchronously. Size depends on if double precision is used
v4.6.0,we will wait for this object in WaitAndGetHistograms
v4.6.0,"when the output is ready, the computation is done"
v4.6.0,values of this feature has been redistributed to multiple bins; need a reduction here
v4.6.0,how many feature-group tuples we have
v4.6.0,leave some safe margin for prefetching
v4.6.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v4.6.0,clear sparse/dense maps
v4.6.0,do nothing if no features can be processed on GPU
v4.6.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v4.6.0,unpin old buffer if necessary before destructing them
v4.6.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v4.6.0,allocate space for gradients and Hessians on device
v4.6.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v4.6.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v4.6.0,copy indices to the device
v4.6.0,histogram bin entry size depends on the precision (single/double)
v4.6.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v4.6.0,each work group generates a sub-histogram of dword_features_ features.
v4.6.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v4.6.0,create atomic counters for inter-group coordination
v4.6.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v4.6.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v4.6.0,looking for dword_features_ non-sparse feature-groups
v4.6.0,decide if we need to redistribute the bin
v4.6.0,multiplier must be a power of 2
v4.6.0,device_bin_mults_.push_back(1);
v4.6.0,found
v4.6.0,for data transfer time
v4.6.0,"Now generate new data structure feature4, and copy data to the device"
v4.6.0,"preallocate arrays for all threads, and pin them"
v4.6.0,building Feature4 bundles; each thread handles dword_features_ features
v4.6.0,one feature datapoint is 4 bits
v4.6.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.6.0,one feature datapoint is one byte
v4.6.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.6.0,Dense bin
v4.6.0,Dense 4-bit bin
v4.6.0,working on the remaining (less than dword_features_) feature groups
v4.6.0,fill the leftover features
v4.6.0,"fill this empty feature with some ""random"" value"
v4.6.0,"fill this empty feature with some ""random"" value"
v4.6.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v4.6.0,deallocate pinned space for feature copying
v4.6.0,data transfer time
v4.6.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v4.6.0,"Something bad happened. Just return ""No log available."""
v4.6.0,"build is okay, log may contain warnings"
v4.6.0,destroy any old kernels
v4.6.0,create OpenCL kernels for different number of workgroups per feature
v4.6.0,currently we don't use constant memory
v4.6.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v4.6.0,kernel with indices in an array
v4.6.0,"kernel with all features enabled, with eliminated branches"
v4.6.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v4.6.0,do nothing if no features can be processed on GPU
v4.6.0,The only argument that needs to be changed later is num_data_
v4.6.0,"hessian is passed as a parameter, but it is not available now."
v4.6.0,hessian will be set in BeforeTrain()
v4.6.0,"Get the max bin size, used for selecting best GPU kernel"
v4.6.0,initialize GPU
v4.6.0,determine which kernel to use based on the max number of bins
v4.6.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.6.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.6.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.6.0,ignore the feature groups that contain categorical features when producing warnings about max_bin.
v4.6.0,"these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin."
v4.6.0,setup GPU kernel arguments after we allocating all the buffers
v4.6.0,GPU memory has to been reallocated because data may have been changed
v4.6.0,setup GPU kernel arguments after we allocating all the buffers
v4.6.0,Copy initial full hessians and gradients to GPU.
v4.6.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v4.6.0,setup hessian parameters only
v4.6.0,hessian is passed as a parameter
v4.6.0,use bagging
v4.6.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v4.6.0,copy used gradients and Hessians to ordered buffer
v4.6.0,transfer the indices to GPU
v4.6.0,transfer hessian to GPU
v4.6.0,setup hessian parameters only
v4.6.0,hessian is passed as a parameter
v4.6.0,transfer gradients to GPU
v4.6.0,only have root
v4.6.0,"Copy indices, gradients and Hessians as early as possible"
v4.6.0,only need to initialize for smaller leaf
v4.6.0,Get leaf boundary
v4.6.0,copy indices to the GPU:
v4.6.0,copy ordered Hessians to the GPU:
v4.6.0,copy ordered gradients to the GPU:
v4.6.0,do nothing if no features can be processed on GPU
v4.6.0,copy data indices if it is not null
v4.6.0,generate and copy ordered_gradients if gradients is not null
v4.6.0,generate and copy ordered_hessians if Hessians is not null
v4.6.0,converted indices in is_feature_used to feature-group indices
v4.6.0,construct the feature masks for dense feature-groups
v4.6.0,"if no feature group is used, just return and do not use GPU"
v4.6.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v4.6.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v4.6.0,"All data have been prepared, now run the GPU kernel"
v4.6.0,construct smaller leaf
v4.6.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v4.6.0,then construct sparse features on CPU
v4.6.0,"wait for GPU to finish, only if GPU is actually used"
v4.6.0,use double precision
v4.6.0,use single precision
v4.6.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v4.6.0,#define GPU_DEBUG_COMPARE
v4.6.0,construct larger leaf
v4.6.0,then construct sparse features on CPU
v4.6.0,"wait for GPU to finish, only if GPU is actually used"
v4.6.0,use double precision
v4.6.0,use single precision
v4.6.0,do some sanity check for the GPU algorithm
v4.6.0,limit top k
v4.6.0,get max bin
v4.6.0,calculate buffer size
v4.6.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.6.0,"left and right on same time, so need double size"
v4.6.0,initialize histograms for global
v4.6.0,sync global data sumup info
v4.6.0,set global sumup info
v4.6.0,init global data count in leaf
v4.6.0,get local sumup
v4.6.0,get local sumup
v4.6.0,get mean number on machines
v4.6.0,weighted gain
v4.6.0,get top k
v4.6.0,"Copy histogram to buffer, and Get local aggregate features"
v4.6.0,copy histograms.
v4.6.0,copy smaller leaf histograms first
v4.6.0,mark local aggregated feature
v4.6.0,copy
v4.6.0,then copy larger leaf histograms
v4.6.0,mark local aggregated feature
v4.6.0,copy
v4.6.0,use local data to find local best splits
v4.6.0,clear histogram buffer before synchronizing
v4.6.0,otherwise histogram contents from the previous iteration will be sent
v4.6.0,find splits
v4.6.0,only has root leaf
v4.6.0,local voting
v4.6.0,gather
v4.6.0,get all top-k from all machines
v4.6.0,global voting
v4.6.0,copy local histgrams to buffer
v4.6.0,Reduce scatter for histogram
v4.6.0,find best split from local aggregated histograms
v4.6.0,restore from buffer
v4.6.0,restore from buffer
v4.6.0,find local best
v4.6.0,find local best split for larger leaf
v4.6.0,sync global best info
v4.6.0,copy back
v4.6.0,set the global number of data for leaves
v4.6.0,init the global sumup info
v4.6.0,"instantiate template classes, otherwise linker cannot find the code"
v4.6.0,allocate CUDA memory
v4.6.0,leave some space for alignment
v4.6.0,input best split info
v4.6.0,for leaf information update
v4.6.0,"gather information for CPU, used for launching kernels"
v4.6.0,for leaf splits information update
v4.6.0,we need restore the order of indices in cuda_data_indices_
v4.6.0,allocate more memory for sum reduction in CUDA
v4.6.0,only the first element records the final sum
v4.6.0,initialize split find task information (a split find task is one pass through the histogram of a feature)
v4.6.0,need to double the size of histogram buffer in global memory when using double precision in histogram construction
v4.6.0,use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes
v4.6.0,use the first gpu by default
v4.6.0,"set the root value by hand, as it is not handled by splits"
v4.6.0,"std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values"
v4.6.0,use feature interaction constraint or sample features by node
v4.5.0,coding: utf-8
v4.5.0,raise deprecation warnings if necessary
v4.5.0,ref: https://github.com/microsoft/LightGBM/issues/6435
v4.5.0,create predictor first
v4.5.0,setting early stopping via global params should be possible
v4.5.0,reduce cost for prediction training data
v4.5.0,process callbacks
v4.5.0,construct booster
v4.5.0,start training
v4.5.0,check evaluation result.
v4.5.0,"ranking task, split according to groups"
v4.5.0,run preprocessing on the data set if needed
v4.5.0,raise deprecation warnings if necessary
v4.5.0,ref: https://github.com/microsoft/LightGBM/issues/6435
v4.5.0,setting early stopping via global params should be possible
v4.5.0,setup callbacks
v4.5.0,coding: utf-8
v4.5.0,"scikit-learn is intentionally imported first here,"
v4.5.0,see https://github.com/microsoft/LightGBM/issues/6509
v4.5.0,dummy function to support older version of scikit-learn
v4.5.0,catching 'ValueError' here because of this:
v4.5.0,https://github.com/microsoft/LightGBM/issues/6365#issuecomment-2002330003
v4.5.0,
v4.5.0,"That's potentially risky as dask does some significant import-time processing,"
v4.5.0,"like loading configuration from environment variables and files, and catching"
v4.5.0,ValueError here might hide issues with that config-loading.
v4.5.0,
v4.5.0,"But in exchange, it's less likely that 'import lightgbm' will fail for"
v4.5.0,"dask-related reasons, which is beneficial for any workloads that are using"
v4.5.0,lightgbm but not its Dask functionality.
v4.5.0,coding: utf-8
v4.5.0,"f(labels, preds)"
v4.5.0,"f(labels, preds, weights)"
v4.5.0,"f(labels, preds, weights, group)"
v4.5.0,"f(labels, preds)"
v4.5.0,"f(labels, preds, weights)"
v4.5.0,"f(labels, preds, weights, group)"
v4.5.0,documentation templates for LGBMModel methods are shared between the classes in
v4.5.0,this module and those in the ``dask`` module
v4.5.0,"It's possible, for example, to pass 3 eval sets through `eval_set`,"
v4.5.0,but only 1 init_score through `eval_init_score`.
v4.5.0,
v4.5.0,This if-else accounts for that possibility.
v4.5.0,register default metric for consistency with callable eval_metric case
v4.5.0,try to deduce from class instance
v4.5.0,overwrite default metric by explicitly set metric
v4.5.0,"use joblib conventions for negative n_jobs, just like scikit-learn"
v4.5.0,"at predict time, this is handled later due to the order of parameter updates"
v4.5.0,Do not modify original args in fit function
v4.5.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v4.5.0,Separate built-in from callable evaluation metrics
v4.5.0,concatenate metric from params (or default if not provided in params) and eval_metric
v4.5.0,copy for consistency
v4.5.0,reduce cost for prediction training data
v4.5.0,free dataset
v4.5.0,retrieve original params that possibly can be used in both training and prediction
v4.5.0,and then overwrite them (considering aliases) with params that were passed directly in prediction
v4.5.0,number of threads can have values with special meaning which is only applied
v4.5.0,"in the scikit-learn interface, these should not reach the c++ side as-is"
v4.5.0,adjust eval metrics to match whether binary or multiclass
v4.5.0,classification is being performed
v4.5.0,"do not modify args, as it causes errors in model selection tools"
v4.5.0,check group data
v4.5.0,coding: utf-8
v4.5.0,coding: utf-8
v4.5.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v4.5.0,coding: utf-8
v4.5.0,coding: utf-8
v4.5.0,typing.TypeGuard was only introduced in Python 3.10
v4.5.0,we don't need lib_lightgbm while building docs
v4.5.0,TypeError: obj is not a string or a number
v4.5.0,ValueError: invalid literal
v4.5.0,Obtain objects to export
v4.5.0,Prepare export
v4.5.0,Export all objects
v4.5.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v4.5.0,ref: https://peps.python.org/pep-0565/#additional-use-case-for-futurewarning
v4.5.0,"lazy evaluation to allow import without dynamic library, e.g., for docs generation"
v4.5.0,"if buffer length is not long enough, re-allocate a buffer"
v4.5.0,avoid side effects on passed-in parameters
v4.5.0,"if main_param_name was provided, keep that value and remove all aliases"
v4.5.0,"if main param name was not found, search for an alias"
v4.5.0,"neither of main_param_name, aliases were found"
v4.5.0,most common case (no nullable dtypes)
v4.5.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.5.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.5.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.5.0,take shallow copy in case we modify categorical columns
v4.5.0,whole column modifications don't change the original df
v4.5.0,determine feature names
v4.5.0,determine categorical features
v4.5.0,use cat cols from DataFrame
v4.5.0,so that the target dtype considers floats
v4.5.0,Get total row number.
v4.5.0,Random access by row index. Used for data sampling.
v4.5.0,Range data access. Used to read data in batch when constructing Dataset.
v4.5.0,Optionally specify batch_size to control range data read size.
v4.5.0,Only required if using ``Dataset.subset()``.
v4.5.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.5.0,avoid memory consumption by arrays concatenation operations
v4.5.0,create numpy array from output arrays
v4.5.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v4.5.0,for CSC there is extra column added
v4.5.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v4.5.0,same shape as input csr or csc matrix except extra column for expected value
v4.5.0,note: make sure we copy data as it will be deallocated next
v4.5.0,"free the temporary native indptr, indices, and data"
v4.5.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.5.0,avoid memory consumption by arrays concatenation operations
v4.5.0,Check that the input is valid: we only handle numbers (for now)
v4.5.0,Prepare prediction output array
v4.5.0,Export Arrow table to C and run prediction
v4.5.0,c type: double**
v4.5.0,each double* element points to start of each column of sample data.
v4.5.0,c type int**
v4.5.0,each int* points to start of indices for each column
v4.5.0,"no min_data, nthreads and verbose in this function"
v4.5.0,check data has header or not
v4.5.0,need to regroup init_score
v4.5.0,process for args
v4.5.0,get categorical features
v4.5.0,"If the params[cat_alias] is equal to categorical_indices, do not report the warning."
v4.5.0,process for reference dataset
v4.5.0,start construct data
v4.5.0,set feature names
v4.5.0,"Select sampled rows, transpose to column order."
v4.5.0,create validation dataset from ref_dataset
v4.5.0,Check that the input is valid: we only handle numbers (for now)
v4.5.0,Export Arrow table to C
v4.5.0,create valid
v4.5.0,construct subset
v4.5.0,create train
v4.5.0,could be updated if data is not freed
v4.5.0,set to None
v4.5.0,"If the data is a arrow data, we can just pass it to C"
v4.5.0,"If a table is being passed, we concatenate the columns. This is only valid for"
v4.5.0,'init_score'.
v4.5.0,we're done if self and reference share a common upstream reference
v4.5.0,Check if the weight contains values other than one
v4.5.0,Set field
v4.5.0,original values can be modified at cpp side
v4.5.0,"if buffer length is not long enough, reallocate buffers"
v4.5.0,"group data from LightGBM is boundaries data, need to convert to group size"
v4.5.0,Training task
v4.5.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v4.5.0,construct booster object
v4.5.0,copy the parameters from train_set
v4.5.0,save reference to data
v4.5.0,buffer for inner predict
v4.5.0,Prediction task
v4.5.0,"if buffer length is not long enough, re-allocate a buffer"
v4.5.0,if a single node tree it won't have `leaf_index` so return 0
v4.5.0,"Create the node record, and populate universal data members"
v4.5.0,Update values to reflect node type (leaf or split)
v4.5.0,traverse the next level of the tree
v4.5.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v4.5.0,and we add node to the list.
v4.5.0,need reset training data
v4.5.0,need to push new valid data
v4.5.0,ensure that existing Booster is freed before replacing it
v4.5.0,with a new one createdfrom file
v4.5.0,"if buffer length is not long enough, re-allocate a buffer"
v4.5.0,"if buffer length is not long enough, reallocate a buffer"
v4.5.0,Copy models
v4.5.0,Get name of features
v4.5.0,"if buffer length is not long enough, reallocate buffers"
v4.5.0,avoid to predict many time in one iteration
v4.5.0,Get num of inner evals
v4.5.0,Get name of eval metrics
v4.5.0,"if buffer length is not long enough, reallocate buffers"
v4.5.0,coding: utf-8
v4.5.0,Callback environment used by callbacks
v4.5.0,"CVBooster holds a list of Booster objects, each needs to be updated"
v4.5.0,"for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set"
v4.5.0,and those metrics are considered for early stopping
v4.5.0,"for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name"
v4.5.0,validation sets are guaranteed to not be identical to the training data in cv()
v4.5.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.5.0,self.best_score_list is initialized to an empty list
v4.5.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.5.0,coding: utf-8
v4.5.0,Acquire port in worker
v4.5.0,schedule futures to retrieve each element of the tuple
v4.5.0,retrieve ports
v4.5.0,Concatenate many parts into one
v4.5.0,construct local eval_set data.
v4.5.0,store indices of eval_set components that were not contained within local parts.
v4.5.0,consolidate parts of each individual eval component.
v4.5.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v4.5.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v4.5.0,filter padding from eval parts then _concat each eval_set component.
v4.5.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v4.5.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v4.5.0,capture whether local_listen_port or its aliases were provided
v4.5.0,capture whether machines or its aliases were provided
v4.5.0,Some passed-in parameters can be removed:
v4.5.0,* 'num_machines': set automatically from Dask worker list
v4.5.0,* 'num_threads': overridden to match nthreads on each Dask process
v4.5.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v4.5.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v4.5.0,X and y are each delayed sub-lists of original eval dask Collections.
v4.5.0,find maximum number of parts in an individual eval set so that we can
v4.5.0,pad eval sets when they come in different sizes.
v4.5.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v4.5.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v4.5.0,first time a chunk of this eval set is added to this part.
v4.5.0,append additional chunks of this eval set to this part.
v4.5.0,ensure that all evaluation parts map uniquely to one part.
v4.5.0,assign sub-eval_set components to worker parts.
v4.5.0,Start computation in the background
v4.5.0,trigger error locally
v4.5.0,Find locations of all parts and map them to particular Dask workers
v4.5.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v4.5.0,data artifacts may not be populated depending on worker returning final estimator.
v4.5.0,assign general validation set settings to fit kwargs.
v4.5.0,resolve aliases for network parameters and pop the result off params.
v4.5.0,these values are added back in calls to `_train_part()`
v4.5.0,figure out network params
v4.5.0,Tell each worker to train on the parts that it has locally
v4.5.0,
v4.5.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v4.5.0,1. there is randomness in the training process unless parameters ``seed``
v4.5.0,and ``deterministic`` are set
v4.5.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v4.5.0,relies on global state (it and all the other LightGBM training processes
v4.5.0,coordinate with each other)
v4.5.0,"if network parameters were changed during training, remove them from the"
v4.5.0,returned model so that they're generated dynamically on every run based
v4.5.0,on the Dask cluster you're connected to and which workers have pieces of
v4.5.0,the training data
v4.5.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v4.5.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v4.5.0,are returned as a list of sparse matrices (one per class)
v4.5.0,"pred_contrib output will have one column per feature,"
v4.5.0,plus one more for the base value
v4.5.0,need to tell Dask the expected type and shape of individual preds
v4.5.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v4.5.0,the code below is used instead to ensure that the sparse type is preserved during concatentation
v4.5.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v4.5.0,Concatenate them to return a list of Dask Arrays.
v4.5.0,"DaskLGBMClassifier does not support group, eval_group."
v4.5.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v4.5.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group."
v4.5.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v4.5.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v4.5.0,DaskLGBMRanker support for callbacks and init_model is not tested
v4.5.0,coding: utf-8
v4.5.0,load or create your dataset
v4.5.0,generate feature names
v4.5.0,create dataset for lightgbm
v4.5.0,"if you want to re-use data, remember to set free_raw_data=False"
v4.5.0,specify your configurations as a dict
v4.5.0,feature_name and categorical_feature
v4.5.0,check feature name
v4.5.0,save model to file
v4.5.0,dump model to JSON (and save to file)
v4.5.0,feature names
v4.5.0,feature importances
v4.5.0,load model to predict
v4.5.0,can only predict with the best iteration (or the saving iteration)
v4.5.0,eval with loaded model
v4.5.0,dump model with pickle
v4.5.0,load model with pickle to predict
v4.5.0,can predict with any iteration when loaded in pickle way
v4.5.0,eval with loaded model
v4.5.0,continue training
v4.5.0,init_model accepts:
v4.5.0,1. model file name
v4.5.0,2. Booster()
v4.5.0,decay learning rates
v4.5.0,reset_parameter callback accepts:
v4.5.0,1. list with length = num_boost_round
v4.5.0,2. function(curr_iter)
v4.5.0,change other parameters during training
v4.5.0,self-defined objective function
v4.5.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v4.5.0,log likelihood loss
v4.5.0,self-defined eval metric
v4.5.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.5.0,binary error
v4.5.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.5.0,This may make built-in evaluation metric calculate wrong results
v4.5.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.5.0,Keep this in mind when you use the customization
v4.5.0,Pass custom objective function through params
v4.5.0,another self-defined eval metric
v4.5.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.5.0,accuracy
v4.5.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.5.0,This may make built-in evaluation metric calculate wrong results
v4.5.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.5.0,Keep this in mind when you use the customization
v4.5.0,Pass custom objective function through params
v4.5.0,callback
v4.5.0,coding: utf-8
v4.5.0,load or create your dataset
v4.5.0,train
v4.5.0,predict
v4.5.0,eval
v4.5.0,feature importances
v4.5.0,self-defined eval metric
v4.5.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.5.0,Root Mean Squared Logarithmic Error (RMSLE)
v4.5.0,train
v4.5.0,another self-defined eval metric
v4.5.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.5.0,Relative Absolute Error (RAE)
v4.5.0,train
v4.5.0,predict
v4.5.0,eval
v4.5.0,other scikit-learn modules
v4.5.0,coding: utf-8
v4.5.0,load or create your dataset
v4.5.0,create dataset for lightgbm
v4.5.0,specify your configurations as a dict
v4.5.0,train
v4.5.0,coding: utf-8
v4.5.0,################
v4.5.0,Simulate some binary data with a single categorical and
v4.5.0,single continuous predictor
v4.5.0,################
v4.5.0,Set up a couple of utilities for our experiments
v4.5.0,################
v4.5.0,Observe the behavior of `binary` and `xentropy` objectives
v4.5.0,Trying this throws an error on non-binary values of y:
v4.5.0,"experiment('binary', label_type='probability', DATA)"
v4.5.0,The speed of `binary` is not drastically different than
v4.5.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v4.5.0,there are reasons to suspect that `binary` should run faster when the
v4.5.0,label is an integer instead of a float
v4.5.0,coding: utf-8
v4.5.0,load or create your dataset
v4.5.0,create dataset for lightgbm
v4.5.0,specify your configurations as a dict
v4.5.0,train
v4.5.0,save model to file
v4.5.0,predict
v4.5.0,eval
v4.5.0,We can also open HDF5 file once and get access to
v4.5.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v4.5.0,
v4.5.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v4.5.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v4.5.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v4.5.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v4.5.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v4.5.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v4.5.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v4.5.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v4.5.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v4.5.0,Here we are just creating a chunk size that matches with batch_size.
v4.5.0,
v4.5.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v4.5.0,lightgbm Dataset.
v4.5.0,Save to 2 HDF5 files for demonstration.
v4.5.0,We can store multiple datasets inside a single HDF5 file.
v4.5.0,Separating X and Y for choosing best chunk size for data loading.
v4.5.0,split training data into two partitions
v4.5.0,make this array dense because we're splitting across
v4.5.0,a sparse boundary to partition the data
v4.5.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v4.5.0,predictions and target values back from workers to the client
v4.5.0,
v4.5.0,"for larger datasets, consider the metrics from dask-ml instead"
v4.5.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v4.5.0,coding: utf-8
v4.5.0,!/usr/bin/env python3
v4.5.0,-*- coding: utf-8 -*-
v4.5.0,
v4.5.0,"LightGBM documentation build configuration file, created by"
v4.5.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v4.5.0,
v4.5.0,This file is execfile()d with the current directory set to its
v4.5.0,containing dir.
v4.5.0,
v4.5.0,Note that not all possible configuration values are present in this
v4.5.0,autogenerated file.
v4.5.0,
v4.5.0,All configuration values have a default; values that are commented out
v4.5.0,serve to show the default.
v4.5.0,"If extensions (or modules to document with autodoc) are in another directory,"
v4.5.0,add these directories to sys.path here. If the directory is relative to the
v4.5.0,"documentation root, use os.path.abspath to make it absolute."
v4.5.0,-- General configuration ------------------------------------------------
v4.5.0,"If your documentation needs a minimal Sphinx version, state it here."
v4.5.0,"Add any Sphinx extension module names here, as strings. They can be"
v4.5.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v4.5.0,ones.
v4.5.0,mock out modules
v4.5.0,hide type hints in API docs
v4.5.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v4.5.0,Only the class' docstring is inserted.
v4.5.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v4.5.0,The master toctree document.
v4.5.0,General information about the project.
v4.5.0,The name of an image file (relative to this directory) to place at the top
v4.5.0,of the sidebar.
v4.5.0,The name of an image file (relative to this directory) to use as a favicon of
v4.5.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v4.5.0,pixels large.
v4.5.0,"The version info for the project you're documenting, acts as replacement for"
v4.5.0,"|version| and |release|, also used in various other places throughout the"
v4.5.0,built documents.
v4.5.0,The short X.Y version.
v4.5.0,"The full version, including alpha/beta/rc tags."
v4.5.0,The language for content autogenerated by Sphinx. Refer to documentation
v4.5.0,for a list of supported languages.
v4.5.0,
v4.5.0,This is also used if you do content translation via gettext catalogs.
v4.5.0,"Usually you set ""language"" from the command line for these cases."
v4.5.0,"List of patterns, relative to source directory, that match files and"
v4.5.0,directories to ignore when looking for source files.
v4.5.0,This patterns also effect to html_static_path and html_extra_path
v4.5.0,The name of the Pygments (syntax highlighting) style to use.
v4.5.0,-- Configuration for C API docs generation ------------------------------
v4.5.0,-- Options for HTML output ----------------------------------------------
v4.5.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v4.5.0,a list of builtin themes.
v4.5.0,Theme options are theme-specific and customize the look and feel of a theme
v4.5.0,"further.  For a list of options available for each theme, see the"
v4.5.0,documentation.
v4.5.0,"Add any paths that contain custom static files (such as style sheets) here,"
v4.5.0,"relative to this directory. They are copied after the builtin static files,"
v4.5.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v4.5.0,-- Options for HTMLHelp output ------------------------------------------
v4.5.0,Output file base name for HTML help builder.
v4.5.0,-- Options for LaTeX output ---------------------------------------------
v4.5.0,The name of an image file (relative to this directory) to place at the top of
v4.5.0,the title page.
v4.5.0,intersphinx configuration
v4.5.0,Warning! The following code can cause buffer overflows on RTD.
v4.5.0,Consider suppressing output completely if RTD project silently fails.
v4.5.0,Refer to https://github.com/svenevs/exhale
v4.5.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.5.0,Warning! The following code can cause buffer overflows on RTD.
v4.5.0,Consider suppressing output completely if RTD project silently fails.
v4.5.0,Refer to https://github.com/svenevs/exhale
v4.5.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.5.0,coding: utf-8
v4.5.0,This is a basic test for floating number parsing.
v4.5.0,Most of the test cases come from:
v4.5.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v4.5.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v4.5.0,FLT_MAX
v4.5.0,FLT_MIN
v4.5.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v4.5.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v4.5.0,Near DBL_MIN
v4.5.0,DBL_MIN 2^-1022
v4.5.0,The behavior for parsing -nan depends on implementation.
v4.5.0,Thus we skip binary check for negative nan.
v4.5.0,See comment in test_cases.
v4.5.0,construct sample data first (use all data for convenience and since size is small)
v4.5.0,Load some test data
v4.5.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.5.0,Add some fake initial_scores and groups so we can test streaming them
v4.5.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.5.0,Load some test data
v4.5.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.5.0,Add some fake initial_scores and groups so we can test streaming them
v4.5.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.5.0,This code is copied and adapted from the official Arrow producer examples:
v4.5.0,https://arrow.apache.org/docs/format/CDataInterface.html#exporting-a-struct-float32-utf8-array
v4.5.0,Free children
v4.5.0,Finalize
v4.5.0,Free children
v4.5.0,Free buffers
v4.5.0,Finalize
v4.5.0,NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests
v4.5.0,"By using `calloc` above, we only need to set 'true' values"
v4.5.0,Arithmetic
v4.5.0,Subscripts
v4.5.0,End
v4.5.0,Check for values in first chunk
v4.5.0,Check for some values in second chunk
v4.5.0,Check end
v4.5.0,Load some test data
v4.5.0,Serialize the reference
v4.5.0,Deserialize the reference
v4.5.0,Confirm 1 successful API call
v4.5.0,Free memory
v4.5.0,Load some test data
v4.5.0,Run a single row prediction and compare with regular Mat prediction:
v4.5.0,"Drop the result from the dataset, we only care about checking that prediction results are equal"
v4.5.0,in both cases
v4.5.0,Now let's run with the single row fast prediction API:
v4.5.0,Free all:
v4.5.0,Test that Data() points to first value written
v4.5.0,Constants
v4.5.0,Start with some content:
v4.5.0,Clear & re-use:
v4.5.0,Output should match new content:
v4.5.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v4.5.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v4.5.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v4.5.0,Compute a new trial address & value & if it is a valid address:
v4.5.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v4.5.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v4.5.0,Check the just-stored value with getitem():
v4.5.0,Also store the just-stored value for future tracking:
v4.5.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v4.5.0,Test in 2 ways that the values are correctly laid out in memory:
v4.5.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.5.0,Use multiple threads to test concurrency
v4.5.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.5.0,Calculate expected boundaries
v4.5.0,Extract a set of rows from the column-based format (still maintaining column based format)
v4.5.0,coding: utf-8
v4.5.0,"at initialization, should be -1"
v4.5.0,updating that value through the C API should work
v4.5.0,resetting to any negative number should set it to -1
v4.5.0,coding: utf-8
v4.5.0,check saved model persistence
v4.5.0,"we need to check the consistency of model file here, so test for exact equal"
v4.5.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v4.5.0,"scores likely to be different, but prediction should still be the same"
v4.5.0,test that shape is checked during prediction
v4.5.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v4.5.0,The following is for demo and testing purpose.
v4.5.0,whole col
v4.5.0,half col
v4.5.0,Create dataset from numpy array directly.
v4.5.0,Create dataset using Sequence.
v4.5.0,Test for validation set.
v4.5.0,Select some random rows as valid data.
v4.5.0,"From Dataset constructor, with dataset from numpy array."
v4.5.0,"From Dataset.create_valid, with dataset from sequence."
v4.5.0,test that method works even with free_raw_data=True
v4.5.0,test that method works but sets raw data to None in case of immergeable data types
v4.5.0,test that method works for different data types
v4.5.0,"Set extremely harsh penalties, so CEGB will block most splits."
v4.5.0,"Compare pairs of penalties, to ensure scaling works as intended"
v4.5.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v4.5.0,"unconstructed, get_* methods should return whatever was provided"
v4.5.0,"before construction, get_field() should raise an exception"
v4.5.0,"constructed, get_* methods should return numpy arrays, even when the provided"
v4.5.0,input was a list of floats or ints
v4.5.0,"get_field(""group"") returns a numpy array with boundaries, instead of size"
v4.5.0,"NOTE: ""position"" is converted to int32 on the C++ side"
v4.5.0,"should resolve duplicate aliases, and prefer the main parameter"
v4.5.0,should choose the highest priority alias and set that value on main param
v4.5.0,if only aliases are used
v4.5.0,should use the default if main param and aliases are missing
v4.5.0,all changes should be made on copies and not modify the original
v4.5.0,preserves None found for main param and still removes aliases
v4.5.0,correctly chooses value when only an alias is provided
v4.5.0,adds None if that's given as the default and param not found
v4.5.0,If callable is found in objective
v4.5.0,Value in params should be preferred to the default_value passed from keyword arguments
v4.5.0,"None of objective or its aliases in params, but default_value is callable."
v4.5.0,"""bad"" = 1 element too many"
v4.5.0,"copy=False is necessary because starting with pandas 3.0, pd.DataFrame() creates"
v4.5.0,a copy of the input numpy array by default
v4.5.0,ref: https://github.com/pandas-dev/pandas/issues/58913
v4.5.0,check that the original data wasn't modified
v4.5.0,check that the built data has the codes
v4.5.0,if all categories were seen during training we just take the codes
v4.5.0,if we only saw 'a' during training we just replace its code
v4.5.0,and leave the rest as nan
v4.5.0,test using defined feature names
v4.5.0,test using default feature names
v4.5.0,check for feature indices outside of range
v4.5.0,"NOTE: this intentionally contains values where num_leaves <, ==, and > (max_depth^2)"
v4.5.0,"NOTE: max_depth < 5 is significant here because the default for num_leaves=31. With max_depth=5,"
v4.5.0,a full depth-wise tree would have 2^5 = 32 leaves.
v4.5.0,coding: utf-8
v4.5.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v4.5.0,set_index ensures partitions are based on group id.
v4.5.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v4.5.0,"separate target, weight from features."
v4.5.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v4.5.0,"so that within each partition, sum(g) = n_samples."
v4.5.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v4.5.0,make one categorical feature relevant to the target
v4.5.0,https://github.com/microsoft/LightGBM/issues/4118
v4.5.0,extra predict() parameters should be passed through correctly
v4.5.0,pref_leaf values should have the right shape
v4.5.0,and values that look like valid tree nodes
v4.5.0,"be sure LightGBM actually used at least one categorical column,"
v4.5.0,and that it was correctly treated as a categorical feature
v4.5.0,shape depends on whether it is binary or multiclass classification
v4.5.0,"in the special case of multi-class classification using scipy sparse matrices,"
v4.5.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v4.5.0,
v4.5.0,"since that case is so different than all other cases, check the relevant things here"
v4.5.0,and then return early
v4.5.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v4.5.0,"be sure LightGBM actually used at least one categorical column,"
v4.5.0,and that it was correctly treated as a categorical feature
v4.5.0,* shape depends on whether it is binary or multiclass classification
v4.5.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v4.5.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v4.5.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v4.5.0,that the output has the right shape and base values are in the right position
v4.5.0,"with a custom objective, prediction result is a raw score instead of predicted class"
v4.5.0,function should have been preserved
v4.5.0,should correctly classify every sample
v4.5.0,probability estimates should be similar
v4.5.0,Scores should be the same
v4.5.0,Predictions should be roughly the same.
v4.5.0,pref_leaf values should have the right shape
v4.5.0,and values that look like valid tree nodes
v4.5.0,extra predict() parameters should be passed through correctly
v4.5.0,"be sure LightGBM actually used at least one categorical column,"
v4.5.0,and that it was correctly treated as a categorical feature
v4.5.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v4.5.0,that the output has the right shape and base values are in the right position
v4.5.0,"be sure LightGBM actually used at least one categorical column,"
v4.5.0,and that it was correctly treated as a categorical feature
v4.5.0,Quantiles should be right
v4.5.0,"be sure LightGBM actually used at least one categorical column,"
v4.5.0,and that it was correctly treated as a categorical feature
v4.5.0,function should have been preserved
v4.5.0,Scores should be the same
v4.5.0,local and Dask predictions should be the same
v4.5.0,predictions should be better than random
v4.5.0,rebalance small dask.Array dataset for better performance.
v4.5.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v4.5.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v4.5.0,distributed ranker should be able to rank decently well and should
v4.5.0,have high rank correlation with scores from serial ranker.
v4.5.0,extra predict() parameters should be passed through correctly
v4.5.0,pref_leaf values should have the right shape
v4.5.0,and values that look like valid tree nodes
v4.5.0,"be sure LightGBM actually used at least one categorical column,"
v4.5.0,and that it was correctly treated as a categorical feature
v4.5.0,rebalance small dask.Array dataset for better performance.
v4.5.0,distributed ranker should be able to rank decently well with the least-squares objective
v4.5.0,and should have high rank correlation with scores from serial ranker.
v4.5.0,function should have been preserved
v4.5.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v4.5.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v4.5.0,"test eval_class_weight, eval_init_score on binary-classification task."
v4.5.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v4.5.0,create eval_sets by creating new datasets or copying training data.
v4.5.0,total number of trees scales up for ova classifier.
v4.5.0,check that early stopping was not applied.
v4.5.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v4.5.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v4.5.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v4.5.0,should be able to use the class without specifying a client
v4.5.0,should be able to set client after construction
v4.5.0,data on cluster1
v4.5.0,create identical data on cluster2
v4.5.0,"at this point, the result of default_client() is client2 since it was the most recently"
v4.5.0,created. So setting client to client1 here to test that you can select a non-default client
v4.5.0,"unfitted model should survive pickling round trip, and pickling"
v4.5.0,shouldn't have side effects on the model object
v4.5.0,client will always be None after unpickling
v4.5.0,"fitted model should survive pickling round trip, and pickling"
v4.5.0,shouldn't have side effects on the model object
v4.5.0,client will always be None after unpickling
v4.5.0,rebalance data to be sure that each worker has a piece of the data
v4.5.0,model 1 - no network parameters given
v4.5.0,model 2 - machines given
v4.5.0,model 3 - local_listen_port given
v4.5.0,training should fail because LightGBM will try to use the same
v4.5.0,port for multiple worker processes on the same machine
v4.5.0,rebalance data to be sure that each worker has a piece of the data
v4.5.0,"test that ""machines"" is actually respected by creating a socket that uses"
v4.5.0,"one of the ports mentioned in ""machines"""
v4.5.0,The above error leaves a worker waiting
v4.5.0,"an informative error should be raised if ""machines"" has duplicates"
v4.5.0,"""client"" should be the only different, and the final argument"
v4.5.0,value of the root node is 0 when init_score is set
v4.5.0,this test is separate because it takes a not-yet-constructed estimator
v4.5.0,coding: utf-8
v4.5.0,coding: utf-8
v4.5.0,"build target, group ID vectors."
v4.5.0,build y/target and group-id vectors with user-specified group sizes.
v4.5.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v4.5.0,groups should contain > 1 element for pairwise learning objective.
v4.5.0,"build feature data, X. Transform first few into informative features."
v4.5.0,"doing this here, at import time, to ensure it only runs once_per import"
v4.5.0,instead of once per assertion
v4.5.0,coding: utf-8
v4.5.0,"NOTE: In the AppVeyor CI, importing pyarrow fails due to an old Visual Studio version. Hence,"
v4.5.0,"we conditionally import pyarrow here (and skip tests if it cannot be imported). However, we"
v4.5.0,"don't want these tests to silently be skipped, hence, we only conditionally import when a"
v4.5.0,specific env var is set.
v4.5.0,----------------------------------------------------------------------------------------------- #
v4.5.0,UTILITIES                                            #
v4.5.0,----------------------------------------------------------------------------------------------- #
v4.5.0,Set random nulls
v4.5.0,Split data into <=2 random chunks
v4.5.0,Turn chunks into array
v4.5.0,----------------------------------------------------------------------------------------------- #
v4.5.0,UNIT TESTS                                           #
v4.5.0,----------------------------------------------------------------------------------------------- #
v4.5.0,------------------------------------------- DATASET ------------------------------------------- #
v4.5.0,-------------------------------------------- FIELDS ------------------------------------------- #
v4.5.0,Check for equality
v4.5.0,-------------------------------------------- LABELS ------------------------------------------- #
v4.5.0,------------------------------------------- WEIGHTS ------------------------------------------- #
v4.5.0,-------------------------------------------- GROUPS ------------------------------------------- #
v4.5.0,----------------------------------------- INIT SCORES ----------------------------------------- #
v4.5.0,------------------------------------------ PREDICTION ----------------------------------------- #
v4.5.0,coding: utf-8
v4.5.0,check that really dummy objective was used and estimator didn't learn anything
v4.5.0,prediction result is actually not transformed (is raw) due to custom objective
v4.5.0,original estimator is unaffected
v4.5.0,"new estimator is unfitted, but has the same parameters"
v4.5.0,Test if random_state is properly stored
v4.5.0,Test if two random states produce identical models
v4.5.0,Test if subsequent fits sample from random_state object and produce different models
v4.5.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v4.5.0,why fixed seed?
v4.5.0,sometimes there is no difference how cols are treated (cat or not cat)
v4.5.0,With default params
v4.5.0,Tests same probabilities
v4.5.0,Tests same predictions
v4.5.0,Tests same raw scores
v4.5.0,Tests same leaf indices
v4.5.0,Tests same feature contributions
v4.5.0,Tests other parameters for the prediction works
v4.5.0,Tests start_iteration
v4.5.0,"Tests same probabilities, starting from iteration 10"
v4.5.0,"Tests same predictions, starting from iteration 10"
v4.5.0,"Tests same raw scores, starting from iteration 10"
v4.5.0,"Tests same leaf indices, starting from iteration 10"
v4.5.0,"Tests same feature contributions, starting from iteration 10"
v4.5.0,"Tests other parameters for the prediction works, starting from iteration 10"
v4.5.0,Test multiclass binary classification
v4.5.0,test that params passed in predict have higher priority
v4.5.0,"no custom objective, no custom metric"
v4.5.0,default metric
v4.5.0,non-default metric
v4.5.0,no metric
v4.5.0,non-default metric in eval_metric
v4.5.0,non-default metric with non-default metric in eval_metric
v4.5.0,non-default metric with multiple metrics in eval_metric
v4.5.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v4.5.0,default metric for non-default objective
v4.5.0,non-default metric for non-default objective
v4.5.0,no metric
v4.5.0,non-default metric in eval_metric for non-default objective
v4.5.0,non-default metric with non-default metric in eval_metric for non-default objective
v4.5.0,non-default metric with multiple metrics in eval_metric for non-default objective
v4.5.0,"custom objective, no custom metric"
v4.5.0,default regression metric for custom objective
v4.5.0,non-default regression metric for custom objective
v4.5.0,multiple regression metrics for custom objective
v4.5.0,no metric
v4.5.0,default regression metric with non-default metric in eval_metric for custom objective
v4.5.0,non-default regression metric with metric in eval_metric for custom objective
v4.5.0,multiple regression metrics with metric in eval_metric for custom objective
v4.5.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v4.5.0,"no custom objective, custom metric"
v4.5.0,default metric with custom metric
v4.5.0,non-default metric with custom metric
v4.5.0,multiple metrics with custom metric
v4.5.0,custom metric (disable default metric)
v4.5.0,default metric for non-default objective with custom metric
v4.5.0,non-default metric for non-default objective with custom metric
v4.5.0,multiple metrics for non-default objective with custom metric
v4.5.0,custom metric (disable default metric for non-default objective)
v4.5.0,"custom objective, custom metric"
v4.5.0,custom metric for custom objective
v4.5.0,non-default regression metric with custom metric for custom objective
v4.5.0,multiple regression metrics with custom metric for custom objective
v4.5.0,default metric and invalid binary metric is replaced with multiclass alternative
v4.5.0,invalid binary metric is replaced with multiclass alternative
v4.5.0,default metric for non-default multiclass objective
v4.5.0,and invalid binary metric is replaced with multiclass alternative
v4.5.0,default metric and invalid multiclass metric is replaced with binary alternative
v4.5.0,invalid multiclass metric is replaced with binary alternative for custom objective
v4.5.0,the evaluation metric changes to multiclass metric even num classes is 2 for multiclass objective
v4.5.0,the evaluation metric changes to multiclass metric even num classes is 2 for ovr objective
v4.5.0,"Verify that can receive a list of metrics, only callable"
v4.5.0,Verify that can receive a list of custom and built-in metrics
v4.5.0,Verify that works as expected when eval_metric is empty
v4.5.0,"Verify that can receive a list of metrics, only built-in"
v4.5.0,Verify that eval_metric is robust to receiving a list with None
v4.5.0,feval
v4.5.0,single eval_set
v4.5.0,two eval_set
v4.5.0,"input is a numpy array, which doesn't have feature names. LightGBM adds"
v4.5.0,"feature names to the fitted model, which is inconsistent with sklearn's behavior"
v4.5.0,'val_minus_two' here is the expected number of threads for n_jobs=-2
v4.5.0,"Note: according to joblib's formula, a value of n_jobs=-2 means"
v4.5.0,"""use all but one thread"" (formula: n_cpus + 1 + n_jobs)"
v4.5.0,try to predict with a different feature
v4.5.0,check that disabling the check doesn't raise the error
v4.5.0,"make weights and init_score same types as y, just to avoid"
v4.5.0,a huge number of combinations and therefore test cases
v4.5.0,"make weights and init_score same types as y, just to avoid"
v4.5.0,a huge number of combinations and therefore test cases
v4.5.0,coding: utf-8
v4.5.0,we're in a leaf now
v4.5.0,check that the rest of the elements have black color
v4.5.0,check that we got to the expected leaf
v4.5.0,coding: utf-8
v4.5.0,coding: utf-8
v4.5.0,check that default gives same result as k = 1
v4.5.0,check against independent calculation for k = 1
v4.5.0,check against independent calculation for k = 2
v4.5.0,check against independent calculation for k = 10
v4.5.0,check cases where predictions are equal
v4.5.0,should give same result as binary auc for 2 classes
v4.5.0,test the case where all predictions are equal
v4.5.0,test that weighted data gives different auc_mu
v4.5.0,test that equal data weights give same auc_mu as unweighted data
v4.5.0,should give 1 when accuracy = 1
v4.5.0,test loading class weights
v4.5.0,Simulates position bias for a given ranking dataset.
v4.5.0,The ouput dataset is identical to the input one with the exception for the relevance labels.
v4.5.0,The new labels are generated according to an instance of a cascade user model:
v4.5.0,"for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker"
v4.5.0,"(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)"
v4.5.0,and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;
v4.5.0,"at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,"
v4.5.0,new_label=0. Thus the generated new labels are biased towards the baseline ranker.
v4.5.0,"The positions of the documents in the ranked lists produced by the baseline, are returned."
v4.5.0,a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it
v4.5.0,an instantiation of a cascade model where the user stops with probability 0.2 after observing each document
v4.5.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.5.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.5.0,add extra row to position file
v4.5.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.5.0,test setting positions through Dataset constructor with numpy array
v4.5.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.5.0,test setting positions through Dataset constructor with pandas Series
v4.5.0,test setting positions through set_position
v4.5.0,test get_position works
v4.5.0,no early stopping
v4.5.0,early stopping occurs
v4.5.0,regular early stopping
v4.5.0,positive min_delta
v4.5.0,test custom eval metrics
v4.5.0,"shuffle = False, override metric in params"
v4.5.0,"shuffle = True, callbacks"
v4.5.0,enable display training loss
v4.5.0,self defined folds
v4.5.0,LambdaRank
v4.5.0,... with l2 metric
v4.5.0,... with NDCG (default) metric
v4.5.0,self defined folds with lambdarank
v4.5.0,init_model from an in-memory Booster
v4.5.0,init_model from a text file
v4.5.0,predictions should be identical
v4.5.0,with early stopping
v4.5.0,predict by each fold booster
v4.5.0,check that each booster predicted using the best iteration
v4.5.0,fold averaging
v4.5.0,without early stopping
v4.5.0,test feature_names with whitespaces
v4.5.0,This has non-ascii strings.
v4.5.0,check that passing parameters to the constructor raises warning and ignores them
v4.5.0,check inference isn't affected by unknown parameter
v4.5.0,entries whose values should reflect params passed to lgb.train()
v4.5.0,'l1' was passed in with alias 'mae'
v4.5.0,NOTE: this was passed in with alias 'sub_row'
v4.5.0,entries with default values of params
v4.5.0,add device-specific entries
v4.5.0,
v4.5.0,passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...
v4.5.0,https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377
v4.5.0,check that model text has all expected param entries
v4.5.0,"since Booster.model_to_string() is used when pickling, check that parameters all"
v4.5.0,roundtrip pickling successfully too
v4.5.0,why fixed seed?
v4.5.0,sometimes there is no difference how cols are treated (cat or not cat)
v4.5.0,take subsets and train
v4.5.0,generate CSR sparse dataset
v4.5.0,convert data to dense and get back same contribs
v4.5.0,validate the values are the same
v4.5.0,validate using CSC matrix
v4.5.0,validate the values are the same
v4.5.0,generate CSR sparse dataset
v4.5.0,convert data to dense and get back same contribs
v4.5.0,validate the values are the same
v4.5.0,validate using CSC matrix
v4.5.0,validate the values are the same
v4.5.0,"@pytest.mark.skipif(psutil.virtual_memory().available / 1024 / 1024 / 1024 < 3, reason=""not enough RAM"")"
v4.5.0,def test_int32_max_sparse_contribs(rng):
v4.5.0,"params = {""objective"": ""binary""}"
v4.5.0,"train_features = rng.uniform(size=(100, 1000))"
v4.5.0,train_targets = [0] * 50 + [1] * 50
v4.5.0,"lgb_train = lgb.Dataset(train_features, train_targets)"
v4.5.0,"gbm = lgb.train(params, lgb_train, num_boost_round=2)"
v4.5.0,"csr_input_shape = (3000000, 1000)"
v4.5.0,test_features = csr_matrix(csr_input_shape)
v4.5.0,"for i in range(0, csr_input_shape[0], csr_input_shape[0] // 6):"
v4.5.0,"for j in range(0, 1000, 100):"
v4.5.0,"test_features[i, j] = random.random()"
v4.5.0,"y_pred_csr = gbm.predict(test_features, pred_contrib=True)"
v4.5.0,# Note there is an extra column added to the output for the expected value
v4.5.0,"csr_output_shape = (csr_input_shape[0], csr_input_shape[1] + 1)"
v4.5.0,assert y_pred_csr.shape == csr_output_shape
v4.5.0,"y_pred_csc = gbm.predict(test_features.tocsc(), pred_contrib=True)"
v4.5.0,# Note output CSC shape should be same as CSR output shape
v4.5.0,assert y_pred_csc.shape == csr_output_shape
v4.5.0,test sliced labels
v4.5.0,append some columns
v4.5.0,append some rows
v4.5.0,test sliced 2d matrix
v4.5.0,test sliced CSR
v4.5.0,trees start at position 1.
v4.5.0,split_features are in 4th line.
v4.5.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v4.5.0,The penalization is so high that the first 2 features should not be used here
v4.5.0,Check that a very high penalization is the same as not using the features at all
v4.5.0,check refit accepts dataset_params
v4.5.0,the following checks that dart and rf with mape can predict outside the 0-1 range
v4.5.0,https://github.com/microsoft/LightGBM/issues/1579
v4.5.0,"no custom objective, no feval"
v4.5.0,default metric
v4.5.0,non-default metric in params
v4.5.0,default metric in args
v4.5.0,non-default metric in args
v4.5.0,metric in args overwrites one in params
v4.5.0,metric in args overwrites one in params
v4.5.0,multiple metrics in params
v4.5.0,multiple metrics in args
v4.5.0,remove default metric by 'None' in list
v4.5.0,remove default metric by 'None' aliases
v4.5.0,"custom objective, no feval"
v4.5.0,no default metric
v4.5.0,metric in params
v4.5.0,metric in args
v4.5.0,metric in args overwrites its' alias in params
v4.5.0,multiple metrics in params
v4.5.0,multiple metrics in args
v4.5.0,"no custom objective, feval"
v4.5.0,default metric with custom one
v4.5.0,non-default metric in params with custom one
v4.5.0,default metric in args with custom one
v4.5.0,non-default metric in args with custom one
v4.5.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.5.0,multiple metrics in params with custom one
v4.5.0,multiple metrics in args with custom one
v4.5.0,custom metric is evaluated despite 'None' is passed
v4.5.0,"custom objective, feval"
v4.5.0,"no default metric, only custom one"
v4.5.0,metric in params with custom one
v4.5.0,metric in args with custom one
v4.5.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.5.0,multiple metrics in params with custom one
v4.5.0,multiple metrics in args with custom one
v4.5.0,custom metric is evaluated despite 'None' is passed
v4.5.0,"no custom objective, no feval"
v4.5.0,default metric
v4.5.0,default metric in params
v4.5.0,non-default metric in params
v4.5.0,multiple metrics in params
v4.5.0,remove default metric by 'None' aliases
v4.5.0,"custom objective, no feval"
v4.5.0,no default metric
v4.5.0,metric in params
v4.5.0,multiple metrics in params
v4.5.0,"no custom objective, feval"
v4.5.0,default metric with custom one
v4.5.0,default metric in params with custom one
v4.5.0,non-default metric in params with custom one
v4.5.0,multiple metrics in params with custom one
v4.5.0,custom metric is evaluated despite 'None' is passed
v4.5.0,"custom objective, feval"
v4.5.0,"no default metric, only custom one"
v4.5.0,metric in params with custom one
v4.5.0,multiple metrics in params with custom one
v4.5.0,custom metric is evaluated despite 'None' is passed
v4.5.0,Custom objective replaces multiclass
v4.5.0,multiclass default metric
v4.5.0,multiclass default metric with custom one
v4.5.0,multiclass metric alias with custom one for custom objective
v4.5.0,no metric for invalid class_num
v4.5.0,custom metric for invalid class_num
v4.5.0,multiclass metric alias with custom one with invalid class_num
v4.5.0,multiclass default metric without num_class
v4.5.0,multiclass metric alias
v4.5.0,multiclass metric
v4.5.0,non-valid metric for multiclass objective
v4.5.0,non-default num_class for default objective
v4.5.0,no metric with non-default num_class for custom objective
v4.5.0,multiclass metric alias for custom objective
v4.5.0,multiclass metric for custom objective
v4.5.0,binary metric with non-default num_class for custom objective
v4.5.0,Expect three metrics but mean and stdv for each metric
v4.5.0,test XGBoost-style return value
v4.5.0,test numpy-style return value
v4.5.0,test bins string type
v4.5.0,test histogram is disabled for categorical features
v4.5.0,test for lgb.train
v4.5.0,test feval for lgb.train
v4.5.0,test with two valid data for lgb.train
v4.5.0,test for lgb.cv
v4.5.0,test feval for lgb.cv
v4.5.0,test that binning works properly for features with only positive or only negative values
v4.5.0,decreasing without freeing raw data is allowed
v4.5.0,decreasing before lazy init is allowed
v4.5.0,increasing is allowed
v4.5.0,decreasing with disabled filter is allowed
v4.5.0,decreasing with enabled filter is disallowed;
v4.5.0,also changes of other params are disallowed
v4.5.0,check extra trees increases regularization
v4.5.0,check path smoothing increases regularization
v4.5.0,test edge case with one leaf
v4.5.0,check that constraint containing all features is equivalent to no constraint
v4.5.0,check that constraint partitioning the features reduces train accuracy
v4.5.0,check that constraints consisting of single features reduce accuracy further
v4.5.0,test that interaction constraints work when not all features are used
v4.5.0,check that number of threads does not affect result
v4.5.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v4.5.0,test again with nans in data
v4.5.0,test again with bagging
v4.5.0,test with a feature that has only one non-nan value
v4.5.0,test with a categorical feature
v4.5.0,test refit: same results on same data
v4.5.0,test refit with save and load
v4.5.0,test refit: different results training on different data
v4.5.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v4.5.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v4.5.0,"test the case where start_iteration <= 0, and num_iteration is None"
v4.5.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v4.5.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v4.5.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v4.5.0,test for regression
v4.5.0,test both with and without early stopping
v4.5.0,test for multi-class
v4.5.0,test both with and without early stopping
v4.5.0,test for binary
v4.5.0,test both with and without early stopping
v4.5.0,test against sklearn average precision metric
v4.5.0,test that average precision is 1 where model predicts perfectly
v4.5.0,data as float64
v4.5.0,test all features were used
v4.5.0,test the score is better than predicting the mean
v4.5.0,test all predictions are equal using different input dtypes
v4.5.0,introduce some missing values
v4.5.0,"in recent versions of pandas, type 'bool' is incompatible with nan values in x4"
v4.5.0,train with regular dtypes
v4.5.0,convert to nullable dtypes
v4.5.0,test training succeeds
v4.5.0,test all features were used
v4.5.0,test the score is better than predicting the mean
v4.5.0,test equal predictions
v4.5.0,test data are taken from bug report
v4.5.0,https://github.com/microsoft/LightGBM/issues/4708
v4.5.0,modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811
v4.5.0,and https://github.com/microsoft/LightGBM/pull/5087
v4.5.0,test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree
v4.5.0,which is done in the fix #5164
v4.5.0,without the fix:
v4.5.0,Check failed: (best_split_info.left_count) > (0)
v4.5.0,try to predict with a different feature
v4.5.0,check that disabling the check doesn't raise the error
v4.5.0,try to refit with a different feature
v4.5.0,check that disabling the check doesn't raise the error
v4.5.0,coding: utf-8
v4.5.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v4.5.0,Double-precision floats are only supported on x86_64 with PoCL
v4.5.0,coding: utf-8
v4.5.0,coding: utf-8
v4.5.0,"Note: MSVC has issues with Altrep classes, so they are disabled for it."
v4.5.0,See: https://github.com/microsoft/LightGBM/pull/6213#issuecomment-2111025768
v4.5.0,These are helper functions to allow doing a stack unwind
v4.5.0,"after an R allocation error, which would trigger a long jump."
v4.5.0,convert from one-based to zero-based index
v4.5.0,"if any feature names were larger than allocated size,"
v4.5.0,allow for a larger size and try again
v4.5.0,convert from boundaries to size
v4.5.0,--- start Booster interfaces
v4.5.0,"if any eval names were larger than allocated size,"
v4.5.0,allow for a larger size and try again
v4.5.0,"Note: for some reason, MSVC crashes when an error is thrown here"
v4.5.0,"if the buffer variable is defined as 'std::unique_ptr<std::vector<char>>',"
v4.5.0,but not if it is defined as '<std::vector<char>'.
v4.5.0,"if the model string was larger than the initial buffer, call the function again, writing directly to the R object"
v4.5.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.5.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.5.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.5.0,.Call() calls
v4.5.0,coding: utf-8
v4.5.0,alias table
v4.5.0,names
v4.5.0,from strings
v4.5.0,tails
v4.5.0,tails
v4.5.0,the following are stored as comma separated strings but are arrays in the wrappers
v4.5.0,coding: utf-8
v4.5.0,Single row predictor to abstract away caching logic
v4.5.0,Prevent the booster from being modified while we have a predictor relying on it during prediction
v4.5.0,If several threads try to predict at the same time using the same SingleRowPredictor
v4.5.0,"we want them to still provide correct values, so the mutex is necessary due to the shared"
v4.5.0,resources in the predictor.
v4.5.0,"However the recommended approach is to instantiate one SingleRowPredictor per thread,"
v4.5.0,to avoid contention here.
v4.5.0,create boosting
v4.5.0,initialize the boosting
v4.5.0,create objective function
v4.5.0,initialize the objective function
v4.5.0,create training metric
v4.5.0,reset the boosting
v4.5.0,create objective function
v4.5.0,initialize the objective function
v4.5.0,Workaround https://github.com/microsoft/LightGBM/issues/6142 by locking here
v4.5.0,"This is only a workaround because if predictors are initialized differently it may still behave incorrectly,"
v4.5.0,and because multiple racing Predictor initializations through LGBM_BoosterPredictForMat suffers from that same issue of Predictor init writing things in the booster.
v4.5.0,"Once #6142 is fixed (predictor doesn't write in the Booster as should have been the case since 1c35c3b9ede9adab8ccc5fd7b4b2b6af188a79f0), this line can be removed."
v4.5.0,calculate the nonzero data and indices size
v4.5.0,allocate data and indices arrays
v4.5.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.5.0,aggregated per row feature contribution results
v4.5.0,keep track of the row_vector sizes for parallelization
v4.5.0,copy vector results to output for each row
v4.5.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.5.0,aggregated per row feature contribution results
v4.5.0,calculate number of elements per column to construct
v4.5.0,the CSC matrix with random access
v4.5.0,keep track of column counts
v4.5.0,keep track of beginning index for each column
v4.5.0,keep track of beginning index for each matrix
v4.5.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v4.5.0,store the row index
v4.5.0,update column count
v4.5.0,explicitly declare symbols from LightGBM namespace
v4.5.0,some help functions used to convert data
v4.5.0,Row iterator of on column for CSC matrix
v4.5.0,"return value at idx, only can access by ascent order"
v4.5.0,"return next non-zero pair, if index < 0, means no more data"
v4.5.0,start of c_api functions
v4.5.0,This API is to keep python binding's behavior the same with C++ implementation.
v4.5.0,"Sample count, random seed etc. should be provided in parameters."
v4.5.0,convert internal thread id to be unique based on external thread id
v4.5.0,convert internal thread id to be unique based on external thread id
v4.5.0,sample data first
v4.5.0,sample data first
v4.5.0,sample data first
v4.5.0,local buffer to re-use memory
v4.5.0,sample data first
v4.5.0,no more data
v4.5.0,Prepare the Arrow data
v4.5.0,Initialize the dataset
v4.5.0,"If there is no reference dataset, we first sample indices"
v4.5.0,"Then, we obtain sample values by parallelizing across columns"
v4.5.0,Values need to be copied from the record batches.
v4.5.0,The chunks are iterated over in the inner loop as columns can be treated independently.
v4.5.0,"Finally, we initialize a loader from the sampled values"
v4.5.0,"After sampling and properly initializing all bins, we can add our data to the dataset. Here,"
v4.5.0,we parallelize across rows.
v4.5.0,---- start of booster
v4.5.0,"Naming: In future versions of LightGBM, public API named around `FastConfig` should be made named around"
v4.5.0,"`SingleRowPredictor`, because it is specific to single row prediction, and doesn't actually hold only config."
v4.5.0,For now this is kept as `FastConfig` for backwards compatibility.
v4.5.0,"At the same time, one should consider removing the old non-fast single row public API that stores its Predictor"
v4.5.0,"in the Booster, because that will enable removing these Predictors from the Booster, and associated initialization"
v4.5.0,code.
v4.5.0,Single row in row-major format:
v4.5.0,Apply the configuration
v4.5.0,Set up chunked array and iterators for all columns
v4.5.0,Build row function
v4.5.0,Run prediction
v4.5.0,---- start of some help functions
v4.5.0,data is array of pointers to individual rows
v4.5.0,set number of threads for openmp
v4.5.0,read parameters from config file
v4.5.0,"remove str after ""#"""
v4.5.0,de-duplicate params
v4.5.0,prediction is needed if using input initial model(continued train)
v4.5.0,need to continue training
v4.5.0,sync up random seed for data partition
v4.5.0,load Training data
v4.5.0,load data for distributed training
v4.5.0,load data for single machine
v4.5.0,need save binary file
v4.5.0,create training metric
v4.5.0,only when have metrics then need to construct validation data
v4.5.0,"Add validation data, if it exists"
v4.5.0,add
v4.5.0,need save binary file
v4.5.0,add metric for validation data
v4.5.0,output used time on each iteration
v4.5.0,need init network
v4.5.0,create boosting
v4.5.0,create objective function
v4.5.0,load training data
v4.5.0,initialize the objective function
v4.5.0,initialize the boosting
v4.5.0,add validation data into boosting
v4.5.0,convert model to if-else statement code
v4.5.0,create predictor
v4.5.0,Free memory
v4.5.0,create predictor
v4.5.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v4.5.0,counts for all labels
v4.5.0,"start from top label, and accumulate DCG"
v4.5.0,counts for all labels
v4.5.0,calculate k Max DCG by one pass
v4.5.0,get sorted indices by score
v4.5.0,calculate multi dcg by one pass
v4.5.0,wait for all client start up
v4.5.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v4.5.0,Instead we will handle finalize/abort for MPI in main().
v4.5.0,default set to -1
v4.5.0,"distance at k-th communication, distance[k] = 2^k"
v4.5.0,set incoming rank at k-th commuication
v4.5.0,set outgoing rank at k-th commuication
v4.5.0,default set as -1
v4.5.0,construct all recursive halving map for all machines
v4.5.0,let 1 << k <= num_machines
v4.5.0,distance of each communication
v4.5.0,"if num_machines = 2^k, don't need to group machines"
v4.5.0,"communication direction, %2 == 0 is positive"
v4.5.0,neighbor at k-th communication
v4.5.0,receive data block at k-th communication
v4.5.0,send data block at k-th communication
v4.5.0,"if num_machines != 2^k, need to group machines"
v4.5.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v4.5.0,let left machine as group leader
v4.5.0,"cache block information for groups, group with 2 machines will have double block size"
v4.5.0,convert from group to node leader
v4.5.0,convert from node to group
v4.5.0,meet new group
v4.5.0,add block len for this group
v4.5.0,calculate the group block start
v4.5.0,not need to construct
v4.5.0,get receive block information
v4.5.0,accumulate block len
v4.5.0,get send block information
v4.5.0,accumulate block len
v4.5.0,static member definition
v4.5.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v4.5.0,assign the blocks to every rank.
v4.5.0,do reduce scatter
v4.5.0,do all gather
v4.5.0,assign blocks
v4.5.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v4.5.0,copy back
v4.5.0,assign blocks
v4.5.0,start all gather
v4.5.0,when num_machines is small and data is large
v4.5.0,use output as receive buffer
v4.5.0,get current local block size
v4.5.0,get out rank
v4.5.0,get in rank
v4.5.0,get send information
v4.5.0,get recv information
v4.5.0,send and recv at same time
v4.5.0,rotate in-place
v4.5.0,use output as receive buffer
v4.5.0,get current local block size
v4.5.0,get send information
v4.5.0,get recv information
v4.5.0,send and recv at same time
v4.5.0,use output as receive buffer
v4.5.0,send and recv at same time
v4.5.0,send local data to neighbor first
v4.5.0,receive neighbor data first
v4.5.0,reduce
v4.5.0,get target
v4.5.0,get send information
v4.5.0,get recv information
v4.5.0,send and recv at same time
v4.5.0,reduce
v4.5.0,send result to neighbor
v4.5.0,receive result from neighbor
v4.5.0,copy result
v4.5.0,start up socket
v4.5.0,parse clients from file
v4.5.0,get ip list of local machine
v4.5.0,get local rank
v4.5.0,construct listener
v4.5.0,construct communication topo
v4.5.0,construct linkers
v4.5.0,free listener
v4.5.0,set timeout
v4.5.0,accept incoming socket
v4.5.0,receive rank
v4.5.0,add new socket
v4.5.0,save ranks that need to connect with
v4.5.0,start listener
v4.5.0,start connect
v4.5.0,let smaller rank connect to larger rank
v4.5.0,send local rank
v4.5.0,wait for listener
v4.5.0,print connected linkers
v4.5.0,only need to copy subset
v4.5.0,avoid to copy subset many times
v4.5.0,avoid out of range
v4.5.0,may need to recopy subset
v4.5.0,valid the type
v4.5.0,parser factory implementation.
v4.5.0,customized parser add-on.
v4.5.0,save header to parser config in case needed.
v4.5.0,save label id to parser config in case needed.
v4.5.0,Constructors
v4.5.0,Get type tag
v4.5.0,Comparisons
v4.5.0,"This has to be separate, not in Statics, because Json() accesses"
v4.5.0,statics().null.
v4.5.0,"advance until next line, or end of input"
v4.5.0,advance until closing tokens
v4.5.0,The usual case: non-escaped characters
v4.5.0,Handle escapes
v4.5.0,Extract 4-byte escape sequence
v4.5.0,Explicitly check length of the substring. The following loop
v4.5.0,relies on std::string returning the terminating NUL when
v4.5.0,accessing str[length]. Checking here reduces brittleness.
v4.5.0,JSON specifies that characters outside the BMP shall be encoded as a
v4.5.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v4.5.0,components. Check whether we're in the middle of such a beast: the
v4.5.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v4.5.0,a trail (low) surrogate.
v4.5.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v4.5.0,per the UTF-16 algorithm.
v4.5.0,Integer part
v4.5.0,Decimal part
v4.5.0,Exponent part
v4.5.0,Check for any trailing garbage
v4.5.0,Documented in json11.hpp
v4.5.0,Check for another object
v4.5.0,get column names
v4.5.0,"support to get header from parser config, so could utilize following label name to id mapping logic."
v4.5.0,load label idx first
v4.5.0,"if parser config file exists, feature names may be changed after customized parser applied."
v4.5.0,clear here so could use default filled feature names during dataset construction.
v4.5.0,may improve by saving real feature names defined in parser in the future.
v4.5.0,erase label column name
v4.5.0,load ignore columns
v4.5.0,load weight idx
v4.5.0,load group idx
v4.5.0,don't support query id in data file when using distributed training
v4.5.0,read data to memory
v4.5.0,sample data
v4.5.0,construct feature bin mappers & clear sample data
v4.5.0,initialize label
v4.5.0,extract features
v4.5.0,sample data from file
v4.5.0,construct feature bin mappers & clear sample data
v4.5.0,initialize label
v4.5.0,extract features
v4.5.0,load data from binary file
v4.5.0,checks whether there's a initial score file when loaded from binary data files
v4.5.0,"the initial score file should with suffix "".bin.init"""
v4.5.0,check meta data
v4.5.0,need to check training data
v4.5.0,read data in memory
v4.5.0,initialize label
v4.5.0,extract features
v4.5.0,Get number of lines of data file
v4.5.0,initialize label
v4.5.0,extract features
v4.5.0,load data from binary file
v4.5.0,checks whether there's a initial score file when loaded from binary data files
v4.5.0,"the initial score file should with suffix "".bin.init"""
v4.5.0,not need to check validation data
v4.5.0,check meta data
v4.5.0,check token
v4.5.0,read feature group definitions
v4.5.0,read feature size
v4.5.0,buffer to read binary file
v4.5.0,check token
v4.5.0,read size of header
v4.5.0,re-allocate space if not enough
v4.5.0,read header
v4.5.0,get header
v4.5.0,read size of meta data
v4.5.0,re-allocate space if not enough
v4.5.0,read meta data
v4.5.0,load meta data
v4.5.0,sample local used data if need to partition
v4.5.0,"if not contain query file, minimal sample unit is one record"
v4.5.0,"if contain query file, minimal sample unit is one query"
v4.5.0,if is new query
v4.5.0,read feature data
v4.5.0,read feature size
v4.5.0,re-allocate space if not enough
v4.5.0,raw data
v4.5.0,fill feature_names_ if not header
v4.5.0,get forced split
v4.5.0,"if only one machine, find bin locally"
v4.5.0,"if have multi-machines, need to find bin distributed"
v4.5.0,different machines will find bin for different features
v4.5.0,start and len will store the process feature indices for different machines
v4.5.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.5.0,free
v4.5.0,gather global feature bin mappers
v4.5.0,restore features bins from buffer
v4.5.0,---- private functions ----
v4.5.0,get header
v4.5.0,num_groups
v4.5.0,real_feature_idx_
v4.5.0,feature2group
v4.5.0,feature2subfeature
v4.5.0,group_bin_boundaries
v4.5.0,group_feature_start_
v4.5.0,group_feature_cnt_
v4.5.0,get feature names
v4.5.0,get forced_bin_bounds_
v4.5.0,"if features are ordered, not need to use hist_buf"
v4.5.0,read all lines
v4.5.0,get query data
v4.5.0,"if not contain query data, minimal sample unit is one record"
v4.5.0,"if contain query data, minimal sample unit is one query"
v4.5.0,if is new query
v4.5.0,get query data
v4.5.0,"if not contain query file, minimal sample unit is one record"
v4.5.0,"if contain query file, minimal sample unit is one query"
v4.5.0,if is new query
v4.5.0,parse features
v4.5.0,get forced split
v4.5.0,"check the range of label_idx, weight_idx and group_idx"
v4.5.0,"skip label check if user input parser config file,"
v4.5.0,because label id is got from raw features while dataset features are consistent with customized parser.
v4.5.0,fill feature_names_ if not header
v4.5.0,start find bins
v4.5.0,"if only one machine, find bin locally"
v4.5.0,start and len will store the process feature indices for different machines
v4.5.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.5.0,free
v4.5.0,gather global feature bin mappers
v4.5.0,restore features bins from buffer
v4.5.0,if doesn't need to prediction with initial model
v4.5.0,parser
v4.5.0,set label
v4.5.0,free processed line:
v4.5.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v4.5.0,text_reader_->Lines()[i].shrink_to_fit();
v4.5.0,push data
v4.5.0,if is used feature
v4.5.0,if need to prediction with initial model
v4.5.0,parser
v4.5.0,set initial score
v4.5.0,set label
v4.5.0,free processed line:
v4.5.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v4.5.0,text_reader_->Lines()[i].shrink_to_fit();
v4.5.0,push data
v4.5.0,if is used feature
v4.5.0,metadata_ will manage space of init_score
v4.5.0,text data can be free after loaded feature values
v4.5.0,parser
v4.5.0,set initial score
v4.5.0,set label
v4.5.0,push data
v4.5.0,if is used feature
v4.5.0,only need part of data
v4.5.0,need full data
v4.5.0,metadata_ will manage space of init_score
v4.5.0,read size of token
v4.5.0,remove duplicates
v4.5.0,deep copy function for BinMapper
v4.5.0,mean size for one bin
v4.5.0,need a new bin
v4.5.0,update bin upper bound
v4.5.0,last bin upper bound
v4.5.0,get number of positive and negative distinct values
v4.5.0,include zero bounds and infinity bound
v4.5.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v4.5.0,find remaining bounds
v4.5.0,find distinct_values first
v4.5.0,push zero in the front
v4.5.0,use the large value
v4.5.0,push zero in the back
v4.5.0,convert to int type first
v4.5.0,sort by counts in descending order
v4.5.0,will ignore the categorical of small counts
v4.5.0,Push the dummy bin for NaN
v4.5.0,Use MissingType::None to represent this bin contains all categoricals
v4.5.0,fix count of NaN bin
v4.5.0,check trivial(num_bin_ == 1) feature
v4.5.0,check useless bin
v4.5.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v4.5.0,so use most_freq_bin_ = default_bin_ when there is not so sparse
v4.5.0,calculate max bin of all features to select the int type in MultiValDenseBin
v4.5.0,"for lambdarank, it needs query data for partition data in distributed learning"
v4.5.0,need convert query_id to boundaries
v4.5.0,check weights
v4.5.0,check positions
v4.5.0,check query boundries
v4.5.0,contain initial score file
v4.5.0,check weights
v4.5.0,get local weights
v4.5.0,check positions
v4.5.0,get local positions
v4.5.0,check query boundries
v4.5.0,get local query boundaries
v4.5.0,contain initial score file
v4.5.0,get local initial scores
v4.5.0,re-calculate query weight
v4.5.0,Clear init scores on empty input
v4.5.0,"Note that len here is row count, not num_init_score, so we compare against num_data"
v4.5.0,"We need to use source_size here, because len might not equal size (due to a partially loaded dataset)"
v4.5.0,CUDA is handled after all insertions are complete
v4.5.0,CUDA is handled after all insertions are complete
v4.5.0,Clear weights on empty input
v4.5.0,CUDA is handled after all insertions are complete
v4.5.0,Clear query boundaries on empty input
v4.5.0,save to nullptr
v4.5.0,CUDA is handled after all insertions are complete
v4.5.0,default weight file name
v4.5.0,default position file name
v4.5.0,default init_score file name
v4.5.0,use first line to count number class
v4.5.0,default query file name
v4.5.0,root is in the depth 0
v4.5.0,non-leaf
v4.5.0,leaf
v4.5.0,use this for the missing value conversion
v4.5.0,Predict func by Map to ifelse
v4.5.0,use this for the missing value conversion
v4.5.0,non-leaf
v4.5.0,left subtree
v4.5.0,right subtree
v4.5.0,leaf
v4.5.0,non-leaf
v4.5.0,left subtree
v4.5.0,right subtree
v4.5.0,leaf
v4.5.0,recursive computation of SHAP values for a decision tree
v4.5.0,extend the unique path
v4.5.0,leaf node
v4.5.0,internal node
v4.5.0,"see if we have already split on this feature,"
v4.5.0,if so we undo that split so we can redo it for this node
v4.5.0,recursive sparse computation of SHAP values for a decision tree
v4.5.0,extend the unique path
v4.5.0,leaf node
v4.5.0,internal node
v4.5.0,"see if we have already split on this feature,"
v4.5.0,if so we undo that split so we can redo it for this node
v4.5.0,"if ""verbosity"" was found in params, prefer that to any other aliases"
v4.5.0,"if ""verbose"" was found in params and ""verbosity"" was not, use that value"
v4.5.0,"if ""verbosity"" and ""verbose"" were both missing from params, don't modify LightGBM's log level"
v4.5.0,"otherwise, update LightGBM's log level based on the passed-in value"
v4.5.0,add names of objective function if not providing metric
v4.5.0,equal weights for all classes
v4.5.0,generate seeds by seed.
v4.5.0,sort eval_at
v4.5.0,Only push the non-training data
v4.5.0,check for conflicts
v4.5.0,"check if objective, metric, and num_class match"
v4.5.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v4.5.0,"max_depth defaults to -1, so max_depth>0 implies ""you explicitly overrode the default"""
v4.5.0,
v4.5.0,Changing max_depth while leaving num_leaves at its default (31) can lead to 2 undesirable situations:
v4.5.0,
v4.5.0,* (0 <= max_depth <= 4) it's not possible to produce a tree with 31 leaves
v4.5.0,- this block reduces num_leaves to 2^max_depth
v4.5.0,"* (max_depth > 4) 31 leaves is less than a full depth-wise tree, which might lead to underfitting"
v4.5.0,- this block warns about that
v4.5.0,ref: https://github.com/microsoft/LightGBM/issues/2898#issuecomment-1002860601
v4.5.0,"Fits in an int, and is more restrictive than the current num_leaves"
v4.5.0,"force col-wise for gpu, and cuda version"
v4.5.0,force row-wise for cuda version
v4.5.0,linear tree learner must be serial type and run on CPU device
v4.5.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v4.5.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v4.5.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v4.5.0,calculated split gain can be positive even with zero gradient and hessian.
v4.5.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v4.5.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v4.5.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v4.5.0,first round: fill the single val group
v4.5.0,always push the last group
v4.5.0,put dense feature first
v4.5.0,sort by non zero cnt
v4.5.0,"sort by non zero cnt, bigger first"
v4.5.0,shuffle groups
v4.5.0,Using std::swap for vector<bool> will cause the wrong result.
v4.5.0,get num_features
v4.5.0,get bin_mappers
v4.5.0,"for sparse multi value bin, we store the feature bin values with offset added"
v4.5.0,"for dense multi value bin, the feature bin values without offsets are used"
v4.5.0,copy feature bin mapper data
v4.5.0,copy feature bin mapper data
v4.5.0,update CUDA storage for column data and metadata
v4.5.0,"if not pass a filename, just append "".bin"" of original file"
v4.5.0,Write the basic header information for the dataset
v4.5.0,get size of meta data
v4.5.0,write meta data
v4.5.0,write feature data
v4.5.0,get size of feature
v4.5.0,write feature
v4.5.0,write raw data; use row-major order so we can read row-by-row
v4.5.0,Calculate approximate size of output and reserve space
v4.5.0,write feature group definitions
v4.5.0,"Give a little extra just in case, to avoid unnecessary resizes"
v4.5.0,"Write token that marks the data as binary reference, and the version"
v4.5.0,Write the basic definition of the overall dataset
v4.5.0,write feature group definitions
v4.5.0,get size of feature
v4.5.0,write feature
v4.5.0,size of feature names and forced bins
v4.5.0,write header
v4.5.0,write feature names
v4.5.0,write forced bins
v4.5.0,"explicitly initialize template methods, for cross module call"
v4.5.0,"explicitly initialize template methods, for cross module call"
v4.5.0,"Only one multi-val group, just simply merge"
v4.5.0,Skip the leading 0 when copying group_bin_boundaries.
v4.5.0,regenerate other fields
v4.5.0,need to iterate bin iterator
v4.5.0,is dense column
v4.5.0,is sparse column
v4.5.0,initialize the subset cuda column data
v4.5.0,"if one column has too many bins, use a separate partition for that column"
v4.5.0,try if adding this column exceed the maximum number per partition
v4.5.0,"if one column has too many bins, use a separate partition for that column"
v4.5.0,try if adding this column exceed the maximum number per partition
v4.5.0,"if LightGBM-specific default has been set, ignore OpenMP-global config"
v4.5.0,"otherwise, default to OpenMP-global config"
v4.5.0,"ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't"
v4.5.0,use more than that many threads
v4.5.0,store the importance first
v4.5.0,PredictRaw
v4.5.0,PredictRawByMap
v4.5.0,Predict
v4.5.0,PredictByMap
v4.5.0,PredictLeafIndex
v4.5.0,PredictLeafIndexByMap
v4.5.0,output model type
v4.5.0,output number of class
v4.5.0,output label index
v4.5.0,output max_feature_idx
v4.5.0,output objective
v4.5.0,output tree models
v4.5.0,store the importance first
v4.5.0,sort the importance
v4.5.0,use serialized string to restore this object
v4.5.0,Use first 128 chars to avoid exceed the message buffer.
v4.5.0,get number of classes
v4.5.0,get index of label
v4.5.0,get max_feature_idx first
v4.5.0,get average_output
v4.5.0,get feature names
v4.5.0,get monotone_constraints
v4.5.0,set zero
v4.5.0,predict all the trees for one iteration
v4.5.0,check early stopping
v4.5.0,set zero
v4.5.0,predict all the trees for one iteration
v4.5.0,check early stopping
v4.5.0,margin_threshold will be captured by value
v4.5.0,copy and sort
v4.5.0,margin_threshold will be captured by value
v4.5.0,Fix for compiler warnings about reaching end of control
v4.5.0,load forced_splits file
v4.5.0,init tree learner
v4.5.0,push training metrics
v4.5.0,get max feature index
v4.5.0,get label index
v4.5.0,get feature names
v4.5.0,get parser config file content
v4.5.0,check that forced splits does not use feature indices larger than dataset size
v4.5.0,"if need bagging, create buffer"
v4.5.0,"for a validation dataset, we need its score and metric"
v4.5.0,update score
v4.5.0,objective function will calculate gradients and hessians
v4.5.0,output used time per iteration
v4.5.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v4.5.0,boosting first
v4.5.0,use customized objective function
v4.5.0,the check below fails unless objective=custom is provided in the parameters on Booster creation
v4.5.0,need to copy customized gradients when using GOSS
v4.5.0,bagging logic
v4.5.0,need to copy gradients for bagging subset.
v4.5.0,shrinkage by learning rate
v4.5.0,update score
v4.5.0,only add default score one-time
v4.5.0,updates scores
v4.5.0,add model
v4.5.0,reset score
v4.5.0,remove model
v4.5.0,print message for metric
v4.5.0,pop last early_stopping_round_ models
v4.5.0,update training score
v4.5.0,we need to predict out-of-bag scores of data for boosting
v4.5.0,update validation score
v4.5.0,print training metric
v4.5.0,print validation metric
v4.5.0,set zero
v4.5.0,predict all the trees for one iteration
v4.5.0,predict all the trees for one iteration
v4.5.0,push training metrics
v4.5.0,"not same training data, need reset score and others"
v4.5.0,create score tracker
v4.5.0,update score
v4.5.0,resize gradient vectors to copy the customized gradients for goss or bagging with subset
v4.5.0,load forced_splits file
v4.5.0,"if exists initial score, will start from it"
v4.5.0,clear host score buffer
v4.5.0,"Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated"
v4.5.0,"with the larger categorical l2, whereas min_split_gain uses the original l2."
v4.5.0,"if data not enough, or sum hessian too small"
v4.5.0,if data not enough
v4.5.0,if sum hessian too small
v4.5.0,current split gain
v4.5.0,gain with split is worse than without split
v4.5.0,mark as able to be split
v4.5.0,better split point
v4.5.0,recover sum of gradient and hessian from the sum of quantized gradient and hessian
v4.5.0,"Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated"
v4.5.0,"with the larger categorical l2, whereas min_split_gain uses the original l2."
v4.5.0,"if data not enough, or sum hessian too small"
v4.5.0,if data not enough
v4.5.0,if sum hessian too small
v4.5.0,current split gain
v4.5.0,gain with split is worse than without split
v4.5.0,mark as able to be split
v4.5.0,better split point
v4.5.0,Get the max size of pool
v4.5.0,at least need 2 leaves
v4.5.0,push split information for all leaves
v4.5.0,initialize splits for leaf
v4.5.0,initialize data partition
v4.5.0,initialize ordered gradients and hessians
v4.5.0,cannot change is_hist_col_wise during training
v4.5.0,initialize splits for leaf
v4.5.0,initialize data partition
v4.5.0,initialize ordered gradients and hessians
v4.5.0,Get the max size of pool
v4.5.0,at least need 2 leaves
v4.5.0,push split information for all leaves
v4.5.0,some initial works before training
v4.5.0,root leaf
v4.5.0,only root leaf can be splitted on first time
v4.5.0,some initial works before finding best split
v4.5.0,find best threshold for every feature
v4.5.0,Get a leaf with max split gain
v4.5.0,Get split information for best leaf
v4.5.0,"cannot split, quit"
v4.5.0,split tree with best leaf
v4.5.0,reset histogram pool
v4.5.0,initialize data partition
v4.5.0,reset the splits for leaves
v4.5.0,Sumup for root
v4.5.0,use all data
v4.5.0,"use bagging, only use part of data"
v4.5.0,check depth of current leaf
v4.5.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v4.5.0,no enough data to continue
v4.5.0,only have root
v4.5.0,put parent(left) leaf's histograms into larger leaf's histograms
v4.5.0,put parent(left) leaf's histograms to larger leaf's histograms
v4.5.0,construct smaller leaf
v4.5.0,construct larger leaf
v4.5.0,find splits
v4.5.0,only has root leaf
v4.5.0,start at root leaf
v4.5.0,Histogram construction require parent features.
v4.5.0,"then, compute own splits"
v4.5.0,split info should exist because searching in bfs fashion - should have added from parent
v4.5.0,update before tree split
v4.5.0,don't need to update this in data-based parallel model
v4.5.0,"split tree, will return right leaf"
v4.5.0,store the true split gain in tree model
v4.5.0,don't need to update this in data-based parallel model
v4.5.0,store the true split gain in tree model
v4.5.0,init the leaves that used on next iteration
v4.5.0,update leave outputs if needed
v4.5.0,bag_mapper[index_mapper[i]]
v4.5.0,it is needed to filter the features after the above code.
v4.5.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v4.5.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v4.5.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v4.5.0,find splits
v4.5.0,identify features containing nans
v4.5.0,preallocate the matrix used to calculate linear model coefficients
v4.5.0,"store only upper triangular half of matrix as an array, in row-major order"
v4.5.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v4.5.0,we add another 8 to ensure cache lines are not shared among processors
v4.5.0,some initial works before training
v4.5.0,root leaf
v4.5.0,only root leaf can be splitted on first time
v4.5.0,some initial works before finding best split
v4.5.0,find best threshold for every feature
v4.5.0,Get a leaf with max split gain
v4.5.0,Get split information for best leaf
v4.5.0,"cannot split, quit"
v4.5.0,split tree with best leaf
v4.5.0,map data to leaf number
v4.5.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v4.5.0,the coefficients vector is given by
v4.5.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v4.5.0,where:
v4.5.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v4.5.0,"H is the diagonal matrix of the hessian,"
v4.5.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v4.5.0,g is the vector of gradients
v4.5.0,the subscript _T denotes the transpose
v4.5.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v4.5.0,clear the coefficient matrices
v4.5.0,aggregate results from different threads
v4.5.0,copy into eigen matrices and solve
v4.5.0,update the tree properties
v4.5.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.5.0,get feature partition
v4.5.0,get local used features
v4.5.0,get best split at smaller leaf
v4.5.0,find local best split for larger leaf
v4.5.0,sync global best info
v4.5.0,update best split
v4.5.0,"instantiate template classes, otherwise linker cannot find the code"
v4.5.0,initialize SerialTreeLearner
v4.5.0,Get local rank and global machine size
v4.5.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.5.0,allocate buffer for communication
v4.5.0,get block start and block len for reduce scatter
v4.5.0,get buffer_write_start_pos
v4.5.0,get buffer_read_start_pos
v4.5.0,generate feature partition for current tree
v4.5.0,get local used feature
v4.5.0,get block start and block len for reduce scatter
v4.5.0,sync global data sumup info
v4.5.0,global sumup reduce
v4.5.0,copy back
v4.5.0,set global sumup info
v4.5.0,init global data count in leaf
v4.5.0,reset hist num bits according to global num data
v4.5.0,sync global data sumup info
v4.5.0,global sumup reduce
v4.5.0,copy back
v4.5.0,set global sumup info
v4.5.0,init global data count in leaf
v4.5.0,clear histogram buffer before synchronizing
v4.5.0,otherwise histogram contents from the previous iteration will be sent
v4.5.0,construct local histograms
v4.5.0,copy to buffer
v4.5.0,Reduce scatter for histogram
v4.5.0,restore global histograms from buffer
v4.5.0,only root leaf
v4.5.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v4.5.0,find local best split for larger leaf
v4.5.0,sync global best info
v4.5.0,set best split
v4.5.0,need update global number of data in leaf
v4.5.0,reset hist num bits according to global num data
v4.5.0,"instantiate template classes, otherwise linker cannot find the code"
v4.5.0,initialize SerialTreeLearner
v4.5.0,some additional variables needed for GPU trainer
v4.5.0,Initialize GPU buffers and kernels
v4.5.0,some functions used for debugging the GPU histogram construction
v4.5.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v4.5.0,goto err;
v4.5.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v4.5.0,also guarantee that there are at least 2K examples per workgroup
v4.5.0,return 0;
v4.5.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v4.5.0,decide the best number of workgroups working on one feature4 tuple
v4.5.0,set work group size based on feature size
v4.5.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v4.5.0,we need to refresh the kernel arguments after reallocating
v4.5.0,The only argument that needs to be changed later is num_data_
v4.5.0,"the GPU kernel will process all features in one call, and each"
v4.5.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v4.5.0,process one feature4 tuple
v4.5.0,"for the root node, indices are not copied"
v4.5.0,"for constant hessian, hessians are not copied except for the root node"
v4.5.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v4.5.0,and we will launch num_feature workgroups for this kernel
v4.5.0,will launch threads for all features
v4.5.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v4.5.0,copy the results asynchronously. Size depends on if double precision is used
v4.5.0,we will wait for this object in WaitAndGetHistograms
v4.5.0,"when the output is ready, the computation is done"
v4.5.0,values of this feature has been redistributed to multiple bins; need a reduction here
v4.5.0,how many feature-group tuples we have
v4.5.0,leave some safe margin for prefetching
v4.5.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v4.5.0,clear sparse/dense maps
v4.5.0,do nothing if no features can be processed on GPU
v4.5.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v4.5.0,unpin old buffer if necessary before destructing them
v4.5.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v4.5.0,allocate space for gradients and Hessians on device
v4.5.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v4.5.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v4.5.0,copy indices to the device
v4.5.0,histogram bin entry size depends on the precision (single/double)
v4.5.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v4.5.0,each work group generates a sub-histogram of dword_features_ features.
v4.5.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v4.5.0,create atomic counters for inter-group coordination
v4.5.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v4.5.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v4.5.0,looking for dword_features_ non-sparse feature-groups
v4.5.0,decide if we need to redistribute the bin
v4.5.0,multiplier must be a power of 2
v4.5.0,device_bin_mults_.push_back(1);
v4.5.0,found
v4.5.0,for data transfer time
v4.5.0,"Now generate new data structure feature4, and copy data to the device"
v4.5.0,"preallocate arrays for all threads, and pin them"
v4.5.0,building Feature4 bundles; each thread handles dword_features_ features
v4.5.0,one feature datapoint is 4 bits
v4.5.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.5.0,one feature datapoint is one byte
v4.5.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.5.0,Dense bin
v4.5.0,Dense 4-bit bin
v4.5.0,working on the remaining (less than dword_features_) feature groups
v4.5.0,fill the leftover features
v4.5.0,"fill this empty feature with some ""random"" value"
v4.5.0,"fill this empty feature with some ""random"" value"
v4.5.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v4.5.0,deallocate pinned space for feature copying
v4.5.0,data transfer time
v4.5.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v4.5.0,"Something bad happened. Just return ""No log available."""
v4.5.0,"build is okay, log may contain warnings"
v4.5.0,destroy any old kernels
v4.5.0,create OpenCL kernels for different number of workgroups per feature
v4.5.0,currently we don't use constant memory
v4.5.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v4.5.0,kernel with indices in an array
v4.5.0,"kernel with all features enabled, with eliminated branches"
v4.5.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v4.5.0,do nothing if no features can be processed on GPU
v4.5.0,The only argument that needs to be changed later is num_data_
v4.5.0,"hessian is passed as a parameter, but it is not available now."
v4.5.0,hessian will be set in BeforeTrain()
v4.5.0,"Get the max bin size, used for selecting best GPU kernel"
v4.5.0,initialize GPU
v4.5.0,determine which kernel to use based on the max number of bins
v4.5.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.5.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.5.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.5.0,ignore the feature groups that contain categorical features when producing warnings about max_bin.
v4.5.0,"these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin."
v4.5.0,setup GPU kernel arguments after we allocating all the buffers
v4.5.0,GPU memory has to been reallocated because data may have been changed
v4.5.0,setup GPU kernel arguments after we allocating all the buffers
v4.5.0,Copy initial full hessians and gradients to GPU.
v4.5.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v4.5.0,setup hessian parameters only
v4.5.0,hessian is passed as a parameter
v4.5.0,use bagging
v4.5.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v4.5.0,copy used gradients and Hessians to ordered buffer
v4.5.0,transfer the indices to GPU
v4.5.0,transfer hessian to GPU
v4.5.0,setup hessian parameters only
v4.5.0,hessian is passed as a parameter
v4.5.0,transfer gradients to GPU
v4.5.0,only have root
v4.5.0,"Copy indices, gradients and Hessians as early as possible"
v4.5.0,only need to initialize for smaller leaf
v4.5.0,Get leaf boundary
v4.5.0,copy indices to the GPU:
v4.5.0,copy ordered Hessians to the GPU:
v4.5.0,copy ordered gradients to the GPU:
v4.5.0,do nothing if no features can be processed on GPU
v4.5.0,copy data indices if it is not null
v4.5.0,generate and copy ordered_gradients if gradients is not null
v4.5.0,generate and copy ordered_hessians if Hessians is not null
v4.5.0,converted indices in is_feature_used to feature-group indices
v4.5.0,construct the feature masks for dense feature-groups
v4.5.0,"if no feature group is used, just return and do not use GPU"
v4.5.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v4.5.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v4.5.0,"All data have been prepared, now run the GPU kernel"
v4.5.0,construct smaller leaf
v4.5.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v4.5.0,then construct sparse features on CPU
v4.5.0,"wait for GPU to finish, only if GPU is actually used"
v4.5.0,use double precision
v4.5.0,use single precision
v4.5.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v4.5.0,#define GPU_DEBUG_COMPARE
v4.5.0,construct larger leaf
v4.5.0,then construct sparse features on CPU
v4.5.0,"wait for GPU to finish, only if GPU is actually used"
v4.5.0,use double precision
v4.5.0,use single precision
v4.5.0,do some sanity check for the GPU algorithm
v4.5.0,limit top k
v4.5.0,get max bin
v4.5.0,calculate buffer size
v4.5.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.5.0,"left and right on same time, so need double size"
v4.5.0,initialize histograms for global
v4.5.0,sync global data sumup info
v4.5.0,set global sumup info
v4.5.0,init global data count in leaf
v4.5.0,get local sumup
v4.5.0,get local sumup
v4.5.0,get mean number on machines
v4.5.0,weighted gain
v4.5.0,get top k
v4.5.0,"Copy histogram to buffer, and Get local aggregate features"
v4.5.0,copy histograms.
v4.5.0,copy smaller leaf histograms first
v4.5.0,mark local aggregated feature
v4.5.0,copy
v4.5.0,then copy larger leaf histograms
v4.5.0,mark local aggregated feature
v4.5.0,copy
v4.5.0,use local data to find local best splits
v4.5.0,clear histogram buffer before synchronizing
v4.5.0,otherwise histogram contents from the previous iteration will be sent
v4.5.0,find splits
v4.5.0,only has root leaf
v4.5.0,local voting
v4.5.0,gather
v4.5.0,get all top-k from all machines
v4.5.0,global voting
v4.5.0,copy local histgrams to buffer
v4.5.0,Reduce scatter for histogram
v4.5.0,find best split from local aggregated histograms
v4.5.0,restore from buffer
v4.5.0,restore from buffer
v4.5.0,find local best
v4.5.0,find local best split for larger leaf
v4.5.0,sync global best info
v4.5.0,copy back
v4.5.0,set the global number of data for leaves
v4.5.0,init the global sumup info
v4.5.0,"instantiate template classes, otherwise linker cannot find the code"
v4.5.0,allocate CUDA memory
v4.5.0,leave some space for alignment
v4.5.0,input best split info
v4.5.0,for leaf information update
v4.5.0,"gather information for CPU, used for launching kernels"
v4.5.0,for leaf splits information update
v4.5.0,we need restore the order of indices in cuda_data_indices_
v4.5.0,allocate more memory for sum reduction in CUDA
v4.5.0,only the first element records the final sum
v4.5.0,intialize split find task information (a split find task is one pass through the histogram of a feature)
v4.5.0,need to double the size of histogram buffer in global memory when using double precision in histogram construction
v4.5.0,use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes
v4.5.0,use the first gpu by default
v4.5.0,"std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values"
v4.5.0,use feature interaction constraint or sample features by node
v4.4.0,coding: utf-8
v4.4.0,raise deprecation warnings if necessary
v4.4.0,ref: https://github.com/microsoft/LightGBM/issues/6435
v4.4.0,create predictor first
v4.4.0,setting early stopping via global params should be possible
v4.4.0,reduce cost for prediction training data
v4.4.0,process callbacks
v4.4.0,construct booster
v4.4.0,start training
v4.4.0,check evaluation result.
v4.4.0,"ranking task, split according to groups"
v4.4.0,run preprocessing on the data set if needed
v4.4.0,raise deprecation warnings if necessary
v4.4.0,ref: https://github.com/microsoft/LightGBM/issues/6435
v4.4.0,setting early stopping via global params should be possible
v4.4.0,setup callbacks
v4.4.0,coding: utf-8
v4.4.0,dummy function to support older version of scikit-learn
v4.4.0,catching 'ValueError' here because of this:
v4.4.0,https://github.com/microsoft/LightGBM/issues/6365#issuecomment-2002330003
v4.4.0,
v4.4.0,"That's potentially risky as dask does some significant import-time processing,"
v4.4.0,"like loading configuration from environment variables and files, and catching"
v4.4.0,ValueError here might hide issues with that config-loading.
v4.4.0,
v4.4.0,"But in exchange, it's less likely that 'import lightgbm' will fail for"
v4.4.0,"dask-related reasons, which is beneficial for any workloads that are using"
v4.4.0,lightgbm but not its Dask functionality.
v4.4.0,coding: utf-8
v4.4.0,"f(labels, preds)"
v4.4.0,"f(labels, preds, weights)"
v4.4.0,"f(labels, preds, weights, group)"
v4.4.0,"f(labels, preds)"
v4.4.0,"f(labels, preds, weights)"
v4.4.0,"f(labels, preds, weights, group)"
v4.4.0,documentation templates for LGBMModel methods are shared between the classes in
v4.4.0,this module and those in the ``dask`` module
v4.4.0,"It's possible, for example, to pass 3 eval sets through `eval_set`,"
v4.4.0,but only 1 init_score through `eval_init_score`.
v4.4.0,
v4.4.0,This if-else accounts for that possiblity.
v4.4.0,register default metric for consistency with callable eval_metric case
v4.4.0,try to deduce from class instance
v4.4.0,overwrite default metric by explicitly set metric
v4.4.0,"use joblib conventions for negative n_jobs, just like scikit-learn"
v4.4.0,"at predict time, this is handled later due to the order of parameter updates"
v4.4.0,Do not modify original args in fit function
v4.4.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v4.4.0,Separate built-in from callable evaluation metrics
v4.4.0,concatenate metric from params (or default if not provided in params) and eval_metric
v4.4.0,copy for consistency
v4.4.0,reduce cost for prediction training data
v4.4.0,free dataset
v4.4.0,retrive original params that possibly can be used in both training and prediction
v4.4.0,and then overwrite them (considering aliases) with params that were passed directly in prediction
v4.4.0,number of threads can have values with special meaning which is only applied
v4.4.0,"in the scikit-learn interface, these should not reach the c++ side as-is"
v4.4.0,adjust eval metrics to match whether binary or multiclass
v4.4.0,classification is being performed
v4.4.0,"do not modify args, as it causes errors in model selection tools"
v4.4.0,check group data
v4.4.0,coding: utf-8
v4.4.0,coding: utf-8
v4.4.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v4.4.0,coding: utf-8
v4.4.0,coding: utf-8
v4.4.0,typing.TypeGuard was only introduced in Python 3.10
v4.4.0,we don't need lib_lightgbm while building docs
v4.4.0,TypeError: obj is not a string or a number
v4.4.0,ValueError: invalid literal
v4.4.0,Obtain objects to export
v4.4.0,Prepare export
v4.4.0,Export all objects
v4.4.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v4.4.0,ref: https://peps.python.org/pep-0565/#additional-use-case-for-futurewarning
v4.4.0,"lazy evaluation to allow import without dynamic library, e.g., for docs generation"
v4.4.0,"if buffer length is not long enough, re-allocate a buffer"
v4.4.0,avoid side effects on passed-in parameters
v4.4.0,"if main_param_name was provided, keep that value and remove all aliases"
v4.4.0,"if main param name was not found, search for an alias"
v4.4.0,"neither of main_param_name, aliases were found"
v4.4.0,most common case (no nullable dtypes)
v4.4.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.4.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.4.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.4.0,take shallow copy in case we modify categorical columns
v4.4.0,whole column modifications don't change the original df
v4.4.0,determine feature names
v4.4.0,determine categorical features
v4.4.0,use cat cols from DataFrame
v4.4.0,so that the target dtype considers floats
v4.4.0,Get total row number.
v4.4.0,Random access by row index. Used for data sampling.
v4.4.0,Range data access. Used to read data in batch when constructing Dataset.
v4.4.0,Optionally specify batch_size to control range data read size.
v4.4.0,Only required if using ``Dataset.subset()``.
v4.4.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.4.0,avoid memory consumption by arrays concatenation operations
v4.4.0,create numpy array from output arrays
v4.4.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v4.4.0,for CSC there is extra column added
v4.4.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v4.4.0,same shape as input csr or csc matrix except extra column for expected value
v4.4.0,note: make sure we copy data as it will be deallocated next
v4.4.0,"free the temporary native indptr, indices, and data"
v4.4.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.4.0,avoid memory consumption by arrays concatenation operations
v4.4.0,Check that the input is valid: we only handle numbers (for now)
v4.4.0,Prepare prediction output array
v4.4.0,Export Arrow table to C and run prediction
v4.4.0,c type: double**
v4.4.0,each double* element points to start of each column of sample data.
v4.4.0,c type int**
v4.4.0,each int* points to start of indices for each column
v4.4.0,"no min_data, nthreads and verbose in this function"
v4.4.0,check data has header or not
v4.4.0,need to regroup init_score
v4.4.0,process for args
v4.4.0,get categorical features
v4.4.0,"If the params[cat_alias] is equal to categorical_indices, do not report the warning."
v4.4.0,process for reference dataset
v4.4.0,start construct data
v4.4.0,set feature names
v4.4.0,"Select sampled rows, transpose to column order."
v4.4.0,create validation dataset from ref_dataset
v4.4.0,Check that the input is valid: we only handle numbers (for now)
v4.4.0,Export Arrow table to C
v4.4.0,create valid
v4.4.0,construct subset
v4.4.0,create train
v4.4.0,could be updated if data is not freed
v4.4.0,set to None
v4.4.0,"If the data is a arrow data, we can just pass it to C"
v4.4.0,"If a table is being passed, we concatenate the columns. This is only valid for"
v4.4.0,'init_score'.
v4.4.0,we're done if self and reference share a common upstream reference
v4.4.0,Check if the weight contains values other than one
v4.4.0,Set field
v4.4.0,original values can be modified at cpp side
v4.4.0,"if buffer length is not long enough, reallocate buffers"
v4.4.0,"group data from LightGBM is boundaries data, need to convert to group size"
v4.4.0,Training task
v4.4.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v4.4.0,construct booster object
v4.4.0,copy the parameters from train_set
v4.4.0,save reference to data
v4.4.0,buffer for inner predict
v4.4.0,Prediction task
v4.4.0,"if buffer length is not long enough, re-allocate a buffer"
v4.4.0,if a single node tree it won't have `leaf_index` so return 0
v4.4.0,"Create the node record, and populate universal data members"
v4.4.0,Update values to reflect node type (leaf or split)
v4.4.0,traverse the next level of the tree
v4.4.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v4.4.0,and we add node to the list.
v4.4.0,need reset training data
v4.4.0,need to push new valid data
v4.4.0,ensure that existing Booster is freed before replacing it
v4.4.0,with a new one createdfrom file
v4.4.0,"if buffer length is not long enough, re-allocate a buffer"
v4.4.0,"if buffer length is not long enough, reallocate a buffer"
v4.4.0,Copy models
v4.4.0,Get name of features
v4.4.0,"if buffer length is not long enough, reallocate buffers"
v4.4.0,avoid to predict many time in one iteration
v4.4.0,Get num of inner evals
v4.4.0,Get name of eval metrics
v4.4.0,"if buffer length is not long enough, reallocate buffers"
v4.4.0,coding: utf-8
v4.4.0,Callback environment used by callbacks
v4.4.0,"CVBooster holds a list of Booster objects, each needs to be updated"
v4.4.0,"for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set"
v4.4.0,and those metrics are considered for early stopping
v4.4.0,"for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name"
v4.4.0,validation sets are guaranteed to not be identical to the training data in cv()
v4.4.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.4.0,self.best_score_list is initialized to an empty list
v4.4.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.4.0,coding: utf-8
v4.4.0,Acquire port in worker
v4.4.0,schedule futures to retrieve each element of the tuple
v4.4.0,retrieve ports
v4.4.0,Concatenate many parts into one
v4.4.0,construct local eval_set data.
v4.4.0,store indices of eval_set components that were not contained within local parts.
v4.4.0,consolidate parts of each individual eval component.
v4.4.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v4.4.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v4.4.0,filter padding from eval parts then _concat each eval_set component.
v4.4.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v4.4.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v4.4.0,capture whether local_listen_port or its aliases were provided
v4.4.0,capture whether machines or its aliases were provided
v4.4.0,Some passed-in parameters can be removed:
v4.4.0,* 'num_machines': set automatically from Dask worker list
v4.4.0,* 'num_threads': overridden to match nthreads on each Dask process
v4.4.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v4.4.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v4.4.0,X and y are each delayed sub-lists of original eval dask Collections.
v4.4.0,find maximum number of parts in an individual eval set so that we can
v4.4.0,pad eval sets when they come in different sizes.
v4.4.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v4.4.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v4.4.0,first time a chunk of this eval set is added to this part.
v4.4.0,append additional chunks of this eval set to this part.
v4.4.0,ensure that all evaluation parts map uniquely to one part.
v4.4.0,assign sub-eval_set components to worker parts.
v4.4.0,Start computation in the background
v4.4.0,trigger error locally
v4.4.0,Find locations of all parts and map them to particular Dask workers
v4.4.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v4.4.0,data artifacts may not be populated depending on worker returning final estimator.
v4.4.0,assign general validation set settings to fit kwargs.
v4.4.0,resolve aliases for network parameters and pop the result off params.
v4.4.0,these values are added back in calls to `_train_part()`
v4.4.0,figure out network params
v4.4.0,Tell each worker to train on the parts that it has locally
v4.4.0,
v4.4.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v4.4.0,1. there is randomness in the training process unless parameters ``seed``
v4.4.0,and ``deterministic`` are set
v4.4.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v4.4.0,relies on global state (it and all the other LightGBM training processes
v4.4.0,coordinate with each other)
v4.4.0,"if network parameters were changed during training, remove them from the"
v4.4.0,returned model so that they're generated dynamically on every run based
v4.4.0,on the Dask cluster you're connected to and which workers have pieces of
v4.4.0,the training data
v4.4.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v4.4.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v4.4.0,are returned as a list of sparse matrices (one per class)
v4.4.0,"pred_contrib output will have one column per feature,"
v4.4.0,plus one more for the base value
v4.4.0,need to tell Dask the expected type and shape of individual preds
v4.4.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v4.4.0,the code below is used instead to ensure that the sparse type is preserved during concatentation
v4.4.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v4.4.0,Concatenate them to return a list of Dask Arrays.
v4.4.0,"DaskLGBMClassifier does not support group, eval_group."
v4.4.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v4.4.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group."
v4.4.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v4.4.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v4.4.0,DaskLGBMRanker support for callbacks and init_model is not tested
v4.4.0,coding: utf-8
v4.4.0,load or create your dataset
v4.4.0,generate feature names
v4.4.0,create dataset for lightgbm
v4.4.0,"if you want to re-use data, remember to set free_raw_data=False"
v4.4.0,specify your configurations as a dict
v4.4.0,feature_name and categorical_feature
v4.4.0,check feature name
v4.4.0,save model to file
v4.4.0,dump model to JSON (and save to file)
v4.4.0,feature names
v4.4.0,feature importances
v4.4.0,load model to predict
v4.4.0,can only predict with the best iteration (or the saving iteration)
v4.4.0,eval with loaded model
v4.4.0,dump model with pickle
v4.4.0,load model with pickle to predict
v4.4.0,can predict with any iteration when loaded in pickle way
v4.4.0,eval with loaded model
v4.4.0,continue training
v4.4.0,init_model accepts:
v4.4.0,1. model file name
v4.4.0,2. Booster()
v4.4.0,decay learning rates
v4.4.0,reset_parameter callback accepts:
v4.4.0,1. list with length = num_boost_round
v4.4.0,2. function(curr_iter)
v4.4.0,change other parameters during training
v4.4.0,self-defined objective function
v4.4.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v4.4.0,log likelihood loss
v4.4.0,self-defined eval metric
v4.4.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.4.0,binary error
v4.4.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.4.0,This may make built-in evaluation metric calculate wrong results
v4.4.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.4.0,Keep this in mind when you use the customization
v4.4.0,Pass custom objective function through params
v4.4.0,another self-defined eval metric
v4.4.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.4.0,accuracy
v4.4.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.4.0,This may make built-in evaluation metric calculate wrong results
v4.4.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.4.0,Keep this in mind when you use the customization
v4.4.0,Pass custom objective function through params
v4.4.0,callback
v4.4.0,coding: utf-8
v4.4.0,load or create your dataset
v4.4.0,train
v4.4.0,predict
v4.4.0,eval
v4.4.0,feature importances
v4.4.0,self-defined eval metric
v4.4.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.4.0,Root Mean Squared Logarithmic Error (RMSLE)
v4.4.0,train
v4.4.0,another self-defined eval metric
v4.4.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.4.0,Relative Absolute Error (RAE)
v4.4.0,train
v4.4.0,predict
v4.4.0,eval
v4.4.0,other scikit-learn modules
v4.4.0,coding: utf-8
v4.4.0,load or create your dataset
v4.4.0,create dataset for lightgbm
v4.4.0,specify your configurations as a dict
v4.4.0,train
v4.4.0,coding: utf-8
v4.4.0,################
v4.4.0,Simulate some binary data with a single categorical and
v4.4.0,single continuous predictor
v4.4.0,################
v4.4.0,Set up a couple of utilities for our experiments
v4.4.0,################
v4.4.0,Observe the behavior of `binary` and `xentropy` objectives
v4.4.0,Trying this throws an error on non-binary values of y:
v4.4.0,"experiment('binary', label_type='probability', DATA)"
v4.4.0,The speed of `binary` is not drastically different than
v4.4.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v4.4.0,there are reasons to suspect that `binary` should run faster when the
v4.4.0,label is an integer instead of a float
v4.4.0,coding: utf-8
v4.4.0,load or create your dataset
v4.4.0,create dataset for lightgbm
v4.4.0,specify your configurations as a dict
v4.4.0,train
v4.4.0,save model to file
v4.4.0,predict
v4.4.0,eval
v4.4.0,We can also open HDF5 file once and get access to
v4.4.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v4.4.0,
v4.4.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v4.4.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v4.4.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v4.4.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v4.4.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v4.4.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v4.4.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v4.4.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v4.4.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v4.4.0,Here we are just creating a chunk size that matches with batch_size.
v4.4.0,
v4.4.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v4.4.0,lightgbm Dataset.
v4.4.0,Save to 2 HDF5 files for demonstration.
v4.4.0,We can store multiple datasets inside a single HDF5 file.
v4.4.0,Separating X and Y for choosing best chunk size for data loading.
v4.4.0,split training data into two partitions
v4.4.0,make this array dense because we're splitting across
v4.4.0,a sparse boundary to partition the data
v4.4.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v4.4.0,predictions and target values back from workers to the client
v4.4.0,
v4.4.0,"for larger datasets, consider the metrics from dask-ml instead"
v4.4.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v4.4.0,coding: utf-8
v4.4.0,!/usr/bin/env python3
v4.4.0,-*- coding: utf-8 -*-
v4.4.0,
v4.4.0,"LightGBM documentation build configuration file, created by"
v4.4.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v4.4.0,
v4.4.0,This file is execfile()d with the current directory set to its
v4.4.0,containing dir.
v4.4.0,
v4.4.0,Note that not all possible configuration values are present in this
v4.4.0,autogenerated file.
v4.4.0,
v4.4.0,All configuration values have a default; values that are commented out
v4.4.0,serve to show the default.
v4.4.0,"If extensions (or modules to document with autodoc) are in another directory,"
v4.4.0,add these directories to sys.path here. If the directory is relative to the
v4.4.0,"documentation root, use os.path.abspath to make it absolute."
v4.4.0,-- General configuration ------------------------------------------------
v4.4.0,"If your documentation needs a minimal Sphinx version, state it here."
v4.4.0,"Add any Sphinx extension module names here, as strings. They can be"
v4.4.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v4.4.0,ones.
v4.4.0,mock out modules
v4.4.0,hide type hints in API docs
v4.4.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v4.4.0,Only the class' docstring is inserted.
v4.4.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v4.4.0,The master toctree document.
v4.4.0,General information about the project.
v4.4.0,The name of an image file (relative to this directory) to place at the top
v4.4.0,of the sidebar.
v4.4.0,The name of an image file (relative to this directory) to use as a favicon of
v4.4.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v4.4.0,pixels large.
v4.4.0,"The version info for the project you're documenting, acts as replacement for"
v4.4.0,"|version| and |release|, also used in various other places throughout the"
v4.4.0,built documents.
v4.4.0,The short X.Y version.
v4.4.0,"The full version, including alpha/beta/rc tags."
v4.4.0,The language for content autogenerated by Sphinx. Refer to documentation
v4.4.0,for a list of supported languages.
v4.4.0,
v4.4.0,This is also used if you do content translation via gettext catalogs.
v4.4.0,"Usually you set ""language"" from the command line for these cases."
v4.4.0,"List of patterns, relative to source directory, that match files and"
v4.4.0,directories to ignore when looking for source files.
v4.4.0,This patterns also effect to html_static_path and html_extra_path
v4.4.0,The name of the Pygments (syntax highlighting) style to use.
v4.4.0,-- Configuration for C API docs generation ------------------------------
v4.4.0,-- Options for HTML output ----------------------------------------------
v4.4.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v4.4.0,a list of builtin themes.
v4.4.0,Theme options are theme-specific and customize the look and feel of a theme
v4.4.0,"further.  For a list of options available for each theme, see the"
v4.4.0,documentation.
v4.4.0,"Add any paths that contain custom static files (such as style sheets) here,"
v4.4.0,"relative to this directory. They are copied after the builtin static files,"
v4.4.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v4.4.0,-- Options for HTMLHelp output ------------------------------------------
v4.4.0,Output file base name for HTML help builder.
v4.4.0,-- Options for LaTeX output ---------------------------------------------
v4.4.0,The name of an image file (relative to this directory) to place at the top of
v4.4.0,the title page.
v4.4.0,intersphinx configuration
v4.4.0,Warning! The following code can cause buffer overflows on RTD.
v4.4.0,Consider suppressing output completely if RTD project silently fails.
v4.4.0,Refer to https://github.com/svenevs/exhale
v4.4.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.4.0,Warning! The following code can cause buffer overflows on RTD.
v4.4.0,Consider suppressing output completely if RTD project silently fails.
v4.4.0,Refer to https://github.com/svenevs/exhale
v4.4.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.4.0,coding: utf-8
v4.4.0,This is a basic test for floating number parsing.
v4.4.0,Most of the test cases come from:
v4.4.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v4.4.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v4.4.0,FLT_MAX
v4.4.0,FLT_MIN
v4.4.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v4.4.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v4.4.0,Near DBL_MIN
v4.4.0,DBL_MIN 2^-1022
v4.4.0,The behavior for parsing -nan depends on implementation.
v4.4.0,Thus we skip binary check for negative nan.
v4.4.0,See comment in test_cases.
v4.4.0,construct sample data first (use all data for convenience and since size is small)
v4.4.0,Load some test data
v4.4.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.4.0,Add some fake initial_scores and groups so we can test streaming them
v4.4.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.4.0,Load some test data
v4.4.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.4.0,Add some fake initial_scores and groups so we can test streaming them
v4.4.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.4.0,This code is copied and adapted from the official Arrow producer examples:
v4.4.0,https://arrow.apache.org/docs/format/CDataInterface.html#exporting-a-struct-float32-utf8-array
v4.4.0,Free children
v4.4.0,Finalize
v4.4.0,Free children
v4.4.0,Free buffers
v4.4.0,Finalize
v4.4.0,NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests
v4.4.0,"By using `calloc` above, we only need to set 'true' values"
v4.4.0,Arithmetic
v4.4.0,Subscripts
v4.4.0,End
v4.4.0,Check for values in first chunk
v4.4.0,Check for some values in second chunk
v4.4.0,Check end
v4.4.0,Load some test data
v4.4.0,Serialize the reference
v4.4.0,Deserialize the reference
v4.4.0,Confirm 1 successful API call
v4.4.0,Free memory
v4.4.0,Load some test data
v4.4.0,Run a single row prediction and compare with regular Mat prediction:
v4.4.0,"Drop the result from the dataset, we only care about checking that prediction results are equal"
v4.4.0,in both cases
v4.4.0,Now let's run with the single row fast prediction API:
v4.4.0,Free all:
v4.4.0,Test that Data() points to first value written
v4.4.0,Constants
v4.4.0,Start with some content:
v4.4.0,Clear & re-use:
v4.4.0,Output should match new content:
v4.4.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v4.4.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v4.4.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v4.4.0,Compute a new trial address & value & if it is a valid address:
v4.4.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v4.4.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v4.4.0,Check the just-stored value with getitem():
v4.4.0,Also store the just-stored value for future tracking:
v4.4.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v4.4.0,Test in 2 ways that the values are correctly laid out in memory:
v4.4.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.4.0,Use multiple threads to test concurrency
v4.4.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.4.0,Calculate expected boundaries
v4.4.0,Extract a set of rows from the column-based format (still maintaining column based format)
v4.4.0,coding: utf-8
v4.4.0,"at initialization, should be -1"
v4.4.0,updating that value through the C API should work
v4.4.0,resetting to any negative number should set it to -1
v4.4.0,coding: utf-8
v4.4.0,check saved model persistence
v4.4.0,"we need to check the consistency of model file here, so test for exact equal"
v4.4.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v4.4.0,"scores likely to be different, but prediction should still be the same"
v4.4.0,test that shape is checked during prediction
v4.4.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v4.4.0,The following is for demo and testing purpose.
v4.4.0,whole col
v4.4.0,half col
v4.4.0,Create dataset from numpy array directly.
v4.4.0,Create dataset using Sequence.
v4.4.0,Test for validation set.
v4.4.0,Select some random rows as valid data.
v4.4.0,"From Dataset constructor, with dataset from numpy array."
v4.4.0,"From Dataset.create_valid, with dataset from sequence."
v4.4.0,test that method works even with free_raw_data=True
v4.4.0,test that method works but sets raw data to None in case of immergeable data types
v4.4.0,test that method works for different data types
v4.4.0,"Set extremely harsh penalties, so CEGB will block most splits."
v4.4.0,"Compare pairs of penalties, to ensure scaling works as intended"
v4.4.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v4.4.0,"unconstructed, get_* methods should return whatever was provided"
v4.4.0,"before construction, get_field() should raise an exception"
v4.4.0,"constructed, get_* methods should return numpy arrays, even when the provided"
v4.4.0,input was a list of floats or ints
v4.4.0,"get_field(""group"") returns a numpy array with boundaries, instead of size"
v4.4.0,"NOTE: ""position"" is converted to int32 on the C++ side"
v4.4.0,"should resolve duplicate aliases, and prefer the main parameter"
v4.4.0,should choose the highest priority alias and set that value on main param
v4.4.0,if only aliases are used
v4.4.0,should use the default if main param and aliases are missing
v4.4.0,all changes should be made on copies and not modify the original
v4.4.0,preserves None found for main param and still removes aliases
v4.4.0,correctly chooses value when only an alias is provided
v4.4.0,adds None if that's given as the default and param not found
v4.4.0,If callable is found in objective
v4.4.0,Value in params should be preferred to the default_value passed from keyword arguments
v4.4.0,"None of objective or its aliases in params, but default_value is callable."
v4.4.0,"""bad"" = 1 element too many"
v4.4.0,"copy=False is necessary because starting with pandas 3.0, pd.DataFrame() creates"
v4.4.0,a copy of the input numpy array by default
v4.4.0,ref: https://github.com/pandas-dev/pandas/issues/58913
v4.4.0,check that the original data wasn't modified
v4.4.0,check that the built data has the codes
v4.4.0,if all categories were seen during training we just take the codes
v4.4.0,if we only saw 'a' during training we just replace its code
v4.4.0,and leave the rest as nan
v4.4.0,test using defined feature names
v4.4.0,test using default feature names
v4.4.0,check for feature indices outside of range
v4.4.0,"NOTE: this intentionally contains values where num_leaves <, ==, and > (max_depth^2)"
v4.4.0,"NOTE: max_depth < 5 is significant here because the default for num_leaves=31. With max_depth=5,"
v4.4.0,a full depth-wise tree would have 2^5 = 32 leaves.
v4.4.0,coding: utf-8
v4.4.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v4.4.0,set_index ensures partitions are based on group id.
v4.4.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v4.4.0,"separate target, weight from features."
v4.4.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v4.4.0,"so that within each partition, sum(g) = n_samples."
v4.4.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v4.4.0,make one categorical feature relevant to the target
v4.4.0,https://github.com/microsoft/LightGBM/issues/4118
v4.4.0,extra predict() parameters should be passed through correctly
v4.4.0,pref_leaf values should have the right shape
v4.4.0,and values that look like valid tree nodes
v4.4.0,"be sure LightGBM actually used at least one categorical column,"
v4.4.0,and that it was correctly treated as a categorical feature
v4.4.0,shape depends on whether it is binary or multiclass classification
v4.4.0,"in the special case of multi-class classification using scipy sparse matrices,"
v4.4.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v4.4.0,
v4.4.0,"since that case is so different than all other cases, check the relevant things here"
v4.4.0,and then return early
v4.4.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v4.4.0,"be sure LightGBM actually used at least one categorical column,"
v4.4.0,and that it was correctly treated as a categorical feature
v4.4.0,* shape depends on whether it is binary or multiclass classification
v4.4.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v4.4.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v4.4.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v4.4.0,that the output has the right shape and base values are in the right position
v4.4.0,"with a custom objective, prediction result is a raw score instead of predicted class"
v4.4.0,function should have been preserved
v4.4.0,should correctly classify every sample
v4.4.0,probability estimates should be similar
v4.4.0,Scores should be the same
v4.4.0,Predictions should be roughly the same.
v4.4.0,pref_leaf values should have the right shape
v4.4.0,and values that look like valid tree nodes
v4.4.0,extra predict() parameters should be passed through correctly
v4.4.0,"be sure LightGBM actually used at least one categorical column,"
v4.4.0,and that it was correctly treated as a categorical feature
v4.4.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v4.4.0,that the output has the right shape and base values are in the right position
v4.4.0,"be sure LightGBM actually used at least one categorical column,"
v4.4.0,and that it was correctly treated as a categorical feature
v4.4.0,Quantiles should be right
v4.4.0,"be sure LightGBM actually used at least one categorical column,"
v4.4.0,and that it was correctly treated as a categorical feature
v4.4.0,function should have been preserved
v4.4.0,Scores should be the same
v4.4.0,local and Dask predictions should be the same
v4.4.0,predictions should be better than random
v4.4.0,rebalance small dask.Array dataset for better performance.
v4.4.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v4.4.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v4.4.0,distributed ranker should be able to rank decently well and should
v4.4.0,have high rank correlation with scores from serial ranker.
v4.4.0,extra predict() parameters should be passed through correctly
v4.4.0,pref_leaf values should have the right shape
v4.4.0,and values that look like valid tree nodes
v4.4.0,"be sure LightGBM actually used at least one categorical column,"
v4.4.0,and that it was correctly treated as a categorical feature
v4.4.0,rebalance small dask.Array dataset for better performance.
v4.4.0,distributed ranker should be able to rank decently well with the least-squares objective
v4.4.0,and should have high rank correlation with scores from serial ranker.
v4.4.0,function should have been preserved
v4.4.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v4.4.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v4.4.0,"test eval_class_weight, eval_init_score on binary-classification task."
v4.4.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v4.4.0,create eval_sets by creating new datasets or copying training data.
v4.4.0,total number of trees scales up for ova classifier.
v4.4.0,check that early stopping was not applied.
v4.4.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v4.4.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v4.4.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v4.4.0,should be able to use the class without specifying a client
v4.4.0,should be able to set client after construction
v4.4.0,data on cluster1
v4.4.0,create identical data on cluster2
v4.4.0,"at this point, the result of default_client() is client2 since it was the most recently"
v4.4.0,created. So setting client to client1 here to test that you can select a non-default client
v4.4.0,"unfitted model should survive pickling round trip, and pickling"
v4.4.0,shouldn't have side effects on the model object
v4.4.0,client will always be None after unpickling
v4.4.0,"fitted model should survive pickling round trip, and pickling"
v4.4.0,shouldn't have side effects on the model object
v4.4.0,client will always be None after unpickling
v4.4.0,rebalance data to be sure that each worker has a piece of the data
v4.4.0,model 1 - no network parameters given
v4.4.0,model 2 - machines given
v4.4.0,model 3 - local_listen_port given
v4.4.0,training should fail because LightGBM will try to use the same
v4.4.0,port for multiple worker processes on the same machine
v4.4.0,rebalance data to be sure that each worker has a piece of the data
v4.4.0,"test that ""machines"" is actually respected by creating a socket that uses"
v4.4.0,"one of the ports mentioned in ""machines"""
v4.4.0,The above error leaves a worker waiting
v4.4.0,"an informative error should be raised if ""machines"" has duplicates"
v4.4.0,"""client"" should be the only different, and the final argument"
v4.4.0,value of the root node is 0 when init_score is set
v4.4.0,this test is separate because it takes a not-yet-constructed estimator
v4.4.0,coding: utf-8
v4.4.0,coding: utf-8
v4.4.0,"build target, group ID vectors."
v4.4.0,build y/target and group-id vectors with user-specified group sizes.
v4.4.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v4.4.0,groups should contain > 1 element for pairwise learning objective.
v4.4.0,"build feature data, X. Transform first few into informative features."
v4.4.0,"doing this here, at import time, to ensure it only runs once_per import"
v4.4.0,instead of once per assertion
v4.4.0,coding: utf-8
v4.4.0,"NOTE: In the AppVeyor CI, importing pyarrow fails due to an old Visual Studio version. Hence,"
v4.4.0,"we conditionally import pyarrow here (and skip tests if it cannot be imported). However, we"
v4.4.0,"don't want these tests to silently be skipped, hence, we only conditionally import when a"
v4.4.0,specific env var is set.
v4.4.0,----------------------------------------------------------------------------------------------- #
v4.4.0,UTILITIES                                            #
v4.4.0,----------------------------------------------------------------------------------------------- #
v4.4.0,Set random nulls
v4.4.0,Split data into <=2 random chunks
v4.4.0,Turn chunks into array
v4.4.0,----------------------------------------------------------------------------------------------- #
v4.4.0,UNIT TESTS                                           #
v4.4.0,----------------------------------------------------------------------------------------------- #
v4.4.0,------------------------------------------- DATASET ------------------------------------------- #
v4.4.0,-------------------------------------------- FIELDS ------------------------------------------- #
v4.4.0,Check for equality
v4.4.0,-------------------------------------------- LABELS ------------------------------------------- #
v4.4.0,------------------------------------------- WEIGHTS ------------------------------------------- #
v4.4.0,-------------------------------------------- GROUPS ------------------------------------------- #
v4.4.0,----------------------------------------- INIT SCORES ----------------------------------------- #
v4.4.0,------------------------------------------ PREDICTION ----------------------------------------- #
v4.4.0,coding: utf-8
v4.4.0,check that really dummy objective was used and estimator didn't learn anything
v4.4.0,prediction result is actually not transformed (is raw) due to custom objective
v4.4.0,original estimator is unaffected
v4.4.0,"new estimator is unfitted, but has the same parameters"
v4.4.0,Test if random_state is properly stored
v4.4.0,Test if two random states produce identical models
v4.4.0,Test if subsequent fits sample from random_state object and produce different models
v4.4.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v4.4.0,why fixed seed?
v4.4.0,sometimes there is no difference how cols are treated (cat or not cat)
v4.4.0,With default params
v4.4.0,Tests same probabilities
v4.4.0,Tests same predictions
v4.4.0,Tests same raw scores
v4.4.0,Tests same leaf indices
v4.4.0,Tests same feature contributions
v4.4.0,Tests other parameters for the prediction works
v4.4.0,Tests start_iteration
v4.4.0,"Tests same probabilities, starting from iteration 10"
v4.4.0,"Tests same predictions, starting from iteration 10"
v4.4.0,"Tests same raw scores, starting from iteration 10"
v4.4.0,"Tests same leaf indices, starting from iteration 10"
v4.4.0,"Tests same feature contributions, starting from iteration 10"
v4.4.0,"Tests other parameters for the prediction works, starting from iteration 10"
v4.4.0,test that params passed in predict have higher priority
v4.4.0,"no custom objective, no custom metric"
v4.4.0,default metric
v4.4.0,non-default metric
v4.4.0,no metric
v4.4.0,non-default metric in eval_metric
v4.4.0,non-default metric with non-default metric in eval_metric
v4.4.0,non-default metric with multiple metrics in eval_metric
v4.4.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v4.4.0,default metric for non-default objective
v4.4.0,non-default metric for non-default objective
v4.4.0,no metric
v4.4.0,non-default metric in eval_metric for non-default objective
v4.4.0,non-default metric with non-default metric in eval_metric for non-default objective
v4.4.0,non-default metric with multiple metrics in eval_metric for non-default objective
v4.4.0,"custom objective, no custom metric"
v4.4.0,default regression metric for custom objective
v4.4.0,non-default regression metric for custom objective
v4.4.0,multiple regression metrics for custom objective
v4.4.0,no metric
v4.4.0,default regression metric with non-default metric in eval_metric for custom objective
v4.4.0,non-default regression metric with metric in eval_metric for custom objective
v4.4.0,multiple regression metrics with metric in eval_metric for custom objective
v4.4.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v4.4.0,"no custom objective, custom metric"
v4.4.0,default metric with custom metric
v4.4.0,non-default metric with custom metric
v4.4.0,multiple metrics with custom metric
v4.4.0,custom metric (disable default metric)
v4.4.0,default metric for non-default objective with custom metric
v4.4.0,non-default metric for non-default objective with custom metric
v4.4.0,multiple metrics for non-default objective with custom metric
v4.4.0,custom metric (disable default metric for non-default objective)
v4.4.0,"custom objective, custom metric"
v4.4.0,custom metric for custom objective
v4.4.0,non-default regression metric with custom metric for custom objective
v4.4.0,multiple regression metrics with custom metric for custom objective
v4.4.0,default metric and invalid binary metric is replaced with multiclass alternative
v4.4.0,invalid binary metric is replaced with multiclass alternative
v4.4.0,default metric for non-default multiclass objective
v4.4.0,and invalid binary metric is replaced with multiclass alternative
v4.4.0,default metric and invalid multiclass metric is replaced with binary alternative
v4.4.0,invalid multiclass metric is replaced with binary alternative for custom objective
v4.4.0,"Verify that can receive a list of metrics, only callable"
v4.4.0,Verify that can receive a list of custom and built-in metrics
v4.4.0,Verify that works as expected when eval_metric is empty
v4.4.0,"Verify that can receive a list of metrics, only built-in"
v4.4.0,Verify that eval_metric is robust to receiving a list with None
v4.4.0,feval
v4.4.0,single eval_set
v4.4.0,two eval_set
v4.4.0,'val_minus_two' here is the expected number of threads for n_jobs=-2
v4.4.0,"Note: according to joblib's formula, a value of n_jobs=-2 means"
v4.4.0,"""use all but one thread"" (formula: n_cpus + 1 + n_jobs)"
v4.4.0,try to predict with a different feature
v4.4.0,check that disabling the check doesn't raise the error
v4.4.0,"make weights and init_score same types as y, just to avoid"
v4.4.0,a huge number of combinations and therefore test cases
v4.4.0,"make weights and init_score same types as y, just to avoid"
v4.4.0,a huge number of combinations and therefore test cases
v4.4.0,coding: utf-8
v4.4.0,we're in a leaf now
v4.4.0,check that the rest of the elements have black color
v4.4.0,check that we got to the expected leaf
v4.4.0,coding: utf-8
v4.4.0,coding: utf-8
v4.4.0,check that default gives same result as k = 1
v4.4.0,check against independent calculation for k = 1
v4.4.0,check against independent calculation for k = 2
v4.4.0,check against independent calculation for k = 10
v4.4.0,check cases where predictions are equal
v4.4.0,should give same result as binary auc for 2 classes
v4.4.0,test the case where all predictions are equal
v4.4.0,test that weighted data gives different auc_mu
v4.4.0,test that equal data weights give same auc_mu as unweighted data
v4.4.0,should give 1 when accuracy = 1
v4.4.0,test loading class weights
v4.4.0,Simulates position bias for a given ranking dataset.
v4.4.0,The ouput dataset is identical to the input one with the exception for the relevance labels.
v4.4.0,The new labels are generated according to an instance of a cascade user model:
v4.4.0,"for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker"
v4.4.0,"(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)"
v4.4.0,and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;
v4.4.0,"at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,"
v4.4.0,new_label=0. Thus the generated new labels are biased towards the baseline ranker.
v4.4.0,"The positions of the documents in the ranked lists produced by the baseline, are returned."
v4.4.0,a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it
v4.4.0,an instantiation of a cascade model where the user stops with probability 0.2 after observing each document
v4.4.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.4.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.4.0,add extra row to position file
v4.4.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.4.0,test setting positions through Dataset constructor with numpy array
v4.4.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.4.0,test setting positions through Dataset constructor with pandas Series
v4.4.0,test setting positions through set_position
v4.4.0,test get_position works
v4.4.0,no early stopping
v4.4.0,early stopping occurs
v4.4.0,regular early stopping
v4.4.0,positive min_delta
v4.4.0,test custom eval metrics
v4.4.0,"shuffle = False, override metric in params"
v4.4.0,"shuffle = True, callbacks"
v4.4.0,enable display training loss
v4.4.0,self defined folds
v4.4.0,LambdaRank
v4.4.0,... with l2 metric
v4.4.0,... with NDCG (default) metric
v4.4.0,self defined folds with lambdarank
v4.4.0,init_model from an in-memory Booster
v4.4.0,init_model from a text file
v4.4.0,predictions should be identical
v4.4.0,with early stopping
v4.4.0,predict by each fold booster
v4.4.0,check that each booster predicted using the best iteration
v4.4.0,fold averaging
v4.4.0,without early stopping
v4.4.0,test feature_names with whitespaces
v4.4.0,This has non-ascii strings.
v4.4.0,check that passing parameters to the constructor raises warning and ignores them
v4.4.0,check inference isn't affected by unknown parameter
v4.4.0,entries whose values should reflect params passed to lgb.train()
v4.4.0,'l1' was passed in with alias 'mae'
v4.4.0,NOTE: this was passed in with alias 'sub_row'
v4.4.0,entries with default values of params
v4.4.0,add device-specific entries
v4.4.0,
v4.4.0,passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...
v4.4.0,https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377
v4.4.0,check that model text has all expected param entries
v4.4.0,"since Booster.model_to_string() is used when pickling, check that parameters all"
v4.4.0,roundtrip pickling successfully too
v4.4.0,why fixed seed?
v4.4.0,sometimes there is no difference how cols are treated (cat or not cat)
v4.4.0,take subsets and train
v4.4.0,generate CSR sparse dataset
v4.4.0,convert data to dense and get back same contribs
v4.4.0,validate the values are the same
v4.4.0,validate using CSC matrix
v4.4.0,validate the values are the same
v4.4.0,generate CSR sparse dataset
v4.4.0,convert data to dense and get back same contribs
v4.4.0,validate the values are the same
v4.4.0,validate using CSC matrix
v4.4.0,validate the values are the same
v4.4.0,"@pytest.mark.skipif(psutil.virtual_memory().available / 1024 / 1024 / 1024 < 3, reason=""not enough RAM"")"
v4.4.0,def test_int32_max_sparse_contribs(rng):
v4.4.0,"params = {""objective"": ""binary""}"
v4.4.0,"train_features = rng.uniform(size=(100, 1000))"
v4.4.0,train_targets = [0] * 50 + [1] * 50
v4.4.0,"lgb_train = lgb.Dataset(train_features, train_targets)"
v4.4.0,"gbm = lgb.train(params, lgb_train, num_boost_round=2)"
v4.4.0,"csr_input_shape = (3000000, 1000)"
v4.4.0,test_features = csr_matrix(csr_input_shape)
v4.4.0,"for i in range(0, csr_input_shape[0], csr_input_shape[0] // 6):"
v4.4.0,"for j in range(0, 1000, 100):"
v4.4.0,"test_features[i, j] = random.random()"
v4.4.0,"y_pred_csr = gbm.predict(test_features, pred_contrib=True)"
v4.4.0,# Note there is an extra column added to the output for the expected value
v4.4.0,"csr_output_shape = (csr_input_shape[0], csr_input_shape[1] + 1)"
v4.4.0,assert y_pred_csr.shape == csr_output_shape
v4.4.0,"y_pred_csc = gbm.predict(test_features.tocsc(), pred_contrib=True)"
v4.4.0,# Note output CSC shape should be same as CSR output shape
v4.4.0,assert y_pred_csc.shape == csr_output_shape
v4.4.0,test sliced labels
v4.4.0,append some columns
v4.4.0,append some rows
v4.4.0,test sliced 2d matrix
v4.4.0,test sliced CSR
v4.4.0,trees start at position 1.
v4.4.0,split_features are in 4th line.
v4.4.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v4.4.0,The penalization is so high that the first 2 features should not be used here
v4.4.0,Check that a very high penalization is the same as not using the features at all
v4.4.0,check refit accepts dataset_params
v4.4.0,the following checks that dart and rf with mape can predict outside the 0-1 range
v4.4.0,https://github.com/microsoft/LightGBM/issues/1579
v4.4.0,"no custom objective, no feval"
v4.4.0,default metric
v4.4.0,non-default metric in params
v4.4.0,default metric in args
v4.4.0,non-default metric in args
v4.4.0,metric in args overwrites one in params
v4.4.0,metric in args overwrites one in params
v4.4.0,multiple metrics in params
v4.4.0,multiple metrics in args
v4.4.0,remove default metric by 'None' in list
v4.4.0,remove default metric by 'None' aliases
v4.4.0,"custom objective, no feval"
v4.4.0,no default metric
v4.4.0,metric in params
v4.4.0,metric in args
v4.4.0,metric in args overwrites its' alias in params
v4.4.0,multiple metrics in params
v4.4.0,multiple metrics in args
v4.4.0,"no custom objective, feval"
v4.4.0,default metric with custom one
v4.4.0,non-default metric in params with custom one
v4.4.0,default metric in args with custom one
v4.4.0,non-default metric in args with custom one
v4.4.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.4.0,multiple metrics in params with custom one
v4.4.0,multiple metrics in args with custom one
v4.4.0,custom metric is evaluated despite 'None' is passed
v4.4.0,"custom objective, feval"
v4.4.0,"no default metric, only custom one"
v4.4.0,metric in params with custom one
v4.4.0,metric in args with custom one
v4.4.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.4.0,multiple metrics in params with custom one
v4.4.0,multiple metrics in args with custom one
v4.4.0,custom metric is evaluated despite 'None' is passed
v4.4.0,"no custom objective, no feval"
v4.4.0,default metric
v4.4.0,default metric in params
v4.4.0,non-default metric in params
v4.4.0,multiple metrics in params
v4.4.0,remove default metric by 'None' aliases
v4.4.0,"custom objective, no feval"
v4.4.0,no default metric
v4.4.0,metric in params
v4.4.0,multiple metrics in params
v4.4.0,"no custom objective, feval"
v4.4.0,default metric with custom one
v4.4.0,default metric in params with custom one
v4.4.0,non-default metric in params with custom one
v4.4.0,multiple metrics in params with custom one
v4.4.0,custom metric is evaluated despite 'None' is passed
v4.4.0,"custom objective, feval"
v4.4.0,"no default metric, only custom one"
v4.4.0,metric in params with custom one
v4.4.0,multiple metrics in params with custom one
v4.4.0,custom metric is evaluated despite 'None' is passed
v4.4.0,Custom objective replaces multiclass
v4.4.0,multiclass default metric
v4.4.0,multiclass default metric with custom one
v4.4.0,multiclass metric alias with custom one for custom objective
v4.4.0,no metric for invalid class_num
v4.4.0,custom metric for invalid class_num
v4.4.0,multiclass metric alias with custom one with invalid class_num
v4.4.0,multiclass default metric without num_class
v4.4.0,multiclass metric alias
v4.4.0,multiclass metric
v4.4.0,non-valid metric for multiclass objective
v4.4.0,non-default num_class for default objective
v4.4.0,no metric with non-default num_class for custom objective
v4.4.0,multiclass metric alias for custom objective
v4.4.0,multiclass metric for custom objective
v4.4.0,binary metric with non-default num_class for custom objective
v4.4.0,Expect three metrics but mean and stdv for each metric
v4.4.0,test XGBoost-style return value
v4.4.0,test numpy-style return value
v4.4.0,test bins string type
v4.4.0,test histogram is disabled for categorical features
v4.4.0,test for lgb.train
v4.4.0,test feval for lgb.train
v4.4.0,test with two valid data for lgb.train
v4.4.0,test for lgb.cv
v4.4.0,test feval for lgb.cv
v4.4.0,test that binning works properly for features with only positive or only negative values
v4.4.0,decreasing without freeing raw data is allowed
v4.4.0,decreasing before lazy init is allowed
v4.4.0,increasing is allowed
v4.4.0,decreasing with disabled filter is allowed
v4.4.0,decreasing with enabled filter is disallowed;
v4.4.0,also changes of other params are disallowed
v4.4.0,check extra trees increases regularization
v4.4.0,check path smoothing increases regularization
v4.4.0,test edge case with one leaf
v4.4.0,check that constraint containing all features is equivalent to no constraint
v4.4.0,check that constraint partitioning the features reduces train accuracy
v4.4.0,check that constraints consisting of single features reduce accuracy further
v4.4.0,test that interaction constraints work when not all features are used
v4.4.0,check that number of threads does not affect result
v4.4.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v4.4.0,test again with nans in data
v4.4.0,test again with bagging
v4.4.0,test with a feature that has only one non-nan value
v4.4.0,test with a categorical feature
v4.4.0,test refit: same results on same data
v4.4.0,test refit with save and load
v4.4.0,test refit: different results training on different data
v4.4.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v4.4.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v4.4.0,"test the case where start_iteration <= 0, and num_iteration is None"
v4.4.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v4.4.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v4.4.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v4.4.0,test for regression
v4.4.0,test both with and without early stopping
v4.4.0,test for multi-class
v4.4.0,test both with and without early stopping
v4.4.0,test for binary
v4.4.0,test both with and without early stopping
v4.4.0,test against sklearn average precision metric
v4.4.0,test that average precision is 1 where model predicts perfectly
v4.4.0,data as float64
v4.4.0,test all features were used
v4.4.0,test the score is better than predicting the mean
v4.4.0,test all predictions are equal using different input dtypes
v4.4.0,introduce some missing values
v4.4.0,the previous line turns x3 into object dtype in recent versions of pandas
v4.4.0,train with regular dtypes
v4.4.0,convert to nullable dtypes
v4.4.0,test training succeeds
v4.4.0,test all features were used
v4.4.0,test the score is better than predicting the mean
v4.4.0,test equal predictions
v4.4.0,test data are taken from bug report
v4.4.0,https://github.com/microsoft/LightGBM/issues/4708
v4.4.0,modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811
v4.4.0,and https://github.com/microsoft/LightGBM/pull/5087
v4.4.0,test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree
v4.4.0,which is done in the fix #5164
v4.4.0,without the fix:
v4.4.0,Check failed: (best_split_info.left_count) > (0)
v4.4.0,try to predict with a different feature
v4.4.0,check that disabling the check doesn't raise the error
v4.4.0,try to refit with a different feature
v4.4.0,check that disabling the check doesn't raise the error
v4.4.0,coding: utf-8
v4.4.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v4.4.0,Double-precision floats are only supported on x86_64 with PoCL
v4.4.0,coding: utf-8
v4.4.0,coding: utf-8
v4.4.0,"Note: MSVC has issues with Altrep classes, so they are disabled for it."
v4.4.0,See: https://github.com/microsoft/LightGBM/pull/6213#issuecomment-2111025768
v4.4.0,These are helper functions to allow doing a stack unwind
v4.4.0,"after an R allocation error, which would trigger a long jump."
v4.4.0,convert from one-based to zero-based index
v4.4.0,"if any feature names were larger than allocated size,"
v4.4.0,allow for a larger size and try again
v4.4.0,convert from boundaries to size
v4.4.0,--- start Booster interfaces
v4.4.0,"if any eval names were larger than allocated size,"
v4.4.0,allow for a larger size and try again
v4.4.0,"Note: for some reason, MSVC crashes when an error is thrown here"
v4.4.0,"if the buffer variable is defined as 'std::unique_ptr<std::vector<char>>',"
v4.4.0,but not if it is defined as '<std::vector<char>'.
v4.4.0,"if the model string was larger than the initial buffer, call the function again, writing directly to the R object"
v4.4.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.4.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.4.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.4.0,.Call() calls
v4.4.0,coding: utf-8
v4.4.0,alias table
v4.4.0,names
v4.4.0,from strings
v4.4.0,tails
v4.4.0,tails
v4.4.0,the following are stored as comma separated strings but are arrays in the wrappers
v4.4.0,coding: utf-8
v4.4.0,Single row predictor to abstract away caching logic
v4.4.0,Prevent the booster from being modified while we have a predictor relying on it during prediction
v4.4.0,If several threads try to predict at the same time using the same SingleRowPredictor
v4.4.0,"we want them to still provide correct values, so the mutex is necessary due to the shared"
v4.4.0,resources in the predictor.
v4.4.0,"However the recommended approach is to instantiate one SingleRowPredictor per thread,"
v4.4.0,to avoid contention here.
v4.4.0,create boosting
v4.4.0,initialize the boosting
v4.4.0,create objective function
v4.4.0,initialize the objective function
v4.4.0,create training metric
v4.4.0,reset the boosting
v4.4.0,create objective function
v4.4.0,initialize the objective function
v4.4.0,Workaround https://github.com/microsoft/LightGBM/issues/6142 by locking here
v4.4.0,"This is only a workaround because if predictors are initialized differently it may still behave incorrectly,"
v4.4.0,and because multiple racing Predictor initializations through LGBM_BoosterPredictForMat suffers from that same issue of Predictor init writing things in the booster.
v4.4.0,"Once #6142 is fixed (predictor doesn't write in the Booster as should have been the case since 1c35c3b9ede9adab8ccc5fd7b4b2b6af188a79f0), this line can be removed."
v4.4.0,calculate the nonzero data and indices size
v4.4.0,allocate data and indices arrays
v4.4.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.4.0,aggregated per row feature contribution results
v4.4.0,keep track of the row_vector sizes for parallelization
v4.4.0,copy vector results to output for each row
v4.4.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.4.0,aggregated per row feature contribution results
v4.4.0,calculate number of elements per column to construct
v4.4.0,the CSC matrix with random access
v4.4.0,keep track of column counts
v4.4.0,keep track of beginning index for each column
v4.4.0,keep track of beginning index for each matrix
v4.4.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v4.4.0,store the row index
v4.4.0,update column count
v4.4.0,explicitly declare symbols from LightGBM namespace
v4.4.0,some help functions used to convert data
v4.4.0,Row iterator of on column for CSC matrix
v4.4.0,"return value at idx, only can access by ascent order"
v4.4.0,"return next non-zero pair, if index < 0, means no more data"
v4.4.0,start of c_api functions
v4.4.0,This API is to keep python binding's behavior the same with C++ implementation.
v4.4.0,"Sample count, random seed etc. should be provided in parameters."
v4.4.0,convert internal thread id to be unique based on external thread id
v4.4.0,convert internal thread id to be unique based on external thread id
v4.4.0,sample data first
v4.4.0,sample data first
v4.4.0,sample data first
v4.4.0,local buffer to re-use memory
v4.4.0,sample data first
v4.4.0,no more data
v4.4.0,Prepare the Arrow data
v4.4.0,Initialize the dataset
v4.4.0,"If there is no reference dataset, we first sample indices"
v4.4.0,"Then, we obtain sample values by parallelizing across columns"
v4.4.0,Values need to be copied from the record batches.
v4.4.0,The chunks are iterated over in the inner loop as columns can be treated independently.
v4.4.0,"Finally, we initialize a loader from the sampled values"
v4.4.0,"After sampling and properly initializing all bins, we can add our data to the dataset. Here,"
v4.4.0,we parallelize across rows.
v4.4.0,---- start of booster
v4.4.0,"Naming: In future versions of LightGBM, public API named around `FastConfig` should be made named around"
v4.4.0,"`SingleRowPredictor`, because it is specific to single row prediction, and doesn't actually hold only config."
v4.4.0,For now this is kept as `FastConfig` for backwards compatibility.
v4.4.0,"At the same time, one should consider removing the old non-fast single row public API that stores its Predictor"
v4.4.0,"in the Booster, because that will enable removing these Predictors from the Booster, and associated initialization"
v4.4.0,code.
v4.4.0,Single row in row-major format:
v4.4.0,Apply the configuration
v4.4.0,Set up chunked array and iterators for all columns
v4.4.0,Build row function
v4.4.0,Run prediction
v4.4.0,---- start of some help functions
v4.4.0,data is array of pointers to individual rows
v4.4.0,set number of threads for openmp
v4.4.0,read parameters from config file
v4.4.0,"remove str after ""#"""
v4.4.0,de-duplicate params
v4.4.0,prediction is needed if using input initial model(continued train)
v4.4.0,need to continue training
v4.4.0,sync up random seed for data partition
v4.4.0,load Training data
v4.4.0,load data for distributed training
v4.4.0,load data for single machine
v4.4.0,need save binary file
v4.4.0,create training metric
v4.4.0,only when have metrics then need to construct validation data
v4.4.0,"Add validation data, if it exists"
v4.4.0,add
v4.4.0,need save binary file
v4.4.0,add metric for validation data
v4.4.0,output used time on each iteration
v4.4.0,need init network
v4.4.0,create boosting
v4.4.0,create objective function
v4.4.0,load training data
v4.4.0,initialize the objective function
v4.4.0,initialize the boosting
v4.4.0,add validation data into boosting
v4.4.0,convert model to if-else statement code
v4.4.0,create predictor
v4.4.0,Free memory
v4.4.0,create predictor
v4.4.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v4.4.0,counts for all labels
v4.4.0,"start from top label, and accumulate DCG"
v4.4.0,counts for all labels
v4.4.0,calculate k Max DCG by one pass
v4.4.0,get sorted indices by score
v4.4.0,calculate multi dcg by one pass
v4.4.0,wait for all client start up
v4.4.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v4.4.0,Instead we will handle finalize/abort for MPI in main().
v4.4.0,default set to -1
v4.4.0,"distance at k-th communication, distance[k] = 2^k"
v4.4.0,set incoming rank at k-th commuication
v4.4.0,set outgoing rank at k-th commuication
v4.4.0,default set as -1
v4.4.0,construct all recursive halving map for all machines
v4.4.0,let 1 << k <= num_machines
v4.4.0,distance of each communication
v4.4.0,"if num_machines = 2^k, don't need to group machines"
v4.4.0,"communication direction, %2 == 0 is positive"
v4.4.0,neighbor at k-th communication
v4.4.0,receive data block at k-th communication
v4.4.0,send data block at k-th communication
v4.4.0,"if num_machines != 2^k, need to group machines"
v4.4.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v4.4.0,let left machine as group leader
v4.4.0,"cache block information for groups, group with 2 machines will have double block size"
v4.4.0,convert from group to node leader
v4.4.0,convert from node to group
v4.4.0,meet new group
v4.4.0,add block len for this group
v4.4.0,calculate the group block start
v4.4.0,not need to construct
v4.4.0,get receive block information
v4.4.0,accumulate block len
v4.4.0,get send block information
v4.4.0,accumulate block len
v4.4.0,static member definition
v4.4.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v4.4.0,assign the blocks to every rank.
v4.4.0,do reduce scatter
v4.4.0,do all gather
v4.4.0,assign blocks
v4.4.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v4.4.0,copy back
v4.4.0,assign blocks
v4.4.0,start all gather
v4.4.0,when num_machines is small and data is large
v4.4.0,use output as receive buffer
v4.4.0,get current local block size
v4.4.0,get out rank
v4.4.0,get in rank
v4.4.0,get send information
v4.4.0,get recv information
v4.4.0,send and recv at same time
v4.4.0,rotate in-place
v4.4.0,use output as receive buffer
v4.4.0,get current local block size
v4.4.0,get send information
v4.4.0,get recv information
v4.4.0,send and recv at same time
v4.4.0,use output as receive buffer
v4.4.0,send and recv at same time
v4.4.0,send local data to neighbor first
v4.4.0,receive neighbor data first
v4.4.0,reduce
v4.4.0,get target
v4.4.0,get send information
v4.4.0,get recv information
v4.4.0,send and recv at same time
v4.4.0,reduce
v4.4.0,send result to neighbor
v4.4.0,receive result from neighbor
v4.4.0,copy result
v4.4.0,start up socket
v4.4.0,parse clients from file
v4.4.0,get ip list of local machine
v4.4.0,get local rank
v4.4.0,construct listener
v4.4.0,construct communication topo
v4.4.0,construct linkers
v4.4.0,free listener
v4.4.0,set timeout
v4.4.0,accept incoming socket
v4.4.0,receive rank
v4.4.0,add new socket
v4.4.0,save ranks that need to connect with
v4.4.0,start listener
v4.4.0,start connect
v4.4.0,let smaller rank connect to larger rank
v4.4.0,send local rank
v4.4.0,wait for listener
v4.4.0,print connected linkers
v4.4.0,only need to copy subset
v4.4.0,avoid to copy subset many times
v4.4.0,avoid out of range
v4.4.0,may need to recopy subset
v4.4.0,valid the type
v4.4.0,parser factory implementation.
v4.4.0,customized parser add-on.
v4.4.0,save header to parser config in case needed.
v4.4.0,save label id to parser config in case needed.
v4.4.0,Constructors
v4.4.0,Get type tag
v4.4.0,Comparisons
v4.4.0,"This has to be separate, not in Statics, because Json() accesses"
v4.4.0,statics().null.
v4.4.0,"advance until next line, or end of input"
v4.4.0,advance until closing tokens
v4.4.0,The usual case: non-escaped characters
v4.4.0,Handle escapes
v4.4.0,Extract 4-byte escape sequence
v4.4.0,Explicitly check length of the substring. The following loop
v4.4.0,relies on std::string returning the terminating NUL when
v4.4.0,accessing str[length]. Checking here reduces brittleness.
v4.4.0,JSON specifies that characters outside the BMP shall be encoded as a
v4.4.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v4.4.0,components. Check whether we're in the middle of such a beast: the
v4.4.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v4.4.0,a trail (low) surrogate.
v4.4.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v4.4.0,per the UTF-16 algorithm.
v4.4.0,Integer part
v4.4.0,Decimal part
v4.4.0,Exponent part
v4.4.0,Check for any trailing garbage
v4.4.0,Documented in json11.hpp
v4.4.0,Check for another object
v4.4.0,get column names
v4.4.0,"support to get header from parser config, so could utilize following label name to id mapping logic."
v4.4.0,load label idx first
v4.4.0,"if parser config file exists, feature names may be changed after customized parser applied."
v4.4.0,clear here so could use default filled feature names during dataset construction.
v4.4.0,may improve by saving real feature names defined in parser in the future.
v4.4.0,erase label column name
v4.4.0,load ignore columns
v4.4.0,load weight idx
v4.4.0,load group idx
v4.4.0,don't support query id in data file when using distributed training
v4.4.0,read data to memory
v4.4.0,sample data
v4.4.0,construct feature bin mappers & clear sample data
v4.4.0,initialize label
v4.4.0,extract features
v4.4.0,sample data from file
v4.4.0,construct feature bin mappers & clear sample data
v4.4.0,initialize label
v4.4.0,extract features
v4.4.0,load data from binary file
v4.4.0,checks whether there's a initial score file when loaded from binary data files
v4.4.0,"the intial score file should with suffix "".bin.init"""
v4.4.0,check meta data
v4.4.0,need to check training data
v4.4.0,read data in memory
v4.4.0,initialize label
v4.4.0,extract features
v4.4.0,Get number of lines of data file
v4.4.0,initialize label
v4.4.0,extract features
v4.4.0,load data from binary file
v4.4.0,checks whether there's a initial score file when loaded from binary data files
v4.4.0,"the intial score file should with suffix "".bin.init"""
v4.4.0,not need to check validation data
v4.4.0,check meta data
v4.4.0,check token
v4.4.0,read feature group definitions
v4.4.0,read feature size
v4.4.0,buffer to read binary file
v4.4.0,check token
v4.4.0,read size of header
v4.4.0,re-allocate space if not enough
v4.4.0,read header
v4.4.0,get header
v4.4.0,read size of meta data
v4.4.0,re-allocate space if not enough
v4.4.0,read meta data
v4.4.0,load meta data
v4.4.0,sample local used data if need to partition
v4.4.0,"if not contain query file, minimal sample unit is one record"
v4.4.0,"if contain query file, minimal sample unit is one query"
v4.4.0,if is new query
v4.4.0,read feature data
v4.4.0,read feature size
v4.4.0,re-allocate space if not enough
v4.4.0,raw data
v4.4.0,fill feature_names_ if not header
v4.4.0,get forced split
v4.4.0,"if only one machine, find bin locally"
v4.4.0,"if have multi-machines, need to find bin distributed"
v4.4.0,different machines will find bin for different features
v4.4.0,start and len will store the process feature indices for different machines
v4.4.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.4.0,free
v4.4.0,gather global feature bin mappers
v4.4.0,restore features bins from buffer
v4.4.0,---- private functions ----
v4.4.0,get header
v4.4.0,num_groups
v4.4.0,real_feature_idx_
v4.4.0,feature2group
v4.4.0,feature2subfeature
v4.4.0,group_bin_boundaries
v4.4.0,group_feature_start_
v4.4.0,group_feature_cnt_
v4.4.0,get feature names
v4.4.0,get forced_bin_bounds_
v4.4.0,"if features are ordered, not need to use hist_buf"
v4.4.0,read all lines
v4.4.0,get query data
v4.4.0,"if not contain query data, minimal sample unit is one record"
v4.4.0,"if contain query data, minimal sample unit is one query"
v4.4.0,if is new query
v4.4.0,get query data
v4.4.0,"if not contain query file, minimal sample unit is one record"
v4.4.0,"if contain query file, minimal sample unit is one query"
v4.4.0,if is new query
v4.4.0,parse features
v4.4.0,get forced split
v4.4.0,"check the range of label_idx, weight_idx and group_idx"
v4.4.0,"skip label check if user input parser config file,"
v4.4.0,because label id is got from raw features while dataset features are consistent with customized parser.
v4.4.0,fill feature_names_ if not header
v4.4.0,start find bins
v4.4.0,"if only one machine, find bin locally"
v4.4.0,start and len will store the process feature indices for different machines
v4.4.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.4.0,free
v4.4.0,gather global feature bin mappers
v4.4.0,restore features bins from buffer
v4.4.0,if doesn't need to prediction with initial model
v4.4.0,parser
v4.4.0,set label
v4.4.0,free processed line:
v4.4.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v4.4.0,text_reader_->Lines()[i].shrink_to_fit();
v4.4.0,push data
v4.4.0,if is used feature
v4.4.0,if need to prediction with initial model
v4.4.0,parser
v4.4.0,set initial score
v4.4.0,set label
v4.4.0,free processed line:
v4.4.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v4.4.0,text_reader_->Lines()[i].shrink_to_fit();
v4.4.0,push data
v4.4.0,if is used feature
v4.4.0,metadata_ will manage space of init_score
v4.4.0,text data can be free after loaded feature values
v4.4.0,parser
v4.4.0,set initial score
v4.4.0,set label
v4.4.0,push data
v4.4.0,if is used feature
v4.4.0,only need part of data
v4.4.0,need full data
v4.4.0,metadata_ will manage space of init_score
v4.4.0,read size of token
v4.4.0,remove duplicates
v4.4.0,deep copy function for BinMapper
v4.4.0,mean size for one bin
v4.4.0,need a new bin
v4.4.0,update bin upper bound
v4.4.0,last bin upper bound
v4.4.0,get number of positive and negative distinct values
v4.4.0,include zero bounds and infinity bound
v4.4.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v4.4.0,find remaining bounds
v4.4.0,find distinct_values first
v4.4.0,push zero in the front
v4.4.0,use the large value
v4.4.0,push zero in the back
v4.4.0,convert to int type first
v4.4.0,sort by counts in descending order
v4.4.0,will ignore the categorical of small counts
v4.4.0,Push the dummy bin for NaN
v4.4.0,Use MissingType::None to represent this bin contains all categoricals
v4.4.0,fix count of NaN bin
v4.4.0,check trivial(num_bin_ == 1) feature
v4.4.0,check useless bin
v4.4.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v4.4.0,so use most_freq_bin_ = default_bin_ when there is not so sparse
v4.4.0,calculate max bin of all features to select the int type in MultiValDenseBin
v4.4.0,"for lambdarank, it needs query data for partition data in distributed learning"
v4.4.0,need convert query_id to boundaries
v4.4.0,check weights
v4.4.0,check positions
v4.4.0,check query boundries
v4.4.0,contain initial score file
v4.4.0,check weights
v4.4.0,get local weights
v4.4.0,check positions
v4.4.0,get local positions
v4.4.0,check query boundries
v4.4.0,get local query boundaries
v4.4.0,contain initial score file
v4.4.0,get local initial scores
v4.4.0,re-calculate query weight
v4.4.0,Clear init scores on empty input
v4.4.0,"Note that len here is row count, not num_init_score, so we compare against num_data"
v4.4.0,"We need to use source_size here, because len might not equal size (due to a partially loaded dataset)"
v4.4.0,CUDA is handled after all insertions are complete
v4.4.0,CUDA is handled after all insertions are complete
v4.4.0,Clear weights on empty input
v4.4.0,CUDA is handled after all insertions are complete
v4.4.0,Clear query boundaries on empty input
v4.4.0,save to nullptr
v4.4.0,CUDA is handled after all insertions are complete
v4.4.0,default weight file name
v4.4.0,default position file name
v4.4.0,default init_score file name
v4.4.0,use first line to count number class
v4.4.0,default query file name
v4.4.0,root is in the depth 0
v4.4.0,non-leaf
v4.4.0,leaf
v4.4.0,use this for the missing value conversion
v4.4.0,Predict func by Map to ifelse
v4.4.0,use this for the missing value conversion
v4.4.0,non-leaf
v4.4.0,left subtree
v4.4.0,right subtree
v4.4.0,leaf
v4.4.0,non-leaf
v4.4.0,left subtree
v4.4.0,right subtree
v4.4.0,leaf
v4.4.0,recursive computation of SHAP values for a decision tree
v4.4.0,extend the unique path
v4.4.0,leaf node
v4.4.0,internal node
v4.4.0,"see if we have already split on this feature,"
v4.4.0,if so we undo that split so we can redo it for this node
v4.4.0,recursive sparse computation of SHAP values for a decision tree
v4.4.0,extend the unique path
v4.4.0,leaf node
v4.4.0,internal node
v4.4.0,"see if we have already split on this feature,"
v4.4.0,if so we undo that split so we can redo it for this node
v4.4.0,add names of objective function if not providing metric
v4.4.0,equal weights for all classes
v4.4.0,generate seeds by seed.
v4.4.0,sort eval_at
v4.4.0,Only push the non-training data
v4.4.0,check for conflicts
v4.4.0,"check if objective, metric, and num_class match"
v4.4.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v4.4.0,"max_depth defaults to -1, so max_depth>0 implies ""you explicitly overrode the default"""
v4.4.0,
v4.4.0,Changing max_depth while leaving num_leaves at its default (31) can lead to 2 undesirable situations:
v4.4.0,
v4.4.0,* (0 <= max_depth <= 4) it's not possible to produce a tree with 31 leaves
v4.4.0,- this block reduces num_leaves to 2^max_depth
v4.4.0,"* (max_depth > 4) 31 leaves is less than a full depth-wise tree, which might lead to underfitting"
v4.4.0,- this block warns about that
v4.4.0,ref: https://github.com/microsoft/LightGBM/issues/2898#issuecomment-1002860601
v4.4.0,"Fits in an int, and is more restrictive than the current num_leaves"
v4.4.0,"force col-wise for gpu, and cuda version"
v4.4.0,force row-wise for cuda version
v4.4.0,linear tree learner must be serial type and run on CPU device
v4.4.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v4.4.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v4.4.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v4.4.0,calculated split gain can be positive even with zero gradient and hessian.
v4.4.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v4.4.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v4.4.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v4.4.0,first round: fill the single val group
v4.4.0,always push the last group
v4.4.0,put dense feature first
v4.4.0,sort by non zero cnt
v4.4.0,"sort by non zero cnt, bigger first"
v4.4.0,shuffle groups
v4.4.0,Using std::swap for vector<bool> will cause the wrong result.
v4.4.0,get num_features
v4.4.0,get bin_mappers
v4.4.0,"for sparse multi value bin, we store the feature bin values with offset added"
v4.4.0,"for dense multi value bin, the feature bin values without offsets are used"
v4.4.0,copy feature bin mapper data
v4.4.0,copy feature bin mapper data
v4.4.0,update CUDA storage for column data and metadata
v4.4.0,"if not pass a filename, just append "".bin"" of original file"
v4.4.0,Write the basic header information for the dataset
v4.4.0,get size of meta data
v4.4.0,write meta data
v4.4.0,write feature data
v4.4.0,get size of feature
v4.4.0,write feature
v4.4.0,write raw data; use row-major order so we can read row-by-row
v4.4.0,Calculate approximate size of output and reserve space
v4.4.0,write feature group definitions
v4.4.0,"Give a little extra just in case, to avoid unnecessary resizes"
v4.4.0,"Write token that marks the data as binary reference, and the version"
v4.4.0,Write the basic definition of the overall dataset
v4.4.0,write feature group definitions
v4.4.0,get size of feature
v4.4.0,write feature
v4.4.0,size of feature names and forced bins
v4.4.0,write header
v4.4.0,write feature names
v4.4.0,write forced bins
v4.4.0,"explicitly initialize template methods, for cross module call"
v4.4.0,"explicitly initialize template methods, for cross module call"
v4.4.0,"Only one multi-val group, just simply merge"
v4.4.0,Skip the leading 0 when copying group_bin_boundaries.
v4.4.0,regenerate other fields
v4.4.0,need to iterate bin iterator
v4.4.0,is dense column
v4.4.0,is sparse column
v4.4.0,initialize the subset cuda column data
v4.4.0,"if one column has too many bins, use a separate partition for that column"
v4.4.0,try if adding this column exceed the maximum number per partition
v4.4.0,"if one column has too many bins, use a separate partition for that column"
v4.4.0,try if adding this column exceed the maximum number per partition
v4.4.0,"if LightGBM-specific default has been set, ignore OpenMP-global config"
v4.4.0,"otherwise, default to OpenMP-global config"
v4.4.0,"ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't"
v4.4.0,use more than that many threads
v4.4.0,store the importance first
v4.4.0,PredictRaw
v4.4.0,PredictRawByMap
v4.4.0,Predict
v4.4.0,PredictByMap
v4.4.0,PredictLeafIndex
v4.4.0,PredictLeafIndexByMap
v4.4.0,output model type
v4.4.0,output number of class
v4.4.0,output label index
v4.4.0,output max_feature_idx
v4.4.0,output objective
v4.4.0,output tree models
v4.4.0,store the importance first
v4.4.0,sort the importance
v4.4.0,use serialized string to restore this object
v4.4.0,Use first 128 chars to avoid exceed the message buffer.
v4.4.0,get number of classes
v4.4.0,get index of label
v4.4.0,get max_feature_idx first
v4.4.0,get average_output
v4.4.0,get feature names
v4.4.0,get monotone_constraints
v4.4.0,set zero
v4.4.0,predict all the trees for one iteration
v4.4.0,check early stopping
v4.4.0,set zero
v4.4.0,predict all the trees for one iteration
v4.4.0,check early stopping
v4.4.0,margin_threshold will be captured by value
v4.4.0,copy and sort
v4.4.0,margin_threshold will be captured by value
v4.4.0,Fix for compiler warnings about reaching end of control
v4.4.0,load forced_splits file
v4.4.0,init tree learner
v4.4.0,push training metrics
v4.4.0,get max feature index
v4.4.0,get label index
v4.4.0,get feature names
v4.4.0,get parser config file content
v4.4.0,check that forced splits does not use feature indices larger than dataset size
v4.4.0,"if need bagging, create buffer"
v4.4.0,"for a validation dataset, we need its score and metric"
v4.4.0,update score
v4.4.0,objective function will calculate gradients and hessians
v4.4.0,output used time per iteration
v4.4.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v4.4.0,boosting first
v4.4.0,use customized objective function
v4.4.0,the check below fails unless objective=custom is provided in the parameters on Booster creation
v4.4.0,need to copy customized gradients when using GOSS
v4.4.0,bagging logic
v4.4.0,need to copy gradients for bagging subset.
v4.4.0,shrinkage by learning rate
v4.4.0,update score
v4.4.0,only add default score one-time
v4.4.0,updates scores
v4.4.0,add model
v4.4.0,reset score
v4.4.0,remove model
v4.4.0,print message for metric
v4.4.0,pop last early_stopping_round_ models
v4.4.0,update training score
v4.4.0,we need to predict out-of-bag scores of data for boosting
v4.4.0,update validation score
v4.4.0,print training metric
v4.4.0,print validation metric
v4.4.0,set zero
v4.4.0,predict all the trees for one iteration
v4.4.0,predict all the trees for one iteration
v4.4.0,push training metrics
v4.4.0,"not same training data, need reset score and others"
v4.4.0,create score tracker
v4.4.0,update score
v4.4.0,resize gradient vectors to copy the customized gradients for goss or bagging with subset
v4.4.0,load forced_splits file
v4.4.0,"if exists initial score, will start from it"
v4.4.0,clear host score buffer
v4.4.0,"Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated"
v4.4.0,"with the larger categorical l2, whereas min_split_gain uses the original l2."
v4.4.0,"if data not enough, or sum hessian too small"
v4.4.0,if data not enough
v4.4.0,if sum hessian too small
v4.4.0,current split gain
v4.4.0,gain with split is worse than without split
v4.4.0,mark as able to be split
v4.4.0,better split point
v4.4.0,recover sum of gradient and hessian from the sum of quantized gradient and hessian
v4.4.0,"Need special case for no smoothing to preserve existing behaviour. If no smoothing, the parent output is calculated"
v4.4.0,"with the larger categorical l2, whereas min_split_gain uses the original l2."
v4.4.0,"if data not enough, or sum hessian too small"
v4.4.0,if data not enough
v4.4.0,if sum hessian too small
v4.4.0,current split gain
v4.4.0,gain with split is worse than without split
v4.4.0,mark as able to be split
v4.4.0,better split point
v4.4.0,Get the max size of pool
v4.4.0,at least need 2 leaves
v4.4.0,push split information for all leaves
v4.4.0,initialize splits for leaf
v4.4.0,initialize data partition
v4.4.0,initialize ordered gradients and hessians
v4.4.0,cannot change is_hist_col_wise during training
v4.4.0,initialize splits for leaf
v4.4.0,initialize data partition
v4.4.0,initialize ordered gradients and hessians
v4.4.0,Get the max size of pool
v4.4.0,at least need 2 leaves
v4.4.0,push split information for all leaves
v4.4.0,some initial works before training
v4.4.0,root leaf
v4.4.0,only root leaf can be splitted on first time
v4.4.0,some initial works before finding best split
v4.4.0,find best threshold for every feature
v4.4.0,Get a leaf with max split gain
v4.4.0,Get split information for best leaf
v4.4.0,"cannot split, quit"
v4.4.0,split tree with best leaf
v4.4.0,reset histogram pool
v4.4.0,initialize data partition
v4.4.0,reset the splits for leaves
v4.4.0,Sumup for root
v4.4.0,use all data
v4.4.0,"use bagging, only use part of data"
v4.4.0,check depth of current leaf
v4.4.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v4.4.0,no enough data to continue
v4.4.0,only have root
v4.4.0,put parent(left) leaf's histograms into larger leaf's histograms
v4.4.0,put parent(left) leaf's histograms to larger leaf's histograms
v4.4.0,construct smaller leaf
v4.4.0,construct larger leaf
v4.4.0,find splits
v4.4.0,only has root leaf
v4.4.0,start at root leaf
v4.4.0,Histogram construction require parent features.
v4.4.0,"then, compute own splits"
v4.4.0,split info should exist because searching in bfs fashion - should have added from parent
v4.4.0,update before tree split
v4.4.0,don't need to update this in data-based parallel model
v4.4.0,"split tree, will return right leaf"
v4.4.0,store the true split gain in tree model
v4.4.0,don't need to update this in data-based parallel model
v4.4.0,store the true split gain in tree model
v4.4.0,init the leaves that used on next iteration
v4.4.0,update leave outputs if needed
v4.4.0,bag_mapper[index_mapper[i]]
v4.4.0,it is needed to filter the features after the above code.
v4.4.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v4.4.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v4.4.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v4.4.0,find splits
v4.4.0,identify features containing nans
v4.4.0,preallocate the matrix used to calculate linear model coefficients
v4.4.0,"store only upper triangular half of matrix as an array, in row-major order"
v4.4.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v4.4.0,we add another 8 to ensure cache lines are not shared among processors
v4.4.0,some initial works before training
v4.4.0,root leaf
v4.4.0,only root leaf can be splitted on first time
v4.4.0,some initial works before finding best split
v4.4.0,find best threshold for every feature
v4.4.0,Get a leaf with max split gain
v4.4.0,Get split information for best leaf
v4.4.0,"cannot split, quit"
v4.4.0,split tree with best leaf
v4.4.0,map data to leaf number
v4.4.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v4.4.0,the coefficients vector is given by
v4.4.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v4.4.0,where:
v4.4.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v4.4.0,"H is the diagonal matrix of the hessian,"
v4.4.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v4.4.0,g is the vector of gradients
v4.4.0,the subscript _T denotes the transpose
v4.4.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v4.4.0,clear the coefficient matrices
v4.4.0,aggregate results from different threads
v4.4.0,copy into eigen matrices and solve
v4.4.0,update the tree properties
v4.4.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.4.0,get feature partition
v4.4.0,get local used features
v4.4.0,get best split at smaller leaf
v4.4.0,find local best split for larger leaf
v4.4.0,sync global best info
v4.4.0,update best split
v4.4.0,"instantiate template classes, otherwise linker cannot find the code"
v4.4.0,initialize SerialTreeLearner
v4.4.0,Get local rank and global machine size
v4.4.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.4.0,allocate buffer for communication
v4.4.0,get block start and block len for reduce scatter
v4.4.0,get buffer_write_start_pos
v4.4.0,get buffer_read_start_pos
v4.4.0,generate feature partition for current tree
v4.4.0,get local used feature
v4.4.0,get block start and block len for reduce scatter
v4.4.0,sync global data sumup info
v4.4.0,global sumup reduce
v4.4.0,copy back
v4.4.0,set global sumup info
v4.4.0,init global data count in leaf
v4.4.0,reset hist num bits according to global num data
v4.4.0,sync global data sumup info
v4.4.0,global sumup reduce
v4.4.0,copy back
v4.4.0,set global sumup info
v4.4.0,init global data count in leaf
v4.4.0,clear histogram buffer before synchronizing
v4.4.0,otherwise histogram contents from the previous iteration will be sent
v4.4.0,construct local histograms
v4.4.0,copy to buffer
v4.4.0,Reduce scatter for histogram
v4.4.0,restore global histograms from buffer
v4.4.0,only root leaf
v4.4.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v4.4.0,find local best split for larger leaf
v4.4.0,sync global best info
v4.4.0,set best split
v4.4.0,need update global number of data in leaf
v4.4.0,reset hist num bits according to global num data
v4.4.0,"instantiate template classes, otherwise linker cannot find the code"
v4.4.0,initialize SerialTreeLearner
v4.4.0,some additional variables needed for GPU trainer
v4.4.0,Initialize GPU buffers and kernels
v4.4.0,some functions used for debugging the GPU histogram construction
v4.4.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v4.4.0,goto err;
v4.4.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v4.4.0,also guarantee that there are at least 2K examples per workgroup
v4.4.0,return 0;
v4.4.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v4.4.0,decide the best number of workgroups working on one feature4 tuple
v4.4.0,set work group size based on feature size
v4.4.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v4.4.0,we need to refresh the kernel arguments after reallocating
v4.4.0,The only argument that needs to be changed later is num_data_
v4.4.0,"the GPU kernel will process all features in one call, and each"
v4.4.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v4.4.0,process one feature4 tuple
v4.4.0,"for the root node, indices are not copied"
v4.4.0,"for constant hessian, hessians are not copied except for the root node"
v4.4.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v4.4.0,and we will launch num_feature workgroups for this kernel
v4.4.0,will launch threads for all features
v4.4.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v4.4.0,copy the results asynchronously. Size depends on if double precision is used
v4.4.0,we will wait for this object in WaitAndGetHistograms
v4.4.0,"when the output is ready, the computation is done"
v4.4.0,values of this feature has been redistributed to multiple bins; need a reduction here
v4.4.0,how many feature-group tuples we have
v4.4.0,leave some safe margin for prefetching
v4.4.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v4.4.0,clear sparse/dense maps
v4.4.0,do nothing if no features can be processed on GPU
v4.4.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v4.4.0,unpin old buffer if necessary before destructing them
v4.4.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v4.4.0,allocate space for gradients and Hessians on device
v4.4.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v4.4.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v4.4.0,copy indices to the device
v4.4.0,histogram bin entry size depends on the precision (single/double)
v4.4.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v4.4.0,each work group generates a sub-histogram of dword_features_ features.
v4.4.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v4.4.0,create atomic counters for inter-group coordination
v4.4.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v4.4.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v4.4.0,looking for dword_features_ non-sparse feature-groups
v4.4.0,decide if we need to redistribute the bin
v4.4.0,multiplier must be a power of 2
v4.4.0,device_bin_mults_.push_back(1);
v4.4.0,found
v4.4.0,for data transfer time
v4.4.0,"Now generate new data structure feature4, and copy data to the device"
v4.4.0,"preallocate arrays for all threads, and pin them"
v4.4.0,building Feature4 bundles; each thread handles dword_features_ features
v4.4.0,one feature datapoint is 4 bits
v4.4.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.4.0,one feature datapoint is one byte
v4.4.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.4.0,Dense bin
v4.4.0,Dense 4-bit bin
v4.4.0,working on the remaining (less than dword_features_) feature groups
v4.4.0,fill the leftover features
v4.4.0,"fill this empty feature with some ""random"" value"
v4.4.0,"fill this empty feature with some ""random"" value"
v4.4.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v4.4.0,deallocate pinned space for feature copying
v4.4.0,data transfer time
v4.4.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v4.4.0,"Something bad happened. Just return ""No log available."""
v4.4.0,"build is okay, log may contain warnings"
v4.4.0,destroy any old kernels
v4.4.0,create OpenCL kernels for different number of workgroups per feature
v4.4.0,currently we don't use constant memory
v4.4.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v4.4.0,kernel with indices in an array
v4.4.0,"kernel with all features enabled, with eliminated branches"
v4.4.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v4.4.0,do nothing if no features can be processed on GPU
v4.4.0,The only argument that needs to be changed later is num_data_
v4.4.0,"hessian is passed as a parameter, but it is not available now."
v4.4.0,hessian will be set in BeforeTrain()
v4.4.0,"Get the max bin size, used for selecting best GPU kernel"
v4.4.0,initialize GPU
v4.4.0,determine which kernel to use based on the max number of bins
v4.4.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.4.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.4.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.4.0,ignore the feature groups that contain categorical features when producing warnings about max_bin.
v4.4.0,"these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin."
v4.4.0,setup GPU kernel arguments after we allocating all the buffers
v4.4.0,GPU memory has to been reallocated because data may have been changed
v4.4.0,setup GPU kernel arguments after we allocating all the buffers
v4.4.0,Copy initial full hessians and gradients to GPU.
v4.4.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v4.4.0,setup hessian parameters only
v4.4.0,hessian is passed as a parameter
v4.4.0,use bagging
v4.4.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v4.4.0,copy used gradients and Hessians to ordered buffer
v4.4.0,transfer the indices to GPU
v4.4.0,transfer hessian to GPU
v4.4.0,setup hessian parameters only
v4.4.0,hessian is passed as a parameter
v4.4.0,transfer gradients to GPU
v4.4.0,only have root
v4.4.0,"Copy indices, gradients and Hessians as early as possible"
v4.4.0,only need to initialize for smaller leaf
v4.4.0,Get leaf boundary
v4.4.0,copy indices to the GPU:
v4.4.0,copy ordered Hessians to the GPU:
v4.4.0,copy ordered gradients to the GPU:
v4.4.0,do nothing if no features can be processed on GPU
v4.4.0,copy data indices if it is not null
v4.4.0,generate and copy ordered_gradients if gradients is not null
v4.4.0,generate and copy ordered_hessians if Hessians is not null
v4.4.0,converted indices in is_feature_used to feature-group indices
v4.4.0,construct the feature masks for dense feature-groups
v4.4.0,"if no feature group is used, just return and do not use GPU"
v4.4.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v4.4.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v4.4.0,"All data have been prepared, now run the GPU kernel"
v4.4.0,construct smaller leaf
v4.4.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v4.4.0,then construct sparse features on CPU
v4.4.0,"wait for GPU to finish, only if GPU is actually used"
v4.4.0,use double precision
v4.4.0,use single precision
v4.4.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v4.4.0,#define GPU_DEBUG_COMPARE
v4.4.0,construct larger leaf
v4.4.0,then construct sparse features on CPU
v4.4.0,"wait for GPU to finish, only if GPU is actually used"
v4.4.0,use double precision
v4.4.0,use single precision
v4.4.0,do some sanity check for the GPU algorithm
v4.4.0,limit top k
v4.4.0,get max bin
v4.4.0,calculate buffer size
v4.4.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.4.0,"left and right on same time, so need double size"
v4.4.0,initialize histograms for global
v4.4.0,sync global data sumup info
v4.4.0,set global sumup info
v4.4.0,init global data count in leaf
v4.4.0,get local sumup
v4.4.0,get local sumup
v4.4.0,get mean number on machines
v4.4.0,weighted gain
v4.4.0,get top k
v4.4.0,"Copy histogram to buffer, and Get local aggregate features"
v4.4.0,copy histograms.
v4.4.0,copy smaller leaf histograms first
v4.4.0,mark local aggregated feature
v4.4.0,copy
v4.4.0,then copy larger leaf histograms
v4.4.0,mark local aggregated feature
v4.4.0,copy
v4.4.0,use local data to find local best splits
v4.4.0,clear histogram buffer before synchronizing
v4.4.0,otherwise histogram contents from the previous iteration will be sent
v4.4.0,find splits
v4.4.0,only has root leaf
v4.4.0,local voting
v4.4.0,gather
v4.4.0,get all top-k from all machines
v4.4.0,global voting
v4.4.0,copy local histgrams to buffer
v4.4.0,Reduce scatter for histogram
v4.4.0,find best split from local aggregated histograms
v4.4.0,restore from buffer
v4.4.0,restore from buffer
v4.4.0,find local best
v4.4.0,find local best split for larger leaf
v4.4.0,sync global best info
v4.4.0,copy back
v4.4.0,set the global number of data for leaves
v4.4.0,init the global sumup info
v4.4.0,"instantiate template classes, otherwise linker cannot find the code"
v4.4.0,allocate CUDA memory
v4.4.0,leave some space for alignment
v4.4.0,input best split info
v4.4.0,for leaf information update
v4.4.0,"gather information for CPU, used for launching kernels"
v4.4.0,for leaf splits information update
v4.4.0,we need restore the order of indices in cuda_data_indices_
v4.4.0,allocate more memory for sum reduction in CUDA
v4.4.0,only the first element records the final sum
v4.4.0,intialize split find task information (a split find task is one pass through the histogram of a feature)
v4.4.0,need to double the size of histogram buffer in global memory when using double precision in histogram construction
v4.4.0,use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes
v4.4.0,use the first gpu by default
v4.4.0,"std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values"
v4.4.0,use feature interaction constraint or sample features by node
v4.3.0,coding: utf-8
v4.3.0,create predictor first
v4.3.0,setting early stopping via global params should be possible
v4.3.0,reduce cost for prediction training data
v4.3.0,process callbacks
v4.3.0,construct booster
v4.3.0,start training
v4.3.0,check evaluation result.
v4.3.0,"ranking task, split according to groups"
v4.3.0,run preprocessing on the data set if needed
v4.3.0,setting early stopping via global params should be possible
v4.3.0,setup callbacks
v4.3.0,coding: utf-8
v4.3.0,dummy function to support older version of scikit-learn
v4.3.0,coding: utf-8
v4.3.0,"f(labels, preds)"
v4.3.0,"f(labels, preds, weights)"
v4.3.0,"f(labels, preds, weights, group)"
v4.3.0,"f(labels, preds)"
v4.3.0,"f(labels, preds, weights)"
v4.3.0,"f(labels, preds, weights, group)"
v4.3.0,documentation templates for LGBMModel methods are shared between the classes in
v4.3.0,this module and those in the ``dask`` module
v4.3.0,register default metric for consistency with callable eval_metric case
v4.3.0,try to deduce from class instance
v4.3.0,overwrite default metric by explicitly set metric
v4.3.0,"use joblib conventions for negative n_jobs, just like scikit-learn"
v4.3.0,"at predict time, this is handled later due to the order of parameter updates"
v4.3.0,Do not modify original args in fit function
v4.3.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v4.3.0,Separate built-in from callable evaluation metrics
v4.3.0,concatenate metric from params (or default if not provided in params) and eval_metric
v4.3.0,copy for consistency
v4.3.0,reduce cost for prediction training data
v4.3.0,free dataset
v4.3.0,retrive original params that possibly can be used in both training and prediction
v4.3.0,and then overwrite them (considering aliases) with params that were passed directly in prediction
v4.3.0,number of threads can have values with special meaning which is only applied
v4.3.0,"in the scikit-learn interface, these should not reach the c++ side as-is"
v4.3.0,adjust eval metrics to match whether binary or multiclass
v4.3.0,classification is being performed
v4.3.0,"do not modify args, as it causes errors in model selection tools"
v4.3.0,check group data
v4.3.0,coding: utf-8
v4.3.0,coding: utf-8
v4.3.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v4.3.0,coding: utf-8
v4.3.0,coding: utf-8
v4.3.0,typing.TypeGuard was only introduced in Python 3.10
v4.3.0,we don't need lib_lightgbm while building docs
v4.3.0,TypeError: obj is not a string or a number
v4.3.0,ValueError: invalid literal
v4.3.0,Obtain objects to export
v4.3.0,Prepare export
v4.3.0,Export all objects
v4.3.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v4.3.0,"lazy evaluation to allow import without dynamic library, e.g., for docs generation"
v4.3.0,"if buffer length is not long enough, re-allocate a buffer"
v4.3.0,avoid side effects on passed-in parameters
v4.3.0,"if main_param_name was provided, keep that value and remove all aliases"
v4.3.0,"if main param name was not found, search for an alias"
v4.3.0,"neither of main_param_name, aliases were found"
v4.3.0,most common case (no nullable dtypes)
v4.3.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.3.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.3.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.3.0,take shallow copy in case we modify categorical columns
v4.3.0,whole column modifications don't change the original df
v4.3.0,determine feature names
v4.3.0,determine categorical features
v4.3.0,use cat cols from DataFrame
v4.3.0,so that the target dtype considers floats
v4.3.0,Get total row number.
v4.3.0,Random access by row index. Used for data sampling.
v4.3.0,Range data access. Used to read data in batch when constructing Dataset.
v4.3.0,Optionally specify batch_size to control range data read size.
v4.3.0,Only required if using ``Dataset.subset()``.
v4.3.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.3.0,avoid memory consumption by arrays concatenation operations
v4.3.0,create numpy array from output arrays
v4.3.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v4.3.0,for CSC there is extra column added
v4.3.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v4.3.0,same shape as input csr or csc matrix except extra column for expected value
v4.3.0,note: make sure we copy data as it will be deallocated next
v4.3.0,"free the temporary native indptr, indices, and data"
v4.3.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.3.0,avoid memory consumption by arrays concatenation operations
v4.3.0,Check that the input is valid: we only handle numbers (for now)
v4.3.0,Prepare prediction output array
v4.3.0,Export Arrow table to C and run prediction
v4.3.0,c type: double**
v4.3.0,each double* element points to start of each column of sample data.
v4.3.0,c type int**
v4.3.0,each int* points to start of indices for each column
v4.3.0,"no min_data, nthreads and verbose in this function"
v4.3.0,check data has header or not
v4.3.0,need to regroup init_score
v4.3.0,process for args
v4.3.0,get categorical features
v4.3.0,"If the params[cat_alias] is equal to categorical_indices, do not report the warning."
v4.3.0,process for reference dataset
v4.3.0,start construct data
v4.3.0,set feature names
v4.3.0,"Select sampled rows, transpose to column order."
v4.3.0,create validation dataset from ref_dataset
v4.3.0,Check that the input is valid: we only handle numbers (for now)
v4.3.0,Export Arrow table to C
v4.3.0,create valid
v4.3.0,construct subset
v4.3.0,create train
v4.3.0,could be updated if data is not freed
v4.3.0,set to None
v4.3.0,"If the data is a arrow data, we can just pass it to C"
v4.3.0,"If a table is being passed, we concatenate the columns. This is only valid for"
v4.3.0,'init_score'.
v4.3.0,we're done if self and reference share a common upstream reference
v4.3.0,Check if the weight contains values other than one
v4.3.0,Set field
v4.3.0,original values can be modified at cpp side
v4.3.0,"if buffer length is not long enough, reallocate buffers"
v4.3.0,"group data from LightGBM is boundaries data, need to convert to group size"
v4.3.0,Training task
v4.3.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v4.3.0,construct booster object
v4.3.0,copy the parameters from train_set
v4.3.0,save reference to data
v4.3.0,buffer for inner predict
v4.3.0,Prediction task
v4.3.0,"if buffer length is not long enough, re-allocate a buffer"
v4.3.0,if a single node tree it won't have `leaf_index` so return 0
v4.3.0,"Create the node record, and populate universal data members"
v4.3.0,Update values to reflect node type (leaf or split)
v4.3.0,traverse the next level of the tree
v4.3.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v4.3.0,and we add node to the list.
v4.3.0,need reset training data
v4.3.0,need to push new valid data
v4.3.0,ensure that existing Booster is freed before replacing it
v4.3.0,with a new one createdfrom file
v4.3.0,"if buffer length is not long enough, re-allocate a buffer"
v4.3.0,"if buffer length is not long enough, reallocate a buffer"
v4.3.0,Copy models
v4.3.0,Get name of features
v4.3.0,"if buffer length is not long enough, reallocate buffers"
v4.3.0,avoid to predict many time in one iteration
v4.3.0,Get num of inner evals
v4.3.0,Get name of eval metrics
v4.3.0,"if buffer length is not long enough, reallocate buffers"
v4.3.0,coding: utf-8
v4.3.0,Callback environment used by callbacks
v4.3.0,"CVBooster holds a list of Booster objects, each needs to be updated"
v4.3.0,"for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set"
v4.3.0,and those metrics are considered for early stopping
v4.3.0,"for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name"
v4.3.0,validation sets are guaranteed to not be identical to the training data in cv()
v4.3.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.3.0,self.best_score_list is initialized to an empty list
v4.3.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.3.0,coding: utf-8
v4.3.0,Acquire port in worker
v4.3.0,schedule futures to retrieve each element of the tuple
v4.3.0,retrieve ports
v4.3.0,Concatenate many parts into one
v4.3.0,construct local eval_set data.
v4.3.0,store indices of eval_set components that were not contained within local parts.
v4.3.0,consolidate parts of each individual eval component.
v4.3.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v4.3.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v4.3.0,filter padding from eval parts then _concat each eval_set component.
v4.3.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v4.3.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v4.3.0,capture whether local_listen_port or its aliases were provided
v4.3.0,capture whether machines or its aliases were provided
v4.3.0,Some passed-in parameters can be removed:
v4.3.0,* 'num_machines': set automatically from Dask worker list
v4.3.0,* 'num_threads': overridden to match nthreads on each Dask process
v4.3.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v4.3.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v4.3.0,X and y are each delayed sub-lists of original eval dask Collections.
v4.3.0,find maximum number of parts in an individual eval set so that we can
v4.3.0,pad eval sets when they come in different sizes.
v4.3.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v4.3.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v4.3.0,first time a chunk of this eval set is added to this part.
v4.3.0,append additional chunks of this eval set to this part.
v4.3.0,ensure that all evaluation parts map uniquely to one part.
v4.3.0,assign sub-eval_set components to worker parts.
v4.3.0,Start computation in the background
v4.3.0,trigger error locally
v4.3.0,Find locations of all parts and map them to particular Dask workers
v4.3.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v4.3.0,data artifacts may not be populated depending on worker returning final estimator.
v4.3.0,assign general validation set settings to fit kwargs.
v4.3.0,resolve aliases for network parameters and pop the result off params.
v4.3.0,these values are added back in calls to `_train_part()`
v4.3.0,figure out network params
v4.3.0,Tell each worker to train on the parts that it has locally
v4.3.0,
v4.3.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v4.3.0,1. there is randomness in the training process unless parameters ``seed``
v4.3.0,and ``deterministic`` are set
v4.3.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v4.3.0,relies on global state (it and all the other LightGBM training processes
v4.3.0,coordinate with each other)
v4.3.0,"if network parameters were changed during training, remove them from the"
v4.3.0,returned model so that they're generated dynamically on every run based
v4.3.0,on the Dask cluster you're connected to and which workers have pieces of
v4.3.0,the training data
v4.3.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v4.3.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v4.3.0,are returned as a list of sparse matrices (one per class)
v4.3.0,"pred_contrib output will have one column per feature,"
v4.3.0,plus one more for the base value
v4.3.0,need to tell Dask the expected type and shape of individual preds
v4.3.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v4.3.0,the code below is used instead to ensure that the sparse type is preserved during concatentation
v4.3.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v4.3.0,Concatenate them to return a list of Dask Arrays.
v4.3.0,"DaskLGBMClassifier does not support group, eval_group."
v4.3.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v4.3.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group."
v4.3.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v4.3.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v4.3.0,DaskLGBMRanker support for callbacks and init_model is not tested
v4.3.0,coding: utf-8
v4.3.0,load or create your dataset
v4.3.0,create dataset for lightgbm
v4.3.0,"if you want to re-use data, remember to set free_raw_data=False"
v4.3.0,specify your configurations as a dict
v4.3.0,generate feature names
v4.3.0,feature_name and categorical_feature
v4.3.0,check feature name
v4.3.0,save model to file
v4.3.0,dump model to JSON (and save to file)
v4.3.0,feature names
v4.3.0,feature importances
v4.3.0,load model to predict
v4.3.0,can only predict with the best iteration (or the saving iteration)
v4.3.0,eval with loaded model
v4.3.0,dump model with pickle
v4.3.0,load model with pickle to predict
v4.3.0,can predict with any iteration when loaded in pickle way
v4.3.0,eval with loaded model
v4.3.0,continue training
v4.3.0,init_model accepts:
v4.3.0,1. model file name
v4.3.0,2. Booster()
v4.3.0,decay learning rates
v4.3.0,reset_parameter callback accepts:
v4.3.0,1. list with length = num_boost_round
v4.3.0,2. function(curr_iter)
v4.3.0,change other parameters during training
v4.3.0,self-defined objective function
v4.3.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v4.3.0,log likelihood loss
v4.3.0,self-defined eval metric
v4.3.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.3.0,binary error
v4.3.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.3.0,This may make built-in evaluation metric calculate wrong results
v4.3.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.3.0,Keep this in mind when you use the customization
v4.3.0,Pass custom objective function through params
v4.3.0,another self-defined eval metric
v4.3.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.3.0,accuracy
v4.3.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.3.0,This may make built-in evaluation metric calculate wrong results
v4.3.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.3.0,Keep this in mind when you use the customization
v4.3.0,Pass custom objective function through params
v4.3.0,callback
v4.3.0,coding: utf-8
v4.3.0,load or create your dataset
v4.3.0,train
v4.3.0,predict
v4.3.0,eval
v4.3.0,feature importances
v4.3.0,self-defined eval metric
v4.3.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.3.0,Root Mean Squared Logarithmic Error (RMSLE)
v4.3.0,train
v4.3.0,another self-defined eval metric
v4.3.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.3.0,Relative Absolute Error (RAE)
v4.3.0,train
v4.3.0,predict
v4.3.0,eval
v4.3.0,other scikit-learn modules
v4.3.0,coding: utf-8
v4.3.0,load or create your dataset
v4.3.0,create dataset for lightgbm
v4.3.0,specify your configurations as a dict
v4.3.0,train
v4.3.0,coding: utf-8
v4.3.0,################
v4.3.0,Simulate some binary data with a single categorical and
v4.3.0,single continuous predictor
v4.3.0,################
v4.3.0,Set up a couple of utilities for our experiments
v4.3.0,################
v4.3.0,Observe the behavior of `binary` and `xentropy` objectives
v4.3.0,Trying this throws an error on non-binary values of y:
v4.3.0,"experiment('binary', label_type='probability', DATA)"
v4.3.0,The speed of `binary` is not drastically different than
v4.3.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v4.3.0,there are reasons to suspect that `binary` should run faster when the
v4.3.0,label is an integer instead of a float
v4.3.0,coding: utf-8
v4.3.0,load or create your dataset
v4.3.0,create dataset for lightgbm
v4.3.0,specify your configurations as a dict
v4.3.0,train
v4.3.0,save model to file
v4.3.0,predict
v4.3.0,eval
v4.3.0,We can also open HDF5 file once and get access to
v4.3.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v4.3.0,
v4.3.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v4.3.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v4.3.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v4.3.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v4.3.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v4.3.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v4.3.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v4.3.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v4.3.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v4.3.0,Here we are just creating a chunk size that matches with batch_size.
v4.3.0,
v4.3.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v4.3.0,lightgbm Dataset.
v4.3.0,Save to 2 HDF5 files for demonstration.
v4.3.0,We can store multiple datasets inside a single HDF5 file.
v4.3.0,Separating X and Y for choosing best chunk size for data loading.
v4.3.0,split training data into two partitions
v4.3.0,make this array dense because we're splitting across
v4.3.0,a sparse boundary to partition the data
v4.3.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v4.3.0,predictions and target values back from workers to the client
v4.3.0,
v4.3.0,"for larger datasets, consider the metrics from dask-ml instead"
v4.3.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v4.3.0,coding: utf-8
v4.3.0,!/usr/bin/env python3
v4.3.0,-*- coding: utf-8 -*-
v4.3.0,
v4.3.0,"LightGBM documentation build configuration file, created by"
v4.3.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v4.3.0,
v4.3.0,This file is execfile()d with the current directory set to its
v4.3.0,containing dir.
v4.3.0,
v4.3.0,Note that not all possible configuration values are present in this
v4.3.0,autogenerated file.
v4.3.0,
v4.3.0,All configuration values have a default; values that are commented out
v4.3.0,serve to show the default.
v4.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
v4.3.0,add these directories to sys.path here. If the directory is relative to the
v4.3.0,"documentation root, use os.path.abspath to make it absolute."
v4.3.0,-- General configuration ------------------------------------------------
v4.3.0,"If your documentation needs a minimal Sphinx version, state it here."
v4.3.0,"Add any Sphinx extension module names here, as strings. They can be"
v4.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v4.3.0,ones.
v4.3.0,mock out modules
v4.3.0,hide type hints in API docs
v4.3.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v4.3.0,Only the class' docstring is inserted.
v4.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v4.3.0,The master toctree document.
v4.3.0,General information about the project.
v4.3.0,The name of an image file (relative to this directory) to place at the top
v4.3.0,of the sidebar.
v4.3.0,The name of an image file (relative to this directory) to use as a favicon of
v4.3.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v4.3.0,pixels large.
v4.3.0,"The version info for the project you're documenting, acts as replacement for"
v4.3.0,"|version| and |release|, also used in various other places throughout the"
v4.3.0,built documents.
v4.3.0,The short X.Y version.
v4.3.0,"The full version, including alpha/beta/rc tags."
v4.3.0,The language for content autogenerated by Sphinx. Refer to documentation
v4.3.0,for a list of supported languages.
v4.3.0,
v4.3.0,This is also used if you do content translation via gettext catalogs.
v4.3.0,"Usually you set ""language"" from the command line for these cases."
v4.3.0,"List of patterns, relative to source directory, that match files and"
v4.3.0,directories to ignore when looking for source files.
v4.3.0,This patterns also effect to html_static_path and html_extra_path
v4.3.0,The name of the Pygments (syntax highlighting) style to use.
v4.3.0,-- Configuration for C API docs generation ------------------------------
v4.3.0,-- Options for HTML output ----------------------------------------------
v4.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v4.3.0,a list of builtin themes.
v4.3.0,Theme options are theme-specific and customize the look and feel of a theme
v4.3.0,"further.  For a list of options available for each theme, see the"
v4.3.0,documentation.
v4.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
v4.3.0,"relative to this directory. They are copied after the builtin static files,"
v4.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v4.3.0,-- Options for HTMLHelp output ------------------------------------------
v4.3.0,Output file base name for HTML help builder.
v4.3.0,-- Options for LaTeX output ---------------------------------------------
v4.3.0,The name of an image file (relative to this directory) to place at the top of
v4.3.0,the title page.
v4.3.0,intersphinx configuration
v4.3.0,Warning! The following code can cause buffer overflows on RTD.
v4.3.0,Consider suppressing output completely if RTD project silently fails.
v4.3.0,Refer to https://github.com/svenevs/exhale
v4.3.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.3.0,Warning! The following code can cause buffer overflows on RTD.
v4.3.0,Consider suppressing output completely if RTD project silently fails.
v4.3.0,Refer to https://github.com/svenevs/exhale
v4.3.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.3.0,coding: utf-8
v4.3.0,This is a basic test for floating number parsing.
v4.3.0,Most of the test cases come from:
v4.3.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v4.3.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v4.3.0,FLT_MAX
v4.3.0,FLT_MIN
v4.3.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v4.3.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v4.3.0,Near DBL_MIN
v4.3.0,DBL_MIN 2^-1022
v4.3.0,The behavior for parsing -nan depends on implementation.
v4.3.0,Thus we skip binary check for negative nan.
v4.3.0,See comment in test_cases.
v4.3.0,construct sample data first (use all data for convenience and since size is small)
v4.3.0,Load some test data
v4.3.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.3.0,Add some fake initial_scores and groups so we can test streaming them
v4.3.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.3.0,Load some test data
v4.3.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.3.0,Add some fake initial_scores and groups so we can test streaming them
v4.3.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.3.0,NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests
v4.3.0,1) Create validity bitmap
v4.3.0,2) Create buffers
v4.3.0,Create arrow array
v4.3.0,Arithmetic
v4.3.0,Subscripts
v4.3.0,End
v4.3.0,Load some test data
v4.3.0,Serialize the reference
v4.3.0,Deserialize the reference
v4.3.0,Confirm 1 successful API call
v4.3.0,Free memory
v4.3.0,Test that Data() points to first value written
v4.3.0,Constants
v4.3.0,Start with some content:
v4.3.0,Clear & re-use:
v4.3.0,Output should match new content:
v4.3.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v4.3.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v4.3.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v4.3.0,Compute a new trial address & value & if it is a valid address:
v4.3.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v4.3.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v4.3.0,Check the just-stored value with getitem():
v4.3.0,Also store the just-stored value for future tracking:
v4.3.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v4.3.0,Test in 2 ways that the values are correctly laid out in memory:
v4.3.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.3.0,Use multiple threads to test concurrency
v4.3.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.3.0,Calculate expected boundaries
v4.3.0,Extract a set of rows from the column-based format (still maintaining column based format)
v4.3.0,coding: utf-8
v4.3.0,"at initialization, should be -1"
v4.3.0,updating that value through the C API should work
v4.3.0,resetting to any negative number should set it to -1
v4.3.0,coding: utf-8
v4.3.0,check saved model persistence
v4.3.0,"we need to check the consistency of model file here, so test for exact equal"
v4.3.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v4.3.0,"scores likely to be different, but prediction should still be the same"
v4.3.0,test that shape is checked during prediction
v4.3.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v4.3.0,The following is for demo and testing purpose.
v4.3.0,whole col
v4.3.0,half col
v4.3.0,Create dataset from numpy array directly.
v4.3.0,Create dataset using Sequence.
v4.3.0,Test for validation set.
v4.3.0,Select some random rows as valid data.
v4.3.0,"From Dataset constructor, with dataset from numpy array."
v4.3.0,"From Dataset.create_valid, with dataset from sequence."
v4.3.0,test that method works even with free_raw_data=True
v4.3.0,test that method works but sets raw data to None in case of immergeable data types
v4.3.0,test that method works for different data types
v4.3.0,"Set extremely harsh penalties, so CEGB will block most splits."
v4.3.0,"Compare pairs of penalties, to ensure scaling works as intended"
v4.3.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v4.3.0,"unconstructed, get_* methods should return whatever was provided"
v4.3.0,"before construction, get_field() should raise an exception"
v4.3.0,"constructed, get_* methods should return numpy arrays, even when the provided"
v4.3.0,input was a list of floats or ints
v4.3.0,"get_field(""group"") returns a numpy array with boundaries, instead of size"
v4.3.0,"NOTE: ""position"" is converted to int32 on the C++ side"
v4.3.0,"should resolve duplicate aliases, and prefer the main parameter"
v4.3.0,should choose the highest priority alias and set that value on main param
v4.3.0,if only aliases are used
v4.3.0,should use the default if main param and aliases are missing
v4.3.0,all changes should be made on copies and not modify the original
v4.3.0,preserves None found for main param and still removes aliases
v4.3.0,correctly chooses value when only an alias is provided
v4.3.0,adds None if that's given as the default and param not found
v4.3.0,If callable is found in objective
v4.3.0,Value in params should be preferred to the default_value passed from keyword arguments
v4.3.0,"None of objective or its aliases in params, but default_value is callable."
v4.3.0,check that the original data wasn't modified
v4.3.0,check that the built data has the codes
v4.3.0,if all categories were seen during training we just take the codes
v4.3.0,if we only saw 'a' during training we just replace its code
v4.3.0,and leave the rest as nan
v4.3.0,test using defined feature names
v4.3.0,test using default feature names
v4.3.0,check for feature indices outside of range
v4.3.0,coding: utf-8
v4.3.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v4.3.0,set_index ensures partitions are based on group id.
v4.3.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v4.3.0,"separate target, weight from features."
v4.3.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v4.3.0,"so that within each partition, sum(g) = n_samples."
v4.3.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v4.3.0,make one categorical feature relevant to the target
v4.3.0,https://github.com/microsoft/LightGBM/issues/4118
v4.3.0,extra predict() parameters should be passed through correctly
v4.3.0,pref_leaf values should have the right shape
v4.3.0,and values that look like valid tree nodes
v4.3.0,"be sure LightGBM actually used at least one categorical column,"
v4.3.0,and that it was correctly treated as a categorical feature
v4.3.0,shape depends on whether it is binary or multiclass classification
v4.3.0,"in the special case of multi-class classification using scipy sparse matrices,"
v4.3.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v4.3.0,
v4.3.0,"since that case is so different than all other cases, check the relevant things here"
v4.3.0,and then return early
v4.3.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v4.3.0,"be sure LightGBM actually used at least one categorical column,"
v4.3.0,and that it was correctly treated as a categorical feature
v4.3.0,* shape depends on whether it is binary or multiclass classification
v4.3.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v4.3.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v4.3.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v4.3.0,that the output has the right shape and base values are in the right position
v4.3.0,"with a custom objective, prediction result is a raw score instead of predicted class"
v4.3.0,function should have been preserved
v4.3.0,should correctly classify every sample
v4.3.0,probability estimates should be similar
v4.3.0,Scores should be the same
v4.3.0,Predictions should be roughly the same.
v4.3.0,pref_leaf values should have the right shape
v4.3.0,and values that look like valid tree nodes
v4.3.0,extra predict() parameters should be passed through correctly
v4.3.0,"be sure LightGBM actually used at least one categorical column,"
v4.3.0,and that it was correctly treated as a categorical feature
v4.3.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v4.3.0,that the output has the right shape and base values are in the right position
v4.3.0,"be sure LightGBM actually used at least one categorical column,"
v4.3.0,and that it was correctly treated as a categorical feature
v4.3.0,Quantiles should be right
v4.3.0,"be sure LightGBM actually used at least one categorical column,"
v4.3.0,and that it was correctly treated as a categorical feature
v4.3.0,function should have been preserved
v4.3.0,Scores should be the same
v4.3.0,local and Dask predictions should be the same
v4.3.0,predictions should be better than random
v4.3.0,rebalance small dask.Array dataset for better performance.
v4.3.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v4.3.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v4.3.0,distributed ranker should be able to rank decently well and should
v4.3.0,have high rank correlation with scores from serial ranker.
v4.3.0,extra predict() parameters should be passed through correctly
v4.3.0,pref_leaf values should have the right shape
v4.3.0,and values that look like valid tree nodes
v4.3.0,"be sure LightGBM actually used at least one categorical column,"
v4.3.0,and that it was correctly treated as a categorical feature
v4.3.0,rebalance small dask.Array dataset for better performance.
v4.3.0,distributed ranker should be able to rank decently well with the least-squares objective
v4.3.0,and should have high rank correlation with scores from serial ranker.
v4.3.0,function should have been preserved
v4.3.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v4.3.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v4.3.0,"test eval_class_weight, eval_init_score on binary-classification task."
v4.3.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v4.3.0,create eval_sets by creating new datasets or copying training data.
v4.3.0,total number of trees scales up for ova classifier.
v4.3.0,check that early stopping was not applied.
v4.3.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v4.3.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v4.3.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v4.3.0,should be able to use the class without specifying a client
v4.3.0,should be able to set client after construction
v4.3.0,data on cluster1
v4.3.0,create identical data on cluster2
v4.3.0,"at this point, the result of default_client() is client2 since it was the most recently"
v4.3.0,created. So setting client to client1 here to test that you can select a non-default client
v4.3.0,"unfitted model should survive pickling round trip, and pickling"
v4.3.0,shouldn't have side effects on the model object
v4.3.0,client will always be None after unpickling
v4.3.0,"fitted model should survive pickling round trip, and pickling"
v4.3.0,shouldn't have side effects on the model object
v4.3.0,client will always be None after unpickling
v4.3.0,rebalance data to be sure that each worker has a piece of the data
v4.3.0,model 1 - no network parameters given
v4.3.0,model 2 - machines given
v4.3.0,model 3 - local_listen_port given
v4.3.0,training should fail because LightGBM will try to use the same
v4.3.0,port for multiple worker processes on the same machine
v4.3.0,rebalance data to be sure that each worker has a piece of the data
v4.3.0,"test that ""machines"" is actually respected by creating a socket that uses"
v4.3.0,"one of the ports mentioned in ""machines"""
v4.3.0,The above error leaves a worker waiting
v4.3.0,"an informative error should be raised if ""machines"" has duplicates"
v4.3.0,"""client"" should be the only different, and the final argument"
v4.3.0,value of the root node is 0 when init_score is set
v4.3.0,this test is separate because it takes a not-yet-constructed estimator
v4.3.0,coding: utf-8
v4.3.0,coding: utf-8
v4.3.0,"build target, group ID vectors."
v4.3.0,build y/target and group-id vectors with user-specified group sizes.
v4.3.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v4.3.0,groups should contain > 1 element for pairwise learning objective.
v4.3.0,"build feature data, X. Transform first few into informative features."
v4.3.0,"doing this here, at import time, to ensure it only runs once_per import"
v4.3.0,instead of once per assertion
v4.3.0,coding: utf-8
v4.3.0,----------------------------------------------------------------------------------------------- #
v4.3.0,UTILITIES                                            #
v4.3.0,----------------------------------------------------------------------------------------------- #
v4.3.0,Set random nulls
v4.3.0,Split data into <=2 random chunks
v4.3.0,Turn chunks into array
v4.3.0,----------------------------------------------------------------------------------------------- #
v4.3.0,UNIT TESTS                                           #
v4.3.0,----------------------------------------------------------------------------------------------- #
v4.3.0,------------------------------------------- DATASET ------------------------------------------- #
v4.3.0,-------------------------------------------- FIELDS ------------------------------------------- #
v4.3.0,Check for equality
v4.3.0,-------------------------------------------- LABELS ------------------------------------------- #
v4.3.0,------------------------------------------- WEIGHTS ------------------------------------------- #
v4.3.0,-------------------------------------------- GROUPS ------------------------------------------- #
v4.3.0,----------------------------------------- INIT SCORES ----------------------------------------- #
v4.3.0,------------------------------------------ PREDICTION ----------------------------------------- #
v4.3.0,coding: utf-8
v4.3.0,check that really dummy objective was used and estimator didn't learn anything
v4.3.0,prediction result is actually not transformed (is raw) due to custom objective
v4.3.0,original estimator is unaffected
v4.3.0,"new estimator is unfitted, but has the same parameters"
v4.3.0,Test if random_state is properly stored
v4.3.0,Test if two random states produce identical models
v4.3.0,Test if subsequent fits sample from random_state object and produce different models
v4.3.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v4.3.0,With default params
v4.3.0,Tests same probabilities
v4.3.0,Tests same predictions
v4.3.0,Tests same raw scores
v4.3.0,Tests same leaf indices
v4.3.0,Tests same feature contributions
v4.3.0,Tests other parameters for the prediction works
v4.3.0,Tests start_iteration
v4.3.0,"Tests same probabilities, starting from iteration 10"
v4.3.0,"Tests same predictions, starting from iteration 10"
v4.3.0,"Tests same raw scores, starting from iteration 10"
v4.3.0,"Tests same leaf indices, starting from iteration 10"
v4.3.0,"Tests same feature contributions, starting from iteration 10"
v4.3.0,"Tests other parameters for the prediction works, starting from iteration 10"
v4.3.0,test that params passed in predict have higher priority
v4.3.0,"no custom objective, no custom metric"
v4.3.0,default metric
v4.3.0,non-default metric
v4.3.0,no metric
v4.3.0,non-default metric in eval_metric
v4.3.0,non-default metric with non-default metric in eval_metric
v4.3.0,non-default metric with multiple metrics in eval_metric
v4.3.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v4.3.0,default metric for non-default objective
v4.3.0,non-default metric for non-default objective
v4.3.0,no metric
v4.3.0,non-default metric in eval_metric for non-default objective
v4.3.0,non-default metric with non-default metric in eval_metric for non-default objective
v4.3.0,non-default metric with multiple metrics in eval_metric for non-default objective
v4.3.0,"custom objective, no custom metric"
v4.3.0,default regression metric for custom objective
v4.3.0,non-default regression metric for custom objective
v4.3.0,multiple regression metrics for custom objective
v4.3.0,no metric
v4.3.0,default regression metric with non-default metric in eval_metric for custom objective
v4.3.0,non-default regression metric with metric in eval_metric for custom objective
v4.3.0,multiple regression metrics with metric in eval_metric for custom objective
v4.3.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v4.3.0,"no custom objective, custom metric"
v4.3.0,default metric with custom metric
v4.3.0,non-default metric with custom metric
v4.3.0,multiple metrics with custom metric
v4.3.0,custom metric (disable default metric)
v4.3.0,default metric for non-default objective with custom metric
v4.3.0,non-default metric for non-default objective with custom metric
v4.3.0,multiple metrics for non-default objective with custom metric
v4.3.0,custom metric (disable default metric for non-default objective)
v4.3.0,"custom objective, custom metric"
v4.3.0,custom metric for custom objective
v4.3.0,non-default regression metric with custom metric for custom objective
v4.3.0,multiple regression metrics with custom metric for custom objective
v4.3.0,default metric and invalid binary metric is replaced with multiclass alternative
v4.3.0,invalid binary metric is replaced with multiclass alternative
v4.3.0,default metric for non-default multiclass objective
v4.3.0,and invalid binary metric is replaced with multiclass alternative
v4.3.0,default metric and invalid multiclass metric is replaced with binary alternative
v4.3.0,invalid multiclass metric is replaced with binary alternative for custom objective
v4.3.0,"Verify that can receive a list of metrics, only callable"
v4.3.0,Verify that can receive a list of custom and built-in metrics
v4.3.0,Verify that works as expected when eval_metric is empty
v4.3.0,"Verify that can receive a list of metrics, only built-in"
v4.3.0,Verify that eval_metric is robust to receiving a list with None
v4.3.0,feval
v4.3.0,single eval_set
v4.3.0,two eval_set
v4.3.0,'val_minus_two' here is the expected number of threads for n_jobs=-2
v4.3.0,"Note: according to joblib's formula, a value of n_jobs=-2 means"
v4.3.0,"""use all but one thread"" (formula: n_cpus + 1 + n_jobs)"
v4.3.0,try to predict with a different feature
v4.3.0,check that disabling the check doesn't raise the error
v4.3.0,"make weights and init_score same types as y, just to avoid"
v4.3.0,a huge number of combinations and therefore test cases
v4.3.0,"make weights and init_score same types as y, just to avoid"
v4.3.0,a huge number of combinations and therefore test cases
v4.3.0,coding: utf-8
v4.3.0,we're in a leaf now
v4.3.0,check that the rest of the elements have black color
v4.3.0,check that we got to the expected leaf
v4.3.0,coding: utf-8
v4.3.0,coding: utf-8
v4.3.0,check that default gives same result as k = 1
v4.3.0,check against independent calculation for k = 1
v4.3.0,check against independent calculation for k = 2
v4.3.0,check against independent calculation for k = 10
v4.3.0,check cases where predictions are equal
v4.3.0,should give same result as binary auc for 2 classes
v4.3.0,test the case where all predictions are equal
v4.3.0,test that weighted data gives different auc_mu
v4.3.0,test that equal data weights give same auc_mu as unweighted data
v4.3.0,should give 1 when accuracy = 1
v4.3.0,test loading class weights
v4.3.0,Simulates position bias for a given ranking dataset.
v4.3.0,The ouput dataset is identical to the input one with the exception for the relevance labels.
v4.3.0,The new labels are generated according to an instance of a cascade user model:
v4.3.0,"for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker"
v4.3.0,"(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)"
v4.3.0,and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;
v4.3.0,"at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,"
v4.3.0,new_label=0. Thus the generated new labels are biased towards the baseline ranker.
v4.3.0,"The positions of the documents in the ranked lists produced by the baseline, are returned."
v4.3.0,a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it
v4.3.0,an instantiation of a cascade model where the user stops with probability 0.2 after observing each document
v4.3.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.3.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.3.0,add extra row to position file
v4.3.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.3.0,test setting positions through Dataset constructor with numpy array
v4.3.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.3.0,test setting positions through Dataset constructor with pandas Series
v4.3.0,test setting positions through set_position
v4.3.0,test get_position works
v4.3.0,no early stopping
v4.3.0,early stopping occurs
v4.3.0,regular early stopping
v4.3.0,positive min_delta
v4.3.0,test custom eval metrics
v4.3.0,"shuffle = False, override metric in params"
v4.3.0,"shuffle = True, callbacks"
v4.3.0,enable display training loss
v4.3.0,self defined folds
v4.3.0,LambdaRank
v4.3.0,... with l2 metric
v4.3.0,... with NDCG (default) metric
v4.3.0,self defined folds with lambdarank
v4.3.0,init_model from an in-memory Booster
v4.3.0,init_model from a text file
v4.3.0,predictions should be identical
v4.3.0,with early stopping
v4.3.0,predict by each fold booster
v4.3.0,check that each booster predicted using the best iteration
v4.3.0,fold averaging
v4.3.0,without early stopping
v4.3.0,test feature_names with whitespaces
v4.3.0,This has non-ascii strings.
v4.3.0,check that passing parameters to the constructor raises warning and ignores them
v4.3.0,check inference isn't affected by unknown parameter
v4.3.0,entries whose values should reflect params passed to lgb.train()
v4.3.0,'l1' was passed in with alias 'mae'
v4.3.0,NOTE: this was passed in with alias 'sub_row'
v4.3.0,entries with default values of params
v4.3.0,add device-specific entries
v4.3.0,
v4.3.0,passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...
v4.3.0,https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377
v4.3.0,check that model text has all expected param entries
v4.3.0,"since Booster.model_to_string() is used when pickling, check that parameters all"
v4.3.0,roundtrip pickling successfully too
v4.3.0,take subsets and train
v4.3.0,generate CSR sparse dataset
v4.3.0,convert data to dense and get back same contribs
v4.3.0,validate the values are the same
v4.3.0,validate using CSC matrix
v4.3.0,validate the values are the same
v4.3.0,generate CSR sparse dataset
v4.3.0,convert data to dense and get back same contribs
v4.3.0,validate the values are the same
v4.3.0,validate using CSC matrix
v4.3.0,validate the values are the same
v4.3.0,Note there is an extra column added to the output for the expected value
v4.3.0,Note output CSC shape should be same as CSR output shape
v4.3.0,test sliced labels
v4.3.0,append some columns
v4.3.0,append some rows
v4.3.0,test sliced 2d matrix
v4.3.0,test sliced CSR
v4.3.0,trees start at position 1.
v4.3.0,split_features are in 4th line.
v4.3.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v4.3.0,The penalization is so high that the first 2 features should not be used here
v4.3.0,Check that a very high penalization is the same as not using the features at all
v4.3.0,check refit accepts dataset_params
v4.3.0,the following checks that dart and rf with mape can predict outside the 0-1 range
v4.3.0,https://github.com/microsoft/LightGBM/issues/1579
v4.3.0,"no custom objective, no feval"
v4.3.0,default metric
v4.3.0,non-default metric in params
v4.3.0,default metric in args
v4.3.0,non-default metric in args
v4.3.0,metric in args overwrites one in params
v4.3.0,metric in args overwrites one in params
v4.3.0,multiple metrics in params
v4.3.0,multiple metrics in args
v4.3.0,remove default metric by 'None' in list
v4.3.0,remove default metric by 'None' aliases
v4.3.0,"custom objective, no feval"
v4.3.0,no default metric
v4.3.0,metric in params
v4.3.0,metric in args
v4.3.0,metric in args overwrites its' alias in params
v4.3.0,multiple metrics in params
v4.3.0,multiple metrics in args
v4.3.0,"no custom objective, feval"
v4.3.0,default metric with custom one
v4.3.0,non-default metric in params with custom one
v4.3.0,default metric in args with custom one
v4.3.0,non-default metric in args with custom one
v4.3.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.3.0,multiple metrics in params with custom one
v4.3.0,multiple metrics in args with custom one
v4.3.0,custom metric is evaluated despite 'None' is passed
v4.3.0,"custom objective, feval"
v4.3.0,"no default metric, only custom one"
v4.3.0,metric in params with custom one
v4.3.0,metric in args with custom one
v4.3.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.3.0,multiple metrics in params with custom one
v4.3.0,multiple metrics in args with custom one
v4.3.0,custom metric is evaluated despite 'None' is passed
v4.3.0,"no custom objective, no feval"
v4.3.0,default metric
v4.3.0,default metric in params
v4.3.0,non-default metric in params
v4.3.0,multiple metrics in params
v4.3.0,remove default metric by 'None' aliases
v4.3.0,"custom objective, no feval"
v4.3.0,no default metric
v4.3.0,metric in params
v4.3.0,multiple metrics in params
v4.3.0,"no custom objective, feval"
v4.3.0,default metric with custom one
v4.3.0,default metric in params with custom one
v4.3.0,non-default metric in params with custom one
v4.3.0,multiple metrics in params with custom one
v4.3.0,custom metric is evaluated despite 'None' is passed
v4.3.0,"custom objective, feval"
v4.3.0,"no default metric, only custom one"
v4.3.0,metric in params with custom one
v4.3.0,multiple metrics in params with custom one
v4.3.0,custom metric is evaluated despite 'None' is passed
v4.3.0,Custom objective replaces multiclass
v4.3.0,multiclass default metric
v4.3.0,multiclass default metric with custom one
v4.3.0,multiclass metric alias with custom one for custom objective
v4.3.0,no metric for invalid class_num
v4.3.0,custom metric for invalid class_num
v4.3.0,multiclass metric alias with custom one with invalid class_num
v4.3.0,multiclass default metric without num_class
v4.3.0,multiclass metric alias
v4.3.0,multiclass metric
v4.3.0,non-valid metric for multiclass objective
v4.3.0,non-default num_class for default objective
v4.3.0,no metric with non-default num_class for custom objective
v4.3.0,multiclass metric alias for custom objective
v4.3.0,multiclass metric for custom objective
v4.3.0,binary metric with non-default num_class for custom objective
v4.3.0,Expect three metrics but mean and stdv for each metric
v4.3.0,test XGBoost-style return value
v4.3.0,test numpy-style return value
v4.3.0,test bins string type
v4.3.0,test histogram is disabled for categorical features
v4.3.0,test for lgb.train
v4.3.0,test feval for lgb.train
v4.3.0,test with two valid data for lgb.train
v4.3.0,test for lgb.cv
v4.3.0,test feval for lgb.cv
v4.3.0,test that binning works properly for features with only positive or only negative values
v4.3.0,decreasing without freeing raw data is allowed
v4.3.0,decreasing before lazy init is allowed
v4.3.0,increasing is allowed
v4.3.0,decreasing with disabled filter is allowed
v4.3.0,decreasing with enabled filter is disallowed;
v4.3.0,also changes of other params are disallowed
v4.3.0,check extra trees increases regularization
v4.3.0,check path smoothing increases regularization
v4.3.0,test edge case with one leaf
v4.3.0,check that constraint containing all features is equivalent to no constraint
v4.3.0,check that constraint partitioning the features reduces train accuracy
v4.3.0,check that constraints consisting of single features reduce accuracy further
v4.3.0,test that interaction constraints work when not all features are used
v4.3.0,check that number of threads does not affect result
v4.3.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v4.3.0,test again with nans in data
v4.3.0,test again with bagging
v4.3.0,test with a feature that has only one non-nan value
v4.3.0,test with a categorical feature
v4.3.0,test refit: same results on same data
v4.3.0,test refit with save and load
v4.3.0,test refit: different results training on different data
v4.3.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v4.3.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v4.3.0,"test the case where start_iteration <= 0, and num_iteration is None"
v4.3.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v4.3.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v4.3.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v4.3.0,test for regression
v4.3.0,test both with and without early stopping
v4.3.0,test for multi-class
v4.3.0,test both with and without early stopping
v4.3.0,test for binary
v4.3.0,test both with and without early stopping
v4.3.0,test against sklearn average precision metric
v4.3.0,test that average precision is 1 where model predicts perfectly
v4.3.0,data as float64
v4.3.0,test all features were used
v4.3.0,test the score is better than predicting the mean
v4.3.0,test all predictions are equal using different input dtypes
v4.3.0,introduce some missing values
v4.3.0,the previous line turns x3 into object dtype in recent versions of pandas
v4.3.0,train with regular dtypes
v4.3.0,convert to nullable dtypes
v4.3.0,test training succeeds
v4.3.0,test all features were used
v4.3.0,test the score is better than predicting the mean
v4.3.0,test equal predictions
v4.3.0,test data are taken from bug report
v4.3.0,https://github.com/microsoft/LightGBM/issues/4708
v4.3.0,modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811
v4.3.0,and https://github.com/microsoft/LightGBM/pull/5087
v4.3.0,test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree
v4.3.0,which is done in the fix #5164
v4.3.0,without the fix:
v4.3.0,Check failed: (best_split_info.left_count) > (0)
v4.3.0,try to predict with a different feature
v4.3.0,check that disabling the check doesn't raise the error
v4.3.0,try to refit with a different feature
v4.3.0,check that disabling the check doesn't raise the error
v4.3.0,coding: utf-8
v4.3.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v4.3.0,Double-precision floats are only supported on x86_64 with PoCL
v4.3.0,coding: utf-8
v4.3.0,coding: utf-8
v4.3.0,These are helper functions to allow doing a stack unwind
v4.3.0,"after an R allocation error, which would trigger a long jump."
v4.3.0,convert from one-based to zero-based index
v4.3.0,"if any feature names were larger than allocated size,"
v4.3.0,allow for a larger size and try again
v4.3.0,convert from boundaries to size
v4.3.0,--- start Booster interfaces
v4.3.0,"if any eval names were larger than allocated size,"
v4.3.0,allow for a larger size and try again
v4.3.0,"if the model string was larger than the initial buffer, call the function again, writing directly to the R object"
v4.3.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.3.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.3.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.3.0,.Call() calls
v4.3.0,coding: utf-8
v4.3.0,alias table
v4.3.0,names
v4.3.0,from strings
v4.3.0,tails
v4.3.0,tails
v4.3.0,the following are stored as comma separated strings but are arrays in the wrappers
v4.3.0,coding: utf-8
v4.3.0,Single row predictor to abstract away caching logic
v4.3.0,create boosting
v4.3.0,initialize the boosting
v4.3.0,create objective function
v4.3.0,initialize the objective function
v4.3.0,create training metric
v4.3.0,reset the boosting
v4.3.0,create objective function
v4.3.0,initialize the objective function
v4.3.0,calculate the nonzero data and indices size
v4.3.0,allocate data and indices arrays
v4.3.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.3.0,aggregated per row feature contribution results
v4.3.0,keep track of the row_vector sizes for parallelization
v4.3.0,copy vector results to output for each row
v4.3.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.3.0,aggregated per row feature contribution results
v4.3.0,calculate number of elements per column to construct
v4.3.0,the CSC matrix with random access
v4.3.0,keep track of column counts
v4.3.0,keep track of beginning index for each column
v4.3.0,keep track of beginning index for each matrix
v4.3.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v4.3.0,store the row index
v4.3.0,update column count
v4.3.0,explicitly declare symbols from LightGBM namespace
v4.3.0,some help functions used to convert data
v4.3.0,Row iterator of on column for CSC matrix
v4.3.0,"return value at idx, only can access by ascent order"
v4.3.0,"return next non-zero pair, if index < 0, means no more data"
v4.3.0,start of c_api functions
v4.3.0,This API is to keep python binding's behavior the same with C++ implementation.
v4.3.0,"Sample count, random seed etc. should be provided in parameters."
v4.3.0,convert internal thread id to be unique based on external thread id
v4.3.0,convert internal thread id to be unique based on external thread id
v4.3.0,sample data first
v4.3.0,sample data first
v4.3.0,sample data first
v4.3.0,local buffer to re-use memory
v4.3.0,sample data first
v4.3.0,no more data
v4.3.0,Prepare the Arrow data
v4.3.0,Initialize the dataset
v4.3.0,"If there is no reference dataset, we first sample indices"
v4.3.0,"Then, we obtain sample values by parallelizing across columns"
v4.3.0,Values need to be copied from the record batches.
v4.3.0,The chunks are iterated over in the inner loop as columns can be treated independently.
v4.3.0,"Finally, we initialize a loader from the sampled values"
v4.3.0,"After sampling and properly initializing all bins, we can add our data to the dataset. Here,"
v4.3.0,we parallelize across rows.
v4.3.0,---- start of booster
v4.3.0,Single row in row-major format:
v4.3.0,Apply the configuration
v4.3.0,Set up chunked array and iterators for all columns
v4.3.0,Build row function
v4.3.0,Run prediction
v4.3.0,---- start of some help functions
v4.3.0,data is array of pointers to individual rows
v4.3.0,set number of threads for openmp
v4.3.0,read parameters from config file
v4.3.0,"remove str after ""#"""
v4.3.0,de-duplicate params
v4.3.0,prediction is needed if using input initial model(continued train)
v4.3.0,need to continue training
v4.3.0,sync up random seed for data partition
v4.3.0,load Training data
v4.3.0,load data for distributed training
v4.3.0,load data for single machine
v4.3.0,need save binary file
v4.3.0,create training metric
v4.3.0,only when have metrics then need to construct validation data
v4.3.0,"Add validation data, if it exists"
v4.3.0,add
v4.3.0,need save binary file
v4.3.0,add metric for validation data
v4.3.0,output used time on each iteration
v4.3.0,need init network
v4.3.0,create boosting
v4.3.0,create objective function
v4.3.0,load training data
v4.3.0,initialize the objective function
v4.3.0,initialize the boosting
v4.3.0,add validation data into boosting
v4.3.0,convert model to if-else statement code
v4.3.0,create predictor
v4.3.0,Free memory
v4.3.0,create predictor
v4.3.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v4.3.0,counts for all labels
v4.3.0,"start from top label, and accumulate DCG"
v4.3.0,counts for all labels
v4.3.0,calculate k Max DCG by one pass
v4.3.0,get sorted indices by score
v4.3.0,calculate multi dcg by one pass
v4.3.0,wait for all client start up
v4.3.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v4.3.0,Instead we will handle finalize/abort for MPI in main().
v4.3.0,default set to -1
v4.3.0,"distance at k-th communication, distance[k] = 2^k"
v4.3.0,set incoming rank at k-th commuication
v4.3.0,set outgoing rank at k-th commuication
v4.3.0,default set as -1
v4.3.0,construct all recursive halving map for all machines
v4.3.0,let 1 << k <= num_machines
v4.3.0,distance of each communication
v4.3.0,"if num_machines = 2^k, don't need to group machines"
v4.3.0,"communication direction, %2 == 0 is positive"
v4.3.0,neighbor at k-th communication
v4.3.0,receive data block at k-th communication
v4.3.0,send data block at k-th communication
v4.3.0,"if num_machines != 2^k, need to group machines"
v4.3.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v4.3.0,let left machine as group leader
v4.3.0,"cache block information for groups, group with 2 machines will have double block size"
v4.3.0,convert from group to node leader
v4.3.0,convert from node to group
v4.3.0,meet new group
v4.3.0,add block len for this group
v4.3.0,calculate the group block start
v4.3.0,not need to construct
v4.3.0,get receive block information
v4.3.0,accumulate block len
v4.3.0,get send block information
v4.3.0,accumulate block len
v4.3.0,static member definition
v4.3.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v4.3.0,assign the blocks to every rank.
v4.3.0,do reduce scatter
v4.3.0,do all gather
v4.3.0,assign blocks
v4.3.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v4.3.0,copy back
v4.3.0,assign blocks
v4.3.0,start all gather
v4.3.0,when num_machines is small and data is large
v4.3.0,use output as receive buffer
v4.3.0,get current local block size
v4.3.0,get out rank
v4.3.0,get in rank
v4.3.0,get send information
v4.3.0,get recv information
v4.3.0,send and recv at same time
v4.3.0,rotate in-place
v4.3.0,use output as receive buffer
v4.3.0,get current local block size
v4.3.0,get send information
v4.3.0,get recv information
v4.3.0,send and recv at same time
v4.3.0,use output as receive buffer
v4.3.0,send and recv at same time
v4.3.0,send local data to neighbor first
v4.3.0,receive neighbor data first
v4.3.0,reduce
v4.3.0,get target
v4.3.0,get send information
v4.3.0,get recv information
v4.3.0,send and recv at same time
v4.3.0,reduce
v4.3.0,send result to neighbor
v4.3.0,receive result from neighbor
v4.3.0,copy result
v4.3.0,start up socket
v4.3.0,parse clients from file
v4.3.0,get ip list of local machine
v4.3.0,get local rank
v4.3.0,construct listener
v4.3.0,construct communication topo
v4.3.0,construct linkers
v4.3.0,free listener
v4.3.0,set timeout
v4.3.0,accept incoming socket
v4.3.0,receive rank
v4.3.0,add new socket
v4.3.0,save ranks that need to connect with
v4.3.0,start listener
v4.3.0,start connect
v4.3.0,let smaller rank connect to larger rank
v4.3.0,send local rank
v4.3.0,wait for listener
v4.3.0,print connected linkers
v4.3.0,only need to copy subset
v4.3.0,avoid to copy subset many times
v4.3.0,avoid out of range
v4.3.0,may need to recopy subset
v4.3.0,valid the type
v4.3.0,parser factory implementation.
v4.3.0,customized parser add-on.
v4.3.0,save header to parser config in case needed.
v4.3.0,save label id to parser config in case needed.
v4.3.0,Constructors
v4.3.0,Get type tag
v4.3.0,Comparisons
v4.3.0,"This has to be separate, not in Statics, because Json() accesses"
v4.3.0,statics().null.
v4.3.0,"advance until next line, or end of input"
v4.3.0,advance until closing tokens
v4.3.0,The usual case: non-escaped characters
v4.3.0,Handle escapes
v4.3.0,Extract 4-byte escape sequence
v4.3.0,Explicitly check length of the substring. The following loop
v4.3.0,relies on std::string returning the terminating NUL when
v4.3.0,accessing str[length]. Checking here reduces brittleness.
v4.3.0,JSON specifies that characters outside the BMP shall be encoded as a
v4.3.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v4.3.0,components. Check whether we're in the middle of such a beast: the
v4.3.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v4.3.0,a trail (low) surrogate.
v4.3.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v4.3.0,per the UTF-16 algorithm.
v4.3.0,Integer part
v4.3.0,Decimal part
v4.3.0,Exponent part
v4.3.0,Check for any trailing garbage
v4.3.0,Documented in json11.hpp
v4.3.0,Check for another object
v4.3.0,get column names
v4.3.0,"support to get header from parser config, so could utilize following label name to id mapping logic."
v4.3.0,load label idx first
v4.3.0,"if parser config file exists, feature names may be changed after customized parser applied."
v4.3.0,clear here so could use default filled feature names during dataset construction.
v4.3.0,may improve by saving real feature names defined in parser in the future.
v4.3.0,erase label column name
v4.3.0,load ignore columns
v4.3.0,load weight idx
v4.3.0,load group idx
v4.3.0,don't support query id in data file when using distributed training
v4.3.0,read data to memory
v4.3.0,sample data
v4.3.0,construct feature bin mappers & clear sample data
v4.3.0,initialize label
v4.3.0,extract features
v4.3.0,sample data from file
v4.3.0,construct feature bin mappers & clear sample data
v4.3.0,initialize label
v4.3.0,extract features
v4.3.0,load data from binary file
v4.3.0,checks whether there's a initial score file when loaded from binary data files
v4.3.0,"the intial score file should with suffix "".bin.init"""
v4.3.0,check meta data
v4.3.0,need to check training data
v4.3.0,read data in memory
v4.3.0,initialize label
v4.3.0,extract features
v4.3.0,Get number of lines of data file
v4.3.0,initialize label
v4.3.0,extract features
v4.3.0,load data from binary file
v4.3.0,checks whether there's a initial score file when loaded from binary data files
v4.3.0,"the intial score file should with suffix "".bin.init"""
v4.3.0,not need to check validation data
v4.3.0,check meta data
v4.3.0,check token
v4.3.0,read feature group definitions
v4.3.0,read feature size
v4.3.0,buffer to read binary file
v4.3.0,check token
v4.3.0,read size of header
v4.3.0,re-allocate space if not enough
v4.3.0,read header
v4.3.0,get header
v4.3.0,read size of meta data
v4.3.0,re-allocate space if not enough
v4.3.0,read meta data
v4.3.0,load meta data
v4.3.0,sample local used data if need to partition
v4.3.0,"if not contain query file, minimal sample unit is one record"
v4.3.0,"if contain query file, minimal sample unit is one query"
v4.3.0,if is new query
v4.3.0,read feature data
v4.3.0,read feature size
v4.3.0,re-allocate space if not enough
v4.3.0,raw data
v4.3.0,fill feature_names_ if not header
v4.3.0,get forced split
v4.3.0,"if only one machine, find bin locally"
v4.3.0,"if have multi-machines, need to find bin distributed"
v4.3.0,different machines will find bin for different features
v4.3.0,start and len will store the process feature indices for different machines
v4.3.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.3.0,free
v4.3.0,gather global feature bin mappers
v4.3.0,restore features bins from buffer
v4.3.0,---- private functions ----
v4.3.0,get header
v4.3.0,num_groups
v4.3.0,real_feature_idx_
v4.3.0,feature2group
v4.3.0,feature2subfeature
v4.3.0,group_bin_boundaries
v4.3.0,group_feature_start_
v4.3.0,group_feature_cnt_
v4.3.0,get feature names
v4.3.0,get forced_bin_bounds_
v4.3.0,"if features are ordered, not need to use hist_buf"
v4.3.0,read all lines
v4.3.0,get query data
v4.3.0,"if not contain query data, minimal sample unit is one record"
v4.3.0,"if contain query data, minimal sample unit is one query"
v4.3.0,if is new query
v4.3.0,get query data
v4.3.0,"if not contain query file, minimal sample unit is one record"
v4.3.0,"if contain query file, minimal sample unit is one query"
v4.3.0,if is new query
v4.3.0,parse features
v4.3.0,get forced split
v4.3.0,"check the range of label_idx, weight_idx and group_idx"
v4.3.0,"skip label check if user input parser config file,"
v4.3.0,because label id is got from raw features while dataset features are consistent with customized parser.
v4.3.0,fill feature_names_ if not header
v4.3.0,start find bins
v4.3.0,"if only one machine, find bin locally"
v4.3.0,start and len will store the process feature indices for different machines
v4.3.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.3.0,free
v4.3.0,gather global feature bin mappers
v4.3.0,restore features bins from buffer
v4.3.0,if doesn't need to prediction with initial model
v4.3.0,parser
v4.3.0,set label
v4.3.0,free processed line:
v4.3.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v4.3.0,text_reader_->Lines()[i].shrink_to_fit();
v4.3.0,push data
v4.3.0,if is used feature
v4.3.0,if need to prediction with initial model
v4.3.0,parser
v4.3.0,set initial score
v4.3.0,set label
v4.3.0,free processed line:
v4.3.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v4.3.0,text_reader_->Lines()[i].shrink_to_fit();
v4.3.0,push data
v4.3.0,if is used feature
v4.3.0,metadata_ will manage space of init_score
v4.3.0,text data can be free after loaded feature values
v4.3.0,parser
v4.3.0,set initial score
v4.3.0,set label
v4.3.0,push data
v4.3.0,if is used feature
v4.3.0,only need part of data
v4.3.0,need full data
v4.3.0,metadata_ will manage space of init_score
v4.3.0,read size of token
v4.3.0,remove duplicates
v4.3.0,deep copy function for BinMapper
v4.3.0,mean size for one bin
v4.3.0,need a new bin
v4.3.0,update bin upper bound
v4.3.0,last bin upper bound
v4.3.0,get number of positive and negative distinct values
v4.3.0,include zero bounds and infinity bound
v4.3.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v4.3.0,find remaining bounds
v4.3.0,find distinct_values first
v4.3.0,push zero in the front
v4.3.0,use the large value
v4.3.0,push zero in the back
v4.3.0,convert to int type first
v4.3.0,sort by counts in descending order
v4.3.0,will ignore the categorical of small counts
v4.3.0,Push the dummy bin for NaN
v4.3.0,Use MissingType::None to represent this bin contains all categoricals
v4.3.0,fix count of NaN bin
v4.3.0,check trivial(num_bin_ == 1) feature
v4.3.0,check useless bin
v4.3.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v4.3.0,so use most_freq_bin_ = default_bin_ when there is not so sparse
v4.3.0,calculate max bin of all features to select the int type in MultiValDenseBin
v4.3.0,"for lambdarank, it needs query data for partition data in distributed learning"
v4.3.0,need convert query_id to boundaries
v4.3.0,check weights
v4.3.0,check positions
v4.3.0,check query boundries
v4.3.0,contain initial score file
v4.3.0,check weights
v4.3.0,get local weights
v4.3.0,check positions
v4.3.0,get local positions
v4.3.0,check query boundries
v4.3.0,get local query boundaries
v4.3.0,contain initial score file
v4.3.0,get local initial scores
v4.3.0,re-calculate query weight
v4.3.0,Clear init scores on empty input
v4.3.0,"Note that len here is row count, not num_init_score, so we compare against num_data"
v4.3.0,"We need to use source_size here, because len might not equal size (due to a partially loaded dataset)"
v4.3.0,CUDA is handled after all insertions are complete
v4.3.0,CUDA is handled after all insertions are complete
v4.3.0,Clear weights on empty input
v4.3.0,CUDA is handled after all insertions are complete
v4.3.0,Clear query boundaries on empty input
v4.3.0,save to nullptr
v4.3.0,CUDA is handled after all insertions are complete
v4.3.0,default weight file name
v4.3.0,default position file name
v4.3.0,default init_score file name
v4.3.0,use first line to count number class
v4.3.0,default query file name
v4.3.0,root is in the depth 0
v4.3.0,non-leaf
v4.3.0,leaf
v4.3.0,use this for the missing value conversion
v4.3.0,Predict func by Map to ifelse
v4.3.0,use this for the missing value conversion
v4.3.0,non-leaf
v4.3.0,left subtree
v4.3.0,right subtree
v4.3.0,leaf
v4.3.0,non-leaf
v4.3.0,left subtree
v4.3.0,right subtree
v4.3.0,leaf
v4.3.0,recursive computation of SHAP values for a decision tree
v4.3.0,extend the unique path
v4.3.0,leaf node
v4.3.0,internal node
v4.3.0,"see if we have already split on this feature,"
v4.3.0,if so we undo that split so we can redo it for this node
v4.3.0,recursive sparse computation of SHAP values for a decision tree
v4.3.0,extend the unique path
v4.3.0,leaf node
v4.3.0,internal node
v4.3.0,"see if we have already split on this feature,"
v4.3.0,if so we undo that split so we can redo it for this node
v4.3.0,add names of objective function if not providing metric
v4.3.0,equal weights for all classes
v4.3.0,generate seeds by seed.
v4.3.0,sort eval_at
v4.3.0,Only push the non-training data
v4.3.0,check for conflicts
v4.3.0,"check if objective, metric, and num_class match"
v4.3.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v4.3.0,Check max_depth and num_leaves
v4.3.0,"Fits in an int, and is more restrictive than the current num_leaves"
v4.3.0,"force col-wise for gpu, and cuda version"
v4.3.0,force row-wise for cuda version
v4.3.0,linear tree learner must be serial type and run on CPU device
v4.3.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v4.3.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v4.3.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v4.3.0,calculated split gain can be positive even with zero gradient and hessian.
v4.3.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v4.3.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v4.3.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v4.3.0,first round: fill the single val group
v4.3.0,always push the last group
v4.3.0,put dense feature first
v4.3.0,sort by non zero cnt
v4.3.0,"sort by non zero cnt, bigger first"
v4.3.0,shuffle groups
v4.3.0,Using std::swap for vector<bool> will cause the wrong result.
v4.3.0,get num_features
v4.3.0,get bin_mappers
v4.3.0,"for sparse multi value bin, we store the feature bin values with offset added"
v4.3.0,"for dense multi value bin, the feature bin values without offsets are used"
v4.3.0,copy feature bin mapper data
v4.3.0,copy feature bin mapper data
v4.3.0,update CUDA storage for column data and metadata
v4.3.0,"if not pass a filename, just append "".bin"" of original file"
v4.3.0,Write the basic header information for the dataset
v4.3.0,get size of meta data
v4.3.0,write meta data
v4.3.0,write feature data
v4.3.0,get size of feature
v4.3.0,write feature
v4.3.0,write raw data; use row-major order so we can read row-by-row
v4.3.0,Calculate approximate size of output and reserve space
v4.3.0,write feature group definitions
v4.3.0,"Give a little extra just in case, to avoid unnecessary resizes"
v4.3.0,"Write token that marks the data as binary reference, and the version"
v4.3.0,Write the basic definition of the overall dataset
v4.3.0,write feature group definitions
v4.3.0,get size of feature
v4.3.0,write feature
v4.3.0,size of feature names and forced bins
v4.3.0,write header
v4.3.0,write feature names
v4.3.0,write forced bins
v4.3.0,"explicitly initialize template methods, for cross module call"
v4.3.0,"explicitly initialize template methods, for cross module call"
v4.3.0,"Only one multi-val group, just simply merge"
v4.3.0,Skip the leading 0 when copying group_bin_boundaries.
v4.3.0,regenerate other fields
v4.3.0,need to iterate bin iterator
v4.3.0,is dense column
v4.3.0,is sparse column
v4.3.0,initialize the subset cuda column data
v4.3.0,"if one column has too many bins, use a separate partition for that column"
v4.3.0,try if adding this column exceed the maximum number per partition
v4.3.0,"if one column has too many bins, use a separate partition for that column"
v4.3.0,try if adding this column exceed the maximum number per partition
v4.3.0,"if LightGBM-specific default has been set, ignore OpenMP-global config"
v4.3.0,"otherwise, default to OpenMP-global config"
v4.3.0,"ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't"
v4.3.0,use more than that many threads
v4.3.0,store the importance first
v4.3.0,PredictRaw
v4.3.0,PredictRawByMap
v4.3.0,Predict
v4.3.0,PredictByMap
v4.3.0,PredictLeafIndex
v4.3.0,PredictLeafIndexByMap
v4.3.0,output model type
v4.3.0,output number of class
v4.3.0,output label index
v4.3.0,output max_feature_idx
v4.3.0,output objective
v4.3.0,output tree models
v4.3.0,store the importance first
v4.3.0,sort the importance
v4.3.0,use serialized string to restore this object
v4.3.0,Use first 128 chars to avoid exceed the message buffer.
v4.3.0,get number of classes
v4.3.0,get index of label
v4.3.0,get max_feature_idx first
v4.3.0,get average_output
v4.3.0,get feature names
v4.3.0,get monotone_constraints
v4.3.0,set zero
v4.3.0,predict all the trees for one iteration
v4.3.0,check early stopping
v4.3.0,set zero
v4.3.0,predict all the trees for one iteration
v4.3.0,check early stopping
v4.3.0,margin_threshold will be captured by value
v4.3.0,copy and sort
v4.3.0,margin_threshold will be captured by value
v4.3.0,Fix for compiler warnings about reaching end of control
v4.3.0,load forced_splits file
v4.3.0,init tree learner
v4.3.0,push training metrics
v4.3.0,get max feature index
v4.3.0,get label index
v4.3.0,get feature names
v4.3.0,get parser config file content
v4.3.0,check that forced splits does not use feature indices larger than dataset size
v4.3.0,"if need bagging, create buffer"
v4.3.0,"for a validation dataset, we need its score and metric"
v4.3.0,update score
v4.3.0,objective function will calculate gradients and hessians
v4.3.0,output used time per iteration
v4.3.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v4.3.0,boosting first
v4.3.0,use customized objective function
v4.3.0,need to copy customized gradients when using GOSS
v4.3.0,bagging logic
v4.3.0,need to copy gradients for bagging subset.
v4.3.0,shrinkage by learning rate
v4.3.0,update score
v4.3.0,only add default score one-time
v4.3.0,updates scores
v4.3.0,add model
v4.3.0,reset score
v4.3.0,remove model
v4.3.0,print message for metric
v4.3.0,pop last early_stopping_round_ models
v4.3.0,update training score
v4.3.0,we need to predict out-of-bag scores of data for boosting
v4.3.0,update validation score
v4.3.0,print training metric
v4.3.0,print validation metric
v4.3.0,set zero
v4.3.0,predict all the trees for one iteration
v4.3.0,predict all the trees for one iteration
v4.3.0,push training metrics
v4.3.0,"not same training data, need reset score and others"
v4.3.0,create score tracker
v4.3.0,update score
v4.3.0,resize gradient vectors to copy the customized gradients for goss or bagging with subset
v4.3.0,load forced_splits file
v4.3.0,"if exists initial score, will start from it"
v4.3.0,clear host score buffer
v4.3.0,Get the max size of pool
v4.3.0,at least need 2 leaves
v4.3.0,push split information for all leaves
v4.3.0,initialize splits for leaf
v4.3.0,initialize data partition
v4.3.0,initialize ordered gradients and hessians
v4.3.0,cannot change is_hist_col_wise during training
v4.3.0,initialize splits for leaf
v4.3.0,initialize data partition
v4.3.0,initialize ordered gradients and hessians
v4.3.0,Get the max size of pool
v4.3.0,at least need 2 leaves
v4.3.0,push split information for all leaves
v4.3.0,some initial works before training
v4.3.0,root leaf
v4.3.0,only root leaf can be splitted on first time
v4.3.0,some initial works before finding best split
v4.3.0,find best threshold for every feature
v4.3.0,Get a leaf with max split gain
v4.3.0,Get split information for best leaf
v4.3.0,"cannot split, quit"
v4.3.0,split tree with best leaf
v4.3.0,reset histogram pool
v4.3.0,initialize data partition
v4.3.0,reset the splits for leaves
v4.3.0,Sumup for root
v4.3.0,use all data
v4.3.0,"use bagging, only use part of data"
v4.3.0,check depth of current leaf
v4.3.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v4.3.0,no enough data to continue
v4.3.0,only have root
v4.3.0,put parent(left) leaf's histograms into larger leaf's histograms
v4.3.0,put parent(left) leaf's histograms to larger leaf's histograms
v4.3.0,construct smaller leaf
v4.3.0,construct larger leaf
v4.3.0,find splits
v4.3.0,only has root leaf
v4.3.0,start at root leaf
v4.3.0,Histogram construction require parent features.
v4.3.0,"then, compute own splits"
v4.3.0,split info should exist because searching in bfs fashion - should have added from parent
v4.3.0,update before tree split
v4.3.0,don't need to update this in data-based parallel model
v4.3.0,"split tree, will return right leaf"
v4.3.0,store the true split gain in tree model
v4.3.0,don't need to update this in data-based parallel model
v4.3.0,store the true split gain in tree model
v4.3.0,init the leaves that used on next iteration
v4.3.0,update leave outputs if needed
v4.3.0,bag_mapper[index_mapper[i]]
v4.3.0,it is needed to filter the features after the above code.
v4.3.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v4.3.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v4.3.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v4.3.0,find splits
v4.3.0,identify features containing nans
v4.3.0,preallocate the matrix used to calculate linear model coefficients
v4.3.0,"store only upper triangular half of matrix as an array, in row-major order"
v4.3.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v4.3.0,we add another 8 to ensure cache lines are not shared among processors
v4.3.0,some initial works before training
v4.3.0,root leaf
v4.3.0,only root leaf can be splitted on first time
v4.3.0,some initial works before finding best split
v4.3.0,find best threshold for every feature
v4.3.0,Get a leaf with max split gain
v4.3.0,Get split information for best leaf
v4.3.0,"cannot split, quit"
v4.3.0,split tree with best leaf
v4.3.0,map data to leaf number
v4.3.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v4.3.0,the coefficients vector is given by
v4.3.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v4.3.0,where:
v4.3.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v4.3.0,"H is the diagonal matrix of the hessian,"
v4.3.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v4.3.0,g is the vector of gradients
v4.3.0,the subscript _T denotes the transpose
v4.3.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v4.3.0,clear the coefficient matrices
v4.3.0,aggregate results from different threads
v4.3.0,copy into eigen matrices and solve
v4.3.0,update the tree properties
v4.3.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.3.0,get feature partition
v4.3.0,get local used features
v4.3.0,get best split at smaller leaf
v4.3.0,find local best split for larger leaf
v4.3.0,sync global best info
v4.3.0,update best split
v4.3.0,"instantiate template classes, otherwise linker cannot find the code"
v4.3.0,initialize SerialTreeLearner
v4.3.0,Get local rank and global machine size
v4.3.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.3.0,allocate buffer for communication
v4.3.0,get block start and block len for reduce scatter
v4.3.0,get buffer_write_start_pos
v4.3.0,get buffer_read_start_pos
v4.3.0,generate feature partition for current tree
v4.3.0,get local used feature
v4.3.0,get block start and block len for reduce scatter
v4.3.0,sync global data sumup info
v4.3.0,global sumup reduce
v4.3.0,copy back
v4.3.0,set global sumup info
v4.3.0,init global data count in leaf
v4.3.0,reset hist num bits according to global num data
v4.3.0,sync global data sumup info
v4.3.0,global sumup reduce
v4.3.0,copy back
v4.3.0,set global sumup info
v4.3.0,init global data count in leaf
v4.3.0,clear histogram buffer before synchronizing
v4.3.0,otherwise histogram contents from the previous iteration will be sent
v4.3.0,construct local histograms
v4.3.0,copy to buffer
v4.3.0,Reduce scatter for histogram
v4.3.0,restore global histograms from buffer
v4.3.0,only root leaf
v4.3.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v4.3.0,find local best split for larger leaf
v4.3.0,sync global best info
v4.3.0,set best split
v4.3.0,need update global number of data in leaf
v4.3.0,reset hist num bits according to global num data
v4.3.0,"instantiate template classes, otherwise linker cannot find the code"
v4.3.0,initialize SerialTreeLearner
v4.3.0,some additional variables needed for GPU trainer
v4.3.0,Initialize GPU buffers and kernels
v4.3.0,some functions used for debugging the GPU histogram construction
v4.3.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v4.3.0,goto err;
v4.3.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v4.3.0,also guarantee that there are at least 2K examples per workgroup
v4.3.0,return 0;
v4.3.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v4.3.0,decide the best number of workgroups working on one feature4 tuple
v4.3.0,set work group size based on feature size
v4.3.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v4.3.0,we need to refresh the kernel arguments after reallocating
v4.3.0,The only argument that needs to be changed later is num_data_
v4.3.0,"the GPU kernel will process all features in one call, and each"
v4.3.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v4.3.0,process one feature4 tuple
v4.3.0,"for the root node, indices are not copied"
v4.3.0,"for constant hessian, hessians are not copied except for the root node"
v4.3.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v4.3.0,and we will launch num_feature workgroups for this kernel
v4.3.0,will launch threads for all features
v4.3.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v4.3.0,copy the results asynchronously. Size depends on if double precision is used
v4.3.0,we will wait for this object in WaitAndGetHistograms
v4.3.0,"when the output is ready, the computation is done"
v4.3.0,values of this feature has been redistributed to multiple bins; need a reduction here
v4.3.0,how many feature-group tuples we have
v4.3.0,leave some safe margin for prefetching
v4.3.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v4.3.0,clear sparse/dense maps
v4.3.0,do nothing if no features can be processed on GPU
v4.3.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v4.3.0,unpin old buffer if necessary before destructing them
v4.3.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v4.3.0,allocate space for gradients and Hessians on device
v4.3.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v4.3.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v4.3.0,copy indices to the device
v4.3.0,histogram bin entry size depends on the precision (single/double)
v4.3.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v4.3.0,each work group generates a sub-histogram of dword_features_ features.
v4.3.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v4.3.0,create atomic counters for inter-group coordination
v4.3.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v4.3.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v4.3.0,looking for dword_features_ non-sparse feature-groups
v4.3.0,decide if we need to redistribute the bin
v4.3.0,multiplier must be a power of 2
v4.3.0,device_bin_mults_.push_back(1);
v4.3.0,found
v4.3.0,for data transfer time
v4.3.0,"Now generate new data structure feature4, and copy data to the device"
v4.3.0,"preallocate arrays for all threads, and pin them"
v4.3.0,building Feature4 bundles; each thread handles dword_features_ features
v4.3.0,one feature datapoint is 4 bits
v4.3.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.3.0,one feature datapoint is one byte
v4.3.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.3.0,Dense bin
v4.3.0,Dense 4-bit bin
v4.3.0,working on the remaining (less than dword_features_) feature groups
v4.3.0,fill the leftover features
v4.3.0,"fill this empty feature with some ""random"" value"
v4.3.0,"fill this empty feature with some ""random"" value"
v4.3.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v4.3.0,deallocate pinned space for feature copying
v4.3.0,data transfer time
v4.3.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v4.3.0,"Something bad happened. Just return ""No log available."""
v4.3.0,"build is okay, log may contain warnings"
v4.3.0,destroy any old kernels
v4.3.0,create OpenCL kernels for different number of workgroups per feature
v4.3.0,currently we don't use constant memory
v4.3.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v4.3.0,kernel with indices in an array
v4.3.0,"kernel with all features enabled, with eliminated branches"
v4.3.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v4.3.0,do nothing if no features can be processed on GPU
v4.3.0,The only argument that needs to be changed later is num_data_
v4.3.0,"hessian is passed as a parameter, but it is not available now."
v4.3.0,hessian will be set in BeforeTrain()
v4.3.0,"Get the max bin size, used for selecting best GPU kernel"
v4.3.0,initialize GPU
v4.3.0,determine which kernel to use based on the max number of bins
v4.3.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.3.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.3.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.3.0,ignore the feature groups that contain categorical features when producing warnings about max_bin.
v4.3.0,"these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin."
v4.3.0,setup GPU kernel arguments after we allocating all the buffers
v4.3.0,GPU memory has to been reallocated because data may have been changed
v4.3.0,setup GPU kernel arguments after we allocating all the buffers
v4.3.0,Copy initial full hessians and gradients to GPU.
v4.3.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v4.3.0,setup hessian parameters only
v4.3.0,hessian is passed as a parameter
v4.3.0,use bagging
v4.3.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v4.3.0,copy used gradients and Hessians to ordered buffer
v4.3.0,transfer the indices to GPU
v4.3.0,transfer hessian to GPU
v4.3.0,setup hessian parameters only
v4.3.0,hessian is passed as a parameter
v4.3.0,transfer gradients to GPU
v4.3.0,only have root
v4.3.0,"Copy indices, gradients and Hessians as early as possible"
v4.3.0,only need to initialize for smaller leaf
v4.3.0,Get leaf boundary
v4.3.0,copy indices to the GPU:
v4.3.0,copy ordered Hessians to the GPU:
v4.3.0,copy ordered gradients to the GPU:
v4.3.0,do nothing if no features can be processed on GPU
v4.3.0,copy data indices if it is not null
v4.3.0,generate and copy ordered_gradients if gradients is not null
v4.3.0,generate and copy ordered_hessians if Hessians is not null
v4.3.0,converted indices in is_feature_used to feature-group indices
v4.3.0,construct the feature masks for dense feature-groups
v4.3.0,"if no feature group is used, just return and do not use GPU"
v4.3.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v4.3.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v4.3.0,"All data have been prepared, now run the GPU kernel"
v4.3.0,construct smaller leaf
v4.3.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v4.3.0,then construct sparse features on CPU
v4.3.0,"wait for GPU to finish, only if GPU is actually used"
v4.3.0,use double precision
v4.3.0,use single precision
v4.3.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v4.3.0,#define GPU_DEBUG_COMPARE
v4.3.0,construct larger leaf
v4.3.0,then construct sparse features on CPU
v4.3.0,"wait for GPU to finish, only if GPU is actually used"
v4.3.0,use double precision
v4.3.0,use single precision
v4.3.0,do some sanity check for the GPU algorithm
v4.3.0,limit top k
v4.3.0,get max bin
v4.3.0,calculate buffer size
v4.3.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.3.0,"left and right on same time, so need double size"
v4.3.0,initialize histograms for global
v4.3.0,sync global data sumup info
v4.3.0,set global sumup info
v4.3.0,init global data count in leaf
v4.3.0,get local sumup
v4.3.0,get local sumup
v4.3.0,get mean number on machines
v4.3.0,weighted gain
v4.3.0,get top k
v4.3.0,"Copy histogram to buffer, and Get local aggregate features"
v4.3.0,copy histograms.
v4.3.0,copy smaller leaf histograms first
v4.3.0,mark local aggregated feature
v4.3.0,copy
v4.3.0,then copy larger leaf histograms
v4.3.0,mark local aggregated feature
v4.3.0,copy
v4.3.0,use local data to find local best splits
v4.3.0,clear histogram buffer before synchronizing
v4.3.0,otherwise histogram contents from the previous iteration will be sent
v4.3.0,find splits
v4.3.0,only has root leaf
v4.3.0,local voting
v4.3.0,gather
v4.3.0,get all top-k from all machines
v4.3.0,global voting
v4.3.0,copy local histgrams to buffer
v4.3.0,Reduce scatter for histogram
v4.3.0,find best split from local aggregated histograms
v4.3.0,restore from buffer
v4.3.0,restore from buffer
v4.3.0,find local best
v4.3.0,find local best split for larger leaf
v4.3.0,sync global best info
v4.3.0,copy back
v4.3.0,set the global number of data for leaves
v4.3.0,init the global sumup info
v4.3.0,"instantiate template classes, otherwise linker cannot find the code"
v4.3.0,allocate CUDA memory
v4.3.0,leave some space for alignment
v4.3.0,input best split info
v4.3.0,for leaf information update
v4.3.0,"gather information for CPU, used for launching kernels"
v4.3.0,for leaf splits information update
v4.3.0,we need restore the order of indices in cuda_data_indices_
v4.3.0,allocate more memory for sum reduction in CUDA
v4.3.0,only the first element records the final sum
v4.3.0,intialize split find task information (a split find task is one pass through the histogram of a feature)
v4.3.0,need to double the size of histogram buffer in global memory when using double precision in histogram construction
v4.3.0,use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes
v4.3.0,use the first gpu by default
v4.3.0,"std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values"
v4.3.0,use feature interaction constraint or sample features by node
v4.2.0,coding: utf-8
v4.2.0,create predictor first
v4.2.0,setting early stopping via global params should be possible
v4.2.0,reduce cost for prediction training data
v4.2.0,process callbacks
v4.2.0,construct booster
v4.2.0,start training
v4.2.0,check evaluation result.
v4.2.0,"ranking task, split according to groups"
v4.2.0,run preprocessing on the data set if needed
v4.2.0,setting early stopping via global params should be possible
v4.2.0,setup callbacks
v4.2.0,coding: utf-8
v4.2.0,dummy function to support older version of scikit-learn
v4.2.0,coding: utf-8
v4.2.0,"f(labels, preds)"
v4.2.0,"f(labels, preds, weights)"
v4.2.0,"f(labels, preds, weights, group)"
v4.2.0,"f(labels, preds)"
v4.2.0,"f(labels, preds, weights)"
v4.2.0,"f(labels, preds, weights, group)"
v4.2.0,documentation templates for LGBMModel methods are shared between the classes in
v4.2.0,this module and those in the ``dask`` module
v4.2.0,register default metric for consistency with callable eval_metric case
v4.2.0,try to deduce from class instance
v4.2.0,overwrite default metric by explicitly set metric
v4.2.0,"use joblib conventions for negative n_jobs, just like scikit-learn"
v4.2.0,"at predict time, this is handled later due to the order of parameter updates"
v4.2.0,Do not modify original args in fit function
v4.2.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v4.2.0,Separate built-in from callable evaluation metrics
v4.2.0,concatenate metric from params (or default if not provided in params) and eval_metric
v4.2.0,copy for consistency
v4.2.0,reduce cost for prediction training data
v4.2.0,free dataset
v4.2.0,retrive original params that possibly can be used in both training and prediction
v4.2.0,and then overwrite them (considering aliases) with params that were passed directly in prediction
v4.2.0,number of threads can have values with special meaning which is only applied
v4.2.0,"in the scikit-learn interface, these should not reach the c++ side as-is"
v4.2.0,adjust eval metrics to match whether binary or multiclass
v4.2.0,classification is being performed
v4.2.0,"do not modify args, as it causes errors in model selection tools"
v4.2.0,check group data
v4.2.0,coding: utf-8
v4.2.0,coding: utf-8
v4.2.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v4.2.0,coding: utf-8
v4.2.0,coding: utf-8
v4.2.0,typing.TypeGuard was only introduced in Python 3.10
v4.2.0,we don't need lib_lightgbm while building docs
v4.2.0,TypeError: obj is not a string or a number
v4.2.0,ValueError: invalid literal
v4.2.0,Obtain objects to export
v4.2.0,Prepare export
v4.2.0,Export all objects
v4.2.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v4.2.0,"lazy evaluation to allow import without dynamic library, e.g., for docs generation"
v4.2.0,"if buffer length is not long enough, re-allocate a buffer"
v4.2.0,avoid side effects on passed-in parameters
v4.2.0,"if main_param_name was provided, keep that value and remove all aliases"
v4.2.0,"if main param name was not found, search for an alias"
v4.2.0,"neither of main_param_name, aliases were found"
v4.2.0,most common case (no nullable dtypes)
v4.2.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.2.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.2.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.2.0,take shallow copy in case we modify categorical columns
v4.2.0,whole column modifications don't change the original df
v4.2.0,determine feature names
v4.2.0,determine categorical features
v4.2.0,so that the target dtype considers floats
v4.2.0,Get total row number.
v4.2.0,Random access by row index. Used for data sampling.
v4.2.0,Range data access. Used to read data in batch when constructing Dataset.
v4.2.0,Optionally specify batch_size to control range data read size.
v4.2.0,Only required if using ``Dataset.subset()``.
v4.2.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.2.0,avoid memory consumption by arrays concatenation operations
v4.2.0,create numpy array from output arrays
v4.2.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v4.2.0,for CSC there is extra column added
v4.2.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v4.2.0,same shape as input csr or csc matrix except extra column for expected value
v4.2.0,note: make sure we copy data as it will be deallocated next
v4.2.0,"free the temporary native indptr, indices, and data"
v4.2.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.2.0,avoid memory consumption by arrays concatenation operations
v4.2.0,Check that the input is valid: we only handle numbers (for now)
v4.2.0,Prepare prediction output array
v4.2.0,Export Arrow table to C and run prediction
v4.2.0,c type: double**
v4.2.0,each double* element points to start of each column of sample data.
v4.2.0,c type int**
v4.2.0,each int* points to start of indices for each column
v4.2.0,"no min_data, nthreads and verbose in this function"
v4.2.0,check data has header or not
v4.2.0,need to regroup init_score
v4.2.0,process for args
v4.2.0,get categorical features
v4.2.0,"If the params[cat_alias] is equal to categorical_indices, do not report the warning."
v4.2.0,process for reference dataset
v4.2.0,start construct data
v4.2.0,set feature names
v4.2.0,"Select sampled rows, transpose to column order."
v4.2.0,create validation dataset from ref_dataset
v4.2.0,Check that the input is valid: we only handle numbers (for now)
v4.2.0,Export Arrow table to C
v4.2.0,create valid
v4.2.0,construct subset
v4.2.0,create train
v4.2.0,could be updated if data is not freed
v4.2.0,set to None
v4.2.0,"If the data is a arrow data, we can just pass it to C"
v4.2.0,"If a table is being passed, we concatenate the columns. This is only valid for"
v4.2.0,'init_score'.
v4.2.0,we're done if self and reference share a common upstream reference
v4.2.0,Check if the weight contains values other than one
v4.2.0,Set field
v4.2.0,original values can be modified at cpp side
v4.2.0,"if buffer length is not long enough, reallocate buffers"
v4.2.0,"group data from LightGBM is boundaries data, need to convert to group size"
v4.2.0,Training task
v4.2.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v4.2.0,construct booster object
v4.2.0,copy the parameters from train_set
v4.2.0,save reference to data
v4.2.0,buffer for inner predict
v4.2.0,Prediction task
v4.2.0,"if buffer length is not long enough, re-allocate a buffer"
v4.2.0,if a single node tree it won't have `leaf_index` so return 0
v4.2.0,"Create the node record, and populate universal data members"
v4.2.0,Update values to reflect node type (leaf or split)
v4.2.0,traverse the next level of the tree
v4.2.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v4.2.0,and we add node to the list.
v4.2.0,need reset training data
v4.2.0,need to push new valid data
v4.2.0,ensure that existing Booster is freed before replacing it
v4.2.0,with a new one createdfrom file
v4.2.0,"if buffer length is not long enough, re-allocate a buffer"
v4.2.0,"if buffer length is not long enough, reallocate a buffer"
v4.2.0,Copy models
v4.2.0,Get name of features
v4.2.0,"if buffer length is not long enough, reallocate buffers"
v4.2.0,avoid to predict many time in one iteration
v4.2.0,Get num of inner evals
v4.2.0,Get name of eval metrics
v4.2.0,"if buffer length is not long enough, reallocate buffers"
v4.2.0,coding: utf-8
v4.2.0,Callback environment used by callbacks
v4.2.0,"CVBooster holds a list of Booster objects, each needs to be updated"
v4.2.0,"for lgb.cv() with eval_train_metric=True, evaluation is also done on the training set"
v4.2.0,and those metrics are considered for early stopping
v4.2.0,"for lgb.train(), it's possible to pass the training data via valid_sets with any eval_name"
v4.2.0,validation sets are guaranteed to not be identical to the training data in cv()
v4.2.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.2.0,self.best_score_list is initialized to an empty list
v4.2.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.2.0,coding: utf-8
v4.2.0,Acquire port in worker
v4.2.0,schedule futures to retrieve each element of the tuple
v4.2.0,retrieve ports
v4.2.0,Concatenate many parts into one
v4.2.0,construct local eval_set data.
v4.2.0,store indices of eval_set components that were not contained within local parts.
v4.2.0,consolidate parts of each individual eval component.
v4.2.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v4.2.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v4.2.0,filter padding from eval parts then _concat each eval_set component.
v4.2.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v4.2.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v4.2.0,capture whether local_listen_port or its aliases were provided
v4.2.0,capture whether machines or its aliases were provided
v4.2.0,Some passed-in parameters can be removed:
v4.2.0,* 'num_machines': set automatically from Dask worker list
v4.2.0,* 'num_threads': overridden to match nthreads on each Dask process
v4.2.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v4.2.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v4.2.0,X and y are each delayed sub-lists of original eval dask Collections.
v4.2.0,find maximum number of parts in an individual eval set so that we can
v4.2.0,pad eval sets when they come in different sizes.
v4.2.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v4.2.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v4.2.0,first time a chunk of this eval set is added to this part.
v4.2.0,append additional chunks of this eval set to this part.
v4.2.0,ensure that all evaluation parts map uniquely to one part.
v4.2.0,assign sub-eval_set components to worker parts.
v4.2.0,Start computation in the background
v4.2.0,trigger error locally
v4.2.0,Find locations of all parts and map them to particular Dask workers
v4.2.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v4.2.0,data artifacts may not be populated depending on worker returning final estimator.
v4.2.0,assign general validation set settings to fit kwargs.
v4.2.0,resolve aliases for network parameters and pop the result off params.
v4.2.0,these values are added back in calls to `_train_part()`
v4.2.0,figure out network params
v4.2.0,Tell each worker to train on the parts that it has locally
v4.2.0,
v4.2.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v4.2.0,1. there is randomness in the training process unless parameters ``seed``
v4.2.0,and ``deterministic`` are set
v4.2.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v4.2.0,relies on global state (it and all the other LightGBM training processes
v4.2.0,coordinate with each other)
v4.2.0,"if network parameters were changed during training, remove them from the"
v4.2.0,returned model so that they're generated dynamically on every run based
v4.2.0,on the Dask cluster you're connected to and which workers have pieces of
v4.2.0,the training data
v4.2.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v4.2.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v4.2.0,are returned as a list of sparse matrices (one per class)
v4.2.0,"pred_contrib output will have one column per feature,"
v4.2.0,plus one more for the base value
v4.2.0,need to tell Dask the expected type and shape of individual preds
v4.2.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v4.2.0,the code below is used instead to ensure that the sparse type is preserved during concatentation
v4.2.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v4.2.0,Concatenate them to return a list of Dask Arrays.
v4.2.0,"DaskLGBMClassifier does not support group, eval_group."
v4.2.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v4.2.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group."
v4.2.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v4.2.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v4.2.0,DaskLGBMRanker support for callbacks and init_model is not tested
v4.2.0,coding: utf-8
v4.2.0,load or create your dataset
v4.2.0,create dataset for lightgbm
v4.2.0,"if you want to re-use data, remember to set free_raw_data=False"
v4.2.0,specify your configurations as a dict
v4.2.0,generate feature names
v4.2.0,feature_name and categorical_feature
v4.2.0,check feature name
v4.2.0,save model to file
v4.2.0,dump model to JSON (and save to file)
v4.2.0,feature names
v4.2.0,feature importances
v4.2.0,load model to predict
v4.2.0,can only predict with the best iteration (or the saving iteration)
v4.2.0,eval with loaded model
v4.2.0,dump model with pickle
v4.2.0,load model with pickle to predict
v4.2.0,can predict with any iteration when loaded in pickle way
v4.2.0,eval with loaded model
v4.2.0,continue training
v4.2.0,init_model accepts:
v4.2.0,1. model file name
v4.2.0,2. Booster()
v4.2.0,decay learning rates
v4.2.0,reset_parameter callback accepts:
v4.2.0,1. list with length = num_boost_round
v4.2.0,2. function(curr_iter)
v4.2.0,change other parameters during training
v4.2.0,self-defined objective function
v4.2.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v4.2.0,log likelihood loss
v4.2.0,self-defined eval metric
v4.2.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.2.0,binary error
v4.2.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.2.0,This may make built-in evaluation metric calculate wrong results
v4.2.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.2.0,Keep this in mind when you use the customization
v4.2.0,Pass custom objective function through params
v4.2.0,another self-defined eval metric
v4.2.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.2.0,accuracy
v4.2.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.2.0,This may make built-in evaluation metric calculate wrong results
v4.2.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.2.0,Keep this in mind when you use the customization
v4.2.0,Pass custom objective function through params
v4.2.0,callback
v4.2.0,coding: utf-8
v4.2.0,load or create your dataset
v4.2.0,train
v4.2.0,predict
v4.2.0,eval
v4.2.0,feature importances
v4.2.0,self-defined eval metric
v4.2.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.2.0,Root Mean Squared Logarithmic Error (RMSLE)
v4.2.0,train
v4.2.0,another self-defined eval metric
v4.2.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.2.0,Relative Absolute Error (RAE)
v4.2.0,train
v4.2.0,predict
v4.2.0,eval
v4.2.0,other scikit-learn modules
v4.2.0,coding: utf-8
v4.2.0,load or create your dataset
v4.2.0,create dataset for lightgbm
v4.2.0,specify your configurations as a dict
v4.2.0,train
v4.2.0,coding: utf-8
v4.2.0,################
v4.2.0,Simulate some binary data with a single categorical and
v4.2.0,single continuous predictor
v4.2.0,################
v4.2.0,Set up a couple of utilities for our experiments
v4.2.0,################
v4.2.0,Observe the behavior of `binary` and `xentropy` objectives
v4.2.0,Trying this throws an error on non-binary values of y:
v4.2.0,"experiment('binary', label_type='probability', DATA)"
v4.2.0,The speed of `binary` is not drastically different than
v4.2.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v4.2.0,there are reasons to suspect that `binary` should run faster when the
v4.2.0,label is an integer instead of a float
v4.2.0,coding: utf-8
v4.2.0,load or create your dataset
v4.2.0,create dataset for lightgbm
v4.2.0,specify your configurations as a dict
v4.2.0,train
v4.2.0,save model to file
v4.2.0,predict
v4.2.0,eval
v4.2.0,We can also open HDF5 file once and get access to
v4.2.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v4.2.0,
v4.2.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v4.2.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v4.2.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v4.2.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v4.2.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v4.2.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v4.2.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v4.2.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v4.2.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v4.2.0,Here we are just creating a chunk size that matches with batch_size.
v4.2.0,
v4.2.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v4.2.0,lightgbm Dataset.
v4.2.0,Save to 2 HDF5 files for demonstration.
v4.2.0,We can store multiple datasets inside a single HDF5 file.
v4.2.0,Separating X and Y for choosing best chunk size for data loading.
v4.2.0,split training data into two partitions
v4.2.0,make this array dense because we're splitting across
v4.2.0,a sparse boundary to partition the data
v4.2.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v4.2.0,predictions and target values back from workers to the client
v4.2.0,
v4.2.0,"for larger datasets, consider the metrics from dask-ml instead"
v4.2.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v4.2.0,coding: utf-8
v4.2.0,!/usr/bin/env python3
v4.2.0,-*- coding: utf-8 -*-
v4.2.0,
v4.2.0,"LightGBM documentation build configuration file, created by"
v4.2.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v4.2.0,
v4.2.0,This file is execfile()d with the current directory set to its
v4.2.0,containing dir.
v4.2.0,
v4.2.0,Note that not all possible configuration values are present in this
v4.2.0,autogenerated file.
v4.2.0,
v4.2.0,All configuration values have a default; values that are commented out
v4.2.0,serve to show the default.
v4.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
v4.2.0,add these directories to sys.path here. If the directory is relative to the
v4.2.0,"documentation root, use os.path.abspath to make it absolute."
v4.2.0,-- General configuration ------------------------------------------------
v4.2.0,"If your documentation needs a minimal Sphinx version, state it here."
v4.2.0,"Add any Sphinx extension module names here, as strings. They can be"
v4.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v4.2.0,ones.
v4.2.0,mock out modules
v4.2.0,hide type hints in API docs
v4.2.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v4.2.0,Only the class' docstring is inserted.
v4.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v4.2.0,The master toctree document.
v4.2.0,General information about the project.
v4.2.0,The name of an image file (relative to this directory) to place at the top
v4.2.0,of the sidebar.
v4.2.0,The name of an image file (relative to this directory) to use as a favicon of
v4.2.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v4.2.0,pixels large.
v4.2.0,"The version info for the project you're documenting, acts as replacement for"
v4.2.0,"|version| and |release|, also used in various other places throughout the"
v4.2.0,built documents.
v4.2.0,The short X.Y version.
v4.2.0,"The full version, including alpha/beta/rc tags."
v4.2.0,The language for content autogenerated by Sphinx. Refer to documentation
v4.2.0,for a list of supported languages.
v4.2.0,
v4.2.0,This is also used if you do content translation via gettext catalogs.
v4.2.0,"Usually you set ""language"" from the command line for these cases."
v4.2.0,"List of patterns, relative to source directory, that match files and"
v4.2.0,directories to ignore when looking for source files.
v4.2.0,This patterns also effect to html_static_path and html_extra_path
v4.2.0,The name of the Pygments (syntax highlighting) style to use.
v4.2.0,-- Configuration for C API docs generation ------------------------------
v4.2.0,-- Options for HTML output ----------------------------------------------
v4.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v4.2.0,a list of builtin themes.
v4.2.0,Theme options are theme-specific and customize the look and feel of a theme
v4.2.0,"further.  For a list of options available for each theme, see the"
v4.2.0,documentation.
v4.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
v4.2.0,"relative to this directory. They are copied after the builtin static files,"
v4.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v4.2.0,-- Options for HTMLHelp output ------------------------------------------
v4.2.0,Output file base name for HTML help builder.
v4.2.0,-- Options for LaTeX output ---------------------------------------------
v4.2.0,The name of an image file (relative to this directory) to place at the top of
v4.2.0,the title page.
v4.2.0,intersphinx configuration
v4.2.0,Warning! The following code can cause buffer overflows on RTD.
v4.2.0,Consider suppressing output completely if RTD project silently fails.
v4.2.0,Refer to https://github.com/svenevs/exhale
v4.2.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.2.0,Warning! The following code can cause buffer overflows on RTD.
v4.2.0,Consider suppressing output completely if RTD project silently fails.
v4.2.0,Refer to https://github.com/svenevs/exhale
v4.2.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.2.0,coding: utf-8
v4.2.0,This is a basic test for floating number parsing.
v4.2.0,Most of the test cases come from:
v4.2.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v4.2.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v4.2.0,FLT_MAX
v4.2.0,FLT_MIN
v4.2.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v4.2.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v4.2.0,Near DBL_MIN
v4.2.0,DBL_MIN 2^-1022
v4.2.0,The behavior for parsing -nan depends on implementation.
v4.2.0,Thus we skip binary check for negative nan.
v4.2.0,See comment in test_cases.
v4.2.0,construct sample data first (use all data for convenience and since size is small)
v4.2.0,Load some test data
v4.2.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.2.0,Add some fake initial_scores and groups so we can test streaming them
v4.2.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.2.0,Load some test data
v4.2.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.2.0,Add some fake initial_scores and groups so we can test streaming them
v4.2.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.2.0,NOTE: Arrow arrays have 64-bit alignment but we can safely ignore this in tests
v4.2.0,1) Create validity bitmap
v4.2.0,2) Create buffers
v4.2.0,Create arrow array
v4.2.0,Arithmetic
v4.2.0,Subscripts
v4.2.0,End
v4.2.0,Load some test data
v4.2.0,Serialize the reference
v4.2.0,Deserialize the reference
v4.2.0,Confirm 1 successful API call
v4.2.0,Free memory
v4.2.0,Test that Data() points to first value written
v4.2.0,Constants
v4.2.0,Start with some content:
v4.2.0,Clear & re-use:
v4.2.0,Output should match new content:
v4.2.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v4.2.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v4.2.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v4.2.0,Compute a new trial address & value & if it is a valid address:
v4.2.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v4.2.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v4.2.0,Check the just-stored value with getitem():
v4.2.0,Also store the just-stored value for future tracking:
v4.2.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v4.2.0,Test in 2 ways that the values are correctly laid out in memory:
v4.2.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.2.0,Use multiple threads to test concurrency
v4.2.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.2.0,Calculate expected boundaries
v4.2.0,Extract a set of rows from the column-based format (still maintaining column based format)
v4.2.0,coding: utf-8
v4.2.0,"at initialization, should be -1"
v4.2.0,updating that value through the C API should work
v4.2.0,resetting to any negative number should set it to -1
v4.2.0,coding: utf-8
v4.2.0,check saved model persistence
v4.2.0,"we need to check the consistency of model file here, so test for exact equal"
v4.2.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v4.2.0,"scores likely to be different, but prediction should still be the same"
v4.2.0,test that shape is checked during prediction
v4.2.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v4.2.0,The following is for demo and testing purpose.
v4.2.0,whole col
v4.2.0,half col
v4.2.0,Create dataset from numpy array directly.
v4.2.0,Create dataset using Sequence.
v4.2.0,Test for validation set.
v4.2.0,Select some random rows as valid data.
v4.2.0,"From Dataset constructor, with dataset from numpy array."
v4.2.0,"From Dataset.create_valid, with dataset from sequence."
v4.2.0,test that method works even with free_raw_data=True
v4.2.0,test that method works but sets raw data to None in case of immergeable data types
v4.2.0,test that method works for different data types
v4.2.0,"Set extremely harsh penalties, so CEGB will block most splits."
v4.2.0,"Compare pairs of penalties, to ensure scaling works as intended"
v4.2.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v4.2.0,"unconstructed, get_* methods should return whatever was provided"
v4.2.0,"before construction, get_field() should raise an exception"
v4.2.0,"constructed, get_* methods should return numpy arrays, even when the provided"
v4.2.0,input was a list of floats or ints
v4.2.0,"get_field(""group"") returns a numpy array with boundaries, instead of size"
v4.2.0,"NOTE: ""position"" is converted to int32 on the C++ side"
v4.2.0,"should resolve duplicate aliases, and prefer the main parameter"
v4.2.0,should choose the highest priority alias and set that value on main param
v4.2.0,if only aliases are used
v4.2.0,should use the default if main param and aliases are missing
v4.2.0,all changes should be made on copies and not modify the original
v4.2.0,preserves None found for main param and still removes aliases
v4.2.0,correctly chooses value when only an alias is provided
v4.2.0,adds None if that's given as the default and param not found
v4.2.0,If callable is found in objective
v4.2.0,Value in params should be preferred to the default_value passed from keyword arguments
v4.2.0,"None of objective or its aliases in params, but default_value is callable."
v4.2.0,check that the original data wasn't modified
v4.2.0,check that the built data has the codes
v4.2.0,if all categories were seen during training we just take the codes
v4.2.0,if we only saw 'a' during training we just replace its code
v4.2.0,and leave the rest as nan
v4.2.0,test using defined feature names
v4.2.0,test using default feature names
v4.2.0,check for feature indices outside of range
v4.2.0,coding: utf-8
v4.2.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v4.2.0,set_index ensures partitions are based on group id.
v4.2.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v4.2.0,"separate target, weight from features."
v4.2.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v4.2.0,"so that within each partition, sum(g) = n_samples."
v4.2.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v4.2.0,make one categorical feature relevant to the target
v4.2.0,https://github.com/microsoft/LightGBM/issues/4118
v4.2.0,extra predict() parameters should be passed through correctly
v4.2.0,pref_leaf values should have the right shape
v4.2.0,and values that look like valid tree nodes
v4.2.0,"be sure LightGBM actually used at least one categorical column,"
v4.2.0,and that it was correctly treated as a categorical feature
v4.2.0,shape depends on whether it is binary or multiclass classification
v4.2.0,"in the special case of multi-class classification using scipy sparse matrices,"
v4.2.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v4.2.0,
v4.2.0,"since that case is so different than all other cases, check the relevant things here"
v4.2.0,and then return early
v4.2.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v4.2.0,"be sure LightGBM actually used at least one categorical column,"
v4.2.0,and that it was correctly treated as a categorical feature
v4.2.0,* shape depends on whether it is binary or multiclass classification
v4.2.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v4.2.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v4.2.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v4.2.0,that the output has the right shape and base values are in the right position
v4.2.0,"with a custom objective, prediction result is a raw score instead of predicted class"
v4.2.0,function should have been preserved
v4.2.0,should correctly classify every sample
v4.2.0,probability estimates should be similar
v4.2.0,Scores should be the same
v4.2.0,Predictions should be roughly the same.
v4.2.0,pref_leaf values should have the right shape
v4.2.0,and values that look like valid tree nodes
v4.2.0,extra predict() parameters should be passed through correctly
v4.2.0,"be sure LightGBM actually used at least one categorical column,"
v4.2.0,and that it was correctly treated as a categorical feature
v4.2.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v4.2.0,that the output has the right shape and base values are in the right position
v4.2.0,"be sure LightGBM actually used at least one categorical column,"
v4.2.0,and that it was correctly treated as a categorical feature
v4.2.0,Quantiles should be right
v4.2.0,"be sure LightGBM actually used at least one categorical column,"
v4.2.0,and that it was correctly treated as a categorical feature
v4.2.0,function should have been preserved
v4.2.0,Scores should be the same
v4.2.0,local and Dask predictions should be the same
v4.2.0,predictions should be better than random
v4.2.0,rebalance small dask.Array dataset for better performance.
v4.2.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v4.2.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v4.2.0,distributed ranker should be able to rank decently well and should
v4.2.0,have high rank correlation with scores from serial ranker.
v4.2.0,extra predict() parameters should be passed through correctly
v4.2.0,pref_leaf values should have the right shape
v4.2.0,and values that look like valid tree nodes
v4.2.0,"be sure LightGBM actually used at least one categorical column,"
v4.2.0,and that it was correctly treated as a categorical feature
v4.2.0,rebalance small dask.Array dataset for better performance.
v4.2.0,distributed ranker should be able to rank decently well with the least-squares objective
v4.2.0,and should have high rank correlation with scores from serial ranker.
v4.2.0,function should have been preserved
v4.2.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v4.2.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v4.2.0,"test eval_class_weight, eval_init_score on binary-classification task."
v4.2.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v4.2.0,create eval_sets by creating new datasets or copying training data.
v4.2.0,total number of trees scales up for ova classifier.
v4.2.0,check that early stopping was not applied.
v4.2.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v4.2.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v4.2.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v4.2.0,should be able to use the class without specifying a client
v4.2.0,should be able to set client after construction
v4.2.0,data on cluster1
v4.2.0,create identical data on cluster2
v4.2.0,"at this point, the result of default_client() is client2 since it was the most recently"
v4.2.0,created. So setting client to client1 here to test that you can select a non-default client
v4.2.0,"unfitted model should survive pickling round trip, and pickling"
v4.2.0,shouldn't have side effects on the model object
v4.2.0,client will always be None after unpickling
v4.2.0,"fitted model should survive pickling round trip, and pickling"
v4.2.0,shouldn't have side effects on the model object
v4.2.0,client will always be None after unpickling
v4.2.0,rebalance data to be sure that each worker has a piece of the data
v4.2.0,model 1 - no network parameters given
v4.2.0,model 2 - machines given
v4.2.0,model 3 - local_listen_port given
v4.2.0,training should fail because LightGBM will try to use the same
v4.2.0,port for multiple worker processes on the same machine
v4.2.0,rebalance data to be sure that each worker has a piece of the data
v4.2.0,"test that ""machines"" is actually respected by creating a socket that uses"
v4.2.0,"one of the ports mentioned in ""machines"""
v4.2.0,The above error leaves a worker waiting
v4.2.0,"an informative error should be raised if ""machines"" has duplicates"
v4.2.0,"""client"" should be the only different, and the final argument"
v4.2.0,value of the root node is 0 when init_score is set
v4.2.0,this test is separate because it takes a not-yet-constructed estimator
v4.2.0,coding: utf-8
v4.2.0,coding: utf-8
v4.2.0,"build target, group ID vectors."
v4.2.0,build y/target and group-id vectors with user-specified group sizes.
v4.2.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v4.2.0,groups should contain > 1 element for pairwise learning objective.
v4.2.0,"build feature data, X. Transform first few into informative features."
v4.2.0,"doing this here, at import time, to ensure it only runs once_per import"
v4.2.0,instead of once per assertion
v4.2.0,coding: utf-8
v4.2.0,----------------------------------------------------------------------------------------------- #
v4.2.0,UTILITIES                                            #
v4.2.0,----------------------------------------------------------------------------------------------- #
v4.2.0,Set random nulls
v4.2.0,Split data into <=2 random chunks
v4.2.0,Turn chunks into array
v4.2.0,----------------------------------------------------------------------------------------------- #
v4.2.0,UNIT TESTS                                           #
v4.2.0,----------------------------------------------------------------------------------------------- #
v4.2.0,------------------------------------------- DATASET ------------------------------------------- #
v4.2.0,-------------------------------------------- FIELDS ------------------------------------------- #
v4.2.0,Check for equality
v4.2.0,-------------------------------------------- LABELS ------------------------------------------- #
v4.2.0,------------------------------------------- WEIGHTS ------------------------------------------- #
v4.2.0,-------------------------------------------- GROUPS ------------------------------------------- #
v4.2.0,----------------------------------------- INIT SCORES ----------------------------------------- #
v4.2.0,------------------------------------------ PREDICTION ----------------------------------------- #
v4.2.0,coding: utf-8
v4.2.0,check that really dummy objective was used and estimator didn't learn anything
v4.2.0,prediction result is actually not transformed (is raw) due to custom objective
v4.2.0,original estimator is unaffected
v4.2.0,"new estimator is unfitted, but has the same parameters"
v4.2.0,Test if random_state is properly stored
v4.2.0,Test if two random states produce identical models
v4.2.0,Test if subsequent fits sample from random_state object and produce different models
v4.2.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v4.2.0,With default params
v4.2.0,Tests same probabilities
v4.2.0,Tests same predictions
v4.2.0,Tests same raw scores
v4.2.0,Tests same leaf indices
v4.2.0,Tests same feature contributions
v4.2.0,Tests other parameters for the prediction works
v4.2.0,Tests start_iteration
v4.2.0,"Tests same probabilities, starting from iteration 10"
v4.2.0,"Tests same predictions, starting from iteration 10"
v4.2.0,"Tests same raw scores, starting from iteration 10"
v4.2.0,"Tests same leaf indices, starting from iteration 10"
v4.2.0,"Tests same feature contributions, starting from iteration 10"
v4.2.0,"Tests other parameters for the prediction works, starting from iteration 10"
v4.2.0,test that params passed in predict have higher priority
v4.2.0,"no custom objective, no custom metric"
v4.2.0,default metric
v4.2.0,non-default metric
v4.2.0,no metric
v4.2.0,non-default metric in eval_metric
v4.2.0,non-default metric with non-default metric in eval_metric
v4.2.0,non-default metric with multiple metrics in eval_metric
v4.2.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v4.2.0,default metric for non-default objective
v4.2.0,non-default metric for non-default objective
v4.2.0,no metric
v4.2.0,non-default metric in eval_metric for non-default objective
v4.2.0,non-default metric with non-default metric in eval_metric for non-default objective
v4.2.0,non-default metric with multiple metrics in eval_metric for non-default objective
v4.2.0,"custom objective, no custom metric"
v4.2.0,default regression metric for custom objective
v4.2.0,non-default regression metric for custom objective
v4.2.0,multiple regression metrics for custom objective
v4.2.0,no metric
v4.2.0,default regression metric with non-default metric in eval_metric for custom objective
v4.2.0,non-default regression metric with metric in eval_metric for custom objective
v4.2.0,multiple regression metrics with metric in eval_metric for custom objective
v4.2.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v4.2.0,"no custom objective, custom metric"
v4.2.0,default metric with custom metric
v4.2.0,non-default metric with custom metric
v4.2.0,multiple metrics with custom metric
v4.2.0,custom metric (disable default metric)
v4.2.0,default metric for non-default objective with custom metric
v4.2.0,non-default metric for non-default objective with custom metric
v4.2.0,multiple metrics for non-default objective with custom metric
v4.2.0,custom metric (disable default metric for non-default objective)
v4.2.0,"custom objective, custom metric"
v4.2.0,custom metric for custom objective
v4.2.0,non-default regression metric with custom metric for custom objective
v4.2.0,multiple regression metrics with custom metric for custom objective
v4.2.0,default metric and invalid binary metric is replaced with multiclass alternative
v4.2.0,invalid binary metric is replaced with multiclass alternative
v4.2.0,default metric for non-default multiclass objective
v4.2.0,and invalid binary metric is replaced with multiclass alternative
v4.2.0,default metric and invalid multiclass metric is replaced with binary alternative
v4.2.0,invalid multiclass metric is replaced with binary alternative for custom objective
v4.2.0,"Verify that can receive a list of metrics, only callable"
v4.2.0,Verify that can receive a list of custom and built-in metrics
v4.2.0,Verify that works as expected when eval_metric is empty
v4.2.0,"Verify that can receive a list of metrics, only built-in"
v4.2.0,Verify that eval_metric is robust to receiving a list with None
v4.2.0,feval
v4.2.0,single eval_set
v4.2.0,two eval_set
v4.2.0,'val_minus_two' here is the expected number of threads for n_jobs=-2
v4.2.0,"Note: according to joblib's formula, a value of n_jobs=-2 means"
v4.2.0,"""use all but one thread"" (formula: n_cpus + 1 + n_jobs)"
v4.2.0,try to predict with a different feature
v4.2.0,check that disabling the check doesn't raise the error
v4.2.0,"make weights and init_score same types as y, just to avoid"
v4.2.0,a huge number of combinations and therefore test cases
v4.2.0,"make weights and init_score same types as y, just to avoid"
v4.2.0,a huge number of combinations and therefore test cases
v4.2.0,coding: utf-8
v4.2.0,we're in a leaf now
v4.2.0,check that the rest of the elements have black color
v4.2.0,check that we got to the expected leaf
v4.2.0,coding: utf-8
v4.2.0,coding: utf-8
v4.2.0,check that default gives same result as k = 1
v4.2.0,check against independent calculation for k = 1
v4.2.0,check against independent calculation for k = 2
v4.2.0,check against independent calculation for k = 10
v4.2.0,check cases where predictions are equal
v4.2.0,should give same result as binary auc for 2 classes
v4.2.0,test the case where all predictions are equal
v4.2.0,test that weighted data gives different auc_mu
v4.2.0,test that equal data weights give same auc_mu as unweighted data
v4.2.0,should give 1 when accuracy = 1
v4.2.0,test loading class weights
v4.2.0,Simulates position bias for a given ranking dataset.
v4.2.0,The ouput dataset is identical to the input one with the exception for the relevance labels.
v4.2.0,The new labels are generated according to an instance of a cascade user model:
v4.2.0,"for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker"
v4.2.0,"(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)"
v4.2.0,and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;
v4.2.0,"at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,"
v4.2.0,new_label=0. Thus the generated new labels are biased towards the baseline ranker.
v4.2.0,"The positions of the documents in the ranked lists produced by the baseline, are returned."
v4.2.0,a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it
v4.2.0,an instantiation of a cascade model where the user stops with probability 0.2 after observing each document
v4.2.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.2.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.2.0,add extra row to position file
v4.2.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.2.0,test setting positions through Dataset constructor with numpy array
v4.2.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.2.0,test setting positions through Dataset constructor with pandas Series
v4.2.0,test setting positions through set_position
v4.2.0,test get_position works
v4.2.0,no early stopping
v4.2.0,early stopping occurs
v4.2.0,regular early stopping
v4.2.0,positive min_delta
v4.2.0,test custom eval metrics
v4.2.0,"shuffle = False, override metric in params"
v4.2.0,"shuffle = True, callbacks"
v4.2.0,enable display training loss
v4.2.0,self defined folds
v4.2.0,LambdaRank
v4.2.0,... with l2 metric
v4.2.0,... with NDCG (default) metric
v4.2.0,self defined folds with lambdarank
v4.2.0,init_model from an in-memory Booster
v4.2.0,init_model from a text file
v4.2.0,predictions should be identical
v4.2.0,with early stopping
v4.2.0,predict by each fold booster
v4.2.0,check that each booster predicted using the best iteration
v4.2.0,fold averaging
v4.2.0,without early stopping
v4.2.0,test feature_names with whitespaces
v4.2.0,This has non-ascii strings.
v4.2.0,check that passing parameters to the constructor raises warning and ignores them
v4.2.0,check inference isn't affected by unknown parameter
v4.2.0,entries whose values should reflect params passed to lgb.train()
v4.2.0,'l1' was passed in with alias 'mae'
v4.2.0,NOTE: this was passed in with alias 'sub_row'
v4.2.0,entries with default values of params
v4.2.0,add device-specific entries
v4.2.0,
v4.2.0,passed-in force_col_wise / force_row_wise parameters are ignored on CUDA and GPU builds...
v4.2.0,https://github.com/microsoft/LightGBM/blob/1d7ee63686272bceffd522284127573b511df6be/src/io/config.cpp#L375-L377
v4.2.0,check that model text has all expected param entries
v4.2.0,"since Booster.model_to_string() is used when pickling, check that parameters all"
v4.2.0,roundtrip pickling successfully too
v4.2.0,take subsets and train
v4.2.0,generate CSR sparse dataset
v4.2.0,convert data to dense and get back same contribs
v4.2.0,validate the values are the same
v4.2.0,validate using CSC matrix
v4.2.0,validate the values are the same
v4.2.0,generate CSR sparse dataset
v4.2.0,convert data to dense and get back same contribs
v4.2.0,validate the values are the same
v4.2.0,validate using CSC matrix
v4.2.0,validate the values are the same
v4.2.0,Note there is an extra column added to the output for the expected value
v4.2.0,Note output CSC shape should be same as CSR output shape
v4.2.0,test sliced labels
v4.2.0,append some columns
v4.2.0,append some rows
v4.2.0,test sliced 2d matrix
v4.2.0,test sliced CSR
v4.2.0,trees start at position 1.
v4.2.0,split_features are in 4th line.
v4.2.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v4.2.0,The penalization is so high that the first 2 features should not be used here
v4.2.0,Check that a very high penalization is the same as not using the features at all
v4.2.0,check refit accepts dataset_params
v4.2.0,the following checks that dart and rf with mape can predict outside the 0-1 range
v4.2.0,https://github.com/microsoft/LightGBM/issues/1579
v4.2.0,"no custom objective, no feval"
v4.2.0,default metric
v4.2.0,non-default metric in params
v4.2.0,default metric in args
v4.2.0,non-default metric in args
v4.2.0,metric in args overwrites one in params
v4.2.0,metric in args overwrites one in params
v4.2.0,multiple metrics in params
v4.2.0,multiple metrics in args
v4.2.0,remove default metric by 'None' in list
v4.2.0,remove default metric by 'None' aliases
v4.2.0,"custom objective, no feval"
v4.2.0,no default metric
v4.2.0,metric in params
v4.2.0,metric in args
v4.2.0,metric in args overwrites its' alias in params
v4.2.0,multiple metrics in params
v4.2.0,multiple metrics in args
v4.2.0,"no custom objective, feval"
v4.2.0,default metric with custom one
v4.2.0,non-default metric in params with custom one
v4.2.0,default metric in args with custom one
v4.2.0,non-default metric in args with custom one
v4.2.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.2.0,multiple metrics in params with custom one
v4.2.0,multiple metrics in args with custom one
v4.2.0,custom metric is evaluated despite 'None' is passed
v4.2.0,"custom objective, feval"
v4.2.0,"no default metric, only custom one"
v4.2.0,metric in params with custom one
v4.2.0,metric in args with custom one
v4.2.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.2.0,multiple metrics in params with custom one
v4.2.0,multiple metrics in args with custom one
v4.2.0,custom metric is evaluated despite 'None' is passed
v4.2.0,"no custom objective, no feval"
v4.2.0,default metric
v4.2.0,default metric in params
v4.2.0,non-default metric in params
v4.2.0,multiple metrics in params
v4.2.0,remove default metric by 'None' aliases
v4.2.0,"custom objective, no feval"
v4.2.0,no default metric
v4.2.0,metric in params
v4.2.0,multiple metrics in params
v4.2.0,"no custom objective, feval"
v4.2.0,default metric with custom one
v4.2.0,default metric in params with custom one
v4.2.0,non-default metric in params with custom one
v4.2.0,multiple metrics in params with custom one
v4.2.0,custom metric is evaluated despite 'None' is passed
v4.2.0,"custom objective, feval"
v4.2.0,"no default metric, only custom one"
v4.2.0,metric in params with custom one
v4.2.0,multiple metrics in params with custom one
v4.2.0,custom metric is evaluated despite 'None' is passed
v4.2.0,Custom objective replaces multiclass
v4.2.0,multiclass default metric
v4.2.0,multiclass default metric with custom one
v4.2.0,multiclass metric alias with custom one for custom objective
v4.2.0,no metric for invalid class_num
v4.2.0,custom metric for invalid class_num
v4.2.0,multiclass metric alias with custom one with invalid class_num
v4.2.0,multiclass default metric without num_class
v4.2.0,multiclass metric alias
v4.2.0,multiclass metric
v4.2.0,non-valid metric for multiclass objective
v4.2.0,non-default num_class for default objective
v4.2.0,no metric with non-default num_class for custom objective
v4.2.0,multiclass metric alias for custom objective
v4.2.0,multiclass metric for custom objective
v4.2.0,binary metric with non-default num_class for custom objective
v4.2.0,Expect three metrics but mean and stdv for each metric
v4.2.0,test XGBoost-style return value
v4.2.0,test numpy-style return value
v4.2.0,test bins string type
v4.2.0,test histogram is disabled for categorical features
v4.2.0,test for lgb.train
v4.2.0,test feval for lgb.train
v4.2.0,test with two valid data for lgb.train
v4.2.0,test for lgb.cv
v4.2.0,test feval for lgb.cv
v4.2.0,test that binning works properly for features with only positive or only negative values
v4.2.0,decreasing without freeing raw data is allowed
v4.2.0,decreasing before lazy init is allowed
v4.2.0,increasing is allowed
v4.2.0,decreasing with disabled filter is allowed
v4.2.0,decreasing with enabled filter is disallowed;
v4.2.0,also changes of other params are disallowed
v4.2.0,check extra trees increases regularization
v4.2.0,check path smoothing increases regularization
v4.2.0,test edge case with one leaf
v4.2.0,check that constraint containing all features is equivalent to no constraint
v4.2.0,check that constraint partitioning the features reduces train accuracy
v4.2.0,check that constraints consisting of single features reduce accuracy further
v4.2.0,test that interaction constraints work when not all features are used
v4.2.0,check that number of threads does not affect result
v4.2.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v4.2.0,test again with nans in data
v4.2.0,test again with bagging
v4.2.0,test with a feature that has only one non-nan value
v4.2.0,test with a categorical feature
v4.2.0,test refit: same results on same data
v4.2.0,test refit with save and load
v4.2.0,test refit: different results training on different data
v4.2.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v4.2.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v4.2.0,"test the case where start_iteration <= 0, and num_iteration is None"
v4.2.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v4.2.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v4.2.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v4.2.0,test for regression
v4.2.0,test both with and without early stopping
v4.2.0,test for multi-class
v4.2.0,test both with and without early stopping
v4.2.0,test for binary
v4.2.0,test both with and without early stopping
v4.2.0,test against sklearn average precision metric
v4.2.0,test that average precision is 1 where model predicts perfectly
v4.2.0,data as float64
v4.2.0,test all features were used
v4.2.0,test the score is better than predicting the mean
v4.2.0,test all predictions are equal using different input dtypes
v4.2.0,introduce some missing values
v4.2.0,the previous line turns x3 into object dtype in recent versions of pandas
v4.2.0,train with regular dtypes
v4.2.0,convert to nullable dtypes
v4.2.0,test training succeeds
v4.2.0,test all features were used
v4.2.0,test the score is better than predicting the mean
v4.2.0,test equal predictions
v4.2.0,test data are taken from bug report
v4.2.0,https://github.com/microsoft/LightGBM/issues/4708
v4.2.0,modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811
v4.2.0,and https://github.com/microsoft/LightGBM/pull/5087
v4.2.0,test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree
v4.2.0,which is done in the fix #5164
v4.2.0,without the fix:
v4.2.0,Check failed: (best_split_info.left_count) > (0)
v4.2.0,try to predict with a different feature
v4.2.0,check that disabling the check doesn't raise the error
v4.2.0,try to refit with a different feature
v4.2.0,check that disabling the check doesn't raise the error
v4.2.0,coding: utf-8
v4.2.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v4.2.0,Double-precision floats are only supported on x86_64 with PoCL
v4.2.0,coding: utf-8
v4.2.0,coding: utf-8
v4.2.0,These are helper functions to allow doing a stack unwind
v4.2.0,"after an R allocation error, which would trigger a long jump."
v4.2.0,convert from one-based to zero-based index
v4.2.0,"if any feature names were larger than allocated size,"
v4.2.0,allow for a larger size and try again
v4.2.0,convert from boundaries to size
v4.2.0,--- start Booster interfaces
v4.2.0,"if any eval names were larger than allocated size,"
v4.2.0,allow for a larger size and try again
v4.2.0,"if the model string was larger than the initial buffer, call the function again, writing directly to the R object"
v4.2.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.2.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.2.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.2.0,.Call() calls
v4.2.0,coding: utf-8
v4.2.0,alias table
v4.2.0,names
v4.2.0,from strings
v4.2.0,tails
v4.2.0,tails
v4.2.0,the following are stored as comma separated strings but are arrays in the wrappers
v4.2.0,coding: utf-8
v4.2.0,Single row predictor to abstract away caching logic
v4.2.0,create boosting
v4.2.0,initialize the boosting
v4.2.0,create objective function
v4.2.0,initialize the objective function
v4.2.0,create training metric
v4.2.0,reset the boosting
v4.2.0,create objective function
v4.2.0,initialize the objective function
v4.2.0,calculate the nonzero data and indices size
v4.2.0,allocate data and indices arrays
v4.2.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.2.0,aggregated per row feature contribution results
v4.2.0,keep track of the row_vector sizes for parallelization
v4.2.0,copy vector results to output for each row
v4.2.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.2.0,aggregated per row feature contribution results
v4.2.0,calculate number of elements per column to construct
v4.2.0,the CSC matrix with random access
v4.2.0,keep track of column counts
v4.2.0,keep track of beginning index for each column
v4.2.0,keep track of beginning index for each matrix
v4.2.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v4.2.0,store the row index
v4.2.0,update column count
v4.2.0,explicitly declare symbols from LightGBM namespace
v4.2.0,some help functions used to convert data
v4.2.0,Row iterator of on column for CSC matrix
v4.2.0,"return value at idx, only can access by ascent order"
v4.2.0,"return next non-zero pair, if index < 0, means no more data"
v4.2.0,start of c_api functions
v4.2.0,This API is to keep python binding's behavior the same with C++ implementation.
v4.2.0,"Sample count, random seed etc. should be provided in parameters."
v4.2.0,convert internal thread id to be unique based on external thread id
v4.2.0,convert internal thread id to be unique based on external thread id
v4.2.0,sample data first
v4.2.0,sample data first
v4.2.0,sample data first
v4.2.0,local buffer to re-use memory
v4.2.0,sample data first
v4.2.0,no more data
v4.2.0,Prepare the Arrow data
v4.2.0,Initialize the dataset
v4.2.0,"If there is no reference dataset, we first sample indices"
v4.2.0,"Then, we obtain sample values by parallelizing across columns"
v4.2.0,Values need to be copied from the record batches.
v4.2.0,The chunks are iterated over in the inner loop as columns can be treated independently.
v4.2.0,"Finally, we initialize a loader from the sampled values"
v4.2.0,"After sampling and properly initializing all bins, we can add our data to the dataset. Here,"
v4.2.0,we parallelize across rows.
v4.2.0,---- start of booster
v4.2.0,Single row in row-major format:
v4.2.0,Apply the configuration
v4.2.0,Set up chunked array and iterators for all columns
v4.2.0,Build row function
v4.2.0,Run prediction
v4.2.0,---- start of some help functions
v4.2.0,data is array of pointers to individual rows
v4.2.0,set number of threads for openmp
v4.2.0,read parameters from config file
v4.2.0,"remove str after ""#"""
v4.2.0,de-duplicate params
v4.2.0,prediction is needed if using input initial model(continued train)
v4.2.0,need to continue training
v4.2.0,sync up random seed for data partition
v4.2.0,load Training data
v4.2.0,load data for distributed training
v4.2.0,load data for single machine
v4.2.0,need save binary file
v4.2.0,create training metric
v4.2.0,only when have metrics then need to construct validation data
v4.2.0,"Add validation data, if it exists"
v4.2.0,add
v4.2.0,need save binary file
v4.2.0,add metric for validation data
v4.2.0,output used time on each iteration
v4.2.0,need init network
v4.2.0,create boosting
v4.2.0,create objective function
v4.2.0,load training data
v4.2.0,initialize the objective function
v4.2.0,initialize the boosting
v4.2.0,add validation data into boosting
v4.2.0,convert model to if-else statement code
v4.2.0,create predictor
v4.2.0,Free memory
v4.2.0,create predictor
v4.2.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v4.2.0,counts for all labels
v4.2.0,"start from top label, and accumulate DCG"
v4.2.0,counts for all labels
v4.2.0,calculate k Max DCG by one pass
v4.2.0,get sorted indices by score
v4.2.0,calculate multi dcg by one pass
v4.2.0,wait for all client start up
v4.2.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v4.2.0,Instead we will handle finalize/abort for MPI in main().
v4.2.0,default set to -1
v4.2.0,"distance at k-th communication, distance[k] = 2^k"
v4.2.0,set incoming rank at k-th commuication
v4.2.0,set outgoing rank at k-th commuication
v4.2.0,default set as -1
v4.2.0,construct all recursive halving map for all machines
v4.2.0,let 1 << k <= num_machines
v4.2.0,distance of each communication
v4.2.0,"if num_machines = 2^k, don't need to group machines"
v4.2.0,"communication direction, %2 == 0 is positive"
v4.2.0,neighbor at k-th communication
v4.2.0,receive data block at k-th communication
v4.2.0,send data block at k-th communication
v4.2.0,"if num_machines != 2^k, need to group machines"
v4.2.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v4.2.0,let left machine as group leader
v4.2.0,"cache block information for groups, group with 2 machines will have double block size"
v4.2.0,convert from group to node leader
v4.2.0,convert from node to group
v4.2.0,meet new group
v4.2.0,add block len for this group
v4.2.0,calculate the group block start
v4.2.0,not need to construct
v4.2.0,get receive block information
v4.2.0,accumulate block len
v4.2.0,get send block information
v4.2.0,accumulate block len
v4.2.0,static member definition
v4.2.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v4.2.0,assign the blocks to every rank.
v4.2.0,do reduce scatter
v4.2.0,do all gather
v4.2.0,assign blocks
v4.2.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v4.2.0,copy back
v4.2.0,assign blocks
v4.2.0,start all gather
v4.2.0,when num_machines is small and data is large
v4.2.0,use output as receive buffer
v4.2.0,get current local block size
v4.2.0,get out rank
v4.2.0,get in rank
v4.2.0,get send information
v4.2.0,get recv information
v4.2.0,send and recv at same time
v4.2.0,rotate in-place
v4.2.0,use output as receive buffer
v4.2.0,get current local block size
v4.2.0,get send information
v4.2.0,get recv information
v4.2.0,send and recv at same time
v4.2.0,use output as receive buffer
v4.2.0,send and recv at same time
v4.2.0,send local data to neighbor first
v4.2.0,receive neighbor data first
v4.2.0,reduce
v4.2.0,get target
v4.2.0,get send information
v4.2.0,get recv information
v4.2.0,send and recv at same time
v4.2.0,reduce
v4.2.0,send result to neighbor
v4.2.0,receive result from neighbor
v4.2.0,copy result
v4.2.0,start up socket
v4.2.0,parse clients from file
v4.2.0,get ip list of local machine
v4.2.0,get local rank
v4.2.0,construct listener
v4.2.0,construct communication topo
v4.2.0,construct linkers
v4.2.0,free listener
v4.2.0,set timeout
v4.2.0,accept incoming socket
v4.2.0,receive rank
v4.2.0,add new socket
v4.2.0,save ranks that need to connect with
v4.2.0,start listener
v4.2.0,start connect
v4.2.0,let smaller rank connect to larger rank
v4.2.0,send local rank
v4.2.0,wait for listener
v4.2.0,print connected linkers
v4.2.0,only need to copy subset
v4.2.0,avoid to copy subset many times
v4.2.0,avoid out of range
v4.2.0,may need to recopy subset
v4.2.0,valid the type
v4.2.0,parser factory implementation.
v4.2.0,customized parser add-on.
v4.2.0,save header to parser config in case needed.
v4.2.0,save label id to parser config in case needed.
v4.2.0,Constructors
v4.2.0,Get type tag
v4.2.0,Comparisons
v4.2.0,"This has to be separate, not in Statics, because Json() accesses"
v4.2.0,statics().null.
v4.2.0,"advance until next line, or end of input"
v4.2.0,advance until closing tokens
v4.2.0,The usual case: non-escaped characters
v4.2.0,Handle escapes
v4.2.0,Extract 4-byte escape sequence
v4.2.0,Explicitly check length of the substring. The following loop
v4.2.0,relies on std::string returning the terminating NUL when
v4.2.0,accessing str[length]. Checking here reduces brittleness.
v4.2.0,JSON specifies that characters outside the BMP shall be encoded as a
v4.2.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v4.2.0,components. Check whether we're in the middle of such a beast: the
v4.2.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v4.2.0,a trail (low) surrogate.
v4.2.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v4.2.0,per the UTF-16 algorithm.
v4.2.0,Integer part
v4.2.0,Decimal part
v4.2.0,Exponent part
v4.2.0,Check for any trailing garbage
v4.2.0,Documented in json11.hpp
v4.2.0,Check for another object
v4.2.0,get column names
v4.2.0,"support to get header from parser config, so could utilize following label name to id mapping logic."
v4.2.0,load label idx first
v4.2.0,"if parser config file exists, feature names may be changed after customized parser applied."
v4.2.0,clear here so could use default filled feature names during dataset construction.
v4.2.0,may improve by saving real feature names defined in parser in the future.
v4.2.0,erase label column name
v4.2.0,load ignore columns
v4.2.0,load weight idx
v4.2.0,load group idx
v4.2.0,don't support query id in data file when using distributed training
v4.2.0,read data to memory
v4.2.0,sample data
v4.2.0,construct feature bin mappers & clear sample data
v4.2.0,initialize label
v4.2.0,extract features
v4.2.0,sample data from file
v4.2.0,construct feature bin mappers & clear sample data
v4.2.0,initialize label
v4.2.0,extract features
v4.2.0,load data from binary file
v4.2.0,checks whether there's a initial score file when loaded from binary data files
v4.2.0,"the intial score file should with suffix "".bin.init"""
v4.2.0,check meta data
v4.2.0,need to check training data
v4.2.0,read data in memory
v4.2.0,initialize label
v4.2.0,extract features
v4.2.0,Get number of lines of data file
v4.2.0,initialize label
v4.2.0,extract features
v4.2.0,load data from binary file
v4.2.0,checks whether there's a initial score file when loaded from binary data files
v4.2.0,"the intial score file should with suffix "".bin.init"""
v4.2.0,not need to check validation data
v4.2.0,check meta data
v4.2.0,check token
v4.2.0,read feature group definitions
v4.2.0,read feature size
v4.2.0,buffer to read binary file
v4.2.0,check token
v4.2.0,read size of header
v4.2.0,re-allocate space if not enough
v4.2.0,read header
v4.2.0,get header
v4.2.0,read size of meta data
v4.2.0,re-allocate space if not enough
v4.2.0,read meta data
v4.2.0,load meta data
v4.2.0,sample local used data if need to partition
v4.2.0,"if not contain query file, minimal sample unit is one record"
v4.2.0,"if contain query file, minimal sample unit is one query"
v4.2.0,if is new query
v4.2.0,read feature data
v4.2.0,read feature size
v4.2.0,re-allocate space if not enough
v4.2.0,raw data
v4.2.0,fill feature_names_ if not header
v4.2.0,get forced split
v4.2.0,"if only one machine, find bin locally"
v4.2.0,"if have multi-machines, need to find bin distributed"
v4.2.0,different machines will find bin for different features
v4.2.0,start and len will store the process feature indices for different machines
v4.2.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.2.0,free
v4.2.0,gather global feature bin mappers
v4.2.0,restore features bins from buffer
v4.2.0,---- private functions ----
v4.2.0,get header
v4.2.0,num_groups
v4.2.0,real_feature_idx_
v4.2.0,feature2group
v4.2.0,feature2subfeature
v4.2.0,group_bin_boundaries
v4.2.0,group_feature_start_
v4.2.0,group_feature_cnt_
v4.2.0,get feature names
v4.2.0,get forced_bin_bounds_
v4.2.0,"if features are ordered, not need to use hist_buf"
v4.2.0,read all lines
v4.2.0,get query data
v4.2.0,"if not contain query data, minimal sample unit is one record"
v4.2.0,"if contain query data, minimal sample unit is one query"
v4.2.0,if is new query
v4.2.0,get query data
v4.2.0,"if not contain query file, minimal sample unit is one record"
v4.2.0,"if contain query file, minimal sample unit is one query"
v4.2.0,if is new query
v4.2.0,parse features
v4.2.0,get forced split
v4.2.0,"check the range of label_idx, weight_idx and group_idx"
v4.2.0,"skip label check if user input parser config file,"
v4.2.0,because label id is got from raw features while dataset features are consistent with customized parser.
v4.2.0,fill feature_names_ if not header
v4.2.0,start find bins
v4.2.0,"if only one machine, find bin locally"
v4.2.0,start and len will store the process feature indices for different machines
v4.2.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.2.0,free
v4.2.0,gather global feature bin mappers
v4.2.0,restore features bins from buffer
v4.2.0,if doesn't need to prediction with initial model
v4.2.0,parser
v4.2.0,set label
v4.2.0,free processed line:
v4.2.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v4.2.0,text_reader_->Lines()[i].shrink_to_fit();
v4.2.0,push data
v4.2.0,if is used feature
v4.2.0,if need to prediction with initial model
v4.2.0,parser
v4.2.0,set initial score
v4.2.0,set label
v4.2.0,free processed line:
v4.2.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v4.2.0,text_reader_->Lines()[i].shrink_to_fit();
v4.2.0,push data
v4.2.0,if is used feature
v4.2.0,metadata_ will manage space of init_score
v4.2.0,text data can be free after loaded feature values
v4.2.0,parser
v4.2.0,set initial score
v4.2.0,set label
v4.2.0,push data
v4.2.0,if is used feature
v4.2.0,only need part of data
v4.2.0,need full data
v4.2.0,metadata_ will manage space of init_score
v4.2.0,read size of token
v4.2.0,remove duplicates
v4.2.0,deep copy function for BinMapper
v4.2.0,mean size for one bin
v4.2.0,need a new bin
v4.2.0,update bin upper bound
v4.2.0,last bin upper bound
v4.2.0,get number of positive and negative distinct values
v4.2.0,include zero bounds and infinity bound
v4.2.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v4.2.0,find remaining bounds
v4.2.0,find distinct_values first
v4.2.0,push zero in the front
v4.2.0,use the large value
v4.2.0,push zero in the back
v4.2.0,convert to int type first
v4.2.0,sort by counts in descending order
v4.2.0,will ignore the categorical of small counts
v4.2.0,Push the dummy bin for NaN
v4.2.0,Use MissingType::None to represent this bin contains all categoricals
v4.2.0,fix count of NaN bin
v4.2.0,check trivial(num_bin_ == 1) feature
v4.2.0,check useless bin
v4.2.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v4.2.0,so use most_freq_bin_ = default_bin_ when there is not so sparse
v4.2.0,calculate max bin of all features to select the int type in MultiValDenseBin
v4.2.0,"for lambdarank, it needs query data for partition data in distributed learning"
v4.2.0,need convert query_id to boundaries
v4.2.0,check weights
v4.2.0,check positions
v4.2.0,check query boundries
v4.2.0,contain initial score file
v4.2.0,check weights
v4.2.0,get local weights
v4.2.0,check positions
v4.2.0,get local positions
v4.2.0,check query boundries
v4.2.0,get local query boundaries
v4.2.0,contain initial score file
v4.2.0,get local initial scores
v4.2.0,re-calculate query weight
v4.2.0,Clear init scores on empty input
v4.2.0,"Note that len here is row count, not num_init_score, so we compare against num_data"
v4.2.0,"We need to use source_size here, because len might not equal size (due to a partially loaded dataset)"
v4.2.0,CUDA is handled after all insertions are complete
v4.2.0,CUDA is handled after all insertions are complete
v4.2.0,Clear weights on empty input
v4.2.0,CUDA is handled after all insertions are complete
v4.2.0,Clear query boundaries on empty input
v4.2.0,save to nullptr
v4.2.0,CUDA is handled after all insertions are complete
v4.2.0,default weight file name
v4.2.0,default position file name
v4.2.0,default init_score file name
v4.2.0,use first line to count number class
v4.2.0,default query file name
v4.2.0,root is in the depth 0
v4.2.0,non-leaf
v4.2.0,leaf
v4.2.0,use this for the missing value conversion
v4.2.0,Predict func by Map to ifelse
v4.2.0,use this for the missing value conversion
v4.2.0,non-leaf
v4.2.0,left subtree
v4.2.0,right subtree
v4.2.0,leaf
v4.2.0,non-leaf
v4.2.0,left subtree
v4.2.0,right subtree
v4.2.0,leaf
v4.2.0,recursive computation of SHAP values for a decision tree
v4.2.0,extend the unique path
v4.2.0,leaf node
v4.2.0,internal node
v4.2.0,"see if we have already split on this feature,"
v4.2.0,if so we undo that split so we can redo it for this node
v4.2.0,recursive sparse computation of SHAP values for a decision tree
v4.2.0,extend the unique path
v4.2.0,leaf node
v4.2.0,internal node
v4.2.0,"see if we have already split on this feature,"
v4.2.0,if so we undo that split so we can redo it for this node
v4.2.0,add names of objective function if not providing metric
v4.2.0,equal weights for all classes
v4.2.0,generate seeds by seed.
v4.2.0,sort eval_at
v4.2.0,Only push the non-training data
v4.2.0,check for conflicts
v4.2.0,"check if objective, metric, and num_class match"
v4.2.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v4.2.0,Check max_depth and num_leaves
v4.2.0,"Fits in an int, and is more restrictive than the current num_leaves"
v4.2.0,"force col-wise for gpu, and cuda version"
v4.2.0,force row-wise for cuda version
v4.2.0,linear tree learner must be serial type and run on CPU device
v4.2.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v4.2.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v4.2.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v4.2.0,calculated split gain can be positive even with zero gradient and hessian.
v4.2.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v4.2.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v4.2.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v4.2.0,first round: fill the single val group
v4.2.0,always push the last group
v4.2.0,put dense feature first
v4.2.0,sort by non zero cnt
v4.2.0,"sort by non zero cnt, bigger first"
v4.2.0,shuffle groups
v4.2.0,Using std::swap for vector<bool> will cause the wrong result.
v4.2.0,get num_features
v4.2.0,get bin_mappers
v4.2.0,"for sparse multi value bin, we store the feature bin values with offset added"
v4.2.0,"for dense multi value bin, the feature bin values without offsets are used"
v4.2.0,copy feature bin mapper data
v4.2.0,copy feature bin mapper data
v4.2.0,update CUDA storage for column data and metadata
v4.2.0,"if not pass a filename, just append "".bin"" of original file"
v4.2.0,Write the basic header information for the dataset
v4.2.0,get size of meta data
v4.2.0,write meta data
v4.2.0,write feature data
v4.2.0,get size of feature
v4.2.0,write feature
v4.2.0,write raw data; use row-major order so we can read row-by-row
v4.2.0,Calculate approximate size of output and reserve space
v4.2.0,write feature group definitions
v4.2.0,"Give a little extra just in case, to avoid unnecessary resizes"
v4.2.0,"Write token that marks the data as binary reference, and the version"
v4.2.0,Write the basic definition of the overall dataset
v4.2.0,write feature group definitions
v4.2.0,get size of feature
v4.2.0,write feature
v4.2.0,size of feature names and forced bins
v4.2.0,write header
v4.2.0,write feature names
v4.2.0,write forced bins
v4.2.0,"explicitly initialize template methods, for cross module call"
v4.2.0,"explicitly initialize template methods, for cross module call"
v4.2.0,"Only one multi-val group, just simply merge"
v4.2.0,Skip the leading 0 when copying group_bin_boundaries.
v4.2.0,regenerate other fields
v4.2.0,need to iterate bin iterator
v4.2.0,is dense column
v4.2.0,is sparse column
v4.2.0,initialize the subset cuda column data
v4.2.0,"if one column has too many bins, use a separate partition for that column"
v4.2.0,try if adding this column exceed the maximum number per partition
v4.2.0,"if one column has too many bins, use a separate partition for that column"
v4.2.0,try if adding this column exceed the maximum number per partition
v4.2.0,"if LightGBM-specific default has been set, ignore OpenMP-global config"
v4.2.0,"otherwise, default to OpenMP-global config"
v4.2.0,"ensure that if LGBM_SetMaxThreads() was ever called, LightGBM doesn't"
v4.2.0,use more than that many threads
v4.2.0,store the importance first
v4.2.0,PredictRaw
v4.2.0,PredictRawByMap
v4.2.0,Predict
v4.2.0,PredictByMap
v4.2.0,PredictLeafIndex
v4.2.0,PredictLeafIndexByMap
v4.2.0,output model type
v4.2.0,output number of class
v4.2.0,output label index
v4.2.0,output max_feature_idx
v4.2.0,output objective
v4.2.0,output tree models
v4.2.0,store the importance first
v4.2.0,sort the importance
v4.2.0,use serialized string to restore this object
v4.2.0,Use first 128 chars to avoid exceed the message buffer.
v4.2.0,get number of classes
v4.2.0,get index of label
v4.2.0,get max_feature_idx first
v4.2.0,get average_output
v4.2.0,get feature names
v4.2.0,get monotone_constraints
v4.2.0,set zero
v4.2.0,predict all the trees for one iteration
v4.2.0,check early stopping
v4.2.0,set zero
v4.2.0,predict all the trees for one iteration
v4.2.0,check early stopping
v4.2.0,margin_threshold will be captured by value
v4.2.0,copy and sort
v4.2.0,margin_threshold will be captured by value
v4.2.0,Fix for compiler warnings about reaching end of control
v4.2.0,load forced_splits file
v4.2.0,init tree learner
v4.2.0,push training metrics
v4.2.0,get max feature index
v4.2.0,get label index
v4.2.0,get feature names
v4.2.0,get parser config file content
v4.2.0,check that forced splits does not use feature indices larger than dataset size
v4.2.0,"if need bagging, create buffer"
v4.2.0,"for a validation dataset, we need its score and metric"
v4.2.0,update score
v4.2.0,objective function will calculate gradients and hessians
v4.2.0,output used time per iteration
v4.2.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v4.2.0,boosting first
v4.2.0,use customized objective function
v4.2.0,need to copy customized gradients when using GOSS
v4.2.0,bagging logic
v4.2.0,need to copy gradients for bagging subset.
v4.2.0,shrinkage by learning rate
v4.2.0,update score
v4.2.0,only add default score one-time
v4.2.0,updates scores
v4.2.0,add model
v4.2.0,reset score
v4.2.0,remove model
v4.2.0,print message for metric
v4.2.0,pop last early_stopping_round_ models
v4.2.0,update training score
v4.2.0,we need to predict out-of-bag scores of data for boosting
v4.2.0,update validation score
v4.2.0,print training metric
v4.2.0,print validation metric
v4.2.0,set zero
v4.2.0,predict all the trees for one iteration
v4.2.0,predict all the trees for one iteration
v4.2.0,push training metrics
v4.2.0,"not same training data, need reset score and others"
v4.2.0,create score tracker
v4.2.0,update score
v4.2.0,resize gradient vectors to copy the customized gradients for goss or bagging with subset
v4.2.0,load forced_splits file
v4.2.0,"if exists initial score, will start from it"
v4.2.0,clear host score buffer
v4.2.0,Get the max size of pool
v4.2.0,at least need 2 leaves
v4.2.0,push split information for all leaves
v4.2.0,initialize splits for leaf
v4.2.0,initialize data partition
v4.2.0,initialize ordered gradients and hessians
v4.2.0,cannot change is_hist_col_wise during training
v4.2.0,initialize splits for leaf
v4.2.0,initialize data partition
v4.2.0,initialize ordered gradients and hessians
v4.2.0,Get the max size of pool
v4.2.0,at least need 2 leaves
v4.2.0,push split information for all leaves
v4.2.0,some initial works before training
v4.2.0,root leaf
v4.2.0,only root leaf can be splitted on first time
v4.2.0,some initial works before finding best split
v4.2.0,find best threshold for every feature
v4.2.0,Get a leaf with max split gain
v4.2.0,Get split information for best leaf
v4.2.0,"cannot split, quit"
v4.2.0,split tree with best leaf
v4.2.0,reset histogram pool
v4.2.0,initialize data partition
v4.2.0,reset the splits for leaves
v4.2.0,Sumup for root
v4.2.0,use all data
v4.2.0,"use bagging, only use part of data"
v4.2.0,check depth of current leaf
v4.2.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v4.2.0,no enough data to continue
v4.2.0,only have root
v4.2.0,put parent(left) leaf's histograms into larger leaf's histograms
v4.2.0,put parent(left) leaf's histograms to larger leaf's histograms
v4.2.0,construct smaller leaf
v4.2.0,construct larger leaf
v4.2.0,find splits
v4.2.0,only has root leaf
v4.2.0,start at root leaf
v4.2.0,Histogram construction require parent features.
v4.2.0,"then, compute own splits"
v4.2.0,split info should exist because searching in bfs fashion - should have added from parent
v4.2.0,update before tree split
v4.2.0,don't need to update this in data-based parallel model
v4.2.0,"split tree, will return right leaf"
v4.2.0,store the true split gain in tree model
v4.2.0,don't need to update this in data-based parallel model
v4.2.0,store the true split gain in tree model
v4.2.0,init the leaves that used on next iteration
v4.2.0,update leave outputs if needed
v4.2.0,bag_mapper[index_mapper[i]]
v4.2.0,it is needed to filter the features after the above code.
v4.2.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v4.2.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v4.2.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v4.2.0,find splits
v4.2.0,identify features containing nans
v4.2.0,preallocate the matrix used to calculate linear model coefficients
v4.2.0,"store only upper triangular half of matrix as an array, in row-major order"
v4.2.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v4.2.0,we add another 8 to ensure cache lines are not shared among processors
v4.2.0,some initial works before training
v4.2.0,root leaf
v4.2.0,only root leaf can be splitted on first time
v4.2.0,some initial works before finding best split
v4.2.0,find best threshold for every feature
v4.2.0,Get a leaf with max split gain
v4.2.0,Get split information for best leaf
v4.2.0,"cannot split, quit"
v4.2.0,split tree with best leaf
v4.2.0,map data to leaf number
v4.2.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v4.2.0,the coefficients vector is given by
v4.2.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v4.2.0,where:
v4.2.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v4.2.0,"H is the diagonal matrix of the hessian,"
v4.2.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v4.2.0,g is the vector of gradients
v4.2.0,the subscript _T denotes the transpose
v4.2.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v4.2.0,clear the coefficient matrices
v4.2.0,aggregate results from different threads
v4.2.0,copy into eigen matrices and solve
v4.2.0,update the tree properties
v4.2.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.2.0,get feature partition
v4.2.0,get local used features
v4.2.0,get best split at smaller leaf
v4.2.0,find local best split for larger leaf
v4.2.0,sync global best info
v4.2.0,update best split
v4.2.0,"instantiate template classes, otherwise linker cannot find the code"
v4.2.0,initialize SerialTreeLearner
v4.2.0,Get local rank and global machine size
v4.2.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.2.0,allocate buffer for communication
v4.2.0,get block start and block len for reduce scatter
v4.2.0,get buffer_write_start_pos
v4.2.0,get buffer_read_start_pos
v4.2.0,generate feature partition for current tree
v4.2.0,get local used feature
v4.2.0,get block start and block len for reduce scatter
v4.2.0,sync global data sumup info
v4.2.0,global sumup reduce
v4.2.0,copy back
v4.2.0,set global sumup info
v4.2.0,init global data count in leaf
v4.2.0,reset hist num bits according to global num data
v4.2.0,sync global data sumup info
v4.2.0,global sumup reduce
v4.2.0,copy back
v4.2.0,set global sumup info
v4.2.0,init global data count in leaf
v4.2.0,clear histogram buffer before synchronizing
v4.2.0,otherwise histogram contents from the previous iteration will be sent
v4.2.0,construct local histograms
v4.2.0,copy to buffer
v4.2.0,Reduce scatter for histogram
v4.2.0,restore global histograms from buffer
v4.2.0,only root leaf
v4.2.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v4.2.0,find local best split for larger leaf
v4.2.0,sync global best info
v4.2.0,set best split
v4.2.0,need update global number of data in leaf
v4.2.0,reset hist num bits according to global num data
v4.2.0,"instantiate template classes, otherwise linker cannot find the code"
v4.2.0,initialize SerialTreeLearner
v4.2.0,some additional variables needed for GPU trainer
v4.2.0,Initialize GPU buffers and kernels
v4.2.0,some functions used for debugging the GPU histogram construction
v4.2.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v4.2.0,goto err;
v4.2.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v4.2.0,also guarantee that there are at least 2K examples per workgroup
v4.2.0,return 0;
v4.2.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v4.2.0,decide the best number of workgroups working on one feature4 tuple
v4.2.0,set work group size based on feature size
v4.2.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v4.2.0,we need to refresh the kernel arguments after reallocating
v4.2.0,The only argument that needs to be changed later is num_data_
v4.2.0,"the GPU kernel will process all features in one call, and each"
v4.2.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v4.2.0,process one feature4 tuple
v4.2.0,"for the root node, indices are not copied"
v4.2.0,"for constant hessian, hessians are not copied except for the root node"
v4.2.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v4.2.0,and we will launch num_feature workgroups for this kernel
v4.2.0,will launch threads for all features
v4.2.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v4.2.0,copy the results asynchronously. Size depends on if double precision is used
v4.2.0,we will wait for this object in WaitAndGetHistograms
v4.2.0,"when the output is ready, the computation is done"
v4.2.0,values of this feature has been redistributed to multiple bins; need a reduction here
v4.2.0,how many feature-group tuples we have
v4.2.0,leave some safe margin for prefetching
v4.2.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v4.2.0,clear sparse/dense maps
v4.2.0,do nothing if no features can be processed on GPU
v4.2.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v4.2.0,unpin old buffer if necessary before destructing them
v4.2.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v4.2.0,allocate space for gradients and Hessians on device
v4.2.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v4.2.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v4.2.0,copy indices to the device
v4.2.0,histogram bin entry size depends on the precision (single/double)
v4.2.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v4.2.0,each work group generates a sub-histogram of dword_features_ features.
v4.2.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v4.2.0,create atomic counters for inter-group coordination
v4.2.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v4.2.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v4.2.0,looking for dword_features_ non-sparse feature-groups
v4.2.0,decide if we need to redistribute the bin
v4.2.0,multiplier must be a power of 2
v4.2.0,device_bin_mults_.push_back(1);
v4.2.0,found
v4.2.0,for data transfer time
v4.2.0,"Now generate new data structure feature4, and copy data to the device"
v4.2.0,"preallocate arrays for all threads, and pin them"
v4.2.0,building Feature4 bundles; each thread handles dword_features_ features
v4.2.0,one feature datapoint is 4 bits
v4.2.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.2.0,one feature datapoint is one byte
v4.2.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.2.0,Dense bin
v4.2.0,Dense 4-bit bin
v4.2.0,working on the remaining (less than dword_features_) feature groups
v4.2.0,fill the leftover features
v4.2.0,"fill this empty feature with some ""random"" value"
v4.2.0,"fill this empty feature with some ""random"" value"
v4.2.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v4.2.0,deallocate pinned space for feature copying
v4.2.0,data transfer time
v4.2.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v4.2.0,"Something bad happened. Just return ""No log available."""
v4.2.0,"build is okay, log may contain warnings"
v4.2.0,destroy any old kernels
v4.2.0,create OpenCL kernels for different number of workgroups per feature
v4.2.0,currently we don't use constant memory
v4.2.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v4.2.0,kernel with indices in an array
v4.2.0,"kernel with all features enabled, with eliminated branches"
v4.2.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v4.2.0,do nothing if no features can be processed on GPU
v4.2.0,The only argument that needs to be changed later is num_data_
v4.2.0,"hessian is passed as a parameter, but it is not available now."
v4.2.0,hessian will be set in BeforeTrain()
v4.2.0,"Get the max bin size, used for selecting best GPU kernel"
v4.2.0,initialize GPU
v4.2.0,determine which kernel to use based on the max number of bins
v4.2.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.2.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.2.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.2.0,ignore the feature groups that contain categorical features when producing warnings about max_bin.
v4.2.0,"these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin."
v4.2.0,setup GPU kernel arguments after we allocating all the buffers
v4.2.0,GPU memory has to been reallocated because data may have been changed
v4.2.0,setup GPU kernel arguments after we allocating all the buffers
v4.2.0,Copy initial full hessians and gradients to GPU.
v4.2.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v4.2.0,setup hessian parameters only
v4.2.0,hessian is passed as a parameter
v4.2.0,use bagging
v4.2.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v4.2.0,copy used gradients and Hessians to ordered buffer
v4.2.0,transfer the indices to GPU
v4.2.0,transfer hessian to GPU
v4.2.0,setup hessian parameters only
v4.2.0,hessian is passed as a parameter
v4.2.0,transfer gradients to GPU
v4.2.0,only have root
v4.2.0,"Copy indices, gradients and Hessians as early as possible"
v4.2.0,only need to initialize for smaller leaf
v4.2.0,Get leaf boundary
v4.2.0,copy indices to the GPU:
v4.2.0,copy ordered Hessians to the GPU:
v4.2.0,copy ordered gradients to the GPU:
v4.2.0,do nothing if no features can be processed on GPU
v4.2.0,copy data indices if it is not null
v4.2.0,generate and copy ordered_gradients if gradients is not null
v4.2.0,generate and copy ordered_hessians if Hessians is not null
v4.2.0,converted indices in is_feature_used to feature-group indices
v4.2.0,construct the feature masks for dense feature-groups
v4.2.0,"if no feature group is used, just return and do not use GPU"
v4.2.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v4.2.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v4.2.0,"All data have been prepared, now run the GPU kernel"
v4.2.0,construct smaller leaf
v4.2.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v4.2.0,then construct sparse features on CPU
v4.2.0,"wait for GPU to finish, only if GPU is actually used"
v4.2.0,use double precision
v4.2.0,use single precision
v4.2.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v4.2.0,#define GPU_DEBUG_COMPARE
v4.2.0,construct larger leaf
v4.2.0,then construct sparse features on CPU
v4.2.0,"wait for GPU to finish, only if GPU is actually used"
v4.2.0,use double precision
v4.2.0,use single precision
v4.2.0,do some sanity check for the GPU algorithm
v4.2.0,limit top k
v4.2.0,get max bin
v4.2.0,calculate buffer size
v4.2.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.2.0,"left and right on same time, so need double size"
v4.2.0,initialize histograms for global
v4.2.0,sync global data sumup info
v4.2.0,set global sumup info
v4.2.0,init global data count in leaf
v4.2.0,get local sumup
v4.2.0,get local sumup
v4.2.0,get mean number on machines
v4.2.0,weighted gain
v4.2.0,get top k
v4.2.0,"Copy histogram to buffer, and Get local aggregate features"
v4.2.0,copy histograms.
v4.2.0,copy smaller leaf histograms first
v4.2.0,mark local aggregated feature
v4.2.0,copy
v4.2.0,then copy larger leaf histograms
v4.2.0,mark local aggregated feature
v4.2.0,copy
v4.2.0,use local data to find local best splits
v4.2.0,clear histogram buffer before synchronizing
v4.2.0,otherwise histogram contents from the previous iteration will be sent
v4.2.0,find splits
v4.2.0,only has root leaf
v4.2.0,local voting
v4.2.0,gather
v4.2.0,get all top-k from all machines
v4.2.0,global voting
v4.2.0,copy local histgrams to buffer
v4.2.0,Reduce scatter for histogram
v4.2.0,find best split from local aggregated histograms
v4.2.0,restore from buffer
v4.2.0,restore from buffer
v4.2.0,find local best
v4.2.0,find local best split for larger leaf
v4.2.0,sync global best info
v4.2.0,copy back
v4.2.0,set the global number of data for leaves
v4.2.0,init the global sumup info
v4.2.0,"instantiate template classes, otherwise linker cannot find the code"
v4.2.0,allocate CUDA memory
v4.2.0,leave some space for alignment
v4.2.0,input best split info
v4.2.0,for leaf information update
v4.2.0,"gather information for CPU, used for launching kernels"
v4.2.0,for leaf splits information update
v4.2.0,we need restore the order of indices in cuda_data_indices_
v4.2.0,allocate more memory for sum reduction in CUDA
v4.2.0,only the first element records the final sum
v4.2.0,intialize split find task information (a split find task is one pass through the histogram of a feature)
v4.2.0,need to double the size of histogram buffer in global memory when using double precision in histogram construction
v4.2.0,use only half the size of histogram buffer in global memory when quantized training since each gradient and hessian takes only 2 bytes
v4.2.0,use the first gpu by default
v4.2.0,"std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values"
v4.2.0,use feature interaction constraint or sample features by node
v4.1.0,coding: utf-8
v4.1.0,create predictor first
v4.1.0,setting early stopping via global params should be possible
v4.1.0,reduce cost for prediction training data
v4.1.0,process callbacks
v4.1.0,construct booster
v4.1.0,start training
v4.1.0,check evaluation result.
v4.1.0,"ranking task, split according to groups"
v4.1.0,run preprocessing on the data set if needed
v4.1.0,setting early stopping via global params should be possible
v4.1.0,setup callbacks
v4.1.0,coding: utf-8
v4.1.0,dummy function to support older version of scikit-learn
v4.1.0,coding: utf-8
v4.1.0,"f(labels, preds)"
v4.1.0,"f(labels, preds, weights)"
v4.1.0,"f(labels, preds, weights, group)"
v4.1.0,"f(labels, preds)"
v4.1.0,"f(labels, preds, weights)"
v4.1.0,"f(labels, preds, weights, group)"
v4.1.0,documentation templates for LGBMModel methods are shared between the classes in
v4.1.0,this module and those in the ``dask`` module
v4.1.0,register default metric for consistency with callable eval_metric case
v4.1.0,try to deduce from class instance
v4.1.0,overwrite default metric by explicitly set metric
v4.1.0,"use joblib conventions for negative n_jobs, just like scikit-learn"
v4.1.0,"at predict time, this is handled later due to the order of parameter updates"
v4.1.0,Do not modify original args in fit function
v4.1.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v4.1.0,Separate built-in from callable evaluation metrics
v4.1.0,concatenate metric from params (or default if not provided in params) and eval_metric
v4.1.0,copy for consistency
v4.1.0,reduce cost for prediction training data
v4.1.0,free dataset
v4.1.0,retrive original params that possibly can be used in both training and prediction
v4.1.0,and then overwrite them (considering aliases) with params that were passed directly in prediction
v4.1.0,number of threads can have values with special meaning which is only applied
v4.1.0,"in the scikit-learn interface, these should not reach the c++ side as-is"
v4.1.0,adjust eval metrics to match whether binary or multiclass
v4.1.0,classification is being performed
v4.1.0,"do not modify args, as it causes errors in model selection tools"
v4.1.0,check group data
v4.1.0,coding: utf-8
v4.1.0,coding: utf-8
v4.1.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v4.1.0,coding: utf-8
v4.1.0,coding: utf-8
v4.1.0,we don't need lib_lightgbm while building docs
v4.1.0,TypeError: obj is not a string or a number
v4.1.0,ValueError: invalid literal
v4.1.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v4.1.0,"lazy evaluation to allow import without dynamic library, e.g., for docs generation"
v4.1.0,"if buffer length is not long enough, re-allocate a buffer"
v4.1.0,avoid side effects on passed-in parameters
v4.1.0,"if main_param_name was provided, keep that value and remove all aliases"
v4.1.0,"if main param name was not found, search for an alias"
v4.1.0,"neither of main_param_name, aliases were found"
v4.1.0,determine feature names
v4.1.0,determine categorical features
v4.1.0,get numpy representation of the data
v4.1.0,most common case (no nullable dtypes)
v4.1.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.1.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.1.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.1.0,Get total row number.
v4.1.0,Random access by row index. Used for data sampling.
v4.1.0,Range data access. Used to read data in batch when constructing Dataset.
v4.1.0,Optionally specify batch_size to control range data read size.
v4.1.0,Only required if using ``Dataset.subset()``.
v4.1.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.1.0,avoid memory consumption by arrays concatenation operations
v4.1.0,create numpy array from output arrays
v4.1.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v4.1.0,for CSC there is extra column added
v4.1.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v4.1.0,same shape as input csr or csc matrix except extra column for expected value
v4.1.0,note: make sure we copy data as it will be deallocated next
v4.1.0,"free the temporary native indptr, indices, and data"
v4.1.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.1.0,avoid memory consumption by arrays concatenation operations
v4.1.0,c type: double**
v4.1.0,each double* element points to start of each column of sample data.
v4.1.0,c type int**
v4.1.0,each int* points to start of indices for each column
v4.1.0,"no min_data, nthreads and verbose in this function"
v4.1.0,check data has header or not
v4.1.0,need to regroup init_score
v4.1.0,process for args
v4.1.0,get categorical features
v4.1.0,"If the params[cat_alias] is equal to categorical_indices, do not report the warning."
v4.1.0,process for reference dataset
v4.1.0,start construct data
v4.1.0,set feature names
v4.1.0,"Select sampled rows, transpose to column order."
v4.1.0,create validation dataset from ref_dataset
v4.1.0,create valid
v4.1.0,construct subset
v4.1.0,create train
v4.1.0,could be updated if data is not freed
v4.1.0,set to None
v4.1.0,we're done if self and reference share a common upstream reference
v4.1.0,most common case (no nullable dtypes)
v4.1.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.1.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.1.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.1.0,"if buffer length is not long enough, reallocate buffers"
v4.1.0,"group data from LightGBM is boundaries data, need to convert to group size"
v4.1.0,Training task
v4.1.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v4.1.0,construct booster object
v4.1.0,copy the parameters from train_set
v4.1.0,save reference to data
v4.1.0,buffer for inner predict
v4.1.0,Prediction task
v4.1.0,"if buffer length is not long enough, re-allocate a buffer"
v4.1.0,if a single node tree it won't have `leaf_index` so return 0
v4.1.0,"Create the node record, and populate universal data members"
v4.1.0,Update values to reflect node type (leaf or split)
v4.1.0,traverse the next level of the tree
v4.1.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v4.1.0,and we add node to the list.
v4.1.0,need reset training data
v4.1.0,need to push new valid data
v4.1.0,ensure that existing Booster is freed before replacing it
v4.1.0,with a new one createdfrom file
v4.1.0,"if buffer length is not long enough, re-allocate a buffer"
v4.1.0,"if buffer length is not long enough, reallocate a buffer"
v4.1.0,Copy models
v4.1.0,Get name of features
v4.1.0,"if buffer length is not long enough, reallocate buffers"
v4.1.0,avoid to predict many time in one iteration
v4.1.0,Get num of inner evals
v4.1.0,Get name of eval metrics
v4.1.0,"if buffer length is not long enough, reallocate buffers"
v4.1.0,coding: utf-8
v4.1.0,Callback environment used by callbacks
v4.1.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.1.0,self.best_score_list is initialized to an empty list
v4.1.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.1.0,coding: utf-8
v4.1.0,Acquire port in worker
v4.1.0,schedule futures to retrieve each element of the tuple
v4.1.0,retrieve ports
v4.1.0,Concatenate many parts into one
v4.1.0,construct local eval_set data.
v4.1.0,store indices of eval_set components that were not contained within local parts.
v4.1.0,consolidate parts of each individual eval component.
v4.1.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v4.1.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v4.1.0,filter padding from eval parts then _concat each eval_set component.
v4.1.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v4.1.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v4.1.0,capture whether local_listen_port or its aliases were provided
v4.1.0,capture whether machines or its aliases were provided
v4.1.0,Some passed-in parameters can be removed:
v4.1.0,* 'num_machines': set automatically from Dask worker list
v4.1.0,* 'num_threads': overridden to match nthreads on each Dask process
v4.1.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v4.1.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v4.1.0,X and y are each delayed sub-lists of original eval dask Collections.
v4.1.0,find maximum number of parts in an individual eval set so that we can
v4.1.0,pad eval sets when they come in different sizes.
v4.1.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v4.1.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v4.1.0,first time a chunk of this eval set is added to this part.
v4.1.0,append additional chunks of this eval set to this part.
v4.1.0,ensure that all evaluation parts map uniquely to one part.
v4.1.0,assign sub-eval_set components to worker parts.
v4.1.0,Start computation in the background
v4.1.0,trigger error locally
v4.1.0,Find locations of all parts and map them to particular Dask workers
v4.1.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v4.1.0,data artifacts may not be populated depending on worker returning final estimator.
v4.1.0,assign general validation set settings to fit kwargs.
v4.1.0,resolve aliases for network parameters and pop the result off params.
v4.1.0,these values are added back in calls to `_train_part()`
v4.1.0,figure out network params
v4.1.0,Tell each worker to train on the parts that it has locally
v4.1.0,
v4.1.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v4.1.0,1. there is randomness in the training process unless parameters ``seed``
v4.1.0,and ``deterministic`` are set
v4.1.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v4.1.0,relies on global state (it and all the other LightGBM training processes
v4.1.0,coordinate with each other)
v4.1.0,"if network parameters were changed during training, remove them from the"
v4.1.0,returned model so that they're generated dynamically on every run based
v4.1.0,on the Dask cluster you're connected to and which workers have pieces of
v4.1.0,the training data
v4.1.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v4.1.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v4.1.0,are returned as a list of sparse matrices (one per class)
v4.1.0,"pred_contrib output will have one column per feature,"
v4.1.0,plus one more for the base value
v4.1.0,need to tell Dask the expected type and shape of individual preds
v4.1.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v4.1.0,the code below is used instead to ensure that the sparse type is preserved during concatentation
v4.1.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v4.1.0,Concatenate them to return a list of Dask Arrays.
v4.1.0,"DaskLGBMClassifier does not support group, eval_group."
v4.1.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v4.1.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group."
v4.1.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v4.1.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v4.1.0,DaskLGBMRanker support for callbacks and init_model is not tested
v4.1.0,coding: utf-8
v4.1.0,load or create your dataset
v4.1.0,create dataset for lightgbm
v4.1.0,"if you want to re-use data, remember to set free_raw_data=False"
v4.1.0,specify your configurations as a dict
v4.1.0,generate feature names
v4.1.0,feature_name and categorical_feature
v4.1.0,check feature name
v4.1.0,save model to file
v4.1.0,dump model to JSON (and save to file)
v4.1.0,feature names
v4.1.0,feature importances
v4.1.0,load model to predict
v4.1.0,can only predict with the best iteration (or the saving iteration)
v4.1.0,eval with loaded model
v4.1.0,dump model with pickle
v4.1.0,load model with pickle to predict
v4.1.0,can predict with any iteration when loaded in pickle way
v4.1.0,eval with loaded model
v4.1.0,continue training
v4.1.0,init_model accepts:
v4.1.0,1. model file name
v4.1.0,2. Booster()
v4.1.0,decay learning rates
v4.1.0,reset_parameter callback accepts:
v4.1.0,1. list with length = num_boost_round
v4.1.0,2. function(curr_iter)
v4.1.0,change other parameters during training
v4.1.0,self-defined objective function
v4.1.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v4.1.0,log likelihood loss
v4.1.0,self-defined eval metric
v4.1.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.1.0,binary error
v4.1.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.1.0,This may make built-in evaluation metric calculate wrong results
v4.1.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.1.0,Keep this in mind when you use the customization
v4.1.0,Pass custom objective function through params
v4.1.0,another self-defined eval metric
v4.1.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.1.0,accuracy
v4.1.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.1.0,This may make built-in evaluation metric calculate wrong results
v4.1.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.1.0,Keep this in mind when you use the customization
v4.1.0,Pass custom objective function through params
v4.1.0,callback
v4.1.0,coding: utf-8
v4.1.0,load or create your dataset
v4.1.0,train
v4.1.0,predict
v4.1.0,eval
v4.1.0,feature importances
v4.1.0,self-defined eval metric
v4.1.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.1.0,Root Mean Squared Logarithmic Error (RMSLE)
v4.1.0,train
v4.1.0,another self-defined eval metric
v4.1.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.1.0,Relative Absolute Error (RAE)
v4.1.0,train
v4.1.0,predict
v4.1.0,eval
v4.1.0,other scikit-learn modules
v4.1.0,coding: utf-8
v4.1.0,load or create your dataset
v4.1.0,create dataset for lightgbm
v4.1.0,specify your configurations as a dict
v4.1.0,train
v4.1.0,coding: utf-8
v4.1.0,################
v4.1.0,Simulate some binary data with a single categorical and
v4.1.0,single continuous predictor
v4.1.0,################
v4.1.0,Set up a couple of utilities for our experiments
v4.1.0,################
v4.1.0,Observe the behavior of `binary` and `xentropy` objectives
v4.1.0,Trying this throws an error on non-binary values of y:
v4.1.0,"experiment('binary', label_type='probability', DATA)"
v4.1.0,The speed of `binary` is not drastically different than
v4.1.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v4.1.0,there are reasons to suspect that `binary` should run faster when the
v4.1.0,label is an integer instead of a float
v4.1.0,coding: utf-8
v4.1.0,load or create your dataset
v4.1.0,create dataset for lightgbm
v4.1.0,specify your configurations as a dict
v4.1.0,train
v4.1.0,save model to file
v4.1.0,predict
v4.1.0,eval
v4.1.0,We can also open HDF5 file once and get access to
v4.1.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v4.1.0,
v4.1.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v4.1.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v4.1.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v4.1.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v4.1.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v4.1.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v4.1.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v4.1.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v4.1.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v4.1.0,Here we are just creating a chunk size that matches with batch_size.
v4.1.0,
v4.1.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v4.1.0,lightgbm Dataset.
v4.1.0,Save to 2 HDF5 files for demonstration.
v4.1.0,We can store multiple datasets inside a single HDF5 file.
v4.1.0,Separating X and Y for choosing best chunk size for data loading.
v4.1.0,split training data into two partitions
v4.1.0,make this array dense because we're splitting across
v4.1.0,a sparse boundary to partition the data
v4.1.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v4.1.0,predictions and target values back from workers to the client
v4.1.0,
v4.1.0,"for larger datasets, consider the metrics from dask-ml instead"
v4.1.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v4.1.0,coding: utf-8
v4.1.0,!/usr/bin/env python3
v4.1.0,-*- coding: utf-8 -*-
v4.1.0,
v4.1.0,"LightGBM documentation build configuration file, created by"
v4.1.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v4.1.0,
v4.1.0,This file is execfile()d with the current directory set to its
v4.1.0,containing dir.
v4.1.0,
v4.1.0,Note that not all possible configuration values are present in this
v4.1.0,autogenerated file.
v4.1.0,
v4.1.0,All configuration values have a default; values that are commented out
v4.1.0,serve to show the default.
v4.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
v4.1.0,add these directories to sys.path here. If the directory is relative to the
v4.1.0,"documentation root, use os.path.abspath to make it absolute."
v4.1.0,-- General configuration ------------------------------------------------
v4.1.0,"If your documentation needs a minimal Sphinx version, state it here."
v4.1.0,"Add any Sphinx extension module names here, as strings. They can be"
v4.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v4.1.0,ones.
v4.1.0,mock out modules
v4.1.0,hide type hints in API docs
v4.1.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v4.1.0,Only the class' docstring is inserted.
v4.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v4.1.0,The master toctree document.
v4.1.0,General information about the project.
v4.1.0,The name of an image file (relative to this directory) to place at the top
v4.1.0,of the sidebar.
v4.1.0,The name of an image file (relative to this directory) to use as a favicon of
v4.1.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v4.1.0,pixels large.
v4.1.0,"The version info for the project you're documenting, acts as replacement for"
v4.1.0,"|version| and |release|, also used in various other places throughout the"
v4.1.0,built documents.
v4.1.0,The short X.Y version.
v4.1.0,"The full version, including alpha/beta/rc tags."
v4.1.0,The language for content autogenerated by Sphinx. Refer to documentation
v4.1.0,for a list of supported languages.
v4.1.0,
v4.1.0,This is also used if you do content translation via gettext catalogs.
v4.1.0,"Usually you set ""language"" from the command line for these cases."
v4.1.0,"List of patterns, relative to source directory, that match files and"
v4.1.0,directories to ignore when looking for source files.
v4.1.0,This patterns also effect to html_static_path and html_extra_path
v4.1.0,The name of the Pygments (syntax highlighting) style to use.
v4.1.0,-- Configuration for C API docs generation ------------------------------
v4.1.0,-- Options for HTML output ----------------------------------------------
v4.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v4.1.0,a list of builtin themes.
v4.1.0,Theme options are theme-specific and customize the look and feel of a theme
v4.1.0,"further.  For a list of options available for each theme, see the"
v4.1.0,documentation.
v4.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
v4.1.0,"relative to this directory. They are copied after the builtin static files,"
v4.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v4.1.0,-- Options for HTMLHelp output ------------------------------------------
v4.1.0,Output file base name for HTML help builder.
v4.1.0,-- Options for LaTeX output ---------------------------------------------
v4.1.0,The name of an image file (relative to this directory) to place at the top of
v4.1.0,the title page.
v4.1.0,intersphinx configuration
v4.1.0,Warning! The following code can cause buffer overflows on RTD.
v4.1.0,Consider suppressing output completely if RTD project silently fails.
v4.1.0,Refer to https://github.com/svenevs/exhale
v4.1.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.1.0,Warning! The following code can cause buffer overflows on RTD.
v4.1.0,Consider suppressing output completely if RTD project silently fails.
v4.1.0,Refer to https://github.com/svenevs/exhale
v4.1.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.1.0,coding: utf-8
v4.1.0,This is a basic test for floating number parsing.
v4.1.0,Most of the test cases come from:
v4.1.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v4.1.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v4.1.0,FLT_MAX
v4.1.0,FLT_MIN
v4.1.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v4.1.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v4.1.0,Near DBL_MIN
v4.1.0,DBL_MIN 2^-1022
v4.1.0,The behavior for parsing -nan depends on implementation.
v4.1.0,Thus we skip binary check for negative nan.
v4.1.0,See comment in test_cases.
v4.1.0,construct sample data first (use all data for convenience and since size is small)
v4.1.0,Load some test data
v4.1.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.1.0,Add some fake initial_scores and groups so we can test streaming them
v4.1.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.1.0,Load some test data
v4.1.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.1.0,Add some fake initial_scores and groups so we can test streaming them
v4.1.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.1.0,Load some test data
v4.1.0,Serialize the reference
v4.1.0,Deserialize the reference
v4.1.0,Confirm 1 successful API call
v4.1.0,Free memory
v4.1.0,Test that Data() points to first value written
v4.1.0,Constants
v4.1.0,Start with some content:
v4.1.0,Clear & re-use:
v4.1.0,Output should match new content:
v4.1.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v4.1.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v4.1.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v4.1.0,Compute a new trial address & value & if it is a valid address:
v4.1.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v4.1.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v4.1.0,Check the just-stored value with getitem():
v4.1.0,Also store the just-stored value for future tracking:
v4.1.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v4.1.0,Test in 2 ways that the values are correctly laid out in memory:
v4.1.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.1.0,Use multiple threads to test concurrency
v4.1.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.1.0,Calculate expected boundaries
v4.1.0,Extract a set of rows from the column-based format (still maintaining column based format)
v4.1.0,coding: utf-8
v4.1.0,coding: utf-8
v4.1.0,check saved model persistence
v4.1.0,"we need to check the consistency of model file here, so test for exact equal"
v4.1.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v4.1.0,"scores likely to be different, but prediction should still be the same"
v4.1.0,test that shape is checked during prediction
v4.1.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v4.1.0,The following is for demo and testing purpose.
v4.1.0,whole col
v4.1.0,half col
v4.1.0,Create dataset from numpy array directly.
v4.1.0,Create dataset using Sequence.
v4.1.0,Test for validation set.
v4.1.0,Select some random rows as valid data.
v4.1.0,"From Dataset constructor, with dataset from numpy array."
v4.1.0,"From Dataset.create_valid, with dataset from sequence."
v4.1.0,test that method works even with free_raw_data=True
v4.1.0,test that method works but sets raw data to None in case of immergeable data types
v4.1.0,test that method works for different data types
v4.1.0,"Set extremely harsh penalties, so CEGB will block most splits."
v4.1.0,"Compare pairs of penalties, to ensure scaling works as intended"
v4.1.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v4.1.0,"should resolve duplicate aliases, and prefer the main parameter"
v4.1.0,should choose the highest priority alias and set that value on main param
v4.1.0,if only aliases are used
v4.1.0,should use the default if main param and aliases are missing
v4.1.0,all changes should be made on copies and not modify the original
v4.1.0,preserves None found for main param and still removes aliases
v4.1.0,correctly chooses value when only an alias is provided
v4.1.0,adds None if that's given as the default and param not found
v4.1.0,If callable is found in objective
v4.1.0,Value in params should be preferred to the default_value passed from keyword arguments
v4.1.0,"None of objective or its aliases in params, but default_value is callable."
v4.1.0,check that the original data wasn't modified
v4.1.0,check that the built data has the codes
v4.1.0,test using defined feature names
v4.1.0,test using default feature names
v4.1.0,check for feature indices outside of range
v4.1.0,coding: utf-8
v4.1.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v4.1.0,set_index ensures partitions are based on group id.
v4.1.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v4.1.0,"separate target, weight from features."
v4.1.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v4.1.0,"so that within each partition, sum(g) = n_samples."
v4.1.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v4.1.0,make one categorical feature relevant to the target
v4.1.0,https://github.com/microsoft/LightGBM/issues/4118
v4.1.0,extra predict() parameters should be passed through correctly
v4.1.0,pref_leaf values should have the right shape
v4.1.0,and values that look like valid tree nodes
v4.1.0,"be sure LightGBM actually used at least one categorical column,"
v4.1.0,and that it was correctly treated as a categorical feature
v4.1.0,shape depends on whether it is binary or multiclass classification
v4.1.0,"in the special case of multi-class classification using scipy sparse matrices,"
v4.1.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v4.1.0,
v4.1.0,"since that case is so different than all other cases, check the relevant things here"
v4.1.0,and then return early
v4.1.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v4.1.0,"be sure LightGBM actually used at least one categorical column,"
v4.1.0,and that it was correctly treated as a categorical feature
v4.1.0,* shape depends on whether it is binary or multiclass classification
v4.1.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v4.1.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v4.1.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v4.1.0,that the output has the right shape and base values are in the right position
v4.1.0,"with a custom objective, prediction result is a raw score instead of predicted class"
v4.1.0,function should have been preserved
v4.1.0,should correctly classify every sample
v4.1.0,probability estimates should be similar
v4.1.0,Scores should be the same
v4.1.0,Predictions should be roughly the same.
v4.1.0,pref_leaf values should have the right shape
v4.1.0,and values that look like valid tree nodes
v4.1.0,extra predict() parameters should be passed through correctly
v4.1.0,"be sure LightGBM actually used at least one categorical column,"
v4.1.0,and that it was correctly treated as a categorical feature
v4.1.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v4.1.0,that the output has the right shape and base values are in the right position
v4.1.0,"be sure LightGBM actually used at least one categorical column,"
v4.1.0,and that it was correctly treated as a categorical feature
v4.1.0,Quantiles should be right
v4.1.0,"be sure LightGBM actually used at least one categorical column,"
v4.1.0,and that it was correctly treated as a categorical feature
v4.1.0,function should have been preserved
v4.1.0,Scores should be the same
v4.1.0,local and Dask predictions should be the same
v4.1.0,predictions should be better than random
v4.1.0,rebalance small dask.Array dataset for better performance.
v4.1.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v4.1.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v4.1.0,distributed ranker should be able to rank decently well and should
v4.1.0,have high rank correlation with scores from serial ranker.
v4.1.0,extra predict() parameters should be passed through correctly
v4.1.0,pref_leaf values should have the right shape
v4.1.0,and values that look like valid tree nodes
v4.1.0,"be sure LightGBM actually used at least one categorical column,"
v4.1.0,and that it was correctly treated as a categorical feature
v4.1.0,rebalance small dask.Array dataset for better performance.
v4.1.0,distributed ranker should be able to rank decently well with the least-squares objective
v4.1.0,and should have high rank correlation with scores from serial ranker.
v4.1.0,function should have been preserved
v4.1.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v4.1.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v4.1.0,"test eval_class_weight, eval_init_score on binary-classification task."
v4.1.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v4.1.0,create eval_sets by creating new datasets or copying training data.
v4.1.0,total number of trees scales up for ova classifier.
v4.1.0,check that early stopping was not applied.
v4.1.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v4.1.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v4.1.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v4.1.0,should be able to use the class without specifying a client
v4.1.0,should be able to set client after construction
v4.1.0,data on cluster1
v4.1.0,create identical data on cluster2
v4.1.0,"at this point, the result of default_client() is client2 since it was the most recently"
v4.1.0,created. So setting client to client1 here to test that you can select a non-default client
v4.1.0,"unfitted model should survive pickling round trip, and pickling"
v4.1.0,shouldn't have side effects on the model object
v4.1.0,client will always be None after unpickling
v4.1.0,"fitted model should survive pickling round trip, and pickling"
v4.1.0,shouldn't have side effects on the model object
v4.1.0,client will always be None after unpickling
v4.1.0,rebalance data to be sure that each worker has a piece of the data
v4.1.0,model 1 - no network parameters given
v4.1.0,model 2 - machines given
v4.1.0,model 3 - local_listen_port given
v4.1.0,training should fail because LightGBM will try to use the same
v4.1.0,port for multiple worker processes on the same machine
v4.1.0,rebalance data to be sure that each worker has a piece of the data
v4.1.0,"test that ""machines"" is actually respected by creating a socket that uses"
v4.1.0,"one of the ports mentioned in ""machines"""
v4.1.0,The above error leaves a worker waiting
v4.1.0,"an informative error should be raised if ""machines"" has duplicates"
v4.1.0,"""client"" should be the only different, and the final argument"
v4.1.0,value of the root node is 0 when init_score is set
v4.1.0,this test is separate because it takes a not-yet-constructed estimator
v4.1.0,coding: utf-8
v4.1.0,coding: utf-8
v4.1.0,"build target, group ID vectors."
v4.1.0,build y/target and group-id vectors with user-specified group sizes.
v4.1.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v4.1.0,groups should contain > 1 element for pairwise learning objective.
v4.1.0,"build feature data, X. Transform first few into informative features."
v4.1.0,coding: utf-8
v4.1.0,check that really dummy objective was used and estimator didn't learn anything
v4.1.0,prediction result is actually not transformed (is raw) due to custom objective
v4.1.0,original estimator is unaffected
v4.1.0,"new estimator is unfitted, but has the same parameters"
v4.1.0,Test if random_state is properly stored
v4.1.0,Test if two random states produce identical models
v4.1.0,Test if subsequent fits sample from random_state object and produce different models
v4.1.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v4.1.0,With default params
v4.1.0,Tests same probabilities
v4.1.0,Tests same predictions
v4.1.0,Tests same raw scores
v4.1.0,Tests same leaf indices
v4.1.0,Tests same feature contributions
v4.1.0,Tests other parameters for the prediction works
v4.1.0,Tests start_iteration
v4.1.0,"Tests same probabilities, starting from iteration 10"
v4.1.0,"Tests same predictions, starting from iteration 10"
v4.1.0,"Tests same raw scores, starting from iteration 10"
v4.1.0,"Tests same leaf indices, starting from iteration 10"
v4.1.0,"Tests same feature contributions, starting from iteration 10"
v4.1.0,"Tests other parameters for the prediction works, starting from iteration 10"
v4.1.0,test that params passed in predict have higher priority
v4.1.0,"no custom objective, no custom metric"
v4.1.0,default metric
v4.1.0,non-default metric
v4.1.0,no metric
v4.1.0,non-default metric in eval_metric
v4.1.0,non-default metric with non-default metric in eval_metric
v4.1.0,non-default metric with multiple metrics in eval_metric
v4.1.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v4.1.0,default metric for non-default objective
v4.1.0,non-default metric for non-default objective
v4.1.0,no metric
v4.1.0,non-default metric in eval_metric for non-default objective
v4.1.0,non-default metric with non-default metric in eval_metric for non-default objective
v4.1.0,non-default metric with multiple metrics in eval_metric for non-default objective
v4.1.0,"custom objective, no custom metric"
v4.1.0,default regression metric for custom objective
v4.1.0,non-default regression metric for custom objective
v4.1.0,multiple regression metrics for custom objective
v4.1.0,no metric
v4.1.0,default regression metric with non-default metric in eval_metric for custom objective
v4.1.0,non-default regression metric with metric in eval_metric for custom objective
v4.1.0,multiple regression metrics with metric in eval_metric for custom objective
v4.1.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v4.1.0,"no custom objective, custom metric"
v4.1.0,default metric with custom metric
v4.1.0,non-default metric with custom metric
v4.1.0,multiple metrics with custom metric
v4.1.0,custom metric (disable default metric)
v4.1.0,default metric for non-default objective with custom metric
v4.1.0,non-default metric for non-default objective with custom metric
v4.1.0,multiple metrics for non-default objective with custom metric
v4.1.0,custom metric (disable default metric for non-default objective)
v4.1.0,"custom objective, custom metric"
v4.1.0,custom metric for custom objective
v4.1.0,non-default regression metric with custom metric for custom objective
v4.1.0,multiple regression metrics with custom metric for custom objective
v4.1.0,default metric and invalid binary metric is replaced with multiclass alternative
v4.1.0,invalid binary metric is replaced with multiclass alternative
v4.1.0,default metric for non-default multiclass objective
v4.1.0,and invalid binary metric is replaced with multiclass alternative
v4.1.0,default metric and invalid multiclass metric is replaced with binary alternative
v4.1.0,invalid multiclass metric is replaced with binary alternative for custom objective
v4.1.0,"Verify that can receive a list of metrics, only callable"
v4.1.0,Verify that can receive a list of custom and built-in metrics
v4.1.0,Verify that works as expected when eval_metric is empty
v4.1.0,"Verify that can receive a list of metrics, only built-in"
v4.1.0,Verify that eval_metric is robust to receiving a list with None
v4.1.0,feval
v4.1.0,single eval_set
v4.1.0,two eval_set
v4.1.0,'val_minus_two' here is the expected number of threads for n_jobs=-2
v4.1.0,"Note: according to joblib's formula, a value of n_jobs=-2 means"
v4.1.0,"""use all but one thread"" (formula: n_cpus + 1 + n_jobs)"
v4.1.0,try to predict with a different feature
v4.1.0,check that disabling the check doesn't raise the error
v4.1.0,"make weights and init_score same types as y, just to avoid"
v4.1.0,a huge number of combinations and therefore test cases
v4.1.0,"make weights and init_score same types as y, just to avoid"
v4.1.0,a huge number of combinations and therefore test cases
v4.1.0,coding: utf-8
v4.1.0,we're in a leaf now
v4.1.0,check that the rest of the elements have black color
v4.1.0,check that we got to the expected leaf
v4.1.0,coding: utf-8
v4.1.0,coding: utf-8
v4.1.0,check that default gives same result as k = 1
v4.1.0,check against independent calculation for k = 1
v4.1.0,check against independent calculation for k = 2
v4.1.0,check against independent calculation for k = 10
v4.1.0,check cases where predictions are equal
v4.1.0,should give same result as binary auc for 2 classes
v4.1.0,test the case where all predictions are equal
v4.1.0,test that weighted data gives different auc_mu
v4.1.0,test that equal data weights give same auc_mu as unweighted data
v4.1.0,should give 1 when accuracy = 1
v4.1.0,test loading class weights
v4.1.0,Simulates position bias for a given ranking dataset.
v4.1.0,The ouput dataset is identical to the input one with the exception for the relevance labels.
v4.1.0,The new labels are generated according to an instance of a cascade user model:
v4.1.0,"for each query, the user is simulated to be traversing the list of documents ranked by a baseline ranker"
v4.1.0,"(in our example it is simply the ordering by some feature correlated with relevance, e.g., 34)"
v4.1.0,and clicks on that document (new_label=1) with some probability 'pclick' depending on its true relevance;
v4.1.0,"at each position the user may stop the traversal with some probability pstop. For the non-clicked documents,"
v4.1.0,new_label=0. Thus the generated new labels are biased towards the baseline ranker.
v4.1.0,"The positions of the documents in the ranked lists produced by the baseline, are returned."
v4.1.0,a mapping of a document's true relevance (defined on a 5-grade scale) into the probability of clicking it
v4.1.0,an instantiation of a cascade model where the user stops with probability 0.2 after observing each document
v4.1.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.1.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.1.0,add extra row to position file
v4.1.0,simulate position bias for the train dataset and put the train dataset with biased labels to temp directory
v4.1.0,test setting positions through Dataset constructor with numpy array
v4.1.0,the performance of the unbiased LambdaMART should outperform the plain LambdaMART on the dataset with position bias
v4.1.0,test setting positions through Dataset constructor with pandas Series
v4.1.0,test setting positions through set_position
v4.1.0,test get_position works
v4.1.0,no early stopping
v4.1.0,early stopping occurs
v4.1.0,regular early stopping
v4.1.0,positive min_delta
v4.1.0,test custom eval metrics
v4.1.0,"shuffle = False, override metric in params"
v4.1.0,"shuffle = True, callbacks"
v4.1.0,enable display training loss
v4.1.0,self defined folds
v4.1.0,LambdaRank
v4.1.0,... with l2 metric
v4.1.0,... with NDCG (default) metric
v4.1.0,self defined folds with lambdarank
v4.1.0,init_model from an in-memory Booster
v4.1.0,init_model from a text file
v4.1.0,predictions should be identical
v4.1.0,with early stopping
v4.1.0,predict by each fold booster
v4.1.0,check that each booster predicted using the best iteration
v4.1.0,fold averaging
v4.1.0,without early stopping
v4.1.0,test feature_names with whitespaces
v4.1.0,This has non-ascii strings.
v4.1.0,check that passing parameters to the constructor raises warning and ignores them
v4.1.0,take subsets and train
v4.1.0,generate CSR sparse dataset
v4.1.0,convert data to dense and get back same contribs
v4.1.0,validate the values are the same
v4.1.0,validate using CSC matrix
v4.1.0,validate the values are the same
v4.1.0,generate CSR sparse dataset
v4.1.0,convert data to dense and get back same contribs
v4.1.0,validate the values are the same
v4.1.0,validate using CSC matrix
v4.1.0,validate the values are the same
v4.1.0,Note there is an extra column added to the output for the expected value
v4.1.0,Note output CSC shape should be same as CSR output shape
v4.1.0,test sliced labels
v4.1.0,append some columns
v4.1.0,append some rows
v4.1.0,test sliced 2d matrix
v4.1.0,test sliced CSR
v4.1.0,trees start at position 1.
v4.1.0,split_features are in 4th line.
v4.1.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v4.1.0,The penalization is so high that the first 2 features should not be used here
v4.1.0,Check that a very high penalization is the same as not using the features at all
v4.1.0,check refit accepts dataset_params
v4.1.0,the following checks that dart and rf with mape can predict outside the 0-1 range
v4.1.0,https://github.com/microsoft/LightGBM/issues/1579
v4.1.0,"no custom objective, no feval"
v4.1.0,default metric
v4.1.0,non-default metric in params
v4.1.0,default metric in args
v4.1.0,non-default metric in args
v4.1.0,metric in args overwrites one in params
v4.1.0,metric in args overwrites one in params
v4.1.0,multiple metrics in params
v4.1.0,multiple metrics in args
v4.1.0,remove default metric by 'None' in list
v4.1.0,remove default metric by 'None' aliases
v4.1.0,"custom objective, no feval"
v4.1.0,no default metric
v4.1.0,metric in params
v4.1.0,metric in args
v4.1.0,metric in args overwrites its' alias in params
v4.1.0,multiple metrics in params
v4.1.0,multiple metrics in args
v4.1.0,"no custom objective, feval"
v4.1.0,default metric with custom one
v4.1.0,non-default metric in params with custom one
v4.1.0,default metric in args with custom one
v4.1.0,non-default metric in args with custom one
v4.1.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.1.0,multiple metrics in params with custom one
v4.1.0,multiple metrics in args with custom one
v4.1.0,custom metric is evaluated despite 'None' is passed
v4.1.0,"custom objective, feval"
v4.1.0,"no default metric, only custom one"
v4.1.0,metric in params with custom one
v4.1.0,metric in args with custom one
v4.1.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.1.0,multiple metrics in params with custom one
v4.1.0,multiple metrics in args with custom one
v4.1.0,custom metric is evaluated despite 'None' is passed
v4.1.0,"no custom objective, no feval"
v4.1.0,default metric
v4.1.0,default metric in params
v4.1.0,non-default metric in params
v4.1.0,multiple metrics in params
v4.1.0,remove default metric by 'None' aliases
v4.1.0,"custom objective, no feval"
v4.1.0,no default metric
v4.1.0,metric in params
v4.1.0,multiple metrics in params
v4.1.0,"no custom objective, feval"
v4.1.0,default metric with custom one
v4.1.0,default metric in params with custom one
v4.1.0,non-default metric in params with custom one
v4.1.0,multiple metrics in params with custom one
v4.1.0,custom metric is evaluated despite 'None' is passed
v4.1.0,"custom objective, feval"
v4.1.0,"no default metric, only custom one"
v4.1.0,metric in params with custom one
v4.1.0,multiple metrics in params with custom one
v4.1.0,custom metric is evaluated despite 'None' is passed
v4.1.0,Custom objective replaces multiclass
v4.1.0,multiclass default metric
v4.1.0,multiclass default metric with custom one
v4.1.0,multiclass metric alias with custom one for custom objective
v4.1.0,no metric for invalid class_num
v4.1.0,custom metric for invalid class_num
v4.1.0,multiclass metric alias with custom one with invalid class_num
v4.1.0,multiclass default metric without num_class
v4.1.0,multiclass metric alias
v4.1.0,multiclass metric
v4.1.0,non-valid metric for multiclass objective
v4.1.0,non-default num_class for default objective
v4.1.0,no metric with non-default num_class for custom objective
v4.1.0,multiclass metric alias for custom objective
v4.1.0,multiclass metric for custom objective
v4.1.0,binary metric with non-default num_class for custom objective
v4.1.0,Expect three metrics but mean and stdv for each metric
v4.1.0,test XGBoost-style return value
v4.1.0,test numpy-style return value
v4.1.0,test bins string type
v4.1.0,test histogram is disabled for categorical features
v4.1.0,test for lgb.train
v4.1.0,test feval for lgb.train
v4.1.0,test with two valid data for lgb.train
v4.1.0,test for lgb.cv
v4.1.0,test feval for lgb.cv
v4.1.0,test that binning works properly for features with only positive or only negative values
v4.1.0,decreasing without freeing raw data is allowed
v4.1.0,decreasing before lazy init is allowed
v4.1.0,increasing is allowed
v4.1.0,decreasing with disabled filter is allowed
v4.1.0,decreasing with enabled filter is disallowed;
v4.1.0,also changes of other params are disallowed
v4.1.0,check extra trees increases regularization
v4.1.0,check path smoothing increases regularization
v4.1.0,test edge case with one leaf
v4.1.0,check that constraint containing all features is equivalent to no constraint
v4.1.0,check that constraint partitioning the features reduces train accuracy
v4.1.0,check that constraints consisting of single features reduce accuracy further
v4.1.0,test that interaction constraints work when not all features are used
v4.1.0,check that number of threads does not affect result
v4.1.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v4.1.0,test again with nans in data
v4.1.0,test again with bagging
v4.1.0,test with a feature that has only one non-nan value
v4.1.0,test with a categorical feature
v4.1.0,test refit: same results on same data
v4.1.0,test refit with save and load
v4.1.0,test refit: different results training on different data
v4.1.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v4.1.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v4.1.0,"test the case where start_iteration <= 0, and num_iteration is None"
v4.1.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v4.1.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v4.1.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v4.1.0,test for regression
v4.1.0,test both with and without early stopping
v4.1.0,test for multi-class
v4.1.0,test both with and without early stopping
v4.1.0,test for binary
v4.1.0,test both with and without early stopping
v4.1.0,test against sklearn average precision metric
v4.1.0,test that average precision is 1 where model predicts perfectly
v4.1.0,data as float64
v4.1.0,test all features were used
v4.1.0,test the score is better than predicting the mean
v4.1.0,test all predictions are equal using different input dtypes
v4.1.0,introduce some missing values
v4.1.0,the previous line turns x3 into object dtype in recent versions of pandas
v4.1.0,train with regular dtypes
v4.1.0,convert to nullable dtypes
v4.1.0,test training succeeds
v4.1.0,test all features were used
v4.1.0,test the score is better than predicting the mean
v4.1.0,test equal predictions
v4.1.0,test data are taken from bug report
v4.1.0,https://github.com/microsoft/LightGBM/issues/4708
v4.1.0,modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811
v4.1.0,and https://github.com/microsoft/LightGBM/pull/5087
v4.1.0,test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree
v4.1.0,which is done in the fix #5164
v4.1.0,without the fix:
v4.1.0,Check failed: (best_split_info.left_count) > (0)
v4.1.0,try to predict with a different feature
v4.1.0,check that disabling the check doesn't raise the error
v4.1.0,try to refit with a different feature
v4.1.0,check that disabling the check doesn't raise the error
v4.1.0,coding: utf-8
v4.1.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v4.1.0,Double-precision floats are only supported on x86_64 with PoCL
v4.1.0,coding: utf-8
v4.1.0,coding: utf-8
v4.1.0,These are helper functions to allow doing a stack unwind
v4.1.0,"after an R allocation error, which would trigger a long jump."
v4.1.0,convert from one-based to zero-based index
v4.1.0,"if any feature names were larger than allocated size,"
v4.1.0,allow for a larger size and try again
v4.1.0,convert from boundaries to size
v4.1.0,--- start Booster interfaces
v4.1.0,"if any eval names were larger than allocated size,"
v4.1.0,allow for a larger size and try again
v4.1.0,"if the model string was larger than the initial buffer, call the function again, writing directly to the R object"
v4.1.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.1.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.1.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.1.0,.Call() calls
v4.1.0,coding: utf-8
v4.1.0,alias table
v4.1.0,names
v4.1.0,from strings
v4.1.0,tails
v4.1.0,tails
v4.1.0,the following are stored as comma separated strings but are arrays in the wrappers
v4.1.0,coding: utf-8
v4.1.0,Single row predictor to abstract away caching logic
v4.1.0,create boosting
v4.1.0,initialize the boosting
v4.1.0,create objective function
v4.1.0,initialize the objective function
v4.1.0,create training metric
v4.1.0,reset the boosting
v4.1.0,create objective function
v4.1.0,initialize the objective function
v4.1.0,calculate the nonzero data and indices size
v4.1.0,allocate data and indices arrays
v4.1.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.1.0,aggregated per row feature contribution results
v4.1.0,keep track of the row_vector sizes for parallelization
v4.1.0,copy vector results to output for each row
v4.1.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.1.0,aggregated per row feature contribution results
v4.1.0,calculate number of elements per column to construct
v4.1.0,the CSC matrix with random access
v4.1.0,keep track of column counts
v4.1.0,keep track of beginning index for each column
v4.1.0,keep track of beginning index for each matrix
v4.1.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v4.1.0,store the row index
v4.1.0,update column count
v4.1.0,explicitly declare symbols from LightGBM namespace
v4.1.0,some help functions used to convert data
v4.1.0,Row iterator of on column for CSC matrix
v4.1.0,"return value at idx, only can access by ascent order"
v4.1.0,"return next non-zero pair, if index < 0, means no more data"
v4.1.0,start of c_api functions
v4.1.0,This API is to keep python binding's behavior the same with C++ implementation.
v4.1.0,"Sample count, random seed etc. should be provided in parameters."
v4.1.0,convert internal thread id to be unique based on external thread id
v4.1.0,convert internal thread id to be unique based on external thread id
v4.1.0,sample data first
v4.1.0,sample data first
v4.1.0,sample data first
v4.1.0,local buffer to re-use memory
v4.1.0,sample data first
v4.1.0,no more data
v4.1.0,---- start of booster
v4.1.0,Single row in row-major format:
v4.1.0,---- start of some help functions
v4.1.0,data is array of pointers to individual rows
v4.1.0,set number of threads for openmp
v4.1.0,read parameters from config file
v4.1.0,"remove str after ""#"""
v4.1.0,de-duplicate params
v4.1.0,prediction is needed if using input initial model(continued train)
v4.1.0,need to continue training
v4.1.0,sync up random seed for data partition
v4.1.0,load Training data
v4.1.0,load data for distributed training
v4.1.0,load data for single machine
v4.1.0,need save binary file
v4.1.0,create training metric
v4.1.0,only when have metrics then need to construct validation data
v4.1.0,"Add validation data, if it exists"
v4.1.0,add
v4.1.0,need save binary file
v4.1.0,add metric for validation data
v4.1.0,output used time on each iteration
v4.1.0,need init network
v4.1.0,create boosting
v4.1.0,create objective function
v4.1.0,load training data
v4.1.0,initialize the objective function
v4.1.0,initialize the boosting
v4.1.0,add validation data into boosting
v4.1.0,convert model to if-else statement code
v4.1.0,create predictor
v4.1.0,Free memory
v4.1.0,create predictor
v4.1.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v4.1.0,counts for all labels
v4.1.0,"start from top label, and accumulate DCG"
v4.1.0,counts for all labels
v4.1.0,calculate k Max DCG by one pass
v4.1.0,get sorted indices by score
v4.1.0,calculate multi dcg by one pass
v4.1.0,wait for all client start up
v4.1.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v4.1.0,Instead we will handle finalize/abort for MPI in main().
v4.1.0,default set to -1
v4.1.0,"distance at k-th communication, distance[k] = 2^k"
v4.1.0,set incoming rank at k-th commuication
v4.1.0,set outgoing rank at k-th commuication
v4.1.0,default set as -1
v4.1.0,construct all recursive halving map for all machines
v4.1.0,let 1 << k <= num_machines
v4.1.0,distance of each communication
v4.1.0,"if num_machines = 2^k, don't need to group machines"
v4.1.0,"communication direction, %2 == 0 is positive"
v4.1.0,neighbor at k-th communication
v4.1.0,receive data block at k-th communication
v4.1.0,send data block at k-th communication
v4.1.0,"if num_machines != 2^k, need to group machines"
v4.1.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v4.1.0,let left machine as group leader
v4.1.0,"cache block information for groups, group with 2 machines will have double block size"
v4.1.0,convert from group to node leader
v4.1.0,convert from node to group
v4.1.0,meet new group
v4.1.0,add block len for this group
v4.1.0,calculate the group block start
v4.1.0,not need to construct
v4.1.0,get receive block information
v4.1.0,accumulate block len
v4.1.0,get send block information
v4.1.0,accumulate block len
v4.1.0,static member definition
v4.1.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v4.1.0,assign the blocks to every rank.
v4.1.0,do reduce scatter
v4.1.0,do all gather
v4.1.0,assign blocks
v4.1.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v4.1.0,copy back
v4.1.0,assign blocks
v4.1.0,start all gather
v4.1.0,when num_machines is small and data is large
v4.1.0,use output as receive buffer
v4.1.0,get current local block size
v4.1.0,get out rank
v4.1.0,get in rank
v4.1.0,get send information
v4.1.0,get recv information
v4.1.0,send and recv at same time
v4.1.0,rotate in-place
v4.1.0,use output as receive buffer
v4.1.0,get current local block size
v4.1.0,get send information
v4.1.0,get recv information
v4.1.0,send and recv at same time
v4.1.0,use output as receive buffer
v4.1.0,send and recv at same time
v4.1.0,send local data to neighbor first
v4.1.0,receive neighbor data first
v4.1.0,reduce
v4.1.0,get target
v4.1.0,get send information
v4.1.0,get recv information
v4.1.0,send and recv at same time
v4.1.0,reduce
v4.1.0,send result to neighbor
v4.1.0,receive result from neighbor
v4.1.0,copy result
v4.1.0,start up socket
v4.1.0,parse clients from file
v4.1.0,get ip list of local machine
v4.1.0,get local rank
v4.1.0,construct listener
v4.1.0,construct communication topo
v4.1.0,construct linkers
v4.1.0,free listener
v4.1.0,set timeout
v4.1.0,accept incoming socket
v4.1.0,receive rank
v4.1.0,add new socket
v4.1.0,save ranks that need to connect with
v4.1.0,start listener
v4.1.0,start connect
v4.1.0,let smaller rank connect to larger rank
v4.1.0,send local rank
v4.1.0,wait for listener
v4.1.0,print connected linkers
v4.1.0,only need to copy subset
v4.1.0,avoid to copy subset many times
v4.1.0,avoid out of range
v4.1.0,may need to recopy subset
v4.1.0,valid the type
v4.1.0,parser factory implementation.
v4.1.0,customized parser add-on.
v4.1.0,save header to parser config in case needed.
v4.1.0,save label id to parser config in case needed.
v4.1.0,Constructors
v4.1.0,Get type tag
v4.1.0,Comparisons
v4.1.0,"This has to be separate, not in Statics, because Json() accesses"
v4.1.0,statics().null.
v4.1.0,"advance until next line, or end of input"
v4.1.0,advance until closing tokens
v4.1.0,The usual case: non-escaped characters
v4.1.0,Handle escapes
v4.1.0,Extract 4-byte escape sequence
v4.1.0,Explicitly check length of the substring. The following loop
v4.1.0,relies on std::string returning the terminating NUL when
v4.1.0,accessing str[length]. Checking here reduces brittleness.
v4.1.0,JSON specifies that characters outside the BMP shall be encoded as a
v4.1.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v4.1.0,components. Check whether we're in the middle of such a beast: the
v4.1.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v4.1.0,a trail (low) surrogate.
v4.1.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v4.1.0,per the UTF-16 algorithm.
v4.1.0,Integer part
v4.1.0,Decimal part
v4.1.0,Exponent part
v4.1.0,Check for any trailing garbage
v4.1.0,Documented in json11.hpp
v4.1.0,Check for another object
v4.1.0,get column names
v4.1.0,"support to get header from parser config, so could utilize following label name to id mapping logic."
v4.1.0,load label idx first
v4.1.0,"if parser config file exists, feature names may be changed after customized parser applied."
v4.1.0,clear here so could use default filled feature names during dataset construction.
v4.1.0,may improve by saving real feature names defined in parser in the future.
v4.1.0,erase label column name
v4.1.0,load ignore columns
v4.1.0,load weight idx
v4.1.0,load group idx
v4.1.0,don't support query id in data file when using distributed training
v4.1.0,read data to memory
v4.1.0,sample data
v4.1.0,construct feature bin mappers & clear sample data
v4.1.0,initialize label
v4.1.0,extract features
v4.1.0,sample data from file
v4.1.0,construct feature bin mappers & clear sample data
v4.1.0,initialize label
v4.1.0,extract features
v4.1.0,load data from binary file
v4.1.0,checks whether there's a initial score file when loaded from binary data files
v4.1.0,"the intial score file should with suffix "".bin.init"""
v4.1.0,check meta data
v4.1.0,need to check training data
v4.1.0,read data in memory
v4.1.0,initialize label
v4.1.0,extract features
v4.1.0,Get number of lines of data file
v4.1.0,initialize label
v4.1.0,extract features
v4.1.0,load data from binary file
v4.1.0,checks whether there's a initial score file when loaded from binary data files
v4.1.0,"the intial score file should with suffix "".bin.init"""
v4.1.0,not need to check validation data
v4.1.0,check meta data
v4.1.0,check token
v4.1.0,read feature group definitions
v4.1.0,read feature size
v4.1.0,buffer to read binary file
v4.1.0,check token
v4.1.0,read size of header
v4.1.0,re-allocate space if not enough
v4.1.0,read header
v4.1.0,get header
v4.1.0,read size of meta data
v4.1.0,re-allocate space if not enough
v4.1.0,read meta data
v4.1.0,load meta data
v4.1.0,sample local used data if need to partition
v4.1.0,"if not contain query file, minimal sample unit is one record"
v4.1.0,"if contain query file, minimal sample unit is one query"
v4.1.0,if is new query
v4.1.0,read feature data
v4.1.0,read feature size
v4.1.0,re-allocate space if not enough
v4.1.0,raw data
v4.1.0,fill feature_names_ if not header
v4.1.0,get forced split
v4.1.0,"if only one machine, find bin locally"
v4.1.0,"if have multi-machines, need to find bin distributed"
v4.1.0,different machines will find bin for different features
v4.1.0,start and len will store the process feature indices for different machines
v4.1.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.1.0,free
v4.1.0,gather global feature bin mappers
v4.1.0,restore features bins from buffer
v4.1.0,---- private functions ----
v4.1.0,get header
v4.1.0,num_groups
v4.1.0,real_feature_idx_
v4.1.0,feature2group
v4.1.0,feature2subfeature
v4.1.0,group_bin_boundaries
v4.1.0,group_feature_start_
v4.1.0,group_feature_cnt_
v4.1.0,get feature names
v4.1.0,get forced_bin_bounds_
v4.1.0,"if features are ordered, not need to use hist_buf"
v4.1.0,read all lines
v4.1.0,get query data
v4.1.0,"if not contain query data, minimal sample unit is one record"
v4.1.0,"if contain query data, minimal sample unit is one query"
v4.1.0,if is new query
v4.1.0,get query data
v4.1.0,"if not contain query file, minimal sample unit is one record"
v4.1.0,"if contain query file, minimal sample unit is one query"
v4.1.0,if is new query
v4.1.0,parse features
v4.1.0,get forced split
v4.1.0,"check the range of label_idx, weight_idx and group_idx"
v4.1.0,"skip label check if user input parser config file,"
v4.1.0,because label id is got from raw features while dataset features are consistent with customized parser.
v4.1.0,fill feature_names_ if not header
v4.1.0,start find bins
v4.1.0,"if only one machine, find bin locally"
v4.1.0,start and len will store the process feature indices for different machines
v4.1.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.1.0,free
v4.1.0,gather global feature bin mappers
v4.1.0,restore features bins from buffer
v4.1.0,if doesn't need to prediction with initial model
v4.1.0,parser
v4.1.0,set label
v4.1.0,free processed line:
v4.1.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v4.1.0,text_reader_->Lines()[i].shrink_to_fit();
v4.1.0,push data
v4.1.0,if is used feature
v4.1.0,if need to prediction with initial model
v4.1.0,parser
v4.1.0,set initial score
v4.1.0,set label
v4.1.0,free processed line:
v4.1.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v4.1.0,text_reader_->Lines()[i].shrink_to_fit();
v4.1.0,push data
v4.1.0,if is used feature
v4.1.0,metadata_ will manage space of init_score
v4.1.0,text data can be free after loaded feature values
v4.1.0,parser
v4.1.0,set initial score
v4.1.0,set label
v4.1.0,push data
v4.1.0,if is used feature
v4.1.0,only need part of data
v4.1.0,need full data
v4.1.0,metadata_ will manage space of init_score
v4.1.0,read size of token
v4.1.0,remove duplicates
v4.1.0,deep copy function for BinMapper
v4.1.0,mean size for one bin
v4.1.0,need a new bin
v4.1.0,update bin upper bound
v4.1.0,last bin upper bound
v4.1.0,get number of positive and negative distinct values
v4.1.0,include zero bounds and infinity bound
v4.1.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v4.1.0,find remaining bounds
v4.1.0,find distinct_values first
v4.1.0,push zero in the front
v4.1.0,use the large value
v4.1.0,push zero in the back
v4.1.0,convert to int type first
v4.1.0,sort by counts in descending order
v4.1.0,will ignore the categorical of small counts
v4.1.0,Push the dummy bin for NaN
v4.1.0,Use MissingType::None to represent this bin contains all categoricals
v4.1.0,fix count of NaN bin
v4.1.0,check trivial(num_bin_ == 1) feature
v4.1.0,check useless bin
v4.1.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v4.1.0,so use most_freq_bin_ = default_bin_ when there is not so sparse
v4.1.0,calculate max bin of all features to select the int type in MultiValDenseBin
v4.1.0,"for lambdarank, it needs query data for partition data in distributed learning"
v4.1.0,need convert query_id to boundaries
v4.1.0,check weights
v4.1.0,check positions
v4.1.0,check query boundries
v4.1.0,contain initial score file
v4.1.0,check weights
v4.1.0,get local weights
v4.1.0,check positions
v4.1.0,get local positions
v4.1.0,check query boundries
v4.1.0,get local query boundaries
v4.1.0,contain initial score file
v4.1.0,get local initial scores
v4.1.0,re-calculate query weight
v4.1.0,save to nullptr
v4.1.0,"Note that len here is row count, not num_init_score, so we compare against num_data"
v4.1.0,"We need to use source_size here, because len might not equal size (due to a partially loaded dataset)"
v4.1.0,CUDA is handled after all insertions are complete
v4.1.0,CUDA is handled after all insertions are complete
v4.1.0,save to nullptr
v4.1.0,CUDA is handled after all insertions are complete
v4.1.0,save to nullptr
v4.1.0,save to nullptr
v4.1.0,CUDA is handled after all insertions are complete
v4.1.0,default weight file name
v4.1.0,default position file name
v4.1.0,default init_score file name
v4.1.0,use first line to count number class
v4.1.0,default query file name
v4.1.0,root is in the depth 0
v4.1.0,non-leaf
v4.1.0,leaf
v4.1.0,use this for the missing value conversion
v4.1.0,Predict func by Map to ifelse
v4.1.0,use this for the missing value conversion
v4.1.0,non-leaf
v4.1.0,left subtree
v4.1.0,right subtree
v4.1.0,leaf
v4.1.0,non-leaf
v4.1.0,left subtree
v4.1.0,right subtree
v4.1.0,leaf
v4.1.0,recursive computation of SHAP values for a decision tree
v4.1.0,extend the unique path
v4.1.0,leaf node
v4.1.0,internal node
v4.1.0,"see if we have already split on this feature,"
v4.1.0,if so we undo that split so we can redo it for this node
v4.1.0,recursive sparse computation of SHAP values for a decision tree
v4.1.0,extend the unique path
v4.1.0,leaf node
v4.1.0,internal node
v4.1.0,"see if we have already split on this feature,"
v4.1.0,if so we undo that split so we can redo it for this node
v4.1.0,add names of objective function if not providing metric
v4.1.0,equal weights for all classes
v4.1.0,generate seeds by seed.
v4.1.0,sort eval_at
v4.1.0,Only push the non-training data
v4.1.0,check for conflicts
v4.1.0,"check if objective, metric, and num_class match"
v4.1.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v4.1.0,Check max_depth and num_leaves
v4.1.0,"Fits in an int, and is more restrictive than the current num_leaves"
v4.1.0,"force col-wise for gpu, and cuda version"
v4.1.0,force row-wise for cuda version
v4.1.0,linear tree learner must be serial type and run on CPU device
v4.1.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v4.1.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v4.1.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v4.1.0,calculated split gain can be positive even with zero gradient and hessian.
v4.1.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v4.1.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v4.1.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v4.1.0,first round: fill the single val group
v4.1.0,always push the last group
v4.1.0,put dense feature first
v4.1.0,sort by non zero cnt
v4.1.0,"sort by non zero cnt, bigger first"
v4.1.0,shuffle groups
v4.1.0,Using std::swap for vector<bool> will cause the wrong result.
v4.1.0,get num_features
v4.1.0,get bin_mappers
v4.1.0,"for sparse multi value bin, we store the feature bin values with offset added"
v4.1.0,"for dense multi value bin, the feature bin values without offsets are used"
v4.1.0,copy feature bin mapper data
v4.1.0,copy feature bin mapper data
v4.1.0,update CUDA storage for column data and metadata
v4.1.0,"if not pass a filename, just append "".bin"" of original file"
v4.1.0,Write the basic header information for the dataset
v4.1.0,get size of meta data
v4.1.0,write meta data
v4.1.0,write feature data
v4.1.0,get size of feature
v4.1.0,write feature
v4.1.0,write raw data; use row-major order so we can read row-by-row
v4.1.0,Calculate approximate size of output and reserve space
v4.1.0,write feature group definitions
v4.1.0,"Give a little extra just in case, to avoid unnecessary resizes"
v4.1.0,"Write token that marks the data as binary reference, and the version"
v4.1.0,Write the basic definition of the overall dataset
v4.1.0,write feature group definitions
v4.1.0,get size of feature
v4.1.0,write feature
v4.1.0,size of feature names and forced bins
v4.1.0,write header
v4.1.0,write feature names
v4.1.0,write forced bins
v4.1.0,"explicitly initialize template methods, for cross module call"
v4.1.0,"explicitly initialize template methods, for cross module call"
v4.1.0,"Only one multi-val group, just simply merge"
v4.1.0,Skip the leading 0 when copying group_bin_boundaries.
v4.1.0,regenerate other fields
v4.1.0,need to iterate bin iterator
v4.1.0,is dense column
v4.1.0,is sparse column
v4.1.0,initialize the subset cuda column data
v4.1.0,"if one column has too many bins, use a separate partition for that column"
v4.1.0,try if adding this column exceed the maximum number per partition
v4.1.0,"if one column has too many bins, use a separate partition for that column"
v4.1.0,try if adding this column exceed the maximum number per partition
v4.1.0,store the importance first
v4.1.0,PredictRaw
v4.1.0,PredictRawByMap
v4.1.0,Predict
v4.1.0,PredictByMap
v4.1.0,PredictLeafIndex
v4.1.0,PredictLeafIndexByMap
v4.1.0,output model type
v4.1.0,output number of class
v4.1.0,output label index
v4.1.0,output max_feature_idx
v4.1.0,output objective
v4.1.0,output tree models
v4.1.0,store the importance first
v4.1.0,sort the importance
v4.1.0,use serialized string to restore this object
v4.1.0,Use first 128 chars to avoid exceed the message buffer.
v4.1.0,get number of classes
v4.1.0,get index of label
v4.1.0,get max_feature_idx first
v4.1.0,get average_output
v4.1.0,get feature names
v4.1.0,get monotone_constraints
v4.1.0,set zero
v4.1.0,predict all the trees for one iteration
v4.1.0,check early stopping
v4.1.0,set zero
v4.1.0,predict all the trees for one iteration
v4.1.0,check early stopping
v4.1.0,margin_threshold will be captured by value
v4.1.0,copy and sort
v4.1.0,margin_threshold will be captured by value
v4.1.0,Fix for compiler warnings about reaching end of control
v4.1.0,load forced_splits file
v4.1.0,init tree learner
v4.1.0,push training metrics
v4.1.0,get max feature index
v4.1.0,get label index
v4.1.0,get feature names
v4.1.0,get parser config file content
v4.1.0,check that forced splits does not use feature indices larger than dataset size
v4.1.0,"if need bagging, create buffer"
v4.1.0,"for a validation dataset, we need its score and metric"
v4.1.0,update score
v4.1.0,objective function will calculate gradients and hessians
v4.1.0,output used time per iteration
v4.1.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v4.1.0,boosting first
v4.1.0,use customized objective function
v4.1.0,need to copy customized gradients when using GOSS
v4.1.0,bagging logic
v4.1.0,need to copy gradients for bagging subset.
v4.1.0,shrinkage by learning rate
v4.1.0,update score
v4.1.0,only add default score one-time
v4.1.0,updates scores
v4.1.0,add model
v4.1.0,reset score
v4.1.0,remove model
v4.1.0,print message for metric
v4.1.0,pop last early_stopping_round_ models
v4.1.0,update training score
v4.1.0,we need to predict out-of-bag scores of data for boosting
v4.1.0,update validation score
v4.1.0,print training metric
v4.1.0,print validation metric
v4.1.0,set zero
v4.1.0,predict all the trees for one iteration
v4.1.0,predict all the trees for one iteration
v4.1.0,push training metrics
v4.1.0,"not same training data, need reset score and others"
v4.1.0,create score tracker
v4.1.0,update score
v4.1.0,resize gradient vectors to copy the customized gradients for goss or bagging with subset
v4.1.0,load forced_splits file
v4.1.0,"if exists initial score, will start from it"
v4.1.0,clear host score buffer
v4.1.0,Get the max size of pool
v4.1.0,at least need 2 leaves
v4.1.0,push split information for all leaves
v4.1.0,initialize splits for leaf
v4.1.0,initialize data partition
v4.1.0,initialize ordered gradients and hessians
v4.1.0,cannot change is_hist_col_wise during training
v4.1.0,initialize splits for leaf
v4.1.0,initialize data partition
v4.1.0,initialize ordered gradients and hessians
v4.1.0,Get the max size of pool
v4.1.0,at least need 2 leaves
v4.1.0,push split information for all leaves
v4.1.0,some initial works before training
v4.1.0,root leaf
v4.1.0,only root leaf can be splitted on first time
v4.1.0,some initial works before finding best split
v4.1.0,find best threshold for every feature
v4.1.0,Get a leaf with max split gain
v4.1.0,Get split information for best leaf
v4.1.0,"cannot split, quit"
v4.1.0,split tree with best leaf
v4.1.0,reset histogram pool
v4.1.0,initialize data partition
v4.1.0,reset the splits for leaves
v4.1.0,Sumup for root
v4.1.0,use all data
v4.1.0,"use bagging, only use part of data"
v4.1.0,check depth of current leaf
v4.1.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v4.1.0,no enough data to continue
v4.1.0,only have root
v4.1.0,put parent(left) leaf's histograms into larger leaf's histograms
v4.1.0,put parent(left) leaf's histograms to larger leaf's histograms
v4.1.0,construct smaller leaf
v4.1.0,construct larger leaf
v4.1.0,find splits
v4.1.0,only has root leaf
v4.1.0,start at root leaf
v4.1.0,Histogram construction require parent features.
v4.1.0,"then, compute own splits"
v4.1.0,split info should exist because searching in bfs fashion - should have added from parent
v4.1.0,update before tree split
v4.1.0,don't need to update this in data-based parallel model
v4.1.0,"split tree, will return right leaf"
v4.1.0,store the true split gain in tree model
v4.1.0,don't need to update this in data-based parallel model
v4.1.0,store the true split gain in tree model
v4.1.0,init the leaves that used on next iteration
v4.1.0,update leave outputs if needed
v4.1.0,bag_mapper[index_mapper[i]]
v4.1.0,it is needed to filter the features after the above code.
v4.1.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v4.1.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v4.1.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v4.1.0,find splits
v4.1.0,identify features containing nans
v4.1.0,preallocate the matrix used to calculate linear model coefficients
v4.1.0,"store only upper triangular half of matrix as an array, in row-major order"
v4.1.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v4.1.0,we add another 8 to ensure cache lines are not shared among processors
v4.1.0,some initial works before training
v4.1.0,root leaf
v4.1.0,only root leaf can be splitted on first time
v4.1.0,some initial works before finding best split
v4.1.0,find best threshold for every feature
v4.1.0,Get a leaf with max split gain
v4.1.0,Get split information for best leaf
v4.1.0,"cannot split, quit"
v4.1.0,split tree with best leaf
v4.1.0,map data to leaf number
v4.1.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v4.1.0,the coefficients vector is given by
v4.1.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v4.1.0,where:
v4.1.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v4.1.0,"H is the diagonal matrix of the hessian,"
v4.1.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v4.1.0,g is the vector of gradients
v4.1.0,the subscript _T denotes the transpose
v4.1.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v4.1.0,clear the coefficient matrices
v4.1.0,aggregate results from different threads
v4.1.0,copy into eigen matrices and solve
v4.1.0,update the tree properties
v4.1.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.1.0,get feature partition
v4.1.0,get local used features
v4.1.0,get best split at smaller leaf
v4.1.0,find local best split for larger leaf
v4.1.0,sync global best info
v4.1.0,update best split
v4.1.0,"instantiate template classes, otherwise linker cannot find the code"
v4.1.0,initialize SerialTreeLearner
v4.1.0,Get local rank and global machine size
v4.1.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.1.0,allocate buffer for communication
v4.1.0,get block start and block len for reduce scatter
v4.1.0,get buffer_write_start_pos
v4.1.0,get buffer_read_start_pos
v4.1.0,generate feature partition for current tree
v4.1.0,get local used feature
v4.1.0,get block start and block len for reduce scatter
v4.1.0,sync global data sumup info
v4.1.0,global sumup reduce
v4.1.0,copy back
v4.1.0,set global sumup info
v4.1.0,init global data count in leaf
v4.1.0,reset hist num bits according to global num data
v4.1.0,sync global data sumup info
v4.1.0,global sumup reduce
v4.1.0,copy back
v4.1.0,set global sumup info
v4.1.0,init global data count in leaf
v4.1.0,clear histogram buffer before synchronizing
v4.1.0,otherwise histogram contents from the previous iteration will be sent
v4.1.0,construct local histograms
v4.1.0,copy to buffer
v4.1.0,Reduce scatter for histogram
v4.1.0,restore global histograms from buffer
v4.1.0,only root leaf
v4.1.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v4.1.0,find local best split for larger leaf
v4.1.0,sync global best info
v4.1.0,set best split
v4.1.0,need update global number of data in leaf
v4.1.0,reset hist num bits according to global num data
v4.1.0,"instantiate template classes, otherwise linker cannot find the code"
v4.1.0,initialize SerialTreeLearner
v4.1.0,some additional variables needed for GPU trainer
v4.1.0,Initialize GPU buffers and kernels
v4.1.0,some functions used for debugging the GPU histogram construction
v4.1.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v4.1.0,goto err;
v4.1.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v4.1.0,also guarantee that there are at least 2K examples per workgroup
v4.1.0,return 0;
v4.1.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v4.1.0,decide the best number of workgroups working on one feature4 tuple
v4.1.0,set work group size based on feature size
v4.1.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v4.1.0,we need to refresh the kernel arguments after reallocating
v4.1.0,The only argument that needs to be changed later is num_data_
v4.1.0,"the GPU kernel will process all features in one call, and each"
v4.1.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v4.1.0,process one feature4 tuple
v4.1.0,"for the root node, indices are not copied"
v4.1.0,"for constant hessian, hessians are not copied except for the root node"
v4.1.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v4.1.0,and we will launch num_feature workgroups for this kernel
v4.1.0,will launch threads for all features
v4.1.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v4.1.0,copy the results asynchronously. Size depends on if double precision is used
v4.1.0,we will wait for this object in WaitAndGetHistograms
v4.1.0,"when the output is ready, the computation is done"
v4.1.0,values of this feature has been redistributed to multiple bins; need a reduction here
v4.1.0,how many feature-group tuples we have
v4.1.0,leave some safe margin for prefetching
v4.1.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v4.1.0,clear sparse/dense maps
v4.1.0,do nothing if no features can be processed on GPU
v4.1.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v4.1.0,unpin old buffer if necessary before destructing them
v4.1.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v4.1.0,allocate space for gradients and Hessians on device
v4.1.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v4.1.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v4.1.0,copy indices to the device
v4.1.0,histogram bin entry size depends on the precision (single/double)
v4.1.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v4.1.0,each work group generates a sub-histogram of dword_features_ features.
v4.1.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v4.1.0,create atomic counters for inter-group coordination
v4.1.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v4.1.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v4.1.0,looking for dword_features_ non-sparse feature-groups
v4.1.0,decide if we need to redistribute the bin
v4.1.0,multiplier must be a power of 2
v4.1.0,device_bin_mults_.push_back(1);
v4.1.0,found
v4.1.0,for data transfer time
v4.1.0,"Now generate new data structure feature4, and copy data to the device"
v4.1.0,"preallocate arrays for all threads, and pin them"
v4.1.0,building Feature4 bundles; each thread handles dword_features_ features
v4.1.0,one feature datapoint is 4 bits
v4.1.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.1.0,one feature datapoint is one byte
v4.1.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.1.0,Dense bin
v4.1.0,Dense 4-bit bin
v4.1.0,working on the remaining (less than dword_features_) feature groups
v4.1.0,fill the leftover features
v4.1.0,"fill this empty feature with some ""random"" value"
v4.1.0,"fill this empty feature with some ""random"" value"
v4.1.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v4.1.0,deallocate pinned space for feature copying
v4.1.0,data transfer time
v4.1.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v4.1.0,"Something bad happened. Just return ""No log available."""
v4.1.0,"build is okay, log may contain warnings"
v4.1.0,destroy any old kernels
v4.1.0,create OpenCL kernels for different number of workgroups per feature
v4.1.0,currently we don't use constant memory
v4.1.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v4.1.0,kernel with indices in an array
v4.1.0,"kernel with all features enabled, with eliminated branches"
v4.1.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v4.1.0,do nothing if no features can be processed on GPU
v4.1.0,The only argument that needs to be changed later is num_data_
v4.1.0,"hessian is passed as a parameter, but it is not available now."
v4.1.0,hessian will be set in BeforeTrain()
v4.1.0,"Get the max bin size, used for selecting best GPU kernel"
v4.1.0,initialize GPU
v4.1.0,determine which kernel to use based on the max number of bins
v4.1.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.1.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.1.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.1.0,ignore the feature groups that contain categorical features when producing warnings about max_bin.
v4.1.0,"these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin."
v4.1.0,setup GPU kernel arguments after we allocating all the buffers
v4.1.0,GPU memory has to been reallocated because data may have been changed
v4.1.0,setup GPU kernel arguments after we allocating all the buffers
v4.1.0,Copy initial full hessians and gradients to GPU.
v4.1.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v4.1.0,setup hessian parameters only
v4.1.0,hessian is passed as a parameter
v4.1.0,use bagging
v4.1.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v4.1.0,copy used gradients and Hessians to ordered buffer
v4.1.0,transfer the indices to GPU
v4.1.0,transfer hessian to GPU
v4.1.0,setup hessian parameters only
v4.1.0,hessian is passed as a parameter
v4.1.0,transfer gradients to GPU
v4.1.0,only have root
v4.1.0,"Copy indices, gradients and Hessians as early as possible"
v4.1.0,only need to initialize for smaller leaf
v4.1.0,Get leaf boundary
v4.1.0,copy indices to the GPU:
v4.1.0,copy ordered Hessians to the GPU:
v4.1.0,copy ordered gradients to the GPU:
v4.1.0,do nothing if no features can be processed on GPU
v4.1.0,copy data indices if it is not null
v4.1.0,generate and copy ordered_gradients if gradients is not null
v4.1.0,generate and copy ordered_hessians if Hessians is not null
v4.1.0,converted indices in is_feature_used to feature-group indices
v4.1.0,construct the feature masks for dense feature-groups
v4.1.0,"if no feature group is used, just return and do not use GPU"
v4.1.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v4.1.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v4.1.0,"All data have been prepared, now run the GPU kernel"
v4.1.0,construct smaller leaf
v4.1.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v4.1.0,then construct sparse features on CPU
v4.1.0,"wait for GPU to finish, only if GPU is actually used"
v4.1.0,use double precision
v4.1.0,use single precision
v4.1.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v4.1.0,#define GPU_DEBUG_COMPARE
v4.1.0,construct larger leaf
v4.1.0,then construct sparse features on CPU
v4.1.0,"wait for GPU to finish, only if GPU is actually used"
v4.1.0,use double precision
v4.1.0,use single precision
v4.1.0,do some sanity check for the GPU algorithm
v4.1.0,limit top k
v4.1.0,get max bin
v4.1.0,calculate buffer size
v4.1.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.1.0,"left and right on same time, so need double size"
v4.1.0,initialize histograms for global
v4.1.0,sync global data sumup info
v4.1.0,set global sumup info
v4.1.0,init global data count in leaf
v4.1.0,get local sumup
v4.1.0,get local sumup
v4.1.0,get mean number on machines
v4.1.0,weighted gain
v4.1.0,get top k
v4.1.0,"Copy histogram to buffer, and Get local aggregate features"
v4.1.0,copy histograms.
v4.1.0,copy smaller leaf histograms first
v4.1.0,mark local aggregated feature
v4.1.0,copy
v4.1.0,then copy larger leaf histograms
v4.1.0,mark local aggregated feature
v4.1.0,copy
v4.1.0,use local data to find local best splits
v4.1.0,clear histogram buffer before synchronizing
v4.1.0,otherwise histogram contents from the previous iteration will be sent
v4.1.0,find splits
v4.1.0,only has root leaf
v4.1.0,local voting
v4.1.0,gather
v4.1.0,get all top-k from all machines
v4.1.0,global voting
v4.1.0,copy local histgrams to buffer
v4.1.0,Reduce scatter for histogram
v4.1.0,find best split from local aggregated histograms
v4.1.0,restore from buffer
v4.1.0,restore from buffer
v4.1.0,find local best
v4.1.0,find local best split for larger leaf
v4.1.0,sync global best info
v4.1.0,copy back
v4.1.0,set the global number of data for leaves
v4.1.0,init the global sumup info
v4.1.0,"instantiate template classes, otherwise linker cannot find the code"
v4.1.0,allocate CUDA memory
v4.1.0,leave some space for alignment
v4.1.0,input best split info
v4.1.0,for leaf information update
v4.1.0,"gather information for CPU, used for launching kernels"
v4.1.0,for leaf splits information update
v4.1.0,we need restore the order of indices in cuda_data_indices_
v4.1.0,allocate more memory for sum reduction in CUDA
v4.1.0,only the first element records the final sum
v4.1.0,intialize split find task information (a split find task is one pass through the histogram of a feature)
v4.1.0,use the first gpu by default
v4.1.0,"std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values"
v4.1.0,use feature interaction constraint or sample features by node
v4.0.0,coding: utf-8
v4.0.0,create predictor first
v4.0.0,setting early stopping via global params should be possible
v4.0.0,reduce cost for prediction training data
v4.0.0,process callbacks
v4.0.0,construct booster
v4.0.0,start training
v4.0.0,check evaluation result.
v4.0.0,"ranking task, split according to groups"
v4.0.0,run preprocessing on the data set if needed
v4.0.0,setting early stopping via global params should be possible
v4.0.0,setup callbacks
v4.0.0,coding: utf-8
v4.0.0,dummy function to support older version of scikit-learn
v4.0.0,coding: utf-8
v4.0.0,"f(labels, preds)"
v4.0.0,"f(labels, preds, weights)"
v4.0.0,"f(labels, preds, weights, group)"
v4.0.0,"f(labels, preds)"
v4.0.0,"f(labels, preds, weights)"
v4.0.0,"f(labels, preds, weights, group)"
v4.0.0,documentation templates for LGBMModel methods are shared between the classes in
v4.0.0,this module and those in the ``dask`` module
v4.0.0,register default metric for consistency with callable eval_metric case
v4.0.0,try to deduce from class instance
v4.0.0,overwrite default metric by explicitly set metric
v4.0.0,"use joblib conventions for negative n_jobs, just like scikit-learn"
v4.0.0,"at predict time, this is handled later due to the order of parameter updates"
v4.0.0,Do not modify original args in fit function
v4.0.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v4.0.0,Separate built-in from callable evaluation metrics
v4.0.0,concatenate metric from params (or default if not provided in params) and eval_metric
v4.0.0,copy for consistency
v4.0.0,reduce cost for prediction training data
v4.0.0,free dataset
v4.0.0,retrive original params that possibly can be used in both training and prediction
v4.0.0,and then overwrite them (considering aliases) with params that were passed directly in prediction
v4.0.0,number of threads can have values with special meaning which is only applied
v4.0.0,"in the scikit-learn interface, these should not reach the c++ side as-is"
v4.0.0,adjust eval metrics to match whether binary or multiclass
v4.0.0,classification is being performed
v4.0.0,"do not modify args, as it causes errors in model selection tools"
v4.0.0,check group data
v4.0.0,coding: utf-8
v4.0.0,coding: utf-8
v4.0.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v4.0.0,coding: utf-8
v4.0.0,coding: utf-8
v4.0.0,we don't need lib_lightgbm while building docs
v4.0.0,TypeError: obj is not a string or a number
v4.0.0,ValueError: invalid literal
v4.0.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v4.0.0,"lazy evaluation to allow import without dynamic library, e.g., for docs generation"
v4.0.0,"if buffer length is not long enough, re-allocate a buffer"
v4.0.0,avoid side effects on passed-in parameters
v4.0.0,"if main_param_name was provided, keep that value and remove all aliases"
v4.0.0,"if main param name was not found, search for an alias"
v4.0.0,"neither of main_param_name, aliases were found"
v4.0.0,most common case (no nullable dtypes)
v4.0.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.0.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.0.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.0.0,Get total row number.
v4.0.0,Random access by row index. Used for data sampling.
v4.0.0,Range data access. Used to read data in batch when constructing Dataset.
v4.0.0,Optionally specify batch_size to control range data read size.
v4.0.0,Only required if using ``Dataset.subset()``.
v4.0.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.0.0,avoid memory consumption by arrays concatenation operations
v4.0.0,create numpy array from output arrays
v4.0.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v4.0.0,for CSC there is extra column added
v4.0.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v4.0.0,same shape as input csr or csc matrix except extra column for expected value
v4.0.0,note: make sure we copy data as it will be deallocated next
v4.0.0,"free the temporary native indptr, indices, and data"
v4.0.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v4.0.0,avoid memory consumption by arrays concatenation operations
v4.0.0,c type: double**
v4.0.0,each double* element points to start of each column of sample data.
v4.0.0,c type int**
v4.0.0,each int* points to start of indices for each column
v4.0.0,"no min_data, nthreads and verbose in this function"
v4.0.0,check data has header or not
v4.0.0,need to regroup init_score
v4.0.0,process for args
v4.0.0,get categorical features
v4.0.0,"If the params[cat_alias] is equal to categorical_indices, do not report the warning."
v4.0.0,process for reference dataset
v4.0.0,start construct data
v4.0.0,set feature names
v4.0.0,"Select sampled rows, transpose to column order."
v4.0.0,create validation dataset from ref_dataset
v4.0.0,create valid
v4.0.0,construct subset
v4.0.0,create train
v4.0.0,could be updated if data is not freed
v4.0.0,set to None
v4.0.0,we're done if self and reference share a common upstream reference
v4.0.0,most common case (no nullable dtypes)
v4.0.0,"1.0 <= pd version < 1.1 and nullable dtypes, least common case"
v4.0.0,raises error because array is casted to type(pd.NA) and there's no na_value argument
v4.0.0,"data has nullable dtypes, but we can specify na_value argument and copy will be made"
v4.0.0,"if buffer length is not long enough, reallocate buffers"
v4.0.0,"group data from LightGBM is boundaries data, need to convert to group size"
v4.0.0,Training task
v4.0.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v4.0.0,construct booster object
v4.0.0,copy the parameters from train_set
v4.0.0,save reference to data
v4.0.0,buffer for inner predict
v4.0.0,Prediction task
v4.0.0,"if buffer length is not long enough, re-allocate a buffer"
v4.0.0,if a single node tree it won't have `leaf_index` so return 0
v4.0.0,"Create the node record, and populate universal data members"
v4.0.0,Update values to reflect node type (leaf or split)
v4.0.0,traverse the next level of the tree
v4.0.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v4.0.0,and we add node to the list.
v4.0.0,need reset training data
v4.0.0,need to push new valid data
v4.0.0,"if buffer length is not long enough, re-allocate a buffer"
v4.0.0,"if buffer length is not long enough, reallocate a buffer"
v4.0.0,Copy models
v4.0.0,Get name of features
v4.0.0,"if buffer length is not long enough, reallocate buffers"
v4.0.0,avoid to predict many time in one iteration
v4.0.0,Get num of inner evals
v4.0.0,Get name of eval metrics
v4.0.0,"if buffer length is not long enough, reallocate buffers"
v4.0.0,coding: utf-8
v4.0.0,Callback environment used by callbacks
v4.0.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.0.0,self.best_score_list is initialized to an empty list
v4.0.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v4.0.0,coding: utf-8
v4.0.0,Acquire port in worker
v4.0.0,schedule futures to retrieve each element of the tuple
v4.0.0,retrieve ports
v4.0.0,Concatenate many parts into one
v4.0.0,construct local eval_set data.
v4.0.0,store indices of eval_set components that were not contained within local parts.
v4.0.0,consolidate parts of each individual eval component.
v4.0.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v4.0.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v4.0.0,filter padding from eval parts then _concat each eval_set component.
v4.0.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v4.0.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v4.0.0,capture whether local_listen_port or its aliases were provided
v4.0.0,capture whether machines or its aliases were provided
v4.0.0,Some passed-in parameters can be removed:
v4.0.0,* 'num_machines': set automatically from Dask worker list
v4.0.0,* 'num_threads': overridden to match nthreads on each Dask process
v4.0.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v4.0.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v4.0.0,X and y are each delayed sub-lists of original eval dask Collections.
v4.0.0,find maximum number of parts in an individual eval set so that we can
v4.0.0,pad eval sets when they come in different sizes.
v4.0.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v4.0.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v4.0.0,first time a chunk of this eval set is added to this part.
v4.0.0,append additional chunks of this eval set to this part.
v4.0.0,ensure that all evaluation parts map uniquely to one part.
v4.0.0,assign sub-eval_set components to worker parts.
v4.0.0,Start computation in the background
v4.0.0,trigger error locally
v4.0.0,Find locations of all parts and map them to particular Dask workers
v4.0.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v4.0.0,data artifacts may not be populated depending on worker returning final estimator.
v4.0.0,assign general validation set settings to fit kwargs.
v4.0.0,resolve aliases for network parameters and pop the result off params.
v4.0.0,these values are added back in calls to `_train_part()`
v4.0.0,figure out network params
v4.0.0,Tell each worker to train on the parts that it has locally
v4.0.0,
v4.0.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v4.0.0,1. there is randomness in the training process unless parameters ``seed``
v4.0.0,and ``deterministic`` are set
v4.0.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v4.0.0,relies on global state (it and all the other LightGBM training processes
v4.0.0,coordinate with each other)
v4.0.0,"if network parameters were changed during training, remove them from the"
v4.0.0,returned model so that they're generated dynamically on every run based
v4.0.0,on the Dask cluster you're connected to and which workers have pieces of
v4.0.0,the training data
v4.0.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v4.0.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v4.0.0,are returned as a list of sparse matrices (one per class)
v4.0.0,"pred_contrib output will have one column per feature,"
v4.0.0,plus one more for the base value
v4.0.0,need to tell Dask the expected type and shape of individual preds
v4.0.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v4.0.0,the code below is used instead to ensure that the sparse type is preserved during concatentation
v4.0.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v4.0.0,Concatenate them to return a list of Dask Arrays.
v4.0.0,"DaskLGBMClassifier does not support group, eval_group."
v4.0.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v4.0.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group."
v4.0.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v4.0.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v4.0.0,DaskLGBMRanker support for callbacks and init_model is not tested
v4.0.0,coding: utf-8
v4.0.0,load or create your dataset
v4.0.0,create dataset for lightgbm
v4.0.0,"if you want to re-use data, remember to set free_raw_data=False"
v4.0.0,specify your configurations as a dict
v4.0.0,generate feature names
v4.0.0,feature_name and categorical_feature
v4.0.0,check feature name
v4.0.0,save model to file
v4.0.0,dump model to JSON (and save to file)
v4.0.0,feature names
v4.0.0,feature importances
v4.0.0,load model to predict
v4.0.0,can only predict with the best iteration (or the saving iteration)
v4.0.0,eval with loaded model
v4.0.0,dump model with pickle
v4.0.0,load model with pickle to predict
v4.0.0,can predict with any iteration when loaded in pickle way
v4.0.0,eval with loaded model
v4.0.0,continue training
v4.0.0,init_model accepts:
v4.0.0,1. model file name
v4.0.0,2. Booster()
v4.0.0,decay learning rates
v4.0.0,reset_parameter callback accepts:
v4.0.0,1. list with length = num_boost_round
v4.0.0,2. function(curr_iter)
v4.0.0,change other parameters during training
v4.0.0,self-defined objective function
v4.0.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v4.0.0,log likelihood loss
v4.0.0,self-defined eval metric
v4.0.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.0.0,binary error
v4.0.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.0.0,This may make built-in evaluation metric calculate wrong results
v4.0.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.0.0,Keep this in mind when you use the customization
v4.0.0,Pass custom objective function through params
v4.0.0,another self-defined eval metric
v4.0.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v4.0.0,accuracy
v4.0.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v4.0.0,This may make built-in evaluation metric calculate wrong results
v4.0.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v4.0.0,Keep this in mind when you use the customization
v4.0.0,Pass custom objective function through params
v4.0.0,callback
v4.0.0,coding: utf-8
v4.0.0,load or create your dataset
v4.0.0,train
v4.0.0,predict
v4.0.0,eval
v4.0.0,feature importances
v4.0.0,self-defined eval metric
v4.0.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.0.0,Root Mean Squared Logarithmic Error (RMSLE)
v4.0.0,train
v4.0.0,another self-defined eval metric
v4.0.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v4.0.0,Relative Absolute Error (RAE)
v4.0.0,train
v4.0.0,predict
v4.0.0,eval
v4.0.0,other scikit-learn modules
v4.0.0,coding: utf-8
v4.0.0,load or create your dataset
v4.0.0,create dataset for lightgbm
v4.0.0,specify your configurations as a dict
v4.0.0,train
v4.0.0,coding: utf-8
v4.0.0,################
v4.0.0,Simulate some binary data with a single categorical and
v4.0.0,single continuous predictor
v4.0.0,################
v4.0.0,Set up a couple of utilities for our experiments
v4.0.0,################
v4.0.0,Observe the behavior of `binary` and `xentropy` objectives
v4.0.0,Trying this throws an error on non-binary values of y:
v4.0.0,"experiment('binary', label_type='probability', DATA)"
v4.0.0,The speed of `binary` is not drastically different than
v4.0.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v4.0.0,there are reasons to suspect that `binary` should run faster when the
v4.0.0,label is an integer instead of a float
v4.0.0,coding: utf-8
v4.0.0,load or create your dataset
v4.0.0,create dataset for lightgbm
v4.0.0,specify your configurations as a dict
v4.0.0,train
v4.0.0,save model to file
v4.0.0,predict
v4.0.0,eval
v4.0.0,We can also open HDF5 file once and get access to
v4.0.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v4.0.0,
v4.0.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v4.0.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v4.0.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v4.0.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v4.0.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v4.0.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v4.0.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v4.0.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v4.0.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v4.0.0,Here we are just creating a chunk size that matches with batch_size.
v4.0.0,
v4.0.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v4.0.0,lightgbm Dataset.
v4.0.0,Save to 2 HDF5 files for demonstration.
v4.0.0,We can store multiple datasets inside a single HDF5 file.
v4.0.0,Separating X and Y for choosing best chunk size for data loading.
v4.0.0,split training data into two partitions
v4.0.0,make this array dense because we're splitting across
v4.0.0,a sparse boundary to partition the data
v4.0.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v4.0.0,predictions and target values back from workers to the client
v4.0.0,
v4.0.0,"for larger datasets, consider the metrics from dask-ml instead"
v4.0.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v4.0.0,coding: utf-8
v4.0.0,!/usr/bin/env python3
v4.0.0,-*- coding: utf-8 -*-
v4.0.0,
v4.0.0,"LightGBM documentation build configuration file, created by"
v4.0.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v4.0.0,
v4.0.0,This file is execfile()d with the current directory set to its
v4.0.0,containing dir.
v4.0.0,
v4.0.0,Note that not all possible configuration values are present in this
v4.0.0,autogenerated file.
v4.0.0,
v4.0.0,All configuration values have a default; values that are commented out
v4.0.0,serve to show the default.
v4.0.0,"If extensions (or modules to document with autodoc) are in another directory,"
v4.0.0,add these directories to sys.path here. If the directory is relative to the
v4.0.0,"documentation root, use os.path.abspath to make it absolute."
v4.0.0,-- General configuration ------------------------------------------------
v4.0.0,"If your documentation needs a minimal Sphinx version, state it here."
v4.0.0,"Add any Sphinx extension module names here, as strings. They can be"
v4.0.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v4.0.0,ones.
v4.0.0,mock out modules
v4.0.0,hide type hints in API docs
v4.0.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v4.0.0,Only the class' docstring is inserted.
v4.0.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v4.0.0,The master toctree document.
v4.0.0,General information about the project.
v4.0.0,The name of an image file (relative to this directory) to place at the top
v4.0.0,of the sidebar.
v4.0.0,The name of an image file (relative to this directory) to use as a favicon of
v4.0.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v4.0.0,pixels large.
v4.0.0,"The version info for the project you're documenting, acts as replacement for"
v4.0.0,"|version| and |release|, also used in various other places throughout the"
v4.0.0,built documents.
v4.0.0,The short X.Y version.
v4.0.0,"The full version, including alpha/beta/rc tags."
v4.0.0,The language for content autogenerated by Sphinx. Refer to documentation
v4.0.0,for a list of supported languages.
v4.0.0,
v4.0.0,This is also used if you do content translation via gettext catalogs.
v4.0.0,"Usually you set ""language"" from the command line for these cases."
v4.0.0,"List of patterns, relative to source directory, that match files and"
v4.0.0,directories to ignore when looking for source files.
v4.0.0,This patterns also effect to html_static_path and html_extra_path
v4.0.0,The name of the Pygments (syntax highlighting) style to use.
v4.0.0,-- Configuration for C API docs generation ------------------------------
v4.0.0,-- Options for HTML output ----------------------------------------------
v4.0.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v4.0.0,a list of builtin themes.
v4.0.0,Theme options are theme-specific and customize the look and feel of a theme
v4.0.0,"further.  For a list of options available for each theme, see the"
v4.0.0,documentation.
v4.0.0,"Add any paths that contain custom static files (such as style sheets) here,"
v4.0.0,"relative to this directory. They are copied after the builtin static files,"
v4.0.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v4.0.0,-- Options for HTMLHelp output ------------------------------------------
v4.0.0,Output file base name for HTML help builder.
v4.0.0,-- Options for LaTeX output ---------------------------------------------
v4.0.0,The name of an image file (relative to this directory) to place at the top of
v4.0.0,the title page.
v4.0.0,intersphinx configuration
v4.0.0,Warning! The following code can cause buffer overflows on RTD.
v4.0.0,Consider suppressing output completely if RTD project silently fails.
v4.0.0,Refer to https://github.com/svenevs/exhale
v4.0.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.0.0,Warning! The following code can cause buffer overflows on RTD.
v4.0.0,Consider suppressing output completely if RTD project silently fails.
v4.0.0,Refer to https://github.com/svenevs/exhale
v4.0.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v4.0.0,coding: utf-8
v4.0.0,This is a basic test for floating number parsing.
v4.0.0,Most of the test cases come from:
v4.0.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v4.0.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v4.0.0,FLT_MAX
v4.0.0,FLT_MIN
v4.0.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v4.0.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v4.0.0,Near DBL_MIN
v4.0.0,DBL_MIN 2^-1022
v4.0.0,The behavior for parsing -nan depends on implementation.
v4.0.0,Thus we skip binary check for negative nan.
v4.0.0,See comment in test_cases.
v4.0.0,construct sample data first (use all data for convenience and since size is small)
v4.0.0,Load some test data
v4.0.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.0.0,Add some fake initial_scores and groups so we can test streaming them
v4.0.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.0.0,Load some test data
v4.0.0,"Use the smaller "".test"" data because we don't care about the actual data and it's smaller"
v4.0.0,Add some fake initial_scores and groups so we can test streaming them
v4.0.0,Now use the reference dataset schema to make some testable Datasets with N rows each
v4.0.0,Load some test data
v4.0.0,Serialize the reference
v4.0.0,Deserialize the reference
v4.0.0,Confirm 1 successful API call
v4.0.0,Free memory
v4.0.0,Test that Data() points to first value written
v4.0.0,Constants
v4.0.0,Start with some content:
v4.0.0,Clear & re-use:
v4.0.0,Output should match new content:
v4.0.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v4.0.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v4.0.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v4.0.0,Compute a new trial address & value & if it is a valid address:
v4.0.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v4.0.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v4.0.0,Check the just-stored value with getitem():
v4.0.0,Also store the just-stored value for future tracking:
v4.0.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v4.0.0,Test in 2 ways that the values are correctly laid out in memory:
v4.0.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.0.0,Use multiple threads to test concurrency
v4.0.0,"Since init_scores are in a column format, but need to be pushed as rows, we have to extract each batch"
v4.0.0,Calculate expected boundaries
v4.0.0,Extract a set of rows from the column-based format (still maintaining column based format)
v4.0.0,coding: utf-8
v4.0.0,coding: utf-8
v4.0.0,check saved model persistence
v4.0.0,"we need to check the consistency of model file here, so test for exact equal"
v4.0.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v4.0.0,"scores likely to be different, but prediction should still be the same"
v4.0.0,test that shape is checked during prediction
v4.0.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v4.0.0,The following is for demo and testing purpose.
v4.0.0,whole col
v4.0.0,half col
v4.0.0,Create dataset from numpy array directly.
v4.0.0,Create dataset using Sequence.
v4.0.0,Test for validation set.
v4.0.0,Select some random rows as valid data.
v4.0.0,"From Dataset constructor, with dataset from numpy array."
v4.0.0,"From Dataset.create_valid, with dataset from sequence."
v4.0.0,test that method works even with free_raw_data=True
v4.0.0,test that method works but sets raw data to None in case of immergeable data types
v4.0.0,test that method works for different data types
v4.0.0,"Set extremely harsh penalties, so CEGB will block most splits."
v4.0.0,"Compare pairs of penalties, to ensure scaling works as intended"
v4.0.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v4.0.0,"should resolve duplicate aliases, and prefer the main parameter"
v4.0.0,should choose the highest priority alias and set that value on main param
v4.0.0,if only aliases are used
v4.0.0,should use the default if main param and aliases are missing
v4.0.0,all changes should be made on copies and not modify the original
v4.0.0,preserves None found for main param and still removes aliases
v4.0.0,correctly chooses value when only an alias is provided
v4.0.0,adds None if that's given as the default and param not found
v4.0.0,If callable is found in objective
v4.0.0,Value in params should be preferred to the default_value passed from keyword arguments
v4.0.0,"None of objective or its aliases in params, but default_value is callable."
v4.0.0,check that the original data wasn't modified
v4.0.0,check that the built data has the codes
v4.0.0,test using defined feature names
v4.0.0,test using default feature names
v4.0.0,check for feature indices outside of range
v4.0.0,coding: utf-8
v4.0.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v4.0.0,set_index ensures partitions are based on group id.
v4.0.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v4.0.0,"separate target, weight from features."
v4.0.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v4.0.0,"so that within each partition, sum(g) = n_samples."
v4.0.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v4.0.0,make one categorical feature relevant to the target
v4.0.0,https://github.com/microsoft/LightGBM/issues/4118
v4.0.0,extra predict() parameters should be passed through correctly
v4.0.0,pref_leaf values should have the right shape
v4.0.0,and values that look like valid tree nodes
v4.0.0,"be sure LightGBM actually used at least one categorical column,"
v4.0.0,and that it was correctly treated as a categorical feature
v4.0.0,shape depends on whether it is binary or multiclass classification
v4.0.0,"in the special case of multi-class classification using scipy sparse matrices,"
v4.0.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v4.0.0,
v4.0.0,"since that case is so different than all other cases, check the relevant things here"
v4.0.0,and then return early
v4.0.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v4.0.0,"be sure LightGBM actually used at least one categorical column,"
v4.0.0,and that it was correctly treated as a categorical feature
v4.0.0,* shape depends on whether it is binary or multiclass classification
v4.0.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v4.0.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v4.0.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v4.0.0,that the output has the right shape and base values are in the right position
v4.0.0,"with a custom objective, prediction result is a raw score instead of predicted class"
v4.0.0,function should have been preserved
v4.0.0,should correctly classify every sample
v4.0.0,probability estimates should be similar
v4.0.0,Scores should be the same
v4.0.0,Predictions should be roughly the same.
v4.0.0,pref_leaf values should have the right shape
v4.0.0,and values that look like valid tree nodes
v4.0.0,extra predict() parameters should be passed through correctly
v4.0.0,"be sure LightGBM actually used at least one categorical column,"
v4.0.0,and that it was correctly treated as a categorical feature
v4.0.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v4.0.0,that the output has the right shape and base values are in the right position
v4.0.0,"be sure LightGBM actually used at least one categorical column,"
v4.0.0,and that it was correctly treated as a categorical feature
v4.0.0,Quantiles should be right
v4.0.0,"be sure LightGBM actually used at least one categorical column,"
v4.0.0,and that it was correctly treated as a categorical feature
v4.0.0,function should have been preserved
v4.0.0,Scores should be the same
v4.0.0,local and Dask predictions should be the same
v4.0.0,predictions should be better than random
v4.0.0,rebalance small dask.Array dataset for better performance.
v4.0.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v4.0.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v4.0.0,distributed ranker should be able to rank decently well and should
v4.0.0,have high rank correlation with scores from serial ranker.
v4.0.0,extra predict() parameters should be passed through correctly
v4.0.0,pref_leaf values should have the right shape
v4.0.0,and values that look like valid tree nodes
v4.0.0,"be sure LightGBM actually used at least one categorical column,"
v4.0.0,and that it was correctly treated as a categorical feature
v4.0.0,rebalance small dask.Array dataset for better performance.
v4.0.0,distributed ranker should be able to rank decently well with the least-squares objective
v4.0.0,and should have high rank correlation with scores from serial ranker.
v4.0.0,function should have been preserved
v4.0.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v4.0.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v4.0.0,"test eval_class_weight, eval_init_score on binary-classification task."
v4.0.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v4.0.0,create eval_sets by creating new datasets or copying training data.
v4.0.0,total number of trees scales up for ova classifier.
v4.0.0,check that early stopping was not applied.
v4.0.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v4.0.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v4.0.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v4.0.0,should be able to use the class without specifying a client
v4.0.0,should be able to set client after construction
v4.0.0,data on cluster1
v4.0.0,create identical data on cluster2
v4.0.0,"at this point, the result of default_client() is client2 since it was the most recently"
v4.0.0,created. So setting client to client1 here to test that you can select a non-default client
v4.0.0,"unfitted model should survive pickling round trip, and pickling"
v4.0.0,shouldn't have side effects on the model object
v4.0.0,client will always be None after unpickling
v4.0.0,"fitted model should survive pickling round trip, and pickling"
v4.0.0,shouldn't have side effects on the model object
v4.0.0,client will always be None after unpickling
v4.0.0,rebalance data to be sure that each worker has a piece of the data
v4.0.0,model 1 - no network parameters given
v4.0.0,model 2 - machines given
v4.0.0,model 3 - local_listen_port given
v4.0.0,training should fail because LightGBM will try to use the same
v4.0.0,port for multiple worker processes on the same machine
v4.0.0,rebalance data to be sure that each worker has a piece of the data
v4.0.0,"test that ""machines"" is actually respected by creating a socket that uses"
v4.0.0,"one of the ports mentioned in ""machines"""
v4.0.0,The above error leaves a worker waiting
v4.0.0,"an informative error should be raised if ""machines"" has duplicates"
v4.0.0,"""client"" should be the only different, and the final argument"
v4.0.0,value of the root node is 0 when init_score is set
v4.0.0,this test is separate because it takes a not-yet-constructed estimator
v4.0.0,coding: utf-8
v4.0.0,coding: utf-8
v4.0.0,"build target, group ID vectors."
v4.0.0,build y/target and group-id vectors with user-specified group sizes.
v4.0.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v4.0.0,groups should contain > 1 element for pairwise learning objective.
v4.0.0,"build feature data, X. Transform first few into informative features."
v4.0.0,coding: utf-8
v4.0.0,check that really dummy objective was used and estimator didn't learn anything
v4.0.0,prediction result is actually not transformed (is raw) due to custom objective
v4.0.0,original estimator is unaffected
v4.0.0,"new estimator is unfitted, but has the same parameters"
v4.0.0,Test if random_state is properly stored
v4.0.0,Test if two random states produce identical models
v4.0.0,Test if subsequent fits sample from random_state object and produce different models
v4.0.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v4.0.0,With default params
v4.0.0,Tests same probabilities
v4.0.0,Tests same predictions
v4.0.0,Tests same raw scores
v4.0.0,Tests same leaf indices
v4.0.0,Tests same feature contributions
v4.0.0,Tests other parameters for the prediction works
v4.0.0,Tests start_iteration
v4.0.0,"Tests same probabilities, starting from iteration 10"
v4.0.0,"Tests same predictions, starting from iteration 10"
v4.0.0,"Tests same raw scores, starting from iteration 10"
v4.0.0,"Tests same leaf indices, starting from iteration 10"
v4.0.0,"Tests same feature contributions, starting from iteration 10"
v4.0.0,"Tests other parameters for the prediction works, starting from iteration 10"
v4.0.0,test that params passed in predict have higher priority
v4.0.0,"no custom objective, no custom metric"
v4.0.0,default metric
v4.0.0,non-default metric
v4.0.0,no metric
v4.0.0,non-default metric in eval_metric
v4.0.0,non-default metric with non-default metric in eval_metric
v4.0.0,non-default metric with multiple metrics in eval_metric
v4.0.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v4.0.0,default metric for non-default objective
v4.0.0,non-default metric for non-default objective
v4.0.0,no metric
v4.0.0,non-default metric in eval_metric for non-default objective
v4.0.0,non-default metric with non-default metric in eval_metric for non-default objective
v4.0.0,non-default metric with multiple metrics in eval_metric for non-default objective
v4.0.0,"custom objective, no custom metric"
v4.0.0,default regression metric for custom objective
v4.0.0,non-default regression metric for custom objective
v4.0.0,multiple regression metrics for custom objective
v4.0.0,no metric
v4.0.0,default regression metric with non-default metric in eval_metric for custom objective
v4.0.0,non-default regression metric with metric in eval_metric for custom objective
v4.0.0,multiple regression metrics with metric in eval_metric for custom objective
v4.0.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v4.0.0,"no custom objective, custom metric"
v4.0.0,default metric with custom metric
v4.0.0,non-default metric with custom metric
v4.0.0,multiple metrics with custom metric
v4.0.0,custom metric (disable default metric)
v4.0.0,default metric for non-default objective with custom metric
v4.0.0,non-default metric for non-default objective with custom metric
v4.0.0,multiple metrics for non-default objective with custom metric
v4.0.0,custom metric (disable default metric for non-default objective)
v4.0.0,"custom objective, custom metric"
v4.0.0,custom metric for custom objective
v4.0.0,non-default regression metric with custom metric for custom objective
v4.0.0,multiple regression metrics with custom metric for custom objective
v4.0.0,default metric and invalid binary metric is replaced with multiclass alternative
v4.0.0,invalid binary metric is replaced with multiclass alternative
v4.0.0,default metric for non-default multiclass objective
v4.0.0,and invalid binary metric is replaced with multiclass alternative
v4.0.0,default metric and invalid multiclass metric is replaced with binary alternative
v4.0.0,invalid multiclass metric is replaced with binary alternative for custom objective
v4.0.0,"Verify that can receive a list of metrics, only callable"
v4.0.0,Verify that can receive a list of custom and built-in metrics
v4.0.0,Verify that works as expected when eval_metric is empty
v4.0.0,"Verify that can receive a list of metrics, only built-in"
v4.0.0,Verify that eval_metric is robust to receiving a list with None
v4.0.0,feval
v4.0.0,single eval_set
v4.0.0,two eval_set
v4.0.0,'val_minus_two' here is the expected number of threads for n_jobs=-2
v4.0.0,"Note: according to joblib's formula, a value of n_jobs=-2 means"
v4.0.0,"""use all but one thread"" (formula: n_cpus + 1 + n_jobs)"
v4.0.0,try to predict with a different feature
v4.0.0,check that disabling the check doesn't raise the error
v4.0.0,"make weights and init_score same types as y, just to avoid"
v4.0.0,a huge number of combinations and therefore test cases
v4.0.0,"make weights and init_score same types as y, just to avoid"
v4.0.0,a huge number of combinations and therefore test cases
v4.0.0,coding: utf-8
v4.0.0,we're in a leaf now
v4.0.0,check that the rest of the elements have black color
v4.0.0,check that we got to the expected leaf
v4.0.0,coding: utf-8
v4.0.0,coding: utf-8
v4.0.0,check that default gives same result as k = 1
v4.0.0,check against independent calculation for k = 1
v4.0.0,check against independent calculation for k = 2
v4.0.0,check against independent calculation for k = 10
v4.0.0,check cases where predictions are equal
v4.0.0,should give same result as binary auc for 2 classes
v4.0.0,test the case where all predictions are equal
v4.0.0,test that weighted data gives different auc_mu
v4.0.0,test that equal data weights give same auc_mu as unweighted data
v4.0.0,should give 1 when accuracy = 1
v4.0.0,test loading class weights
v4.0.0,no early stopping
v4.0.0,early stopping occurs
v4.0.0,regular early stopping
v4.0.0,positive min_delta
v4.0.0,test custom eval metrics
v4.0.0,"shuffle = False, override metric in params"
v4.0.0,"shuffle = True, callbacks"
v4.0.0,enable display training loss
v4.0.0,self defined folds
v4.0.0,LambdaRank
v4.0.0,... with l2 metric
v4.0.0,... with NDCG (default) metric
v4.0.0,self defined folds with lambdarank
v4.0.0,init_model from an in-memory Booster
v4.0.0,init_model from a text file
v4.0.0,predictions should be identical
v4.0.0,with early stopping
v4.0.0,predict by each fold booster
v4.0.0,check that each booster predicted using the best iteration
v4.0.0,fold averaging
v4.0.0,without early stopping
v4.0.0,test feature_names with whitespaces
v4.0.0,This has non-ascii strings.
v4.0.0,check that passing parameters to the constructor raises warning and ignores them
v4.0.0,take subsets and train
v4.0.0,generate CSR sparse dataset
v4.0.0,convert data to dense and get back same contribs
v4.0.0,validate the values are the same
v4.0.0,validate using CSC matrix
v4.0.0,validate the values are the same
v4.0.0,generate CSR sparse dataset
v4.0.0,convert data to dense and get back same contribs
v4.0.0,validate the values are the same
v4.0.0,validate using CSC matrix
v4.0.0,validate the values are the same
v4.0.0,Note there is an extra column added to the output for the expected value
v4.0.0,Note output CSC shape should be same as CSR output shape
v4.0.0,test sliced labels
v4.0.0,append some columns
v4.0.0,append some rows
v4.0.0,test sliced 2d matrix
v4.0.0,test sliced CSR
v4.0.0,trees start at position 1.
v4.0.0,split_features are in 4th line.
v4.0.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v4.0.0,The penalization is so high that the first 2 features should not be used here
v4.0.0,Check that a very high penalization is the same as not using the features at all
v4.0.0,check refit accepts dataset_params
v4.0.0,the following checks that dart and rf with mape can predict outside the 0-1 range
v4.0.0,https://github.com/microsoft/LightGBM/issues/1579
v4.0.0,"no custom objective, no feval"
v4.0.0,default metric
v4.0.0,non-default metric in params
v4.0.0,default metric in args
v4.0.0,non-default metric in args
v4.0.0,metric in args overwrites one in params
v4.0.0,metric in args overwrites one in params
v4.0.0,multiple metrics in params
v4.0.0,multiple metrics in args
v4.0.0,remove default metric by 'None' in list
v4.0.0,remove default metric by 'None' aliases
v4.0.0,"custom objective, no feval"
v4.0.0,no default metric
v4.0.0,metric in params
v4.0.0,metric in args
v4.0.0,metric in args overwrites its' alias in params
v4.0.0,multiple metrics in params
v4.0.0,multiple metrics in args
v4.0.0,"no custom objective, feval"
v4.0.0,default metric with custom one
v4.0.0,non-default metric in params with custom one
v4.0.0,default metric in args with custom one
v4.0.0,non-default metric in args with custom one
v4.0.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.0.0,multiple metrics in params with custom one
v4.0.0,multiple metrics in args with custom one
v4.0.0,custom metric is evaluated despite 'None' is passed
v4.0.0,"custom objective, feval"
v4.0.0,"no default metric, only custom one"
v4.0.0,metric in params with custom one
v4.0.0,metric in args with custom one
v4.0.0,"metric in args overwrites one in params, custom one is evaluated too"
v4.0.0,multiple metrics in params with custom one
v4.0.0,multiple metrics in args with custom one
v4.0.0,custom metric is evaluated despite 'None' is passed
v4.0.0,"no custom objective, no feval"
v4.0.0,default metric
v4.0.0,default metric in params
v4.0.0,non-default metric in params
v4.0.0,multiple metrics in params
v4.0.0,remove default metric by 'None' aliases
v4.0.0,"custom objective, no feval"
v4.0.0,no default metric
v4.0.0,metric in params
v4.0.0,multiple metrics in params
v4.0.0,"no custom objective, feval"
v4.0.0,default metric with custom one
v4.0.0,default metric in params with custom one
v4.0.0,non-default metric in params with custom one
v4.0.0,multiple metrics in params with custom one
v4.0.0,custom metric is evaluated despite 'None' is passed
v4.0.0,"custom objective, feval"
v4.0.0,"no default metric, only custom one"
v4.0.0,metric in params with custom one
v4.0.0,multiple metrics in params with custom one
v4.0.0,custom metric is evaluated despite 'None' is passed
v4.0.0,Custom objective replaces multiclass
v4.0.0,multiclass default metric
v4.0.0,multiclass default metric with custom one
v4.0.0,multiclass metric alias with custom one for custom objective
v4.0.0,no metric for invalid class_num
v4.0.0,custom metric for invalid class_num
v4.0.0,multiclass metric alias with custom one with invalid class_num
v4.0.0,multiclass default metric without num_class
v4.0.0,multiclass metric alias
v4.0.0,multiclass metric
v4.0.0,non-valid metric for multiclass objective
v4.0.0,non-default num_class for default objective
v4.0.0,no metric with non-default num_class for custom objective
v4.0.0,multiclass metric alias for custom objective
v4.0.0,multiclass metric for custom objective
v4.0.0,binary metric with non-default num_class for custom objective
v4.0.0,Expect three metrics but mean and stdv for each metric
v4.0.0,test XGBoost-style return value
v4.0.0,test numpy-style return value
v4.0.0,test bins string type
v4.0.0,test histogram is disabled for categorical features
v4.0.0,test for lgb.train
v4.0.0,test feval for lgb.train
v4.0.0,test with two valid data for lgb.train
v4.0.0,test for lgb.cv
v4.0.0,test feval for lgb.cv
v4.0.0,test that binning works properly for features with only positive or only negative values
v4.0.0,decreasing without freeing raw data is allowed
v4.0.0,decreasing before lazy init is allowed
v4.0.0,increasing is allowed
v4.0.0,decreasing with disabled filter is allowed
v4.0.0,decreasing with enabled filter is disallowed;
v4.0.0,also changes of other params are disallowed
v4.0.0,check extra trees increases regularization
v4.0.0,check path smoothing increases regularization
v4.0.0,test edge case with one leaf
v4.0.0,check that constraint containing all features is equivalent to no constraint
v4.0.0,check that constraint partitioning the features reduces train accuracy
v4.0.0,check that constraints consisting of single features reduce accuracy further
v4.0.0,test that interaction constraints work when not all features are used
v4.0.0,check that number of threads does not affect result
v4.0.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v4.0.0,test again with nans in data
v4.0.0,test again with bagging
v4.0.0,test with a feature that has only one non-nan value
v4.0.0,test with a categorical feature
v4.0.0,test refit: same results on same data
v4.0.0,test refit with save and load
v4.0.0,test refit: different results training on different data
v4.0.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v4.0.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v4.0.0,"test the case where start_iteration <= 0, and num_iteration is None"
v4.0.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v4.0.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v4.0.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v4.0.0,test for regression
v4.0.0,test both with and without early stopping
v4.0.0,test for multi-class
v4.0.0,test both with and without early stopping
v4.0.0,test for binary
v4.0.0,test both with and without early stopping
v4.0.0,test against sklearn average precision metric
v4.0.0,test that average precision is 1 where model predicts perfectly
v4.0.0,data as float64
v4.0.0,test all features were used
v4.0.0,test the score is better than predicting the mean
v4.0.0,test all predictions are equal using different input dtypes
v4.0.0,introduce some missing values
v4.0.0,the previous line turns x3 into object dtype in recent versions of pandas
v4.0.0,train with regular dtypes
v4.0.0,convert to nullable dtypes
v4.0.0,test training succeeds
v4.0.0,test all features were used
v4.0.0,test the score is better than predicting the mean
v4.0.0,test equal predictions
v4.0.0,test data are taken from bug report
v4.0.0,https://github.com/microsoft/LightGBM/issues/4708
v4.0.0,modified from https://github.com/microsoft/LightGBM/issues/3679#issuecomment-938652811
v4.0.0,and https://github.com/microsoft/LightGBM/pull/5087
v4.0.0,test that the ``splits_per_leaf_`` of CEGB is cleaned before training a new tree
v4.0.0,which is done in the fix #5164
v4.0.0,without the fix:
v4.0.0,Check failed: (best_split_info.left_count) > (0)
v4.0.0,try to predict with a different feature
v4.0.0,check that disabling the check doesn't raise the error
v4.0.0,try to refit with a different feature
v4.0.0,check that disabling the check doesn't raise the error
v4.0.0,coding: utf-8
v4.0.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v4.0.0,Double-precision floats are only supported on x86_64 with PoCL
v4.0.0,coding: utf-8
v4.0.0,coding: utf-8
v4.0.0,These are helper functions to allow doing a stack unwind
v4.0.0,"after an R allocation error, which would trigger a long jump."
v4.0.0,convert from one-based to zero-based index
v4.0.0,"if any feature names were larger than allocated size,"
v4.0.0,allow for a larger size and try again
v4.0.0,convert from boundaries to size
v4.0.0,--- start Booster interfaces
v4.0.0,"if any eval names were larger than allocated size,"
v4.0.0,allow for a larger size and try again
v4.0.0,"if the model string was larger than the initial buffer, call the function again, writing directly to the R object"
v4.0.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.0.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.0.0,"if aliases string was larger than the initial buffer, allocate a bigger buffer and try again"
v4.0.0,.Call() calls
v4.0.0,coding: utf-8
v4.0.0,alias table
v4.0.0,names
v4.0.0,from strings
v4.0.0,tails
v4.0.0,tails
v4.0.0,the following are stored as comma separated strings but are arrays in the wrappers
v4.0.0,coding: utf-8
v4.0.0,Single row predictor to abstract away caching logic
v4.0.0,create boosting
v4.0.0,initialize the boosting
v4.0.0,create objective function
v4.0.0,initialize the objective function
v4.0.0,create training metric
v4.0.0,reset the boosting
v4.0.0,create objective function
v4.0.0,initialize the objective function
v4.0.0,calculate the nonzero data and indices size
v4.0.0,allocate data and indices arrays
v4.0.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.0.0,aggregated per row feature contribution results
v4.0.0,keep track of the row_vector sizes for parallelization
v4.0.0,copy vector results to output for each row
v4.0.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v4.0.0,aggregated per row feature contribution results
v4.0.0,calculate number of elements per column to construct
v4.0.0,the CSC matrix with random access
v4.0.0,keep track of column counts
v4.0.0,keep track of beginning index for each column
v4.0.0,keep track of beginning index for each matrix
v4.0.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v4.0.0,store the row index
v4.0.0,update column count
v4.0.0,explicitly declare symbols from LightGBM namespace
v4.0.0,some help functions used to convert data
v4.0.0,Row iterator of on column for CSC matrix
v4.0.0,"return value at idx, only can access by ascent order"
v4.0.0,"return next non-zero pair, if index < 0, means no more data"
v4.0.0,start of c_api functions
v4.0.0,This API is to keep python binding's behavior the same with C++ implementation.
v4.0.0,"Sample count, random seed etc. should be provided in parameters."
v4.0.0,convert internal thread id to be unique based on external thread id
v4.0.0,convert internal thread id to be unique based on external thread id
v4.0.0,sample data first
v4.0.0,sample data first
v4.0.0,sample data first
v4.0.0,local buffer to re-use memory
v4.0.0,sample data first
v4.0.0,no more data
v4.0.0,---- start of booster
v4.0.0,Single row in row-major format:
v4.0.0,---- start of some help functions
v4.0.0,data is array of pointers to individual rows
v4.0.0,set number of threads for openmp
v4.0.0,read parameters from config file
v4.0.0,"remove str after ""#"""
v4.0.0,de-duplicate params
v4.0.0,prediction is needed if using input initial model(continued train)
v4.0.0,need to continue training
v4.0.0,sync up random seed for data partition
v4.0.0,load Training data
v4.0.0,load data for distributed training
v4.0.0,load data for single machine
v4.0.0,need save binary file
v4.0.0,create training metric
v4.0.0,only when have metrics then need to construct validation data
v4.0.0,"Add validation data, if it exists"
v4.0.0,add
v4.0.0,need save binary file
v4.0.0,add metric for validation data
v4.0.0,output used time on each iteration
v4.0.0,need init network
v4.0.0,create boosting
v4.0.0,create objective function
v4.0.0,load training data
v4.0.0,initialize the objective function
v4.0.0,initialize the boosting
v4.0.0,add validation data into boosting
v4.0.0,convert model to if-else statement code
v4.0.0,create predictor
v4.0.0,Free memory
v4.0.0,create predictor
v4.0.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v4.0.0,counts for all labels
v4.0.0,"start from top label, and accumulate DCG"
v4.0.0,counts for all labels
v4.0.0,calculate k Max DCG by one pass
v4.0.0,get sorted indices by score
v4.0.0,calculate multi dcg by one pass
v4.0.0,wait for all client start up
v4.0.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v4.0.0,Instead we will handle finalize/abort for MPI in main().
v4.0.0,default set to -1
v4.0.0,"distance at k-th communication, distance[k] = 2^k"
v4.0.0,set incoming rank at k-th commuication
v4.0.0,set outgoing rank at k-th commuication
v4.0.0,default set as -1
v4.0.0,construct all recursive halving map for all machines
v4.0.0,let 1 << k <= num_machines
v4.0.0,distance of each communication
v4.0.0,"if num_machines = 2^k, don't need to group machines"
v4.0.0,"communication direction, %2 == 0 is positive"
v4.0.0,neighbor at k-th communication
v4.0.0,receive data block at k-th communication
v4.0.0,send data block at k-th communication
v4.0.0,"if num_machines != 2^k, need to group machines"
v4.0.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v4.0.0,let left machine as group leader
v4.0.0,"cache block information for groups, group with 2 machines will have double block size"
v4.0.0,convert from group to node leader
v4.0.0,convert from node to group
v4.0.0,meet new group
v4.0.0,add block len for this group
v4.0.0,calculate the group block start
v4.0.0,not need to construct
v4.0.0,get receive block information
v4.0.0,accumulate block len
v4.0.0,get send block information
v4.0.0,accumulate block len
v4.0.0,static member definition
v4.0.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v4.0.0,assign the blocks to every rank.
v4.0.0,do reduce scatter
v4.0.0,do all gather
v4.0.0,assign blocks
v4.0.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v4.0.0,copy back
v4.0.0,assign blocks
v4.0.0,start all gather
v4.0.0,when num_machines is small and data is large
v4.0.0,use output as receive buffer
v4.0.0,get current local block size
v4.0.0,get out rank
v4.0.0,get in rank
v4.0.0,get send information
v4.0.0,get recv information
v4.0.0,send and recv at same time
v4.0.0,rotate in-place
v4.0.0,use output as receive buffer
v4.0.0,get current local block size
v4.0.0,get send information
v4.0.0,get recv information
v4.0.0,send and recv at same time
v4.0.0,use output as receive buffer
v4.0.0,send and recv at same time
v4.0.0,send local data to neighbor first
v4.0.0,receive neighbor data first
v4.0.0,reduce
v4.0.0,get target
v4.0.0,get send information
v4.0.0,get recv information
v4.0.0,send and recv at same time
v4.0.0,reduce
v4.0.0,send result to neighbor
v4.0.0,receive result from neighbor
v4.0.0,copy result
v4.0.0,start up socket
v4.0.0,parse clients from file
v4.0.0,get ip list of local machine
v4.0.0,get local rank
v4.0.0,construct listener
v4.0.0,construct communication topo
v4.0.0,construct linkers
v4.0.0,free listener
v4.0.0,set timeout
v4.0.0,accept incoming socket
v4.0.0,receive rank
v4.0.0,add new socket
v4.0.0,save ranks that need to connect with
v4.0.0,start listener
v4.0.0,start connect
v4.0.0,let smaller rank connect to larger rank
v4.0.0,send local rank
v4.0.0,wait for listener
v4.0.0,print connected linkers
v4.0.0,only need to copy subset
v4.0.0,avoid to copy subset many times
v4.0.0,avoid out of range
v4.0.0,may need to recopy subset
v4.0.0,valid the type
v4.0.0,parser factory implementation.
v4.0.0,customized parser add-on.
v4.0.0,save header to parser config in case needed.
v4.0.0,save label id to parser config in case needed.
v4.0.0,Constructors
v4.0.0,Get type tag
v4.0.0,Comparisons
v4.0.0,"This has to be separate, not in Statics, because Json() accesses"
v4.0.0,statics().null.
v4.0.0,"advance until next line, or end of input"
v4.0.0,advance until closing tokens
v4.0.0,The usual case: non-escaped characters
v4.0.0,Handle escapes
v4.0.0,Extract 4-byte escape sequence
v4.0.0,Explicitly check length of the substring. The following loop
v4.0.0,relies on std::string returning the terminating NUL when
v4.0.0,accessing str[length]. Checking here reduces brittleness.
v4.0.0,JSON specifies that characters outside the BMP shall be encoded as a
v4.0.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v4.0.0,components. Check whether we're in the middle of such a beast: the
v4.0.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v4.0.0,a trail (low) surrogate.
v4.0.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v4.0.0,per the UTF-16 algorithm.
v4.0.0,Integer part
v4.0.0,Decimal part
v4.0.0,Exponent part
v4.0.0,Check for any trailing garbage
v4.0.0,Documented in json11.hpp
v4.0.0,Check for another object
v4.0.0,get column names
v4.0.0,"support to get header from parser config, so could utilize following label name to id mapping logic."
v4.0.0,load label idx first
v4.0.0,"if parser config file exists, feature names may be changed after customized parser applied."
v4.0.0,clear here so could use default filled feature names during dataset construction.
v4.0.0,may improve by saving real feature names defined in parser in the future.
v4.0.0,erase label column name
v4.0.0,load ignore columns
v4.0.0,load weight idx
v4.0.0,load group idx
v4.0.0,don't support query id in data file when using distributed training
v4.0.0,read data to memory
v4.0.0,sample data
v4.0.0,construct feature bin mappers & clear sample data
v4.0.0,initialize label
v4.0.0,extract features
v4.0.0,sample data from file
v4.0.0,construct feature bin mappers & clear sample data
v4.0.0,initialize label
v4.0.0,extract features
v4.0.0,load data from binary file
v4.0.0,checks whether there's a initial score file when loaded from binary data files
v4.0.0,"the intial score file should with suffix "".bin.init"""
v4.0.0,check meta data
v4.0.0,need to check training data
v4.0.0,read data in memory
v4.0.0,initialize label
v4.0.0,extract features
v4.0.0,Get number of lines of data file
v4.0.0,initialize label
v4.0.0,extract features
v4.0.0,load data from binary file
v4.0.0,checks whether there's a initial score file when loaded from binary data files
v4.0.0,"the intial score file should with suffix "".bin.init"""
v4.0.0,not need to check validation data
v4.0.0,check meta data
v4.0.0,check token
v4.0.0,read feature group definitions
v4.0.0,read feature size
v4.0.0,buffer to read binary file
v4.0.0,check token
v4.0.0,read size of header
v4.0.0,re-allocate space if not enough
v4.0.0,read header
v4.0.0,get header
v4.0.0,read size of meta data
v4.0.0,re-allocate space if not enough
v4.0.0,read meta data
v4.0.0,load meta data
v4.0.0,sample local used data if need to partition
v4.0.0,"if not contain query file, minimal sample unit is one record"
v4.0.0,"if contain query file, minimal sample unit is one query"
v4.0.0,if is new query
v4.0.0,read feature data
v4.0.0,read feature size
v4.0.0,re-allocate space if not enough
v4.0.0,raw data
v4.0.0,fill feature_names_ if not header
v4.0.0,get forced split
v4.0.0,"if only one machine, find bin locally"
v4.0.0,"if have multi-machines, need to find bin distributed"
v4.0.0,different machines will find bin for different features
v4.0.0,start and len will store the process feature indices for different machines
v4.0.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.0.0,free
v4.0.0,gather global feature bin mappers
v4.0.0,restore features bins from buffer
v4.0.0,---- private functions ----
v4.0.0,get header
v4.0.0,num_groups
v4.0.0,real_feature_idx_
v4.0.0,feature2group
v4.0.0,feature2subfeature
v4.0.0,group_bin_boundaries
v4.0.0,group_feature_start_
v4.0.0,group_feature_cnt_
v4.0.0,get feature names
v4.0.0,get forced_bin_bounds_
v4.0.0,"if features are ordered, not need to use hist_buf"
v4.0.0,read all lines
v4.0.0,get query data
v4.0.0,"if not contain query data, minimal sample unit is one record"
v4.0.0,"if contain query data, minimal sample unit is one query"
v4.0.0,if is new query
v4.0.0,get query data
v4.0.0,"if not contain query file, minimal sample unit is one record"
v4.0.0,"if contain query file, minimal sample unit is one query"
v4.0.0,if is new query
v4.0.0,parse features
v4.0.0,get forced split
v4.0.0,"check the range of label_idx, weight_idx and group_idx"
v4.0.0,"skip label check if user input parser config file,"
v4.0.0,because label id is got from raw features while dataset features are consistent with customized parser.
v4.0.0,fill feature_names_ if not header
v4.0.0,start find bins
v4.0.0,"if only one machine, find bin locally"
v4.0.0,start and len will store the process feature indices for different machines
v4.0.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v4.0.0,free
v4.0.0,gather global feature bin mappers
v4.0.0,restore features bins from buffer
v4.0.0,if doesn't need to prediction with initial model
v4.0.0,parser
v4.0.0,set label
v4.0.0,free processed line:
v4.0.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v4.0.0,text_reader_->Lines()[i].shrink_to_fit();
v4.0.0,push data
v4.0.0,if is used feature
v4.0.0,if need to prediction with initial model
v4.0.0,parser
v4.0.0,set initial score
v4.0.0,set label
v4.0.0,free processed line:
v4.0.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v4.0.0,text_reader_->Lines()[i].shrink_to_fit();
v4.0.0,push data
v4.0.0,if is used feature
v4.0.0,metadata_ will manage space of init_score
v4.0.0,text data can be free after loaded feature values
v4.0.0,parser
v4.0.0,set initial score
v4.0.0,set label
v4.0.0,push data
v4.0.0,if is used feature
v4.0.0,only need part of data
v4.0.0,need full data
v4.0.0,metadata_ will manage space of init_score
v4.0.0,read size of token
v4.0.0,remove duplicates
v4.0.0,deep copy function for BinMapper
v4.0.0,mean size for one bin
v4.0.0,need a new bin
v4.0.0,update bin upper bound
v4.0.0,last bin upper bound
v4.0.0,get number of positive and negative distinct values
v4.0.0,include zero bounds and infinity bound
v4.0.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v4.0.0,find remaining bounds
v4.0.0,find distinct_values first
v4.0.0,push zero in the front
v4.0.0,use the large value
v4.0.0,push zero in the back
v4.0.0,convert to int type first
v4.0.0,sort by counts in descending order
v4.0.0,will ignore the categorical of small counts
v4.0.0,Push the dummy bin for NaN
v4.0.0,Use MissingType::None to represent this bin contains all categoricals
v4.0.0,fix count of NaN bin
v4.0.0,check trivial(num_bin_ == 1) feature
v4.0.0,check useless bin
v4.0.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v4.0.0,so use most_freq_bin_ = default_bin_ when there is not so sparse
v4.0.0,calculate max bin of all features to select the int type in MultiValDenseBin
v4.0.0,"for lambdarank, it needs query data for partition data in distributed learning"
v4.0.0,need convert query_id to boundaries
v4.0.0,check weights
v4.0.0,check query boundries
v4.0.0,contain initial score file
v4.0.0,check weights
v4.0.0,get local weights
v4.0.0,check query boundries
v4.0.0,get local query boundaries
v4.0.0,contain initial score file
v4.0.0,get local initial scores
v4.0.0,re-calculate query weight
v4.0.0,save to nullptr
v4.0.0,"Note that len here is row count, not num_init_score, so we compare against num_data"
v4.0.0,"We need to use source_size here, because len might not equal size (due to a partially loaded dataset)"
v4.0.0,CUDA is handled after all insertions are complete
v4.0.0,CUDA is handled after all insertions are complete
v4.0.0,save to nullptr
v4.0.0,CUDA is handled after all insertions are complete
v4.0.0,save to nullptr
v4.0.0,CUDA is handled after all insertions are complete
v4.0.0,default weight file name
v4.0.0,default init_score file name
v4.0.0,use first line to count number class
v4.0.0,default query file name
v4.0.0,root is in the depth 0
v4.0.0,non-leaf
v4.0.0,leaf
v4.0.0,use this for the missing value conversion
v4.0.0,Predict func by Map to ifelse
v4.0.0,use this for the missing value conversion
v4.0.0,non-leaf
v4.0.0,left subtree
v4.0.0,right subtree
v4.0.0,leaf
v4.0.0,non-leaf
v4.0.0,left subtree
v4.0.0,right subtree
v4.0.0,leaf
v4.0.0,recursive computation of SHAP values for a decision tree
v4.0.0,extend the unique path
v4.0.0,leaf node
v4.0.0,internal node
v4.0.0,"see if we have already split on this feature,"
v4.0.0,if so we undo that split so we can redo it for this node
v4.0.0,recursive sparse computation of SHAP values for a decision tree
v4.0.0,extend the unique path
v4.0.0,leaf node
v4.0.0,internal node
v4.0.0,"see if we have already split on this feature,"
v4.0.0,if so we undo that split so we can redo it for this node
v4.0.0,add names of objective function if not providing metric
v4.0.0,equal weights for all classes
v4.0.0,generate seeds by seed.
v4.0.0,sort eval_at
v4.0.0,Only push the non-training data
v4.0.0,check for conflicts
v4.0.0,"check if objective, metric, and num_class match"
v4.0.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v4.0.0,Check max_depth and num_leaves
v4.0.0,"Fits in an int, and is more restrictive than the current num_leaves"
v4.0.0,"force col-wise for gpu, and cuda version"
v4.0.0,force row-wise for cuda version
v4.0.0,linear tree learner must be serial type and run on CPU device
v4.0.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v4.0.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v4.0.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v4.0.0,calculated split gain can be positive even with zero gradient and hessian.
v4.0.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v4.0.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v4.0.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v4.0.0,first round: fill the single val group
v4.0.0,always push the last group
v4.0.0,put dense feature first
v4.0.0,sort by non zero cnt
v4.0.0,"sort by non zero cnt, bigger first"
v4.0.0,shuffle groups
v4.0.0,Using std::swap for vector<bool> will cause the wrong result.
v4.0.0,get num_features
v4.0.0,get bin_mappers
v4.0.0,"for sparse multi value bin, we store the feature bin values with offset added"
v4.0.0,"for dense multi value bin, the feature bin values without offsets are used"
v4.0.0,copy feature bin mapper data
v4.0.0,copy feature bin mapper data
v4.0.0,update CUDA storage for column data and metadata
v4.0.0,"if not pass a filename, just append "".bin"" of original file"
v4.0.0,Write the basic header information for the dataset
v4.0.0,get size of meta data
v4.0.0,write meta data
v4.0.0,write feature data
v4.0.0,get size of feature
v4.0.0,write feature
v4.0.0,write raw data; use row-major order so we can read row-by-row
v4.0.0,Calculate approximate size of output and reserve space
v4.0.0,write feature group definitions
v4.0.0,"Give a little extra just in case, to avoid unnecessary resizes"
v4.0.0,"Write token that marks the data as binary reference, and the version"
v4.0.0,Write the basic definition of the overall dataset
v4.0.0,write feature group definitions
v4.0.0,get size of feature
v4.0.0,write feature
v4.0.0,size of feature names and forced bins
v4.0.0,write header
v4.0.0,write feature names
v4.0.0,write forced bins
v4.0.0,"explicitly initialize template methods, for cross module call"
v4.0.0,"explicitly initialize template methods, for cross module call"
v4.0.0,"Only one multi-val group, just simply merge"
v4.0.0,Skip the leading 0 when copying group_bin_boundaries.
v4.0.0,regenerate other fields
v4.0.0,need to iterate bin iterator
v4.0.0,is dense column
v4.0.0,is sparse column
v4.0.0,initialize the subset cuda column data
v4.0.0,"if one column has too many bins, use a separate partition for that column"
v4.0.0,try if adding this column exceed the maximum number per partition
v4.0.0,"if one column has too many bins, use a separate partition for that column"
v4.0.0,try if adding this column exceed the maximum number per partition
v4.0.0,store the importance first
v4.0.0,PredictRaw
v4.0.0,PredictRawByMap
v4.0.0,Predict
v4.0.0,PredictByMap
v4.0.0,PredictLeafIndex
v4.0.0,PredictLeafIndexByMap
v4.0.0,output model type
v4.0.0,output number of class
v4.0.0,output label index
v4.0.0,output max_feature_idx
v4.0.0,output objective
v4.0.0,output tree models
v4.0.0,store the importance first
v4.0.0,sort the importance
v4.0.0,use serialized string to restore this object
v4.0.0,Use first 128 chars to avoid exceed the message buffer.
v4.0.0,get number of classes
v4.0.0,get index of label
v4.0.0,get max_feature_idx first
v4.0.0,get average_output
v4.0.0,get feature names
v4.0.0,get monotone_constraints
v4.0.0,set zero
v4.0.0,predict all the trees for one iteration
v4.0.0,check early stopping
v4.0.0,set zero
v4.0.0,predict all the trees for one iteration
v4.0.0,check early stopping
v4.0.0,margin_threshold will be captured by value
v4.0.0,copy and sort
v4.0.0,margin_threshold will be captured by value
v4.0.0,Fix for compiler warnings about reaching end of control
v4.0.0,load forced_splits file
v4.0.0,init tree learner
v4.0.0,push training metrics
v4.0.0,get max feature index
v4.0.0,get label index
v4.0.0,get feature names
v4.0.0,get parser config file content
v4.0.0,check that forced splits does not use feature indices larger than dataset size
v4.0.0,"if need bagging, create buffer"
v4.0.0,"for a validation dataset, we need its score and metric"
v4.0.0,update score
v4.0.0,objective function will calculate gradients and hessians
v4.0.0,output used time per iteration
v4.0.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v4.0.0,boosting first
v4.0.0,use customized objective function
v4.0.0,need to copy customized gradients when using GOSS
v4.0.0,bagging logic
v4.0.0,need to copy gradients for bagging subset.
v4.0.0,shrinkage by learning rate
v4.0.0,update score
v4.0.0,only add default score one-time
v4.0.0,updates scores
v4.0.0,add model
v4.0.0,reset score
v4.0.0,remove model
v4.0.0,print message for metric
v4.0.0,pop last early_stopping_round_ models
v4.0.0,update training score
v4.0.0,we need to predict out-of-bag scores of data for boosting
v4.0.0,update validation score
v4.0.0,print training metric
v4.0.0,print validation metric
v4.0.0,set zero
v4.0.0,predict all the trees for one iteration
v4.0.0,predict all the trees for one iteration
v4.0.0,push training metrics
v4.0.0,"not same training data, need reset score and others"
v4.0.0,create score tracker
v4.0.0,update score
v4.0.0,resize gradient vectors to copy the customized gradients for goss or bagging with subset
v4.0.0,load forced_splits file
v4.0.0,"if exists initial score, will start from it"
v4.0.0,clear host score buffer
v4.0.0,Get the max size of pool
v4.0.0,at least need 2 leaves
v4.0.0,push split information for all leaves
v4.0.0,initialize splits for leaf
v4.0.0,initialize data partition
v4.0.0,initialize ordered gradients and hessians
v4.0.0,cannot change is_hist_col_wise during training
v4.0.0,initialize splits for leaf
v4.0.0,initialize data partition
v4.0.0,initialize ordered gradients and hessians
v4.0.0,Get the max size of pool
v4.0.0,at least need 2 leaves
v4.0.0,push split information for all leaves
v4.0.0,some initial works before training
v4.0.0,root leaf
v4.0.0,only root leaf can be splitted on first time
v4.0.0,some initial works before finding best split
v4.0.0,find best threshold for every feature
v4.0.0,Get a leaf with max split gain
v4.0.0,Get split information for best leaf
v4.0.0,"cannot split, quit"
v4.0.0,split tree with best leaf
v4.0.0,reset histogram pool
v4.0.0,initialize data partition
v4.0.0,reset the splits for leaves
v4.0.0,Sumup for root
v4.0.0,use all data
v4.0.0,"use bagging, only use part of data"
v4.0.0,check depth of current leaf
v4.0.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v4.0.0,no enough data to continue
v4.0.0,only have root
v4.0.0,put parent(left) leaf's histograms into larger leaf's histograms
v4.0.0,put parent(left) leaf's histograms to larger leaf's histograms
v4.0.0,construct smaller leaf
v4.0.0,construct larger leaf
v4.0.0,find splits
v4.0.0,only has root leaf
v4.0.0,start at root leaf
v4.0.0,Histogram construction require parent features.
v4.0.0,"then, compute own splits"
v4.0.0,split info should exist because searching in bfs fashion - should have added from parent
v4.0.0,update before tree split
v4.0.0,don't need to update this in data-based parallel model
v4.0.0,"split tree, will return right leaf"
v4.0.0,store the true split gain in tree model
v4.0.0,don't need to update this in data-based parallel model
v4.0.0,store the true split gain in tree model
v4.0.0,init the leaves that used on next iteration
v4.0.0,update leave outputs if needed
v4.0.0,bag_mapper[index_mapper[i]]
v4.0.0,it is needed to filter the features after the above code.
v4.0.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v4.0.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v4.0.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v4.0.0,find splits
v4.0.0,identify features containing nans
v4.0.0,preallocate the matrix used to calculate linear model coefficients
v4.0.0,"store only upper triangular half of matrix as an array, in row-major order"
v4.0.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v4.0.0,we add another 8 to ensure cache lines are not shared among processors
v4.0.0,some initial works before training
v4.0.0,root leaf
v4.0.0,only root leaf can be splitted on first time
v4.0.0,some initial works before finding best split
v4.0.0,find best threshold for every feature
v4.0.0,Get a leaf with max split gain
v4.0.0,Get split information for best leaf
v4.0.0,"cannot split, quit"
v4.0.0,split tree with best leaf
v4.0.0,map data to leaf number
v4.0.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v4.0.0,the coefficients vector is given by
v4.0.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v4.0.0,where:
v4.0.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v4.0.0,"H is the diagonal matrix of the hessian,"
v4.0.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v4.0.0,g is the vector of gradients
v4.0.0,the subscript _T denotes the transpose
v4.0.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v4.0.0,clear the coefficient matrices
v4.0.0,aggregate results from different threads
v4.0.0,copy into eigen matrices and solve
v4.0.0,update the tree properties
v4.0.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.0.0,get feature partition
v4.0.0,get local used features
v4.0.0,get best split at smaller leaf
v4.0.0,find local best split for larger leaf
v4.0.0,sync global best info
v4.0.0,update best split
v4.0.0,"instantiate template classes, otherwise linker cannot find the code"
v4.0.0,initialize SerialTreeLearner
v4.0.0,Get local rank and global machine size
v4.0.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.0.0,allocate buffer for communication
v4.0.0,get block start and block len for reduce scatter
v4.0.0,get buffer_write_start_pos
v4.0.0,get buffer_read_start_pos
v4.0.0,generate feature partition for current tree
v4.0.0,get local used feature
v4.0.0,get block start and block len for reduce scatter
v4.0.0,sync global data sumup info
v4.0.0,global sumup reduce
v4.0.0,copy back
v4.0.0,set global sumup info
v4.0.0,init global data count in leaf
v4.0.0,reset hist num bits according to global num data
v4.0.0,sync global data sumup info
v4.0.0,global sumup reduce
v4.0.0,copy back
v4.0.0,set global sumup info
v4.0.0,init global data count in leaf
v4.0.0,clear histogram buffer before synchronizing
v4.0.0,otherwise histogram contents from the previous iteration will be sent
v4.0.0,construct local histograms
v4.0.0,copy to buffer
v4.0.0,Reduce scatter for histogram
v4.0.0,restore global histograms from buffer
v4.0.0,only root leaf
v4.0.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v4.0.0,find local best split for larger leaf
v4.0.0,sync global best info
v4.0.0,set best split
v4.0.0,need update global number of data in leaf
v4.0.0,reset hist num bits according to global num data
v4.0.0,"instantiate template classes, otherwise linker cannot find the code"
v4.0.0,initialize SerialTreeLearner
v4.0.0,some additional variables needed for GPU trainer
v4.0.0,Initialize GPU buffers and kernels
v4.0.0,some functions used for debugging the GPU histogram construction
v4.0.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v4.0.0,goto err;
v4.0.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v4.0.0,also guarantee that there are at least 2K examples per workgroup
v4.0.0,return 0;
v4.0.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v4.0.0,decide the best number of workgroups working on one feature4 tuple
v4.0.0,set work group size based on feature size
v4.0.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v4.0.0,we need to refresh the kernel arguments after reallocating
v4.0.0,The only argument that needs to be changed later is num_data_
v4.0.0,"the GPU kernel will process all features in one call, and each"
v4.0.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v4.0.0,process one feature4 tuple
v4.0.0,"for the root node, indices are not copied"
v4.0.0,"for constant hessian, hessians are not copied except for the root node"
v4.0.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v4.0.0,and we will launch num_feature workgroups for this kernel
v4.0.0,will launch threads for all features
v4.0.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v4.0.0,copy the results asynchronously. Size depends on if double precision is used
v4.0.0,we will wait for this object in WaitAndGetHistograms
v4.0.0,"when the output is ready, the computation is done"
v4.0.0,values of this feature has been redistributed to multiple bins; need a reduction here
v4.0.0,how many feature-group tuples we have
v4.0.0,leave some safe margin for prefetching
v4.0.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v4.0.0,clear sparse/dense maps
v4.0.0,do nothing if no features can be processed on GPU
v4.0.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v4.0.0,unpin old buffer if necessary before destructing them
v4.0.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v4.0.0,allocate space for gradients and Hessians on device
v4.0.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v4.0.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v4.0.0,copy indices to the device
v4.0.0,histogram bin entry size depends on the precision (single/double)
v4.0.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v4.0.0,each work group generates a sub-histogram of dword_features_ features.
v4.0.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v4.0.0,create atomic counters for inter-group coordination
v4.0.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v4.0.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v4.0.0,looking for dword_features_ non-sparse feature-groups
v4.0.0,decide if we need to redistribute the bin
v4.0.0,multiplier must be a power of 2
v4.0.0,device_bin_mults_.push_back(1);
v4.0.0,found
v4.0.0,for data transfer time
v4.0.0,"Now generate new data structure feature4, and copy data to the device"
v4.0.0,"preallocate arrays for all threads, and pin them"
v4.0.0,building Feature4 bundles; each thread handles dword_features_ features
v4.0.0,one feature datapoint is 4 bits
v4.0.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.0.0,one feature datapoint is one byte
v4.0.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v4.0.0,Dense bin
v4.0.0,Dense 4-bit bin
v4.0.0,working on the remaining (less than dword_features_) feature groups
v4.0.0,fill the leftover features
v4.0.0,"fill this empty feature with some ""random"" value"
v4.0.0,"fill this empty feature with some ""random"" value"
v4.0.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v4.0.0,deallocate pinned space for feature copying
v4.0.0,data transfer time
v4.0.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v4.0.0,"Something bad happened. Just return ""No log available."""
v4.0.0,"build is okay, log may contain warnings"
v4.0.0,destroy any old kernels
v4.0.0,create OpenCL kernels for different number of workgroups per feature
v4.0.0,currently we don't use constant memory
v4.0.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v4.0.0,kernel with indices in an array
v4.0.0,"kernel with all features enabled, with eliminated branches"
v4.0.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v4.0.0,do nothing if no features can be processed on GPU
v4.0.0,The only argument that needs to be changed later is num_data_
v4.0.0,"hessian is passed as a parameter, but it is not available now."
v4.0.0,hessian will be set in BeforeTrain()
v4.0.0,"Get the max bin size, used for selecting best GPU kernel"
v4.0.0,initialize GPU
v4.0.0,determine which kernel to use based on the max number of bins
v4.0.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.0.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.0.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v4.0.0,ignore the feature groups that contain categorical features when producing warnings about max_bin.
v4.0.0,"these groups may contain larger number of bins due to categorical features, but not due to the setting of max_bin."
v4.0.0,setup GPU kernel arguments after we allocating all the buffers
v4.0.0,GPU memory has to been reallocated because data may have been changed
v4.0.0,setup GPU kernel arguments after we allocating all the buffers
v4.0.0,Copy initial full hessians and gradients to GPU.
v4.0.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v4.0.0,setup hessian parameters only
v4.0.0,hessian is passed as a parameter
v4.0.0,use bagging
v4.0.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v4.0.0,copy used gradients and Hessians to ordered buffer
v4.0.0,transfer the indices to GPU
v4.0.0,transfer hessian to GPU
v4.0.0,setup hessian parameters only
v4.0.0,hessian is passed as a parameter
v4.0.0,transfer gradients to GPU
v4.0.0,only have root
v4.0.0,"Copy indices, gradients and Hessians as early as possible"
v4.0.0,only need to initialize for smaller leaf
v4.0.0,Get leaf boundary
v4.0.0,copy indices to the GPU:
v4.0.0,copy ordered Hessians to the GPU:
v4.0.0,copy ordered gradients to the GPU:
v4.0.0,do nothing if no features can be processed on GPU
v4.0.0,copy data indices if it is not null
v4.0.0,generate and copy ordered_gradients if gradients is not null
v4.0.0,generate and copy ordered_hessians if Hessians is not null
v4.0.0,converted indices in is_feature_used to feature-group indices
v4.0.0,construct the feature masks for dense feature-groups
v4.0.0,"if no feature group is used, just return and do not use GPU"
v4.0.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v4.0.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v4.0.0,"All data have been prepared, now run the GPU kernel"
v4.0.0,construct smaller leaf
v4.0.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v4.0.0,then construct sparse features on CPU
v4.0.0,"wait for GPU to finish, only if GPU is actually used"
v4.0.0,use double precision
v4.0.0,use single precision
v4.0.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v4.0.0,#define GPU_DEBUG_COMPARE
v4.0.0,construct larger leaf
v4.0.0,then construct sparse features on CPU
v4.0.0,"wait for GPU to finish, only if GPU is actually used"
v4.0.0,use double precision
v4.0.0,use single precision
v4.0.0,do some sanity check for the GPU algorithm
v4.0.0,limit top k
v4.0.0,get max bin
v4.0.0,calculate buffer size
v4.0.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v4.0.0,"left and right on same time, so need double size"
v4.0.0,initialize histograms for global
v4.0.0,sync global data sumup info
v4.0.0,set global sumup info
v4.0.0,init global data count in leaf
v4.0.0,get local sumup
v4.0.0,get local sumup
v4.0.0,get mean number on machines
v4.0.0,weighted gain
v4.0.0,get top k
v4.0.0,"Copy histogram to buffer, and Get local aggregate features"
v4.0.0,copy histograms.
v4.0.0,copy smaller leaf histograms first
v4.0.0,mark local aggregated feature
v4.0.0,copy
v4.0.0,then copy larger leaf histograms
v4.0.0,mark local aggregated feature
v4.0.0,copy
v4.0.0,use local data to find local best splits
v4.0.0,clear histogram buffer before synchronizing
v4.0.0,otherwise histogram contents from the previous iteration will be sent
v4.0.0,find splits
v4.0.0,only has root leaf
v4.0.0,local voting
v4.0.0,gather
v4.0.0,get all top-k from all machines
v4.0.0,global voting
v4.0.0,copy local histgrams to buffer
v4.0.0,Reduce scatter for histogram
v4.0.0,find best split from local aggregated histograms
v4.0.0,restore from buffer
v4.0.0,restore from buffer
v4.0.0,find local best
v4.0.0,find local best split for larger leaf
v4.0.0,sync global best info
v4.0.0,copy back
v4.0.0,set the global number of data for leaves
v4.0.0,init the global sumup info
v4.0.0,"instantiate template classes, otherwise linker cannot find the code"
v4.0.0,allocate CUDA memory
v4.0.0,leave some space for alignment
v4.0.0,input best split info
v4.0.0,for leaf information update
v4.0.0,"gather information for CPU, used for launching kernels"
v4.0.0,for leaf splits information update
v4.0.0,we need restore the order of indices in cuda_data_indices_
v4.0.0,allocate more memory for sum reduction in CUDA
v4.0.0,only the first element records the final sum
v4.0.0,intialize split find task information (a split find task is one pass through the histogram of a feature)
v4.0.0,use the first gpu by default
v4.0.0,"std::max(..., 1UL) to avoid error in the case when there are NaN's in the categorical values"
v4.0.0,use feature interaction constraint or sample features by node
v3.3.5,coding: utf-8
v3.3.5,coding: utf-8
v3.3.5,create predictor first
v3.3.5,"show deprecation warning only for early stop argument, setting early stop via global params should still be possible"
v3.3.5,check dataset
v3.3.5,reduce cost for prediction training data
v3.3.5,process callbacks
v3.3.5,Most of legacy advanced options becomes callbacks
v3.3.5,construct booster
v3.3.5,start training
v3.3.5,check evaluation result.
v3.3.5,"ranking task, split according to groups"
v3.3.5,run preprocessing on the data set if needed
v3.3.5,setup callbacks
v3.3.5,coding: utf-8
v3.3.5,dummy function to support older version of scikit-learn
v3.3.5,coding: utf-8
v3.3.5,documentation templates for LGBMModel methods are shared between the classes in
v3.3.5,this module and those in the ``dask`` module
v3.3.5,"user can set verbose with kwargs, it has higher priority"
v3.3.5,Do not modify original args in fit function
v3.3.5,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.3.5,Separate built-in from callable evaluation metrics
v3.3.5,register default metric for consistency with callable eval_metric case
v3.3.5,try to deduce from class instance
v3.3.5,overwrite default metric by explicitly set metric
v3.3.5,concatenate metric from params (or default if not provided in params) and eval_metric
v3.3.5,copy for consistency
v3.3.5,reduce cost for prediction training data
v3.3.5,free dataset
v3.3.5,Switch to using a multiclass objective in the underlying LGBM instance
v3.3.5,"do not modify args, as it causes errors in model selection tools"
v3.3.5,check group data
v3.3.5,coding: utf-8
v3.3.5,we don't need lib_lightgbm while building docs
v3.3.5,coding: utf-8
v3.3.5,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.3.5,coding: utf-8
v3.3.5,coding: utf-8
v3.3.5,TypeError: obj is not a string or a number
v3.3.5,ValueError: invalid literal
v3.3.5,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.3.5,avoid side effects on passed-in parameters
v3.3.5,"find a value, and remove other aliases with .pop()"
v3.3.5,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.3.5,Get total row number.
v3.3.5,Random access by row index. Used for data sampling.
v3.3.5,Range data access. Used to read data in batch when constructing Dataset.
v3.3.5,Optionally specify batch_size to control range data read size.
v3.3.5,Only required if using ``Dataset.subset()``.
v3.3.5,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.5,avoid memory consumption by arrays concatenation operations
v3.3.5,create numpy array from output arrays
v3.3.5,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.3.5,for CSC there is extra column added
v3.3.5,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.3.5,same shape as input csr or csc matrix except extra column for expected value
v3.3.5,note: make sure we copy data as it will be deallocated next
v3.3.5,"free the temporary native indptr, indices, and data"
v3.3.5,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.5,avoid memory consumption by arrays concatenation operations
v3.3.5,c type: double**
v3.3.5,each double* element points to start of each column of sample data.
v3.3.5,c type int**
v3.3.5,each int* points to start of indices for each column
v3.3.5,"no min_data, nthreads and verbose in this function"
v3.3.5,check data has header or not
v3.3.5,need to regroup init_score
v3.3.5,process for args
v3.3.5,"user can set verbose with params, it has higher priority"
v3.3.5,get categorical features
v3.3.5,process for reference dataset
v3.3.5,start construct data
v3.3.5,set feature names
v3.3.5,"Select sampled rows, transpose to column order."
v3.3.5,create validation dataset from ref_dataset
v3.3.5,create valid
v3.3.5,construct subset
v3.3.5,create train
v3.3.5,could be updated if data is not freed
v3.3.5,set to None
v3.3.5,we're done if self and reference share a common upstream reference
v3.3.5,"if buffer length is not long enough, reallocate buffers"
v3.3.5,"group data from LightGBM is boundaries data, need to convert to group size"
v3.3.5,"user can set verbose with params, it has higher priority"
v3.3.5,Training task
v3.3.5,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.3.5,construct booster object
v3.3.5,copy the parameters from train_set
v3.3.5,save reference to data
v3.3.5,buffer for inner predict
v3.3.5,Prediction task
v3.3.5,if a single node tree it won't have `leaf_index` so return 0
v3.3.5,"Create the node record, and populate universal data members"
v3.3.5,Update values to reflect node type (leaf or split)
v3.3.5,traverse the next level of the tree
v3.3.5,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.3.5,and we add node to the list.
v3.3.5,need reset training data
v3.3.5,need to push new valid data
v3.3.5,"if buffer length is not long enough, re-allocate a buffer"
v3.3.5,"if buffer length is not long enough, reallocate a buffer"
v3.3.5,Copy models
v3.3.5,Get name of features
v3.3.5,"if buffer length is not long enough, reallocate buffers"
v3.3.5,avoid to predict many time in one iteration
v3.3.5,Get num of inner evals
v3.3.5,Get name of eval metrics
v3.3.5,"if buffer length is not long enough, reallocate buffers"
v3.3.5,coding: utf-8
v3.3.5,Callback environment used by callbacks
v3.3.5,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.5,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.5,coding: utf-8
v3.3.5,Concatenate many parts into one
v3.3.5,construct local eval_set data.
v3.3.5,store indices of eval_set components that were not contained within local parts.
v3.3.5,consolidate parts of each individual eval component.
v3.3.5,require that eval_name exists in evaluated result data in case dropped due to padding.
v3.3.5,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v3.3.5,filter padding from eval parts then _concat each eval_set component.
v3.3.5,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v3.3.5,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v3.3.5,capture whether local_listen_port or its aliases were provided
v3.3.5,capture whether machines or its aliases were provided
v3.3.5,Some passed-in parameters can be removed:
v3.3.5,* 'num_machines': set automatically from Dask worker list
v3.3.5,* 'num_threads': overridden to match nthreads on each Dask process
v3.3.5,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.3.5,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v3.3.5,X and y are each delayed sub-lists of original eval dask Collections.
v3.3.5,find maximum number of parts in an individual eval set so that we can
v3.3.5,pad eval sets when they come in different sizes.
v3.3.5,"when individual eval set is equivalent to training data, skip recomputing parts."
v3.3.5,add None-padding for individual eval_set member if it is smaller than the largest member.
v3.3.5,first time a chunk of this eval set is added to this part.
v3.3.5,append additional chunks of this eval set to this part.
v3.3.5,ensure that all evaluation parts map uniquely to one part.
v3.3.5,assign sub-eval_set components to worker parts.
v3.3.5,Start computation in the background
v3.3.5,Find locations of all parts and map them to particular Dask workers
v3.3.5,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v3.3.5,data artifacts may not be populated depending on worker returning final estimator.
v3.3.5,assign general validation set settings to fit kwargs.
v3.3.5,resolve aliases for network parameters and pop the result off params.
v3.3.5,these values are added back in calls to `_train_part()`
v3.3.5,figure out network params
v3.3.5,Tell each worker to train on the parts that it has locally
v3.3.5,
v3.3.5,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v3.3.5,1. there is randomness in the training process unless parameters ``seed``
v3.3.5,and ``deterministic`` are set
v3.3.5,"2. even with those parameters set, the output of one ``_train_part()`` call"
v3.3.5,relies on global state (it and all the other LightGBM training processes
v3.3.5,coordinate with each other)
v3.3.5,"if network parameters were changed during training, remove them from the"
v3.3.5,returned model so that they're generated dynamically on every run based
v3.3.5,on the Dask cluster you're connected to and which workers have pieces of
v3.3.5,the training data
v3.3.5,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.3.5,"for multi-class classification with sparse matrices, pred_contrib predictions"
v3.3.5,are returned as a list of sparse matrices (one per class)
v3.3.5,"pred_contrib output will have one column per feature,"
v3.3.5,plus one more for the base value
v3.3.5,need to tell Dask the expected type and shape of individual preds
v3.3.5,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v3.3.5,the code below is used instead to ensure that the sparse type is preserved during concatentation
v3.3.5,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v3.3.5,Concatenate them to return a list of Dask Arrays.
v3.3.5,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.5,currently relevant for the Dask estimators
v3.3.5,"DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds."
v3.3.5,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.3.5,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.5,currently relevant for the Dask estimators
v3.3.5,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds."
v3.3.5,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.3.5,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.5,currently relevant for the Dask estimators
v3.3.5,DaskLGBMRanker does not support eval_class_weight or early stopping
v3.3.5,DaskLGBMRanker support for callbacks and init_model is not tested
v3.3.5,coding: utf-8
v3.3.5,load or create your dataset
v3.3.5,create dataset for lightgbm
v3.3.5,"if you want to re-use data, remember to set free_raw_data=False"
v3.3.5,specify your configurations as a dict
v3.3.5,generate feature names
v3.3.5,feature_name and categorical_feature
v3.3.5,check feature name
v3.3.5,save model to file
v3.3.5,dump model to JSON (and save to file)
v3.3.5,feature names
v3.3.5,feature importances
v3.3.5,load model to predict
v3.3.5,can only predict with the best iteration (or the saving iteration)
v3.3.5,eval with loaded model
v3.3.5,dump model with pickle
v3.3.5,load model with pickle to predict
v3.3.5,can predict with any iteration when loaded in pickle way
v3.3.5,eval with loaded model
v3.3.5,continue training
v3.3.5,init_model accepts:
v3.3.5,1. model file name
v3.3.5,2. Booster()
v3.3.5,decay learning rates
v3.3.5,learning_rates accepts:
v3.3.5,1. list/tuple with length = num_boost_round
v3.3.5,2. function(curr_iter)
v3.3.5,change other parameters during training
v3.3.5,self-defined objective function
v3.3.5,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.3.5,log likelihood loss
v3.3.5,self-defined eval metric
v3.3.5,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.5,binary error
v3.3.5,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.5,This may make built-in evaluation metric calculate wrong results
v3.3.5,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.5,Keep this in mind when you use the customization
v3.3.5,another self-defined eval metric
v3.3.5,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.5,accuracy
v3.3.5,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.5,This may make built-in evaluation metric calculate wrong results
v3.3.5,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.5,Keep this in mind when you use the customization
v3.3.5,callback
v3.3.5,coding: utf-8
v3.3.5,load or create your dataset
v3.3.5,train
v3.3.5,predict
v3.3.5,eval
v3.3.5,feature importances
v3.3.5,self-defined eval metric
v3.3.5,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.5,Root Mean Squared Logarithmic Error (RMSLE)
v3.3.5,train
v3.3.5,another self-defined eval metric
v3.3.5,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.5,Relative Absolute Error (RAE)
v3.3.5,train
v3.3.5,predict
v3.3.5,eval
v3.3.5,other scikit-learn modules
v3.3.5,coding: utf-8
v3.3.5,load or create your dataset
v3.3.5,create dataset for lightgbm
v3.3.5,specify your configurations as a dict
v3.3.5,train
v3.3.5,coding: utf-8
v3.3.5,################
v3.3.5,Simulate some binary data with a single categorical and
v3.3.5,single continuous predictor
v3.3.5,################
v3.3.5,Set up a couple of utilities for our experiments
v3.3.5,################
v3.3.5,Observe the behavior of `binary` and `xentropy` objectives
v3.3.5,Trying this throws an error on non-binary values of y:
v3.3.5,"experiment('binary', label_type='probability', DATA)"
v3.3.5,The speed of `binary` is not drastically different than
v3.3.5,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.3.5,there are reasons to suspect that `binary` should run faster when the
v3.3.5,label is an integer instead of a float
v3.3.5,coding: utf-8
v3.3.5,load or create your dataset
v3.3.5,create dataset for lightgbm
v3.3.5,specify your configurations as a dict
v3.3.5,train
v3.3.5,save model to file
v3.3.5,predict
v3.3.5,eval
v3.3.5,We can also open HDF5 file once and get access to
v3.3.5,"With binary dataset created, we can use either Python API or cmdline version to train."
v3.3.5,
v3.3.5,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v3.3.5,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v3.3.5,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v3.3.5,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v3.3.5,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v3.3.5,Y has a single column and we read it in single shot. So store it as an 1-d array.
v3.3.5,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v3.3.5,"When accessing any element in a HDF5 chunk, it's read entirely."
v3.3.5,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v3.3.5,Here we are just creating a chunk size that matches with batch_size.
v3.3.5,
v3.3.5,Also note that the data is stored in row major order to avoid extra copy when passing to
v3.3.5,lightgbm Dataset.
v3.3.5,Save to 2 HDF5 files for demonstration.
v3.3.5,We can store multiple datasets inside a single HDF5 file.
v3.3.5,Separating X and Y for choosing best chunk size for data loading.
v3.3.5,split training data into two partitions
v3.3.5,make this array dense because we're splitting across
v3.3.5,a sparse boundary to partition the data
v3.3.5,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.3.5,predictions and target values back from workers to the client
v3.3.5,
v3.3.5,"for larger datasets, consider the metrics from dask-ml instead"
v3.3.5,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.3.5,coding: utf-8
v3.3.5,!/usr/bin/env python3
v3.3.5,-*- coding: utf-8 -*-
v3.3.5,
v3.3.5,"LightGBM documentation build configuration file, created by"
v3.3.5,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.3.5,
v3.3.5,This file is execfile()d with the current directory set to its
v3.3.5,containing dir.
v3.3.5,
v3.3.5,Note that not all possible configuration values are present in this
v3.3.5,autogenerated file.
v3.3.5,
v3.3.5,All configuration values have a default; values that are commented out
v3.3.5,serve to show the default.
v3.3.5,"If extensions (or modules to document with autodoc) are in another directory,"
v3.3.5,add these directories to sys.path here. If the directory is relative to the
v3.3.5,"documentation root, use os.path.abspath to make it absolute."
v3.3.5,-- General configuration ------------------------------------------------
v3.3.5,"If your documentation needs a minimal Sphinx version, state it here."
v3.3.5,"Add any Sphinx extension module names here, as strings. They can be"
v3.3.5,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.3.5,ones.
v3.3.5,mock out modules
v3.3.5,hide type hints in API docs
v3.3.5,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.3.5,Only the class' docstring is inserted.
v3.3.5,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.3.5,The master toctree document.
v3.3.5,General information about the project.
v3.3.5,The name of an image file (relative to this directory) to place at the top
v3.3.5,of the sidebar.
v3.3.5,The name of an image file (relative to this directory) to use as a favicon of
v3.3.5,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.3.5,pixels large.
v3.3.5,"The version info for the project you're documenting, acts as replacement for"
v3.3.5,"|version| and |release|, also used in various other places throughout the"
v3.3.5,built documents.
v3.3.5,The short X.Y version.
v3.3.5,"The full version, including alpha/beta/rc tags."
v3.3.5,The language for content autogenerated by Sphinx. Refer to documentation
v3.3.5,for a list of supported languages.
v3.3.5,
v3.3.5,This is also used if you do content translation via gettext catalogs.
v3.3.5,"Usually you set ""language"" from the command line for these cases."
v3.3.5,"List of patterns, relative to source directory, that match files and"
v3.3.5,directories to ignore when looking for source files.
v3.3.5,This patterns also effect to html_static_path and html_extra_path
v3.3.5,The name of the Pygments (syntax highlighting) style to use.
v3.3.5,-- Configuration for C API docs generation ------------------------------
v3.3.5,-- Options for HTML output ----------------------------------------------
v3.3.5,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.3.5,a list of builtin themes.
v3.3.5,Theme options are theme-specific and customize the look and feel of a theme
v3.3.5,"further.  For a list of options available for each theme, see the"
v3.3.5,documentation.
v3.3.5,"Add any paths that contain custom static files (such as style sheets) here,"
v3.3.5,"relative to this directory. They are copied after the builtin static files,"
v3.3.5,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.3.5,-- Options for HTMLHelp output ------------------------------------------
v3.3.5,Output file base name for HTML help builder.
v3.3.5,-- Options for LaTeX output ---------------------------------------------
v3.3.5,The name of an image file (relative to this directory) to place at the top of
v3.3.5,the title page.
v3.3.5,Warning! The following code can cause buffer overflows on RTD.
v3.3.5,Consider suppressing output completely if RTD project silently fails.
v3.3.5,Refer to https://github.com/svenevs/exhale
v3.3.5,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.5,Warning! The following code can cause buffer overflows on RTD.
v3.3.5,Consider suppressing output completely if RTD project silently fails.
v3.3.5,Refer to https://github.com/svenevs/exhale
v3.3.5,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.5,coding: utf-8
v3.3.5,This is a basic test for floating number parsing.
v3.3.5,Most of the test cases come from:
v3.3.5,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v3.3.5,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v3.3.5,FLT_MAX
v3.3.5,FLT_MIN
v3.3.5,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v3.3.5,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v3.3.5,Near DBL_MIN
v3.3.5,DBL_MIN 2^-1022
v3.3.5,The behavior for parsing -nan depends on implementation.
v3.3.5,Thus we skip binary check for negative nan.
v3.3.5,See comment in test_cases.
v3.3.5,Constants
v3.3.5,Start with some content:
v3.3.5,Clear & re-use:
v3.3.5,Output should match new content:
v3.3.5,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.3.5,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.3.5,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.3.5,Compute a new trial address & value & if it is a valid address:
v3.3.5,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.3.5,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.3.5,Check the just-stored value with getitem():
v3.3.5,Also store the just-stored value for future tracking:
v3.3.5,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.3.5,Test in 2 ways that the values are correctly laid out in memory:
v3.3.5,coding: utf-8
v3.3.5,we don't need lib_lightgbm while building docs
v3.3.5,coding: utf-8
v3.3.5,check saved model persistence
v3.3.5,"we need to check the consistency of model file here, so test for exact equal"
v3.3.5,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.3.5,"scores likely to be different, but prediction should still be the same"
v3.3.5,test that shape is checked during prediction
v3.3.5,"The simple implementation is just a single ""return self.ndarray[idx]"""
v3.3.5,The following is for demo and testing purpose.
v3.3.5,whole col
v3.3.5,half col
v3.3.5,Create dataset from numpy array directly.
v3.3.5,Create dataset using Sequence.
v3.3.5,Test for validation set.
v3.3.5,Select some random rows as valid data.
v3.3.5,"From Dataset constructor, with dataset from numpy array."
v3.3.5,"From Dataset.create_valid, with dataset from sequence."
v3.3.5,test that method works even with free_raw_data=True
v3.3.5,test that method works but sets raw data to None in case of immergeable data types
v3.3.5,test that method works for different data types
v3.3.5,"Set extremely harsh penalties, so CEGB will block most splits."
v3.3.5,"Compare pairs of penalties, to ensure scaling works as intended"
v3.3.5,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.3.5,"should resolve duplicate aliases, and prefer the main parameter"
v3.3.5,should choose a value from an alias and set that value on main param
v3.3.5,if only an alias is used
v3.3.5,should use the default if main param and aliases are missing
v3.3.5,all changes should be made on copies and not modify the original
v3.3.5,coding: utf-8
v3.3.5,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.3.5,set_index ensures partitions are based on group id.
v3.3.5,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.3.5,"separate target, weight from features."
v3.3.5,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.3.5,"so that within each partition, sum(g) = n_samples."
v3.3.5,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.3.5,make one categorical feature relevant to the target
v3.3.5,https://github.com/microsoft/LightGBM/issues/4118
v3.3.5,extra predict() parameters should be passed through correctly
v3.3.5,pref_leaf values should have the right shape
v3.3.5,and values that look like valid tree nodes
v3.3.5,"be sure LightGBM actually used at least one categorical column,"
v3.3.5,and that it was correctly treated as a categorical feature
v3.3.5,shape depends on whether it is binary or multiclass classification
v3.3.5,"in the special case of multi-class classification using scipy sparse matrices,"
v3.3.5,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v3.3.5,
v3.3.5,"since that case is so different than all other cases, check the relevant things here"
v3.3.5,and then return early
v3.3.5,"raw scores will probably be different, but at least check that all predicted classes are the same"
v3.3.5,"be sure LightGBM actually used at least one categorical column,"
v3.3.5,and that it was correctly treated as a categorical feature
v3.3.5,* shape depends on whether it is binary or multiclass classification
v3.3.5,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.3.5,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.3.5,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.5,that the output has the right shape and base values are in the right position
v3.3.5,check that found ports are different for same address (LocalCluster)
v3.3.5,check that the ports are indeed open
v3.3.5,Scores should be the same
v3.3.5,Predictions should be roughly the same.
v3.3.5,pref_leaf values should have the right shape
v3.3.5,and values that look like valid tree nodes
v3.3.5,extra predict() parameters should be passed through correctly
v3.3.5,"be sure LightGBM actually used at least one categorical column,"
v3.3.5,and that it was correctly treated as a categorical feature
v3.3.5,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.5,that the output has the right shape and base values are in the right position
v3.3.5,"be sure LightGBM actually used at least one categorical column,"
v3.3.5,and that it was correctly treated as a categorical feature
v3.3.5,Quantiles should be right
v3.3.5,"be sure LightGBM actually used at least one categorical column,"
v3.3.5,and that it was correctly treated as a categorical feature
v3.3.5,rebalance small dask.Array dataset for better performance.
v3.3.5,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.3.5,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.3.5,distributed ranker should be able to rank decently well and should
v3.3.5,have high rank correlation with scores from serial ranker.
v3.3.5,extra predict() parameters should be passed through correctly
v3.3.5,pref_leaf values should have the right shape
v3.3.5,and values that look like valid tree nodes
v3.3.5,"be sure LightGBM actually used at least one categorical column,"
v3.3.5,and that it was correctly treated as a categorical feature
v3.3.5,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v3.3.5,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v3.3.5,"test eval_class_weight, eval_init_score on binary-classification task."
v3.3.5,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v3.3.5,create eval_sets by creating new datasets or copying training data.
v3.3.5,total number of trees scales up for ova classifier.
v3.3.5,check that early stopping was not applied.
v3.3.5,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v3.3.5,"check that each eval_name and metric exists for all eval sets, allowing for the"
v3.3.5,case when a worker receives a fully-padded eval_set component which is not evaluated.
v3.3.5,should be able to use the class without specifying a client
v3.3.5,should be able to set client after construction
v3.3.5,data on cluster1
v3.3.5,create identical data on cluster2
v3.3.5,"at this point, the result of default_client() is client2 since it was the most recently"
v3.3.5,created. So setting client to client1 here to test that you can select a non-default client
v3.3.5,"unfitted model should survive pickling round trip, and pickling"
v3.3.5,shouldn't have side effects on the model object
v3.3.5,client will always be None after unpickling
v3.3.5,"fitted model should survive pickling round trip, and pickling"
v3.3.5,shouldn't have side effects on the model object
v3.3.5,client will always be None after unpickling
v3.3.5,rebalance data to be sure that each worker has a piece of the data
v3.3.5,model 1 - no network parameters given
v3.3.5,model 2 - machines given
v3.3.5,model 3 - local_listen_port given
v3.3.5,training should fail because LightGBM will try to use the same
v3.3.5,port for multiple worker processes on the same machine
v3.3.5,rebalance data to be sure that each worker has a piece of the data
v3.3.5,"test that ""machines"" is actually respected by creating a socket that uses"
v3.3.5,"one of the ports mentioned in ""machines"""
v3.3.5,The above error leaves a worker waiting
v3.3.5,"an informative error should be raised if ""machines"" has duplicates"
v3.3.5,"""client"" should be the only different, and the final argument"
v3.3.5,value of the root node is 0 when init_score is set
v3.3.5,this test is separate because it takes a not-yet-constructed estimator
v3.3.5,coding: utf-8
v3.3.5,coding: utf-8
v3.3.5,"build target, group ID vectors."
v3.3.5,build y/target and group-id vectors with user-specified group sizes.
v3.3.5,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.3.5,groups should contain > 1 element for pairwise learning objective.
v3.3.5,"build feature data, X. Transform first few into informative features."
v3.3.5,coding: utf-8
v3.3.5,prediction result is actually not transformed (is raw) due to custom objective
v3.3.5,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.3.5,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.3.5,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.5,sklearn < 0.23 does not have as_frame parameter
v3.3.5,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.5,sklearn < 0.23 does not have as_frame parameter
v3.3.5,Test if random_state is properly stored
v3.3.5,Test if two random states produce identical models
v3.3.5,Test if subsequent fits sample from random_state object and produce different models
v3.3.5,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.3.5,With default params
v3.3.5,Tests same probabilities
v3.3.5,Tests same predictions
v3.3.5,Tests same raw scores
v3.3.5,Tests same leaf indices
v3.3.5,Tests same feature contributions
v3.3.5,Tests other parameters for the prediction works
v3.3.5,Tests start_iteration
v3.3.5,"Tests same probabilities, starting from iteration 10"
v3.3.5,"Tests same predictions, starting from iteration 10"
v3.3.5,"Tests same raw scores, starting from iteration 10"
v3.3.5,"Tests same leaf indices, starting from iteration 10"
v3.3.5,"Tests same feature contributions, starting from iteration 10"
v3.3.5,"Tests other parameters for the prediction works, starting from iteration 10"
v3.3.5,"no custom objective, no custom metric"
v3.3.5,default metric
v3.3.5,non-default metric
v3.3.5,no metric
v3.3.5,non-default metric in eval_metric
v3.3.5,non-default metric with non-default metric in eval_metric
v3.3.5,non-default metric with multiple metrics in eval_metric
v3.3.5,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.3.5,default metric for non-default objective
v3.3.5,non-default metric for non-default objective
v3.3.5,no metric
v3.3.5,non-default metric in eval_metric for non-default objective
v3.3.5,non-default metric with non-default metric in eval_metric for non-default objective
v3.3.5,non-default metric with multiple metrics in eval_metric for non-default objective
v3.3.5,"custom objective, no custom metric"
v3.3.5,default regression metric for custom objective
v3.3.5,non-default regression metric for custom objective
v3.3.5,multiple regression metrics for custom objective
v3.3.5,no metric
v3.3.5,default regression metric with non-default metric in eval_metric for custom objective
v3.3.5,non-default regression metric with metric in eval_metric for custom objective
v3.3.5,multiple regression metrics with metric in eval_metric for custom objective
v3.3.5,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.3.5,"no custom objective, custom metric"
v3.3.5,default metric with custom metric
v3.3.5,non-default metric with custom metric
v3.3.5,multiple metrics with custom metric
v3.3.5,custom metric (disable default metric)
v3.3.5,default metric for non-default objective with custom metric
v3.3.5,non-default metric for non-default objective with custom metric
v3.3.5,multiple metrics for non-default objective with custom metric
v3.3.5,custom metric (disable default metric for non-default objective)
v3.3.5,"custom objective, custom metric"
v3.3.5,custom metric for custom objective
v3.3.5,non-default regression metric with custom metric for custom objective
v3.3.5,multiple regression metrics with custom metric for custom objective
v3.3.5,default metric and invalid binary metric is replaced with multiclass alternative
v3.3.5,invalid objective is replaced with default multiclass one
v3.3.5,and invalid binary metric is replaced with multiclass alternative
v3.3.5,default metric for non-default multiclass objective
v3.3.5,and invalid binary metric is replaced with multiclass alternative
v3.3.5,default metric and invalid multiclass metric is replaced with binary alternative
v3.3.5,invalid multiclass metric is replaced with binary alternative for custom objective
v3.3.5,"Verify that can receive a list of metrics, only callable"
v3.3.5,Verify that can receive a list of custom and built-in metrics
v3.3.5,Verify that works as expected when eval_metric is empty
v3.3.5,"Verify that can receive a list of metrics, only built-in"
v3.3.5,Verify that eval_metric is robust to receiving a list with None
v3.3.5,training data as eval_set
v3.3.5,feval
v3.3.5,single eval_set
v3.3.5,two eval_set
v3.3.5,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.3.5,Test that estimators are default-constructible
v3.3.5,coding: utf-8
v3.3.5,coding: utf-8
v3.3.5,check that default gives same result as k = 1
v3.3.5,check against independent calculation for k = 1
v3.3.5,check against independent calculation for k = 2
v3.3.5,check against independent calculation for k = 10
v3.3.5,check cases where predictions are equal
v3.3.5,should give same result as binary auc for 2 classes
v3.3.5,test the case where all predictions are equal
v3.3.5,test that weighted data gives different auc_mu
v3.3.5,test that equal data weights give same auc_mu as unweighted data
v3.3.5,should give 1 when accuracy = 1
v3.3.5,test loading class weights
v3.3.5,no early stopping
v3.3.5,early stopping occurs
v3.3.5,test custom eval metrics
v3.3.5,"shuffle = False, override metric in params"
v3.3.5,"shuffle = True, callbacks"
v3.3.5,enable display training loss
v3.3.5,self defined folds
v3.3.5,LambdaRank
v3.3.5,... with l2 metric
v3.3.5,... with NDCG (default) metric
v3.3.5,self defined folds with lambdarank
v3.3.5,with early stopping
v3.3.5,predict by each fold booster
v3.3.5,fold averaging
v3.3.5,without early stopping
v3.3.5,test feature_names with whitespaces
v3.3.5,This has non-ascii strings.
v3.3.5,take subsets and train
v3.3.5,generate CSR sparse dataset
v3.3.5,convert data to dense and get back same contribs
v3.3.5,validate the values are the same
v3.3.5,validate using CSC matrix
v3.3.5,validate the values are the same
v3.3.5,generate CSR sparse dataset
v3.3.5,convert data to dense and get back same contribs
v3.3.5,validate the values are the same
v3.3.5,validate using CSC matrix
v3.3.5,validate the values are the same
v3.3.5,Note there is an extra column added to the output for the expected value
v3.3.5,Note output CSC shape should be same as CSR output shape
v3.3.5,test sliced labels
v3.3.5,append some columns
v3.3.5,append some rows
v3.3.5,test sliced 2d matrix
v3.3.5,test sliced CSR
v3.3.5,trees start at position 1.
v3.3.5,split_features are in 4th line.
v3.3.5,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.3.5,The penalization is so high that the first 2 features should not be used here
v3.3.5,Check that a very high penalization is the same as not using the features at all
v3.3.5,"no fobj, no feval"
v3.3.5,default metric
v3.3.5,non-default metric in params
v3.3.5,default metric in args
v3.3.5,non-default metric in args
v3.3.5,metric in args overwrites one in params
v3.3.5,multiple metrics in params
v3.3.5,multiple metrics in args
v3.3.5,remove default metric by 'None' in list
v3.3.5,remove default metric by 'None' aliases
v3.3.5,"fobj, no feval"
v3.3.5,no default metric
v3.3.5,metric in params
v3.3.5,metric in args
v3.3.5,metric in args overwrites its' alias in params
v3.3.5,multiple metrics in params
v3.3.5,multiple metrics in args
v3.3.5,"no fobj, feval"
v3.3.5,default metric with custom one
v3.3.5,non-default metric in params with custom one
v3.3.5,default metric in args with custom one
v3.3.5,non-default metric in args with custom one
v3.3.5,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.5,multiple metrics in params with custom one
v3.3.5,multiple metrics in args with custom one
v3.3.5,custom metric is evaluated despite 'None' is passed
v3.3.5,"fobj, feval"
v3.3.5,"no default metric, only custom one"
v3.3.5,metric in params with custom one
v3.3.5,metric in args with custom one
v3.3.5,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.5,multiple metrics in params with custom one
v3.3.5,multiple metrics in args with custom one
v3.3.5,custom metric is evaluated despite 'None' is passed
v3.3.5,"no fobj, no feval"
v3.3.5,default metric
v3.3.5,default metric in params
v3.3.5,non-default metric in params
v3.3.5,multiple metrics in params
v3.3.5,remove default metric by 'None' aliases
v3.3.5,"fobj, no feval"
v3.3.5,no default metric
v3.3.5,metric in params
v3.3.5,multiple metrics in params
v3.3.5,"no fobj, feval"
v3.3.5,default metric with custom one
v3.3.5,default metric in params with custom one
v3.3.5,non-default metric in params with custom one
v3.3.5,multiple metrics in params with custom one
v3.3.5,custom metric is evaluated despite 'None' is passed
v3.3.5,"fobj, feval"
v3.3.5,"no default metric, only custom one"
v3.3.5,metric in params with custom one
v3.3.5,multiple metrics in params with custom one
v3.3.5,custom metric is evaluated despite 'None' is passed
v3.3.5,multiclass default metric
v3.3.5,multiclass default metric with custom one
v3.3.5,multiclass metric alias with custom one for custom objective
v3.3.5,no metric for invalid class_num
v3.3.5,custom metric for invalid class_num
v3.3.5,multiclass metric alias with custom one with invalid class_num
v3.3.5,multiclass default metric without num_class
v3.3.5,multiclass metric alias
v3.3.5,multiclass metric
v3.3.5,non-valid metric for multiclass objective
v3.3.5,non-default num_class for default objective
v3.3.5,no metric with non-default num_class for custom objective
v3.3.5,multiclass metric alias for custom objective
v3.3.5,multiclass metric for custom objective
v3.3.5,binary metric with non-default num_class for custom objective
v3.3.5,Expect three metrics but mean and stdv for each metric
v3.3.5,test XGBoost-style return value
v3.3.5,test numpy-style return value
v3.3.5,test bins string type
v3.3.5,test histogram is disabled for categorical features
v3.3.5,test for lgb.train
v3.3.5,test feval for lgb.train
v3.3.5,test with two valid data for lgb.train
v3.3.5,test for lgb.cv
v3.3.5,test feval for lgb.cv
v3.3.5,test that binning works properly for features with only positive or only negative values
v3.3.5,decreasing without freeing raw data is allowed
v3.3.5,decreasing before lazy init is allowed
v3.3.5,increasing is allowed
v3.3.5,decreasing with disabled filter is allowed
v3.3.5,decreasing with enabled filter is disallowed;
v3.3.5,also changes of other params are disallowed
v3.3.5,check extra trees increases regularization
v3.3.5,check path smoothing increases regularization
v3.3.5,test edge case with one leaf
v3.3.5,check that constraint containing all features is equivalent to no constraint
v3.3.5,check that constraint partitioning the features reduces train accuracy
v3.3.5,check that constraints consisting of single features reduce accuracy further
v3.3.5,test that interaction constraints work when not all features are used
v3.3.5,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.3.5,test again with nans in data
v3.3.5,test again with bagging
v3.3.5,test with a feature that has only one non-nan value
v3.3.5,test with a categorical feature
v3.3.5,test refit: same results on same data
v3.3.5,test refit with save and load
v3.3.5,test refit: different results training on different data
v3.3.5,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.3.5,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.3.5,"test the case where start_iteration <= 0, and num_iteration is None"
v3.3.5,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.3.5,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.3.5,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.3.5,test for regression
v3.3.5,test both with and without early stopping
v3.3.5,test for multi-class
v3.3.5,test both with and without early stopping
v3.3.5,test for binary
v3.3.5,test both with and without early stopping
v3.3.5,test against sklearn average precision metric
v3.3.5,test that average precision is 1 where model predicts perfectly
v3.3.5,coding: utf-8
v3.3.5,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.3.5,coding: utf-8
v3.3.5,coding: utf-8
v3.3.5,These are helper functions to allow doing a stack unwind
v3.3.5,"after an R allocation error, which would trigger a long jump."
v3.3.5,convert from one-based to zero-based index
v3.3.5,"if any feature names were larger than allocated size,"
v3.3.5,allow for a larger size and try again
v3.3.5,convert from boundaries to size
v3.3.5,--- start Booster interfaces
v3.3.5,"if any eval names were larger than allocated size,"
v3.3.5,allow for a larger size and try again
v3.3.5,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.5,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.5,.Call() calls
v3.3.5,coding: utf-8
v3.3.5,alias table
v3.3.5,names
v3.3.5,from strings
v3.3.5,tails
v3.3.5,tails
v3.3.5,coding: utf-8
v3.3.5,Single row predictor to abstract away caching logic
v3.3.5,create boosting
v3.3.5,initialize the boosting
v3.3.5,create objective function
v3.3.5,initialize the objective function
v3.3.5,create training metric
v3.3.5,reset the boosting
v3.3.5,create objective function
v3.3.5,initialize the objective function
v3.3.5,calculate the nonzero data and indices size
v3.3.5,allocate data and indices arrays
v3.3.5,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.5,aggregated per row feature contribution results
v3.3.5,keep track of the row_vector sizes for parallelization
v3.3.5,copy vector results to output for each row
v3.3.5,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.5,aggregated per row feature contribution results
v3.3.5,calculate number of elements per column to construct
v3.3.5,the CSC matrix with random access
v3.3.5,keep track of column counts
v3.3.5,keep track of beginning index for each column
v3.3.5,keep track of beginning index for each matrix
v3.3.5,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.3.5,store the row index
v3.3.5,update column count
v3.3.5,explicitly declare symbols from LightGBM namespace
v3.3.5,some help functions used to convert data
v3.3.5,Row iterator of on column for CSC matrix
v3.3.5,"return value at idx, only can access by ascent order"
v3.3.5,"return next non-zero pair, if index < 0, means no more data"
v3.3.5,start of c_api functions
v3.3.5,This API is to keep python binding's behavior the same with C++ implementation.
v3.3.5,"Sample count, random seed etc. should be provided in parameters."
v3.3.5,sample data first
v3.3.5,sample data first
v3.3.5,sample data first
v3.3.5,local buffer to re-use memory
v3.3.5,sample data first
v3.3.5,no more data
v3.3.5,---- start of booster
v3.3.5,Single row in row-major format:
v3.3.5,---- start of some help functions
v3.3.5,data is array of pointers to individual rows
v3.3.5,set number of threads for openmp
v3.3.5,check for alias
v3.3.5,read parameters from config file
v3.3.5,"remove str after ""#"""
v3.3.5,check for alias again
v3.3.5,load configs
v3.3.5,prediction is needed if using input initial model(continued train)
v3.3.5,need to continue training
v3.3.5,sync up random seed for data partition
v3.3.5,load Training data
v3.3.5,load data for distributed training
v3.3.5,load data for single machine
v3.3.5,need save binary file
v3.3.5,create training metric
v3.3.5,only when have metrics then need to construct validation data
v3.3.5,"Add validation data, if it exists"
v3.3.5,add
v3.3.5,need save binary file
v3.3.5,add metric for validation data
v3.3.5,output used time on each iteration
v3.3.5,need init network
v3.3.5,create boosting
v3.3.5,create objective function
v3.3.5,load training data
v3.3.5,initialize the objective function
v3.3.5,initialize the boosting
v3.3.5,add validation data into boosting
v3.3.5,convert model to if-else statement code
v3.3.5,create predictor
v3.3.5,Free memory
v3.3.5,create predictor
v3.3.5,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.3.5,counts for all labels
v3.3.5,"start from top label, and accumulate DCG"
v3.3.5,counts for all labels
v3.3.5,calculate k Max DCG by one pass
v3.3.5,get sorted indices by score
v3.3.5,calculate multi dcg by one pass
v3.3.5,wait for all client start up
v3.3.5,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.3.5,Instead we will handle finalize/abort for MPI in main().
v3.3.5,default set to -1
v3.3.5,"distance at k-th communication, distance[k] = 2^k"
v3.3.5,set incoming rank at k-th commuication
v3.3.5,set outgoing rank at k-th commuication
v3.3.5,default set as -1
v3.3.5,construct all recursive halving map for all machines
v3.3.5,let 1 << k <= num_machines
v3.3.5,distance of each communication
v3.3.5,"if num_machines = 2^k, don't need to group machines"
v3.3.5,"communication direction, %2 == 0 is positive"
v3.3.5,neighbor at k-th communication
v3.3.5,receive data block at k-th communication
v3.3.5,send data block at k-th communication
v3.3.5,"if num_machines != 2^k, need to group machines"
v3.3.5,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.3.5,let left machine as group leader
v3.3.5,"cache block information for groups, group with 2 machines will have double block size"
v3.3.5,convert from group to node leader
v3.3.5,convert from node to group
v3.3.5,meet new group
v3.3.5,add block len for this group
v3.3.5,calculate the group block start
v3.3.5,not need to construct
v3.3.5,get receive block information
v3.3.5,accumulate block len
v3.3.5,get send block information
v3.3.5,accumulate block len
v3.3.5,static member definition
v3.3.5,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.3.5,assign the blocks to every rank.
v3.3.5,do reduce scatter
v3.3.5,do all gather
v3.3.5,assign blocks
v3.3.5,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.3.5,copy back
v3.3.5,assign blocks
v3.3.5,start all gather
v3.3.5,when num_machines is small and data is large
v3.3.5,use output as receive buffer
v3.3.5,get current local block size
v3.3.5,get out rank
v3.3.5,get in rank
v3.3.5,get send information
v3.3.5,get recv information
v3.3.5,send and recv at same time
v3.3.5,rotate in-place
v3.3.5,use output as receive buffer
v3.3.5,get current local block size
v3.3.5,get send information
v3.3.5,get recv information
v3.3.5,send and recv at same time
v3.3.5,use output as receive buffer
v3.3.5,send and recv at same time
v3.3.5,send local data to neighbor first
v3.3.5,receive neighbor data first
v3.3.5,reduce
v3.3.5,get target
v3.3.5,get send information
v3.3.5,get recv information
v3.3.5,send and recv at same time
v3.3.5,reduce
v3.3.5,send result to neighbor
v3.3.5,receive result from neighbor
v3.3.5,copy result
v3.3.5,start up socket
v3.3.5,parse clients from file
v3.3.5,get ip list of local machine
v3.3.5,get local rank
v3.3.5,construct listener
v3.3.5,construct communication topo
v3.3.5,construct linkers
v3.3.5,free listener
v3.3.5,set timeout
v3.3.5,accept incoming socket
v3.3.5,receive rank
v3.3.5,add new socket
v3.3.5,save ranks that need to connect with
v3.3.5,start listener
v3.3.5,start connect
v3.3.5,let smaller rank connect to larger rank
v3.3.5,send local rank
v3.3.5,wait for listener
v3.3.5,print connected linkers
v3.3.5,only need to copy subset
v3.3.5,avoid to copy subset many times
v3.3.5,avoid out of range
v3.3.5,may need to recopy subset
v3.3.5,valid the type
v3.3.5,Constructors
v3.3.5,Get type tag
v3.3.5,Comparisons
v3.3.5,"This has to be separate, not in Statics, because Json() accesses"
v3.3.5,statics().null.
v3.3.5,"advance until next line, or end of input"
v3.3.5,advance until closing tokens
v3.3.5,The usual case: non-escaped characters
v3.3.5,Handle escapes
v3.3.5,Extract 4-byte escape sequence
v3.3.5,Explicitly check length of the substring. The following loop
v3.3.5,relies on std::string returning the terminating NUL when
v3.3.5,accessing str[length]. Checking here reduces brittleness.
v3.3.5,JSON specifies that characters outside the BMP shall be encoded as a
v3.3.5,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.3.5,components. Check whether we're in the middle of such a beast: the
v3.3.5,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.3.5,a trail (low) surrogate.
v3.3.5,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.3.5,per the UTF-16 algorithm.
v3.3.5,Integer part
v3.3.5,Decimal part
v3.3.5,Exponent part
v3.3.5,Check for any trailing garbage
v3.3.5,Documented in json11.hpp
v3.3.5,Check for another object
v3.3.5,get column names
v3.3.5,load label idx first
v3.3.5,erase label column name
v3.3.5,load ignore columns
v3.3.5,load weight idx
v3.3.5,load group idx
v3.3.5,don't support query id in data file when using distributed training
v3.3.5,read data to memory
v3.3.5,sample data
v3.3.5,construct feature bin mappers
v3.3.5,initialize label
v3.3.5,extract features
v3.3.5,sample data from file
v3.3.5,construct feature bin mappers
v3.3.5,initialize label
v3.3.5,extract features
v3.3.5,load data from binary file
v3.3.5,check meta data
v3.3.5,need to check training data
v3.3.5,read data in memory
v3.3.5,initialize label
v3.3.5,extract features
v3.3.5,Get number of lines of data file
v3.3.5,initialize label
v3.3.5,extract features
v3.3.5,load data from binary file
v3.3.5,not need to check validation data
v3.3.5,check meta data
v3.3.5,buffer to read binary file
v3.3.5,check token
v3.3.5,read size of header
v3.3.5,re-allocmate space if not enough
v3.3.5,read header
v3.3.5,get header
v3.3.5,num_groups
v3.3.5,real_feature_idx_
v3.3.5,feature2group
v3.3.5,feature2subfeature
v3.3.5,group_bin_boundaries
v3.3.5,group_feature_start_
v3.3.5,group_feature_cnt_
v3.3.5,get feature names
v3.3.5,write feature names
v3.3.5,get forced_bin_bounds_
v3.3.5,read size of meta data
v3.3.5,re-allocate space if not enough
v3.3.5,read meta data
v3.3.5,load meta data
v3.3.5,sample local used data if need to partition
v3.3.5,"if not contain query file, minimal sample unit is one record"
v3.3.5,"if contain query file, minimal sample unit is one query"
v3.3.5,if is new query
v3.3.5,read feature data
v3.3.5,read feature size
v3.3.5,re-allocate space if not enough
v3.3.5,raw data
v3.3.5,fill feature_names_ if not header
v3.3.5,get forced split
v3.3.5,"if only one machine, find bin locally"
v3.3.5,"if have multi-machines, need to find bin distributed"
v3.3.5,different machines will find bin for different features
v3.3.5,start and len will store the process feature indices for different machines
v3.3.5,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.5,free
v3.3.5,gather global feature bin mappers
v3.3.5,restore features bins from buffer
v3.3.5,---- private functions ----
v3.3.5,"if features are ordered, not need to use hist_buf"
v3.3.5,read all lines
v3.3.5,get query data
v3.3.5,"if not contain query data, minimal sample unit is one record"
v3.3.5,"if contain query data, minimal sample unit is one query"
v3.3.5,if is new query
v3.3.5,get query data
v3.3.5,"if not contain query file, minimal sample unit is one record"
v3.3.5,"if contain query file, minimal sample unit is one query"
v3.3.5,if is new query
v3.3.5,parse features
v3.3.5,get forced split
v3.3.5,"check the range of label_idx, weight_idx and group_idx"
v3.3.5,fill feature_names_ if not header
v3.3.5,start find bins
v3.3.5,"if only one machine, find bin locally"
v3.3.5,start and len will store the process feature indices for different machines
v3.3.5,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.5,free
v3.3.5,gather global feature bin mappers
v3.3.5,restore features bins from buffer
v3.3.5,if doesn't need to prediction with initial model
v3.3.5,parser
v3.3.5,set label
v3.3.5,free processed line:
v3.3.5,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.3.5,text_reader_->Lines()[i].shrink_to_fit();
v3.3.5,push data
v3.3.5,if is used feature
v3.3.5,if need to prediction with initial model
v3.3.5,parser
v3.3.5,set initial score
v3.3.5,set label
v3.3.5,free processed line:
v3.3.5,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v3.3.5,text_reader_->Lines()[i].shrink_to_fit();
v3.3.5,push data
v3.3.5,if is used feature
v3.3.5,metadata_ will manage space of init_score
v3.3.5,text data can be free after loaded feature values
v3.3.5,parser
v3.3.5,set initial score
v3.3.5,set label
v3.3.5,push data
v3.3.5,if is used feature
v3.3.5,only need part of data
v3.3.5,need full data
v3.3.5,metadata_ will manage space of init_score
v3.3.5,read size of token
v3.3.5,remove duplicates
v3.3.5,deep copy function for BinMapper
v3.3.5,mean size for one bin
v3.3.5,need a new bin
v3.3.5,update bin upper bound
v3.3.5,last bin upper bound
v3.3.5,get list of distinct values
v3.3.5,get number of positive and negative distinct values
v3.3.5,include zero bounds and infinity bound
v3.3.5,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.3.5,find remaining bounds
v3.3.5,find distinct_values first
v3.3.5,push zero in the front
v3.3.5,use the large value
v3.3.5,push zero in the back
v3.3.5,convert to int type first
v3.3.5,sort by counts
v3.3.5,will ignore the categorical of small counts
v3.3.5,Push the dummy bin for NaN
v3.3.5,Use MissingType::None to represent this bin contains all categoricals
v3.3.5,fix count of NaN bin
v3.3.5,check trivial(num_bin_ == 1) feature
v3.3.5,check useless bin
v3.3.5,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.3.5,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.3.5,calculate max bin of all features to select the int type in MultiValDenseBin
v3.3.5,"for lambdarank, it needs query data for partition data in distributed learning"
v3.3.5,need convert query_id to boundaries
v3.3.5,check weights
v3.3.5,check query boundries
v3.3.5,contain initial score file
v3.3.5,check weights
v3.3.5,get local weights
v3.3.5,check query boundries
v3.3.5,get local query boundaries
v3.3.5,contain initial score file
v3.3.5,get local initial scores
v3.3.5,re-load query weight
v3.3.5,save to nullptr
v3.3.5,save to nullptr
v3.3.5,save to nullptr
v3.3.5,default weight file name
v3.3.5,default init_score file name
v3.3.5,use first line to count number class
v3.3.5,default query file name
v3.3.5,root is in the depth 0
v3.3.5,non-leaf
v3.3.5,leaf
v3.3.5,use this for the missing value conversion
v3.3.5,Predict func by Map to ifelse
v3.3.5,use this for the missing value conversion
v3.3.5,non-leaf
v3.3.5,left subtree
v3.3.5,right subtree
v3.3.5,leaf
v3.3.5,non-leaf
v3.3.5,left subtree
v3.3.5,right subtree
v3.3.5,leaf
v3.3.5,recursive computation of SHAP values for a decision tree
v3.3.5,extend the unique path
v3.3.5,leaf node
v3.3.5,internal node
v3.3.5,"see if we have already split on this feature,"
v3.3.5,if so we undo that split so we can redo it for this node
v3.3.5,recursive sparse computation of SHAP values for a decision tree
v3.3.5,extend the unique path
v3.3.5,leaf node
v3.3.5,internal node
v3.3.5,"see if we have already split on this feature,"
v3.3.5,if so we undo that split so we can redo it for this node
v3.3.5,add names of objective function if not providing metric
v3.3.5,equal weights for all classes
v3.3.5,generate seeds by seed.
v3.3.5,sort eval_at
v3.3.5,Only push the non-training data
v3.3.5,check for conflicts
v3.3.5,"check if objective, metric, and num_class match"
v3.3.5,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.3.5,Check max_depth and num_leaves
v3.3.5,"Fits in an int, and is more restrictive than the current num_leaves"
v3.3.5,force col-wise for gpu & CUDA
v3.3.5,force gpu_use_dp for CUDA
v3.3.5,linear tree learner must be serial type and run on CPU device
v3.3.5,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.3.5,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.3.5,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.3.5,calculated split gain can be positive even with zero gradient and hessian.
v3.3.5,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.3.5,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.3.5,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.3.5,first round: fill the single val group
v3.3.5,always push the last group
v3.3.5,put dense feature first
v3.3.5,sort by non zero cnt
v3.3.5,"sort by non zero cnt, bigger first"
v3.3.5,shuffle groups
v3.3.5,Using std::swap for vector<bool> will cause the wrong result.
v3.3.5,get num_features
v3.3.5,get bin_mappers
v3.3.5,"for sparse multi value bin, we store the feature bin values with offset added"
v3.3.5,"for dense multi value bin, the feature bin values without offsets are used"
v3.3.5,copy feature bin mapper data
v3.3.5,copy feature bin mapper data
v3.3.5,"if not pass a filename, just append "".bin"" of original file"
v3.3.5,get size of header
v3.3.5,size of feature names
v3.3.5,size of forced bins
v3.3.5,write header
v3.3.5,write feature names
v3.3.5,write forced bins
v3.3.5,get size of meta data
v3.3.5,write meta data
v3.3.5,write feature data
v3.3.5,get size of feature
v3.3.5,write feature
v3.3.5,write raw data; use row-major order so we can read row-by-row
v3.3.5,"explicitly initialize template methods, for cross module call"
v3.3.5,"Only one multi-val group, just simply merge"
v3.3.5,Skip the leading 0 when copying group_bin_boundaries.
v3.3.5,regenerate other fields
v3.3.5,store the importance first
v3.3.5,PredictRaw
v3.3.5,PredictRawByMap
v3.3.5,Predict
v3.3.5,PredictByMap
v3.3.5,PredictLeafIndex
v3.3.5,PredictLeafIndexByMap
v3.3.5,output model type
v3.3.5,output number of class
v3.3.5,output label index
v3.3.5,output max_feature_idx
v3.3.5,output objective
v3.3.5,output tree models
v3.3.5,store the importance first
v3.3.5,sort the importance
v3.3.5,use serialized string to restore this object
v3.3.5,Use first 128 chars to avoid exceed the message buffer.
v3.3.5,get number of classes
v3.3.5,get index of label
v3.3.5,get max_feature_idx first
v3.3.5,get average_output
v3.3.5,get feature names
v3.3.5,get monotone_constraints
v3.3.5,set zero
v3.3.5,predict all the trees for one iteration
v3.3.5,check early stopping
v3.3.5,set zero
v3.3.5,predict all the trees for one iteration
v3.3.5,check early stopping
v3.3.5,margin_threshold will be captured by value
v3.3.5,copy and sort
v3.3.5,margin_threshold will be captured by value
v3.3.5,Fix for compiler warnings about reaching end of control
v3.3.5,load forced_splits file
v3.3.5,init tree learner
v3.3.5,push training metrics
v3.3.5,create buffer for gradients and Hessians
v3.3.5,get max feature index
v3.3.5,get label index
v3.3.5,get feature names
v3.3.5,"if need bagging, create buffer"
v3.3.5,"for a validation dataset, we need its score and metric"
v3.3.5,update score
v3.3.5,objective function will calculate gradients and hessians
v3.3.5,"random bagging, minimal unit is one record"
v3.3.5,"random bagging, minimal unit is one record"
v3.3.5,if need bagging
v3.3.5,set bagging data to tree learner
v3.3.5,get subset
v3.3.5,output used time per iteration
v3.3.5,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.3.5,boosting first
v3.3.5,bagging logic
v3.3.5,need to copy gradients for bagging subset.
v3.3.5,shrinkage by learning rate
v3.3.5,update score
v3.3.5,only add default score one-time
v3.3.5,updates scores
v3.3.5,add model
v3.3.5,reset score
v3.3.5,remove model
v3.3.5,print message for metric
v3.3.5,pop last early_stopping_round_ models
v3.3.5,update training score
v3.3.5,we need to predict out-of-bag scores of data for boosting
v3.3.5,update validation score
v3.3.5,print training metric
v3.3.5,print validation metric
v3.3.5,set zero
v3.3.5,predict all the trees for one iteration
v3.3.5,predict all the trees for one iteration
v3.3.5,push training metrics
v3.3.5,"not same training data, need reset score and others"
v3.3.5,create score tracker
v3.3.5,update score
v3.3.5,create buffer for gradients and hessians
v3.3.5,load forced_splits file
v3.3.5,"if need bagging, create buffer"
v3.3.5,Get the max size of pool
v3.3.5,at least need 2 leaves
v3.3.5,push split information for all leaves
v3.3.5,initialize splits for leaf
v3.3.5,initialize data partition
v3.3.5,initialize ordered gradients and hessians
v3.3.5,cannot change is_hist_col_wise during training
v3.3.5,initialize splits for leaf
v3.3.5,initialize data partition
v3.3.5,initialize ordered gradients and hessians
v3.3.5,Get the max size of pool
v3.3.5,at least need 2 leaves
v3.3.5,push split information for all leaves
v3.3.5,some initial works before training
v3.3.5,root leaf
v3.3.5,only root leaf can be splitted on first time
v3.3.5,some initial works before finding best split
v3.3.5,find best threshold for every feature
v3.3.5,Get a leaf with max split gain
v3.3.5,Get split information for best leaf
v3.3.5,"cannot split, quit"
v3.3.5,split tree with best leaf
v3.3.5,reset histogram pool
v3.3.5,initialize data partition
v3.3.5,reset the splits for leaves
v3.3.5,Sumup for root
v3.3.5,use all data
v3.3.5,"use bagging, only use part of data"
v3.3.5,check depth of current leaf
v3.3.5,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.3.5,no enough data to continue
v3.3.5,only have root
v3.3.5,put parent(left) leaf's histograms into larger leaf's histograms
v3.3.5,put parent(left) leaf's histograms to larger leaf's histograms
v3.3.5,construct smaller leaf
v3.3.5,construct larger leaf
v3.3.5,find splits
v3.3.5,only has root leaf
v3.3.5,start at root leaf
v3.3.5,"before processing next node from queue, store info for current left/right leaf"
v3.3.5,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.3.5,"then, compute own splits"
v3.3.5,split info should exist because searching in bfs fashion - should have added from parent
v3.3.5,update before tree split
v3.3.5,don't need to update this in data-based parallel model
v3.3.5,"split tree, will return right leaf"
v3.3.5,store the true split gain in tree model
v3.3.5,don't need to update this in data-based parallel model
v3.3.5,store the true split gain in tree model
v3.3.5,init the leaves that used on next iteration
v3.3.5,update leave outputs if needed
v3.3.5,bag_mapper[index_mapper[i]]
v3.3.5,it is needed to filter the features after the above code.
v3.3.5,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.3.5,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.3.5,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.3.5,find splits
v3.3.5,identify features containing nans
v3.3.5,preallocate the matrix used to calculate linear model coefficients
v3.3.5,"store only upper triangular half of matrix as an array, in row-major order"
v3.3.5,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.3.5,we add another 8 to ensure cache lines are not shared among processors
v3.3.5,some initial works before training
v3.3.5,root leaf
v3.3.5,only root leaf can be splitted on first time
v3.3.5,some initial works before finding best split
v3.3.5,find best threshold for every feature
v3.3.5,Get a leaf with max split gain
v3.3.5,Get split information for best leaf
v3.3.5,"cannot split, quit"
v3.3.5,split tree with best leaf
v3.3.5,map data to leaf number
v3.3.5,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.3.5,the coefficients vector is given by
v3.3.5,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.3.5,where:
v3.3.5,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.3.5,"H is the diagonal matrix of the hessian,"
v3.3.5,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.3.5,g is the vector of gradients
v3.3.5,the subscript _T denotes the transpose
v3.3.5,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.3.5,clear the coefficient matrices
v3.3.5,aggregate results from different threads
v3.3.5,copy into eigen matrices and solve
v3.3.5,update the tree properties
v3.3.5,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.5,get feature partition
v3.3.5,get local used features
v3.3.5,get best split at smaller leaf
v3.3.5,find local best split for larger leaf
v3.3.5,sync global best info
v3.3.5,update best split
v3.3.5,"instantiate template classes, otherwise linker cannot find the code"
v3.3.5,initialize SerialTreeLearner
v3.3.5,Get local rank and global machine size
v3.3.5,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.5,allocate buffer for communication
v3.3.5,generate feature partition for current tree
v3.3.5,get local used feature
v3.3.5,get block start and block len for reduce scatter
v3.3.5,get buffer_write_start_pos_
v3.3.5,get buffer_read_start_pos_
v3.3.5,sync global data sumup info
v3.3.5,global sumup reduce
v3.3.5,copy back
v3.3.5,set global sumup info
v3.3.5,init global data count in leaf
v3.3.5,clear histogram buffer before synchronizing
v3.3.5,otherwise histogram contents from the previous iteration will be sent
v3.3.5,construct local histograms
v3.3.5,copy to buffer
v3.3.5,Reduce scatter for histogram
v3.3.5,restore global histograms from buffer
v3.3.5,only root leaf
v3.3.5,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.3.5,find local best split for larger leaf
v3.3.5,sync global best info
v3.3.5,set best split
v3.3.5,need update global number of data in leaf
v3.3.5,"instantiate template classes, otherwise linker cannot find the code"
v3.3.5,initialize SerialTreeLearner
v3.3.5,some additional variables needed for GPU trainer
v3.3.5,Initialize GPU buffers and kernels
v3.3.5,some functions used for debugging the GPU histogram construction
v3.3.5,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.3.5,goto err;
v3.3.5,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.5,also guarantee that there are at least 2K examples per workgroup
v3.3.5,return 0;
v3.3.5,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v3.3.5,decide the best number of workgroups working on one feature4 tuple
v3.3.5,set work group size based on feature size
v3.3.5,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.5,we need to refresh the kernel arguments after reallocating
v3.3.5,The only argument that needs to be changed later is num_data_
v3.3.5,"the GPU kernel will process all features in one call, and each"
v3.3.5,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.3.5,process one feature4 tuple
v3.3.5,"for the root node, indices are not copied"
v3.3.5,"for constant hessian, hessians are not copied except for the root node"
v3.3.5,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.3.5,and we will launch num_feature workgroups for this kernel
v3.3.5,will launch threads for all features
v3.3.5,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.3.5,copy the results asynchronously. Size depends on if double precision is used
v3.3.5,we will wait for this object in WaitAndGetHistograms
v3.3.5,"when the output is ready, the computation is done"
v3.3.5,values of this feature has been redistributed to multiple bins; need a reduction here
v3.3.5,how many feature-group tuples we have
v3.3.5,leave some safe margin for prefetching
v3.3.5,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.5,clear sparse/dense maps
v3.3.5,do nothing if no features can be processed on GPU
v3.3.5,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.3.5,unpin old buffer if necessary before destructing them
v3.3.5,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v3.3.5,allocate space for gradients and Hessians on device
v3.3.5,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.5,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.3.5,copy indices to the device
v3.3.5,histogram bin entry size depends on the precision (single/double)
v3.3.5,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.5,each work group generates a sub-histogram of dword_features_ features.
v3.3.5,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.5,create atomic counters for inter-group coordination
v3.3.5,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.5,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.5,looking for dword_features_ non-sparse feature-groups
v3.3.5,decide if we need to redistribute the bin
v3.3.5,multiplier must be a power of 2
v3.3.5,device_bin_mults_.push_back(1);
v3.3.5,found
v3.3.5,for data transfer time
v3.3.5,"Now generate new data structure feature4, and copy data to the device"
v3.3.5,"preallocate arrays for all threads, and pin them"
v3.3.5,building Feature4 bundles; each thread handles dword_features_ features
v3.3.5,one feature datapoint is 4 bits
v3.3.5,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.5,one feature datapoint is one byte
v3.3.5,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.5,Dense bin
v3.3.5,Dense 4-bit bin
v3.3.5,working on the remaining (less than dword_features_) feature groups
v3.3.5,fill the leftover features
v3.3.5,"fill this empty feature with some ""random"" value"
v3.3.5,"fill this empty feature with some ""random"" value"
v3.3.5,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.3.5,deallocate pinned space for feature copying
v3.3.5,data transfer time
v3.3.5,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.3.5,"Something bad happened. Just return ""No log available."""
v3.3.5,"build is okay, log may contain warnings"
v3.3.5,destroy any old kernels
v3.3.5,create OpenCL kernels for different number of workgroups per feature
v3.3.5,currently we don't use constant memory
v3.3.5,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.3.5,kernel with indices in an array
v3.3.5,"kernel with all features enabled, with eliminated branches"
v3.3.5,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.3.5,do nothing if no features can be processed on GPU
v3.3.5,The only argument that needs to be changed later is num_data_
v3.3.5,"hessian is passed as a parameter, but it is not available now."
v3.3.5,hessian will be set in BeforeTrain()
v3.3.5,"Get the max bin size, used for selecting best GPU kernel"
v3.3.5,initialize GPU
v3.3.5,determine which kernel to use based on the max number of bins
v3.3.5,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.5,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.5,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.5,setup GPU kernel arguments after we allocating all the buffers
v3.3.5,GPU memory has to been reallocated because data may have been changed
v3.3.5,setup GPU kernel arguments after we allocating all the buffers
v3.3.5,Copy initial full hessians and gradients to GPU.
v3.3.5,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.5,setup hessian parameters only
v3.3.5,hessian is passed as a parameter
v3.3.5,use bagging
v3.3.5,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v3.3.5,copy used gradients and Hessians to ordered buffer
v3.3.5,transfer the indices to GPU
v3.3.5,transfer hessian to GPU
v3.3.5,setup hessian parameters only
v3.3.5,hessian is passed as a parameter
v3.3.5,transfer gradients to GPU
v3.3.5,only have root
v3.3.5,"Copy indices, gradients and Hessians as early as possible"
v3.3.5,only need to initialize for smaller leaf
v3.3.5,Get leaf boundary
v3.3.5,copy indices to the GPU:
v3.3.5,copy ordered Hessians to the GPU:
v3.3.5,copy ordered gradients to the GPU:
v3.3.5,do nothing if no features can be processed on GPU
v3.3.5,copy data indices if it is not null
v3.3.5,generate and copy ordered_gradients if gradients is not null
v3.3.5,generate and copy ordered_hessians if Hessians is not null
v3.3.5,converted indices in is_feature_used to feature-group indices
v3.3.5,construct the feature masks for dense feature-groups
v3.3.5,"if no feature group is used, just return and do not use GPU"
v3.3.5,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.5,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.5,"All data have been prepared, now run the GPU kernel"
v3.3.5,construct smaller leaf
v3.3.5,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v3.3.5,then construct sparse features on CPU
v3.3.5,"wait for GPU to finish, only if GPU is actually used"
v3.3.5,use double precision
v3.3.5,use single precision
v3.3.5,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v3.3.5,#define GPU_DEBUG_COMPARE
v3.3.5,construct larger leaf
v3.3.5,then construct sparse features on CPU
v3.3.5,"wait for GPU to finish, only if GPU is actually used"
v3.3.5,use double precision
v3.3.5,use single precision
v3.3.5,do some sanity check for the GPU algorithm
v3.3.5,limit top k
v3.3.5,get max bin
v3.3.5,calculate buffer size
v3.3.5,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.5,"left and right on same time, so need double size"
v3.3.5,initialize histograms for global
v3.3.5,sync global data sumup info
v3.3.5,set global sumup info
v3.3.5,init global data count in leaf
v3.3.5,get local sumup
v3.3.5,get local sumup
v3.3.5,get mean number on machines
v3.3.5,weighted gain
v3.3.5,get top k
v3.3.5,"Copy histogram to buffer, and Get local aggregate features"
v3.3.5,copy histograms.
v3.3.5,copy smaller leaf histograms first
v3.3.5,mark local aggregated feature
v3.3.5,copy
v3.3.5,then copy larger leaf histograms
v3.3.5,mark local aggregated feature
v3.3.5,copy
v3.3.5,use local data to find local best splits
v3.3.5,clear histogram buffer before synchronizing
v3.3.5,otherwise histogram contents from the previous iteration will be sent
v3.3.5,find splits
v3.3.5,only has root leaf
v3.3.5,local voting
v3.3.5,gather
v3.3.5,get all top-k from all machines
v3.3.5,global voting
v3.3.5,copy local histgrams to buffer
v3.3.5,Reduce scatter for histogram
v3.3.5,find best split from local aggregated histograms
v3.3.5,restore from buffer
v3.3.5,restore from buffer
v3.3.5,find local best
v3.3.5,find local best split for larger leaf
v3.3.5,sync global best info
v3.3.5,copy back
v3.3.5,set the global number of data for leaves
v3.3.5,init the global sumup info
v3.3.5,"instantiate template classes, otherwise linker cannot find the code"
v3.3.5,launch cuda kernel
v3.3.5,initialize SerialTreeLearner
v3.3.5,some additional variables needed for GPU trainer
v3.3.5,Initialize GPU buffers and kernels: get device info
v3.3.5,some functions used for debugging the GPU histogram construction
v3.3.5,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.5,also guarantee that there are at least 2K examples per workgroup
v3.3.5,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.3.5,decide the best number of workgroups working on one feature4 tuple
v3.3.5,set work group size based on feature size
v3.3.5,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.5,set thread_data
v3.3.5,copy the results asynchronously. Size depends on if double precision is used
v3.3.5,"when the output is ready, the computation is done"
v3.3.5,how many feature-group tuples we have
v3.3.5,leave some safe margin for prefetching
v3.3.5,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.5,clear sparse/dense maps
v3.3.5,do nothing it there is no dense feature
v3.3.5,calculate number of feature groups per gpu
v3.3.5,histogram bin entry size depends on the precision (single/double)
v3.3.5,allocate GPU memory for each GPU
v3.3.5,do nothing it there is no gpu feature
v3.3.5,allocate memory for all features
v3.3.5,allocate space for gradients and hessians on device
v3.3.5,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.5,copy indices to the device
v3.3.5,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.5,each work group generates a sub-histogram of dword_features_ features.
v3.3.5,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.5,create atomic counters for inter-group coordination
v3.3.5,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.5,clear sparse/dense maps
v3.3.5,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.5,set device info
v3.3.5,looking for dword_features_ non-sparse feature-groups
v3.3.5,reset device info
v3.3.5,InitGPU w/ num_gpu
v3.3.5,"Get the max bin size, used for selecting best GPU kernel"
v3.3.5,get num_dense_feature_groups_
v3.3.5,initialize GPU
v3.3.5,set cpu threads
v3.3.5,resize device memory pointers
v3.3.5,create stream & events to handle multiple GPUs
v3.3.5,check data size
v3.3.5,GPU memory has to been reallocated because data may have been changed
v3.3.5,AllocateGPUMemory only when the number of data increased
v3.3.5,setup GPU kernel arguments after we allocating all the buffers
v3.3.5,Copy initial full hessians and gradients to GPU.
v3.3.5,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.5,use bagging
v3.3.5,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.3.5,copy used gradients and hessians to ordered buffer
v3.3.5,transfer the indices to GPU
v3.3.5,only have root
v3.3.5,"Copy indices, gradients and hessians as early as possible"
v3.3.5,only need to initialize for smaller leaf
v3.3.5,Get leaf boundary
v3.3.5,do nothing if no features can be processed on GPU
v3.3.5,copy data indices if it is not null
v3.3.5,converted indices in is_feature_used to feature-group indices
v3.3.5,construct the feature masks for dense feature-groups
v3.3.5,"if no feature group is used, just return and do not use GPU"
v3.3.5,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.5,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.5,We now copy even if all features are used.
v3.3.5,"All data have been prepared, now run the GPU kernel"
v3.3.5,construct smaller leaf
v3.3.5,Check workgroups per feature4 tuple..
v3.3.5,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.3.5,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.3.5,then construct sparse features on CPU
v3.3.5,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.5,"wait for GPU to finish, only if GPU is actually used"
v3.3.5,use double precision
v3.3.5,use single precision
v3.3.5,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.3.5,#define CUDA_DEBUG_COMPARE
v3.3.5,construct larger leaf
v3.3.5,then construct sparse features on CPU
v3.3.5,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.5,"wait for GPU to finish, only if GPU is actually used"
v3.3.5,use double precision
v3.3.5,use single precision
v3.3.5,do some sanity check for the GPU algorithm
v3.3.4,coding: utf-8
v3.3.4,coding: utf-8
v3.3.4,create predictor first
v3.3.4,"show deprecation warning only for early stop argument, setting early stop via global params should still be possible"
v3.3.4,check dataset
v3.3.4,reduce cost for prediction training data
v3.3.4,process callbacks
v3.3.4,Most of legacy advanced options becomes callbacks
v3.3.4,construct booster
v3.3.4,start training
v3.3.4,check evaluation result.
v3.3.4,"ranking task, split according to groups"
v3.3.4,run preprocessing on the data set if needed
v3.3.4,setup callbacks
v3.3.4,coding: utf-8
v3.3.4,dummy function to support older version of scikit-learn
v3.3.4,coding: utf-8
v3.3.4,documentation templates for LGBMModel methods are shared between the classes in
v3.3.4,this module and those in the ``dask`` module
v3.3.4,"user can set verbose with kwargs, it has higher priority"
v3.3.4,Do not modify original args in fit function
v3.3.4,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.3.4,Separate built-in from callable evaluation metrics
v3.3.4,register default metric for consistency with callable eval_metric case
v3.3.4,try to deduce from class instance
v3.3.4,overwrite default metric by explicitly set metric
v3.3.4,concatenate metric from params (or default if not provided in params) and eval_metric
v3.3.4,copy for consistency
v3.3.4,reduce cost for prediction training data
v3.3.4,free dataset
v3.3.4,Switch to using a multiclass objective in the underlying LGBM instance
v3.3.4,"do not modify args, as it causes errors in model selection tools"
v3.3.4,check group data
v3.3.4,coding: utf-8
v3.3.4,we don't need lib_lightgbm while building docs
v3.3.4,coding: utf-8
v3.3.4,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.3.4,coding: utf-8
v3.3.4,coding: utf-8
v3.3.4,TypeError: obj is not a string or a number
v3.3.4,ValueError: invalid literal
v3.3.4,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.3.4,avoid side effects on passed-in parameters
v3.3.4,"find a value, and remove other aliases with .pop()"
v3.3.4,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.3.4,Get total row number.
v3.3.4,Random access by row index. Used for data sampling.
v3.3.4,Range data access. Used to read data in batch when constructing Dataset.
v3.3.4,Optionally specify batch_size to control range data read size.
v3.3.4,Only required if using ``Dataset.subset()``.
v3.3.4,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.4,avoid memory consumption by arrays concatenation operations
v3.3.4,create numpy array from output arrays
v3.3.4,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.3.4,for CSC there is extra column added
v3.3.4,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.3.4,same shape as input csr or csc matrix except extra column for expected value
v3.3.4,note: make sure we copy data as it will be deallocated next
v3.3.4,"free the temporary native indptr, indices, and data"
v3.3.4,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.4,avoid memory consumption by arrays concatenation operations
v3.3.4,c type: double**
v3.3.4,each double* element points to start of each column of sample data.
v3.3.4,c type int**
v3.3.4,each int* points to start of indices for each column
v3.3.4,"no min_data, nthreads and verbose in this function"
v3.3.4,check data has header or not
v3.3.4,need to regroup init_score
v3.3.4,process for args
v3.3.4,"user can set verbose with params, it has higher priority"
v3.3.4,get categorical features
v3.3.4,process for reference dataset
v3.3.4,start construct data
v3.3.4,set feature names
v3.3.4,"Select sampled rows, transpose to column order."
v3.3.4,create validation dataset from ref_dataset
v3.3.4,create valid
v3.3.4,construct subset
v3.3.4,create train
v3.3.4,could be updated if data is not freed
v3.3.4,set to None
v3.3.4,we're done if self and reference share a common upstream reference
v3.3.4,"if buffer length is not long enough, reallocate buffers"
v3.3.4,"group data from LightGBM is boundaries data, need to convert to group size"
v3.3.4,"user can set verbose with params, it has higher priority"
v3.3.4,Training task
v3.3.4,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.3.4,construct booster object
v3.3.4,copy the parameters from train_set
v3.3.4,save reference to data
v3.3.4,buffer for inner predict
v3.3.4,Prediction task
v3.3.4,if a single node tree it won't have `leaf_index` so return 0
v3.3.4,"Create the node record, and populate universal data members"
v3.3.4,Update values to reflect node type (leaf or split)
v3.3.4,traverse the next level of the tree
v3.3.4,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.3.4,and we add node to the list.
v3.3.4,need reset training data
v3.3.4,need to push new valid data
v3.3.4,"if buffer length is not long enough, re-allocate a buffer"
v3.3.4,"if buffer length is not long enough, reallocate a buffer"
v3.3.4,Copy models
v3.3.4,Get name of features
v3.3.4,"if buffer length is not long enough, reallocate buffers"
v3.3.4,avoid to predict many time in one iteration
v3.3.4,Get num of inner evals
v3.3.4,Get name of eval metrics
v3.3.4,"if buffer length is not long enough, reallocate buffers"
v3.3.4,coding: utf-8
v3.3.4,Callback environment used by callbacks
v3.3.4,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.4,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.4,coding: utf-8
v3.3.4,Concatenate many parts into one
v3.3.4,construct local eval_set data.
v3.3.4,store indices of eval_set components that were not contained within local parts.
v3.3.4,consolidate parts of each individual eval component.
v3.3.4,require that eval_name exists in evaluated result data in case dropped due to padding.
v3.3.4,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v3.3.4,filter padding from eval parts then _concat each eval_set component.
v3.3.4,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v3.3.4,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v3.3.4,capture whether local_listen_port or its aliases were provided
v3.3.4,capture whether machines or its aliases were provided
v3.3.4,Some passed-in parameters can be removed:
v3.3.4,* 'num_machines': set automatically from Dask worker list
v3.3.4,* 'num_threads': overridden to match nthreads on each Dask process
v3.3.4,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.3.4,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v3.3.4,X and y are each delayed sub-lists of original eval dask Collections.
v3.3.4,find maximum number of parts in an individual eval set so that we can
v3.3.4,pad eval sets when they come in different sizes.
v3.3.4,"when individual eval set is equivalent to training data, skip recomputing parts."
v3.3.4,add None-padding for individual eval_set member if it is smaller than the largest member.
v3.3.4,first time a chunk of this eval set is added to this part.
v3.3.4,append additional chunks of this eval set to this part.
v3.3.4,ensure that all evaluation parts map uniquely to one part.
v3.3.4,assign sub-eval_set components to worker parts.
v3.3.4,Start computation in the background
v3.3.4,Find locations of all parts and map them to particular Dask workers
v3.3.4,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v3.3.4,data artifacts may not be populated depending on worker returning final estimator.
v3.3.4,assign general validation set settings to fit kwargs.
v3.3.4,resolve aliases for network parameters and pop the result off params.
v3.3.4,these values are added back in calls to `_train_part()`
v3.3.4,figure out network params
v3.3.4,Tell each worker to train on the parts that it has locally
v3.3.4,
v3.3.4,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v3.3.4,1. there is randomness in the training process unless parameters ``seed``
v3.3.4,and ``deterministic`` are set
v3.3.4,"2. even with those parameters set, the output of one ``_train_part()`` call"
v3.3.4,relies on global state (it and all the other LightGBM training processes
v3.3.4,coordinate with each other)
v3.3.4,"if network parameters were changed during training, remove them from the"
v3.3.4,returned model so that they're generated dynamically on every run based
v3.3.4,on the Dask cluster you're connected to and which workers have pieces of
v3.3.4,the training data
v3.3.4,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.3.4,"for multi-class classification with sparse matrices, pred_contrib predictions"
v3.3.4,are returned as a list of sparse matrices (one per class)
v3.3.4,"pred_contrib output will have one column per feature,"
v3.3.4,plus one more for the base value
v3.3.4,need to tell Dask the expected type and shape of individual preds
v3.3.4,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v3.3.4,the code below is used instead to ensure that the sparse type is preserved during concatentation
v3.3.4,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v3.3.4,Concatenate them to return a list of Dask Arrays.
v3.3.4,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.4,currently relevant for the Dask estimators
v3.3.4,"DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds."
v3.3.4,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.3.4,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.4,currently relevant for the Dask estimators
v3.3.4,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds."
v3.3.4,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.3.4,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.4,currently relevant for the Dask estimators
v3.3.4,DaskLGBMRanker does not support eval_class_weight or early stopping
v3.3.4,DaskLGBMRanker support for callbacks and init_model is not tested
v3.3.4,coding: utf-8
v3.3.4,load or create your dataset
v3.3.4,create dataset for lightgbm
v3.3.4,"if you want to re-use data, remember to set free_raw_data=False"
v3.3.4,specify your configurations as a dict
v3.3.4,generate feature names
v3.3.4,feature_name and categorical_feature
v3.3.4,check feature name
v3.3.4,save model to file
v3.3.4,dump model to JSON (and save to file)
v3.3.4,feature names
v3.3.4,feature importances
v3.3.4,load model to predict
v3.3.4,can only predict with the best iteration (or the saving iteration)
v3.3.4,eval with loaded model
v3.3.4,dump model with pickle
v3.3.4,load model with pickle to predict
v3.3.4,can predict with any iteration when loaded in pickle way
v3.3.4,eval with loaded model
v3.3.4,continue training
v3.3.4,init_model accepts:
v3.3.4,1. model file name
v3.3.4,2. Booster()
v3.3.4,decay learning rates
v3.3.4,learning_rates accepts:
v3.3.4,1. list/tuple with length = num_boost_round
v3.3.4,2. function(curr_iter)
v3.3.4,change other parameters during training
v3.3.4,self-defined objective function
v3.3.4,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.3.4,log likelihood loss
v3.3.4,self-defined eval metric
v3.3.4,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.4,binary error
v3.3.4,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.4,This may make built-in evaluation metric calculate wrong results
v3.3.4,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.4,Keep this in mind when you use the customization
v3.3.4,another self-defined eval metric
v3.3.4,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.4,accuracy
v3.3.4,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.4,This may make built-in evaluation metric calculate wrong results
v3.3.4,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.4,Keep this in mind when you use the customization
v3.3.4,callback
v3.3.4,coding: utf-8
v3.3.4,load or create your dataset
v3.3.4,train
v3.3.4,predict
v3.3.4,eval
v3.3.4,feature importances
v3.3.4,self-defined eval metric
v3.3.4,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.4,Root Mean Squared Logarithmic Error (RMSLE)
v3.3.4,train
v3.3.4,another self-defined eval metric
v3.3.4,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.4,Relative Absolute Error (RAE)
v3.3.4,train
v3.3.4,predict
v3.3.4,eval
v3.3.4,other scikit-learn modules
v3.3.4,coding: utf-8
v3.3.4,load or create your dataset
v3.3.4,create dataset for lightgbm
v3.3.4,specify your configurations as a dict
v3.3.4,train
v3.3.4,coding: utf-8
v3.3.4,################
v3.3.4,Simulate some binary data with a single categorical and
v3.3.4,single continuous predictor
v3.3.4,################
v3.3.4,Set up a couple of utilities for our experiments
v3.3.4,################
v3.3.4,Observe the behavior of `binary` and `xentropy` objectives
v3.3.4,Trying this throws an error on non-binary values of y:
v3.3.4,"experiment('binary', label_type='probability', DATA)"
v3.3.4,The speed of `binary` is not drastically different than
v3.3.4,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.3.4,there are reasons to suspect that `binary` should run faster when the
v3.3.4,label is an integer instead of a float
v3.3.4,coding: utf-8
v3.3.4,load or create your dataset
v3.3.4,create dataset for lightgbm
v3.3.4,specify your configurations as a dict
v3.3.4,train
v3.3.4,save model to file
v3.3.4,predict
v3.3.4,eval
v3.3.4,We can also open HDF5 file once and get access to
v3.3.4,"With binary dataset created, we can use either Python API or cmdline version to train."
v3.3.4,
v3.3.4,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v3.3.4,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v3.3.4,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v3.3.4,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v3.3.4,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v3.3.4,Y has a single column and we read it in single shot. So store it as an 1-d array.
v3.3.4,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v3.3.4,"When accessing any element in a HDF5 chunk, it's read entirely."
v3.3.4,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v3.3.4,Here we are just creating a chunk size that matches with batch_size.
v3.3.4,
v3.3.4,Also note that the data is stored in row major order to avoid extra copy when passing to
v3.3.4,lightgbm Dataset.
v3.3.4,Save to 2 HDF5 files for demonstration.
v3.3.4,We can store multiple datasets inside a single HDF5 file.
v3.3.4,Separating X and Y for choosing best chunk size for data loading.
v3.3.4,split training data into two partitions
v3.3.4,make this array dense because we're splitting across
v3.3.4,a sparse boundary to partition the data
v3.3.4,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.3.4,predictions and target values back from workers to the client
v3.3.4,
v3.3.4,"for larger datasets, consider the metrics from dask-ml instead"
v3.3.4,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.3.4,coding: utf-8
v3.3.4,!/usr/bin/env python3
v3.3.4,-*- coding: utf-8 -*-
v3.3.4,
v3.3.4,"LightGBM documentation build configuration file, created by"
v3.3.4,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.3.4,
v3.3.4,This file is execfile()d with the current directory set to its
v3.3.4,containing dir.
v3.3.4,
v3.3.4,Note that not all possible configuration values are present in this
v3.3.4,autogenerated file.
v3.3.4,
v3.3.4,All configuration values have a default; values that are commented out
v3.3.4,serve to show the default.
v3.3.4,"If extensions (or modules to document with autodoc) are in another directory,"
v3.3.4,add these directories to sys.path here. If the directory is relative to the
v3.3.4,"documentation root, use os.path.abspath to make it absolute."
v3.3.4,-- General configuration ------------------------------------------------
v3.3.4,"If your documentation needs a minimal Sphinx version, state it here."
v3.3.4,"Add any Sphinx extension module names here, as strings. They can be"
v3.3.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.3.4,ones.
v3.3.4,mock out modules
v3.3.4,hide type hints in API docs
v3.3.4,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.3.4,Only the class' docstring is inserted.
v3.3.4,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.3.4,The master toctree document.
v3.3.4,General information about the project.
v3.3.4,The name of an image file (relative to this directory) to place at the top
v3.3.4,of the sidebar.
v3.3.4,The name of an image file (relative to this directory) to use as a favicon of
v3.3.4,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.3.4,pixels large.
v3.3.4,"The version info for the project you're documenting, acts as replacement for"
v3.3.4,"|version| and |release|, also used in various other places throughout the"
v3.3.4,built documents.
v3.3.4,The short X.Y version.
v3.3.4,"The full version, including alpha/beta/rc tags."
v3.3.4,The language for content autogenerated by Sphinx. Refer to documentation
v3.3.4,for a list of supported languages.
v3.3.4,
v3.3.4,This is also used if you do content translation via gettext catalogs.
v3.3.4,"Usually you set ""language"" from the command line for these cases."
v3.3.4,"List of patterns, relative to source directory, that match files and"
v3.3.4,directories to ignore when looking for source files.
v3.3.4,This patterns also effect to html_static_path and html_extra_path
v3.3.4,The name of the Pygments (syntax highlighting) style to use.
v3.3.4,-- Configuration for C API docs generation ------------------------------
v3.3.4,-- Options for HTML output ----------------------------------------------
v3.3.4,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.3.4,a list of builtin themes.
v3.3.4,Theme options are theme-specific and customize the look and feel of a theme
v3.3.4,"further.  For a list of options available for each theme, see the"
v3.3.4,documentation.
v3.3.4,"Add any paths that contain custom static files (such as style sheets) here,"
v3.3.4,"relative to this directory. They are copied after the builtin static files,"
v3.3.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.3.4,-- Options for HTMLHelp output ------------------------------------------
v3.3.4,Output file base name for HTML help builder.
v3.3.4,-- Options for LaTeX output ---------------------------------------------
v3.3.4,The name of an image file (relative to this directory) to place at the top of
v3.3.4,the title page.
v3.3.4,Warning! The following code can cause buffer overflows on RTD.
v3.3.4,Consider suppressing output completely if RTD project silently fails.
v3.3.4,Refer to https://github.com/svenevs/exhale
v3.3.4,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.4,Warning! The following code can cause buffer overflows on RTD.
v3.3.4,Consider suppressing output completely if RTD project silently fails.
v3.3.4,Refer to https://github.com/svenevs/exhale
v3.3.4,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.4,coding: utf-8
v3.3.4,This is a basic test for floating number parsing.
v3.3.4,Most of the test cases come from:
v3.3.4,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v3.3.4,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v3.3.4,FLT_MAX
v3.3.4,FLT_MIN
v3.3.4,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v3.3.4,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v3.3.4,Near DBL_MIN
v3.3.4,DBL_MIN 2^-1022
v3.3.4,The behavior for parsing -nan depends on implementation.
v3.3.4,Thus we skip binary check for negative nan.
v3.3.4,See comment in test_cases.
v3.3.4,Constants
v3.3.4,Start with some content:
v3.3.4,Clear & re-use:
v3.3.4,Output should match new content:
v3.3.4,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.3.4,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.3.4,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.3.4,Compute a new trial address & value & if it is a valid address:
v3.3.4,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.3.4,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.3.4,Check the just-stored value with getitem():
v3.3.4,Also store the just-stored value for future tracking:
v3.3.4,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.3.4,Test in 2 ways that the values are correctly laid out in memory:
v3.3.4,coding: utf-8
v3.3.4,we don't need lib_lightgbm while building docs
v3.3.4,coding: utf-8
v3.3.4,check saved model persistence
v3.3.4,"we need to check the consistency of model file here, so test for exact equal"
v3.3.4,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.3.4,"scores likely to be different, but prediction should still be the same"
v3.3.4,test that shape is checked during prediction
v3.3.4,"The simple implementation is just a single ""return self.ndarray[idx]"""
v3.3.4,The following is for demo and testing purpose.
v3.3.4,whole col
v3.3.4,half col
v3.3.4,Create dataset from numpy array directly.
v3.3.4,Create dataset using Sequence.
v3.3.4,Test for validation set.
v3.3.4,Select some random rows as valid data.
v3.3.4,"From Dataset constructor, with dataset from numpy array."
v3.3.4,"From Dataset.create_valid, with dataset from sequence."
v3.3.4,test that method works even with free_raw_data=True
v3.3.4,test that method works but sets raw data to None in case of immergeable data types
v3.3.4,test that method works for different data types
v3.3.4,"Set extremely harsh penalties, so CEGB will block most splits."
v3.3.4,"Compare pairs of penalties, to ensure scaling works as intended"
v3.3.4,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.3.4,"should resolve duplicate aliases, and prefer the main parameter"
v3.3.4,should choose a value from an alias and set that value on main param
v3.3.4,if only an alias is used
v3.3.4,should use the default if main param and aliases are missing
v3.3.4,all changes should be made on copies and not modify the original
v3.3.4,coding: utf-8
v3.3.4,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.3.4,set_index ensures partitions are based on group id.
v3.3.4,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.3.4,"separate target, weight from features."
v3.3.4,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.3.4,"so that within each partition, sum(g) = n_samples."
v3.3.4,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.3.4,make one categorical feature relevant to the target
v3.3.4,https://github.com/microsoft/LightGBM/issues/4118
v3.3.4,extra predict() parameters should be passed through correctly
v3.3.4,pref_leaf values should have the right shape
v3.3.4,and values that look like valid tree nodes
v3.3.4,"be sure LightGBM actually used at least one categorical column,"
v3.3.4,and that it was correctly treated as a categorical feature
v3.3.4,shape depends on whether it is binary or multiclass classification
v3.3.4,"in the special case of multi-class classification using scipy sparse matrices,"
v3.3.4,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v3.3.4,
v3.3.4,"since that case is so different than all other cases, check the relevant things here"
v3.3.4,and then return early
v3.3.4,"raw scores will probably be different, but at least check that all predicted classes are the same"
v3.3.4,"be sure LightGBM actually used at least one categorical column,"
v3.3.4,and that it was correctly treated as a categorical feature
v3.3.4,* shape depends on whether it is binary or multiclass classification
v3.3.4,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.3.4,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.3.4,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.4,that the output has the right shape and base values are in the right position
v3.3.4,check that found ports are different for same address (LocalCluster)
v3.3.4,check that the ports are indeed open
v3.3.4,Scores should be the same
v3.3.4,Predictions should be roughly the same.
v3.3.4,pref_leaf values should have the right shape
v3.3.4,and values that look like valid tree nodes
v3.3.4,extra predict() parameters should be passed through correctly
v3.3.4,"be sure LightGBM actually used at least one categorical column,"
v3.3.4,and that it was correctly treated as a categorical feature
v3.3.4,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.4,that the output has the right shape and base values are in the right position
v3.3.4,"be sure LightGBM actually used at least one categorical column,"
v3.3.4,and that it was correctly treated as a categorical feature
v3.3.4,Quantiles should be right
v3.3.4,"be sure LightGBM actually used at least one categorical column,"
v3.3.4,and that it was correctly treated as a categorical feature
v3.3.4,rebalance small dask.Array dataset for better performance.
v3.3.4,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.3.4,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.3.4,distributed ranker should be able to rank decently well and should
v3.3.4,have high rank correlation with scores from serial ranker.
v3.3.4,extra predict() parameters should be passed through correctly
v3.3.4,pref_leaf values should have the right shape
v3.3.4,and values that look like valid tree nodes
v3.3.4,"be sure LightGBM actually used at least one categorical column,"
v3.3.4,and that it was correctly treated as a categorical feature
v3.3.4,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v3.3.4,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v3.3.4,"test eval_class_weight, eval_init_score on binary-classification task."
v3.3.4,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v3.3.4,create eval_sets by creating new datasets or copying training data.
v3.3.4,total number of trees scales up for ova classifier.
v3.3.4,check that early stopping was not applied.
v3.3.4,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v3.3.4,"check that each eval_name and metric exists for all eval sets, allowing for the"
v3.3.4,case when a worker receives a fully-padded eval_set component which is not evaluated.
v3.3.4,should be able to use the class without specifying a client
v3.3.4,should be able to set client after construction
v3.3.4,data on cluster1
v3.3.4,create identical data on cluster2
v3.3.4,"at this point, the result of default_client() is client2 since it was the most recently"
v3.3.4,created. So setting client to client1 here to test that you can select a non-default client
v3.3.4,"unfitted model should survive pickling round trip, and pickling"
v3.3.4,shouldn't have side effects on the model object
v3.3.4,client will always be None after unpickling
v3.3.4,"fitted model should survive pickling round trip, and pickling"
v3.3.4,shouldn't have side effects on the model object
v3.3.4,client will always be None after unpickling
v3.3.4,rebalance data to be sure that each worker has a piece of the data
v3.3.4,model 1 - no network parameters given
v3.3.4,model 2 - machines given
v3.3.4,model 3 - local_listen_port given
v3.3.4,training should fail because LightGBM will try to use the same
v3.3.4,port for multiple worker processes on the same machine
v3.3.4,rebalance data to be sure that each worker has a piece of the data
v3.3.4,"test that ""machines"" is actually respected by creating a socket that uses"
v3.3.4,"one of the ports mentioned in ""machines"""
v3.3.4,The above error leaves a worker waiting
v3.3.4,"an informative error should be raised if ""machines"" has duplicates"
v3.3.4,"""client"" should be the only different, and the final argument"
v3.3.4,value of the root node is 0 when init_score is set
v3.3.4,this test is separate because it takes a not-yet-constructed estimator
v3.3.4,coding: utf-8
v3.3.4,coding: utf-8
v3.3.4,"build target, group ID vectors."
v3.3.4,build y/target and group-id vectors with user-specified group sizes.
v3.3.4,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.3.4,groups should contain > 1 element for pairwise learning objective.
v3.3.4,"build feature data, X. Transform first few into informative features."
v3.3.4,coding: utf-8
v3.3.4,prediction result is actually not transformed (is raw) due to custom objective
v3.3.4,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.3.4,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.3.4,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.4,sklearn < 0.23 does not have as_frame parameter
v3.3.4,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.4,sklearn < 0.23 does not have as_frame parameter
v3.3.4,Test if random_state is properly stored
v3.3.4,Test if two random states produce identical models
v3.3.4,Test if subsequent fits sample from random_state object and produce different models
v3.3.4,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.3.4,With default params
v3.3.4,Tests same probabilities
v3.3.4,Tests same predictions
v3.3.4,Tests same raw scores
v3.3.4,Tests same leaf indices
v3.3.4,Tests same feature contributions
v3.3.4,Tests other parameters for the prediction works
v3.3.4,Tests start_iteration
v3.3.4,"Tests same probabilities, starting from iteration 10"
v3.3.4,"Tests same predictions, starting from iteration 10"
v3.3.4,"Tests same raw scores, starting from iteration 10"
v3.3.4,"Tests same leaf indices, starting from iteration 10"
v3.3.4,"Tests same feature contributions, starting from iteration 10"
v3.3.4,"Tests other parameters for the prediction works, starting from iteration 10"
v3.3.4,"no custom objective, no custom metric"
v3.3.4,default metric
v3.3.4,non-default metric
v3.3.4,no metric
v3.3.4,non-default metric in eval_metric
v3.3.4,non-default metric with non-default metric in eval_metric
v3.3.4,non-default metric with multiple metrics in eval_metric
v3.3.4,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.3.4,default metric for non-default objective
v3.3.4,non-default metric for non-default objective
v3.3.4,no metric
v3.3.4,non-default metric in eval_metric for non-default objective
v3.3.4,non-default metric with non-default metric in eval_metric for non-default objective
v3.3.4,non-default metric with multiple metrics in eval_metric for non-default objective
v3.3.4,"custom objective, no custom metric"
v3.3.4,default regression metric for custom objective
v3.3.4,non-default regression metric for custom objective
v3.3.4,multiple regression metrics for custom objective
v3.3.4,no metric
v3.3.4,default regression metric with non-default metric in eval_metric for custom objective
v3.3.4,non-default regression metric with metric in eval_metric for custom objective
v3.3.4,multiple regression metrics with metric in eval_metric for custom objective
v3.3.4,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.3.4,"no custom objective, custom metric"
v3.3.4,default metric with custom metric
v3.3.4,non-default metric with custom metric
v3.3.4,multiple metrics with custom metric
v3.3.4,custom metric (disable default metric)
v3.3.4,default metric for non-default objective with custom metric
v3.3.4,non-default metric for non-default objective with custom metric
v3.3.4,multiple metrics for non-default objective with custom metric
v3.3.4,custom metric (disable default metric for non-default objective)
v3.3.4,"custom objective, custom metric"
v3.3.4,custom metric for custom objective
v3.3.4,non-default regression metric with custom metric for custom objective
v3.3.4,multiple regression metrics with custom metric for custom objective
v3.3.4,default metric and invalid binary metric is replaced with multiclass alternative
v3.3.4,invalid objective is replaced with default multiclass one
v3.3.4,and invalid binary metric is replaced with multiclass alternative
v3.3.4,default metric for non-default multiclass objective
v3.3.4,and invalid binary metric is replaced with multiclass alternative
v3.3.4,default metric and invalid multiclass metric is replaced with binary alternative
v3.3.4,invalid multiclass metric is replaced with binary alternative for custom objective
v3.3.4,"Verify that can receive a list of metrics, only callable"
v3.3.4,Verify that can receive a list of custom and built-in metrics
v3.3.4,Verify that works as expected when eval_metric is empty
v3.3.4,"Verify that can receive a list of metrics, only built-in"
v3.3.4,Verify that eval_metric is robust to receiving a list with None
v3.3.4,training data as eval_set
v3.3.4,feval
v3.3.4,single eval_set
v3.3.4,two eval_set
v3.3.4,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.3.4,Test that estimators are default-constructible
v3.3.4,coding: utf-8
v3.3.4,coding: utf-8
v3.3.4,check that default gives same result as k = 1
v3.3.4,check against independent calculation for k = 1
v3.3.4,check against independent calculation for k = 2
v3.3.4,check against independent calculation for k = 10
v3.3.4,check cases where predictions are equal
v3.3.4,should give same result as binary auc for 2 classes
v3.3.4,test the case where all predictions are equal
v3.3.4,test that weighted data gives different auc_mu
v3.3.4,test that equal data weights give same auc_mu as unweighted data
v3.3.4,should give 1 when accuracy = 1
v3.3.4,test loading class weights
v3.3.4,no early stopping
v3.3.4,early stopping occurs
v3.3.4,test custom eval metrics
v3.3.4,"shuffle = False, override metric in params"
v3.3.4,"shuffle = True, callbacks"
v3.3.4,enable display training loss
v3.3.4,self defined folds
v3.3.4,LambdaRank
v3.3.4,... with l2 metric
v3.3.4,... with NDCG (default) metric
v3.3.4,self defined folds with lambdarank
v3.3.4,with early stopping
v3.3.4,predict by each fold booster
v3.3.4,fold averaging
v3.3.4,without early stopping
v3.3.4,test feature_names with whitespaces
v3.3.4,This has non-ascii strings.
v3.3.4,take subsets and train
v3.3.4,generate CSR sparse dataset
v3.3.4,convert data to dense and get back same contribs
v3.3.4,validate the values are the same
v3.3.4,validate using CSC matrix
v3.3.4,validate the values are the same
v3.3.4,generate CSR sparse dataset
v3.3.4,convert data to dense and get back same contribs
v3.3.4,validate the values are the same
v3.3.4,validate using CSC matrix
v3.3.4,validate the values are the same
v3.3.4,Note there is an extra column added to the output for the expected value
v3.3.4,Note output CSC shape should be same as CSR output shape
v3.3.4,test sliced labels
v3.3.4,append some columns
v3.3.4,append some rows
v3.3.4,test sliced 2d matrix
v3.3.4,test sliced CSR
v3.3.4,trees start at position 1.
v3.3.4,split_features are in 4th line.
v3.3.4,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.3.4,The penalization is so high that the first 2 features should not be used here
v3.3.4,Check that a very high penalization is the same as not using the features at all
v3.3.4,"no fobj, no feval"
v3.3.4,default metric
v3.3.4,non-default metric in params
v3.3.4,default metric in args
v3.3.4,non-default metric in args
v3.3.4,metric in args overwrites one in params
v3.3.4,multiple metrics in params
v3.3.4,multiple metrics in args
v3.3.4,remove default metric by 'None' in list
v3.3.4,remove default metric by 'None' aliases
v3.3.4,"fobj, no feval"
v3.3.4,no default metric
v3.3.4,metric in params
v3.3.4,metric in args
v3.3.4,metric in args overwrites its' alias in params
v3.3.4,multiple metrics in params
v3.3.4,multiple metrics in args
v3.3.4,"no fobj, feval"
v3.3.4,default metric with custom one
v3.3.4,non-default metric in params with custom one
v3.3.4,default metric in args with custom one
v3.3.4,non-default metric in args with custom one
v3.3.4,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.4,multiple metrics in params with custom one
v3.3.4,multiple metrics in args with custom one
v3.3.4,custom metric is evaluated despite 'None' is passed
v3.3.4,"fobj, feval"
v3.3.4,"no default metric, only custom one"
v3.3.4,metric in params with custom one
v3.3.4,metric in args with custom one
v3.3.4,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.4,multiple metrics in params with custom one
v3.3.4,multiple metrics in args with custom one
v3.3.4,custom metric is evaluated despite 'None' is passed
v3.3.4,"no fobj, no feval"
v3.3.4,default metric
v3.3.4,default metric in params
v3.3.4,non-default metric in params
v3.3.4,multiple metrics in params
v3.3.4,remove default metric by 'None' aliases
v3.3.4,"fobj, no feval"
v3.3.4,no default metric
v3.3.4,metric in params
v3.3.4,multiple metrics in params
v3.3.4,"no fobj, feval"
v3.3.4,default metric with custom one
v3.3.4,default metric in params with custom one
v3.3.4,non-default metric in params with custom one
v3.3.4,multiple metrics in params with custom one
v3.3.4,custom metric is evaluated despite 'None' is passed
v3.3.4,"fobj, feval"
v3.3.4,"no default metric, only custom one"
v3.3.4,metric in params with custom one
v3.3.4,multiple metrics in params with custom one
v3.3.4,custom metric is evaluated despite 'None' is passed
v3.3.4,multiclass default metric
v3.3.4,multiclass default metric with custom one
v3.3.4,multiclass metric alias with custom one for custom objective
v3.3.4,no metric for invalid class_num
v3.3.4,custom metric for invalid class_num
v3.3.4,multiclass metric alias with custom one with invalid class_num
v3.3.4,multiclass default metric without num_class
v3.3.4,multiclass metric alias
v3.3.4,multiclass metric
v3.3.4,non-valid metric for multiclass objective
v3.3.4,non-default num_class for default objective
v3.3.4,no metric with non-default num_class for custom objective
v3.3.4,multiclass metric alias for custom objective
v3.3.4,multiclass metric for custom objective
v3.3.4,binary metric with non-default num_class for custom objective
v3.3.4,Expect three metrics but mean and stdv for each metric
v3.3.4,test XGBoost-style return value
v3.3.4,test numpy-style return value
v3.3.4,test bins string type
v3.3.4,test histogram is disabled for categorical features
v3.3.4,test for lgb.train
v3.3.4,test feval for lgb.train
v3.3.4,test with two valid data for lgb.train
v3.3.4,test for lgb.cv
v3.3.4,test feval for lgb.cv
v3.3.4,test that binning works properly for features with only positive or only negative values
v3.3.4,decreasing without freeing raw data is allowed
v3.3.4,decreasing before lazy init is allowed
v3.3.4,increasing is allowed
v3.3.4,decreasing with disabled filter is allowed
v3.3.4,decreasing with enabled filter is disallowed;
v3.3.4,also changes of other params are disallowed
v3.3.4,check extra trees increases regularization
v3.3.4,check path smoothing increases regularization
v3.3.4,test edge case with one leaf
v3.3.4,check that constraint containing all features is equivalent to no constraint
v3.3.4,check that constraint partitioning the features reduces train accuracy
v3.3.4,check that constraints consisting of single features reduce accuracy further
v3.3.4,test that interaction constraints work when not all features are used
v3.3.4,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.3.4,test again with nans in data
v3.3.4,test again with bagging
v3.3.4,test with a feature that has only one non-nan value
v3.3.4,test with a categorical feature
v3.3.4,test refit: same results on same data
v3.3.4,test refit with save and load
v3.3.4,test refit: different results training on different data
v3.3.4,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.3.4,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.3.4,"test the case where start_iteration <= 0, and num_iteration is None"
v3.3.4,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.3.4,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.3.4,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.3.4,test for regression
v3.3.4,test both with and without early stopping
v3.3.4,test for multi-class
v3.3.4,test both with and without early stopping
v3.3.4,test for binary
v3.3.4,test both with and without early stopping
v3.3.4,test against sklearn average precision metric
v3.3.4,test that average precision is 1 where model predicts perfectly
v3.3.4,coding: utf-8
v3.3.4,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.3.4,coding: utf-8
v3.3.4,coding: utf-8
v3.3.4,These are helper functions to allow doing a stack unwind
v3.3.4,"after an R allocation error, which would trigger a long jump."
v3.3.4,convert from one-based to zero-based index
v3.3.4,"if any feature names were larger than allocated size,"
v3.3.4,allow for a larger size and try again
v3.3.4,convert from boundaries to size
v3.3.4,--- start Booster interfaces
v3.3.4,"if any eval names were larger than allocated size,"
v3.3.4,allow for a larger size and try again
v3.3.4,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.4,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.4,.Call() calls
v3.3.4,coding: utf-8
v3.3.4,alias table
v3.3.4,names
v3.3.4,from strings
v3.3.4,tails
v3.3.4,tails
v3.3.4,coding: utf-8
v3.3.4,Single row predictor to abstract away caching logic
v3.3.4,create boosting
v3.3.4,initialize the boosting
v3.3.4,create objective function
v3.3.4,initialize the objective function
v3.3.4,create training metric
v3.3.4,reset the boosting
v3.3.4,create objective function
v3.3.4,initialize the objective function
v3.3.4,calculate the nonzero data and indices size
v3.3.4,allocate data and indices arrays
v3.3.4,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.4,aggregated per row feature contribution results
v3.3.4,keep track of the row_vector sizes for parallelization
v3.3.4,copy vector results to output for each row
v3.3.4,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.4,aggregated per row feature contribution results
v3.3.4,calculate number of elements per column to construct
v3.3.4,the CSC matrix with random access
v3.3.4,keep track of column counts
v3.3.4,keep track of beginning index for each column
v3.3.4,keep track of beginning index for each matrix
v3.3.4,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.3.4,store the row index
v3.3.4,update column count
v3.3.4,explicitly declare symbols from LightGBM namespace
v3.3.4,some help functions used to convert data
v3.3.4,Row iterator of on column for CSC matrix
v3.3.4,"return value at idx, only can access by ascent order"
v3.3.4,"return next non-zero pair, if index < 0, means no more data"
v3.3.4,start of c_api functions
v3.3.4,This API is to keep python binding's behavior the same with C++ implementation.
v3.3.4,"Sample count, random seed etc. should be provided in parameters."
v3.3.4,sample data first
v3.3.4,sample data first
v3.3.4,sample data first
v3.3.4,local buffer to re-use memory
v3.3.4,sample data first
v3.3.4,no more data
v3.3.4,---- start of booster
v3.3.4,Single row in row-major format:
v3.3.4,---- start of some help functions
v3.3.4,data is array of pointers to individual rows
v3.3.4,set number of threads for openmp
v3.3.4,check for alias
v3.3.4,read parameters from config file
v3.3.4,"remove str after ""#"""
v3.3.4,check for alias again
v3.3.4,load configs
v3.3.4,prediction is needed if using input initial model(continued train)
v3.3.4,need to continue training
v3.3.4,sync up random seed for data partition
v3.3.4,load Training data
v3.3.4,load data for distributed training
v3.3.4,load data for single machine
v3.3.4,need save binary file
v3.3.4,create training metric
v3.3.4,only when have metrics then need to construct validation data
v3.3.4,"Add validation data, if it exists"
v3.3.4,add
v3.3.4,need save binary file
v3.3.4,add metric for validation data
v3.3.4,output used time on each iteration
v3.3.4,need init network
v3.3.4,create boosting
v3.3.4,create objective function
v3.3.4,load training data
v3.3.4,initialize the objective function
v3.3.4,initialize the boosting
v3.3.4,add validation data into boosting
v3.3.4,convert model to if-else statement code
v3.3.4,create predictor
v3.3.4,Free memory
v3.3.4,create predictor
v3.3.4,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.3.4,counts for all labels
v3.3.4,"start from top label, and accumulate DCG"
v3.3.4,counts for all labels
v3.3.4,calculate k Max DCG by one pass
v3.3.4,get sorted indices by score
v3.3.4,calculate multi dcg by one pass
v3.3.4,wait for all client start up
v3.3.4,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.3.4,Instead we will handle finalize/abort for MPI in main().
v3.3.4,default set to -1
v3.3.4,"distance at k-th communication, distance[k] = 2^k"
v3.3.4,set incoming rank at k-th commuication
v3.3.4,set outgoing rank at k-th commuication
v3.3.4,default set as -1
v3.3.4,construct all recursive halving map for all machines
v3.3.4,let 1 << k <= num_machines
v3.3.4,distance of each communication
v3.3.4,"if num_machines = 2^k, don't need to group machines"
v3.3.4,"communication direction, %2 == 0 is positive"
v3.3.4,neighbor at k-th communication
v3.3.4,receive data block at k-th communication
v3.3.4,send data block at k-th communication
v3.3.4,"if num_machines != 2^k, need to group machines"
v3.3.4,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.3.4,let left machine as group leader
v3.3.4,"cache block information for groups, group with 2 machines will have double block size"
v3.3.4,convert from group to node leader
v3.3.4,convert from node to group
v3.3.4,meet new group
v3.3.4,add block len for this group
v3.3.4,calculate the group block start
v3.3.4,not need to construct
v3.3.4,get receive block information
v3.3.4,accumulate block len
v3.3.4,get send block information
v3.3.4,accumulate block len
v3.3.4,static member definition
v3.3.4,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.3.4,assign the blocks to every rank.
v3.3.4,do reduce scatter
v3.3.4,do all gather
v3.3.4,assign blocks
v3.3.4,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.3.4,copy back
v3.3.4,assign blocks
v3.3.4,start all gather
v3.3.4,when num_machines is small and data is large
v3.3.4,use output as receive buffer
v3.3.4,get current local block size
v3.3.4,get out rank
v3.3.4,get in rank
v3.3.4,get send information
v3.3.4,get recv information
v3.3.4,send and recv at same time
v3.3.4,rotate in-place
v3.3.4,use output as receive buffer
v3.3.4,get current local block size
v3.3.4,get send information
v3.3.4,get recv information
v3.3.4,send and recv at same time
v3.3.4,use output as receive buffer
v3.3.4,send and recv at same time
v3.3.4,send local data to neighbor first
v3.3.4,receive neighbor data first
v3.3.4,reduce
v3.3.4,get target
v3.3.4,get send information
v3.3.4,get recv information
v3.3.4,send and recv at same time
v3.3.4,reduce
v3.3.4,send result to neighbor
v3.3.4,receive result from neighbor
v3.3.4,copy result
v3.3.4,start up socket
v3.3.4,parse clients from file
v3.3.4,get ip list of local machine
v3.3.4,get local rank
v3.3.4,construct listener
v3.3.4,construct communication topo
v3.3.4,construct linkers
v3.3.4,free listener
v3.3.4,set timeout
v3.3.4,accept incoming socket
v3.3.4,receive rank
v3.3.4,add new socket
v3.3.4,save ranks that need to connect with
v3.3.4,start listener
v3.3.4,start connect
v3.3.4,let smaller rank connect to larger rank
v3.3.4,send local rank
v3.3.4,wait for listener
v3.3.4,print connected linkers
v3.3.4,only need to copy subset
v3.3.4,avoid to copy subset many times
v3.3.4,avoid out of range
v3.3.4,may need to recopy subset
v3.3.4,valid the type
v3.3.4,Constructors
v3.3.4,Get type tag
v3.3.4,Comparisons
v3.3.4,"This has to be separate, not in Statics, because Json() accesses"
v3.3.4,statics().null.
v3.3.4,"advance until next line, or end of input"
v3.3.4,advance until closing tokens
v3.3.4,The usual case: non-escaped characters
v3.3.4,Handle escapes
v3.3.4,Extract 4-byte escape sequence
v3.3.4,Explicitly check length of the substring. The following loop
v3.3.4,relies on std::string returning the terminating NUL when
v3.3.4,accessing str[length]. Checking here reduces brittleness.
v3.3.4,JSON specifies that characters outside the BMP shall be encoded as a
v3.3.4,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.3.4,components. Check whether we're in the middle of such a beast: the
v3.3.4,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.3.4,a trail (low) surrogate.
v3.3.4,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.3.4,per the UTF-16 algorithm.
v3.3.4,Integer part
v3.3.4,Decimal part
v3.3.4,Exponent part
v3.3.4,Check for any trailing garbage
v3.3.4,Documented in json11.hpp
v3.3.4,Check for another object
v3.3.4,get column names
v3.3.4,load label idx first
v3.3.4,erase label column name
v3.3.4,load ignore columns
v3.3.4,load weight idx
v3.3.4,load group idx
v3.3.4,don't support query id in data file when using distributed training
v3.3.4,read data to memory
v3.3.4,sample data
v3.3.4,construct feature bin mappers
v3.3.4,initialize label
v3.3.4,extract features
v3.3.4,sample data from file
v3.3.4,construct feature bin mappers
v3.3.4,initialize label
v3.3.4,extract features
v3.3.4,load data from binary file
v3.3.4,check meta data
v3.3.4,need to check training data
v3.3.4,read data in memory
v3.3.4,initialize label
v3.3.4,extract features
v3.3.4,Get number of lines of data file
v3.3.4,initialize label
v3.3.4,extract features
v3.3.4,load data from binary file
v3.3.4,not need to check validation data
v3.3.4,check meta data
v3.3.4,buffer to read binary file
v3.3.4,check token
v3.3.4,read size of header
v3.3.4,re-allocmate space if not enough
v3.3.4,read header
v3.3.4,get header
v3.3.4,num_groups
v3.3.4,real_feature_idx_
v3.3.4,feature2group
v3.3.4,feature2subfeature
v3.3.4,group_bin_boundaries
v3.3.4,group_feature_start_
v3.3.4,group_feature_cnt_
v3.3.4,get feature names
v3.3.4,write feature names
v3.3.4,get forced_bin_bounds_
v3.3.4,read size of meta data
v3.3.4,re-allocate space if not enough
v3.3.4,read meta data
v3.3.4,load meta data
v3.3.4,sample local used data if need to partition
v3.3.4,"if not contain query file, minimal sample unit is one record"
v3.3.4,"if contain query file, minimal sample unit is one query"
v3.3.4,if is new query
v3.3.4,read feature data
v3.3.4,read feature size
v3.3.4,re-allocate space if not enough
v3.3.4,raw data
v3.3.4,fill feature_names_ if not header
v3.3.4,get forced split
v3.3.4,"if only one machine, find bin locally"
v3.3.4,"if have multi-machines, need to find bin distributed"
v3.3.4,different machines will find bin for different features
v3.3.4,start and len will store the process feature indices for different machines
v3.3.4,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.4,free
v3.3.4,gather global feature bin mappers
v3.3.4,restore features bins from buffer
v3.3.4,---- private functions ----
v3.3.4,"if features are ordered, not need to use hist_buf"
v3.3.4,read all lines
v3.3.4,get query data
v3.3.4,"if not contain query data, minimal sample unit is one record"
v3.3.4,"if contain query data, minimal sample unit is one query"
v3.3.4,if is new query
v3.3.4,get query data
v3.3.4,"if not contain query file, minimal sample unit is one record"
v3.3.4,"if contain query file, minimal sample unit is one query"
v3.3.4,if is new query
v3.3.4,parse features
v3.3.4,get forced split
v3.3.4,"check the range of label_idx, weight_idx and group_idx"
v3.3.4,fill feature_names_ if not header
v3.3.4,start find bins
v3.3.4,"if only one machine, find bin locally"
v3.3.4,start and len will store the process feature indices for different machines
v3.3.4,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.4,free
v3.3.4,gather global feature bin mappers
v3.3.4,restore features bins from buffer
v3.3.4,if doesn't need to prediction with initial model
v3.3.4,parser
v3.3.4,set label
v3.3.4,free processed line:
v3.3.4,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.3.4,text_reader_->Lines()[i].shrink_to_fit();
v3.3.4,push data
v3.3.4,if is used feature
v3.3.4,if need to prediction with initial model
v3.3.4,parser
v3.3.4,set initial score
v3.3.4,set label
v3.3.4,free processed line:
v3.3.4,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v3.3.4,text_reader_->Lines()[i].shrink_to_fit();
v3.3.4,push data
v3.3.4,if is used feature
v3.3.4,metadata_ will manage space of init_score
v3.3.4,text data can be free after loaded feature values
v3.3.4,parser
v3.3.4,set initial score
v3.3.4,set label
v3.3.4,push data
v3.3.4,if is used feature
v3.3.4,only need part of data
v3.3.4,need full data
v3.3.4,metadata_ will manage space of init_score
v3.3.4,read size of token
v3.3.4,remove duplicates
v3.3.4,deep copy function for BinMapper
v3.3.4,mean size for one bin
v3.3.4,need a new bin
v3.3.4,update bin upper bound
v3.3.4,last bin upper bound
v3.3.4,get list of distinct values
v3.3.4,get number of positive and negative distinct values
v3.3.4,include zero bounds and infinity bound
v3.3.4,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.3.4,find remaining bounds
v3.3.4,find distinct_values first
v3.3.4,push zero in the front
v3.3.4,use the large value
v3.3.4,push zero in the back
v3.3.4,convert to int type first
v3.3.4,sort by counts
v3.3.4,will ignore the categorical of small counts
v3.3.4,Push the dummy bin for NaN
v3.3.4,Use MissingType::None to represent this bin contains all categoricals
v3.3.4,fix count of NaN bin
v3.3.4,check trivial(num_bin_ == 1) feature
v3.3.4,check useless bin
v3.3.4,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.3.4,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.3.4,calculate max bin of all features to select the int type in MultiValDenseBin
v3.3.4,"for lambdarank, it needs query data for partition data in distributed learning"
v3.3.4,need convert query_id to boundaries
v3.3.4,check weights
v3.3.4,check query boundries
v3.3.4,contain initial score file
v3.3.4,check weights
v3.3.4,get local weights
v3.3.4,check query boundries
v3.3.4,get local query boundaries
v3.3.4,contain initial score file
v3.3.4,get local initial scores
v3.3.4,re-load query weight
v3.3.4,save to nullptr
v3.3.4,save to nullptr
v3.3.4,save to nullptr
v3.3.4,default weight file name
v3.3.4,default init_score file name
v3.3.4,use first line to count number class
v3.3.4,default query file name
v3.3.4,root is in the depth 0
v3.3.4,non-leaf
v3.3.4,leaf
v3.3.4,use this for the missing value conversion
v3.3.4,Predict func by Map to ifelse
v3.3.4,use this for the missing value conversion
v3.3.4,non-leaf
v3.3.4,left subtree
v3.3.4,right subtree
v3.3.4,leaf
v3.3.4,non-leaf
v3.3.4,left subtree
v3.3.4,right subtree
v3.3.4,leaf
v3.3.4,recursive computation of SHAP values for a decision tree
v3.3.4,extend the unique path
v3.3.4,leaf node
v3.3.4,internal node
v3.3.4,"see if we have already split on this feature,"
v3.3.4,if so we undo that split so we can redo it for this node
v3.3.4,recursive sparse computation of SHAP values for a decision tree
v3.3.4,extend the unique path
v3.3.4,leaf node
v3.3.4,internal node
v3.3.4,"see if we have already split on this feature,"
v3.3.4,if so we undo that split so we can redo it for this node
v3.3.4,add names of objective function if not providing metric
v3.3.4,equal weights for all classes
v3.3.4,generate seeds by seed.
v3.3.4,sort eval_at
v3.3.4,Only push the non-training data
v3.3.4,check for conflicts
v3.3.4,"check if objective, metric, and num_class match"
v3.3.4,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.3.4,Check max_depth and num_leaves
v3.3.4,"Fits in an int, and is more restrictive than the current num_leaves"
v3.3.4,force col-wise for gpu & CUDA
v3.3.4,force gpu_use_dp for CUDA
v3.3.4,linear tree learner must be serial type and run on CPU device
v3.3.4,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.3.4,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.3.4,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.3.4,calculated split gain can be positive even with zero gradient and hessian.
v3.3.4,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.3.4,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.3.4,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.3.4,first round: fill the single val group
v3.3.4,always push the last group
v3.3.4,put dense feature first
v3.3.4,sort by non zero cnt
v3.3.4,"sort by non zero cnt, bigger first"
v3.3.4,shuffle groups
v3.3.4,Using std::swap for vector<bool> will cause the wrong result.
v3.3.4,get num_features
v3.3.4,get bin_mappers
v3.3.4,"for sparse multi value bin, we store the feature bin values with offset added"
v3.3.4,"for dense multi value bin, the feature bin values without offsets are used"
v3.3.4,copy feature bin mapper data
v3.3.4,copy feature bin mapper data
v3.3.4,"if not pass a filename, just append "".bin"" of original file"
v3.3.4,get size of header
v3.3.4,size of feature names
v3.3.4,size of forced bins
v3.3.4,write header
v3.3.4,write feature names
v3.3.4,write forced bins
v3.3.4,get size of meta data
v3.3.4,write meta data
v3.3.4,write feature data
v3.3.4,get size of feature
v3.3.4,write feature
v3.3.4,write raw data; use row-major order so we can read row-by-row
v3.3.4,"explicitly initialize template methods, for cross module call"
v3.3.4,"Only one multi-val group, just simply merge"
v3.3.4,Skip the leading 0 when copying group_bin_boundaries.
v3.3.4,regenerate other fields
v3.3.4,store the importance first
v3.3.4,PredictRaw
v3.3.4,PredictRawByMap
v3.3.4,Predict
v3.3.4,PredictByMap
v3.3.4,PredictLeafIndex
v3.3.4,PredictLeafIndexByMap
v3.3.4,output model type
v3.3.4,output number of class
v3.3.4,output label index
v3.3.4,output max_feature_idx
v3.3.4,output objective
v3.3.4,output tree models
v3.3.4,store the importance first
v3.3.4,sort the importance
v3.3.4,use serialized string to restore this object
v3.3.4,Use first 128 chars to avoid exceed the message buffer.
v3.3.4,get number of classes
v3.3.4,get index of label
v3.3.4,get max_feature_idx first
v3.3.4,get average_output
v3.3.4,get feature names
v3.3.4,get monotone_constraints
v3.3.4,set zero
v3.3.4,predict all the trees for one iteration
v3.3.4,check early stopping
v3.3.4,set zero
v3.3.4,predict all the trees for one iteration
v3.3.4,check early stopping
v3.3.4,margin_threshold will be captured by value
v3.3.4,copy and sort
v3.3.4,margin_threshold will be captured by value
v3.3.4,Fix for compiler warnings about reaching end of control
v3.3.4,load forced_splits file
v3.3.4,init tree learner
v3.3.4,push training metrics
v3.3.4,create buffer for gradients and Hessians
v3.3.4,get max feature index
v3.3.4,get label index
v3.3.4,get feature names
v3.3.4,"if need bagging, create buffer"
v3.3.4,"for a validation dataset, we need its score and metric"
v3.3.4,update score
v3.3.4,objective function will calculate gradients and hessians
v3.3.4,"random bagging, minimal unit is one record"
v3.3.4,"random bagging, minimal unit is one record"
v3.3.4,if need bagging
v3.3.4,set bagging data to tree learner
v3.3.4,get subset
v3.3.4,output used time per iteration
v3.3.4,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.3.4,boosting first
v3.3.4,bagging logic
v3.3.4,need to copy gradients for bagging subset.
v3.3.4,shrinkage by learning rate
v3.3.4,update score
v3.3.4,only add default score one-time
v3.3.4,updates scores
v3.3.4,add model
v3.3.4,reset score
v3.3.4,remove model
v3.3.4,print message for metric
v3.3.4,pop last early_stopping_round_ models
v3.3.4,update training score
v3.3.4,we need to predict out-of-bag scores of data for boosting
v3.3.4,update validation score
v3.3.4,print training metric
v3.3.4,print validation metric
v3.3.4,set zero
v3.3.4,predict all the trees for one iteration
v3.3.4,predict all the trees for one iteration
v3.3.4,push training metrics
v3.3.4,"not same training data, need reset score and others"
v3.3.4,create score tracker
v3.3.4,update score
v3.3.4,create buffer for gradients and hessians
v3.3.4,load forced_splits file
v3.3.4,"if need bagging, create buffer"
v3.3.4,Get the max size of pool
v3.3.4,at least need 2 leaves
v3.3.4,push split information for all leaves
v3.3.4,initialize splits for leaf
v3.3.4,initialize data partition
v3.3.4,initialize ordered gradients and hessians
v3.3.4,cannot change is_hist_col_wise during training
v3.3.4,initialize splits for leaf
v3.3.4,initialize data partition
v3.3.4,initialize ordered gradients and hessians
v3.3.4,Get the max size of pool
v3.3.4,at least need 2 leaves
v3.3.4,push split information for all leaves
v3.3.4,some initial works before training
v3.3.4,root leaf
v3.3.4,only root leaf can be splitted on first time
v3.3.4,some initial works before finding best split
v3.3.4,find best threshold for every feature
v3.3.4,Get a leaf with max split gain
v3.3.4,Get split information for best leaf
v3.3.4,"cannot split, quit"
v3.3.4,split tree with best leaf
v3.3.4,reset histogram pool
v3.3.4,initialize data partition
v3.3.4,reset the splits for leaves
v3.3.4,Sumup for root
v3.3.4,use all data
v3.3.4,"use bagging, only use part of data"
v3.3.4,check depth of current leaf
v3.3.4,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.3.4,no enough data to continue
v3.3.4,only have root
v3.3.4,put parent(left) leaf's histograms into larger leaf's histograms
v3.3.4,put parent(left) leaf's histograms to larger leaf's histograms
v3.3.4,construct smaller leaf
v3.3.4,construct larger leaf
v3.3.4,find splits
v3.3.4,only has root leaf
v3.3.4,start at root leaf
v3.3.4,"before processing next node from queue, store info for current left/right leaf"
v3.3.4,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.3.4,"then, compute own splits"
v3.3.4,split info should exist because searching in bfs fashion - should have added from parent
v3.3.4,update before tree split
v3.3.4,don't need to update this in data-based parallel model
v3.3.4,"split tree, will return right leaf"
v3.3.4,store the true split gain in tree model
v3.3.4,don't need to update this in data-based parallel model
v3.3.4,store the true split gain in tree model
v3.3.4,init the leaves that used on next iteration
v3.3.4,update leave outputs if needed
v3.3.4,bag_mapper[index_mapper[i]]
v3.3.4,it is needed to filter the features after the above code.
v3.3.4,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.3.4,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.3.4,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.3.4,find splits
v3.3.4,identify features containing nans
v3.3.4,preallocate the matrix used to calculate linear model coefficients
v3.3.4,"store only upper triangular half of matrix as an array, in row-major order"
v3.3.4,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.3.4,we add another 8 to ensure cache lines are not shared among processors
v3.3.4,some initial works before training
v3.3.4,root leaf
v3.3.4,only root leaf can be splitted on first time
v3.3.4,some initial works before finding best split
v3.3.4,find best threshold for every feature
v3.3.4,Get a leaf with max split gain
v3.3.4,Get split information for best leaf
v3.3.4,"cannot split, quit"
v3.3.4,split tree with best leaf
v3.3.4,map data to leaf number
v3.3.4,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.3.4,the coefficients vector is given by
v3.3.4,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.3.4,where:
v3.3.4,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.3.4,"H is the diagonal matrix of the hessian,"
v3.3.4,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.3.4,g is the vector of gradients
v3.3.4,the subscript _T denotes the transpose
v3.3.4,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.3.4,clear the coefficient matrices
v3.3.4,aggregate results from different threads
v3.3.4,copy into eigen matrices and solve
v3.3.4,update the tree properties
v3.3.4,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.4,get feature partition
v3.3.4,get local used features
v3.3.4,get best split at smaller leaf
v3.3.4,find local best split for larger leaf
v3.3.4,sync global best info
v3.3.4,update best split
v3.3.4,"instantiate template classes, otherwise linker cannot find the code"
v3.3.4,initialize SerialTreeLearner
v3.3.4,Get local rank and global machine size
v3.3.4,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.4,allocate buffer for communication
v3.3.4,generate feature partition for current tree
v3.3.4,get local used feature
v3.3.4,get block start and block len for reduce scatter
v3.3.4,get buffer_write_start_pos_
v3.3.4,get buffer_read_start_pos_
v3.3.4,sync global data sumup info
v3.3.4,global sumup reduce
v3.3.4,copy back
v3.3.4,set global sumup info
v3.3.4,init global data count in leaf
v3.3.4,clear histogram buffer before synchronizing
v3.3.4,otherwise histogram contents from the previous iteration will be sent
v3.3.4,construct local histograms
v3.3.4,copy to buffer
v3.3.4,Reduce scatter for histogram
v3.3.4,restore global histograms from buffer
v3.3.4,only root leaf
v3.3.4,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.3.4,find local best split for larger leaf
v3.3.4,sync global best info
v3.3.4,set best split
v3.3.4,need update global number of data in leaf
v3.3.4,"instantiate template classes, otherwise linker cannot find the code"
v3.3.4,initialize SerialTreeLearner
v3.3.4,some additional variables needed for GPU trainer
v3.3.4,Initialize GPU buffers and kernels
v3.3.4,some functions used for debugging the GPU histogram construction
v3.3.4,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.3.4,goto err;
v3.3.4,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.4,also guarantee that there are at least 2K examples per workgroup
v3.3.4,return 0;
v3.3.4,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v3.3.4,decide the best number of workgroups working on one feature4 tuple
v3.3.4,set work group size based on feature size
v3.3.4,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.4,we need to refresh the kernel arguments after reallocating
v3.3.4,The only argument that needs to be changed later is num_data_
v3.3.4,"the GPU kernel will process all features in one call, and each"
v3.3.4,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.3.4,process one feature4 tuple
v3.3.4,"for the root node, indices are not copied"
v3.3.4,"for constant hessian, hessians are not copied except for the root node"
v3.3.4,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.3.4,and we will launch num_feature workgroups for this kernel
v3.3.4,will launch threads for all features
v3.3.4,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.3.4,copy the results asynchronously. Size depends on if double precision is used
v3.3.4,we will wait for this object in WaitAndGetHistograms
v3.3.4,"when the output is ready, the computation is done"
v3.3.4,values of this feature has been redistributed to multiple bins; need a reduction here
v3.3.4,how many feature-group tuples we have
v3.3.4,leave some safe margin for prefetching
v3.3.4,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.4,clear sparse/dense maps
v3.3.4,do nothing if no features can be processed on GPU
v3.3.4,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.3.4,unpin old buffer if necessary before destructing them
v3.3.4,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v3.3.4,allocate space for gradients and Hessians on device
v3.3.4,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.4,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.3.4,copy indices to the device
v3.3.4,histogram bin entry size depends on the precision (single/double)
v3.3.4,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.4,each work group generates a sub-histogram of dword_features_ features.
v3.3.4,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.4,create atomic counters for inter-group coordination
v3.3.4,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.4,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.4,looking for dword_features_ non-sparse feature-groups
v3.3.4,decide if we need to redistribute the bin
v3.3.4,multiplier must be a power of 2
v3.3.4,device_bin_mults_.push_back(1);
v3.3.4,found
v3.3.4,for data transfer time
v3.3.4,"Now generate new data structure feature4, and copy data to the device"
v3.3.4,"preallocate arrays for all threads, and pin them"
v3.3.4,building Feature4 bundles; each thread handles dword_features_ features
v3.3.4,one feature datapoint is 4 bits
v3.3.4,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.4,one feature datapoint is one byte
v3.3.4,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.4,Dense bin
v3.3.4,Dense 4-bit bin
v3.3.4,working on the remaining (less than dword_features_) feature groups
v3.3.4,fill the leftover features
v3.3.4,"fill this empty feature with some ""random"" value"
v3.3.4,"fill this empty feature with some ""random"" value"
v3.3.4,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.3.4,deallocate pinned space for feature copying
v3.3.4,data transfer time
v3.3.4,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.3.4,"Something bad happened. Just return ""No log available."""
v3.3.4,"build is okay, log may contain warnings"
v3.3.4,destroy any old kernels
v3.3.4,create OpenCL kernels for different number of workgroups per feature
v3.3.4,currently we don't use constant memory
v3.3.4,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.3.4,kernel with indices in an array
v3.3.4,"kernel with all features enabled, with eliminated branches"
v3.3.4,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.3.4,do nothing if no features can be processed on GPU
v3.3.4,The only argument that needs to be changed later is num_data_
v3.3.4,"hessian is passed as a parameter, but it is not available now."
v3.3.4,hessian will be set in BeforeTrain()
v3.3.4,"Get the max bin size, used for selecting best GPU kernel"
v3.3.4,initialize GPU
v3.3.4,determine which kernel to use based on the max number of bins
v3.3.4,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.4,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.4,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.4,setup GPU kernel arguments after we allocating all the buffers
v3.3.4,GPU memory has to been reallocated because data may have been changed
v3.3.4,setup GPU kernel arguments after we allocating all the buffers
v3.3.4,Copy initial full hessians and gradients to GPU.
v3.3.4,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.4,setup hessian parameters only
v3.3.4,hessian is passed as a parameter
v3.3.4,use bagging
v3.3.4,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v3.3.4,copy used gradients and Hessians to ordered buffer
v3.3.4,transfer the indices to GPU
v3.3.4,transfer hessian to GPU
v3.3.4,setup hessian parameters only
v3.3.4,hessian is passed as a parameter
v3.3.4,transfer gradients to GPU
v3.3.4,only have root
v3.3.4,"Copy indices, gradients and Hessians as early as possible"
v3.3.4,only need to initialize for smaller leaf
v3.3.4,Get leaf boundary
v3.3.4,copy indices to the GPU:
v3.3.4,copy ordered Hessians to the GPU:
v3.3.4,copy ordered gradients to the GPU:
v3.3.4,do nothing if no features can be processed on GPU
v3.3.4,copy data indices if it is not null
v3.3.4,generate and copy ordered_gradients if gradients is not null
v3.3.4,generate and copy ordered_hessians if Hessians is not null
v3.3.4,converted indices in is_feature_used to feature-group indices
v3.3.4,construct the feature masks for dense feature-groups
v3.3.4,"if no feature group is used, just return and do not use GPU"
v3.3.4,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.4,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.4,"All data have been prepared, now run the GPU kernel"
v3.3.4,construct smaller leaf
v3.3.4,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v3.3.4,then construct sparse features on CPU
v3.3.4,"wait for GPU to finish, only if GPU is actually used"
v3.3.4,use double precision
v3.3.4,use single precision
v3.3.4,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v3.3.4,#define GPU_DEBUG_COMPARE
v3.3.4,construct larger leaf
v3.3.4,then construct sparse features on CPU
v3.3.4,"wait for GPU to finish, only if GPU is actually used"
v3.3.4,use double precision
v3.3.4,use single precision
v3.3.4,do some sanity check for the GPU algorithm
v3.3.4,limit top k
v3.3.4,get max bin
v3.3.4,calculate buffer size
v3.3.4,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.4,"left and right on same time, so need double size"
v3.3.4,initialize histograms for global
v3.3.4,sync global data sumup info
v3.3.4,set global sumup info
v3.3.4,init global data count in leaf
v3.3.4,get local sumup
v3.3.4,get local sumup
v3.3.4,get mean number on machines
v3.3.4,weighted gain
v3.3.4,get top k
v3.3.4,"Copy histogram to buffer, and Get local aggregate features"
v3.3.4,copy histograms.
v3.3.4,copy smaller leaf histograms first
v3.3.4,mark local aggregated feature
v3.3.4,copy
v3.3.4,then copy larger leaf histograms
v3.3.4,mark local aggregated feature
v3.3.4,copy
v3.3.4,use local data to find local best splits
v3.3.4,clear histogram buffer before synchronizing
v3.3.4,otherwise histogram contents from the previous iteration will be sent
v3.3.4,find splits
v3.3.4,only has root leaf
v3.3.4,local voting
v3.3.4,gather
v3.3.4,get all top-k from all machines
v3.3.4,global voting
v3.3.4,copy local histgrams to buffer
v3.3.4,Reduce scatter for histogram
v3.3.4,find best split from local aggregated histograms
v3.3.4,restore from buffer
v3.3.4,restore from buffer
v3.3.4,find local best
v3.3.4,find local best split for larger leaf
v3.3.4,sync global best info
v3.3.4,copy back
v3.3.4,set the global number of data for leaves
v3.3.4,init the global sumup info
v3.3.4,"instantiate template classes, otherwise linker cannot find the code"
v3.3.4,launch cuda kernel
v3.3.4,initialize SerialTreeLearner
v3.3.4,some additional variables needed for GPU trainer
v3.3.4,Initialize GPU buffers and kernels: get device info
v3.3.4,some functions used for debugging the GPU histogram construction
v3.3.4,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.4,also guarantee that there are at least 2K examples per workgroup
v3.3.4,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.3.4,decide the best number of workgroups working on one feature4 tuple
v3.3.4,set work group size based on feature size
v3.3.4,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.4,set thread_data
v3.3.4,copy the results asynchronously. Size depends on if double precision is used
v3.3.4,"when the output is ready, the computation is done"
v3.3.4,how many feature-group tuples we have
v3.3.4,leave some safe margin for prefetching
v3.3.4,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.4,clear sparse/dense maps
v3.3.4,do nothing it there is no dense feature
v3.3.4,calculate number of feature groups per gpu
v3.3.4,histogram bin entry size depends on the precision (single/double)
v3.3.4,allocate GPU memory for each GPU
v3.3.4,do nothing it there is no gpu feature
v3.3.4,allocate memory for all features
v3.3.4,allocate space for gradients and hessians on device
v3.3.4,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.4,copy indices to the device
v3.3.4,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.4,each work group generates a sub-histogram of dword_features_ features.
v3.3.4,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.4,create atomic counters for inter-group coordination
v3.3.4,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.4,clear sparse/dense maps
v3.3.4,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.4,set device info
v3.3.4,looking for dword_features_ non-sparse feature-groups
v3.3.4,reset device info
v3.3.4,InitGPU w/ num_gpu
v3.3.4,"Get the max bin size, used for selecting best GPU kernel"
v3.3.4,get num_dense_feature_groups_
v3.3.4,initialize GPU
v3.3.4,set cpu threads
v3.3.4,resize device memory pointers
v3.3.4,create stream & events to handle multiple GPUs
v3.3.4,check data size
v3.3.4,GPU memory has to been reallocated because data may have been changed
v3.3.4,AllocateGPUMemory only when the number of data increased
v3.3.4,setup GPU kernel arguments after we allocating all the buffers
v3.3.4,Copy initial full hessians and gradients to GPU.
v3.3.4,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.4,use bagging
v3.3.4,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.3.4,copy used gradients and hessians to ordered buffer
v3.3.4,transfer the indices to GPU
v3.3.4,only have root
v3.3.4,"Copy indices, gradients and hessians as early as possible"
v3.3.4,only need to initialize for smaller leaf
v3.3.4,Get leaf boundary
v3.3.4,do nothing if no features can be processed on GPU
v3.3.4,copy data indices if it is not null
v3.3.4,converted indices in is_feature_used to feature-group indices
v3.3.4,construct the feature masks for dense feature-groups
v3.3.4,"if no feature group is used, just return and do not use GPU"
v3.3.4,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.4,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.4,We now copy even if all features are used.
v3.3.4,"All data have been prepared, now run the GPU kernel"
v3.3.4,construct smaller leaf
v3.3.4,Check workgroups per feature4 tuple..
v3.3.4,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.3.4,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.3.4,then construct sparse features on CPU
v3.3.4,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.4,"wait for GPU to finish, only if GPU is actually used"
v3.3.4,use double precision
v3.3.4,use single precision
v3.3.4,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.3.4,#define CUDA_DEBUG_COMPARE
v3.3.4,construct larger leaf
v3.3.4,then construct sparse features on CPU
v3.3.4,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.4,"wait for GPU to finish, only if GPU is actually used"
v3.3.4,use double precision
v3.3.4,use single precision
v3.3.4,do some sanity check for the GPU algorithm
v3.3.3,coding: utf-8
v3.3.3,coding: utf-8
v3.3.3,create predictor first
v3.3.3,"show deprecation warning only for early stop argument, setting early stop via global params should still be possible"
v3.3.3,check dataset
v3.3.3,reduce cost for prediction training data
v3.3.3,process callbacks
v3.3.3,Most of legacy advanced options becomes callbacks
v3.3.3,construct booster
v3.3.3,start training
v3.3.3,check evaluation result.
v3.3.3,"ranking task, split according to groups"
v3.3.3,run preprocessing on the data set if needed
v3.3.3,setup callbacks
v3.3.3,coding: utf-8
v3.3.3,dummy function to support older version of scikit-learn
v3.3.3,coding: utf-8
v3.3.3,documentation templates for LGBMModel methods are shared between the classes in
v3.3.3,this module and those in the ``dask`` module
v3.3.3,"user can set verbose with kwargs, it has higher priority"
v3.3.3,Do not modify original args in fit function
v3.3.3,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.3.3,Separate built-in from callable evaluation metrics
v3.3.3,register default metric for consistency with callable eval_metric case
v3.3.3,try to deduce from class instance
v3.3.3,overwrite default metric by explicitly set metric
v3.3.3,concatenate metric from params (or default if not provided in params) and eval_metric
v3.3.3,copy for consistency
v3.3.3,reduce cost for prediction training data
v3.3.3,free dataset
v3.3.3,Switch to using a multiclass objective in the underlying LGBM instance
v3.3.3,"do not modify args, as it causes errors in model selection tools"
v3.3.3,check group data
v3.3.3,coding: utf-8
v3.3.3,we don't need lib_lightgbm while building docs
v3.3.3,coding: utf-8
v3.3.3,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.3.3,coding: utf-8
v3.3.3,coding: utf-8
v3.3.3,TypeError: obj is not a string or a number
v3.3.3,ValueError: invalid literal
v3.3.3,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.3.3,avoid side effects on passed-in parameters
v3.3.3,"find a value, and remove other aliases with .pop()"
v3.3.3,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.3.3,Get total row number.
v3.3.3,Random access by row index. Used for data sampling.
v3.3.3,Range data access. Used to read data in batch when constructing Dataset.
v3.3.3,Optionally specify batch_size to control range data read size.
v3.3.3,Only required if using ``Dataset.subset()``.
v3.3.3,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.3,avoid memory consumption by arrays concatenation operations
v3.3.3,create numpy array from output arrays
v3.3.3,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.3.3,for CSC there is extra column added
v3.3.3,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.3.3,same shape as input csr or csc matrix except extra column for expected value
v3.3.3,note: make sure we copy data as it will be deallocated next
v3.3.3,"free the temporary native indptr, indices, and data"
v3.3.3,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.3,avoid memory consumption by arrays concatenation operations
v3.3.3,c type: double**
v3.3.3,each double* element points to start of each column of sample data.
v3.3.3,c type int**
v3.3.3,each int* points to start of indices for each column
v3.3.3,"no min_data, nthreads and verbose in this function"
v3.3.3,check data has header or not
v3.3.3,need to regroup init_score
v3.3.3,process for args
v3.3.3,"user can set verbose with params, it has higher priority"
v3.3.3,get categorical features
v3.3.3,process for reference dataset
v3.3.3,start construct data
v3.3.3,set feature names
v3.3.3,"Select sampled rows, transpose to column order."
v3.3.3,create validation dataset from ref_dataset
v3.3.3,create valid
v3.3.3,construct subset
v3.3.3,create train
v3.3.3,could be updated if data is not freed
v3.3.3,set to None
v3.3.3,we're done if self and reference share a common upstream reference
v3.3.3,"if buffer length is not long enough, reallocate buffers"
v3.3.3,"group data from LightGBM is boundaries data, need to convert to group size"
v3.3.3,"user can set verbose with params, it has higher priority"
v3.3.3,Training task
v3.3.3,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.3.3,construct booster object
v3.3.3,copy the parameters from train_set
v3.3.3,save reference to data
v3.3.3,buffer for inner predict
v3.3.3,Prediction task
v3.3.3,if a single node tree it won't have `leaf_index` so return 0
v3.3.3,"Create the node record, and populate universal data members"
v3.3.3,Update values to reflect node type (leaf or split)
v3.3.3,traverse the next level of the tree
v3.3.3,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.3.3,and we add node to the list.
v3.3.3,need reset training data
v3.3.3,need to push new valid data
v3.3.3,"if buffer length is not long enough, re-allocate a buffer"
v3.3.3,"if buffer length is not long enough, reallocate a buffer"
v3.3.3,Copy models
v3.3.3,Get name of features
v3.3.3,"if buffer length is not long enough, reallocate buffers"
v3.3.3,avoid to predict many time in one iteration
v3.3.3,Get num of inner evals
v3.3.3,Get name of eval metrics
v3.3.3,"if buffer length is not long enough, reallocate buffers"
v3.3.3,coding: utf-8
v3.3.3,Callback environment used by callbacks
v3.3.3,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.3,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.3,coding: utf-8
v3.3.3,Concatenate many parts into one
v3.3.3,construct local eval_set data.
v3.3.3,store indices of eval_set components that were not contained within local parts.
v3.3.3,consolidate parts of each individual eval component.
v3.3.3,require that eval_name exists in evaluated result data in case dropped due to padding.
v3.3.3,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v3.3.3,filter padding from eval parts then _concat each eval_set component.
v3.3.3,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v3.3.3,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v3.3.3,capture whether local_listen_port or its aliases were provided
v3.3.3,capture whether machines or its aliases were provided
v3.3.3,Some passed-in parameters can be removed:
v3.3.3,* 'num_machines': set automatically from Dask worker list
v3.3.3,* 'num_threads': overridden to match nthreads on each Dask process
v3.3.3,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.3.3,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v3.3.3,X and y are each delayed sub-lists of original eval dask Collections.
v3.3.3,find maximum number of parts in an individual eval set so that we can
v3.3.3,pad eval sets when they come in different sizes.
v3.3.3,"when individual eval set is equivalent to training data, skip recomputing parts."
v3.3.3,add None-padding for individual eval_set member if it is smaller than the largest member.
v3.3.3,first time a chunk of this eval set is added to this part.
v3.3.3,append additional chunks of this eval set to this part.
v3.3.3,ensure that all evaluation parts map uniquely to one part.
v3.3.3,assign sub-eval_set components to worker parts.
v3.3.3,Start computation in the background
v3.3.3,Find locations of all parts and map them to particular Dask workers
v3.3.3,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v3.3.3,data artifacts may not be populated depending on worker returning final estimator.
v3.3.3,assign general validation set settings to fit kwargs.
v3.3.3,resolve aliases for network parameters and pop the result off params.
v3.3.3,these values are added back in calls to `_train_part()`
v3.3.3,figure out network params
v3.3.3,Tell each worker to train on the parts that it has locally
v3.3.3,
v3.3.3,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v3.3.3,1. there is randomness in the training process unless parameters ``seed``
v3.3.3,and ``deterministic`` are set
v3.3.3,"2. even with those parameters set, the output of one ``_train_part()`` call"
v3.3.3,relies on global state (it and all the other LightGBM training processes
v3.3.3,coordinate with each other)
v3.3.3,"if network parameters were changed during training, remove them from the"
v3.3.3,returned model so that they're generated dynamically on every run based
v3.3.3,on the Dask cluster you're connected to and which workers have pieces of
v3.3.3,the training data
v3.3.3,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.3.3,"for multi-class classification with sparse matrices, pred_contrib predictions"
v3.3.3,are returned as a list of sparse matrices (one per class)
v3.3.3,"pred_contrib output will have one column per feature,"
v3.3.3,plus one more for the base value
v3.3.3,need to tell Dask the expected type and shape of individual preds
v3.3.3,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v3.3.3,the code below is used instead to ensure that the sparse type is preserved during concatentation
v3.3.3,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v3.3.3,Concatenate them to return a list of Dask Arrays.
v3.3.3,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.3,currently relevant for the Dask estimators
v3.3.3,"DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds."
v3.3.3,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.3.3,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.3,currently relevant for the Dask estimators
v3.3.3,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds."
v3.3.3,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.3.3,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.3,currently relevant for the Dask estimators
v3.3.3,DaskLGBMRanker does not support eval_class_weight or early stopping
v3.3.3,DaskLGBMRanker support for callbacks and init_model is not tested
v3.3.3,coding: utf-8
v3.3.3,load or create your dataset
v3.3.3,create dataset for lightgbm
v3.3.3,"if you want to re-use data, remember to set free_raw_data=False"
v3.3.3,specify your configurations as a dict
v3.3.3,generate feature names
v3.3.3,feature_name and categorical_feature
v3.3.3,check feature name
v3.3.3,save model to file
v3.3.3,dump model to JSON (and save to file)
v3.3.3,feature names
v3.3.3,feature importances
v3.3.3,load model to predict
v3.3.3,can only predict with the best iteration (or the saving iteration)
v3.3.3,eval with loaded model
v3.3.3,dump model with pickle
v3.3.3,load model with pickle to predict
v3.3.3,can predict with any iteration when loaded in pickle way
v3.3.3,eval with loaded model
v3.3.3,continue training
v3.3.3,init_model accepts:
v3.3.3,1. model file name
v3.3.3,2. Booster()
v3.3.3,decay learning rates
v3.3.3,learning_rates accepts:
v3.3.3,1. list/tuple with length = num_boost_round
v3.3.3,2. function(curr_iter)
v3.3.3,change other parameters during training
v3.3.3,self-defined objective function
v3.3.3,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.3.3,log likelihood loss
v3.3.3,self-defined eval metric
v3.3.3,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.3,binary error
v3.3.3,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.3,This may make built-in evaluation metric calculate wrong results
v3.3.3,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.3,Keep this in mind when you use the customization
v3.3.3,another self-defined eval metric
v3.3.3,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.3,accuracy
v3.3.3,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.3,This may make built-in evaluation metric calculate wrong results
v3.3.3,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.3,Keep this in mind when you use the customization
v3.3.3,callback
v3.3.3,coding: utf-8
v3.3.3,load or create your dataset
v3.3.3,train
v3.3.3,predict
v3.3.3,eval
v3.3.3,feature importances
v3.3.3,self-defined eval metric
v3.3.3,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.3,Root Mean Squared Logarithmic Error (RMSLE)
v3.3.3,train
v3.3.3,another self-defined eval metric
v3.3.3,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.3,Relative Absolute Error (RAE)
v3.3.3,train
v3.3.3,predict
v3.3.3,eval
v3.3.3,other scikit-learn modules
v3.3.3,coding: utf-8
v3.3.3,load or create your dataset
v3.3.3,create dataset for lightgbm
v3.3.3,specify your configurations as a dict
v3.3.3,train
v3.3.3,coding: utf-8
v3.3.3,################
v3.3.3,Simulate some binary data with a single categorical and
v3.3.3,single continuous predictor
v3.3.3,################
v3.3.3,Set up a couple of utilities for our experiments
v3.3.3,################
v3.3.3,Observe the behavior of `binary` and `xentropy` objectives
v3.3.3,Trying this throws an error on non-binary values of y:
v3.3.3,"experiment('binary', label_type='probability', DATA)"
v3.3.3,The speed of `binary` is not drastically different than
v3.3.3,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.3.3,there are reasons to suspect that `binary` should run faster when the
v3.3.3,label is an integer instead of a float
v3.3.3,coding: utf-8
v3.3.3,load or create your dataset
v3.3.3,create dataset for lightgbm
v3.3.3,specify your configurations as a dict
v3.3.3,train
v3.3.3,save model to file
v3.3.3,predict
v3.3.3,eval
v3.3.3,We can also open HDF5 file once and get access to
v3.3.3,"With binary dataset created, we can use either Python API or cmdline version to train."
v3.3.3,
v3.3.3,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v3.3.3,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v3.3.3,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v3.3.3,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v3.3.3,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v3.3.3,Y has a single column and we read it in single shot. So store it as an 1-d array.
v3.3.3,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v3.3.3,"When accessing any element in a HDF5 chunk, it's read entirely."
v3.3.3,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v3.3.3,Here we are just creating a chunk size that matches with batch_size.
v3.3.3,
v3.3.3,Also note that the data is stored in row major order to avoid extra copy when passing to
v3.3.3,lightgbm Dataset.
v3.3.3,Save to 2 HDF5 files for demonstration.
v3.3.3,We can store multiple datasets inside a single HDF5 file.
v3.3.3,Separating X and Y for choosing best chunk size for data loading.
v3.3.3,split training data into two partitions
v3.3.3,make this array dense because we're splitting across
v3.3.3,a sparse boundary to partition the data
v3.3.3,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.3.3,predictions and target values back from workers to the client
v3.3.3,
v3.3.3,"for larger datasets, consider the metrics from dask-ml instead"
v3.3.3,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.3.3,coding: utf-8
v3.3.3,!/usr/bin/env python3
v3.3.3,-*- coding: utf-8 -*-
v3.3.3,
v3.3.3,"LightGBM documentation build configuration file, created by"
v3.3.3,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.3.3,
v3.3.3,This file is execfile()d with the current directory set to its
v3.3.3,containing dir.
v3.3.3,
v3.3.3,Note that not all possible configuration values are present in this
v3.3.3,autogenerated file.
v3.3.3,
v3.3.3,All configuration values have a default; values that are commented out
v3.3.3,serve to show the default.
v3.3.3,"If extensions (or modules to document with autodoc) are in another directory,"
v3.3.3,add these directories to sys.path here. If the directory is relative to the
v3.3.3,"documentation root, use os.path.abspath to make it absolute."
v3.3.3,-- General configuration ------------------------------------------------
v3.3.3,"If your documentation needs a minimal Sphinx version, state it here."
v3.3.3,"Add any Sphinx extension module names here, as strings. They can be"
v3.3.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.3.3,ones.
v3.3.3,mock out modules
v3.3.3,hide type hints in API docs
v3.3.3,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.3.3,Only the class' docstring is inserted.
v3.3.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.3.3,The master toctree document.
v3.3.3,General information about the project.
v3.3.3,The name of an image file (relative to this directory) to place at the top
v3.3.3,of the sidebar.
v3.3.3,The name of an image file (relative to this directory) to use as a favicon of
v3.3.3,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.3.3,pixels large.
v3.3.3,"The version info for the project you're documenting, acts as replacement for"
v3.3.3,"|version| and |release|, also used in various other places throughout the"
v3.3.3,built documents.
v3.3.3,The short X.Y version.
v3.3.3,"The full version, including alpha/beta/rc tags."
v3.3.3,The language for content autogenerated by Sphinx. Refer to documentation
v3.3.3,for a list of supported languages.
v3.3.3,
v3.3.3,This is also used if you do content translation via gettext catalogs.
v3.3.3,"Usually you set ""language"" from the command line for these cases."
v3.3.3,"List of patterns, relative to source directory, that match files and"
v3.3.3,directories to ignore when looking for source files.
v3.3.3,This patterns also effect to html_static_path and html_extra_path
v3.3.3,The name of the Pygments (syntax highlighting) style to use.
v3.3.3,-- Configuration for C API docs generation ------------------------------
v3.3.3,-- Options for HTML output ----------------------------------------------
v3.3.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.3.3,a list of builtin themes.
v3.3.3,Theme options are theme-specific and customize the look and feel of a theme
v3.3.3,"further.  For a list of options available for each theme, see the"
v3.3.3,documentation.
v3.3.3,"Add any paths that contain custom static files (such as style sheets) here,"
v3.3.3,"relative to this directory. They are copied after the builtin static files,"
v3.3.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.3.3,-- Options for HTMLHelp output ------------------------------------------
v3.3.3,Output file base name for HTML help builder.
v3.3.3,-- Options for LaTeX output ---------------------------------------------
v3.3.3,The name of an image file (relative to this directory) to place at the top of
v3.3.3,the title page.
v3.3.3,Warning! The following code can cause buffer overflows on RTD.
v3.3.3,Consider suppressing output completely if RTD project silently fails.
v3.3.3,Refer to https://github.com/svenevs/exhale
v3.3.3,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.3,Warning! The following code can cause buffer overflows on RTD.
v3.3.3,Consider suppressing output completely if RTD project silently fails.
v3.3.3,Refer to https://github.com/svenevs/exhale
v3.3.3,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.3,coding: utf-8
v3.3.3,This is a basic test for floating number parsing.
v3.3.3,Most of the test cases come from:
v3.3.3,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v3.3.3,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v3.3.3,FLT_MAX
v3.3.3,FLT_MIN
v3.3.3,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v3.3.3,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v3.3.3,Near DBL_MIN
v3.3.3,DBL_MIN 2^-1022
v3.3.3,The behavior for parsing -nan depends on implementation.
v3.3.3,Thus we skip binary check for negative nan.
v3.3.3,See comment in test_cases.
v3.3.3,Constants
v3.3.3,Start with some content:
v3.3.3,Clear & re-use:
v3.3.3,Output should match new content:
v3.3.3,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.3.3,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.3.3,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.3.3,Compute a new trial address & value & if it is a valid address:
v3.3.3,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.3.3,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.3.3,Check the just-stored value with getitem():
v3.3.3,Also store the just-stored value for future tracking:
v3.3.3,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.3.3,Test in 2 ways that the values are correctly laid out in memory:
v3.3.3,coding: utf-8
v3.3.3,we don't need lib_lightgbm while building docs
v3.3.3,coding: utf-8
v3.3.3,check saved model persistence
v3.3.3,"we need to check the consistency of model file here, so test for exact equal"
v3.3.3,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.3.3,"scores likely to be different, but prediction should still be the same"
v3.3.3,test that shape is checked during prediction
v3.3.3,"The simple implementation is just a single ""return self.ndarray[idx]"""
v3.3.3,The following is for demo and testing purpose.
v3.3.3,whole col
v3.3.3,half col
v3.3.3,Create dataset from numpy array directly.
v3.3.3,Create dataset using Sequence.
v3.3.3,Test for validation set.
v3.3.3,Select some random rows as valid data.
v3.3.3,"From Dataset constructor, with dataset from numpy array."
v3.3.3,"From Dataset.create_valid, with dataset from sequence."
v3.3.3,test that method works even with free_raw_data=True
v3.3.3,test that method works but sets raw data to None in case of immergeable data types
v3.3.3,test that method works for different data types
v3.3.3,"Set extremely harsh penalties, so CEGB will block most splits."
v3.3.3,"Compare pairs of penalties, to ensure scaling works as intended"
v3.3.3,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.3.3,"should resolve duplicate aliases, and prefer the main parameter"
v3.3.3,should choose a value from an alias and set that value on main param
v3.3.3,if only an alias is used
v3.3.3,should use the default if main param and aliases are missing
v3.3.3,all changes should be made on copies and not modify the original
v3.3.3,coding: utf-8
v3.3.3,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.3.3,set_index ensures partitions are based on group id.
v3.3.3,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.3.3,"separate target, weight from features."
v3.3.3,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.3.3,"so that within each partition, sum(g) = n_samples."
v3.3.3,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.3.3,make one categorical feature relevant to the target
v3.3.3,https://github.com/microsoft/LightGBM/issues/4118
v3.3.3,extra predict() parameters should be passed through correctly
v3.3.3,pref_leaf values should have the right shape
v3.3.3,and values that look like valid tree nodes
v3.3.3,"be sure LightGBM actually used at least one categorical column,"
v3.3.3,and that it was correctly treated as a categorical feature
v3.3.3,shape depends on whether it is binary or multiclass classification
v3.3.3,"in the special case of multi-class classification using scipy sparse matrices,"
v3.3.3,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v3.3.3,
v3.3.3,"since that case is so different than all other cases, check the relevant things here"
v3.3.3,and then return early
v3.3.3,"raw scores will probably be different, but at least check that all predicted classes are the same"
v3.3.3,"be sure LightGBM actually used at least one categorical column,"
v3.3.3,and that it was correctly treated as a categorical feature
v3.3.3,* shape depends on whether it is binary or multiclass classification
v3.3.3,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.3.3,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.3.3,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.3,that the output has the right shape and base values are in the right position
v3.3.3,check that found ports are different for same address (LocalCluster)
v3.3.3,check that the ports are indeed open
v3.3.3,Scores should be the same
v3.3.3,Predictions should be roughly the same.
v3.3.3,pref_leaf values should have the right shape
v3.3.3,and values that look like valid tree nodes
v3.3.3,extra predict() parameters should be passed through correctly
v3.3.3,"be sure LightGBM actually used at least one categorical column,"
v3.3.3,and that it was correctly treated as a categorical feature
v3.3.3,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.3,that the output has the right shape and base values are in the right position
v3.3.3,"be sure LightGBM actually used at least one categorical column,"
v3.3.3,and that it was correctly treated as a categorical feature
v3.3.3,Quantiles should be right
v3.3.3,"be sure LightGBM actually used at least one categorical column,"
v3.3.3,and that it was correctly treated as a categorical feature
v3.3.3,rebalance small dask.Array dataset for better performance.
v3.3.3,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.3.3,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.3.3,distributed ranker should be able to rank decently well and should
v3.3.3,have high rank correlation with scores from serial ranker.
v3.3.3,extra predict() parameters should be passed through correctly
v3.3.3,pref_leaf values should have the right shape
v3.3.3,and values that look like valid tree nodes
v3.3.3,"be sure LightGBM actually used at least one categorical column,"
v3.3.3,and that it was correctly treated as a categorical feature
v3.3.3,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v3.3.3,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v3.3.3,"test eval_class_weight, eval_init_score on binary-classification task."
v3.3.3,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v3.3.3,create eval_sets by creating new datasets or copying training data.
v3.3.3,total number of trees scales up for ova classifier.
v3.3.3,check that early stopping was not applied.
v3.3.3,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v3.3.3,"check that each eval_name and metric exists for all eval sets, allowing for the"
v3.3.3,case when a worker receives a fully-padded eval_set component which is not evaluated.
v3.3.3,should be able to use the class without specifying a client
v3.3.3,should be able to set client after construction
v3.3.3,data on cluster1
v3.3.3,create identical data on cluster2
v3.3.3,"at this point, the result of default_client() is client2 since it was the most recently"
v3.3.3,created. So setting client to client1 here to test that you can select a non-default client
v3.3.3,"unfitted model should survive pickling round trip, and pickling"
v3.3.3,shouldn't have side effects on the model object
v3.3.3,client will always be None after unpickling
v3.3.3,"fitted model should survive pickling round trip, and pickling"
v3.3.3,shouldn't have side effects on the model object
v3.3.3,client will always be None after unpickling
v3.3.3,rebalance data to be sure that each worker has a piece of the data
v3.3.3,model 1 - no network parameters given
v3.3.3,model 2 - machines given
v3.3.3,model 3 - local_listen_port given
v3.3.3,training should fail because LightGBM will try to use the same
v3.3.3,port for multiple worker processes on the same machine
v3.3.3,rebalance data to be sure that each worker has a piece of the data
v3.3.3,"test that ""machines"" is actually respected by creating a socket that uses"
v3.3.3,"one of the ports mentioned in ""machines"""
v3.3.3,The above error leaves a worker waiting
v3.3.3,"an informative error should be raised if ""machines"" has duplicates"
v3.3.3,"""client"" should be the only different, and the final argument"
v3.3.3,value of the root node is 0 when init_score is set
v3.3.3,this test is separate because it takes a not-yet-constructed estimator
v3.3.3,coding: utf-8
v3.3.3,coding: utf-8
v3.3.3,"build target, group ID vectors."
v3.3.3,build y/target and group-id vectors with user-specified group sizes.
v3.3.3,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.3.3,groups should contain > 1 element for pairwise learning objective.
v3.3.3,"build feature data, X. Transform first few into informative features."
v3.3.3,coding: utf-8
v3.3.3,prediction result is actually not transformed (is raw) due to custom objective
v3.3.3,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.3.3,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.3.3,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.3,sklearn < 0.23 does not have as_frame parameter
v3.3.3,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.3,sklearn < 0.23 does not have as_frame parameter
v3.3.3,Test if random_state is properly stored
v3.3.3,Test if two random states produce identical models
v3.3.3,Test if subsequent fits sample from random_state object and produce different models
v3.3.3,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.3.3,With default params
v3.3.3,Tests same probabilities
v3.3.3,Tests same predictions
v3.3.3,Tests same raw scores
v3.3.3,Tests same leaf indices
v3.3.3,Tests same feature contributions
v3.3.3,Tests other parameters for the prediction works
v3.3.3,Tests start_iteration
v3.3.3,"Tests same probabilities, starting from iteration 10"
v3.3.3,"Tests same predictions, starting from iteration 10"
v3.3.3,"Tests same raw scores, starting from iteration 10"
v3.3.3,"Tests same leaf indices, starting from iteration 10"
v3.3.3,"Tests same feature contributions, starting from iteration 10"
v3.3.3,"Tests other parameters for the prediction works, starting from iteration 10"
v3.3.3,"no custom objective, no custom metric"
v3.3.3,default metric
v3.3.3,non-default metric
v3.3.3,no metric
v3.3.3,non-default metric in eval_metric
v3.3.3,non-default metric with non-default metric in eval_metric
v3.3.3,non-default metric with multiple metrics in eval_metric
v3.3.3,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.3.3,default metric for non-default objective
v3.3.3,non-default metric for non-default objective
v3.3.3,no metric
v3.3.3,non-default metric in eval_metric for non-default objective
v3.3.3,non-default metric with non-default metric in eval_metric for non-default objective
v3.3.3,non-default metric with multiple metrics in eval_metric for non-default objective
v3.3.3,"custom objective, no custom metric"
v3.3.3,default regression metric for custom objective
v3.3.3,non-default regression metric for custom objective
v3.3.3,multiple regression metrics for custom objective
v3.3.3,no metric
v3.3.3,default regression metric with non-default metric in eval_metric for custom objective
v3.3.3,non-default regression metric with metric in eval_metric for custom objective
v3.3.3,multiple regression metrics with metric in eval_metric for custom objective
v3.3.3,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.3.3,"no custom objective, custom metric"
v3.3.3,default metric with custom metric
v3.3.3,non-default metric with custom metric
v3.3.3,multiple metrics with custom metric
v3.3.3,custom metric (disable default metric)
v3.3.3,default metric for non-default objective with custom metric
v3.3.3,non-default metric for non-default objective with custom metric
v3.3.3,multiple metrics for non-default objective with custom metric
v3.3.3,custom metric (disable default metric for non-default objective)
v3.3.3,"custom objective, custom metric"
v3.3.3,custom metric for custom objective
v3.3.3,non-default regression metric with custom metric for custom objective
v3.3.3,multiple regression metrics with custom metric for custom objective
v3.3.3,default metric and invalid binary metric is replaced with multiclass alternative
v3.3.3,invalid objective is replaced with default multiclass one
v3.3.3,and invalid binary metric is replaced with multiclass alternative
v3.3.3,default metric for non-default multiclass objective
v3.3.3,and invalid binary metric is replaced with multiclass alternative
v3.3.3,default metric and invalid multiclass metric is replaced with binary alternative
v3.3.3,invalid multiclass metric is replaced with binary alternative for custom objective
v3.3.3,"Verify that can receive a list of metrics, only callable"
v3.3.3,Verify that can receive a list of custom and built-in metrics
v3.3.3,Verify that works as expected when eval_metric is empty
v3.3.3,"Verify that can receive a list of metrics, only built-in"
v3.3.3,Verify that eval_metric is robust to receiving a list with None
v3.3.3,training data as eval_set
v3.3.3,feval
v3.3.3,single eval_set
v3.3.3,two eval_set
v3.3.3,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.3.3,Test that estimators are default-constructible
v3.3.3,coding: utf-8
v3.3.3,coding: utf-8
v3.3.3,check that default gives same result as k = 1
v3.3.3,check against independent calculation for k = 1
v3.3.3,check against independent calculation for k = 2
v3.3.3,check against independent calculation for k = 10
v3.3.3,check cases where predictions are equal
v3.3.3,should give same result as binary auc for 2 classes
v3.3.3,test the case where all predictions are equal
v3.3.3,test that weighted data gives different auc_mu
v3.3.3,test that equal data weights give same auc_mu as unweighted data
v3.3.3,should give 1 when accuracy = 1
v3.3.3,test loading class weights
v3.3.3,no early stopping
v3.3.3,early stopping occurs
v3.3.3,test custom eval metrics
v3.3.3,"shuffle = False, override metric in params"
v3.3.3,"shuffle = True, callbacks"
v3.3.3,enable display training loss
v3.3.3,self defined folds
v3.3.3,LambdaRank
v3.3.3,... with l2 metric
v3.3.3,... with NDCG (default) metric
v3.3.3,self defined folds with lambdarank
v3.3.3,with early stopping
v3.3.3,predict by each fold booster
v3.3.3,fold averaging
v3.3.3,without early stopping
v3.3.3,test feature_names with whitespaces
v3.3.3,This has non-ascii strings.
v3.3.3,take subsets and train
v3.3.3,generate CSR sparse dataset
v3.3.3,convert data to dense and get back same contribs
v3.3.3,validate the values are the same
v3.3.3,validate using CSC matrix
v3.3.3,validate the values are the same
v3.3.3,generate CSR sparse dataset
v3.3.3,convert data to dense and get back same contribs
v3.3.3,validate the values are the same
v3.3.3,validate using CSC matrix
v3.3.3,validate the values are the same
v3.3.3,Note there is an extra column added to the output for the expected value
v3.3.3,Note output CSC shape should be same as CSR output shape
v3.3.3,test sliced labels
v3.3.3,append some columns
v3.3.3,append some rows
v3.3.3,test sliced 2d matrix
v3.3.3,test sliced CSR
v3.3.3,trees start at position 1.
v3.3.3,split_features are in 4th line.
v3.3.3,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.3.3,The penalization is so high that the first 2 features should not be used here
v3.3.3,Check that a very high penalization is the same as not using the features at all
v3.3.3,"no fobj, no feval"
v3.3.3,default metric
v3.3.3,non-default metric in params
v3.3.3,default metric in args
v3.3.3,non-default metric in args
v3.3.3,metric in args overwrites one in params
v3.3.3,multiple metrics in params
v3.3.3,multiple metrics in args
v3.3.3,remove default metric by 'None' in list
v3.3.3,remove default metric by 'None' aliases
v3.3.3,"fobj, no feval"
v3.3.3,no default metric
v3.3.3,metric in params
v3.3.3,metric in args
v3.3.3,metric in args overwrites its' alias in params
v3.3.3,multiple metrics in params
v3.3.3,multiple metrics in args
v3.3.3,"no fobj, feval"
v3.3.3,default metric with custom one
v3.3.3,non-default metric in params with custom one
v3.3.3,default metric in args with custom one
v3.3.3,non-default metric in args with custom one
v3.3.3,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.3,multiple metrics in params with custom one
v3.3.3,multiple metrics in args with custom one
v3.3.3,custom metric is evaluated despite 'None' is passed
v3.3.3,"fobj, feval"
v3.3.3,"no default metric, only custom one"
v3.3.3,metric in params with custom one
v3.3.3,metric in args with custom one
v3.3.3,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.3,multiple metrics in params with custom one
v3.3.3,multiple metrics in args with custom one
v3.3.3,custom metric is evaluated despite 'None' is passed
v3.3.3,"no fobj, no feval"
v3.3.3,default metric
v3.3.3,default metric in params
v3.3.3,non-default metric in params
v3.3.3,multiple metrics in params
v3.3.3,remove default metric by 'None' aliases
v3.3.3,"fobj, no feval"
v3.3.3,no default metric
v3.3.3,metric in params
v3.3.3,multiple metrics in params
v3.3.3,"no fobj, feval"
v3.3.3,default metric with custom one
v3.3.3,default metric in params with custom one
v3.3.3,non-default metric in params with custom one
v3.3.3,multiple metrics in params with custom one
v3.3.3,custom metric is evaluated despite 'None' is passed
v3.3.3,"fobj, feval"
v3.3.3,"no default metric, only custom one"
v3.3.3,metric in params with custom one
v3.3.3,multiple metrics in params with custom one
v3.3.3,custom metric is evaluated despite 'None' is passed
v3.3.3,multiclass default metric
v3.3.3,multiclass default metric with custom one
v3.3.3,multiclass metric alias with custom one for custom objective
v3.3.3,no metric for invalid class_num
v3.3.3,custom metric for invalid class_num
v3.3.3,multiclass metric alias with custom one with invalid class_num
v3.3.3,multiclass default metric without num_class
v3.3.3,multiclass metric alias
v3.3.3,multiclass metric
v3.3.3,non-valid metric for multiclass objective
v3.3.3,non-default num_class for default objective
v3.3.3,no metric with non-default num_class for custom objective
v3.3.3,multiclass metric alias for custom objective
v3.3.3,multiclass metric for custom objective
v3.3.3,binary metric with non-default num_class for custom objective
v3.3.3,Expect three metrics but mean and stdv for each metric
v3.3.3,test XGBoost-style return value
v3.3.3,test numpy-style return value
v3.3.3,test bins string type
v3.3.3,test histogram is disabled for categorical features
v3.3.3,test for lgb.train
v3.3.3,test feval for lgb.train
v3.3.3,test with two valid data for lgb.train
v3.3.3,test for lgb.cv
v3.3.3,test feval for lgb.cv
v3.3.3,test that binning works properly for features with only positive or only negative values
v3.3.3,decreasing without freeing raw data is allowed
v3.3.3,decreasing before lazy init is allowed
v3.3.3,increasing is allowed
v3.3.3,decreasing with disabled filter is allowed
v3.3.3,decreasing with enabled filter is disallowed;
v3.3.3,also changes of other params are disallowed
v3.3.3,check extra trees increases regularization
v3.3.3,check path smoothing increases regularization
v3.3.3,test edge case with one leaf
v3.3.3,check that constraint containing all features is equivalent to no constraint
v3.3.3,check that constraint partitioning the features reduces train accuracy
v3.3.3,check that constraints consisting of single features reduce accuracy further
v3.3.3,test that interaction constraints work when not all features are used
v3.3.3,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.3.3,test again with nans in data
v3.3.3,test again with bagging
v3.3.3,test with a feature that has only one non-nan value
v3.3.3,test with a categorical feature
v3.3.3,test refit: same results on same data
v3.3.3,test refit with save and load
v3.3.3,test refit: different results training on different data
v3.3.3,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.3.3,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.3.3,"test the case where start_iteration <= 0, and num_iteration is None"
v3.3.3,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.3.3,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.3.3,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.3.3,test for regression
v3.3.3,test both with and without early stopping
v3.3.3,test for multi-class
v3.3.3,test both with and without early stopping
v3.3.3,test for binary
v3.3.3,test both with and without early stopping
v3.3.3,test against sklearn average precision metric
v3.3.3,test that average precision is 1 where model predicts perfectly
v3.3.3,coding: utf-8
v3.3.3,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.3.3,coding: utf-8
v3.3.3,coding: utf-8
v3.3.3,These are helper functions to allow doing a stack unwind
v3.3.3,"after an R allocation error, which would trigger a long jump."
v3.3.3,convert from one-based to zero-based index
v3.3.3,"if any feature names were larger than allocated size,"
v3.3.3,allow for a larger size and try again
v3.3.3,convert from boundaries to size
v3.3.3,--- start Booster interfaces
v3.3.3,"if any eval names were larger than allocated size,"
v3.3.3,allow for a larger size and try again
v3.3.3,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.3,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.3,.Call() calls
v3.3.3,coding: utf-8
v3.3.3,alias table
v3.3.3,names
v3.3.3,from strings
v3.3.3,tails
v3.3.3,tails
v3.3.3,coding: utf-8
v3.3.3,Single row predictor to abstract away caching logic
v3.3.3,create boosting
v3.3.3,initialize the boosting
v3.3.3,create objective function
v3.3.3,initialize the objective function
v3.3.3,create training metric
v3.3.3,reset the boosting
v3.3.3,create objective function
v3.3.3,initialize the objective function
v3.3.3,calculate the nonzero data and indices size
v3.3.3,allocate data and indices arrays
v3.3.3,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.3,aggregated per row feature contribution results
v3.3.3,keep track of the row_vector sizes for parallelization
v3.3.3,copy vector results to output for each row
v3.3.3,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.3,aggregated per row feature contribution results
v3.3.3,calculate number of elements per column to construct
v3.3.3,the CSC matrix with random access
v3.3.3,keep track of column counts
v3.3.3,keep track of beginning index for each column
v3.3.3,keep track of beginning index for each matrix
v3.3.3,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.3.3,store the row index
v3.3.3,update column count
v3.3.3,explicitly declare symbols from LightGBM namespace
v3.3.3,some help functions used to convert data
v3.3.3,Row iterator of on column for CSC matrix
v3.3.3,"return value at idx, only can access by ascent order"
v3.3.3,"return next non-zero pair, if index < 0, means no more data"
v3.3.3,start of c_api functions
v3.3.3,This API is to keep python binding's behavior the same with C++ implementation.
v3.3.3,"Sample count, random seed etc. should be provided in parameters."
v3.3.3,sample data first
v3.3.3,sample data first
v3.3.3,sample data first
v3.3.3,local buffer to re-use memory
v3.3.3,sample data first
v3.3.3,no more data
v3.3.3,---- start of booster
v3.3.3,Single row in row-major format:
v3.3.3,---- start of some help functions
v3.3.3,data is array of pointers to individual rows
v3.3.3,set number of threads for openmp
v3.3.3,check for alias
v3.3.3,read parameters from config file
v3.3.3,"remove str after ""#"""
v3.3.3,check for alias again
v3.3.3,load configs
v3.3.3,prediction is needed if using input initial model(continued train)
v3.3.3,need to continue training
v3.3.3,sync up random seed for data partition
v3.3.3,load Training data
v3.3.3,load data for distributed training
v3.3.3,load data for single machine
v3.3.3,need save binary file
v3.3.3,create training metric
v3.3.3,only when have metrics then need to construct validation data
v3.3.3,"Add validation data, if it exists"
v3.3.3,add
v3.3.3,need save binary file
v3.3.3,add metric for validation data
v3.3.3,output used time on each iteration
v3.3.3,need init network
v3.3.3,create boosting
v3.3.3,create objective function
v3.3.3,load training data
v3.3.3,initialize the objective function
v3.3.3,initialize the boosting
v3.3.3,add validation data into boosting
v3.3.3,convert model to if-else statement code
v3.3.3,create predictor
v3.3.3,Free memory
v3.3.3,create predictor
v3.3.3,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.3.3,counts for all labels
v3.3.3,"start from top label, and accumulate DCG"
v3.3.3,counts for all labels
v3.3.3,calculate k Max DCG by one pass
v3.3.3,get sorted indices by score
v3.3.3,calculate multi dcg by one pass
v3.3.3,wait for all client start up
v3.3.3,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.3.3,Instead we will handle finalize/abort for MPI in main().
v3.3.3,default set to -1
v3.3.3,"distance at k-th communication, distance[k] = 2^k"
v3.3.3,set incoming rank at k-th commuication
v3.3.3,set outgoing rank at k-th commuication
v3.3.3,default set as -1
v3.3.3,construct all recursive halving map for all machines
v3.3.3,let 1 << k <= num_machines
v3.3.3,distance of each communication
v3.3.3,"if num_machines = 2^k, don't need to group machines"
v3.3.3,"communication direction, %2 == 0 is positive"
v3.3.3,neighbor at k-th communication
v3.3.3,receive data block at k-th communication
v3.3.3,send data block at k-th communication
v3.3.3,"if num_machines != 2^k, need to group machines"
v3.3.3,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.3.3,let left machine as group leader
v3.3.3,"cache block information for groups, group with 2 machines will have double block size"
v3.3.3,convert from group to node leader
v3.3.3,convert from node to group
v3.3.3,meet new group
v3.3.3,add block len for this group
v3.3.3,calculate the group block start
v3.3.3,not need to construct
v3.3.3,get receive block information
v3.3.3,accumulate block len
v3.3.3,get send block information
v3.3.3,accumulate block len
v3.3.3,static member definition
v3.3.3,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.3.3,assign the blocks to every rank.
v3.3.3,do reduce scatter
v3.3.3,do all gather
v3.3.3,assign blocks
v3.3.3,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.3.3,copy back
v3.3.3,assign blocks
v3.3.3,start all gather
v3.3.3,when num_machines is small and data is large
v3.3.3,use output as receive buffer
v3.3.3,get current local block size
v3.3.3,get out rank
v3.3.3,get in rank
v3.3.3,get send information
v3.3.3,get recv information
v3.3.3,send and recv at same time
v3.3.3,rotate in-place
v3.3.3,use output as receive buffer
v3.3.3,get current local block size
v3.3.3,get send information
v3.3.3,get recv information
v3.3.3,send and recv at same time
v3.3.3,use output as receive buffer
v3.3.3,send and recv at same time
v3.3.3,send local data to neighbor first
v3.3.3,receive neighbor data first
v3.3.3,reduce
v3.3.3,get target
v3.3.3,get send information
v3.3.3,get recv information
v3.3.3,send and recv at same time
v3.3.3,reduce
v3.3.3,send result to neighbor
v3.3.3,receive result from neighbor
v3.3.3,copy result
v3.3.3,start up socket
v3.3.3,parse clients from file
v3.3.3,get ip list of local machine
v3.3.3,get local rank
v3.3.3,construct listener
v3.3.3,construct communication topo
v3.3.3,construct linkers
v3.3.3,free listener
v3.3.3,set timeout
v3.3.3,accept incoming socket
v3.3.3,receive rank
v3.3.3,add new socket
v3.3.3,save ranks that need to connect with
v3.3.3,start listener
v3.3.3,start connect
v3.3.3,let smaller rank connect to larger rank
v3.3.3,send local rank
v3.3.3,wait for listener
v3.3.3,print connected linkers
v3.3.3,only need to copy subset
v3.3.3,avoid to copy subset many times
v3.3.3,avoid out of range
v3.3.3,may need to recopy subset
v3.3.3,valid the type
v3.3.3,Constructors
v3.3.3,Get type tag
v3.3.3,Comparisons
v3.3.3,"This has to be separate, not in Statics, because Json() accesses"
v3.3.3,statics().null.
v3.3.3,"advance until next line, or end of input"
v3.3.3,advance until closing tokens
v3.3.3,The usual case: non-escaped characters
v3.3.3,Handle escapes
v3.3.3,Extract 4-byte escape sequence
v3.3.3,Explicitly check length of the substring. The following loop
v3.3.3,relies on std::string returning the terminating NUL when
v3.3.3,accessing str[length]. Checking here reduces brittleness.
v3.3.3,JSON specifies that characters outside the BMP shall be encoded as a
v3.3.3,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.3.3,components. Check whether we're in the middle of such a beast: the
v3.3.3,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.3.3,a trail (low) surrogate.
v3.3.3,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.3.3,per the UTF-16 algorithm.
v3.3.3,Integer part
v3.3.3,Decimal part
v3.3.3,Exponent part
v3.3.3,Check for any trailing garbage
v3.3.3,Documented in json11.hpp
v3.3.3,Check for another object
v3.3.3,get column names
v3.3.3,load label idx first
v3.3.3,erase label column name
v3.3.3,load ignore columns
v3.3.3,load weight idx
v3.3.3,load group idx
v3.3.3,don't support query id in data file when using distributed training
v3.3.3,read data to memory
v3.3.3,sample data
v3.3.3,construct feature bin mappers
v3.3.3,initialize label
v3.3.3,extract features
v3.3.3,sample data from file
v3.3.3,construct feature bin mappers
v3.3.3,initialize label
v3.3.3,extract features
v3.3.3,load data from binary file
v3.3.3,check meta data
v3.3.3,need to check training data
v3.3.3,read data in memory
v3.3.3,initialize label
v3.3.3,extract features
v3.3.3,Get number of lines of data file
v3.3.3,initialize label
v3.3.3,extract features
v3.3.3,load data from binary file
v3.3.3,not need to check validation data
v3.3.3,check meta data
v3.3.3,buffer to read binary file
v3.3.3,check token
v3.3.3,read size of header
v3.3.3,re-allocmate space if not enough
v3.3.3,read header
v3.3.3,get header
v3.3.3,num_groups
v3.3.3,real_feature_idx_
v3.3.3,feature2group
v3.3.3,feature2subfeature
v3.3.3,group_bin_boundaries
v3.3.3,group_feature_start_
v3.3.3,group_feature_cnt_
v3.3.3,get feature names
v3.3.3,write feature names
v3.3.3,get forced_bin_bounds_
v3.3.3,read size of meta data
v3.3.3,re-allocate space if not enough
v3.3.3,read meta data
v3.3.3,load meta data
v3.3.3,sample local used data if need to partition
v3.3.3,"if not contain query file, minimal sample unit is one record"
v3.3.3,"if contain query file, minimal sample unit is one query"
v3.3.3,if is new query
v3.3.3,read feature data
v3.3.3,read feature size
v3.3.3,re-allocate space if not enough
v3.3.3,raw data
v3.3.3,fill feature_names_ if not header
v3.3.3,get forced split
v3.3.3,"if only one machine, find bin locally"
v3.3.3,"if have multi-machines, need to find bin distributed"
v3.3.3,different machines will find bin for different features
v3.3.3,start and len will store the process feature indices for different machines
v3.3.3,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.3,free
v3.3.3,gather global feature bin mappers
v3.3.3,restore features bins from buffer
v3.3.3,---- private functions ----
v3.3.3,"if features are ordered, not need to use hist_buf"
v3.3.3,read all lines
v3.3.3,get query data
v3.3.3,"if not contain query data, minimal sample unit is one record"
v3.3.3,"if contain query data, minimal sample unit is one query"
v3.3.3,if is new query
v3.3.3,get query data
v3.3.3,"if not contain query file, minimal sample unit is one record"
v3.3.3,"if contain query file, minimal sample unit is one query"
v3.3.3,if is new query
v3.3.3,parse features
v3.3.3,get forced split
v3.3.3,"check the range of label_idx, weight_idx and group_idx"
v3.3.3,fill feature_names_ if not header
v3.3.3,start find bins
v3.3.3,"if only one machine, find bin locally"
v3.3.3,start and len will store the process feature indices for different machines
v3.3.3,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.3,free
v3.3.3,gather global feature bin mappers
v3.3.3,restore features bins from buffer
v3.3.3,if doesn't need to prediction with initial model
v3.3.3,parser
v3.3.3,set label
v3.3.3,free processed line:
v3.3.3,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.3.3,text_reader_->Lines()[i].shrink_to_fit();
v3.3.3,push data
v3.3.3,if is used feature
v3.3.3,if need to prediction with initial model
v3.3.3,parser
v3.3.3,set initial score
v3.3.3,set label
v3.3.3,free processed line:
v3.3.3,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v3.3.3,text_reader_->Lines()[i].shrink_to_fit();
v3.3.3,push data
v3.3.3,if is used feature
v3.3.3,metadata_ will manage space of init_score
v3.3.3,text data can be free after loaded feature values
v3.3.3,parser
v3.3.3,set initial score
v3.3.3,set label
v3.3.3,push data
v3.3.3,if is used feature
v3.3.3,only need part of data
v3.3.3,need full data
v3.3.3,metadata_ will manage space of init_score
v3.3.3,read size of token
v3.3.3,remove duplicates
v3.3.3,deep copy function for BinMapper
v3.3.3,mean size for one bin
v3.3.3,need a new bin
v3.3.3,update bin upper bound
v3.3.3,last bin upper bound
v3.3.3,get list of distinct values
v3.3.3,get number of positive and negative distinct values
v3.3.3,include zero bounds and infinity bound
v3.3.3,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.3.3,find remaining bounds
v3.3.3,find distinct_values first
v3.3.3,push zero in the front
v3.3.3,use the large value
v3.3.3,push zero in the back
v3.3.3,convert to int type first
v3.3.3,sort by counts
v3.3.3,will ignore the categorical of small counts
v3.3.3,Push the dummy bin for NaN
v3.3.3,Use MissingType::None to represent this bin contains all categoricals
v3.3.3,fix count of NaN bin
v3.3.3,check trivial(num_bin_ == 1) feature
v3.3.3,check useless bin
v3.3.3,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.3.3,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.3.3,calculate max bin of all features to select the int type in MultiValDenseBin
v3.3.3,"for lambdarank, it needs query data for partition data in distributed learning"
v3.3.3,need convert query_id to boundaries
v3.3.3,check weights
v3.3.3,check query boundries
v3.3.3,contain initial score file
v3.3.3,check weights
v3.3.3,get local weights
v3.3.3,check query boundries
v3.3.3,get local query boundaries
v3.3.3,contain initial score file
v3.3.3,get local initial scores
v3.3.3,re-load query weight
v3.3.3,save to nullptr
v3.3.3,save to nullptr
v3.3.3,save to nullptr
v3.3.3,default weight file name
v3.3.3,default init_score file name
v3.3.3,use first line to count number class
v3.3.3,default query file name
v3.3.3,root is in the depth 0
v3.3.3,non-leaf
v3.3.3,leaf
v3.3.3,use this for the missing value conversion
v3.3.3,Predict func by Map to ifelse
v3.3.3,use this for the missing value conversion
v3.3.3,non-leaf
v3.3.3,left subtree
v3.3.3,right subtree
v3.3.3,leaf
v3.3.3,non-leaf
v3.3.3,left subtree
v3.3.3,right subtree
v3.3.3,leaf
v3.3.3,recursive computation of SHAP values for a decision tree
v3.3.3,extend the unique path
v3.3.3,leaf node
v3.3.3,internal node
v3.3.3,"see if we have already split on this feature,"
v3.3.3,if so we undo that split so we can redo it for this node
v3.3.3,recursive sparse computation of SHAP values for a decision tree
v3.3.3,extend the unique path
v3.3.3,leaf node
v3.3.3,internal node
v3.3.3,"see if we have already split on this feature,"
v3.3.3,if so we undo that split so we can redo it for this node
v3.3.3,add names of objective function if not providing metric
v3.3.3,equal weights for all classes
v3.3.3,generate seeds by seed.
v3.3.3,sort eval_at
v3.3.3,Only push the non-training data
v3.3.3,check for conflicts
v3.3.3,"check if objective, metric, and num_class match"
v3.3.3,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.3.3,Check max_depth and num_leaves
v3.3.3,"Fits in an int, and is more restrictive than the current num_leaves"
v3.3.3,force col-wise for gpu & CUDA
v3.3.3,force gpu_use_dp for CUDA
v3.3.3,linear tree learner must be serial type and run on CPU device
v3.3.3,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.3.3,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.3.3,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.3.3,calculated split gain can be positive even with zero gradient and hessian.
v3.3.3,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.3.3,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.3.3,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.3.3,first round: fill the single val group
v3.3.3,always push the last group
v3.3.3,put dense feature first
v3.3.3,sort by non zero cnt
v3.3.3,"sort by non zero cnt, bigger first"
v3.3.3,shuffle groups
v3.3.3,Using std::swap for vector<bool> will cause the wrong result.
v3.3.3,get num_features
v3.3.3,get bin_mappers
v3.3.3,"for sparse multi value bin, we store the feature bin values with offset added"
v3.3.3,"for dense multi value bin, the feature bin values without offsets are used"
v3.3.3,copy feature bin mapper data
v3.3.3,copy feature bin mapper data
v3.3.3,"if not pass a filename, just append "".bin"" of original file"
v3.3.3,get size of header
v3.3.3,size of feature names
v3.3.3,size of forced bins
v3.3.3,write header
v3.3.3,write feature names
v3.3.3,write forced bins
v3.3.3,get size of meta data
v3.3.3,write meta data
v3.3.3,write feature data
v3.3.3,get size of feature
v3.3.3,write feature
v3.3.3,write raw data; use row-major order so we can read row-by-row
v3.3.3,"explicitly initialize template methods, for cross module call"
v3.3.3,"Only one multi-val group, just simply merge"
v3.3.3,Skip the leading 0 when copying group_bin_boundaries.
v3.3.3,regenerate other fields
v3.3.3,store the importance first
v3.3.3,PredictRaw
v3.3.3,PredictRawByMap
v3.3.3,Predict
v3.3.3,PredictByMap
v3.3.3,PredictLeafIndex
v3.3.3,PredictLeafIndexByMap
v3.3.3,output model type
v3.3.3,output number of class
v3.3.3,output label index
v3.3.3,output max_feature_idx
v3.3.3,output objective
v3.3.3,output tree models
v3.3.3,store the importance first
v3.3.3,sort the importance
v3.3.3,use serialized string to restore this object
v3.3.3,Use first 128 chars to avoid exceed the message buffer.
v3.3.3,get number of classes
v3.3.3,get index of label
v3.3.3,get max_feature_idx first
v3.3.3,get average_output
v3.3.3,get feature names
v3.3.3,get monotone_constraints
v3.3.3,set zero
v3.3.3,predict all the trees for one iteration
v3.3.3,check early stopping
v3.3.3,set zero
v3.3.3,predict all the trees for one iteration
v3.3.3,check early stopping
v3.3.3,margin_threshold will be captured by value
v3.3.3,copy and sort
v3.3.3,margin_threshold will be captured by value
v3.3.3,Fix for compiler warnings about reaching end of control
v3.3.3,load forced_splits file
v3.3.3,init tree learner
v3.3.3,push training metrics
v3.3.3,create buffer for gradients and Hessians
v3.3.3,get max feature index
v3.3.3,get label index
v3.3.3,get feature names
v3.3.3,"if need bagging, create buffer"
v3.3.3,"for a validation dataset, we need its score and metric"
v3.3.3,update score
v3.3.3,objective function will calculate gradients and hessians
v3.3.3,"random bagging, minimal unit is one record"
v3.3.3,"random bagging, minimal unit is one record"
v3.3.3,if need bagging
v3.3.3,set bagging data to tree learner
v3.3.3,get subset
v3.3.3,output used time per iteration
v3.3.3,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.3.3,boosting first
v3.3.3,bagging logic
v3.3.3,need to copy gradients for bagging subset.
v3.3.3,shrinkage by learning rate
v3.3.3,update score
v3.3.3,only add default score one-time
v3.3.3,updates scores
v3.3.3,add model
v3.3.3,reset score
v3.3.3,remove model
v3.3.3,print message for metric
v3.3.3,pop last early_stopping_round_ models
v3.3.3,update training score
v3.3.3,we need to predict out-of-bag scores of data for boosting
v3.3.3,update validation score
v3.3.3,print training metric
v3.3.3,print validation metric
v3.3.3,set zero
v3.3.3,predict all the trees for one iteration
v3.3.3,predict all the trees for one iteration
v3.3.3,push training metrics
v3.3.3,"not same training data, need reset score and others"
v3.3.3,create score tracker
v3.3.3,update score
v3.3.3,create buffer for gradients and hessians
v3.3.3,load forced_splits file
v3.3.3,"if need bagging, create buffer"
v3.3.3,Get the max size of pool
v3.3.3,at least need 2 leaves
v3.3.3,push split information for all leaves
v3.3.3,initialize splits for leaf
v3.3.3,initialize data partition
v3.3.3,initialize ordered gradients and hessians
v3.3.3,cannot change is_hist_col_wise during training
v3.3.3,initialize splits for leaf
v3.3.3,initialize data partition
v3.3.3,initialize ordered gradients and hessians
v3.3.3,Get the max size of pool
v3.3.3,at least need 2 leaves
v3.3.3,push split information for all leaves
v3.3.3,some initial works before training
v3.3.3,root leaf
v3.3.3,only root leaf can be splitted on first time
v3.3.3,some initial works before finding best split
v3.3.3,find best threshold for every feature
v3.3.3,Get a leaf with max split gain
v3.3.3,Get split information for best leaf
v3.3.3,"cannot split, quit"
v3.3.3,split tree with best leaf
v3.3.3,reset histogram pool
v3.3.3,initialize data partition
v3.3.3,reset the splits for leaves
v3.3.3,Sumup for root
v3.3.3,use all data
v3.3.3,"use bagging, only use part of data"
v3.3.3,check depth of current leaf
v3.3.3,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.3.3,no enough data to continue
v3.3.3,only have root
v3.3.3,put parent(left) leaf's histograms into larger leaf's histograms
v3.3.3,put parent(left) leaf's histograms to larger leaf's histograms
v3.3.3,construct smaller leaf
v3.3.3,construct larger leaf
v3.3.3,find splits
v3.3.3,only has root leaf
v3.3.3,start at root leaf
v3.3.3,"before processing next node from queue, store info for current left/right leaf"
v3.3.3,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.3.3,"then, compute own splits"
v3.3.3,split info should exist because searching in bfs fashion - should have added from parent
v3.3.3,update before tree split
v3.3.3,don't need to update this in data-based parallel model
v3.3.3,"split tree, will return right leaf"
v3.3.3,store the true split gain in tree model
v3.3.3,don't need to update this in data-based parallel model
v3.3.3,store the true split gain in tree model
v3.3.3,init the leaves that used on next iteration
v3.3.3,update leave outputs if needed
v3.3.3,bag_mapper[index_mapper[i]]
v3.3.3,it is needed to filter the features after the above code.
v3.3.3,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.3.3,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.3.3,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.3.3,find splits
v3.3.3,identify features containing nans
v3.3.3,preallocate the matrix used to calculate linear model coefficients
v3.3.3,"store only upper triangular half of matrix as an array, in row-major order"
v3.3.3,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.3.3,we add another 8 to ensure cache lines are not shared among processors
v3.3.3,some initial works before training
v3.3.3,root leaf
v3.3.3,only root leaf can be splitted on first time
v3.3.3,some initial works before finding best split
v3.3.3,find best threshold for every feature
v3.3.3,Get a leaf with max split gain
v3.3.3,Get split information for best leaf
v3.3.3,"cannot split, quit"
v3.3.3,split tree with best leaf
v3.3.3,map data to leaf number
v3.3.3,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.3.3,the coefficients vector is given by
v3.3.3,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.3.3,where:
v3.3.3,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.3.3,"H is the diagonal matrix of the hessian,"
v3.3.3,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.3.3,g is the vector of gradients
v3.3.3,the subscript _T denotes the transpose
v3.3.3,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.3.3,clear the coefficient matrices
v3.3.3,aggregate results from different threads
v3.3.3,copy into eigen matrices and solve
v3.3.3,update the tree properties
v3.3.3,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.3,get feature partition
v3.3.3,get local used features
v3.3.3,get best split at smaller leaf
v3.3.3,find local best split for larger leaf
v3.3.3,sync global best info
v3.3.3,update best split
v3.3.3,"instantiate template classes, otherwise linker cannot find the code"
v3.3.3,initialize SerialTreeLearner
v3.3.3,Get local rank and global machine size
v3.3.3,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.3,allocate buffer for communication
v3.3.3,generate feature partition for current tree
v3.3.3,get local used feature
v3.3.3,get block start and block len for reduce scatter
v3.3.3,get buffer_write_start_pos_
v3.3.3,get buffer_read_start_pos_
v3.3.3,sync global data sumup info
v3.3.3,global sumup reduce
v3.3.3,copy back
v3.3.3,set global sumup info
v3.3.3,init global data count in leaf
v3.3.3,clear histogram buffer before synchronizing
v3.3.3,otherwise histogram contents from the previous iteration will be sent
v3.3.3,construct local histograms
v3.3.3,copy to buffer
v3.3.3,Reduce scatter for histogram
v3.3.3,restore global histograms from buffer
v3.3.3,only root leaf
v3.3.3,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.3.3,find local best split for larger leaf
v3.3.3,sync global best info
v3.3.3,set best split
v3.3.3,need update global number of data in leaf
v3.3.3,"instantiate template classes, otherwise linker cannot find the code"
v3.3.3,initialize SerialTreeLearner
v3.3.3,some additional variables needed for GPU trainer
v3.3.3,Initialize GPU buffers and kernels
v3.3.3,some functions used for debugging the GPU histogram construction
v3.3.3,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.3.3,goto err;
v3.3.3,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.3,also guarantee that there are at least 2K examples per workgroup
v3.3.3,return 0;
v3.3.3,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v3.3.3,decide the best number of workgroups working on one feature4 tuple
v3.3.3,set work group size based on feature size
v3.3.3,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.3,we need to refresh the kernel arguments after reallocating
v3.3.3,The only argument that needs to be changed later is num_data_
v3.3.3,"the GPU kernel will process all features in one call, and each"
v3.3.3,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.3.3,process one feature4 tuple
v3.3.3,"for the root node, indices are not copied"
v3.3.3,"for constant hessian, hessians are not copied except for the root node"
v3.3.3,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.3.3,and we will launch num_feature workgroups for this kernel
v3.3.3,will launch threads for all features
v3.3.3,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.3.3,copy the results asynchronously. Size depends on if double precision is used
v3.3.3,we will wait for this object in WaitAndGetHistograms
v3.3.3,"when the output is ready, the computation is done"
v3.3.3,values of this feature has been redistributed to multiple bins; need a reduction here
v3.3.3,how many feature-group tuples we have
v3.3.3,leave some safe margin for prefetching
v3.3.3,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.3,clear sparse/dense maps
v3.3.3,do nothing if no features can be processed on GPU
v3.3.3,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.3.3,unpin old buffer if necessary before destructing them
v3.3.3,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v3.3.3,allocate space for gradients and Hessians on device
v3.3.3,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.3,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.3.3,copy indices to the device
v3.3.3,histogram bin entry size depends on the precision (single/double)
v3.3.3,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.3,each work group generates a sub-histogram of dword_features_ features.
v3.3.3,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.3,create atomic counters for inter-group coordination
v3.3.3,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.3,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.3,looking for dword_features_ non-sparse feature-groups
v3.3.3,decide if we need to redistribute the bin
v3.3.3,multiplier must be a power of 2
v3.3.3,device_bin_mults_.push_back(1);
v3.3.3,found
v3.3.3,for data transfer time
v3.3.3,"Now generate new data structure feature4, and copy data to the device"
v3.3.3,"preallocate arrays for all threads, and pin them"
v3.3.3,building Feature4 bundles; each thread handles dword_features_ features
v3.3.3,one feature datapoint is 4 bits
v3.3.3,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.3,one feature datapoint is one byte
v3.3.3,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.3,Dense bin
v3.3.3,Dense 4-bit bin
v3.3.3,working on the remaining (less than dword_features_) feature groups
v3.3.3,fill the leftover features
v3.3.3,"fill this empty feature with some ""random"" value"
v3.3.3,"fill this empty feature with some ""random"" value"
v3.3.3,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.3.3,deallocate pinned space for feature copying
v3.3.3,data transfer time
v3.3.3,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.3.3,"Something bad happened. Just return ""No log available."""
v3.3.3,"build is okay, log may contain warnings"
v3.3.3,destroy any old kernels
v3.3.3,create OpenCL kernels for different number of workgroups per feature
v3.3.3,currently we don't use constant memory
v3.3.3,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.3.3,kernel with indices in an array
v3.3.3,"kernel with all features enabled, with eliminated branches"
v3.3.3,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.3.3,do nothing if no features can be processed on GPU
v3.3.3,The only argument that needs to be changed later is num_data_
v3.3.3,"hessian is passed as a parameter, but it is not available now."
v3.3.3,hessian will be set in BeforeTrain()
v3.3.3,"Get the max bin size, used for selecting best GPU kernel"
v3.3.3,initialize GPU
v3.3.3,determine which kernel to use based on the max number of bins
v3.3.3,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.3,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.3,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.3,setup GPU kernel arguments after we allocating all the buffers
v3.3.3,GPU memory has to been reallocated because data may have been changed
v3.3.3,setup GPU kernel arguments after we allocating all the buffers
v3.3.3,Copy initial full hessians and gradients to GPU.
v3.3.3,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.3,setup hessian parameters only
v3.3.3,hessian is passed as a parameter
v3.3.3,use bagging
v3.3.3,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v3.3.3,copy used gradients and Hessians to ordered buffer
v3.3.3,transfer the indices to GPU
v3.3.3,transfer hessian to GPU
v3.3.3,setup hessian parameters only
v3.3.3,hessian is passed as a parameter
v3.3.3,transfer gradients to GPU
v3.3.3,only have root
v3.3.3,"Copy indices, gradients and Hessians as early as possible"
v3.3.3,only need to initialize for smaller leaf
v3.3.3,Get leaf boundary
v3.3.3,copy indices to the GPU:
v3.3.3,copy ordered Hessians to the GPU:
v3.3.3,copy ordered gradients to the GPU:
v3.3.3,do nothing if no features can be processed on GPU
v3.3.3,copy data indices if it is not null
v3.3.3,generate and copy ordered_gradients if gradients is not null
v3.3.3,generate and copy ordered_hessians if Hessians is not null
v3.3.3,converted indices in is_feature_used to feature-group indices
v3.3.3,construct the feature masks for dense feature-groups
v3.3.3,"if no feature group is used, just return and do not use GPU"
v3.3.3,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.3,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.3,"All data have been prepared, now run the GPU kernel"
v3.3.3,construct smaller leaf
v3.3.3,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v3.3.3,then construct sparse features on CPU
v3.3.3,"wait for GPU to finish, only if GPU is actually used"
v3.3.3,use double precision
v3.3.3,use single precision
v3.3.3,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v3.3.3,#define GPU_DEBUG_COMPARE
v3.3.3,construct larger leaf
v3.3.3,then construct sparse features on CPU
v3.3.3,"wait for GPU to finish, only if GPU is actually used"
v3.3.3,use double precision
v3.3.3,use single precision
v3.3.3,do some sanity check for the GPU algorithm
v3.3.3,limit top k
v3.3.3,get max bin
v3.3.3,calculate buffer size
v3.3.3,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.3,"left and right on same time, so need double size"
v3.3.3,initialize histograms for global
v3.3.3,sync global data sumup info
v3.3.3,set global sumup info
v3.3.3,init global data count in leaf
v3.3.3,get local sumup
v3.3.3,get local sumup
v3.3.3,get mean number on machines
v3.3.3,weighted gain
v3.3.3,get top k
v3.3.3,"Copy histogram to buffer, and Get local aggregate features"
v3.3.3,copy histograms.
v3.3.3,copy smaller leaf histograms first
v3.3.3,mark local aggregated feature
v3.3.3,copy
v3.3.3,then copy larger leaf histograms
v3.3.3,mark local aggregated feature
v3.3.3,copy
v3.3.3,use local data to find local best splits
v3.3.3,clear histogram buffer before synchronizing
v3.3.3,otherwise histogram contents from the previous iteration will be sent
v3.3.3,find splits
v3.3.3,only has root leaf
v3.3.3,local voting
v3.3.3,gather
v3.3.3,get all top-k from all machines
v3.3.3,global voting
v3.3.3,copy local histgrams to buffer
v3.3.3,Reduce scatter for histogram
v3.3.3,find best split from local aggregated histograms
v3.3.3,restore from buffer
v3.3.3,restore from buffer
v3.3.3,find local best
v3.3.3,find local best split for larger leaf
v3.3.3,sync global best info
v3.3.3,copy back
v3.3.3,set the global number of data for leaves
v3.3.3,init the global sumup info
v3.3.3,"instantiate template classes, otherwise linker cannot find the code"
v3.3.3,launch cuda kernel
v3.3.3,initialize SerialTreeLearner
v3.3.3,some additional variables needed for GPU trainer
v3.3.3,Initialize GPU buffers and kernels: get device info
v3.3.3,some functions used for debugging the GPU histogram construction
v3.3.3,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.3,also guarantee that there are at least 2K examples per workgroup
v3.3.3,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.3.3,decide the best number of workgroups working on one feature4 tuple
v3.3.3,set work group size based on feature size
v3.3.3,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.3,set thread_data
v3.3.3,copy the results asynchronously. Size depends on if double precision is used
v3.3.3,"when the output is ready, the computation is done"
v3.3.3,how many feature-group tuples we have
v3.3.3,leave some safe margin for prefetching
v3.3.3,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.3,clear sparse/dense maps
v3.3.3,do nothing it there is no dense feature
v3.3.3,calculate number of feature groups per gpu
v3.3.3,histogram bin entry size depends on the precision (single/double)
v3.3.3,allocate GPU memory for each GPU
v3.3.3,do nothing it there is no gpu feature
v3.3.3,allocate memory for all features
v3.3.3,allocate space for gradients and hessians on device
v3.3.3,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.3,copy indices to the device
v3.3.3,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.3,each work group generates a sub-histogram of dword_features_ features.
v3.3.3,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.3,create atomic counters for inter-group coordination
v3.3.3,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.3,clear sparse/dense maps
v3.3.3,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.3,set device info
v3.3.3,looking for dword_features_ non-sparse feature-groups
v3.3.3,reset device info
v3.3.3,InitGPU w/ num_gpu
v3.3.3,"Get the max bin size, used for selecting best GPU kernel"
v3.3.3,get num_dense_feature_groups_
v3.3.3,initialize GPU
v3.3.3,set cpu threads
v3.3.3,resize device memory pointers
v3.3.3,create stream & events to handle multiple GPUs
v3.3.3,check data size
v3.3.3,GPU memory has to been reallocated because data may have been changed
v3.3.3,AllocateGPUMemory only when the number of data increased
v3.3.3,setup GPU kernel arguments after we allocating all the buffers
v3.3.3,Copy initial full hessians and gradients to GPU.
v3.3.3,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.3,use bagging
v3.3.3,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.3.3,copy used gradients and hessians to ordered buffer
v3.3.3,transfer the indices to GPU
v3.3.3,only have root
v3.3.3,"Copy indices, gradients and hessians as early as possible"
v3.3.3,only need to initialize for smaller leaf
v3.3.3,Get leaf boundary
v3.3.3,do nothing if no features can be processed on GPU
v3.3.3,copy data indices if it is not null
v3.3.3,converted indices in is_feature_used to feature-group indices
v3.3.3,construct the feature masks for dense feature-groups
v3.3.3,"if no feature group is used, just return and do not use GPU"
v3.3.3,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.3,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.3,We now copy even if all features are used.
v3.3.3,"All data have been prepared, now run the GPU kernel"
v3.3.3,construct smaller leaf
v3.3.3,Check workgroups per feature4 tuple..
v3.3.3,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.3.3,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.3.3,then construct sparse features on CPU
v3.3.3,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.3,"wait for GPU to finish, only if GPU is actually used"
v3.3.3,use double precision
v3.3.3,use single precision
v3.3.3,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.3.3,#define CUDA_DEBUG_COMPARE
v3.3.3,construct larger leaf
v3.3.3,then construct sparse features on CPU
v3.3.3,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.3,"wait for GPU to finish, only if GPU is actually used"
v3.3.3,use double precision
v3.3.3,use single precision
v3.3.3,do some sanity check for the GPU algorithm
v3.3.2,coding: utf-8
v3.3.2,coding: utf-8
v3.3.2,create predictor first
v3.3.2,"show deprecation warning only for early stop argument, setting early stop via global params should still be possible"
v3.3.2,check dataset
v3.3.2,reduce cost for prediction training data
v3.3.2,process callbacks
v3.3.2,Most of legacy advanced options becomes callbacks
v3.3.2,construct booster
v3.3.2,start training
v3.3.2,check evaluation result.
v3.3.2,"ranking task, split according to groups"
v3.3.2,run preprocessing on the data set if needed
v3.3.2,setup callbacks
v3.3.2,coding: utf-8
v3.3.2,dummy function to support older version of scikit-learn
v3.3.2,coding: utf-8
v3.3.2,documentation templates for LGBMModel methods are shared between the classes in
v3.3.2,this module and those in the ``dask`` module
v3.3.2,"user can set verbose with kwargs, it has higher priority"
v3.3.2,Do not modify original args in fit function
v3.3.2,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.3.2,Separate built-in from callable evaluation metrics
v3.3.2,register default metric for consistency with callable eval_metric case
v3.3.2,try to deduce from class instance
v3.3.2,overwrite default metric by explicitly set metric
v3.3.2,concatenate metric from params (or default if not provided in params) and eval_metric
v3.3.2,copy for consistency
v3.3.2,reduce cost for prediction training data
v3.3.2,free dataset
v3.3.2,Switch to using a multiclass objective in the underlying LGBM instance
v3.3.2,"do not modify args, as it causes errors in model selection tools"
v3.3.2,check group data
v3.3.2,coding: utf-8
v3.3.2,we don't need lib_lightgbm while building docs
v3.3.2,coding: utf-8
v3.3.2,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.3.2,coding: utf-8
v3.3.2,coding: utf-8
v3.3.2,TypeError: obj is not a string or a number
v3.3.2,ValueError: invalid literal
v3.3.2,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.3.2,avoid side effects on passed-in parameters
v3.3.2,"find a value, and remove other aliases with .pop()"
v3.3.2,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.3.2,Get total row number.
v3.3.2,Random access by row index. Used for data sampling.
v3.3.2,Range data access. Used to read data in batch when constructing Dataset.
v3.3.2,Optionally specify batch_size to control range data read size.
v3.3.2,Only required if using ``Dataset.subset()``.
v3.3.2,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.2,avoid memory consumption by arrays concatenation operations
v3.3.2,create numpy array from output arrays
v3.3.2,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.3.2,for CSC there is extra column added
v3.3.2,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.3.2,same shape as input csr or csc matrix except extra column for expected value
v3.3.2,note: make sure we copy data as it will be deallocated next
v3.3.2,"free the temporary native indptr, indices, and data"
v3.3.2,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.2,avoid memory consumption by arrays concatenation operations
v3.3.2,c type: double**
v3.3.2,each double* element points to start of each column of sample data.
v3.3.2,c type int**
v3.3.2,each int* points to start of indices for each column
v3.3.2,"no min_data, nthreads and verbose in this function"
v3.3.2,check data has header or not
v3.3.2,need to regroup init_score
v3.3.2,process for args
v3.3.2,"user can set verbose with params, it has higher priority"
v3.3.2,get categorical features
v3.3.2,process for reference dataset
v3.3.2,start construct data
v3.3.2,set feature names
v3.3.2,"Select sampled rows, transpose to column order."
v3.3.2,create validation dataset from ref_dataset
v3.3.2,create valid
v3.3.2,construct subset
v3.3.2,create train
v3.3.2,could be updated if data is not freed
v3.3.2,set to None
v3.3.2,we're done if self and reference share a common upstream reference
v3.3.2,"if buffer length is not long enough, reallocate buffers"
v3.3.2,"group data from LightGBM is boundaries data, need to convert to group size"
v3.3.2,"user can set verbose with params, it has higher priority"
v3.3.2,Training task
v3.3.2,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.3.2,construct booster object
v3.3.2,copy the parameters from train_set
v3.3.2,save reference to data
v3.3.2,buffer for inner predict
v3.3.2,Prediction task
v3.3.2,if a single node tree it won't have `leaf_index` so return 0
v3.3.2,"Create the node record, and populate universal data members"
v3.3.2,Update values to reflect node type (leaf or split)
v3.3.2,traverse the next level of the tree
v3.3.2,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.3.2,and we add node to the list.
v3.3.2,need reset training data
v3.3.2,need to push new valid data
v3.3.2,"if buffer length is not long enough, re-allocate a buffer"
v3.3.2,"if buffer length is not long enough, reallocate a buffer"
v3.3.2,Copy models
v3.3.2,Get name of features
v3.3.2,"if buffer length is not long enough, reallocate buffers"
v3.3.2,avoid to predict many time in one iteration
v3.3.2,Get num of inner evals
v3.3.2,Get name of eval metrics
v3.3.2,"if buffer length is not long enough, reallocate buffers"
v3.3.2,coding: utf-8
v3.3.2,Callback environment used by callbacks
v3.3.2,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.2,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.2,coding: utf-8
v3.3.2,Concatenate many parts into one
v3.3.2,construct local eval_set data.
v3.3.2,store indices of eval_set components that were not contained within local parts.
v3.3.2,consolidate parts of each individual eval component.
v3.3.2,require that eval_name exists in evaluated result data in case dropped due to padding.
v3.3.2,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v3.3.2,filter padding from eval parts then _concat each eval_set component.
v3.3.2,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v3.3.2,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v3.3.2,capture whether local_listen_port or its aliases were provided
v3.3.2,capture whether machines or its aliases were provided
v3.3.2,Some passed-in parameters can be removed:
v3.3.2,* 'num_machines': set automatically from Dask worker list
v3.3.2,* 'num_threads': overridden to match nthreads on each Dask process
v3.3.2,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.3.2,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v3.3.2,X and y are each delayed sub-lists of original eval dask Collections.
v3.3.2,find maximum number of parts in an individual eval set so that we can
v3.3.2,pad eval sets when they come in different sizes.
v3.3.2,"when individual eval set is equivalent to training data, skip recomputing parts."
v3.3.2,add None-padding for individual eval_set member if it is smaller than the largest member.
v3.3.2,first time a chunk of this eval set is added to this part.
v3.3.2,append additional chunks of this eval set to this part.
v3.3.2,ensure that all evaluation parts map uniquely to one part.
v3.3.2,assign sub-eval_set components to worker parts.
v3.3.2,Start computation in the background
v3.3.2,Find locations of all parts and map them to particular Dask workers
v3.3.2,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v3.3.2,data artifacts may not be populated depending on worker returning final estimator.
v3.3.2,assign general validation set settings to fit kwargs.
v3.3.2,resolve aliases for network parameters and pop the result off params.
v3.3.2,these values are added back in calls to `_train_part()`
v3.3.2,figure out network params
v3.3.2,Tell each worker to train on the parts that it has locally
v3.3.2,
v3.3.2,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v3.3.2,1. there is randomness in the training process unless parameters ``seed``
v3.3.2,and ``deterministic`` are set
v3.3.2,"2. even with those parameters set, the output of one ``_train_part()`` call"
v3.3.2,relies on global state (it and all the other LightGBM training processes
v3.3.2,coordinate with each other)
v3.3.2,"if network parameters were changed during training, remove them from the"
v3.3.2,returned model so that they're generated dynamically on every run based
v3.3.2,on the Dask cluster you're connected to and which workers have pieces of
v3.3.2,the training data
v3.3.2,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.3.2,"for multi-class classification with sparse matrices, pred_contrib predictions"
v3.3.2,are returned as a list of sparse matrices (one per class)
v3.3.2,"pred_contrib output will have one column per feature,"
v3.3.2,plus one more for the base value
v3.3.2,need to tell Dask the expected type and shape of individual preds
v3.3.2,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v3.3.2,the code below is used instead to ensure that the sparse type is preserved during concatentation
v3.3.2,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v3.3.2,Concatenate them to return a list of Dask Arrays.
v3.3.2,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.2,currently relevant for the Dask estimators
v3.3.2,"DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds."
v3.3.2,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.3.2,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.2,currently relevant for the Dask estimators
v3.3.2,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds."
v3.3.2,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.3.2,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.2,currently relevant for the Dask estimators
v3.3.2,DaskLGBMRanker does not support eval_class_weight or early stopping
v3.3.2,DaskLGBMRanker support for callbacks and init_model is not tested
v3.3.2,coding: utf-8
v3.3.2,load or create your dataset
v3.3.2,create dataset for lightgbm
v3.3.2,"if you want to re-use data, remember to set free_raw_data=False"
v3.3.2,specify your configurations as a dict
v3.3.2,generate feature names
v3.3.2,feature_name and categorical_feature
v3.3.2,check feature name
v3.3.2,save model to file
v3.3.2,dump model to JSON (and save to file)
v3.3.2,feature names
v3.3.2,feature importances
v3.3.2,load model to predict
v3.3.2,can only predict with the best iteration (or the saving iteration)
v3.3.2,eval with loaded model
v3.3.2,dump model with pickle
v3.3.2,load model with pickle to predict
v3.3.2,can predict with any iteration when loaded in pickle way
v3.3.2,eval with loaded model
v3.3.2,continue training
v3.3.2,init_model accepts:
v3.3.2,1. model file name
v3.3.2,2. Booster()
v3.3.2,decay learning rates
v3.3.2,learning_rates accepts:
v3.3.2,1. list/tuple with length = num_boost_round
v3.3.2,2. function(curr_iter)
v3.3.2,change other parameters during training
v3.3.2,self-defined objective function
v3.3.2,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.3.2,log likelihood loss
v3.3.2,self-defined eval metric
v3.3.2,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.2,binary error
v3.3.2,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.2,This may make built-in evaluation metric calculate wrong results
v3.3.2,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.2,Keep this in mind when you use the customization
v3.3.2,another self-defined eval metric
v3.3.2,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.2,accuracy
v3.3.2,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.2,This may make built-in evaluation metric calculate wrong results
v3.3.2,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.2,Keep this in mind when you use the customization
v3.3.2,callback
v3.3.2,coding: utf-8
v3.3.2,load or create your dataset
v3.3.2,train
v3.3.2,predict
v3.3.2,eval
v3.3.2,feature importances
v3.3.2,self-defined eval metric
v3.3.2,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.2,Root Mean Squared Logarithmic Error (RMSLE)
v3.3.2,train
v3.3.2,another self-defined eval metric
v3.3.2,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.2,Relative Absolute Error (RAE)
v3.3.2,train
v3.3.2,predict
v3.3.2,eval
v3.3.2,other scikit-learn modules
v3.3.2,coding: utf-8
v3.3.2,load or create your dataset
v3.3.2,create dataset for lightgbm
v3.3.2,specify your configurations as a dict
v3.3.2,train
v3.3.2,coding: utf-8
v3.3.2,################
v3.3.2,Simulate some binary data with a single categorical and
v3.3.2,single continuous predictor
v3.3.2,################
v3.3.2,Set up a couple of utilities for our experiments
v3.3.2,################
v3.3.2,Observe the behavior of `binary` and `xentropy` objectives
v3.3.2,Trying this throws an error on non-binary values of y:
v3.3.2,"experiment('binary', label_type='probability', DATA)"
v3.3.2,The speed of `binary` is not drastically different than
v3.3.2,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.3.2,there are reasons to suspect that `binary` should run faster when the
v3.3.2,label is an integer instead of a float
v3.3.2,coding: utf-8
v3.3.2,load or create your dataset
v3.3.2,create dataset for lightgbm
v3.3.2,specify your configurations as a dict
v3.3.2,train
v3.3.2,save model to file
v3.3.2,predict
v3.3.2,eval
v3.3.2,We can also open HDF5 file once and get access to
v3.3.2,"With binary dataset created, we can use either Python API or cmdline version to train."
v3.3.2,
v3.3.2,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v3.3.2,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v3.3.2,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v3.3.2,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v3.3.2,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v3.3.2,Y has a single column and we read it in single shot. So store it as an 1-d array.
v3.3.2,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v3.3.2,"When accessing any element in a HDF5 chunk, it's read entirely."
v3.3.2,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v3.3.2,Here we are just creating a chunk size that matches with batch_size.
v3.3.2,
v3.3.2,Also note that the data is stored in row major order to avoid extra copy when passing to
v3.3.2,lightgbm Dataset.
v3.3.2,Save to 2 HDF5 files for demonstration.
v3.3.2,We can store multiple datasets inside a single HDF5 file.
v3.3.2,Separating X and Y for choosing best chunk size for data loading.
v3.3.2,split training data into two partitions
v3.3.2,make this array dense because we're splitting across
v3.3.2,a sparse boundary to partition the data
v3.3.2,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.3.2,predictions and target values back from workers to the client
v3.3.2,
v3.3.2,"for larger datasets, consider the metrics from dask-ml instead"
v3.3.2,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.3.2,coding: utf-8
v3.3.2,!/usr/bin/env python3
v3.3.2,-*- coding: utf-8 -*-
v3.3.2,
v3.3.2,"LightGBM documentation build configuration file, created by"
v3.3.2,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.3.2,
v3.3.2,This file is execfile()d with the current directory set to its
v3.3.2,containing dir.
v3.3.2,
v3.3.2,Note that not all possible configuration values are present in this
v3.3.2,autogenerated file.
v3.3.2,
v3.3.2,All configuration values have a default; values that are commented out
v3.3.2,serve to show the default.
v3.3.2,"If extensions (or modules to document with autodoc) are in another directory,"
v3.3.2,add these directories to sys.path here. If the directory is relative to the
v3.3.2,"documentation root, use os.path.abspath to make it absolute."
v3.3.2,-- mock out modules
v3.3.2,-- General configuration ------------------------------------------------
v3.3.2,"If your documentation needs a minimal Sphinx version, state it here."
v3.3.2,"Add any Sphinx extension module names here, as strings. They can be"
v3.3.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.3.2,ones.
v3.3.2,hide type hints in API docs
v3.3.2,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.3.2,Only the class' docstring is inserted.
v3.3.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.3.2,The master toctree document.
v3.3.2,General information about the project.
v3.3.2,The name of an image file (relative to this directory) to place at the top
v3.3.2,of the sidebar.
v3.3.2,The name of an image file (relative to this directory) to use as a favicon of
v3.3.2,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.3.2,pixels large.
v3.3.2,"The version info for the project you're documenting, acts as replacement for"
v3.3.2,"|version| and |release|, also used in various other places throughout the"
v3.3.2,built documents.
v3.3.2,The short X.Y version.
v3.3.2,"The full version, including alpha/beta/rc tags."
v3.3.2,The language for content autogenerated by Sphinx. Refer to documentation
v3.3.2,for a list of supported languages.
v3.3.2,
v3.3.2,This is also used if you do content translation via gettext catalogs.
v3.3.2,"Usually you set ""language"" from the command line for these cases."
v3.3.2,"List of patterns, relative to source directory, that match files and"
v3.3.2,directories to ignore when looking for source files.
v3.3.2,This patterns also effect to html_static_path and html_extra_path
v3.3.2,The name of the Pygments (syntax highlighting) style to use.
v3.3.2,-- Configuration for C API docs generation ------------------------------
v3.3.2,-- Options for HTML output ----------------------------------------------
v3.3.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.3.2,a list of builtin themes.
v3.3.2,Theme options are theme-specific and customize the look and feel of a theme
v3.3.2,"further.  For a list of options available for each theme, see the"
v3.3.2,documentation.
v3.3.2,"Add any paths that contain custom static files (such as style sheets) here,"
v3.3.2,"relative to this directory. They are copied after the builtin static files,"
v3.3.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.3.2,-- Options for HTMLHelp output ------------------------------------------
v3.3.2,Output file base name for HTML help builder.
v3.3.2,-- Options for LaTeX output ---------------------------------------------
v3.3.2,The name of an image file (relative to this directory) to place at the top of
v3.3.2,the title page.
v3.3.2,Warning! The following code can cause buffer overflows on RTD.
v3.3.2,Consider suppressing output completely if RTD project silently fails.
v3.3.2,Refer to https://github.com/svenevs/exhale
v3.3.2,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.2,Warning! The following code can cause buffer overflows on RTD.
v3.3.2,Consider suppressing output completely if RTD project silently fails.
v3.3.2,Refer to https://github.com/svenevs/exhale
v3.3.2,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.2,coding: utf-8
v3.3.2,This is a basic test for floating number parsing.
v3.3.2,Most of the test cases come from:
v3.3.2,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v3.3.2,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v3.3.2,FLT_MAX
v3.3.2,FLT_MIN
v3.3.2,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v3.3.2,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v3.3.2,Near DBL_MIN
v3.3.2,DBL_MIN 2^-1022
v3.3.2,The behavior for parsing -nan depends on implementation.
v3.3.2,Thus we skip binary check for negative nan.
v3.3.2,See comment in test_cases.
v3.3.2,Constants
v3.3.2,Start with some content:
v3.3.2,Clear & re-use:
v3.3.2,Output should match new content:
v3.3.2,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.3.2,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.3.2,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.3.2,Compute a new trial address & value & if it is a valid address:
v3.3.2,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.3.2,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.3.2,Check the just-stored value with getitem():
v3.3.2,Also store the just-stored value for future tracking:
v3.3.2,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.3.2,Test in 2 ways that the values are correctly laid out in memory:
v3.3.2,coding: utf-8
v3.3.2,we don't need lib_lightgbm while building docs
v3.3.2,coding: utf-8
v3.3.2,check saved model persistence
v3.3.2,"we need to check the consistency of model file here, so test for exact equal"
v3.3.2,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.3.2,"scores likely to be different, but prediction should still be the same"
v3.3.2,test that shape is checked during prediction
v3.3.2,"The simple implementation is just a single ""return self.ndarray[idx]"""
v3.3.2,The following is for demo and testing purpose.
v3.3.2,whole col
v3.3.2,half col
v3.3.2,Create dataset from numpy array directly.
v3.3.2,Create dataset using Sequence.
v3.3.2,Test for validation set.
v3.3.2,Select some random rows as valid data.
v3.3.2,"From Dataset constructor, with dataset from numpy array."
v3.3.2,"From Dataset.create_valid, with dataset from sequence."
v3.3.2,test that method works even with free_raw_data=True
v3.3.2,test that method works but sets raw data to None in case of immergeable data types
v3.3.2,test that method works for different data types
v3.3.2,"Set extremely harsh penalties, so CEGB will block most splits."
v3.3.2,"Compare pairs of penalties, to ensure scaling works as intended"
v3.3.2,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.3.2,"should resolve duplicate aliases, and prefer the main parameter"
v3.3.2,should choose a value from an alias and set that value on main param
v3.3.2,if only an alias is used
v3.3.2,should use the default if main param and aliases are missing
v3.3.2,all changes should be made on copies and not modify the original
v3.3.2,coding: utf-8
v3.3.2,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.3.2,set_index ensures partitions are based on group id.
v3.3.2,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.3.2,"separate target, weight from features."
v3.3.2,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.3.2,"so that within each partition, sum(g) = n_samples."
v3.3.2,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.3.2,make one categorical feature relevant to the target
v3.3.2,https://github.com/microsoft/LightGBM/issues/4118
v3.3.2,extra predict() parameters should be passed through correctly
v3.3.2,pref_leaf values should have the right shape
v3.3.2,and values that look like valid tree nodes
v3.3.2,"be sure LightGBM actually used at least one categorical column,"
v3.3.2,and that it was correctly treated as a categorical feature
v3.3.2,shape depends on whether it is binary or multiclass classification
v3.3.2,"in the special case of multi-class classification using scipy sparse matrices,"
v3.3.2,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v3.3.2,
v3.3.2,"since that case is so different than all other cases, check the relevant things here"
v3.3.2,and then return early
v3.3.2,"raw scores will probably be different, but at least check that all predicted classes are the same"
v3.3.2,"be sure LightGBM actually used at least one categorical column,"
v3.3.2,and that it was correctly treated as a categorical feature
v3.3.2,* shape depends on whether it is binary or multiclass classification
v3.3.2,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.3.2,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.3.2,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.2,that the output has the right shape and base values are in the right position
v3.3.2,check that found ports are different for same address (LocalCluster)
v3.3.2,check that the ports are indeed open
v3.3.2,Scores should be the same
v3.3.2,Predictions should be roughly the same.
v3.3.2,pref_leaf values should have the right shape
v3.3.2,and values that look like valid tree nodes
v3.3.2,extra predict() parameters should be passed through correctly
v3.3.2,"be sure LightGBM actually used at least one categorical column,"
v3.3.2,and that it was correctly treated as a categorical feature
v3.3.2,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.2,that the output has the right shape and base values are in the right position
v3.3.2,"be sure LightGBM actually used at least one categorical column,"
v3.3.2,and that it was correctly treated as a categorical feature
v3.3.2,Quantiles should be right
v3.3.2,"be sure LightGBM actually used at least one categorical column,"
v3.3.2,and that it was correctly treated as a categorical feature
v3.3.2,rebalance small dask.Array dataset for better performance.
v3.3.2,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.3.2,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.3.2,distributed ranker should be able to rank decently well and should
v3.3.2,have high rank correlation with scores from serial ranker.
v3.3.2,extra predict() parameters should be passed through correctly
v3.3.2,pref_leaf values should have the right shape
v3.3.2,and values that look like valid tree nodes
v3.3.2,"be sure LightGBM actually used at least one categorical column,"
v3.3.2,and that it was correctly treated as a categorical feature
v3.3.2,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v3.3.2,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v3.3.2,"test eval_class_weight, eval_init_score on binary-classification task."
v3.3.2,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v3.3.2,create eval_sets by creating new datasets or copying training data.
v3.3.2,total number of trees scales up for ova classifier.
v3.3.2,check that early stopping was not applied.
v3.3.2,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v3.3.2,"check that each eval_name and metric exists for all eval sets, allowing for the"
v3.3.2,case when a worker receives a fully-padded eval_set component which is not evaluated.
v3.3.2,should be able to use the class without specifying a client
v3.3.2,should be able to set client after construction
v3.3.2,data on cluster1
v3.3.2,create identical data on cluster2
v3.3.2,"at this point, the result of default_client() is client2 since it was the most recently"
v3.3.2,created. So setting client to client1 here to test that you can select a non-default client
v3.3.2,"unfitted model should survive pickling round trip, and pickling"
v3.3.2,shouldn't have side effects on the model object
v3.3.2,client will always be None after unpickling
v3.3.2,"fitted model should survive pickling round trip, and pickling"
v3.3.2,shouldn't have side effects on the model object
v3.3.2,client will always be None after unpickling
v3.3.2,rebalance data to be sure that each worker has a piece of the data
v3.3.2,model 1 - no network parameters given
v3.3.2,model 2 - machines given
v3.3.2,model 3 - local_listen_port given
v3.3.2,training should fail because LightGBM will try to use the same
v3.3.2,port for multiple worker processes on the same machine
v3.3.2,rebalance data to be sure that each worker has a piece of the data
v3.3.2,"test that ""machines"" is actually respected by creating a socket that uses"
v3.3.2,"one of the ports mentioned in ""machines"""
v3.3.2,The above error leaves a worker waiting
v3.3.2,"an informative error should be raised if ""machines"" has duplicates"
v3.3.2,"""client"" should be the only different, and the final argument"
v3.3.2,value of the root node is 0 when init_score is set
v3.3.2,this test is separate because it takes a not-yet-constructed estimator
v3.3.2,coding: utf-8
v3.3.2,coding: utf-8
v3.3.2,"build target, group ID vectors."
v3.3.2,build y/target and group-id vectors with user-specified group sizes.
v3.3.2,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.3.2,groups should contain > 1 element for pairwise learning objective.
v3.3.2,"build feature data, X. Transform first few into informative features."
v3.3.2,coding: utf-8
v3.3.2,prediction result is actually not transformed (is raw) due to custom objective
v3.3.2,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.3.2,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.3.2,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.2,sklearn < 0.23 does not have as_frame parameter
v3.3.2,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.2,sklearn < 0.23 does not have as_frame parameter
v3.3.2,Test if random_state is properly stored
v3.3.2,Test if two random states produce identical models
v3.3.2,Test if subsequent fits sample from random_state object and produce different models
v3.3.2,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.3.2,With default params
v3.3.2,Tests same probabilities
v3.3.2,Tests same predictions
v3.3.2,Tests same raw scores
v3.3.2,Tests same leaf indices
v3.3.2,Tests same feature contributions
v3.3.2,Tests other parameters for the prediction works
v3.3.2,Tests start_iteration
v3.3.2,"Tests same probabilities, starting from iteration 10"
v3.3.2,"Tests same predictions, starting from iteration 10"
v3.3.2,"Tests same raw scores, starting from iteration 10"
v3.3.2,"Tests same leaf indices, starting from iteration 10"
v3.3.2,"Tests same feature contributions, starting from iteration 10"
v3.3.2,"Tests other parameters for the prediction works, starting from iteration 10"
v3.3.2,"no custom objective, no custom metric"
v3.3.2,default metric
v3.3.2,non-default metric
v3.3.2,no metric
v3.3.2,non-default metric in eval_metric
v3.3.2,non-default metric with non-default metric in eval_metric
v3.3.2,non-default metric with multiple metrics in eval_metric
v3.3.2,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.3.2,default metric for non-default objective
v3.3.2,non-default metric for non-default objective
v3.3.2,no metric
v3.3.2,non-default metric in eval_metric for non-default objective
v3.3.2,non-default metric with non-default metric in eval_metric for non-default objective
v3.3.2,non-default metric with multiple metrics in eval_metric for non-default objective
v3.3.2,"custom objective, no custom metric"
v3.3.2,default regression metric for custom objective
v3.3.2,non-default regression metric for custom objective
v3.3.2,multiple regression metrics for custom objective
v3.3.2,no metric
v3.3.2,default regression metric with non-default metric in eval_metric for custom objective
v3.3.2,non-default regression metric with metric in eval_metric for custom objective
v3.3.2,multiple regression metrics with metric in eval_metric for custom objective
v3.3.2,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.3.2,"no custom objective, custom metric"
v3.3.2,default metric with custom metric
v3.3.2,non-default metric with custom metric
v3.3.2,multiple metrics with custom metric
v3.3.2,custom metric (disable default metric)
v3.3.2,default metric for non-default objective with custom metric
v3.3.2,non-default metric for non-default objective with custom metric
v3.3.2,multiple metrics for non-default objective with custom metric
v3.3.2,custom metric (disable default metric for non-default objective)
v3.3.2,"custom objective, custom metric"
v3.3.2,custom metric for custom objective
v3.3.2,non-default regression metric with custom metric for custom objective
v3.3.2,multiple regression metrics with custom metric for custom objective
v3.3.2,default metric and invalid binary metric is replaced with multiclass alternative
v3.3.2,invalid objective is replaced with default multiclass one
v3.3.2,and invalid binary metric is replaced with multiclass alternative
v3.3.2,default metric for non-default multiclass objective
v3.3.2,and invalid binary metric is replaced with multiclass alternative
v3.3.2,default metric and invalid multiclass metric is replaced with binary alternative
v3.3.2,invalid multiclass metric is replaced with binary alternative for custom objective
v3.3.2,"Verify that can receive a list of metrics, only callable"
v3.3.2,Verify that can receive a list of custom and built-in metrics
v3.3.2,Verify that works as expected when eval_metric is empty
v3.3.2,"Verify that can receive a list of metrics, only built-in"
v3.3.2,Verify that eval_metric is robust to receiving a list with None
v3.3.2,training data as eval_set
v3.3.2,feval
v3.3.2,single eval_set
v3.3.2,two eval_set
v3.3.2,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.3.2,Test that estimators are default-constructible
v3.3.2,coding: utf-8
v3.3.2,coding: utf-8
v3.3.2,check that default gives same result as k = 1
v3.3.2,check against independent calculation for k = 1
v3.3.2,check against independent calculation for k = 2
v3.3.2,check against independent calculation for k = 10
v3.3.2,check cases where predictions are equal
v3.3.2,should give same result as binary auc for 2 classes
v3.3.2,test the case where all predictions are equal
v3.3.2,test that weighted data gives different auc_mu
v3.3.2,test that equal data weights give same auc_mu as unweighted data
v3.3.2,should give 1 when accuracy = 1
v3.3.2,test loading class weights
v3.3.2,no early stopping
v3.3.2,early stopping occurs
v3.3.2,test custom eval metrics
v3.3.2,"shuffle = False, override metric in params"
v3.3.2,"shuffle = True, callbacks"
v3.3.2,enable display training loss
v3.3.2,self defined folds
v3.3.2,LambdaRank
v3.3.2,... with l2 metric
v3.3.2,... with NDCG (default) metric
v3.3.2,self defined folds with lambdarank
v3.3.2,with early stopping
v3.3.2,predict by each fold booster
v3.3.2,fold averaging
v3.3.2,without early stopping
v3.3.2,test feature_names with whitespaces
v3.3.2,This has non-ascii strings.
v3.3.2,take subsets and train
v3.3.2,generate CSR sparse dataset
v3.3.2,convert data to dense and get back same contribs
v3.3.2,validate the values are the same
v3.3.2,validate using CSC matrix
v3.3.2,validate the values are the same
v3.3.2,generate CSR sparse dataset
v3.3.2,convert data to dense and get back same contribs
v3.3.2,validate the values are the same
v3.3.2,validate using CSC matrix
v3.3.2,validate the values are the same
v3.3.2,Note there is an extra column added to the output for the expected value
v3.3.2,Note output CSC shape should be same as CSR output shape
v3.3.2,test sliced labels
v3.3.2,append some columns
v3.3.2,append some rows
v3.3.2,test sliced 2d matrix
v3.3.2,test sliced CSR
v3.3.2,trees start at position 1.
v3.3.2,split_features are in 4th line.
v3.3.2,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.3.2,The penalization is so high that the first 2 features should not be used here
v3.3.2,Check that a very high penalization is the same as not using the features at all
v3.3.2,"no fobj, no feval"
v3.3.2,default metric
v3.3.2,non-default metric in params
v3.3.2,default metric in args
v3.3.2,non-default metric in args
v3.3.2,metric in args overwrites one in params
v3.3.2,multiple metrics in params
v3.3.2,multiple metrics in args
v3.3.2,remove default metric by 'None' in list
v3.3.2,remove default metric by 'None' aliases
v3.3.2,"fobj, no feval"
v3.3.2,no default metric
v3.3.2,metric in params
v3.3.2,metric in args
v3.3.2,metric in args overwrites its' alias in params
v3.3.2,multiple metrics in params
v3.3.2,multiple metrics in args
v3.3.2,"no fobj, feval"
v3.3.2,default metric with custom one
v3.3.2,non-default metric in params with custom one
v3.3.2,default metric in args with custom one
v3.3.2,non-default metric in args with custom one
v3.3.2,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.2,multiple metrics in params with custom one
v3.3.2,multiple metrics in args with custom one
v3.3.2,custom metric is evaluated despite 'None' is passed
v3.3.2,"fobj, feval"
v3.3.2,"no default metric, only custom one"
v3.3.2,metric in params with custom one
v3.3.2,metric in args with custom one
v3.3.2,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.2,multiple metrics in params with custom one
v3.3.2,multiple metrics in args with custom one
v3.3.2,custom metric is evaluated despite 'None' is passed
v3.3.2,"no fobj, no feval"
v3.3.2,default metric
v3.3.2,default metric in params
v3.3.2,non-default metric in params
v3.3.2,multiple metrics in params
v3.3.2,remove default metric by 'None' aliases
v3.3.2,"fobj, no feval"
v3.3.2,no default metric
v3.3.2,metric in params
v3.3.2,multiple metrics in params
v3.3.2,"no fobj, feval"
v3.3.2,default metric with custom one
v3.3.2,default metric in params with custom one
v3.3.2,non-default metric in params with custom one
v3.3.2,multiple metrics in params with custom one
v3.3.2,custom metric is evaluated despite 'None' is passed
v3.3.2,"fobj, feval"
v3.3.2,"no default metric, only custom one"
v3.3.2,metric in params with custom one
v3.3.2,multiple metrics in params with custom one
v3.3.2,custom metric is evaluated despite 'None' is passed
v3.3.2,multiclass default metric
v3.3.2,multiclass default metric with custom one
v3.3.2,multiclass metric alias with custom one for custom objective
v3.3.2,no metric for invalid class_num
v3.3.2,custom metric for invalid class_num
v3.3.2,multiclass metric alias with custom one with invalid class_num
v3.3.2,multiclass default metric without num_class
v3.3.2,multiclass metric alias
v3.3.2,multiclass metric
v3.3.2,non-valid metric for multiclass objective
v3.3.2,non-default num_class for default objective
v3.3.2,no metric with non-default num_class for custom objective
v3.3.2,multiclass metric alias for custom objective
v3.3.2,multiclass metric for custom objective
v3.3.2,binary metric with non-default num_class for custom objective
v3.3.2,Expect three metrics but mean and stdv for each metric
v3.3.2,test XGBoost-style return value
v3.3.2,test numpy-style return value
v3.3.2,test bins string type
v3.3.2,test histogram is disabled for categorical features
v3.3.2,test for lgb.train
v3.3.2,test feval for lgb.train
v3.3.2,test with two valid data for lgb.train
v3.3.2,test for lgb.cv
v3.3.2,test feval for lgb.cv
v3.3.2,test that binning works properly for features with only positive or only negative values
v3.3.2,decreasing without freeing raw data is allowed
v3.3.2,decreasing before lazy init is allowed
v3.3.2,increasing is allowed
v3.3.2,decreasing with disabled filter is allowed
v3.3.2,decreasing with enabled filter is disallowed;
v3.3.2,also changes of other params are disallowed
v3.3.2,check extra trees increases regularization
v3.3.2,check path smoothing increases regularization
v3.3.2,test edge case with one leaf
v3.3.2,check that constraint containing all features is equivalent to no constraint
v3.3.2,check that constraint partitioning the features reduces train accuracy
v3.3.2,check that constraints consisting of single features reduce accuracy further
v3.3.2,test that interaction constraints work when not all features are used
v3.3.2,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.3.2,test again with nans in data
v3.3.2,test again with bagging
v3.3.2,test with a feature that has only one non-nan value
v3.3.2,test with a categorical feature
v3.3.2,test refit: same results on same data
v3.3.2,test refit with save and load
v3.3.2,test refit: different results training on different data
v3.3.2,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.3.2,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.3.2,"test the case where start_iteration <= 0, and num_iteration is None"
v3.3.2,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.3.2,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.3.2,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.3.2,test for regression
v3.3.2,test both with and without early stopping
v3.3.2,test for multi-class
v3.3.2,test both with and without early stopping
v3.3.2,test for binary
v3.3.2,test both with and without early stopping
v3.3.2,test against sklearn average precision metric
v3.3.2,test that average precision is 1 where model predicts perfectly
v3.3.2,coding: utf-8
v3.3.2,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.3.2,coding: utf-8
v3.3.2,coding: utf-8
v3.3.2,These are helper functions to allow doing a stack unwind
v3.3.2,"after an R allocation error, which would trigger a long jump."
v3.3.2,convert from one-based to zero-based index
v3.3.2,"if any feature names were larger than allocated size,"
v3.3.2,allow for a larger size and try again
v3.3.2,convert from boundaries to size
v3.3.2,--- start Booster interfaces
v3.3.2,"if any eval names were larger than allocated size,"
v3.3.2,allow for a larger size and try again
v3.3.2,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.2,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.2,.Call() calls
v3.3.2,coding: utf-8
v3.3.2,alias table
v3.3.2,names
v3.3.2,from strings
v3.3.2,tails
v3.3.2,tails
v3.3.2,coding: utf-8
v3.3.2,Single row predictor to abstract away caching logic
v3.3.2,create boosting
v3.3.2,initialize the boosting
v3.3.2,create objective function
v3.3.2,initialize the objective function
v3.3.2,create training metric
v3.3.2,reset the boosting
v3.3.2,create objective function
v3.3.2,initialize the objective function
v3.3.2,calculate the nonzero data and indices size
v3.3.2,allocate data and indices arrays
v3.3.2,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.2,aggregated per row feature contribution results
v3.3.2,keep track of the row_vector sizes for parallelization
v3.3.2,copy vector results to output for each row
v3.3.2,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.2,aggregated per row feature contribution results
v3.3.2,calculate number of elements per column to construct
v3.3.2,the CSC matrix with random access
v3.3.2,keep track of column counts
v3.3.2,keep track of beginning index for each column
v3.3.2,keep track of beginning index for each matrix
v3.3.2,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.3.2,store the row index
v3.3.2,update column count
v3.3.2,explicitly declare symbols from LightGBM namespace
v3.3.2,some help functions used to convert data
v3.3.2,Row iterator of on column for CSC matrix
v3.3.2,"return value at idx, only can access by ascent order"
v3.3.2,"return next non-zero pair, if index < 0, means no more data"
v3.3.2,start of c_api functions
v3.3.2,This API is to keep python binding's behavior the same with C++ implementation.
v3.3.2,"Sample count, random seed etc. should be provided in parameters."
v3.3.2,sample data first
v3.3.2,sample data first
v3.3.2,sample data first
v3.3.2,local buffer to re-use memory
v3.3.2,sample data first
v3.3.2,no more data
v3.3.2,---- start of booster
v3.3.2,Single row in row-major format:
v3.3.2,---- start of some help functions
v3.3.2,data is array of pointers to individual rows
v3.3.2,set number of threads for openmp
v3.3.2,check for alias
v3.3.2,read parameters from config file
v3.3.2,"remove str after ""#"""
v3.3.2,check for alias again
v3.3.2,load configs
v3.3.2,prediction is needed if using input initial model(continued train)
v3.3.2,need to continue training
v3.3.2,sync up random seed for data partition
v3.3.2,load Training data
v3.3.2,load data for distributed training
v3.3.2,load data for single machine
v3.3.2,need save binary file
v3.3.2,create training metric
v3.3.2,only when have metrics then need to construct validation data
v3.3.2,"Add validation data, if it exists"
v3.3.2,add
v3.3.2,need save binary file
v3.3.2,add metric for validation data
v3.3.2,output used time on each iteration
v3.3.2,need init network
v3.3.2,create boosting
v3.3.2,create objective function
v3.3.2,load training data
v3.3.2,initialize the objective function
v3.3.2,initialize the boosting
v3.3.2,add validation data into boosting
v3.3.2,convert model to if-else statement code
v3.3.2,create predictor
v3.3.2,Free memory
v3.3.2,create predictor
v3.3.2,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.3.2,counts for all labels
v3.3.2,"start from top label, and accumulate DCG"
v3.3.2,counts for all labels
v3.3.2,calculate k Max DCG by one pass
v3.3.2,get sorted indices by score
v3.3.2,calculate multi dcg by one pass
v3.3.2,wait for all client start up
v3.3.2,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.3.2,Instead we will handle finalize/abort for MPI in main().
v3.3.2,default set to -1
v3.3.2,"distance at k-th communication, distance[k] = 2^k"
v3.3.2,set incoming rank at k-th commuication
v3.3.2,set outgoing rank at k-th commuication
v3.3.2,default set as -1
v3.3.2,construct all recursive halving map for all machines
v3.3.2,let 1 << k <= num_machines
v3.3.2,distance of each communication
v3.3.2,"if num_machines = 2^k, don't need to group machines"
v3.3.2,"communication direction, %2 == 0 is positive"
v3.3.2,neighbor at k-th communication
v3.3.2,receive data block at k-th communication
v3.3.2,send data block at k-th communication
v3.3.2,"if num_machines != 2^k, need to group machines"
v3.3.2,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.3.2,let left machine as group leader
v3.3.2,"cache block information for groups, group with 2 machines will have double block size"
v3.3.2,convert from group to node leader
v3.3.2,convert from node to group
v3.3.2,meet new group
v3.3.2,add block len for this group
v3.3.2,calculate the group block start
v3.3.2,not need to construct
v3.3.2,get receive block information
v3.3.2,accumulate block len
v3.3.2,get send block information
v3.3.2,accumulate block len
v3.3.2,static member definition
v3.3.2,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.3.2,assign the blocks to every rank.
v3.3.2,do reduce scatter
v3.3.2,do all gather
v3.3.2,assign blocks
v3.3.2,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.3.2,copy back
v3.3.2,assign blocks
v3.3.2,start all gather
v3.3.2,when num_machines is small and data is large
v3.3.2,use output as receive buffer
v3.3.2,get current local block size
v3.3.2,get out rank
v3.3.2,get in rank
v3.3.2,get send information
v3.3.2,get recv information
v3.3.2,send and recv at same time
v3.3.2,rotate in-place
v3.3.2,use output as receive buffer
v3.3.2,get current local block size
v3.3.2,get send information
v3.3.2,get recv information
v3.3.2,send and recv at same time
v3.3.2,use output as receive buffer
v3.3.2,send and recv at same time
v3.3.2,send local data to neighbor first
v3.3.2,receive neighbor data first
v3.3.2,reduce
v3.3.2,get target
v3.3.2,get send information
v3.3.2,get recv information
v3.3.2,send and recv at same time
v3.3.2,reduce
v3.3.2,send result to neighbor
v3.3.2,receive result from neighbor
v3.3.2,copy result
v3.3.2,start up socket
v3.3.2,parse clients from file
v3.3.2,get ip list of local machine
v3.3.2,get local rank
v3.3.2,construct listener
v3.3.2,construct communication topo
v3.3.2,construct linkers
v3.3.2,free listener
v3.3.2,set timeout
v3.3.2,accept incoming socket
v3.3.2,receive rank
v3.3.2,add new socket
v3.3.2,save ranks that need to connect with
v3.3.2,start listener
v3.3.2,start connect
v3.3.2,let smaller rank connect to larger rank
v3.3.2,send local rank
v3.3.2,wait for listener
v3.3.2,print connected linkers
v3.3.2,only need to copy subset
v3.3.2,avoid to copy subset many times
v3.3.2,avoid out of range
v3.3.2,may need to recopy subset
v3.3.2,valid the type
v3.3.2,Constructors
v3.3.2,Get type tag
v3.3.2,Comparisons
v3.3.2,"This has to be separate, not in Statics, because Json() accesses"
v3.3.2,statics().null.
v3.3.2,"advance until next line, or end of input"
v3.3.2,advance until closing tokens
v3.3.2,The usual case: non-escaped characters
v3.3.2,Handle escapes
v3.3.2,Extract 4-byte escape sequence
v3.3.2,Explicitly check length of the substring. The following loop
v3.3.2,relies on std::string returning the terminating NUL when
v3.3.2,accessing str[length]. Checking here reduces brittleness.
v3.3.2,JSON specifies that characters outside the BMP shall be encoded as a
v3.3.2,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.3.2,components. Check whether we're in the middle of such a beast: the
v3.3.2,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.3.2,a trail (low) surrogate.
v3.3.2,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.3.2,per the UTF-16 algorithm.
v3.3.2,Integer part
v3.3.2,Decimal part
v3.3.2,Exponent part
v3.3.2,Check for any trailing garbage
v3.3.2,Documented in json11.hpp
v3.3.2,Check for another object
v3.3.2,get column names
v3.3.2,load label idx first
v3.3.2,erase label column name
v3.3.2,load ignore columns
v3.3.2,load weight idx
v3.3.2,load group idx
v3.3.2,don't support query id in data file when using distributed training
v3.3.2,read data to memory
v3.3.2,sample data
v3.3.2,construct feature bin mappers
v3.3.2,initialize label
v3.3.2,extract features
v3.3.2,sample data from file
v3.3.2,construct feature bin mappers
v3.3.2,initialize label
v3.3.2,extract features
v3.3.2,load data from binary file
v3.3.2,check meta data
v3.3.2,need to check training data
v3.3.2,read data in memory
v3.3.2,initialize label
v3.3.2,extract features
v3.3.2,Get number of lines of data file
v3.3.2,initialize label
v3.3.2,extract features
v3.3.2,load data from binary file
v3.3.2,not need to check validation data
v3.3.2,check meta data
v3.3.2,buffer to read binary file
v3.3.2,check token
v3.3.2,read size of header
v3.3.2,re-allocmate space if not enough
v3.3.2,read header
v3.3.2,get header
v3.3.2,num_groups
v3.3.2,real_feature_idx_
v3.3.2,feature2group
v3.3.2,feature2subfeature
v3.3.2,group_bin_boundaries
v3.3.2,group_feature_start_
v3.3.2,group_feature_cnt_
v3.3.2,get feature names
v3.3.2,write feature names
v3.3.2,get forced_bin_bounds_
v3.3.2,read size of meta data
v3.3.2,re-allocate space if not enough
v3.3.2,read meta data
v3.3.2,load meta data
v3.3.2,sample local used data if need to partition
v3.3.2,"if not contain query file, minimal sample unit is one record"
v3.3.2,"if contain query file, minimal sample unit is one query"
v3.3.2,if is new query
v3.3.2,read feature data
v3.3.2,read feature size
v3.3.2,re-allocate space if not enough
v3.3.2,raw data
v3.3.2,fill feature_names_ if not header
v3.3.2,get forced split
v3.3.2,"if only one machine, find bin locally"
v3.3.2,"if have multi-machines, need to find bin distributed"
v3.3.2,different machines will find bin for different features
v3.3.2,start and len will store the process feature indices for different machines
v3.3.2,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.2,free
v3.3.2,gather global feature bin mappers
v3.3.2,restore features bins from buffer
v3.3.2,---- private functions ----
v3.3.2,"if features are ordered, not need to use hist_buf"
v3.3.2,read all lines
v3.3.2,get query data
v3.3.2,"if not contain query data, minimal sample unit is one record"
v3.3.2,"if contain query data, minimal sample unit is one query"
v3.3.2,if is new query
v3.3.2,get query data
v3.3.2,"if not contain query file, minimal sample unit is one record"
v3.3.2,"if contain query file, minimal sample unit is one query"
v3.3.2,if is new query
v3.3.2,parse features
v3.3.2,get forced split
v3.3.2,"check the range of label_idx, weight_idx and group_idx"
v3.3.2,fill feature_names_ if not header
v3.3.2,start find bins
v3.3.2,"if only one machine, find bin locally"
v3.3.2,start and len will store the process feature indices for different machines
v3.3.2,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.2,free
v3.3.2,gather global feature bin mappers
v3.3.2,restore features bins from buffer
v3.3.2,if doesn't need to prediction with initial model
v3.3.2,parser
v3.3.2,set label
v3.3.2,free processed line:
v3.3.2,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.3.2,text_reader_->Lines()[i].shrink_to_fit();
v3.3.2,push data
v3.3.2,if is used feature
v3.3.2,if need to prediction with initial model
v3.3.2,parser
v3.3.2,set initial score
v3.3.2,set label
v3.3.2,free processed line:
v3.3.2,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v3.3.2,text_reader_->Lines()[i].shrink_to_fit();
v3.3.2,push data
v3.3.2,if is used feature
v3.3.2,metadata_ will manage space of init_score
v3.3.2,text data can be free after loaded feature values
v3.3.2,parser
v3.3.2,set initial score
v3.3.2,set label
v3.3.2,push data
v3.3.2,if is used feature
v3.3.2,only need part of data
v3.3.2,need full data
v3.3.2,metadata_ will manage space of init_score
v3.3.2,read size of token
v3.3.2,remove duplicates
v3.3.2,deep copy function for BinMapper
v3.3.2,mean size for one bin
v3.3.2,need a new bin
v3.3.2,update bin upper bound
v3.3.2,last bin upper bound
v3.3.2,get list of distinct values
v3.3.2,get number of positive and negative distinct values
v3.3.2,include zero bounds and infinity bound
v3.3.2,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.3.2,find remaining bounds
v3.3.2,find distinct_values first
v3.3.2,push zero in the front
v3.3.2,use the large value
v3.3.2,push zero in the back
v3.3.2,convert to int type first
v3.3.2,sort by counts
v3.3.2,will ignore the categorical of small counts
v3.3.2,Push the dummy bin for NaN
v3.3.2,Use MissingType::None to represent this bin contains all categoricals
v3.3.2,fix count of NaN bin
v3.3.2,check trivial(num_bin_ == 1) feature
v3.3.2,check useless bin
v3.3.2,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.3.2,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.3.2,calculate max bin of all features to select the int type in MultiValDenseBin
v3.3.2,"for lambdarank, it needs query data for partition data in distributed learning"
v3.3.2,need convert query_id to boundaries
v3.3.2,check weights
v3.3.2,check query boundries
v3.3.2,contain initial score file
v3.3.2,check weights
v3.3.2,get local weights
v3.3.2,check query boundries
v3.3.2,get local query boundaries
v3.3.2,contain initial score file
v3.3.2,get local initial scores
v3.3.2,re-load query weight
v3.3.2,save to nullptr
v3.3.2,save to nullptr
v3.3.2,save to nullptr
v3.3.2,default weight file name
v3.3.2,default init_score file name
v3.3.2,use first line to count number class
v3.3.2,default query file name
v3.3.2,root is in the depth 0
v3.3.2,non-leaf
v3.3.2,leaf
v3.3.2,use this for the missing value conversion
v3.3.2,Predict func by Map to ifelse
v3.3.2,use this for the missing value conversion
v3.3.2,non-leaf
v3.3.2,left subtree
v3.3.2,right subtree
v3.3.2,leaf
v3.3.2,non-leaf
v3.3.2,left subtree
v3.3.2,right subtree
v3.3.2,leaf
v3.3.2,recursive computation of SHAP values for a decision tree
v3.3.2,extend the unique path
v3.3.2,leaf node
v3.3.2,internal node
v3.3.2,"see if we have already split on this feature,"
v3.3.2,if so we undo that split so we can redo it for this node
v3.3.2,recursive sparse computation of SHAP values for a decision tree
v3.3.2,extend the unique path
v3.3.2,leaf node
v3.3.2,internal node
v3.3.2,"see if we have already split on this feature,"
v3.3.2,if so we undo that split so we can redo it for this node
v3.3.2,add names of objective function if not providing metric
v3.3.2,equal weights for all classes
v3.3.2,generate seeds by seed.
v3.3.2,sort eval_at
v3.3.2,Only push the non-training data
v3.3.2,check for conflicts
v3.3.2,"check if objective, metric, and num_class match"
v3.3.2,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.3.2,Check max_depth and num_leaves
v3.3.2,"Fits in an int, and is more restrictive than the current num_leaves"
v3.3.2,force col-wise for gpu & CUDA
v3.3.2,force gpu_use_dp for CUDA
v3.3.2,linear tree learner must be serial type and run on CPU device
v3.3.2,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.3.2,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.3.2,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.3.2,calculated split gain can be positive even with zero gradient and hessian.
v3.3.2,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.3.2,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.3.2,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.3.2,first round: fill the single val group
v3.3.2,always push the last group
v3.3.2,put dense feature first
v3.3.2,sort by non zero cnt
v3.3.2,"sort by non zero cnt, bigger first"
v3.3.2,shuffle groups
v3.3.2,Using std::swap for vector<bool> will cause the wrong result.
v3.3.2,get num_features
v3.3.2,get bin_mappers
v3.3.2,"for sparse multi value bin, we store the feature bin values with offset added"
v3.3.2,"for dense multi value bin, the feature bin values without offsets are used"
v3.3.2,copy feature bin mapper data
v3.3.2,copy feature bin mapper data
v3.3.2,"if not pass a filename, just append "".bin"" of original file"
v3.3.2,get size of header
v3.3.2,size of feature names
v3.3.2,size of forced bins
v3.3.2,write header
v3.3.2,write feature names
v3.3.2,write forced bins
v3.3.2,get size of meta data
v3.3.2,write meta data
v3.3.2,write feature data
v3.3.2,get size of feature
v3.3.2,write feature
v3.3.2,write raw data; use row-major order so we can read row-by-row
v3.3.2,"explicitly initialize template methods, for cross module call"
v3.3.2,"Only one multi-val group, just simply merge"
v3.3.2,Skip the leading 0 when copying group_bin_boundaries.
v3.3.2,regenerate other fields
v3.3.2,store the importance first
v3.3.2,PredictRaw
v3.3.2,PredictRawByMap
v3.3.2,Predict
v3.3.2,PredictByMap
v3.3.2,PredictLeafIndex
v3.3.2,PredictLeafIndexByMap
v3.3.2,output model type
v3.3.2,output number of class
v3.3.2,output label index
v3.3.2,output max_feature_idx
v3.3.2,output objective
v3.3.2,output tree models
v3.3.2,store the importance first
v3.3.2,sort the importance
v3.3.2,use serialized string to restore this object
v3.3.2,Use first 128 chars to avoid exceed the message buffer.
v3.3.2,get number of classes
v3.3.2,get index of label
v3.3.2,get max_feature_idx first
v3.3.2,get average_output
v3.3.2,get feature names
v3.3.2,get monotone_constraints
v3.3.2,set zero
v3.3.2,predict all the trees for one iteration
v3.3.2,check early stopping
v3.3.2,set zero
v3.3.2,predict all the trees for one iteration
v3.3.2,check early stopping
v3.3.2,margin_threshold will be captured by value
v3.3.2,copy and sort
v3.3.2,margin_threshold will be captured by value
v3.3.2,Fix for compiler warnings about reaching end of control
v3.3.2,load forced_splits file
v3.3.2,init tree learner
v3.3.2,push training metrics
v3.3.2,create buffer for gradients and Hessians
v3.3.2,get max feature index
v3.3.2,get label index
v3.3.2,get feature names
v3.3.2,"if need bagging, create buffer"
v3.3.2,"for a validation dataset, we need its score and metric"
v3.3.2,update score
v3.3.2,objective function will calculate gradients and hessians
v3.3.2,"random bagging, minimal unit is one record"
v3.3.2,"random bagging, minimal unit is one record"
v3.3.2,if need bagging
v3.3.2,set bagging data to tree learner
v3.3.2,get subset
v3.3.2,output used time per iteration
v3.3.2,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.3.2,boosting first
v3.3.2,bagging logic
v3.3.2,need to copy gradients for bagging subset.
v3.3.2,shrinkage by learning rate
v3.3.2,update score
v3.3.2,only add default score one-time
v3.3.2,updates scores
v3.3.2,add model
v3.3.2,reset score
v3.3.2,remove model
v3.3.2,print message for metric
v3.3.2,pop last early_stopping_round_ models
v3.3.2,update training score
v3.3.2,we need to predict out-of-bag scores of data for boosting
v3.3.2,update validation score
v3.3.2,print training metric
v3.3.2,print validation metric
v3.3.2,set zero
v3.3.2,predict all the trees for one iteration
v3.3.2,predict all the trees for one iteration
v3.3.2,push training metrics
v3.3.2,"not same training data, need reset score and others"
v3.3.2,create score tracker
v3.3.2,update score
v3.3.2,create buffer for gradients and hessians
v3.3.2,load forced_splits file
v3.3.2,"if need bagging, create buffer"
v3.3.2,Get the max size of pool
v3.3.2,at least need 2 leaves
v3.3.2,push split information for all leaves
v3.3.2,initialize splits for leaf
v3.3.2,initialize data partition
v3.3.2,initialize ordered gradients and hessians
v3.3.2,cannot change is_hist_col_wise during training
v3.3.2,initialize splits for leaf
v3.3.2,initialize data partition
v3.3.2,initialize ordered gradients and hessians
v3.3.2,Get the max size of pool
v3.3.2,at least need 2 leaves
v3.3.2,push split information for all leaves
v3.3.2,some initial works before training
v3.3.2,root leaf
v3.3.2,only root leaf can be splitted on first time
v3.3.2,some initial works before finding best split
v3.3.2,find best threshold for every feature
v3.3.2,Get a leaf with max split gain
v3.3.2,Get split information for best leaf
v3.3.2,"cannot split, quit"
v3.3.2,split tree with best leaf
v3.3.2,reset histogram pool
v3.3.2,initialize data partition
v3.3.2,reset the splits for leaves
v3.3.2,Sumup for root
v3.3.2,use all data
v3.3.2,"use bagging, only use part of data"
v3.3.2,check depth of current leaf
v3.3.2,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.3.2,no enough data to continue
v3.3.2,only have root
v3.3.2,put parent(left) leaf's histograms into larger leaf's histograms
v3.3.2,put parent(left) leaf's histograms to larger leaf's histograms
v3.3.2,construct smaller leaf
v3.3.2,construct larger leaf
v3.3.2,find splits
v3.3.2,only has root leaf
v3.3.2,start at root leaf
v3.3.2,"before processing next node from queue, store info for current left/right leaf"
v3.3.2,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.3.2,"then, compute own splits"
v3.3.2,split info should exist because searching in bfs fashion - should have added from parent
v3.3.2,update before tree split
v3.3.2,don't need to update this in data-based parallel model
v3.3.2,"split tree, will return right leaf"
v3.3.2,store the true split gain in tree model
v3.3.2,don't need to update this in data-based parallel model
v3.3.2,store the true split gain in tree model
v3.3.2,init the leaves that used on next iteration
v3.3.2,update leave outputs if needed
v3.3.2,bag_mapper[index_mapper[i]]
v3.3.2,it is needed to filter the features after the above code.
v3.3.2,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.3.2,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.3.2,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.3.2,find splits
v3.3.2,identify features containing nans
v3.3.2,preallocate the matrix used to calculate linear model coefficients
v3.3.2,"store only upper triangular half of matrix as an array, in row-major order"
v3.3.2,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.3.2,we add another 8 to ensure cache lines are not shared among processors
v3.3.2,some initial works before training
v3.3.2,root leaf
v3.3.2,only root leaf can be splitted on first time
v3.3.2,some initial works before finding best split
v3.3.2,find best threshold for every feature
v3.3.2,Get a leaf with max split gain
v3.3.2,Get split information for best leaf
v3.3.2,"cannot split, quit"
v3.3.2,split tree with best leaf
v3.3.2,map data to leaf number
v3.3.2,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.3.2,the coefficients vector is given by
v3.3.2,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.3.2,where:
v3.3.2,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.3.2,"H is the diagonal matrix of the hessian,"
v3.3.2,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.3.2,g is the vector of gradients
v3.3.2,the subscript _T denotes the transpose
v3.3.2,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.3.2,clear the coefficient matrices
v3.3.2,aggregate results from different threads
v3.3.2,copy into eigen matrices and solve
v3.3.2,update the tree properties
v3.3.2,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.2,get feature partition
v3.3.2,get local used features
v3.3.2,get best split at smaller leaf
v3.3.2,find local best split for larger leaf
v3.3.2,sync global best info
v3.3.2,update best split
v3.3.2,"instantiate template classes, otherwise linker cannot find the code"
v3.3.2,initialize SerialTreeLearner
v3.3.2,Get local rank and global machine size
v3.3.2,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.2,allocate buffer for communication
v3.3.2,generate feature partition for current tree
v3.3.2,get local used feature
v3.3.2,get block start and block len for reduce scatter
v3.3.2,get buffer_write_start_pos_
v3.3.2,get buffer_read_start_pos_
v3.3.2,sync global data sumup info
v3.3.2,global sumup reduce
v3.3.2,copy back
v3.3.2,set global sumup info
v3.3.2,init global data count in leaf
v3.3.2,clear histogram buffer before synchronizing
v3.3.2,otherwise histogram contents from the previous iteration will be sent
v3.3.2,construct local histograms
v3.3.2,copy to buffer
v3.3.2,Reduce scatter for histogram
v3.3.2,restore global histograms from buffer
v3.3.2,only root leaf
v3.3.2,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.3.2,find local best split for larger leaf
v3.3.2,sync global best info
v3.3.2,set best split
v3.3.2,need update global number of data in leaf
v3.3.2,"instantiate template classes, otherwise linker cannot find the code"
v3.3.2,initialize SerialTreeLearner
v3.3.2,some additional variables needed for GPU trainer
v3.3.2,Initialize GPU buffers and kernels
v3.3.2,some functions used for debugging the GPU histogram construction
v3.3.2,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.3.2,goto err;
v3.3.2,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.2,also guarantee that there are at least 2K examples per workgroup
v3.3.2,return 0;
v3.3.2,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v3.3.2,decide the best number of workgroups working on one feature4 tuple
v3.3.2,set work group size based on feature size
v3.3.2,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.2,we need to refresh the kernel arguments after reallocating
v3.3.2,The only argument that needs to be changed later is num_data_
v3.3.2,"the GPU kernel will process all features in one call, and each"
v3.3.2,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.3.2,process one feature4 tuple
v3.3.2,"for the root node, indices are not copied"
v3.3.2,"for constant hessian, hessians are not copied except for the root node"
v3.3.2,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.3.2,and we will launch num_feature workgroups for this kernel
v3.3.2,will launch threads for all features
v3.3.2,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.3.2,copy the results asynchronously. Size depends on if double precision is used
v3.3.2,we will wait for this object in WaitAndGetHistograms
v3.3.2,"when the output is ready, the computation is done"
v3.3.2,values of this feature has been redistributed to multiple bins; need a reduction here
v3.3.2,how many feature-group tuples we have
v3.3.2,leave some safe margin for prefetching
v3.3.2,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.2,clear sparse/dense maps
v3.3.2,do nothing if no features can be processed on GPU
v3.3.2,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.3.2,unpin old buffer if necessary before destructing them
v3.3.2,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v3.3.2,allocate space for gradients and Hessians on device
v3.3.2,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.2,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.3.2,copy indices to the device
v3.3.2,histogram bin entry size depends on the precision (single/double)
v3.3.2,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.2,each work group generates a sub-histogram of dword_features_ features.
v3.3.2,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.2,create atomic counters for inter-group coordination
v3.3.2,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.2,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.2,looking for dword_features_ non-sparse feature-groups
v3.3.2,decide if we need to redistribute the bin
v3.3.2,multiplier must be a power of 2
v3.3.2,device_bin_mults_.push_back(1);
v3.3.2,found
v3.3.2,for data transfer time
v3.3.2,"Now generate new data structure feature4, and copy data to the device"
v3.3.2,"preallocate arrays for all threads, and pin them"
v3.3.2,building Feature4 bundles; each thread handles dword_features_ features
v3.3.2,one feature datapoint is 4 bits
v3.3.2,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.2,one feature datapoint is one byte
v3.3.2,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.2,Dense bin
v3.3.2,Dense 4-bit bin
v3.3.2,working on the remaining (less than dword_features_) feature groups
v3.3.2,fill the leftover features
v3.3.2,"fill this empty feature with some ""random"" value"
v3.3.2,"fill this empty feature with some ""random"" value"
v3.3.2,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.3.2,deallocate pinned space for feature copying
v3.3.2,data transfer time
v3.3.2,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.3.2,"Something bad happened. Just return ""No log available."""
v3.3.2,"build is okay, log may contain warnings"
v3.3.2,destroy any old kernels
v3.3.2,create OpenCL kernels for different number of workgroups per feature
v3.3.2,currently we don't use constant memory
v3.3.2,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.3.2,kernel with indices in an array
v3.3.2,"kernel with all features enabled, with eliminated branches"
v3.3.2,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.3.2,do nothing if no features can be processed on GPU
v3.3.2,The only argument that needs to be changed later is num_data_
v3.3.2,"hessian is passed as a parameter, but it is not available now."
v3.3.2,hessian will be set in BeforeTrain()
v3.3.2,"Get the max bin size, used for selecting best GPU kernel"
v3.3.2,initialize GPU
v3.3.2,determine which kernel to use based on the max number of bins
v3.3.2,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.2,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.2,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.2,setup GPU kernel arguments after we allocating all the buffers
v3.3.2,GPU memory has to been reallocated because data may have been changed
v3.3.2,setup GPU kernel arguments after we allocating all the buffers
v3.3.2,Copy initial full hessians and gradients to GPU.
v3.3.2,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.2,setup hessian parameters only
v3.3.2,hessian is passed as a parameter
v3.3.2,use bagging
v3.3.2,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v3.3.2,copy used gradients and Hessians to ordered buffer
v3.3.2,transfer the indices to GPU
v3.3.2,transfer hessian to GPU
v3.3.2,setup hessian parameters only
v3.3.2,hessian is passed as a parameter
v3.3.2,transfer gradients to GPU
v3.3.2,only have root
v3.3.2,"Copy indices, gradients and Hessians as early as possible"
v3.3.2,only need to initialize for smaller leaf
v3.3.2,Get leaf boundary
v3.3.2,copy indices to the GPU:
v3.3.2,copy ordered Hessians to the GPU:
v3.3.2,copy ordered gradients to the GPU:
v3.3.2,do nothing if no features can be processed on GPU
v3.3.2,copy data indices if it is not null
v3.3.2,generate and copy ordered_gradients if gradients is not null
v3.3.2,generate and copy ordered_hessians if Hessians is not null
v3.3.2,converted indices in is_feature_used to feature-group indices
v3.3.2,construct the feature masks for dense feature-groups
v3.3.2,"if no feature group is used, just return and do not use GPU"
v3.3.2,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.2,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.2,"All data have been prepared, now run the GPU kernel"
v3.3.2,construct smaller leaf
v3.3.2,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v3.3.2,then construct sparse features on CPU
v3.3.2,"wait for GPU to finish, only if GPU is actually used"
v3.3.2,use double precision
v3.3.2,use single precision
v3.3.2,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v3.3.2,#define GPU_DEBUG_COMPARE
v3.3.2,construct larger leaf
v3.3.2,then construct sparse features on CPU
v3.3.2,"wait for GPU to finish, only if GPU is actually used"
v3.3.2,use double precision
v3.3.2,use single precision
v3.3.2,do some sanity check for the GPU algorithm
v3.3.2,limit top k
v3.3.2,get max bin
v3.3.2,calculate buffer size
v3.3.2,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.2,"left and right on same time, so need double size"
v3.3.2,initialize histograms for global
v3.3.2,sync global data sumup info
v3.3.2,set global sumup info
v3.3.2,init global data count in leaf
v3.3.2,get local sumup
v3.3.2,get local sumup
v3.3.2,get mean number on machines
v3.3.2,weighted gain
v3.3.2,get top k
v3.3.2,"Copy histogram to buffer, and Get local aggregate features"
v3.3.2,copy histograms.
v3.3.2,copy smaller leaf histograms first
v3.3.2,mark local aggregated feature
v3.3.2,copy
v3.3.2,then copy larger leaf histograms
v3.3.2,mark local aggregated feature
v3.3.2,copy
v3.3.2,use local data to find local best splits
v3.3.2,clear histogram buffer before synchronizing
v3.3.2,otherwise histogram contents from the previous iteration will be sent
v3.3.2,find splits
v3.3.2,only has root leaf
v3.3.2,local voting
v3.3.2,gather
v3.3.2,get all top-k from all machines
v3.3.2,global voting
v3.3.2,copy local histgrams to buffer
v3.3.2,Reduce scatter for histogram
v3.3.2,find best split from local aggregated histograms
v3.3.2,restore from buffer
v3.3.2,restore from buffer
v3.3.2,find local best
v3.3.2,find local best split for larger leaf
v3.3.2,sync global best info
v3.3.2,copy back
v3.3.2,set the global number of data for leaves
v3.3.2,init the global sumup info
v3.3.2,"instantiate template classes, otherwise linker cannot find the code"
v3.3.2,launch cuda kernel
v3.3.2,initialize SerialTreeLearner
v3.3.2,some additional variables needed for GPU trainer
v3.3.2,Initialize GPU buffers and kernels: get device info
v3.3.2,some functions used for debugging the GPU histogram construction
v3.3.2,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.2,also guarantee that there are at least 2K examples per workgroup
v3.3.2,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.3.2,decide the best number of workgroups working on one feature4 tuple
v3.3.2,set work group size based on feature size
v3.3.2,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.2,set thread_data
v3.3.2,copy the results asynchronously. Size depends on if double precision is used
v3.3.2,"when the output is ready, the computation is done"
v3.3.2,how many feature-group tuples we have
v3.3.2,leave some safe margin for prefetching
v3.3.2,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.2,clear sparse/dense maps
v3.3.2,do nothing it there is no dense feature
v3.3.2,calculate number of feature groups per gpu
v3.3.2,histogram bin entry size depends on the precision (single/double)
v3.3.2,allocate GPU memory for each GPU
v3.3.2,do nothing it there is no gpu feature
v3.3.2,allocate memory for all features
v3.3.2,allocate space for gradients and hessians on device
v3.3.2,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.2,copy indices to the device
v3.3.2,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.2,each work group generates a sub-histogram of dword_features_ features.
v3.3.2,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.2,create atomic counters for inter-group coordination
v3.3.2,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.2,clear sparse/dense maps
v3.3.2,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.2,set device info
v3.3.2,looking for dword_features_ non-sparse feature-groups
v3.3.2,reset device info
v3.3.2,InitGPU w/ num_gpu
v3.3.2,"Get the max bin size, used for selecting best GPU kernel"
v3.3.2,get num_dense_feature_groups_
v3.3.2,initialize GPU
v3.3.2,set cpu threads
v3.3.2,resize device memory pointers
v3.3.2,create stream & events to handle multiple GPUs
v3.3.2,check data size
v3.3.2,GPU memory has to been reallocated because data may have been changed
v3.3.2,AllocateGPUMemory only when the number of data increased
v3.3.2,setup GPU kernel arguments after we allocating all the buffers
v3.3.2,Copy initial full hessians and gradients to GPU.
v3.3.2,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.2,use bagging
v3.3.2,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.3.2,copy used gradients and hessians to ordered buffer
v3.3.2,transfer the indices to GPU
v3.3.2,only have root
v3.3.2,"Copy indices, gradients and hessians as early as possible"
v3.3.2,only need to initialize for smaller leaf
v3.3.2,Get leaf boundary
v3.3.2,do nothing if no features can be processed on GPU
v3.3.2,copy data indices if it is not null
v3.3.2,converted indices in is_feature_used to feature-group indices
v3.3.2,construct the feature masks for dense feature-groups
v3.3.2,"if no feature group is used, just return and do not use GPU"
v3.3.2,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.2,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.2,We now copy even if all features are used.
v3.3.2,"All data have been prepared, now run the GPU kernel"
v3.3.2,construct smaller leaf
v3.3.2,Check workgroups per feature4 tuple..
v3.3.2,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.3.2,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.3.2,then construct sparse features on CPU
v3.3.2,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.2,"wait for GPU to finish, only if GPU is actually used"
v3.3.2,use double precision
v3.3.2,use single precision
v3.3.2,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.3.2,#define CUDA_DEBUG_COMPARE
v3.3.2,construct larger leaf
v3.3.2,then construct sparse features on CPU
v3.3.2,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.2,"wait for GPU to finish, only if GPU is actually used"
v3.3.2,use double precision
v3.3.2,use single precision
v3.3.2,do some sanity check for the GPU algorithm
v3.3.1,coding: utf-8
v3.3.1,coding: utf-8
v3.3.1,create predictor first
v3.3.1,"show deprecation warning only for early stop argument, setting early stop via global params should still be possible"
v3.3.1,check dataset
v3.3.1,reduce cost for prediction training data
v3.3.1,process callbacks
v3.3.1,Most of legacy advanced options becomes callbacks
v3.3.1,construct booster
v3.3.1,start training
v3.3.1,check evaluation result.
v3.3.1,"ranking task, split according to groups"
v3.3.1,run preprocessing on the data set if needed
v3.3.1,setup callbacks
v3.3.1,coding: utf-8
v3.3.1,dummy function to support older version of scikit-learn
v3.3.1,coding: utf-8
v3.3.1,documentation templates for LGBMModel methods are shared between the classes in
v3.3.1,this module and those in the ``dask`` module
v3.3.1,"user can set verbose with kwargs, it has higher priority"
v3.3.1,Do not modify original args in fit function
v3.3.1,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.3.1,Separate built-in from callable evaluation metrics
v3.3.1,register default metric for consistency with callable eval_metric case
v3.3.1,try to deduce from class instance
v3.3.1,overwrite default metric by explicitly set metric
v3.3.1,concatenate metric from params (or default if not provided in params) and eval_metric
v3.3.1,copy for consistency
v3.3.1,reduce cost for prediction training data
v3.3.1,free dataset
v3.3.1,Switch to using a multiclass objective in the underlying LGBM instance
v3.3.1,"do not modify args, as it causes errors in model selection tools"
v3.3.1,check group data
v3.3.1,coding: utf-8
v3.3.1,we don't need lib_lightgbm while building docs
v3.3.1,coding: utf-8
v3.3.1,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.3.1,coding: utf-8
v3.3.1,coding: utf-8
v3.3.1,TypeError: obj is not a string or a number
v3.3.1,ValueError: invalid literal
v3.3.1,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.3.1,avoid side effects on passed-in parameters
v3.3.1,"find a value, and remove other aliases with .pop()"
v3.3.1,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.3.1,Get total row number.
v3.3.1,Random access by row index. Used for data sampling.
v3.3.1,Range data access. Used to read data in batch when constructing Dataset.
v3.3.1,Optionally specify batch_size to control range data read size.
v3.3.1,Only required if using ``Dataset.subset()``.
v3.3.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.1,avoid memory consumption by arrays concatenation operations
v3.3.1,create numpy array from output arrays
v3.3.1,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.3.1,for CSC there is extra column added
v3.3.1,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.3.1,same shape as input csr or csc matrix except extra column for expected value
v3.3.1,note: make sure we copy data as it will be deallocated next
v3.3.1,"free the temporary native indptr, indices, and data"
v3.3.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.1,avoid memory consumption by arrays concatenation operations
v3.3.1,c type: double**
v3.3.1,each double* element points to start of each column of sample data.
v3.3.1,c type int**
v3.3.1,each int* points to start of indices for each column
v3.3.1,"no min_data, nthreads and verbose in this function"
v3.3.1,check data has header or not
v3.3.1,need to regroup init_score
v3.3.1,process for args
v3.3.1,"user can set verbose with params, it has higher priority"
v3.3.1,get categorical features
v3.3.1,process for reference dataset
v3.3.1,start construct data
v3.3.1,set feature names
v3.3.1,"Select sampled rows, transpose to column order."
v3.3.1,create validation dataset from ref_dataset
v3.3.1,create valid
v3.3.1,construct subset
v3.3.1,create train
v3.3.1,could be updated if data is not freed
v3.3.1,set to None
v3.3.1,we're done if self and reference share a common upstream reference
v3.3.1,"if buffer length is not long enough, reallocate buffers"
v3.3.1,"group data from LightGBM is boundaries data, need to convert to group size"
v3.3.1,"user can set verbose with params, it has higher priority"
v3.3.1,Training task
v3.3.1,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.3.1,construct booster object
v3.3.1,copy the parameters from train_set
v3.3.1,save reference to data
v3.3.1,buffer for inner predict
v3.3.1,Prediction task
v3.3.1,if a single node tree it won't have `leaf_index` so return 0
v3.3.1,"Create the node record, and populate universal data members"
v3.3.1,Update values to reflect node type (leaf or split)
v3.3.1,traverse the next level of the tree
v3.3.1,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.3.1,and we add node to the list.
v3.3.1,need reset training data
v3.3.1,need to push new valid data
v3.3.1,"if buffer length is not long enough, re-allocate a buffer"
v3.3.1,"if buffer length is not long enough, reallocate a buffer"
v3.3.1,Copy models
v3.3.1,Get name of features
v3.3.1,"if buffer length is not long enough, reallocate buffers"
v3.3.1,avoid to predict many time in one iteration
v3.3.1,Get num of inner evals
v3.3.1,Get name of eval metrics
v3.3.1,"if buffer length is not long enough, reallocate buffers"
v3.3.1,coding: utf-8
v3.3.1,Callback environment used by callbacks
v3.3.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.1,coding: utf-8
v3.3.1,Concatenate many parts into one
v3.3.1,construct local eval_set data.
v3.3.1,store indices of eval_set components that were not contained within local parts.
v3.3.1,consolidate parts of each individual eval component.
v3.3.1,require that eval_name exists in evaluated result data in case dropped due to padding.
v3.3.1,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v3.3.1,filter padding from eval parts then _concat each eval_set component.
v3.3.1,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v3.3.1,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v3.3.1,capture whether local_listen_port or its aliases were provided
v3.3.1,capture whether machines or its aliases were provided
v3.3.1,Some passed-in parameters can be removed:
v3.3.1,* 'num_machines': set automatically from Dask worker list
v3.3.1,* 'num_threads': overridden to match nthreads on each Dask process
v3.3.1,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.3.1,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v3.3.1,X and y are each delayed sub-lists of original eval dask Collections.
v3.3.1,find maximum number of parts in an individual eval set so that we can
v3.3.1,pad eval sets when they come in different sizes.
v3.3.1,"when individual eval set is equivalent to training data, skip recomputing parts."
v3.3.1,add None-padding for individual eval_set member if it is smaller than the largest member.
v3.3.1,first time a chunk of this eval set is added to this part.
v3.3.1,append additional chunks of this eval set to this part.
v3.3.1,ensure that all evaluation parts map uniquely to one part.
v3.3.1,assign sub-eval_set components to worker parts.
v3.3.1,Start computation in the background
v3.3.1,Find locations of all parts and map them to particular Dask workers
v3.3.1,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v3.3.1,data artifacts may not be populated depending on worker returning final estimator.
v3.3.1,assign general validation set settings to fit kwargs.
v3.3.1,resolve aliases for network parameters and pop the result off params.
v3.3.1,these values are added back in calls to `_train_part()`
v3.3.1,figure out network params
v3.3.1,Tell each worker to train on the parts that it has locally
v3.3.1,
v3.3.1,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v3.3.1,1. there is randomness in the training process unless parameters ``seed``
v3.3.1,and ``deterministic`` are set
v3.3.1,"2. even with those parameters set, the output of one ``_train_part()`` call"
v3.3.1,relies on global state (it and all the other LightGBM training processes
v3.3.1,coordinate with each other)
v3.3.1,"if network parameters were changed during training, remove them from the"
v3.3.1,returned model so that they're generated dynamically on every run based
v3.3.1,on the Dask cluster you're connected to and which workers have pieces of
v3.3.1,the training data
v3.3.1,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.3.1,"for multi-class classification with sparse matrices, pred_contrib predictions"
v3.3.1,are returned as a list of sparse matrices (one per class)
v3.3.1,"pred_contrib output will have one column per feature,"
v3.3.1,plus one more for the base value
v3.3.1,need to tell Dask the expected type and shape of individual preds
v3.3.1,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v3.3.1,the code below is used instead to ensure that the sparse type is preserved during concatentation
v3.3.1,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v3.3.1,Concatenate them to return a list of Dask Arrays.
v3.3.1,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.1,currently relevant for the Dask estimators
v3.3.1,"DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds."
v3.3.1,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.3.1,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.1,currently relevant for the Dask estimators
v3.3.1,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds."
v3.3.1,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.3.1,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.1,currently relevant for the Dask estimators
v3.3.1,DaskLGBMRanker does not support eval_class_weight or early stopping
v3.3.1,DaskLGBMRanker support for callbacks and init_model is not tested
v3.3.1,coding: utf-8
v3.3.1,load or create your dataset
v3.3.1,create dataset for lightgbm
v3.3.1,"if you want to re-use data, remember to set free_raw_data=False"
v3.3.1,specify your configurations as a dict
v3.3.1,generate feature names
v3.3.1,feature_name and categorical_feature
v3.3.1,check feature name
v3.3.1,save model to file
v3.3.1,dump model to JSON (and save to file)
v3.3.1,feature names
v3.3.1,feature importances
v3.3.1,load model to predict
v3.3.1,can only predict with the best iteration (or the saving iteration)
v3.3.1,eval with loaded model
v3.3.1,dump model with pickle
v3.3.1,load model with pickle to predict
v3.3.1,can predict with any iteration when loaded in pickle way
v3.3.1,eval with loaded model
v3.3.1,continue training
v3.3.1,init_model accepts:
v3.3.1,1. model file name
v3.3.1,2. Booster()
v3.3.1,decay learning rates
v3.3.1,learning_rates accepts:
v3.3.1,1. list/tuple with length = num_boost_round
v3.3.1,2. function(curr_iter)
v3.3.1,change other parameters during training
v3.3.1,self-defined objective function
v3.3.1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.3.1,log likelihood loss
v3.3.1,self-defined eval metric
v3.3.1,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.1,binary error
v3.3.1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.1,This may make built-in evaluation metric calculate wrong results
v3.3.1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.1,Keep this in mind when you use the customization
v3.3.1,another self-defined eval metric
v3.3.1,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.1,accuracy
v3.3.1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.1,This may make built-in evaluation metric calculate wrong results
v3.3.1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.1,Keep this in mind when you use the customization
v3.3.1,callback
v3.3.1,coding: utf-8
v3.3.1,load or create your dataset
v3.3.1,train
v3.3.1,predict
v3.3.1,eval
v3.3.1,feature importances
v3.3.1,self-defined eval metric
v3.3.1,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.1,Root Mean Squared Logarithmic Error (RMSLE)
v3.3.1,train
v3.3.1,another self-defined eval metric
v3.3.1,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.1,Relative Absolute Error (RAE)
v3.3.1,train
v3.3.1,predict
v3.3.1,eval
v3.3.1,other scikit-learn modules
v3.3.1,coding: utf-8
v3.3.1,load or create your dataset
v3.3.1,create dataset for lightgbm
v3.3.1,specify your configurations as a dict
v3.3.1,train
v3.3.1,coding: utf-8
v3.3.1,################
v3.3.1,Simulate some binary data with a single categorical and
v3.3.1,single continuous predictor
v3.3.1,################
v3.3.1,Set up a couple of utilities for our experiments
v3.3.1,################
v3.3.1,Observe the behavior of `binary` and `xentropy` objectives
v3.3.1,Trying this throws an error on non-binary values of y:
v3.3.1,"experiment('binary', label_type='probability', DATA)"
v3.3.1,The speed of `binary` is not drastically different than
v3.3.1,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.3.1,there are reasons to suspect that `binary` should run faster when the
v3.3.1,label is an integer instead of a float
v3.3.1,coding: utf-8
v3.3.1,load or create your dataset
v3.3.1,create dataset for lightgbm
v3.3.1,specify your configurations as a dict
v3.3.1,train
v3.3.1,save model to file
v3.3.1,predict
v3.3.1,eval
v3.3.1,We can also open HDF5 file once and get access to
v3.3.1,"With binary dataset created, we can use either Python API or cmdline version to train."
v3.3.1,
v3.3.1,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v3.3.1,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v3.3.1,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v3.3.1,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v3.3.1,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v3.3.1,Y has a single column and we read it in single shot. So store it as an 1-d array.
v3.3.1,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v3.3.1,"When accessing any element in a HDF5 chunk, it's read entirely."
v3.3.1,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v3.3.1,Here we are just creating a chunk size that matches with batch_size.
v3.3.1,
v3.3.1,Also note that the data is stored in row major order to avoid extra copy when passing to
v3.3.1,lightgbm Dataset.
v3.3.1,Save to 2 HDF5 files for demonstration.
v3.3.1,We can store multiple datasets inside a single HDF5 file.
v3.3.1,Separating X and Y for choosing best chunk size for data loading.
v3.3.1,split training data into two partitions
v3.3.1,make this array dense because we're splitting across
v3.3.1,a sparse boundary to partition the data
v3.3.1,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.3.1,predictions and target values back from workers to the client
v3.3.1,
v3.3.1,"for larger datasets, consider the metrics from dask-ml instead"
v3.3.1,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.3.1,coding: utf-8
v3.3.1,!/usr/bin/env python3
v3.3.1,-*- coding: utf-8 -*-
v3.3.1,
v3.3.1,"LightGBM documentation build configuration file, created by"
v3.3.1,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.3.1,
v3.3.1,This file is execfile()d with the current directory set to its
v3.3.1,containing dir.
v3.3.1,
v3.3.1,Note that not all possible configuration values are present in this
v3.3.1,autogenerated file.
v3.3.1,
v3.3.1,All configuration values have a default; values that are commented out
v3.3.1,serve to show the default.
v3.3.1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.3.1,add these directories to sys.path here. If the directory is relative to the
v3.3.1,"documentation root, use os.path.abspath to make it absolute."
v3.3.1,-- mock out modules
v3.3.1,-- General configuration ------------------------------------------------
v3.3.1,"If your documentation needs a minimal Sphinx version, state it here."
v3.3.1,"Add any Sphinx extension module names here, as strings. They can be"
v3.3.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.3.1,ones.
v3.3.1,hide type hints in API docs
v3.3.1,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.3.1,Only the class' docstring is inserted.
v3.3.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.3.1,The master toctree document.
v3.3.1,General information about the project.
v3.3.1,The name of an image file (relative to this directory) to place at the top
v3.3.1,of the sidebar.
v3.3.1,The name of an image file (relative to this directory) to use as a favicon of
v3.3.1,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.3.1,pixels large.
v3.3.1,"The version info for the project you're documenting, acts as replacement for"
v3.3.1,"|version| and |release|, also used in various other places throughout the"
v3.3.1,built documents.
v3.3.1,The short X.Y version.
v3.3.1,"The full version, including alpha/beta/rc tags."
v3.3.1,The language for content autogenerated by Sphinx. Refer to documentation
v3.3.1,for a list of supported languages.
v3.3.1,
v3.3.1,This is also used if you do content translation via gettext catalogs.
v3.3.1,"Usually you set ""language"" from the command line for these cases."
v3.3.1,"List of patterns, relative to source directory, that match files and"
v3.3.1,directories to ignore when looking for source files.
v3.3.1,This patterns also effect to html_static_path and html_extra_path
v3.3.1,The name of the Pygments (syntax highlighting) style to use.
v3.3.1,-- Configuration for C API docs generation ------------------------------
v3.3.1,-- Options for HTML output ----------------------------------------------
v3.3.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.3.1,a list of builtin themes.
v3.3.1,Theme options are theme-specific and customize the look and feel of a theme
v3.3.1,"further.  For a list of options available for each theme, see the"
v3.3.1,documentation.
v3.3.1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.3.1,"relative to this directory. They are copied after the builtin static files,"
v3.3.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.3.1,-- Options for HTMLHelp output ------------------------------------------
v3.3.1,Output file base name for HTML help builder.
v3.3.1,-- Options for LaTeX output ---------------------------------------------
v3.3.1,The name of an image file (relative to this directory) to place at the top of
v3.3.1,the title page.
v3.3.1,Warning! The following code can cause buffer overflows on RTD.
v3.3.1,Consider suppressing output completely if RTD project silently fails.
v3.3.1,Refer to https://github.com/svenevs/exhale
v3.3.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.1,Warning! The following code can cause buffer overflows on RTD.
v3.3.1,Consider suppressing output completely if RTD project silently fails.
v3.3.1,Refer to https://github.com/svenevs/exhale
v3.3.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.1,coding: utf-8
v3.3.1,This is a basic test for floating number parsing.
v3.3.1,Most of the test cases come from:
v3.3.1,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v3.3.1,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v3.3.1,FLT_MAX
v3.3.1,FLT_MIN
v3.3.1,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v3.3.1,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v3.3.1,Near DBL_MIN
v3.3.1,DBL_MIN 2^-1022
v3.3.1,The behavior for parsing -nan depends on implementation.
v3.3.1,Thus we skip binary check for negative nan.
v3.3.1,See comment in test_cases.
v3.3.1,Constants
v3.3.1,Start with some content:
v3.3.1,Clear & re-use:
v3.3.1,Output should match new content:
v3.3.1,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.3.1,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.3.1,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.3.1,Compute a new trial address & value & if it is a valid address:
v3.3.1,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.3.1,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.3.1,Check the just-stored value with getitem():
v3.3.1,Also store the just-stored value for future tracking:
v3.3.1,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.3.1,Test in 2 ways that the values are correctly laid out in memory:
v3.3.1,coding: utf-8
v3.3.1,we don't need lib_lightgbm while building docs
v3.3.1,coding: utf-8
v3.3.1,check saved model persistence
v3.3.1,"we need to check the consistency of model file here, so test for exact equal"
v3.3.1,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.3.1,"scores likely to be different, but prediction should still be the same"
v3.3.1,test that shape is checked during prediction
v3.3.1,"The simple implementation is just a single ""return self.ndarray[idx]"""
v3.3.1,The following is for demo and testing purpose.
v3.3.1,whole col
v3.3.1,half col
v3.3.1,Create dataset from numpy array directly.
v3.3.1,Create dataset using Sequence.
v3.3.1,Test for validation set.
v3.3.1,Select some random rows as valid data.
v3.3.1,"From Dataset constructor, with dataset from numpy array."
v3.3.1,"From Dataset.create_valid, with dataset from sequence."
v3.3.1,test that method works even with free_raw_data=True
v3.3.1,test that method works but sets raw data to None in case of immergeable data types
v3.3.1,test that method works for different data types
v3.3.1,"Set extremely harsh penalties, so CEGB will block most splits."
v3.3.1,"Compare pairs of penalties, to ensure scaling works as intended"
v3.3.1,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.3.1,"should resolve duplicate aliases, and prefer the main parameter"
v3.3.1,should choose a value from an alias and set that value on main param
v3.3.1,if only an alias is used
v3.3.1,should use the default if main param and aliases are missing
v3.3.1,all changes should be made on copies and not modify the original
v3.3.1,coding: utf-8
v3.3.1,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.3.1,set_index ensures partitions are based on group id.
v3.3.1,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.3.1,"separate target, weight from features."
v3.3.1,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.3.1,"so that within each partition, sum(g) = n_samples."
v3.3.1,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.3.1,make one categorical feature relevant to the target
v3.3.1,https://github.com/microsoft/LightGBM/issues/4118
v3.3.1,extra predict() parameters should be passed through correctly
v3.3.1,pref_leaf values should have the right shape
v3.3.1,and values that look like valid tree nodes
v3.3.1,"be sure LightGBM actually used at least one categorical column,"
v3.3.1,and that it was correctly treated as a categorical feature
v3.3.1,shape depends on whether it is binary or multiclass classification
v3.3.1,"in the special case of multi-class classification using scipy sparse matrices,"
v3.3.1,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v3.3.1,
v3.3.1,"since that case is so different than all other cases, check the relevant things here"
v3.3.1,and then return early
v3.3.1,"raw scores will probably be different, but at least check that all predicted classes are the same"
v3.3.1,"be sure LightGBM actually used at least one categorical column,"
v3.3.1,and that it was correctly treated as a categorical feature
v3.3.1,* shape depends on whether it is binary or multiclass classification
v3.3.1,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.3.1,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.3.1,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.1,that the output has the right shape and base values are in the right position
v3.3.1,check that found ports are different for same address (LocalCluster)
v3.3.1,check that the ports are indeed open
v3.3.1,Scores should be the same
v3.3.1,Predictions should be roughly the same.
v3.3.1,pref_leaf values should have the right shape
v3.3.1,and values that look like valid tree nodes
v3.3.1,extra predict() parameters should be passed through correctly
v3.3.1,"be sure LightGBM actually used at least one categorical column,"
v3.3.1,and that it was correctly treated as a categorical feature
v3.3.1,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.1,that the output has the right shape and base values are in the right position
v3.3.1,"be sure LightGBM actually used at least one categorical column,"
v3.3.1,and that it was correctly treated as a categorical feature
v3.3.1,Quantiles should be right
v3.3.1,"be sure LightGBM actually used at least one categorical column,"
v3.3.1,and that it was correctly treated as a categorical feature
v3.3.1,rebalance small dask.Array dataset for better performance.
v3.3.1,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.3.1,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.3.1,distributed ranker should be able to rank decently well and should
v3.3.1,have high rank correlation with scores from serial ranker.
v3.3.1,extra predict() parameters should be passed through correctly
v3.3.1,pref_leaf values should have the right shape
v3.3.1,and values that look like valid tree nodes
v3.3.1,"be sure LightGBM actually used at least one categorical column,"
v3.3.1,and that it was correctly treated as a categorical feature
v3.3.1,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v3.3.1,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v3.3.1,"test eval_class_weight, eval_init_score on binary-classification task."
v3.3.1,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v3.3.1,create eval_sets by creating new datasets or copying training data.
v3.3.1,total number of trees scales up for ova classifier.
v3.3.1,check that early stopping was not applied.
v3.3.1,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v3.3.1,"check that each eval_name and metric exists for all eval sets, allowing for the"
v3.3.1,case when a worker receives a fully-padded eval_set component which is not evaluated.
v3.3.1,should be able to use the class without specifying a client
v3.3.1,should be able to set client after construction
v3.3.1,data on cluster1
v3.3.1,create identical data on cluster2
v3.3.1,"at this point, the result of default_client() is client2 since it was the most recently"
v3.3.1,created. So setting client to client1 here to test that you can select a non-default client
v3.3.1,"unfitted model should survive pickling round trip, and pickling"
v3.3.1,shouldn't have side effects on the model object
v3.3.1,client will always be None after unpickling
v3.3.1,"fitted model should survive pickling round trip, and pickling"
v3.3.1,shouldn't have side effects on the model object
v3.3.1,client will always be None after unpickling
v3.3.1,rebalance data to be sure that each worker has a piece of the data
v3.3.1,model 1 - no network parameters given
v3.3.1,model 2 - machines given
v3.3.1,model 3 - local_listen_port given
v3.3.1,training should fail because LightGBM will try to use the same
v3.3.1,port for multiple worker processes on the same machine
v3.3.1,rebalance data to be sure that each worker has a piece of the data
v3.3.1,"test that ""machines"" is actually respected by creating a socket that uses"
v3.3.1,"one of the ports mentioned in ""machines"""
v3.3.1,The above error leaves a worker waiting
v3.3.1,"an informative error should be raised if ""machines"" has duplicates"
v3.3.1,"""client"" should be the only different, and the final argument"
v3.3.1,value of the root node is 0 when init_score is set
v3.3.1,this test is separate because it takes a not-yet-constructed estimator
v3.3.1,coding: utf-8
v3.3.1,coding: utf-8
v3.3.1,"build target, group ID vectors."
v3.3.1,build y/target and group-id vectors with user-specified group sizes.
v3.3.1,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.3.1,groups should contain > 1 element for pairwise learning objective.
v3.3.1,"build feature data, X. Transform first few into informative features."
v3.3.1,coding: utf-8
v3.3.1,prediction result is actually not transformed (is raw) due to custom objective
v3.3.1,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.3.1,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.3.1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.1,sklearn < 0.23 does not have as_frame parameter
v3.3.1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.1,sklearn < 0.23 does not have as_frame parameter
v3.3.1,Test if random_state is properly stored
v3.3.1,Test if two random states produce identical models
v3.3.1,Test if subsequent fits sample from random_state object and produce different models
v3.3.1,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.3.1,With default params
v3.3.1,Tests same probabilities
v3.3.1,Tests same predictions
v3.3.1,Tests same raw scores
v3.3.1,Tests same leaf indices
v3.3.1,Tests same feature contributions
v3.3.1,Tests other parameters for the prediction works
v3.3.1,Tests start_iteration
v3.3.1,"Tests same probabilities, starting from iteration 10"
v3.3.1,"Tests same predictions, starting from iteration 10"
v3.3.1,"Tests same raw scores, starting from iteration 10"
v3.3.1,"Tests same leaf indices, starting from iteration 10"
v3.3.1,"Tests same feature contributions, starting from iteration 10"
v3.3.1,"Tests other parameters for the prediction works, starting from iteration 10"
v3.3.1,"no custom objective, no custom metric"
v3.3.1,default metric
v3.3.1,non-default metric
v3.3.1,no metric
v3.3.1,non-default metric in eval_metric
v3.3.1,non-default metric with non-default metric in eval_metric
v3.3.1,non-default metric with multiple metrics in eval_metric
v3.3.1,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.3.1,default metric for non-default objective
v3.3.1,non-default metric for non-default objective
v3.3.1,no metric
v3.3.1,non-default metric in eval_metric for non-default objective
v3.3.1,non-default metric with non-default metric in eval_metric for non-default objective
v3.3.1,non-default metric with multiple metrics in eval_metric for non-default objective
v3.3.1,"custom objective, no custom metric"
v3.3.1,default regression metric for custom objective
v3.3.1,non-default regression metric for custom objective
v3.3.1,multiple regression metrics for custom objective
v3.3.1,no metric
v3.3.1,default regression metric with non-default metric in eval_metric for custom objective
v3.3.1,non-default regression metric with metric in eval_metric for custom objective
v3.3.1,multiple regression metrics with metric in eval_metric for custom objective
v3.3.1,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.3.1,"no custom objective, custom metric"
v3.3.1,default metric with custom metric
v3.3.1,non-default metric with custom metric
v3.3.1,multiple metrics with custom metric
v3.3.1,custom metric (disable default metric)
v3.3.1,default metric for non-default objective with custom metric
v3.3.1,non-default metric for non-default objective with custom metric
v3.3.1,multiple metrics for non-default objective with custom metric
v3.3.1,custom metric (disable default metric for non-default objective)
v3.3.1,"custom objective, custom metric"
v3.3.1,custom metric for custom objective
v3.3.1,non-default regression metric with custom metric for custom objective
v3.3.1,multiple regression metrics with custom metric for custom objective
v3.3.1,default metric and invalid binary metric is replaced with multiclass alternative
v3.3.1,invalid objective is replaced with default multiclass one
v3.3.1,and invalid binary metric is replaced with multiclass alternative
v3.3.1,default metric for non-default multiclass objective
v3.3.1,and invalid binary metric is replaced with multiclass alternative
v3.3.1,default metric and invalid multiclass metric is replaced with binary alternative
v3.3.1,invalid multiclass metric is replaced with binary alternative for custom objective
v3.3.1,"Verify that can receive a list of metrics, only callable"
v3.3.1,Verify that can receive a list of custom and built-in metrics
v3.3.1,Verify that works as expected when eval_metric is empty
v3.3.1,"Verify that can receive a list of metrics, only built-in"
v3.3.1,Verify that eval_metric is robust to receiving a list with None
v3.3.1,training data as eval_set
v3.3.1,feval
v3.3.1,single eval_set
v3.3.1,two eval_set
v3.3.1,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.3.1,Test that estimators are default-constructible
v3.3.1,coding: utf-8
v3.3.1,coding: utf-8
v3.3.1,check that default gives same result as k = 1
v3.3.1,check against independent calculation for k = 1
v3.3.1,check against independent calculation for k = 2
v3.3.1,check against independent calculation for k = 10
v3.3.1,check cases where predictions are equal
v3.3.1,should give same result as binary auc for 2 classes
v3.3.1,test the case where all predictions are equal
v3.3.1,test that weighted data gives different auc_mu
v3.3.1,test that equal data weights give same auc_mu as unweighted data
v3.3.1,should give 1 when accuracy = 1
v3.3.1,test loading class weights
v3.3.1,no early stopping
v3.3.1,early stopping occurs
v3.3.1,test custom eval metrics
v3.3.1,"shuffle = False, override metric in params"
v3.3.1,"shuffle = True, callbacks"
v3.3.1,enable display training loss
v3.3.1,self defined folds
v3.3.1,LambdaRank
v3.3.1,... with l2 metric
v3.3.1,... with NDCG (default) metric
v3.3.1,self defined folds with lambdarank
v3.3.1,with early stopping
v3.3.1,predict by each fold booster
v3.3.1,fold averaging
v3.3.1,without early stopping
v3.3.1,test feature_names with whitespaces
v3.3.1,This has non-ascii strings.
v3.3.1,take subsets and train
v3.3.1,generate CSR sparse dataset
v3.3.1,convert data to dense and get back same contribs
v3.3.1,validate the values are the same
v3.3.1,validate using CSC matrix
v3.3.1,validate the values are the same
v3.3.1,generate CSR sparse dataset
v3.3.1,convert data to dense and get back same contribs
v3.3.1,validate the values are the same
v3.3.1,validate using CSC matrix
v3.3.1,validate the values are the same
v3.3.1,Note there is an extra column added to the output for the expected value
v3.3.1,Note output CSC shape should be same as CSR output shape
v3.3.1,test sliced labels
v3.3.1,append some columns
v3.3.1,append some rows
v3.3.1,test sliced 2d matrix
v3.3.1,test sliced CSR
v3.3.1,trees start at position 1.
v3.3.1,split_features are in 4th line.
v3.3.1,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.3.1,The penalization is so high that the first 2 features should not be used here
v3.3.1,Check that a very high penalization is the same as not using the features at all
v3.3.1,"no fobj, no feval"
v3.3.1,default metric
v3.3.1,non-default metric in params
v3.3.1,default metric in args
v3.3.1,non-default metric in args
v3.3.1,metric in args overwrites one in params
v3.3.1,multiple metrics in params
v3.3.1,multiple metrics in args
v3.3.1,remove default metric by 'None' in list
v3.3.1,remove default metric by 'None' aliases
v3.3.1,"fobj, no feval"
v3.3.1,no default metric
v3.3.1,metric in params
v3.3.1,metric in args
v3.3.1,metric in args overwrites its' alias in params
v3.3.1,multiple metrics in params
v3.3.1,multiple metrics in args
v3.3.1,"no fobj, feval"
v3.3.1,default metric with custom one
v3.3.1,non-default metric in params with custom one
v3.3.1,default metric in args with custom one
v3.3.1,non-default metric in args with custom one
v3.3.1,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.1,multiple metrics in params with custom one
v3.3.1,multiple metrics in args with custom one
v3.3.1,custom metric is evaluated despite 'None' is passed
v3.3.1,"fobj, feval"
v3.3.1,"no default metric, only custom one"
v3.3.1,metric in params with custom one
v3.3.1,metric in args with custom one
v3.3.1,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.1,multiple metrics in params with custom one
v3.3.1,multiple metrics in args with custom one
v3.3.1,custom metric is evaluated despite 'None' is passed
v3.3.1,"no fobj, no feval"
v3.3.1,default metric
v3.3.1,default metric in params
v3.3.1,non-default metric in params
v3.3.1,multiple metrics in params
v3.3.1,remove default metric by 'None' aliases
v3.3.1,"fobj, no feval"
v3.3.1,no default metric
v3.3.1,metric in params
v3.3.1,multiple metrics in params
v3.3.1,"no fobj, feval"
v3.3.1,default metric with custom one
v3.3.1,default metric in params with custom one
v3.3.1,non-default metric in params with custom one
v3.3.1,multiple metrics in params with custom one
v3.3.1,custom metric is evaluated despite 'None' is passed
v3.3.1,"fobj, feval"
v3.3.1,"no default metric, only custom one"
v3.3.1,metric in params with custom one
v3.3.1,multiple metrics in params with custom one
v3.3.1,custom metric is evaluated despite 'None' is passed
v3.3.1,multiclass default metric
v3.3.1,multiclass default metric with custom one
v3.3.1,multiclass metric alias with custom one for custom objective
v3.3.1,no metric for invalid class_num
v3.3.1,custom metric for invalid class_num
v3.3.1,multiclass metric alias with custom one with invalid class_num
v3.3.1,multiclass default metric without num_class
v3.3.1,multiclass metric alias
v3.3.1,multiclass metric
v3.3.1,non-valid metric for multiclass objective
v3.3.1,non-default num_class for default objective
v3.3.1,no metric with non-default num_class for custom objective
v3.3.1,multiclass metric alias for custom objective
v3.3.1,multiclass metric for custom objective
v3.3.1,binary metric with non-default num_class for custom objective
v3.3.1,Expect three metrics but mean and stdv for each metric
v3.3.1,test XGBoost-style return value
v3.3.1,test numpy-style return value
v3.3.1,test bins string type
v3.3.1,test histogram is disabled for categorical features
v3.3.1,test for lgb.train
v3.3.1,test feval for lgb.train
v3.3.1,test with two valid data for lgb.train
v3.3.1,test for lgb.cv
v3.3.1,test feval for lgb.cv
v3.3.1,test that binning works properly for features with only positive or only negative values
v3.3.1,decreasing without freeing raw data is allowed
v3.3.1,decreasing before lazy init is allowed
v3.3.1,increasing is allowed
v3.3.1,decreasing with disabled filter is allowed
v3.3.1,decreasing with enabled filter is disallowed;
v3.3.1,also changes of other params are disallowed
v3.3.1,check extra trees increases regularization
v3.3.1,check path smoothing increases regularization
v3.3.1,test edge case with one leaf
v3.3.1,check that constraint containing all features is equivalent to no constraint
v3.3.1,check that constraint partitioning the features reduces train accuracy
v3.3.1,check that constraints consisting of single features reduce accuracy further
v3.3.1,test that interaction constraints work when not all features are used
v3.3.1,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.3.1,test again with nans in data
v3.3.1,test again with bagging
v3.3.1,test with a feature that has only one non-nan value
v3.3.1,test with a categorical feature
v3.3.1,test refit: same results on same data
v3.3.1,test refit with save and load
v3.3.1,test refit: different results training on different data
v3.3.1,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.3.1,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.3.1,"test the case where start_iteration <= 0, and num_iteration is None"
v3.3.1,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.3.1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.3.1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.3.1,test for regression
v3.3.1,test both with and without early stopping
v3.3.1,test for multi-class
v3.3.1,test both with and without early stopping
v3.3.1,test for binary
v3.3.1,test both with and without early stopping
v3.3.1,test against sklearn average precision metric
v3.3.1,test that average precision is 1 where model predicts perfectly
v3.3.1,coding: utf-8
v3.3.1,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.3.1,coding: utf-8
v3.3.1,coding: utf-8
v3.3.1,These are helper functions to allow doing a stack unwind
v3.3.1,"after an R allocation error, which would trigger a long jump."
v3.3.1,convert from one-based to zero-based index
v3.3.1,"if any feature names were larger than allocated size,"
v3.3.1,allow for a larger size and try again
v3.3.1,convert from boundaries to size
v3.3.1,--- start Booster interfaces
v3.3.1,"if any eval names were larger than allocated size,"
v3.3.1,allow for a larger size and try again
v3.3.1,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.1,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.1,.Call() calls
v3.3.1,coding: utf-8
v3.3.1,alias table
v3.3.1,names
v3.3.1,from strings
v3.3.1,tails
v3.3.1,tails
v3.3.1,coding: utf-8
v3.3.1,Single row predictor to abstract away caching logic
v3.3.1,create boosting
v3.3.1,initialize the boosting
v3.3.1,create objective function
v3.3.1,initialize the objective function
v3.3.1,create training metric
v3.3.1,reset the boosting
v3.3.1,create objective function
v3.3.1,initialize the objective function
v3.3.1,calculate the nonzero data and indices size
v3.3.1,allocate data and indices arrays
v3.3.1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.1,aggregated per row feature contribution results
v3.3.1,keep track of the row_vector sizes for parallelization
v3.3.1,copy vector results to output for each row
v3.3.1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.1,aggregated per row feature contribution results
v3.3.1,calculate number of elements per column to construct
v3.3.1,the CSC matrix with random access
v3.3.1,keep track of column counts
v3.3.1,keep track of beginning index for each column
v3.3.1,keep track of beginning index for each matrix
v3.3.1,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.3.1,store the row index
v3.3.1,update column count
v3.3.1,explicitly declare symbols from LightGBM namespace
v3.3.1,some help functions used to convert data
v3.3.1,Row iterator of on column for CSC matrix
v3.3.1,"return value at idx, only can access by ascent order"
v3.3.1,"return next non-zero pair, if index < 0, means no more data"
v3.3.1,start of c_api functions
v3.3.1,This API is to keep python binding's behavior the same with C++ implementation.
v3.3.1,"Sample count, random seed etc. should be provided in parameters."
v3.3.1,sample data first
v3.3.1,sample data first
v3.3.1,sample data first
v3.3.1,local buffer to re-use memory
v3.3.1,sample data first
v3.3.1,no more data
v3.3.1,---- start of booster
v3.3.1,Single row in row-major format:
v3.3.1,---- start of some help functions
v3.3.1,data is array of pointers to individual rows
v3.3.1,set number of threads for openmp
v3.3.1,check for alias
v3.3.1,read parameters from config file
v3.3.1,"remove str after ""#"""
v3.3.1,check for alias again
v3.3.1,load configs
v3.3.1,prediction is needed if using input initial model(continued train)
v3.3.1,need to continue training
v3.3.1,sync up random seed for data partition
v3.3.1,load Training data
v3.3.1,load data for distributed training
v3.3.1,load data for single machine
v3.3.1,need save binary file
v3.3.1,create training metric
v3.3.1,only when have metrics then need to construct validation data
v3.3.1,"Add validation data, if it exists"
v3.3.1,add
v3.3.1,need save binary file
v3.3.1,add metric for validation data
v3.3.1,output used time on each iteration
v3.3.1,need init network
v3.3.1,create boosting
v3.3.1,create objective function
v3.3.1,load training data
v3.3.1,initialize the objective function
v3.3.1,initialize the boosting
v3.3.1,add validation data into boosting
v3.3.1,convert model to if-else statement code
v3.3.1,create predictor
v3.3.1,Free memory
v3.3.1,create predictor
v3.3.1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.3.1,counts for all labels
v3.3.1,"start from top label, and accumulate DCG"
v3.3.1,counts for all labels
v3.3.1,calculate k Max DCG by one pass
v3.3.1,get sorted indices by score
v3.3.1,calculate multi dcg by one pass
v3.3.1,wait for all client start up
v3.3.1,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.3.1,Instead we will handle finalize/abort for MPI in main().
v3.3.1,default set to -1
v3.3.1,"distance at k-th communication, distance[k] = 2^k"
v3.3.1,set incoming rank at k-th commuication
v3.3.1,set outgoing rank at k-th commuication
v3.3.1,default set as -1
v3.3.1,construct all recursive halving map for all machines
v3.3.1,let 1 << k <= num_machines
v3.3.1,distance of each communication
v3.3.1,"if num_machines = 2^k, don't need to group machines"
v3.3.1,"communication direction, %2 == 0 is positive"
v3.3.1,neighbor at k-th communication
v3.3.1,receive data block at k-th communication
v3.3.1,send data block at k-th communication
v3.3.1,"if num_machines != 2^k, need to group machines"
v3.3.1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.3.1,let left machine as group leader
v3.3.1,"cache block information for groups, group with 2 machines will have double block size"
v3.3.1,convert from group to node leader
v3.3.1,convert from node to group
v3.3.1,meet new group
v3.3.1,add block len for this group
v3.3.1,calculate the group block start
v3.3.1,not need to construct
v3.3.1,get receive block information
v3.3.1,accumulate block len
v3.3.1,get send block information
v3.3.1,accumulate block len
v3.3.1,static member definition
v3.3.1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.3.1,assign the blocks to every rank.
v3.3.1,do reduce scatter
v3.3.1,do all gather
v3.3.1,assign blocks
v3.3.1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.3.1,copy back
v3.3.1,assign blocks
v3.3.1,start all gather
v3.3.1,when num_machines is small and data is large
v3.3.1,use output as receive buffer
v3.3.1,get current local block size
v3.3.1,get out rank
v3.3.1,get in rank
v3.3.1,get send information
v3.3.1,get recv information
v3.3.1,send and recv at same time
v3.3.1,rotate in-place
v3.3.1,use output as receive buffer
v3.3.1,get current local block size
v3.3.1,get send information
v3.3.1,get recv information
v3.3.1,send and recv at same time
v3.3.1,use output as receive buffer
v3.3.1,send and recv at same time
v3.3.1,send local data to neighbor first
v3.3.1,receive neighbor data first
v3.3.1,reduce
v3.3.1,get target
v3.3.1,get send information
v3.3.1,get recv information
v3.3.1,send and recv at same time
v3.3.1,reduce
v3.3.1,send result to neighbor
v3.3.1,receive result from neighbor
v3.3.1,copy result
v3.3.1,start up socket
v3.3.1,parse clients from file
v3.3.1,get ip list of local machine
v3.3.1,get local rank
v3.3.1,construct listener
v3.3.1,construct communication topo
v3.3.1,construct linkers
v3.3.1,free listener
v3.3.1,set timeout
v3.3.1,accept incoming socket
v3.3.1,receive rank
v3.3.1,add new socket
v3.3.1,save ranks that need to connect with
v3.3.1,start listener
v3.3.1,start connect
v3.3.1,let smaller rank connect to larger rank
v3.3.1,send local rank
v3.3.1,wait for listener
v3.3.1,print connected linkers
v3.3.1,only need to copy subset
v3.3.1,avoid to copy subset many times
v3.3.1,avoid out of range
v3.3.1,may need to recopy subset
v3.3.1,valid the type
v3.3.1,Constructors
v3.3.1,Get type tag
v3.3.1,Comparisons
v3.3.1,"This has to be separate, not in Statics, because Json() accesses"
v3.3.1,statics().null.
v3.3.1,"advance until next line, or end of input"
v3.3.1,advance until closing tokens
v3.3.1,The usual case: non-escaped characters
v3.3.1,Handle escapes
v3.3.1,Extract 4-byte escape sequence
v3.3.1,Explicitly check length of the substring. The following loop
v3.3.1,relies on std::string returning the terminating NUL when
v3.3.1,accessing str[length]. Checking here reduces brittleness.
v3.3.1,JSON specifies that characters outside the BMP shall be encoded as a
v3.3.1,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.3.1,components. Check whether we're in the middle of such a beast: the
v3.3.1,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.3.1,a trail (low) surrogate.
v3.3.1,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.3.1,per the UTF-16 algorithm.
v3.3.1,Integer part
v3.3.1,Decimal part
v3.3.1,Exponent part
v3.3.1,Check for any trailing garbage
v3.3.1,Documented in json11.hpp
v3.3.1,Check for another object
v3.3.1,get column names
v3.3.1,load label idx first
v3.3.1,erase label column name
v3.3.1,load ignore columns
v3.3.1,load weight idx
v3.3.1,load group idx
v3.3.1,don't support query id in data file when using distributed training
v3.3.1,read data to memory
v3.3.1,sample data
v3.3.1,construct feature bin mappers
v3.3.1,initialize label
v3.3.1,extract features
v3.3.1,sample data from file
v3.3.1,construct feature bin mappers
v3.3.1,initialize label
v3.3.1,extract features
v3.3.1,load data from binary file
v3.3.1,check meta data
v3.3.1,need to check training data
v3.3.1,read data in memory
v3.3.1,initialize label
v3.3.1,extract features
v3.3.1,Get number of lines of data file
v3.3.1,initialize label
v3.3.1,extract features
v3.3.1,load data from binary file
v3.3.1,not need to check validation data
v3.3.1,check meta data
v3.3.1,buffer to read binary file
v3.3.1,check token
v3.3.1,read size of header
v3.3.1,re-allocmate space if not enough
v3.3.1,read header
v3.3.1,get header
v3.3.1,num_groups
v3.3.1,real_feature_idx_
v3.3.1,feature2group
v3.3.1,feature2subfeature
v3.3.1,group_bin_boundaries
v3.3.1,group_feature_start_
v3.3.1,group_feature_cnt_
v3.3.1,get feature names
v3.3.1,write feature names
v3.3.1,get forced_bin_bounds_
v3.3.1,read size of meta data
v3.3.1,re-allocate space if not enough
v3.3.1,read meta data
v3.3.1,load meta data
v3.3.1,sample local used data if need to partition
v3.3.1,"if not contain query file, minimal sample unit is one record"
v3.3.1,"if contain query file, minimal sample unit is one query"
v3.3.1,if is new query
v3.3.1,read feature data
v3.3.1,read feature size
v3.3.1,re-allocate space if not enough
v3.3.1,raw data
v3.3.1,fill feature_names_ if not header
v3.3.1,get forced split
v3.3.1,"if only one machine, find bin locally"
v3.3.1,"if have multi-machines, need to find bin distributed"
v3.3.1,different machines will find bin for different features
v3.3.1,start and len will store the process feature indices for different machines
v3.3.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.1,free
v3.3.1,gather global feature bin mappers
v3.3.1,restore features bins from buffer
v3.3.1,---- private functions ----
v3.3.1,"if features are ordered, not need to use hist_buf"
v3.3.1,read all lines
v3.3.1,get query data
v3.3.1,"if not contain query data, minimal sample unit is one record"
v3.3.1,"if contain query data, minimal sample unit is one query"
v3.3.1,if is new query
v3.3.1,get query data
v3.3.1,"if not contain query file, minimal sample unit is one record"
v3.3.1,"if contain query file, minimal sample unit is one query"
v3.3.1,if is new query
v3.3.1,parse features
v3.3.1,get forced split
v3.3.1,"check the range of label_idx, weight_idx and group_idx"
v3.3.1,fill feature_names_ if not header
v3.3.1,start find bins
v3.3.1,"if only one machine, find bin locally"
v3.3.1,start and len will store the process feature indices for different machines
v3.3.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.1,free
v3.3.1,gather global feature bin mappers
v3.3.1,restore features bins from buffer
v3.3.1,if doesn't need to prediction with initial model
v3.3.1,parser
v3.3.1,set label
v3.3.1,free processed line:
v3.3.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.3.1,text_reader_->Lines()[i].shrink_to_fit();
v3.3.1,push data
v3.3.1,if is used feature
v3.3.1,if need to prediction with initial model
v3.3.1,parser
v3.3.1,set initial score
v3.3.1,set label
v3.3.1,free processed line:
v3.3.1,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v3.3.1,text_reader_->Lines()[i].shrink_to_fit();
v3.3.1,push data
v3.3.1,if is used feature
v3.3.1,metadata_ will manage space of init_score
v3.3.1,text data can be free after loaded feature values
v3.3.1,parser
v3.3.1,set initial score
v3.3.1,set label
v3.3.1,push data
v3.3.1,if is used feature
v3.3.1,only need part of data
v3.3.1,need full data
v3.3.1,metadata_ will manage space of init_score
v3.3.1,read size of token
v3.3.1,remove duplicates
v3.3.1,deep copy function for BinMapper
v3.3.1,mean size for one bin
v3.3.1,need a new bin
v3.3.1,update bin upper bound
v3.3.1,last bin upper bound
v3.3.1,get list of distinct values
v3.3.1,get number of positive and negative distinct values
v3.3.1,include zero bounds and infinity bound
v3.3.1,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.3.1,find remaining bounds
v3.3.1,find distinct_values first
v3.3.1,push zero in the front
v3.3.1,use the large value
v3.3.1,push zero in the back
v3.3.1,convert to int type first
v3.3.1,sort by counts
v3.3.1,will ignore the categorical of small counts
v3.3.1,Push the dummy bin for NaN
v3.3.1,Use MissingType::None to represent this bin contains all categoricals
v3.3.1,fix count of NaN bin
v3.3.1,check trivial(num_bin_ == 1) feature
v3.3.1,check useless bin
v3.3.1,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.3.1,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.3.1,calculate max bin of all features to select the int type in MultiValDenseBin
v3.3.1,"for lambdarank, it needs query data for partition data in distributed learning"
v3.3.1,need convert query_id to boundaries
v3.3.1,check weights
v3.3.1,check query boundries
v3.3.1,contain initial score file
v3.3.1,check weights
v3.3.1,get local weights
v3.3.1,check query boundries
v3.3.1,get local query boundaries
v3.3.1,contain initial score file
v3.3.1,get local initial scores
v3.3.1,re-load query weight
v3.3.1,save to nullptr
v3.3.1,save to nullptr
v3.3.1,save to nullptr
v3.3.1,default weight file name
v3.3.1,default init_score file name
v3.3.1,use first line to count number class
v3.3.1,default query file name
v3.3.1,root is in the depth 0
v3.3.1,non-leaf
v3.3.1,leaf
v3.3.1,use this for the missing value conversion
v3.3.1,Predict func by Map to ifelse
v3.3.1,use this for the missing value conversion
v3.3.1,non-leaf
v3.3.1,left subtree
v3.3.1,right subtree
v3.3.1,leaf
v3.3.1,non-leaf
v3.3.1,left subtree
v3.3.1,right subtree
v3.3.1,leaf
v3.3.1,recursive computation of SHAP values for a decision tree
v3.3.1,extend the unique path
v3.3.1,leaf node
v3.3.1,internal node
v3.3.1,"see if we have already split on this feature,"
v3.3.1,if so we undo that split so we can redo it for this node
v3.3.1,recursive sparse computation of SHAP values for a decision tree
v3.3.1,extend the unique path
v3.3.1,leaf node
v3.3.1,internal node
v3.3.1,"see if we have already split on this feature,"
v3.3.1,if so we undo that split so we can redo it for this node
v3.3.1,add names of objective function if not providing metric
v3.3.1,equal weights for all classes
v3.3.1,generate seeds by seed.
v3.3.1,sort eval_at
v3.3.1,Only push the non-training data
v3.3.1,check for conflicts
v3.3.1,"check if objective, metric, and num_class match"
v3.3.1,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.3.1,Check max_depth and num_leaves
v3.3.1,"Fits in an int, and is more restrictive than the current num_leaves"
v3.3.1,force col-wise for gpu & CUDA
v3.3.1,force gpu_use_dp for CUDA
v3.3.1,linear tree learner must be serial type and run on CPU device
v3.3.1,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.3.1,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.3.1,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.3.1,calculated split gain can be positive even with zero gradient and hessian.
v3.3.1,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.3.1,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.3.1,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.3.1,first round: fill the single val group
v3.3.1,always push the last group
v3.3.1,put dense feature first
v3.3.1,sort by non zero cnt
v3.3.1,"sort by non zero cnt, bigger first"
v3.3.1,shuffle groups
v3.3.1,Using std::swap for vector<bool> will cause the wrong result.
v3.3.1,get num_features
v3.3.1,get bin_mappers
v3.3.1,"for sparse multi value bin, we store the feature bin values with offset added"
v3.3.1,"for dense multi value bin, the feature bin values without offsets are used"
v3.3.1,copy feature bin mapper data
v3.3.1,copy feature bin mapper data
v3.3.1,"if not pass a filename, just append "".bin"" of original file"
v3.3.1,get size of header
v3.3.1,size of feature names
v3.3.1,size of forced bins
v3.3.1,write header
v3.3.1,write feature names
v3.3.1,write forced bins
v3.3.1,get size of meta data
v3.3.1,write meta data
v3.3.1,write feature data
v3.3.1,get size of feature
v3.3.1,write feature
v3.3.1,write raw data; use row-major order so we can read row-by-row
v3.3.1,"explicitly initialize template methods, for cross module call"
v3.3.1,"Only one multi-val group, just simply merge"
v3.3.1,Skip the leading 0 when copying group_bin_boundaries.
v3.3.1,regenerate other fields
v3.3.1,store the importance first
v3.3.1,PredictRaw
v3.3.1,PredictRawByMap
v3.3.1,Predict
v3.3.1,PredictByMap
v3.3.1,PredictLeafIndex
v3.3.1,PredictLeafIndexByMap
v3.3.1,output model type
v3.3.1,output number of class
v3.3.1,output label index
v3.3.1,output max_feature_idx
v3.3.1,output objective
v3.3.1,output tree models
v3.3.1,store the importance first
v3.3.1,sort the importance
v3.3.1,use serialized string to restore this object
v3.3.1,Use first 128 chars to avoid exceed the message buffer.
v3.3.1,get number of classes
v3.3.1,get index of label
v3.3.1,get max_feature_idx first
v3.3.1,get average_output
v3.3.1,get feature names
v3.3.1,get monotone_constraints
v3.3.1,set zero
v3.3.1,predict all the trees for one iteration
v3.3.1,check early stopping
v3.3.1,set zero
v3.3.1,predict all the trees for one iteration
v3.3.1,check early stopping
v3.3.1,margin_threshold will be captured by value
v3.3.1,copy and sort
v3.3.1,margin_threshold will be captured by value
v3.3.1,Fix for compiler warnings about reaching end of control
v3.3.1,load forced_splits file
v3.3.1,init tree learner
v3.3.1,push training metrics
v3.3.1,create buffer for gradients and Hessians
v3.3.1,get max feature index
v3.3.1,get label index
v3.3.1,get feature names
v3.3.1,"if need bagging, create buffer"
v3.3.1,"for a validation dataset, we need its score and metric"
v3.3.1,update score
v3.3.1,objective function will calculate gradients and hessians
v3.3.1,"random bagging, minimal unit is one record"
v3.3.1,"random bagging, minimal unit is one record"
v3.3.1,if need bagging
v3.3.1,set bagging data to tree learner
v3.3.1,get subset
v3.3.1,output used time per iteration
v3.3.1,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.3.1,boosting first
v3.3.1,bagging logic
v3.3.1,need to copy gradients for bagging subset.
v3.3.1,shrinkage by learning rate
v3.3.1,update score
v3.3.1,only add default score one-time
v3.3.1,updates scores
v3.3.1,add model
v3.3.1,reset score
v3.3.1,remove model
v3.3.1,print message for metric
v3.3.1,pop last early_stopping_round_ models
v3.3.1,update training score
v3.3.1,we need to predict out-of-bag scores of data for boosting
v3.3.1,update validation score
v3.3.1,print training metric
v3.3.1,print validation metric
v3.3.1,set zero
v3.3.1,predict all the trees for one iteration
v3.3.1,predict all the trees for one iteration
v3.3.1,push training metrics
v3.3.1,"not same training data, need reset score and others"
v3.3.1,create score tracker
v3.3.1,update score
v3.3.1,create buffer for gradients and hessians
v3.3.1,load forced_splits file
v3.3.1,"if need bagging, create buffer"
v3.3.1,Get the max size of pool
v3.3.1,at least need 2 leaves
v3.3.1,push split information for all leaves
v3.3.1,initialize splits for leaf
v3.3.1,initialize data partition
v3.3.1,initialize ordered gradients and hessians
v3.3.1,cannot change is_hist_col_wise during training
v3.3.1,initialize splits for leaf
v3.3.1,initialize data partition
v3.3.1,initialize ordered gradients and hessians
v3.3.1,Get the max size of pool
v3.3.1,at least need 2 leaves
v3.3.1,push split information for all leaves
v3.3.1,some initial works before training
v3.3.1,root leaf
v3.3.1,only root leaf can be splitted on first time
v3.3.1,some initial works before finding best split
v3.3.1,find best threshold for every feature
v3.3.1,Get a leaf with max split gain
v3.3.1,Get split information for best leaf
v3.3.1,"cannot split, quit"
v3.3.1,split tree with best leaf
v3.3.1,reset histogram pool
v3.3.1,initialize data partition
v3.3.1,reset the splits for leaves
v3.3.1,Sumup for root
v3.3.1,use all data
v3.3.1,"use bagging, only use part of data"
v3.3.1,check depth of current leaf
v3.3.1,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.3.1,no enough data to continue
v3.3.1,only have root
v3.3.1,put parent(left) leaf's histograms into larger leaf's histograms
v3.3.1,put parent(left) leaf's histograms to larger leaf's histograms
v3.3.1,construct smaller leaf
v3.3.1,construct larger leaf
v3.3.1,find splits
v3.3.1,only has root leaf
v3.3.1,start at root leaf
v3.3.1,"before processing next node from queue, store info for current left/right leaf"
v3.3.1,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.3.1,"then, compute own splits"
v3.3.1,split info should exist because searching in bfs fashion - should have added from parent
v3.3.1,update before tree split
v3.3.1,don't need to update this in data-based parallel model
v3.3.1,"split tree, will return right leaf"
v3.3.1,store the true split gain in tree model
v3.3.1,don't need to update this in data-based parallel model
v3.3.1,store the true split gain in tree model
v3.3.1,init the leaves that used on next iteration
v3.3.1,update leave outputs if needed
v3.3.1,bag_mapper[index_mapper[i]]
v3.3.1,it is needed to filter the features after the above code.
v3.3.1,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.3.1,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.3.1,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.3.1,find splits
v3.3.1,identify features containing nans
v3.3.1,preallocate the matrix used to calculate linear model coefficients
v3.3.1,"store only upper triangular half of matrix as an array, in row-major order"
v3.3.1,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.3.1,we add another 8 to ensure cache lines are not shared among processors
v3.3.1,some initial works before training
v3.3.1,root leaf
v3.3.1,only root leaf can be splitted on first time
v3.3.1,some initial works before finding best split
v3.3.1,find best threshold for every feature
v3.3.1,Get a leaf with max split gain
v3.3.1,Get split information for best leaf
v3.3.1,"cannot split, quit"
v3.3.1,split tree with best leaf
v3.3.1,map data to leaf number
v3.3.1,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.3.1,the coefficients vector is given by
v3.3.1,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.3.1,where:
v3.3.1,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.3.1,"H is the diagonal matrix of the hessian,"
v3.3.1,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.3.1,g is the vector of gradients
v3.3.1,the subscript _T denotes the transpose
v3.3.1,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.3.1,clear the coefficient matrices
v3.3.1,aggregate results from different threads
v3.3.1,copy into eigen matrices and solve
v3.3.1,update the tree properties
v3.3.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.1,get feature partition
v3.3.1,get local used features
v3.3.1,get best split at smaller leaf
v3.3.1,find local best split for larger leaf
v3.3.1,sync global best info
v3.3.1,update best split
v3.3.1,"instantiate template classes, otherwise linker cannot find the code"
v3.3.1,initialize SerialTreeLearner
v3.3.1,Get local rank and global machine size
v3.3.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.1,allocate buffer for communication
v3.3.1,generate feature partition for current tree
v3.3.1,get local used feature
v3.3.1,get block start and block len for reduce scatter
v3.3.1,get buffer_write_start_pos_
v3.3.1,get buffer_read_start_pos_
v3.3.1,sync global data sumup info
v3.3.1,global sumup reduce
v3.3.1,copy back
v3.3.1,set global sumup info
v3.3.1,init global data count in leaf
v3.3.1,clear histogram buffer before synchronizing
v3.3.1,otherwise histogram contents from the previous iteration will be sent
v3.3.1,construct local histograms
v3.3.1,copy to buffer
v3.3.1,Reduce scatter for histogram
v3.3.1,restore global histograms from buffer
v3.3.1,only root leaf
v3.3.1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.3.1,find local best split for larger leaf
v3.3.1,sync global best info
v3.3.1,set best split
v3.3.1,need update global number of data in leaf
v3.3.1,"instantiate template classes, otherwise linker cannot find the code"
v3.3.1,initialize SerialTreeLearner
v3.3.1,some additional variables needed for GPU trainer
v3.3.1,Initialize GPU buffers and kernels
v3.3.1,some functions used for debugging the GPU histogram construction
v3.3.1,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.3.1,goto err;
v3.3.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.1,also guarantee that there are at least 2K examples per workgroup
v3.3.1,return 0;
v3.3.1,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v3.3.1,decide the best number of workgroups working on one feature4 tuple
v3.3.1,set work group size based on feature size
v3.3.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.1,we need to refresh the kernel arguments after reallocating
v3.3.1,The only argument that needs to be changed later is num_data_
v3.3.1,"the GPU kernel will process all features in one call, and each"
v3.3.1,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.3.1,process one feature4 tuple
v3.3.1,"for the root node, indices are not copied"
v3.3.1,"for constant hessian, hessians are not copied except for the root node"
v3.3.1,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.3.1,and we will launch num_feature workgroups for this kernel
v3.3.1,will launch threads for all features
v3.3.1,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.3.1,copy the results asynchronously. Size depends on if double precision is used
v3.3.1,we will wait for this object in WaitAndGetHistograms
v3.3.1,"when the output is ready, the computation is done"
v3.3.1,values of this feature has been redistributed to multiple bins; need a reduction here
v3.3.1,how many feature-group tuples we have
v3.3.1,leave some safe margin for prefetching
v3.3.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.1,clear sparse/dense maps
v3.3.1,do nothing if no features can be processed on GPU
v3.3.1,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.3.1,unpin old buffer if necessary before destructing them
v3.3.1,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v3.3.1,allocate space for gradients and Hessians on device
v3.3.1,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.1,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.3.1,copy indices to the device
v3.3.1,histogram bin entry size depends on the precision (single/double)
v3.3.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.1,each work group generates a sub-histogram of dword_features_ features.
v3.3.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.1,create atomic counters for inter-group coordination
v3.3.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.1,looking for dword_features_ non-sparse feature-groups
v3.3.1,decide if we need to redistribute the bin
v3.3.1,multiplier must be a power of 2
v3.3.1,device_bin_mults_.push_back(1);
v3.3.1,found
v3.3.1,for data transfer time
v3.3.1,"Now generate new data structure feature4, and copy data to the device"
v3.3.1,"preallocate arrays for all threads, and pin them"
v3.3.1,building Feature4 bundles; each thread handles dword_features_ features
v3.3.1,one feature datapoint is 4 bits
v3.3.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.1,one feature datapoint is one byte
v3.3.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.1,Dense bin
v3.3.1,Dense 4-bit bin
v3.3.1,working on the remaining (less than dword_features_) feature groups
v3.3.1,fill the leftover features
v3.3.1,"fill this empty feature with some ""random"" value"
v3.3.1,"fill this empty feature with some ""random"" value"
v3.3.1,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.3.1,deallocate pinned space for feature copying
v3.3.1,data transfer time
v3.3.1,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.3.1,"Something bad happened. Just return ""No log available."""
v3.3.1,"build is okay, log may contain warnings"
v3.3.1,destroy any old kernels
v3.3.1,create OpenCL kernels for different number of workgroups per feature
v3.3.1,currently we don't use constant memory
v3.3.1,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.3.1,kernel with indices in an array
v3.3.1,"kernel with all features enabled, with eliminated branches"
v3.3.1,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.3.1,do nothing if no features can be processed on GPU
v3.3.1,The only argument that needs to be changed later is num_data_
v3.3.1,"hessian is passed as a parameter, but it is not available now."
v3.3.1,hessian will be set in BeforeTrain()
v3.3.1,"Get the max bin size, used for selecting best GPU kernel"
v3.3.1,initialize GPU
v3.3.1,determine which kernel to use based on the max number of bins
v3.3.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.1,setup GPU kernel arguments after we allocating all the buffers
v3.3.1,GPU memory has to been reallocated because data may have been changed
v3.3.1,setup GPU kernel arguments after we allocating all the buffers
v3.3.1,Copy initial full hessians and gradients to GPU.
v3.3.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.1,setup hessian parameters only
v3.3.1,hessian is passed as a parameter
v3.3.1,use bagging
v3.3.1,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v3.3.1,copy used gradients and Hessians to ordered buffer
v3.3.1,transfer the indices to GPU
v3.3.1,transfer hessian to GPU
v3.3.1,setup hessian parameters only
v3.3.1,hessian is passed as a parameter
v3.3.1,transfer gradients to GPU
v3.3.1,only have root
v3.3.1,"Copy indices, gradients and Hessians as early as possible"
v3.3.1,only need to initialize for smaller leaf
v3.3.1,Get leaf boundary
v3.3.1,copy indices to the GPU:
v3.3.1,copy ordered Hessians to the GPU:
v3.3.1,copy ordered gradients to the GPU:
v3.3.1,do nothing if no features can be processed on GPU
v3.3.1,copy data indices if it is not null
v3.3.1,generate and copy ordered_gradients if gradients is not null
v3.3.1,generate and copy ordered_hessians if Hessians is not null
v3.3.1,converted indices in is_feature_used to feature-group indices
v3.3.1,construct the feature masks for dense feature-groups
v3.3.1,"if no feature group is used, just return and do not use GPU"
v3.3.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.1,"All data have been prepared, now run the GPU kernel"
v3.3.1,construct smaller leaf
v3.3.1,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v3.3.1,then construct sparse features on CPU
v3.3.1,"wait for GPU to finish, only if GPU is actually used"
v3.3.1,use double precision
v3.3.1,use single precision
v3.3.1,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v3.3.1,#define GPU_DEBUG_COMPARE
v3.3.1,construct larger leaf
v3.3.1,then construct sparse features on CPU
v3.3.1,"wait for GPU to finish, only if GPU is actually used"
v3.3.1,use double precision
v3.3.1,use single precision
v3.3.1,do some sanity check for the GPU algorithm
v3.3.1,limit top k
v3.3.1,get max bin
v3.3.1,calculate buffer size
v3.3.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.1,"left and right on same time, so need double size"
v3.3.1,initialize histograms for global
v3.3.1,sync global data sumup info
v3.3.1,set global sumup info
v3.3.1,init global data count in leaf
v3.3.1,get local sumup
v3.3.1,get local sumup
v3.3.1,get mean number on machines
v3.3.1,weighted gain
v3.3.1,get top k
v3.3.1,"Copy histogram to buffer, and Get local aggregate features"
v3.3.1,copy histograms.
v3.3.1,copy smaller leaf histograms first
v3.3.1,mark local aggregated feature
v3.3.1,copy
v3.3.1,then copy larger leaf histograms
v3.3.1,mark local aggregated feature
v3.3.1,copy
v3.3.1,use local data to find local best splits
v3.3.1,clear histogram buffer before synchronizing
v3.3.1,otherwise histogram contents from the previous iteration will be sent
v3.3.1,find splits
v3.3.1,only has root leaf
v3.3.1,local voting
v3.3.1,gather
v3.3.1,get all top-k from all machines
v3.3.1,global voting
v3.3.1,copy local histgrams to buffer
v3.3.1,Reduce scatter for histogram
v3.3.1,find best split from local aggregated histograms
v3.3.1,restore from buffer
v3.3.1,restore from buffer
v3.3.1,find local best
v3.3.1,find local best split for larger leaf
v3.3.1,sync global best info
v3.3.1,copy back
v3.3.1,set the global number of data for leaves
v3.3.1,init the global sumup info
v3.3.1,"instantiate template classes, otherwise linker cannot find the code"
v3.3.1,launch cuda kernel
v3.3.1,initialize SerialTreeLearner
v3.3.1,some additional variables needed for GPU trainer
v3.3.1,Initialize GPU buffers and kernels: get device info
v3.3.1,some functions used for debugging the GPU histogram construction
v3.3.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.1,also guarantee that there are at least 2K examples per workgroup
v3.3.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.3.1,decide the best number of workgroups working on one feature4 tuple
v3.3.1,set work group size based on feature size
v3.3.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.1,set thread_data
v3.3.1,copy the results asynchronously. Size depends on if double precision is used
v3.3.1,"when the output is ready, the computation is done"
v3.3.1,how many feature-group tuples we have
v3.3.1,leave some safe margin for prefetching
v3.3.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.1,clear sparse/dense maps
v3.3.1,do nothing it there is no dense feature
v3.3.1,calculate number of feature groups per gpu
v3.3.1,histogram bin entry size depends on the precision (single/double)
v3.3.1,allocate GPU memory for each GPU
v3.3.1,do nothing it there is no gpu feature
v3.3.1,allocate memory for all features
v3.3.1,allocate space for gradients and hessians on device
v3.3.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.1,copy indices to the device
v3.3.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.1,each work group generates a sub-histogram of dword_features_ features.
v3.3.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.1,create atomic counters for inter-group coordination
v3.3.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.1,clear sparse/dense maps
v3.3.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.1,set device info
v3.3.1,looking for dword_features_ non-sparse feature-groups
v3.3.1,reset device info
v3.3.1,InitGPU w/ num_gpu
v3.3.1,"Get the max bin size, used for selecting best GPU kernel"
v3.3.1,get num_dense_feature_groups_
v3.3.1,initialize GPU
v3.3.1,set cpu threads
v3.3.1,resize device memory pointers
v3.3.1,create stream & events to handle multiple GPUs
v3.3.1,check data size
v3.3.1,GPU memory has to been reallocated because data may have been changed
v3.3.1,AllocateGPUMemory only when the number of data increased
v3.3.1,setup GPU kernel arguments after we allocating all the buffers
v3.3.1,Copy initial full hessians and gradients to GPU.
v3.3.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.1,use bagging
v3.3.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.3.1,copy used gradients and hessians to ordered buffer
v3.3.1,transfer the indices to GPU
v3.3.1,only have root
v3.3.1,"Copy indices, gradients and hessians as early as possible"
v3.3.1,only need to initialize for smaller leaf
v3.3.1,Get leaf boundary
v3.3.1,do nothing if no features can be processed on GPU
v3.3.1,copy data indices if it is not null
v3.3.1,converted indices in is_feature_used to feature-group indices
v3.3.1,construct the feature masks for dense feature-groups
v3.3.1,"if no feature group is used, just return and do not use GPU"
v3.3.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.1,We now copy even if all features are used.
v3.3.1,"All data have been prepared, now run the GPU kernel"
v3.3.1,construct smaller leaf
v3.3.1,Check workgroups per feature4 tuple..
v3.3.1,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.3.1,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.3.1,then construct sparse features on CPU
v3.3.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.1,"wait for GPU to finish, only if GPU is actually used"
v3.3.1,use double precision
v3.3.1,use single precision
v3.3.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.3.1,#define CUDA_DEBUG_COMPARE
v3.3.1,construct larger leaf
v3.3.1,then construct sparse features on CPU
v3.3.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.1,"wait for GPU to finish, only if GPU is actually used"
v3.3.1,use double precision
v3.3.1,use single precision
v3.3.1,do some sanity check for the GPU algorithm
v3.3.0,coding: utf-8
v3.3.0,coding: utf-8
v3.3.0,create predictor first
v3.3.0,"show deprecation warning only for early stop argument, setting early stop via global params should still be possible"
v3.3.0,check dataset
v3.3.0,reduce cost for prediction training data
v3.3.0,process callbacks
v3.3.0,Most of legacy advanced options becomes callbacks
v3.3.0,construct booster
v3.3.0,start training
v3.3.0,check evaluation result.
v3.3.0,"ranking task, split according to groups"
v3.3.0,run preprocessing on the data set if needed
v3.3.0,setup callbacks
v3.3.0,coding: utf-8
v3.3.0,dummy function to support older version of scikit-learn
v3.3.0,coding: utf-8
v3.3.0,documentation templates for LGBMModel methods are shared between the classes in
v3.3.0,this module and those in the ``dask`` module
v3.3.0,"user can set verbose with kwargs, it has higher priority"
v3.3.0,Do not modify original args in fit function
v3.3.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.3.0,Separate built-in from callable evaluation metrics
v3.3.0,register default metric for consistency with callable eval_metric case
v3.3.0,try to deduce from class instance
v3.3.0,overwrite default metric by explicitly set metric
v3.3.0,concatenate metric from params (or default if not provided in params) and eval_metric
v3.3.0,copy for consistency
v3.3.0,reduce cost for prediction training data
v3.3.0,free dataset
v3.3.0,Switch to using a multiclass objective in the underlying LGBM instance
v3.3.0,"do not modify args, as it causes errors in model selection tools"
v3.3.0,check group data
v3.3.0,coding: utf-8
v3.3.0,we don't need lib_lightgbm while building docs
v3.3.0,coding: utf-8
v3.3.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.3.0,coding: utf-8
v3.3.0,coding: utf-8
v3.3.0,TypeError: obj is not a string or a number
v3.3.0,ValueError: invalid literal
v3.3.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.3.0,avoid side effects on passed-in parameters
v3.3.0,"find a value, and remove other aliases with .pop()"
v3.3.0,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.3.0,Get total row number.
v3.3.0,Random access by row index. Used for data sampling.
v3.3.0,Range data access. Used to read data in batch when constructing Dataset.
v3.3.0,Optionally specify batch_size to control range data read size.
v3.3.0,Only required if using ``Dataset.subset()``.
v3.3.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.0,avoid memory consumption by arrays concatenation operations
v3.3.0,create numpy array from output arrays
v3.3.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.3.0,for CSC there is extra column added
v3.3.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.3.0,same shape as input csr or csc matrix except extra column for expected value
v3.3.0,note: make sure we copy data as it will be deallocated next
v3.3.0,"free the temporary native indptr, indices, and data"
v3.3.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.3.0,avoid memory consumption by arrays concatenation operations
v3.3.0,c type: double**
v3.3.0,each double* element points to start of each column of sample data.
v3.3.0,c type int**
v3.3.0,each int* points to start of indices for each column
v3.3.0,"no min_data, nthreads and verbose in this function"
v3.3.0,check data has header or not
v3.3.0,need to regroup init_score
v3.3.0,process for args
v3.3.0,"user can set verbose with params, it has higher priority"
v3.3.0,get categorical features
v3.3.0,process for reference dataset
v3.3.0,start construct data
v3.3.0,set feature names
v3.3.0,"Select sampled rows, transpose to column order."
v3.3.0,create validation dataset from ref_dataset
v3.3.0,create valid
v3.3.0,construct subset
v3.3.0,create train
v3.3.0,could be updated if data is not freed
v3.3.0,set to None
v3.3.0,we're done if self and reference share a common upstream reference
v3.3.0,"if buffer length is not long enough, reallocate buffers"
v3.3.0,"group data from LightGBM is boundaries data, need to convert to group size"
v3.3.0,"user can set verbose with params, it has higher priority"
v3.3.0,Training task
v3.3.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.3.0,construct booster object
v3.3.0,copy the parameters from train_set
v3.3.0,save reference to data
v3.3.0,buffer for inner predict
v3.3.0,Prediction task
v3.3.0,if a single node tree it won't have `leaf_index` so return 0
v3.3.0,"Create the node record, and populate universal data members"
v3.3.0,Update values to reflect node type (leaf or split)
v3.3.0,traverse the next level of the tree
v3.3.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.3.0,and we add node to the list.
v3.3.0,need reset training data
v3.3.0,need to push new valid data
v3.3.0,"if buffer length is not long enough, re-allocate a buffer"
v3.3.0,"if buffer length is not long enough, reallocate a buffer"
v3.3.0,Copy models
v3.3.0,Get name of features
v3.3.0,"if buffer length is not long enough, reallocate buffers"
v3.3.0,avoid to predict many time in one iteration
v3.3.0,Get num of inner evals
v3.3.0,Get name of eval metrics
v3.3.0,"if buffer length is not long enough, reallocate buffers"
v3.3.0,coding: utf-8
v3.3.0,Callback environment used by callbacks
v3.3.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.3.0,coding: utf-8
v3.3.0,Concatenate many parts into one
v3.3.0,construct local eval_set data.
v3.3.0,store indices of eval_set components that were not contained within local parts.
v3.3.0,consolidate parts of each individual eval component.
v3.3.0,require that eval_name exists in evaluated result data in case dropped due to padding.
v3.3.0,"in distributed training the 'training' eval_set is not detected, will have name 'valid_<index>'."
v3.3.0,filter padding from eval parts then _concat each eval_set component.
v3.3.0,reconstruct eval_set fit args/kwargs depending on which components of eval_set are on worker.
v3.3.0,ensure that expected keys for evals_result_ and best_score_ exist regardless of padding.
v3.3.0,capture whether local_listen_port or its aliases were provided
v3.3.0,capture whether machines or its aliases were provided
v3.3.0,Some passed-in parameters can be removed:
v3.3.0,* 'num_machines': set automatically from Dask worker list
v3.3.0,* 'num_threads': overridden to match nthreads on each Dask process
v3.3.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.3.0,"evals_set will to be re-constructed into smaller lists of (X, y) tuples, where"
v3.3.0,X and y are each delayed sub-lists of original eval dask Collections.
v3.3.0,find maximum number of parts in an individual eval set so that we can
v3.3.0,pad eval sets when they come in different sizes.
v3.3.0,"when individual eval set is equivalent to training data, skip recomputing parts."
v3.3.0,add None-padding for individual eval_set member if it is smaller than the largest member.
v3.3.0,first time a chunk of this eval set is added to this part.
v3.3.0,append additional chunks of this eval set to this part.
v3.3.0,ensure that all evaluation parts map uniquely to one part.
v3.3.0,assign sub-eval_set components to worker parts.
v3.3.0,Start computation in the background
v3.3.0,Find locations of all parts and map them to particular Dask workers
v3.3.0,Check that all workers were provided some of eval_set. Otherwise warn user that validation
v3.3.0,data artifacts may not be populated depending on worker returning final estimator.
v3.3.0,assign general validation set settings to fit kwargs.
v3.3.0,resolve aliases for network parameters and pop the result off params.
v3.3.0,these values are added back in calls to `_train_part()`
v3.3.0,figure out network params
v3.3.0,Tell each worker to train on the parts that it has locally
v3.3.0,
v3.3.0,"This code treats ``_train_part()`` calls as not ""pure"" because:"
v3.3.0,1. there is randomness in the training process unless parameters ``seed``
v3.3.0,and ``deterministic`` are set
v3.3.0,"2. even with those parameters set, the output of one ``_train_part()`` call"
v3.3.0,relies on global state (it and all the other LightGBM training processes
v3.3.0,coordinate with each other)
v3.3.0,"if network parameters were changed during training, remove them from the"
v3.3.0,returned model so that they're generated dynamically on every run based
v3.3.0,on the Dask cluster you're connected to and which workers have pieces of
v3.3.0,the training data
v3.3.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.3.0,"for multi-class classification with sparse matrices, pred_contrib predictions"
v3.3.0,are returned as a list of sparse matrices (one per class)
v3.3.0,"pred_contrib output will have one column per feature,"
v3.3.0,plus one more for the base value
v3.3.0,need to tell Dask the expected type and shape of individual preds
v3.3.0,"by default, dask.array.concatenate() concatenates sparse arrays into a COO matrix"
v3.3.0,the code below is used instead to ensure that the sparse type is preserved during concatentation
v3.3.0,"At this point, `out` is a list of lists of delayeds (each of which points to a matrix)."
v3.3.0,Concatenate them to return a list of Dask Arrays.
v3.3.0,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.0,currently relevant for the Dask estimators
v3.3.0,"DaskLGBMClassifier does not support group, eval_group, early_stopping_rounds."
v3.3.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.3.0,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.0,currently relevant for the Dask estimators
v3.3.0,"DaskLGBMRegressor does not support group, eval_class_weight, eval_group, early_stopping_rounds."
v3.3.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.3.0,the note on custom objective functions in LGBMModel.__init__ is not
v3.3.0,currently relevant for the Dask estimators
v3.3.0,DaskLGBMRanker does not support eval_class_weight or early stopping
v3.3.0,DaskLGBMRanker support for callbacks and init_model is not tested
v3.3.0,coding: utf-8
v3.3.0,load or create your dataset
v3.3.0,create dataset for lightgbm
v3.3.0,"if you want to re-use data, remember to set free_raw_data=False"
v3.3.0,specify your configurations as a dict
v3.3.0,generate feature names
v3.3.0,feature_name and categorical_feature
v3.3.0,check feature name
v3.3.0,save model to file
v3.3.0,dump model to JSON (and save to file)
v3.3.0,feature names
v3.3.0,feature importances
v3.3.0,load model to predict
v3.3.0,can only predict with the best iteration (or the saving iteration)
v3.3.0,eval with loaded model
v3.3.0,dump model with pickle
v3.3.0,load model with pickle to predict
v3.3.0,can predict with any iteration when loaded in pickle way
v3.3.0,eval with loaded model
v3.3.0,continue training
v3.3.0,init_model accepts:
v3.3.0,1. model file name
v3.3.0,2. Booster()
v3.3.0,decay learning rates
v3.3.0,learning_rates accepts:
v3.3.0,1. list/tuple with length = num_boost_round
v3.3.0,2. function(curr_iter)
v3.3.0,change other parameters during training
v3.3.0,self-defined objective function
v3.3.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.3.0,log likelihood loss
v3.3.0,self-defined eval metric
v3.3.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.0,binary error
v3.3.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.0,This may make built-in evaluation metric calculate wrong results
v3.3.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.0,Keep this in mind when you use the customization
v3.3.0,another self-defined eval metric
v3.3.0,"f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.0,accuracy
v3.3.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.3.0,This may make built-in evaluation metric calculate wrong results
v3.3.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.3.0,Keep this in mind when you use the customization
v3.3.0,callback
v3.3.0,coding: utf-8
v3.3.0,load or create your dataset
v3.3.0,train
v3.3.0,predict
v3.3.0,eval
v3.3.0,feature importances
v3.3.0,self-defined eval metric
v3.3.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.0,Root Mean Squared Logarithmic Error (RMSLE)
v3.3.0,train
v3.3.0,another self-defined eval metric
v3.3.0,"f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool"
v3.3.0,Relative Absolute Error (RAE)
v3.3.0,train
v3.3.0,predict
v3.3.0,eval
v3.3.0,other scikit-learn modules
v3.3.0,coding: utf-8
v3.3.0,load or create your dataset
v3.3.0,create dataset for lightgbm
v3.3.0,specify your configurations as a dict
v3.3.0,train
v3.3.0,coding: utf-8
v3.3.0,################
v3.3.0,Simulate some binary data with a single categorical and
v3.3.0,single continuous predictor
v3.3.0,################
v3.3.0,Set up a couple of utilities for our experiments
v3.3.0,################
v3.3.0,Observe the behavior of `binary` and `xentropy` objectives
v3.3.0,Trying this throws an error on non-binary values of y:
v3.3.0,"experiment('binary', label_type='probability', DATA)"
v3.3.0,The speed of `binary` is not drastically different than
v3.3.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.3.0,there are reasons to suspect that `binary` should run faster when the
v3.3.0,label is an integer instead of a float
v3.3.0,coding: utf-8
v3.3.0,load or create your dataset
v3.3.0,create dataset for lightgbm
v3.3.0,specify your configurations as a dict
v3.3.0,train
v3.3.0,save model to file
v3.3.0,predict
v3.3.0,eval
v3.3.0,We can also open HDF5 file once and get access to
v3.3.0,"With binary dataset created, we can use either Python API or cmdline version to train."
v3.3.0,
v3.3.0,"Note: in order to create exactly the same dataset with the one created in simple_example.py, we need"
v3.3.0,to modify simple_example.py to pass numpy array instead of pandas DataFrame to Dataset constructor.
v3.3.0,The reason is that DataFrame column names will be used in Dataset. For a DataFrame with Int64Index
v3.3.0,"as columns, Dataset will use column names like [""0"", ""1"", ""2"", ...]. While for numpy array, column names"
v3.3.0,"are using the default one assigned in C++ code (dataset_loader.cpp), like [""Column_0"", ""Column_1"", ...]."
v3.3.0,Y has a single column and we read it in single shot. So store it as an 1-d array.
v3.3.0,We use random access for data sampling when creating LightGBM Dataset from Sequence.
v3.3.0,"When accessing any element in a HDF5 chunk, it's read entirely."
v3.3.0,"To save I/O for sampling, we should keep number of total chunks much larger than sample count."
v3.3.0,Here we are just creating a chunk size that matches with batch_size.
v3.3.0,
v3.3.0,Also note that the data is stored in row major order to avoid extra copy when passing to
v3.3.0,lightgbm Dataset.
v3.3.0,Save to 2 HDF5 files for demonstration.
v3.3.0,We can store multiple datasets inside a single HDF5 file.
v3.3.0,Separating X and Y for choosing best chunk size for data loading.
v3.3.0,split training data into two partitions
v3.3.0,make this array dense because we're splitting across
v3.3.0,a sparse boundary to partition the data
v3.3.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.3.0,predictions and target values back from workers to the client
v3.3.0,
v3.3.0,"for larger datasets, consider the metrics from dask-ml instead"
v3.3.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.3.0,coding: utf-8
v3.3.0,!/usr/bin/env python3
v3.3.0,-*- coding: utf-8 -*-
v3.3.0,
v3.3.0,"LightGBM documentation build configuration file, created by"
v3.3.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.3.0,
v3.3.0,This file is execfile()d with the current directory set to its
v3.3.0,containing dir.
v3.3.0,
v3.3.0,Note that not all possible configuration values are present in this
v3.3.0,autogenerated file.
v3.3.0,
v3.3.0,All configuration values have a default; values that are commented out
v3.3.0,serve to show the default.
v3.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.3.0,add these directories to sys.path here. If the directory is relative to the
v3.3.0,"documentation root, use os.path.abspath to make it absolute."
v3.3.0,-- mock out modules
v3.3.0,-- General configuration ------------------------------------------------
v3.3.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.3.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.3.0,ones.
v3.3.0,hide type hints in API docs
v3.3.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.3.0,Only the class' docstring is inserted.
v3.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.3.0,The master toctree document.
v3.3.0,General information about the project.
v3.3.0,The name of an image file (relative to this directory) to place at the top
v3.3.0,of the sidebar.
v3.3.0,The name of an image file (relative to this directory) to use as a favicon of
v3.3.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.3.0,pixels large.
v3.3.0,"The version info for the project you're documenting, acts as replacement for"
v3.3.0,"|version| and |release|, also used in various other places throughout the"
v3.3.0,built documents.
v3.3.0,The short X.Y version.
v3.3.0,"The full version, including alpha/beta/rc tags."
v3.3.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.3.0,for a list of supported languages.
v3.3.0,
v3.3.0,This is also used if you do content translation via gettext catalogs.
v3.3.0,"Usually you set ""language"" from the command line for these cases."
v3.3.0,"List of patterns, relative to source directory, that match files and"
v3.3.0,directories to ignore when looking for source files.
v3.3.0,This patterns also effect to html_static_path and html_extra_path
v3.3.0,The name of the Pygments (syntax highlighting) style to use.
v3.3.0,-- Configuration for C API docs generation ------------------------------
v3.3.0,-- Options for HTML output ----------------------------------------------
v3.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.3.0,a list of builtin themes.
v3.3.0,Theme options are theme-specific and customize the look and feel of a theme
v3.3.0,"further.  For a list of options available for each theme, see the"
v3.3.0,documentation.
v3.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.3.0,"relative to this directory. They are copied after the builtin static files,"
v3.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.3.0,-- Options for HTMLHelp output ------------------------------------------
v3.3.0,Output file base name for HTML help builder.
v3.3.0,-- Options for LaTeX output ---------------------------------------------
v3.3.0,The name of an image file (relative to this directory) to place at the top of
v3.3.0,the title page.
v3.3.0,Warning! The following code can cause buffer overflows on RTD.
v3.3.0,Consider suppressing output completely if RTD project silently fails.
v3.3.0,Refer to https://github.com/svenevs/exhale
v3.3.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.0,Warning! The following code can cause buffer overflows on RTD.
v3.3.0,Consider suppressing output completely if RTD project silently fails.
v3.3.0,Refer to https://github.com/svenevs/exhale
v3.3.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.3.0,coding: utf-8
v3.3.0,This is a basic test for floating number parsing.
v3.3.0,Most of the test cases come from:
v3.3.0,https://github.com/dmlc/xgboost/blob/master/tests/cpp/common/test_charconv.cc
v3.3.0,https://github.com/Alexhuszagh/rust-lexical/blob/master/data/test-parse-unittests/strtod_tests.toml
v3.3.0,FLT_MAX
v3.3.0,FLT_MIN
v3.3.0,DBL_MAX (1 + (1 - 2^-52)) * 2^1023 = (2^53 - 1) * 2^971
v3.3.0,2^971 * (2^53 - 1 + 1/2) : the smallest number resolving to inf
v3.3.0,Near DBL_MIN
v3.3.0,DBL_MIN 2^-1022
v3.3.0,The behavior for parsing -nan depends on implementation.
v3.3.0,Thus we skip binary check for negative nan.
v3.3.0,See comment in test_cases.
v3.3.0,Constants
v3.3.0,Start with some content:
v3.3.0,Clear & re-use:
v3.3.0,Output should match new content:
v3.3.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.3.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.3.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.3.0,Compute a new trial address & value & if it is a valid address:
v3.3.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.3.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.3.0,Check the just-stored value with getitem():
v3.3.0,Also store the just-stored value for future tracking:
v3.3.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.3.0,Test in 2 ways that the values are correctly laid out in memory:
v3.3.0,coding: utf-8
v3.3.0,we don't need lib_lightgbm while building docs
v3.3.0,coding: utf-8
v3.3.0,check saved model persistence
v3.3.0,"we need to check the consistency of model file here, so test for exact equal"
v3.3.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.3.0,"scores likely to be different, but prediction should still be the same"
v3.3.0,test that shape is checked during prediction
v3.3.0,"The simple implementation is just a single ""return self.ndarray[idx]"""
v3.3.0,The following is for demo and testing purpose.
v3.3.0,whole col
v3.3.0,half col
v3.3.0,Create dataset from numpy array directly.
v3.3.0,Create dataset using Sequence.
v3.3.0,Test for validation set.
v3.3.0,Select some random rows as valid data.
v3.3.0,"From Dataset constructor, with dataset from numpy array."
v3.3.0,"From Dataset.create_valid, with dataset from sequence."
v3.3.0,test that method works even with free_raw_data=True
v3.3.0,test that method works but sets raw data to None in case of immergeable data types
v3.3.0,test that method works for different data types
v3.3.0,"Set extremely harsh penalties, so CEGB will block most splits."
v3.3.0,"Compare pairs of penalties, to ensure scaling works as intended"
v3.3.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.3.0,"should resolve duplicate aliases, and prefer the main parameter"
v3.3.0,should choose a value from an alias and set that value on main param
v3.3.0,if only an alias is used
v3.3.0,should use the default if main param and aliases are missing
v3.3.0,all changes should be made on copies and not modify the original
v3.3.0,coding: utf-8
v3.3.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.3.0,set_index ensures partitions are based on group id.
v3.3.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.3.0,"separate target, weight from features."
v3.3.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.3.0,"so that within each partition, sum(g) = n_samples."
v3.3.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.3.0,make one categorical feature relevant to the target
v3.3.0,https://github.com/microsoft/LightGBM/issues/4118
v3.3.0,extra predict() parameters should be passed through correctly
v3.3.0,pref_leaf values should have the right shape
v3.3.0,and values that look like valid tree nodes
v3.3.0,"be sure LightGBM actually used at least one categorical column,"
v3.3.0,and that it was correctly treated as a categorical feature
v3.3.0,shape depends on whether it is binary or multiclass classification
v3.3.0,"in the special case of multi-class classification using scipy sparse matrices,"
v3.3.0,"the output of `.predict(..., pred_contrib=True)` is a list of sparse matrices (one per class)"
v3.3.0,
v3.3.0,"since that case is so different than all other cases, check the relevant things here"
v3.3.0,and then return early
v3.3.0,"raw scores will probably be different, but at least check that all predicted classes are the same"
v3.3.0,"be sure LightGBM actually used at least one categorical column,"
v3.3.0,and that it was correctly treated as a categorical feature
v3.3.0,* shape depends on whether it is binary or multiclass classification
v3.3.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.3.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.3.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.0,that the output has the right shape and base values are in the right position
v3.3.0,check that found ports are different for same address (LocalCluster)
v3.3.0,check that the ports are indeed open
v3.3.0,Scores should be the same
v3.3.0,Predictions should be roughly the same.
v3.3.0,pref_leaf values should have the right shape
v3.3.0,and values that look like valid tree nodes
v3.3.0,extra predict() parameters should be passed through correctly
v3.3.0,"be sure LightGBM actually used at least one categorical column,"
v3.3.0,and that it was correctly treated as a categorical feature
v3.3.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.3.0,that the output has the right shape and base values are in the right position
v3.3.0,"be sure LightGBM actually used at least one categorical column,"
v3.3.0,and that it was correctly treated as a categorical feature
v3.3.0,Quantiles should be right
v3.3.0,"be sure LightGBM actually used at least one categorical column,"
v3.3.0,and that it was correctly treated as a categorical feature
v3.3.0,rebalance small dask.Array dataset for better performance.
v3.3.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.3.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.3.0,distributed ranker should be able to rank decently well and should
v3.3.0,have high rank correlation with scores from serial ranker.
v3.3.0,extra predict() parameters should be passed through correctly
v3.3.0,pref_leaf values should have the right shape
v3.3.0,and values that look like valid tree nodes
v3.3.0,"be sure LightGBM actually used at least one categorical column,"
v3.3.0,and that it was correctly treated as a categorical feature
v3.3.0,"Use larger trainset to prevent premature stopping due to zero loss, causing num_trees() < n_estimators."
v3.3.0,Use small chunk_size to avoid single-worker allocation of eval data partitions.
v3.3.0,"test eval_class_weight, eval_init_score on binary-classification task."
v3.3.0,Note: objective's default `metric` will be evaluated in evals_result_ in addition to all eval_metrics.
v3.3.0,create eval_sets by creating new datasets or copying training data.
v3.3.0,total number of trees scales up for ova classifier.
v3.3.0,check that early stopping was not applied.
v3.3.0,checks that evals_result_ and best_score_ contain expected data and eval_set names.
v3.3.0,"check that each eval_name and metric exists for all eval sets, allowing for the"
v3.3.0,case when a worker receives a fully-padded eval_set component which is not evaluated.
v3.3.0,should be able to use the class without specifying a client
v3.3.0,should be able to set client after construction
v3.3.0,data on cluster1
v3.3.0,create identical data on cluster2
v3.3.0,"at this point, the result of default_client() is client2 since it was the most recently"
v3.3.0,created. So setting client to client1 here to test that you can select a non-default client
v3.3.0,"unfitted model should survive pickling round trip, and pickling"
v3.3.0,shouldn't have side effects on the model object
v3.3.0,client will always be None after unpickling
v3.3.0,"fitted model should survive pickling round trip, and pickling"
v3.3.0,shouldn't have side effects on the model object
v3.3.0,client will always be None after unpickling
v3.3.0,rebalance data to be sure that each worker has a piece of the data
v3.3.0,model 1 - no network parameters given
v3.3.0,model 2 - machines given
v3.3.0,model 3 - local_listen_port given
v3.3.0,training should fail because LightGBM will try to use the same
v3.3.0,port for multiple worker processes on the same machine
v3.3.0,rebalance data to be sure that each worker has a piece of the data
v3.3.0,"test that ""machines"" is actually respected by creating a socket that uses"
v3.3.0,"one of the ports mentioned in ""machines"""
v3.3.0,The above error leaves a worker waiting
v3.3.0,"an informative error should be raised if ""machines"" has duplicates"
v3.3.0,"""client"" should be the only different, and the final argument"
v3.3.0,value of the root node is 0 when init_score is set
v3.3.0,this test is separate because it takes a not-yet-constructed estimator
v3.3.0,coding: utf-8
v3.3.0,coding: utf-8
v3.3.0,"build target, group ID vectors."
v3.3.0,build y/target and group-id vectors with user-specified group sizes.
v3.3.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.3.0,groups should contain > 1 element for pairwise learning objective.
v3.3.0,"build feature data, X. Transform first few into informative features."
v3.3.0,coding: utf-8
v3.3.0,prediction result is actually not transformed (is raw) due to custom objective
v3.3.0,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.3.0,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.3.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.0,sklearn < 0.23 does not have as_frame parameter
v3.3.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.3.0,sklearn < 0.23 does not have as_frame parameter
v3.3.0,Test if random_state is properly stored
v3.3.0,Test if two random states produce identical models
v3.3.0,Test if subsequent fits sample from random_state object and produce different models
v3.3.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.3.0,With default params
v3.3.0,Tests same probabilities
v3.3.0,Tests same predictions
v3.3.0,Tests same raw scores
v3.3.0,Tests same leaf indices
v3.3.0,Tests same feature contributions
v3.3.0,Tests other parameters for the prediction works
v3.3.0,Tests start_iteration
v3.3.0,"Tests same probabilities, starting from iteration 10"
v3.3.0,"Tests same predictions, starting from iteration 10"
v3.3.0,"Tests same raw scores, starting from iteration 10"
v3.3.0,"Tests same leaf indices, starting from iteration 10"
v3.3.0,"Tests same feature contributions, starting from iteration 10"
v3.3.0,"Tests other parameters for the prediction works, starting from iteration 10"
v3.3.0,"no custom objective, no custom metric"
v3.3.0,default metric
v3.3.0,non-default metric
v3.3.0,no metric
v3.3.0,non-default metric in eval_metric
v3.3.0,non-default metric with non-default metric in eval_metric
v3.3.0,non-default metric with multiple metrics in eval_metric
v3.3.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.3.0,default metric for non-default objective
v3.3.0,non-default metric for non-default objective
v3.3.0,no metric
v3.3.0,non-default metric in eval_metric for non-default objective
v3.3.0,non-default metric with non-default metric in eval_metric for non-default objective
v3.3.0,non-default metric with multiple metrics in eval_metric for non-default objective
v3.3.0,"custom objective, no custom metric"
v3.3.0,default regression metric for custom objective
v3.3.0,non-default regression metric for custom objective
v3.3.0,multiple regression metrics for custom objective
v3.3.0,no metric
v3.3.0,default regression metric with non-default metric in eval_metric for custom objective
v3.3.0,non-default regression metric with metric in eval_metric for custom objective
v3.3.0,multiple regression metrics with metric in eval_metric for custom objective
v3.3.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.3.0,"no custom objective, custom metric"
v3.3.0,default metric with custom metric
v3.3.0,non-default metric with custom metric
v3.3.0,multiple metrics with custom metric
v3.3.0,custom metric (disable default metric)
v3.3.0,default metric for non-default objective with custom metric
v3.3.0,non-default metric for non-default objective with custom metric
v3.3.0,multiple metrics for non-default objective with custom metric
v3.3.0,custom metric (disable default metric for non-default objective)
v3.3.0,"custom objective, custom metric"
v3.3.0,custom metric for custom objective
v3.3.0,non-default regression metric with custom metric for custom objective
v3.3.0,multiple regression metrics with custom metric for custom objective
v3.3.0,default metric and invalid binary metric is replaced with multiclass alternative
v3.3.0,invalid objective is replaced with default multiclass one
v3.3.0,and invalid binary metric is replaced with multiclass alternative
v3.3.0,default metric for non-default multiclass objective
v3.3.0,and invalid binary metric is replaced with multiclass alternative
v3.3.0,default metric and invalid multiclass metric is replaced with binary alternative
v3.3.0,invalid multiclass metric is replaced with binary alternative for custom objective
v3.3.0,"Verify that can receive a list of metrics, only callable"
v3.3.0,Verify that can receive a list of custom and built-in metrics
v3.3.0,Verify that works as expected when eval_metric is empty
v3.3.0,"Verify that can receive a list of metrics, only built-in"
v3.3.0,Verify that eval_metric is robust to receiving a list with None
v3.3.0,training data as eval_set
v3.3.0,feval
v3.3.0,single eval_set
v3.3.0,two eval_set
v3.3.0,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.3.0,Test that estimators are default-constructible
v3.3.0,coding: utf-8
v3.3.0,coding: utf-8
v3.3.0,check that default gives same result as k = 1
v3.3.0,check against independent calculation for k = 1
v3.3.0,check against independent calculation for k = 2
v3.3.0,check against independent calculation for k = 10
v3.3.0,check cases where predictions are equal
v3.3.0,should give same result as binary auc for 2 classes
v3.3.0,test the case where all predictions are equal
v3.3.0,test that weighted data gives different auc_mu
v3.3.0,test that equal data weights give same auc_mu as unweighted data
v3.3.0,should give 1 when accuracy = 1
v3.3.0,test loading class weights
v3.3.0,no early stopping
v3.3.0,early stopping occurs
v3.3.0,test custom eval metrics
v3.3.0,"shuffle = False, override metric in params"
v3.3.0,"shuffle = True, callbacks"
v3.3.0,enable display training loss
v3.3.0,self defined folds
v3.3.0,LambdaRank
v3.3.0,... with l2 metric
v3.3.0,... with NDCG (default) metric
v3.3.0,self defined folds with lambdarank
v3.3.0,with early stopping
v3.3.0,predict by each fold booster
v3.3.0,fold averaging
v3.3.0,without early stopping
v3.3.0,test feature_names with whitespaces
v3.3.0,This has non-ascii strings.
v3.3.0,take subsets and train
v3.3.0,generate CSR sparse dataset
v3.3.0,convert data to dense and get back same contribs
v3.3.0,validate the values are the same
v3.3.0,validate using CSC matrix
v3.3.0,validate the values are the same
v3.3.0,generate CSR sparse dataset
v3.3.0,convert data to dense and get back same contribs
v3.3.0,validate the values are the same
v3.3.0,validate using CSC matrix
v3.3.0,validate the values are the same
v3.3.0,Note there is an extra column added to the output for the expected value
v3.3.0,Note output CSC shape should be same as CSR output shape
v3.3.0,test sliced labels
v3.3.0,append some columns
v3.3.0,append some rows
v3.3.0,test sliced 2d matrix
v3.3.0,test sliced CSR
v3.3.0,trees start at position 1.
v3.3.0,split_features are in 4th line.
v3.3.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.3.0,The penalization is so high that the first 2 features should not be used here
v3.3.0,Check that a very high penalization is the same as not using the features at all
v3.3.0,"no fobj, no feval"
v3.3.0,default metric
v3.3.0,non-default metric in params
v3.3.0,default metric in args
v3.3.0,non-default metric in args
v3.3.0,metric in args overwrites one in params
v3.3.0,multiple metrics in params
v3.3.0,multiple metrics in args
v3.3.0,remove default metric by 'None' in list
v3.3.0,remove default metric by 'None' aliases
v3.3.0,"fobj, no feval"
v3.3.0,no default metric
v3.3.0,metric in params
v3.3.0,metric in args
v3.3.0,metric in args overwrites its' alias in params
v3.3.0,multiple metrics in params
v3.3.0,multiple metrics in args
v3.3.0,"no fobj, feval"
v3.3.0,default metric with custom one
v3.3.0,non-default metric in params with custom one
v3.3.0,default metric in args with custom one
v3.3.0,non-default metric in args with custom one
v3.3.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.0,multiple metrics in params with custom one
v3.3.0,multiple metrics in args with custom one
v3.3.0,custom metric is evaluated despite 'None' is passed
v3.3.0,"fobj, feval"
v3.3.0,"no default metric, only custom one"
v3.3.0,metric in params with custom one
v3.3.0,metric in args with custom one
v3.3.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.3.0,multiple metrics in params with custom one
v3.3.0,multiple metrics in args with custom one
v3.3.0,custom metric is evaluated despite 'None' is passed
v3.3.0,"no fobj, no feval"
v3.3.0,default metric
v3.3.0,default metric in params
v3.3.0,non-default metric in params
v3.3.0,multiple metrics in params
v3.3.0,remove default metric by 'None' aliases
v3.3.0,"fobj, no feval"
v3.3.0,no default metric
v3.3.0,metric in params
v3.3.0,multiple metrics in params
v3.3.0,"no fobj, feval"
v3.3.0,default metric with custom one
v3.3.0,default metric in params with custom one
v3.3.0,non-default metric in params with custom one
v3.3.0,multiple metrics in params with custom one
v3.3.0,custom metric is evaluated despite 'None' is passed
v3.3.0,"fobj, feval"
v3.3.0,"no default metric, only custom one"
v3.3.0,metric in params with custom one
v3.3.0,multiple metrics in params with custom one
v3.3.0,custom metric is evaluated despite 'None' is passed
v3.3.0,multiclass default metric
v3.3.0,multiclass default metric with custom one
v3.3.0,multiclass metric alias with custom one for custom objective
v3.3.0,no metric for invalid class_num
v3.3.0,custom metric for invalid class_num
v3.3.0,multiclass metric alias with custom one with invalid class_num
v3.3.0,multiclass default metric without num_class
v3.3.0,multiclass metric alias
v3.3.0,multiclass metric
v3.3.0,non-valid metric for multiclass objective
v3.3.0,non-default num_class for default objective
v3.3.0,no metric with non-default num_class for custom objective
v3.3.0,multiclass metric alias for custom objective
v3.3.0,multiclass metric for custom objective
v3.3.0,binary metric with non-default num_class for custom objective
v3.3.0,Expect three metrics but mean and stdv for each metric
v3.3.0,test XGBoost-style return value
v3.3.0,test numpy-style return value
v3.3.0,test bins string type
v3.3.0,test histogram is disabled for categorical features
v3.3.0,test for lgb.train
v3.3.0,test feval for lgb.train
v3.3.0,test with two valid data for lgb.train
v3.3.0,test for lgb.cv
v3.3.0,test feval for lgb.cv
v3.3.0,test that binning works properly for features with only positive or only negative values
v3.3.0,decreasing without freeing raw data is allowed
v3.3.0,decreasing before lazy init is allowed
v3.3.0,increasing is allowed
v3.3.0,decreasing with disabled filter is allowed
v3.3.0,decreasing with enabled filter is disallowed;
v3.3.0,also changes of other params are disallowed
v3.3.0,check extra trees increases regularization
v3.3.0,check path smoothing increases regularization
v3.3.0,test edge case with one leaf
v3.3.0,check that constraint containing all features is equivalent to no constraint
v3.3.0,check that constraint partitioning the features reduces train accuracy
v3.3.0,check that constraints consisting of single features reduce accuracy further
v3.3.0,test that interaction constraints work when not all features are used
v3.3.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.3.0,test again with nans in data
v3.3.0,test again with bagging
v3.3.0,test with a feature that has only one non-nan value
v3.3.0,test with a categorical feature
v3.3.0,test refit: same results on same data
v3.3.0,test refit with save and load
v3.3.0,test refit: different results training on different data
v3.3.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.3.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.3.0,"test the case where start_iteration <= 0, and num_iteration is None"
v3.3.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.3.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.3.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.3.0,test for regression
v3.3.0,test both with and without early stopping
v3.3.0,test for multi-class
v3.3.0,test both with and without early stopping
v3.3.0,test for binary
v3.3.0,test both with and without early stopping
v3.3.0,test against sklearn average precision metric
v3.3.0,test that average precision is 1 where model predicts perfectly
v3.3.0,coding: utf-8
v3.3.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.3.0,coding: utf-8
v3.3.0,coding: utf-8
v3.3.0,These are helper functions to allow doing a stack unwind
v3.3.0,"after an R allocation error, which would trigger a long jump."
v3.3.0,convert from one-based to zero-based index
v3.3.0,"if any feature names were larger than allocated size,"
v3.3.0,allow for a larger size and try again
v3.3.0,convert from boundaries to size
v3.3.0,--- start Booster interfaces
v3.3.0,"if any eval names were larger than allocated size,"
v3.3.0,allow for a larger size and try again
v3.3.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.0,"if the model string was larger than the initial buffer, allocate a bigger buffer and try again"
v3.3.0,.Call() calls
v3.3.0,coding: utf-8
v3.3.0,alias table
v3.3.0,names
v3.3.0,from strings
v3.3.0,tails
v3.3.0,tails
v3.3.0,coding: utf-8
v3.3.0,Single row predictor to abstract away caching logic
v3.3.0,create boosting
v3.3.0,initialize the boosting
v3.3.0,create objective function
v3.3.0,initialize the objective function
v3.3.0,create training metric
v3.3.0,reset the boosting
v3.3.0,create objective function
v3.3.0,initialize the objective function
v3.3.0,calculate the nonzero data and indices size
v3.3.0,allocate data and indices arrays
v3.3.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.0,aggregated per row feature contribution results
v3.3.0,keep track of the row_vector sizes for parallelization
v3.3.0,copy vector results to output for each row
v3.3.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.3.0,aggregated per row feature contribution results
v3.3.0,calculate number of elements per column to construct
v3.3.0,the CSC matrix with random access
v3.3.0,keep track of column counts
v3.3.0,keep track of beginning index for each column
v3.3.0,keep track of beginning index for each matrix
v3.3.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.3.0,store the row index
v3.3.0,update column count
v3.3.0,explicitly declare symbols from LightGBM namespace
v3.3.0,some help functions used to convert data
v3.3.0,Row iterator of on column for CSC matrix
v3.3.0,"return value at idx, only can access by ascent order"
v3.3.0,"return next non-zero pair, if index < 0, means no more data"
v3.3.0,start of c_api functions
v3.3.0,This API is to keep python binding's behavior the same with C++ implementation.
v3.3.0,"Sample count, random seed etc. should be provided in parameters."
v3.3.0,sample data first
v3.3.0,sample data first
v3.3.0,sample data first
v3.3.0,local buffer to re-use memory
v3.3.0,sample data first
v3.3.0,no more data
v3.3.0,---- start of booster
v3.3.0,Single row in row-major format:
v3.3.0,---- start of some help functions
v3.3.0,data is array of pointers to individual rows
v3.3.0,set number of threads for openmp
v3.3.0,check for alias
v3.3.0,read parameters from config file
v3.3.0,"remove str after ""#"""
v3.3.0,check for alias again
v3.3.0,load configs
v3.3.0,prediction is needed if using input initial model(continued train)
v3.3.0,need to continue training
v3.3.0,sync up random seed for data partition
v3.3.0,load Training data
v3.3.0,load data for distributed training
v3.3.0,load data for single machine
v3.3.0,need save binary file
v3.3.0,create training metric
v3.3.0,only when have metrics then need to construct validation data
v3.3.0,"Add validation data, if it exists"
v3.3.0,add
v3.3.0,need save binary file
v3.3.0,add metric for validation data
v3.3.0,output used time on each iteration
v3.3.0,need init network
v3.3.0,create boosting
v3.3.0,create objective function
v3.3.0,load training data
v3.3.0,initialize the objective function
v3.3.0,initialize the boosting
v3.3.0,add validation data into boosting
v3.3.0,convert model to if-else statement code
v3.3.0,create predictor
v3.3.0,Free memory
v3.3.0,create predictor
v3.3.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.3.0,counts for all labels
v3.3.0,"start from top label, and accumulate DCG"
v3.3.0,counts for all labels
v3.3.0,calculate k Max DCG by one pass
v3.3.0,get sorted indices by score
v3.3.0,calculate multi dcg by one pass
v3.3.0,wait for all client start up
v3.3.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.3.0,Instead we will handle finalize/abort for MPI in main().
v3.3.0,default set to -1
v3.3.0,"distance at k-th communication, distance[k] = 2^k"
v3.3.0,set incoming rank at k-th commuication
v3.3.0,set outgoing rank at k-th commuication
v3.3.0,default set as -1
v3.3.0,construct all recursive halving map for all machines
v3.3.0,let 1 << k <= num_machines
v3.3.0,distance of each communication
v3.3.0,"if num_machines = 2^k, don't need to group machines"
v3.3.0,"communication direction, %2 == 0 is positive"
v3.3.0,neighbor at k-th communication
v3.3.0,receive data block at k-th communication
v3.3.0,send data block at k-th communication
v3.3.0,"if num_machines != 2^k, need to group machines"
v3.3.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.3.0,let left machine as group leader
v3.3.0,"cache block information for groups, group with 2 machines will have double block size"
v3.3.0,convert from group to node leader
v3.3.0,convert from node to group
v3.3.0,meet new group
v3.3.0,add block len for this group
v3.3.0,calculate the group block start
v3.3.0,not need to construct
v3.3.0,get receive block information
v3.3.0,accumulate block len
v3.3.0,get send block information
v3.3.0,accumulate block len
v3.3.0,static member definition
v3.3.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.3.0,assign the blocks to every rank.
v3.3.0,do reduce scatter
v3.3.0,do all gather
v3.3.0,assign blocks
v3.3.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.3.0,copy back
v3.3.0,assign blocks
v3.3.0,start all gather
v3.3.0,when num_machines is small and data is large
v3.3.0,use output as receive buffer
v3.3.0,get current local block size
v3.3.0,get out rank
v3.3.0,get in rank
v3.3.0,get send information
v3.3.0,get recv information
v3.3.0,send and recv at same time
v3.3.0,rotate in-place
v3.3.0,use output as receive buffer
v3.3.0,get current local block size
v3.3.0,get send information
v3.3.0,get recv information
v3.3.0,send and recv at same time
v3.3.0,use output as receive buffer
v3.3.0,send and recv at same time
v3.3.0,send local data to neighbor first
v3.3.0,receive neighbor data first
v3.3.0,reduce
v3.3.0,get target
v3.3.0,get send information
v3.3.0,get recv information
v3.3.0,send and recv at same time
v3.3.0,reduce
v3.3.0,send result to neighbor
v3.3.0,receive result from neighbor
v3.3.0,copy result
v3.3.0,start up socket
v3.3.0,parse clients from file
v3.3.0,get ip list of local machine
v3.3.0,get local rank
v3.3.0,construct listener
v3.3.0,construct communication topo
v3.3.0,construct linkers
v3.3.0,free listener
v3.3.0,set timeout
v3.3.0,accept incoming socket
v3.3.0,receive rank
v3.3.0,add new socket
v3.3.0,save ranks that need to connect with
v3.3.0,start listener
v3.3.0,start connect
v3.3.0,let smaller rank connect to larger rank
v3.3.0,send local rank
v3.3.0,wait for listener
v3.3.0,print connected linkers
v3.3.0,only need to copy subset
v3.3.0,avoid to copy subset many times
v3.3.0,avoid out of range
v3.3.0,may need to recopy subset
v3.3.0,valid the type
v3.3.0,Constructors
v3.3.0,Get type tag
v3.3.0,Comparisons
v3.3.0,"This has to be separate, not in Statics, because Json() accesses"
v3.3.0,statics().null.
v3.3.0,"advance until next line, or end of input"
v3.3.0,advance until closing tokens
v3.3.0,The usual case: non-escaped characters
v3.3.0,Handle escapes
v3.3.0,Extract 4-byte escape sequence
v3.3.0,Explicitly check length of the substring. The following loop
v3.3.0,relies on std::string returning the terminating NUL when
v3.3.0,accessing str[length]. Checking here reduces brittleness.
v3.3.0,JSON specifies that characters outside the BMP shall be encoded as a
v3.3.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.3.0,components. Check whether we're in the middle of such a beast: the
v3.3.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.3.0,a trail (low) surrogate.
v3.3.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.3.0,per the UTF-16 algorithm.
v3.3.0,Integer part
v3.3.0,Decimal part
v3.3.0,Exponent part
v3.3.0,Check for any trailing garbage
v3.3.0,Documented in json11.hpp
v3.3.0,Check for another object
v3.3.0,get column names
v3.3.0,load label idx first
v3.3.0,erase label column name
v3.3.0,load ignore columns
v3.3.0,load weight idx
v3.3.0,load group idx
v3.3.0,don't support query id in data file when using distributed training
v3.3.0,read data to memory
v3.3.0,sample data
v3.3.0,construct feature bin mappers
v3.3.0,initialize label
v3.3.0,extract features
v3.3.0,sample data from file
v3.3.0,construct feature bin mappers
v3.3.0,initialize label
v3.3.0,extract features
v3.3.0,load data from binary file
v3.3.0,check meta data
v3.3.0,need to check training data
v3.3.0,read data in memory
v3.3.0,initialize label
v3.3.0,extract features
v3.3.0,Get number of lines of data file
v3.3.0,initialize label
v3.3.0,extract features
v3.3.0,load data from binary file
v3.3.0,not need to check validation data
v3.3.0,check meta data
v3.3.0,buffer to read binary file
v3.3.0,check token
v3.3.0,read size of header
v3.3.0,re-allocmate space if not enough
v3.3.0,read header
v3.3.0,get header
v3.3.0,num_groups
v3.3.0,real_feature_idx_
v3.3.0,feature2group
v3.3.0,feature2subfeature
v3.3.0,group_bin_boundaries
v3.3.0,group_feature_start_
v3.3.0,group_feature_cnt_
v3.3.0,get feature names
v3.3.0,write feature names
v3.3.0,get forced_bin_bounds_
v3.3.0,read size of meta data
v3.3.0,re-allocate space if not enough
v3.3.0,read meta data
v3.3.0,load meta data
v3.3.0,sample local used data if need to partition
v3.3.0,"if not contain query file, minimal sample unit is one record"
v3.3.0,"if contain query file, minimal sample unit is one query"
v3.3.0,if is new query
v3.3.0,read feature data
v3.3.0,read feature size
v3.3.0,re-allocate space if not enough
v3.3.0,raw data
v3.3.0,fill feature_names_ if not header
v3.3.0,get forced split
v3.3.0,"if only one machine, find bin locally"
v3.3.0,"if have multi-machines, need to find bin distributed"
v3.3.0,different machines will find bin for different features
v3.3.0,start and len will store the process feature indices for different machines
v3.3.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.0,free
v3.3.0,gather global feature bin mappers
v3.3.0,restore features bins from buffer
v3.3.0,---- private functions ----
v3.3.0,"if features are ordered, not need to use hist_buf"
v3.3.0,read all lines
v3.3.0,get query data
v3.3.0,"if not contain query data, minimal sample unit is one record"
v3.3.0,"if contain query data, minimal sample unit is one query"
v3.3.0,if is new query
v3.3.0,get query data
v3.3.0,"if not contain query file, minimal sample unit is one record"
v3.3.0,"if contain query file, minimal sample unit is one query"
v3.3.0,if is new query
v3.3.0,parse features
v3.3.0,get forced split
v3.3.0,"check the range of label_idx, weight_idx and group_idx"
v3.3.0,fill feature_names_ if not header
v3.3.0,start find bins
v3.3.0,"if only one machine, find bin locally"
v3.3.0,start and len will store the process feature indices for different machines
v3.3.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.3.0,free
v3.3.0,gather global feature bin mappers
v3.3.0,restore features bins from buffer
v3.3.0,if doesn't need to prediction with initial model
v3.3.0,parser
v3.3.0,set label
v3.3.0,free processed line:
v3.3.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.3.0,text_reader_->Lines()[i].shrink_to_fit();
v3.3.0,push data
v3.3.0,if is used feature
v3.3.0,if need to prediction with initial model
v3.3.0,parser
v3.3.0,set initial score
v3.3.0,set label
v3.3.0,free processed line:
v3.3.0,"shrink_to_fit will be very slow in Linux, and seems not free memory, disable for now"
v3.3.0,text_reader_->Lines()[i].shrink_to_fit();
v3.3.0,push data
v3.3.0,if is used feature
v3.3.0,metadata_ will manage space of init_score
v3.3.0,text data can be free after loaded feature values
v3.3.0,parser
v3.3.0,set initial score
v3.3.0,set label
v3.3.0,push data
v3.3.0,if is used feature
v3.3.0,only need part of data
v3.3.0,need full data
v3.3.0,metadata_ will manage space of init_score
v3.3.0,read size of token
v3.3.0,remove duplicates
v3.3.0,deep copy function for BinMapper
v3.3.0,mean size for one bin
v3.3.0,need a new bin
v3.3.0,update bin upper bound
v3.3.0,last bin upper bound
v3.3.0,get list of distinct values
v3.3.0,get number of positive and negative distinct values
v3.3.0,include zero bounds and infinity bound
v3.3.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.3.0,find remaining bounds
v3.3.0,find distinct_values first
v3.3.0,push zero in the front
v3.3.0,use the large value
v3.3.0,push zero in the back
v3.3.0,convert to int type first
v3.3.0,sort by counts
v3.3.0,will ignore the categorical of small counts
v3.3.0,Push the dummy bin for NaN
v3.3.0,Use MissingType::None to represent this bin contains all categoricals
v3.3.0,fix count of NaN bin
v3.3.0,check trivial(num_bin_ == 1) feature
v3.3.0,check useless bin
v3.3.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.3.0,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.3.0,calculate max bin of all features to select the int type in MultiValDenseBin
v3.3.0,"for lambdarank, it needs query data for partition data in distributed learning"
v3.3.0,need convert query_id to boundaries
v3.3.0,check weights
v3.3.0,check query boundries
v3.3.0,contain initial score file
v3.3.0,check weights
v3.3.0,get local weights
v3.3.0,check query boundries
v3.3.0,get local query boundaries
v3.3.0,contain initial score file
v3.3.0,get local initial scores
v3.3.0,re-load query weight
v3.3.0,save to nullptr
v3.3.0,save to nullptr
v3.3.0,save to nullptr
v3.3.0,default weight file name
v3.3.0,default init_score file name
v3.3.0,use first line to count number class
v3.3.0,default query file name
v3.3.0,root is in the depth 0
v3.3.0,non-leaf
v3.3.0,leaf
v3.3.0,use this for the missing value conversion
v3.3.0,Predict func by Map to ifelse
v3.3.0,use this for the missing value conversion
v3.3.0,non-leaf
v3.3.0,left subtree
v3.3.0,right subtree
v3.3.0,leaf
v3.3.0,non-leaf
v3.3.0,left subtree
v3.3.0,right subtree
v3.3.0,leaf
v3.3.0,recursive computation of SHAP values for a decision tree
v3.3.0,extend the unique path
v3.3.0,leaf node
v3.3.0,internal node
v3.3.0,"see if we have already split on this feature,"
v3.3.0,if so we undo that split so we can redo it for this node
v3.3.0,recursive sparse computation of SHAP values for a decision tree
v3.3.0,extend the unique path
v3.3.0,leaf node
v3.3.0,internal node
v3.3.0,"see if we have already split on this feature,"
v3.3.0,if so we undo that split so we can redo it for this node
v3.3.0,add names of objective function if not providing metric
v3.3.0,equal weights for all classes
v3.3.0,generate seeds by seed.
v3.3.0,sort eval_at
v3.3.0,Only push the non-training data
v3.3.0,check for conflicts
v3.3.0,"check if objective, metric, and num_class match"
v3.3.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.3.0,Check max_depth and num_leaves
v3.3.0,"Fits in an int, and is more restrictive than the current num_leaves"
v3.3.0,force col-wise for gpu & CUDA
v3.3.0,force gpu_use_dp for CUDA
v3.3.0,linear tree learner must be serial type and run on CPU device
v3.3.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.3.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.3.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.3.0,calculated split gain can be positive even with zero gradient and hessian.
v3.3.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.3.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.3.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.3.0,first round: fill the single val group
v3.3.0,always push the last group
v3.3.0,put dense feature first
v3.3.0,sort by non zero cnt
v3.3.0,"sort by non zero cnt, bigger first"
v3.3.0,shuffle groups
v3.3.0,Using std::swap for vector<bool> will cause the wrong result.
v3.3.0,get num_features
v3.3.0,get bin_mappers
v3.3.0,"for sparse multi value bin, we store the feature bin values with offset added"
v3.3.0,"for dense multi value bin, the feature bin values without offsets are used"
v3.3.0,copy feature bin mapper data
v3.3.0,copy feature bin mapper data
v3.3.0,"if not pass a filename, just append "".bin"" of original file"
v3.3.0,get size of header
v3.3.0,size of feature names
v3.3.0,size of forced bins
v3.3.0,write header
v3.3.0,write feature names
v3.3.0,write forced bins
v3.3.0,get size of meta data
v3.3.0,write meta data
v3.3.0,write feature data
v3.3.0,get size of feature
v3.3.0,write feature
v3.3.0,write raw data; use row-major order so we can read row-by-row
v3.3.0,"explicitly initialize template methods, for cross module call"
v3.3.0,"Only one multi-val group, just simply merge"
v3.3.0,Skip the leading 0 when copying group_bin_boundaries.
v3.3.0,regenerate other fields
v3.3.0,store the importance first
v3.3.0,PredictRaw
v3.3.0,PredictRawByMap
v3.3.0,Predict
v3.3.0,PredictByMap
v3.3.0,PredictLeafIndex
v3.3.0,PredictLeafIndexByMap
v3.3.0,output model type
v3.3.0,output number of class
v3.3.0,output label index
v3.3.0,output max_feature_idx
v3.3.0,output objective
v3.3.0,output tree models
v3.3.0,store the importance first
v3.3.0,sort the importance
v3.3.0,use serialized string to restore this object
v3.3.0,Use first 128 chars to avoid exceed the message buffer.
v3.3.0,get number of classes
v3.3.0,get index of label
v3.3.0,get max_feature_idx first
v3.3.0,get average_output
v3.3.0,get feature names
v3.3.0,get monotone_constraints
v3.3.0,set zero
v3.3.0,predict all the trees for one iteration
v3.3.0,check early stopping
v3.3.0,set zero
v3.3.0,predict all the trees for one iteration
v3.3.0,check early stopping
v3.3.0,margin_threshold will be captured by value
v3.3.0,copy and sort
v3.3.0,margin_threshold will be captured by value
v3.3.0,Fix for compiler warnings about reaching end of control
v3.3.0,load forced_splits file
v3.3.0,init tree learner
v3.3.0,push training metrics
v3.3.0,create buffer for gradients and Hessians
v3.3.0,get max feature index
v3.3.0,get label index
v3.3.0,get feature names
v3.3.0,"if need bagging, create buffer"
v3.3.0,"for a validation dataset, we need its score and metric"
v3.3.0,update score
v3.3.0,objective function will calculate gradients and hessians
v3.3.0,"random bagging, minimal unit is one record"
v3.3.0,"random bagging, minimal unit is one record"
v3.3.0,if need bagging
v3.3.0,set bagging data to tree learner
v3.3.0,get subset
v3.3.0,output used time per iteration
v3.3.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.3.0,boosting first
v3.3.0,bagging logic
v3.3.0,need to copy gradients for bagging subset.
v3.3.0,shrinkage by learning rate
v3.3.0,update score
v3.3.0,only add default score one-time
v3.3.0,updates scores
v3.3.0,add model
v3.3.0,reset score
v3.3.0,remove model
v3.3.0,print message for metric
v3.3.0,pop last early_stopping_round_ models
v3.3.0,update training score
v3.3.0,we need to predict out-of-bag scores of data for boosting
v3.3.0,update validation score
v3.3.0,print training metric
v3.3.0,print validation metric
v3.3.0,set zero
v3.3.0,predict all the trees for one iteration
v3.3.0,predict all the trees for one iteration
v3.3.0,push training metrics
v3.3.0,"not same training data, need reset score and others"
v3.3.0,create score tracker
v3.3.0,update score
v3.3.0,create buffer for gradients and hessians
v3.3.0,load forced_splits file
v3.3.0,"if need bagging, create buffer"
v3.3.0,Get the max size of pool
v3.3.0,at least need 2 leaves
v3.3.0,push split information for all leaves
v3.3.0,initialize splits for leaf
v3.3.0,initialize data partition
v3.3.0,initialize ordered gradients and hessians
v3.3.0,cannot change is_hist_col_wise during training
v3.3.0,initialize splits for leaf
v3.3.0,initialize data partition
v3.3.0,initialize ordered gradients and hessians
v3.3.0,Get the max size of pool
v3.3.0,at least need 2 leaves
v3.3.0,push split information for all leaves
v3.3.0,some initial works before training
v3.3.0,root leaf
v3.3.0,only root leaf can be splitted on first time
v3.3.0,some initial works before finding best split
v3.3.0,find best threshold for every feature
v3.3.0,Get a leaf with max split gain
v3.3.0,Get split information for best leaf
v3.3.0,"cannot split, quit"
v3.3.0,split tree with best leaf
v3.3.0,reset histogram pool
v3.3.0,initialize data partition
v3.3.0,reset the splits for leaves
v3.3.0,Sumup for root
v3.3.0,use all data
v3.3.0,"use bagging, only use part of data"
v3.3.0,check depth of current leaf
v3.3.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.3.0,no enough data to continue
v3.3.0,only have root
v3.3.0,put parent(left) leaf's histograms into larger leaf's histograms
v3.3.0,put parent(left) leaf's histograms to larger leaf's histograms
v3.3.0,construct smaller leaf
v3.3.0,construct larger leaf
v3.3.0,find splits
v3.3.0,only has root leaf
v3.3.0,start at root leaf
v3.3.0,"before processing next node from queue, store info for current left/right leaf"
v3.3.0,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.3.0,"then, compute own splits"
v3.3.0,split info should exist because searching in bfs fashion - should have added from parent
v3.3.0,update before tree split
v3.3.0,don't need to update this in data-based parallel model
v3.3.0,"split tree, will return right leaf"
v3.3.0,store the true split gain in tree model
v3.3.0,don't need to update this in data-based parallel model
v3.3.0,store the true split gain in tree model
v3.3.0,init the leaves that used on next iteration
v3.3.0,update leave outputs if needed
v3.3.0,bag_mapper[index_mapper[i]]
v3.3.0,it is needed to filter the features after the above code.
v3.3.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.3.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.3.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.3.0,find splits
v3.3.0,identify features containing nans
v3.3.0,preallocate the matrix used to calculate linear model coefficients
v3.3.0,"store only upper triangular half of matrix as an array, in row-major order"
v3.3.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.3.0,we add another 8 to ensure cache lines are not shared among processors
v3.3.0,some initial works before training
v3.3.0,root leaf
v3.3.0,only root leaf can be splitted on first time
v3.3.0,some initial works before finding best split
v3.3.0,find best threshold for every feature
v3.3.0,Get a leaf with max split gain
v3.3.0,Get split information for best leaf
v3.3.0,"cannot split, quit"
v3.3.0,split tree with best leaf
v3.3.0,map data to leaf number
v3.3.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.3.0,the coefficients vector is given by
v3.3.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.3.0,where:
v3.3.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.3.0,"H is the diagonal matrix of the hessian,"
v3.3.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.3.0,g is the vector of gradients
v3.3.0,the subscript _T denotes the transpose
v3.3.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.3.0,clear the coefficient matrices
v3.3.0,aggregate results from different threads
v3.3.0,copy into eigen matrices and solve
v3.3.0,update the tree properties
v3.3.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.0,get feature partition
v3.3.0,get local used features
v3.3.0,get best split at smaller leaf
v3.3.0,find local best split for larger leaf
v3.3.0,sync global best info
v3.3.0,update best split
v3.3.0,"instantiate template classes, otherwise linker cannot find the code"
v3.3.0,initialize SerialTreeLearner
v3.3.0,Get local rank and global machine size
v3.3.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.0,allocate buffer for communication
v3.3.0,generate feature partition for current tree
v3.3.0,get local used feature
v3.3.0,get block start and block len for reduce scatter
v3.3.0,get buffer_write_start_pos_
v3.3.0,get buffer_read_start_pos_
v3.3.0,sync global data sumup info
v3.3.0,global sumup reduce
v3.3.0,copy back
v3.3.0,set global sumup info
v3.3.0,init global data count in leaf
v3.3.0,clear histogram buffer before synchronizing
v3.3.0,otherwise histogram contents from the previous iteration will be sent
v3.3.0,construct local histograms
v3.3.0,copy to buffer
v3.3.0,Reduce scatter for histogram
v3.3.0,restore global histograms from buffer
v3.3.0,only root leaf
v3.3.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.3.0,find local best split for larger leaf
v3.3.0,sync global best info
v3.3.0,set best split
v3.3.0,need update global number of data in leaf
v3.3.0,"instantiate template classes, otherwise linker cannot find the code"
v3.3.0,initialize SerialTreeLearner
v3.3.0,some additional variables needed for GPU trainer
v3.3.0,Initialize GPU buffers and kernels
v3.3.0,some functions used for debugging the GPU histogram construction
v3.3.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.3.0,goto err;
v3.3.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.0,also guarantee that there are at least 2K examples per workgroup
v3.3.0,return 0;
v3.3.0,"we have already copied ordered gradients, ordered Hessians and indices to GPU"
v3.3.0,decide the best number of workgroups working on one feature4 tuple
v3.3.0,set work group size based on feature size
v3.3.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.0,we need to refresh the kernel arguments after reallocating
v3.3.0,The only argument that needs to be changed later is num_data_
v3.3.0,"the GPU kernel will process all features in one call, and each"
v3.3.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.3.0,process one feature4 tuple
v3.3.0,"for the root node, indices are not copied"
v3.3.0,"for constant hessian, hessians are not copied except for the root node"
v3.3.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.3.0,and we will launch num_feature workgroups for this kernel
v3.3.0,will launch threads for all features
v3.3.0,"the queue should be asynchronous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.3.0,copy the results asynchronously. Size depends on if double precision is used
v3.3.0,we will wait for this object in WaitAndGetHistograms
v3.3.0,"when the output is ready, the computation is done"
v3.3.0,values of this feature has been redistributed to multiple bins; need a reduction here
v3.3.0,how many feature-group tuples we have
v3.3.0,leave some safe margin for prefetching
v3.3.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.0,clear sparse/dense maps
v3.3.0,do nothing if no features can be processed on GPU
v3.3.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.3.0,unpin old buffer if necessary before destructing them
v3.3.0,"make ordered_gradients and Hessians larger (including extra room for prefetching), and pin them"
v3.3.0,allocate space for gradients and Hessians on device
v3.3.0,we will copy gradients and Hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.3.0,copy indices to the device
v3.3.0,histogram bin entry size depends on the precision (single/double)
v3.3.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.0,each work group generates a sub-histogram of dword_features_ features.
v3.3.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.0,create atomic counters for inter-group coordination
v3.3.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.0,looking for dword_features_ non-sparse feature-groups
v3.3.0,decide if we need to redistribute the bin
v3.3.0,multiplier must be a power of 2
v3.3.0,device_bin_mults_.push_back(1);
v3.3.0,found
v3.3.0,for data transfer time
v3.3.0,"Now generate new data structure feature4, and copy data to the device"
v3.3.0,"preallocate arrays for all threads, and pin them"
v3.3.0,building Feature4 bundles; each thread handles dword_features_ features
v3.3.0,one feature datapoint is 4 bits
v3.3.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.0,one feature datapoint is one byte
v3.3.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.3.0,Dense bin
v3.3.0,Dense 4-bit bin
v3.3.0,working on the remaining (less than dword_features_) feature groups
v3.3.0,fill the leftover features
v3.3.0,"fill this empty feature with some ""random"" value"
v3.3.0,"fill this empty feature with some ""random"" value"
v3.3.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.3.0,deallocate pinned space for feature copying
v3.3.0,data transfer time
v3.3.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.3.0,"Something bad happened. Just return ""No log available."""
v3.3.0,"build is okay, log may contain warnings"
v3.3.0,destroy any old kernels
v3.3.0,create OpenCL kernels for different number of workgroups per feature
v3.3.0,currently we don't use constant memory
v3.3.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.3.0,kernel with indices in an array
v3.3.0,"kernel with all features enabled, with eliminated branches"
v3.3.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.3.0,do nothing if no features can be processed on GPU
v3.3.0,The only argument that needs to be changed later is num_data_
v3.3.0,"hessian is passed as a parameter, but it is not available now."
v3.3.0,hessian will be set in BeforeTrain()
v3.3.0,"Get the max bin size, used for selecting best GPU kernel"
v3.3.0,initialize GPU
v3.3.0,determine which kernel to use based on the max number of bins
v3.3.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.3.0,setup GPU kernel arguments after we allocating all the buffers
v3.3.0,GPU memory has to been reallocated because data may have been changed
v3.3.0,setup GPU kernel arguments after we allocating all the buffers
v3.3.0,Copy initial full hessians and gradients to GPU.
v3.3.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.0,setup hessian parameters only
v3.3.0,hessian is passed as a parameter
v3.3.0,use bagging
v3.3.0,"On GPU, we start copying indices, gradients and Hessians now, instead at ConstructHistogram()"
v3.3.0,copy used gradients and Hessians to ordered buffer
v3.3.0,transfer the indices to GPU
v3.3.0,transfer hessian to GPU
v3.3.0,setup hessian parameters only
v3.3.0,hessian is passed as a parameter
v3.3.0,transfer gradients to GPU
v3.3.0,only have root
v3.3.0,"Copy indices, gradients and Hessians as early as possible"
v3.3.0,only need to initialize for smaller leaf
v3.3.0,Get leaf boundary
v3.3.0,copy indices to the GPU:
v3.3.0,copy ordered Hessians to the GPU:
v3.3.0,copy ordered gradients to the GPU:
v3.3.0,do nothing if no features can be processed on GPU
v3.3.0,copy data indices if it is not null
v3.3.0,generate and copy ordered_gradients if gradients is not null
v3.3.0,generate and copy ordered_hessians if Hessians is not null
v3.3.0,converted indices in is_feature_used to feature-group indices
v3.3.0,construct the feature masks for dense feature-groups
v3.3.0,"if no feature group is used, just return and do not use GPU"
v3.3.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.0,"All data have been prepared, now run the GPU kernel"
v3.3.0,construct smaller leaf
v3.3.0,ConstructGPUHistogramsAsync will return true if there are available feature groups dispatched to GPU
v3.3.0,then construct sparse features on CPU
v3.3.0,"wait for GPU to finish, only if GPU is actually used"
v3.3.0,use double precision
v3.3.0,use single precision
v3.3.0,"Compare GPU histogram with CPU histogram, useful for debugging GPU code problem"
v3.3.0,#define GPU_DEBUG_COMPARE
v3.3.0,construct larger leaf
v3.3.0,then construct sparse features on CPU
v3.3.0,"wait for GPU to finish, only if GPU is actually used"
v3.3.0,use double precision
v3.3.0,use single precision
v3.3.0,do some sanity check for the GPU algorithm
v3.3.0,limit top k
v3.3.0,get max bin
v3.3.0,calculate buffer size
v3.3.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.3.0,"left and right on same time, so need double size"
v3.3.0,initialize histograms for global
v3.3.0,sync global data sumup info
v3.3.0,set global sumup info
v3.3.0,init global data count in leaf
v3.3.0,get local sumup
v3.3.0,get local sumup
v3.3.0,get mean number on machines
v3.3.0,weighted gain
v3.3.0,get top k
v3.3.0,"Copy histogram to buffer, and Get local aggregate features"
v3.3.0,copy histograms.
v3.3.0,copy smaller leaf histograms first
v3.3.0,mark local aggregated feature
v3.3.0,copy
v3.3.0,then copy larger leaf histograms
v3.3.0,mark local aggregated feature
v3.3.0,copy
v3.3.0,use local data to find local best splits
v3.3.0,clear histogram buffer before synchronizing
v3.3.0,otherwise histogram contents from the previous iteration will be sent
v3.3.0,find splits
v3.3.0,only has root leaf
v3.3.0,local voting
v3.3.0,gather
v3.3.0,get all top-k from all machines
v3.3.0,global voting
v3.3.0,copy local histgrams to buffer
v3.3.0,Reduce scatter for histogram
v3.3.0,find best split from local aggregated histograms
v3.3.0,restore from buffer
v3.3.0,restore from buffer
v3.3.0,find local best
v3.3.0,find local best split for larger leaf
v3.3.0,sync global best info
v3.3.0,copy back
v3.3.0,set the global number of data for leaves
v3.3.0,init the global sumup info
v3.3.0,"instantiate template classes, otherwise linker cannot find the code"
v3.3.0,launch cuda kernel
v3.3.0,initialize SerialTreeLearner
v3.3.0,some additional variables needed for GPU trainer
v3.3.0,Initialize GPU buffers and kernels: get device info
v3.3.0,some functions used for debugging the GPU histogram construction
v3.3.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.3.0,also guarantee that there are at least 2K examples per workgroup
v3.3.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.3.0,decide the best number of workgroups working on one feature4 tuple
v3.3.0,set work group size based on feature size
v3.3.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.3.0,set thread_data
v3.3.0,copy the results asynchronously. Size depends on if double precision is used
v3.3.0,"when the output is ready, the computation is done"
v3.3.0,how many feature-group tuples we have
v3.3.0,leave some safe margin for prefetching
v3.3.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.3.0,clear sparse/dense maps
v3.3.0,do nothing it there is no dense feature
v3.3.0,calculate number of feature groups per gpu
v3.3.0,histogram bin entry size depends on the precision (single/double)
v3.3.0,allocate GPU memory for each GPU
v3.3.0,do nothing it there is no gpu feature
v3.3.0,allocate memory for all features
v3.3.0,allocate space for gradients and hessians on device
v3.3.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.3.0,copy indices to the device
v3.3.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.3.0,each work group generates a sub-histogram of dword_features_ features.
v3.3.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.3.0,create atomic counters for inter-group coordination
v3.3.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.3.0,clear sparse/dense maps
v3.3.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.3.0,set device info
v3.3.0,looking for dword_features_ non-sparse feature-groups
v3.3.0,reset device info
v3.3.0,InitGPU w/ num_gpu
v3.3.0,"Get the max bin size, used for selecting best GPU kernel"
v3.3.0,get num_dense_feature_groups_
v3.3.0,initialize GPU
v3.3.0,set cpu threads
v3.3.0,resize device memory pointers
v3.3.0,create stream & events to handle multiple GPUs
v3.3.0,check data size
v3.3.0,GPU memory has to been reallocated because data may have been changed
v3.3.0,AllocateGPUMemory only when the number of data increased
v3.3.0,setup GPU kernel arguments after we allocating all the buffers
v3.3.0,Copy initial full hessians and gradients to GPU.
v3.3.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.3.0,use bagging
v3.3.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.3.0,copy used gradients and hessians to ordered buffer
v3.3.0,transfer the indices to GPU
v3.3.0,only have root
v3.3.0,"Copy indices, gradients and hessians as early as possible"
v3.3.0,only need to initialize for smaller leaf
v3.3.0,Get leaf boundary
v3.3.0,do nothing if no features can be processed on GPU
v3.3.0,copy data indices if it is not null
v3.3.0,converted indices in is_feature_used to feature-group indices
v3.3.0,construct the feature masks for dense feature-groups
v3.3.0,"if no feature group is used, just return and do not use GPU"
v3.3.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.3.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.3.0,We now copy even if all features are used.
v3.3.0,"All data have been prepared, now run the GPU kernel"
v3.3.0,construct smaller leaf
v3.3.0,Check workgroups per feature4 tuple..
v3.3.0,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.3.0,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.3.0,then construct sparse features on CPU
v3.3.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.0,"wait for GPU to finish, only if GPU is actually used"
v3.3.0,use double precision
v3.3.0,use single precision
v3.3.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.3.0,#define CUDA_DEBUG_COMPARE
v3.3.0,construct larger leaf
v3.3.0,then construct sparse features on CPU
v3.3.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.3.0,"wait for GPU to finish, only if GPU is actually used"
v3.3.0,use double precision
v3.3.0,use single precision
v3.3.0,do some sanity check for the GPU algorithm
v3.2.1,coding: utf-8
v3.2.1,see https://github.com/pypa/distutils/pull/21
v3.2.1,coding: utf-8
v3.2.1,create predictor first
v3.2.1,check dataset
v3.2.1,reduce cost for prediction training data
v3.2.1,process callbacks
v3.2.1,Most of legacy advanced options becomes callbacks
v3.2.1,construct booster
v3.2.1,start training
v3.2.1,check evaluation result.
v3.2.1,"ranking task, split according to groups"
v3.2.1,run preprocessing on the data set if needed
v3.2.1,setup callbacks
v3.2.1,coding: utf-8
v3.2.1,dummy function to support older version of scikit-learn
v3.2.1,coding: utf-8
v3.2.1,documentation templates for LGBMModel methods are shared between the classes in
v3.2.1,this module and those in the ``dask`` module
v3.2.1,"user can set verbose with kwargs, it has higher priority"
v3.2.1,Do not modify original args in fit function
v3.2.1,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.2.1,Separate built-in from callable evaluation metrics
v3.2.1,register default metric for consistency with callable eval_metric case
v3.2.1,try to deduce from class instance
v3.2.1,overwrite default metric by explicitly set metric
v3.2.1,concatenate metric from params (or default if not provided in params) and eval_metric
v3.2.1,copy for consistency
v3.2.1,reduce cost for prediction training data
v3.2.1,free dataset
v3.2.1,Switch to using a multiclass objective in the underlying LGBM instance
v3.2.1,"do not modify args, as it causes errors in model selection tools"
v3.2.1,check group data
v3.2.1,coding: utf-8
v3.2.1,we don't need lib_lightgbm while building docs
v3.2.1,coding: utf-8
v3.2.1,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.2.1,coding: utf-8
v3.2.1,coding: utf-8
v3.2.1,TypeError: obj is not a string or a number
v3.2.1,ValueError: invalid literal
v3.2.1,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.2.1,avoid side effects on passed-in parameters
v3.2.1,"find a value, and remove other aliases with .pop()"
v3.2.1,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.2.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.2.1,avoid memory consumption by arrays concatenation operations
v3.2.1,create numpy array from output arrays
v3.2.1,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.2.1,for CSC there is extra column added
v3.2.1,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.2.1,same shape as input csr or csc matrix except extra column for expected value
v3.2.1,note: make sure we copy data as it will be deallocated next
v3.2.1,"free the temporary native indptr, indices, and data"
v3.2.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.2.1,avoid memory consumption by arrays concatenation operations
v3.2.1,"no min_data, nthreads and verbose in this function"
v3.2.1,check data has header or not
v3.2.1,need to regroup init_score
v3.2.1,process for args
v3.2.1,"user can set verbose with params, it has higher priority"
v3.2.1,get categorical features
v3.2.1,process for reference dataset
v3.2.1,start construct data
v3.2.1,set feature names
v3.2.1,create valid
v3.2.1,construct subset
v3.2.1,create train
v3.2.1,could be updated if data is not freed
v3.2.1,set to None
v3.2.1,we're done if self and reference share a common upstrem reference
v3.2.1,"group data from LightGBM is boundaries data, need to convert to group size"
v3.2.1,"user can set verbose with params, it has higher priority"
v3.2.1,Training task
v3.2.1,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.2.1,construct booster object
v3.2.1,copy the parameters from train_set
v3.2.1,save reference to data
v3.2.1,buffer for inner predict
v3.2.1,Prediction task
v3.2.1,if a single node tree it won't have `leaf_index` so return 0
v3.2.1,"Create the node record, and populate universal data members"
v3.2.1,Update values to reflect node type (leaf or split)
v3.2.1,traverse the next level of the tree
v3.2.1,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.2.1,and we add node to the list.
v3.2.1,need reset training data
v3.2.1,need to push new valid data
v3.2.1,"if buffer length is not long enough, re-allocate a buffer"
v3.2.1,"if buffer length is not long enough, reallocate a buffer"
v3.2.1,Copy models
v3.2.1,Get name of features
v3.2.1,avoid to predict many time in one iteration
v3.2.1,Get num of inner evals
v3.2.1,Get name of evals
v3.2.1,coding: utf-8
v3.2.1,Callback environment used by callbacks
v3.2.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.2.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.2.1,coding: utf-8
v3.2.1,Concatenate many parts into one
v3.2.1,"if any duplicates were found, search for new ports one by one"
v3.2.1,capture whether local_listen_port or its aliases were provided
v3.2.1,capture whether machines or its aliases were provided
v3.2.1,Some passed-in parameters can be removed:
v3.2.1,* 'num_machines': set automatically from Dask worker list
v3.2.1,* 'num_threads': overridden to match nthreads on each Dask process
v3.2.1,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.2.1,Start computation in the background
v3.2.1,Find locations of all parts and map them to particular Dask workers
v3.2.1,resolve aliases for network parameters and pop the result off params.
v3.2.1,these values are added back in calls to `_train_part()`
v3.2.1,figure out network params
v3.2.1,this approach with client.run() is faster than searching for ports
v3.2.1,"serially, but can produce duplicates sometimes. Try the fast approach one"
v3.2.1,"time, then pass it through a function that will use a slower but more reliable"
v3.2.1,approach if duplicates are found.
v3.2.1,Tell each worker to train on the parts that it has locally
v3.2.1,
v3.2.1,"This code treates ``_train_part()`` calls as not ""pure"" because:"
v3.2.1,1. there is randomness in the training process unless parameters ``seed``
v3.2.1,and ``deterministic`` are set
v3.2.1,"2. even with those parameters set, the output of one ``_train_part()`` call"
v3.2.1,relies on global state (it and all the other LightGBM training processes
v3.2.1,coordinate with each other)
v3.2.1,"if network parameters were changed during training, remove them from the"
v3.2.1,returned moodel so that they're generated dynamically on every run based
v3.2.1,on the Dask cluster you're connected to and which workers have pieces of
v3.2.1,the training data
v3.2.1,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.2.1,the note on custom objective functions in LGBMModel.__init__ is not
v3.2.1,currently relevant for the Dask estimators
v3.2.1,"DaskLGBMClassifier does not support evaluation data, or early stopping"
v3.2.1,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.2.1,the note on custom objective functions in LGBMModel.__init__ is not
v3.2.1,currently relevant for the Dask estimators
v3.2.1,"DaskLGBMRegressor does not support evaluation data, or early stopping"
v3.2.1,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.2.1,the note on custom objective functions in LGBMModel.__init__ is not
v3.2.1,currently relevant for the Dask estimators
v3.2.1,"DaskLGBMRanker does not support evaluation data, or early stopping"
v3.2.1,DaskLGBMRanker support for callbacks and init_model is not tested
v3.2.1,coding: utf-8
v3.2.1,load or create your dataset
v3.2.1,create dataset for lightgbm
v3.2.1,"if you want to re-use data, remember to set free_raw_data=False"
v3.2.1,specify your configurations as a dict
v3.2.1,generate feature names
v3.2.1,feature_name and categorical_feature
v3.2.1,check feature name
v3.2.1,save model to file
v3.2.1,dump model to JSON (and save to file)
v3.2.1,feature names
v3.2.1,feature importances
v3.2.1,load model to predict
v3.2.1,can only predict with the best iteration (or the saving iteration)
v3.2.1,eval with loaded model
v3.2.1,dump model with pickle
v3.2.1,load model with pickle to predict
v3.2.1,can predict with any iteration when loaded in pickle way
v3.2.1,eval with loaded model
v3.2.1,continue training
v3.2.1,init_model accepts:
v3.2.1,1. model file name
v3.2.1,2. Booster()
v3.2.1,decay learning rates
v3.2.1,learning_rates accepts:
v3.2.1,1. list/tuple with length = num_boost_round
v3.2.1,2. function(curr_iter)
v3.2.1,change other parameters during training
v3.2.1,self-defined objective function
v3.2.1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.2.1,log likelihood loss
v3.2.1,self-defined eval metric
v3.2.1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.1,binary error
v3.2.1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.2.1,This may make built-in evalution metric calculate wrong results
v3.2.1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.2.1,Keep this in mind when you use the customization
v3.2.1,another self-defined eval metric
v3.2.1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.1,accuracy
v3.2.1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.2.1,This may make built-in evalution metric calculate wrong results
v3.2.1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.2.1,Keep this in mind when you use the customization
v3.2.1,callback
v3.2.1,coding: utf-8
v3.2.1,load or create your dataset
v3.2.1,train
v3.2.1,predict
v3.2.1,eval
v3.2.1,feature importances
v3.2.1,self-defined eval metric
v3.2.1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.1,Root Mean Squared Logarithmic Error (RMSLE)
v3.2.1,train
v3.2.1,another self-defined eval metric
v3.2.1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.1,Relative Absolute Error (RAE)
v3.2.1,train
v3.2.1,predict
v3.2.1,eval
v3.2.1,other scikit-learn modules
v3.2.1,coding: utf-8
v3.2.1,load or create your dataset
v3.2.1,create dataset for lightgbm
v3.2.1,specify your configurations as a dict
v3.2.1,train
v3.2.1,coding: utf-8
v3.2.1,################
v3.2.1,Simulate some binary data with a single categorical and
v3.2.1,single continuous predictor
v3.2.1,################
v3.2.1,Set up a couple of utilities for our experiments
v3.2.1,################
v3.2.1,Observe the behavior of `binary` and `xentropy` objectives
v3.2.1,Trying this throws an error on non-binary values of y:
v3.2.1,"experiment('binary', label_type='probability', DATA)"
v3.2.1,The speed of `binary` is not drastically different than
v3.2.1,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.2.1,there are reasons to suspect that `binary` should run faster when the
v3.2.1,label is an integer instead of a float
v3.2.1,coding: utf-8
v3.2.1,load or create your dataset
v3.2.1,create dataset for lightgbm
v3.2.1,specify your configurations as a dict
v3.2.1,train
v3.2.1,save model to file
v3.2.1,predict
v3.2.1,eval
v3.2.1,split training data into two partitions
v3.2.1,make this array dense because we're splitting across
v3.2.1,a sparse boundary to partition the data
v3.2.1,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.2.1,predictions and target values back from workers to the client
v3.2.1,
v3.2.1,"for larger datasets, consider the metrics from dask-ml instead"
v3.2.1,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.2.1,coding: utf-8
v3.2.1,!/usr/bin/env python3
v3.2.1,-*- coding: utf-8 -*-
v3.2.1,
v3.2.1,"LightGBM documentation build configuration file, created by"
v3.2.1,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.2.1,
v3.2.1,This file is execfile()d with the current directory set to its
v3.2.1,containing dir.
v3.2.1,
v3.2.1,Note that not all possible configuration values are present in this
v3.2.1,autogenerated file.
v3.2.1,
v3.2.1,All configuration values have a default; values that are commented out
v3.2.1,serve to show the default.
v3.2.1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.2.1,add these directories to sys.path here. If the directory is relative to the
v3.2.1,"documentation root, use os.path.abspath to make it absolute."
v3.2.1,-- mock out modules
v3.2.1,-- General configuration ------------------------------------------------
v3.2.1,"If your documentation needs a minimal Sphinx version, state it here."
v3.2.1,"Add any Sphinx extension module names here, as strings. They can be"
v3.2.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.2.1,ones.
v3.2.1,hide type hints in API docs
v3.2.1,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.2.1,Only the class' docstring is inserted.
v3.2.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.2.1,The master toctree document.
v3.2.1,General information about the project.
v3.2.1,The name of an image file (relative to this directory) to place at the top
v3.2.1,of the sidebar.
v3.2.1,The name of an image file (relative to this directory) to use as a favicon of
v3.2.1,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.2.1,pixels large.
v3.2.1,"The version info for the project you're documenting, acts as replacement for"
v3.2.1,"|version| and |release|, also used in various other places throughout the"
v3.2.1,built documents.
v3.2.1,The short X.Y version.
v3.2.1,"The full version, including alpha/beta/rc tags."
v3.2.1,The language for content autogenerated by Sphinx. Refer to documentation
v3.2.1,for a list of supported languages.
v3.2.1,
v3.2.1,This is also used if you do content translation via gettext catalogs.
v3.2.1,"Usually you set ""language"" from the command line for these cases."
v3.2.1,"List of patterns, relative to source directory, that match files and"
v3.2.1,directories to ignore when looking for source files.
v3.2.1,This patterns also effect to html_static_path and html_extra_path
v3.2.1,The name of the Pygments (syntax highlighting) style to use.
v3.2.1,-- Configuration for C API docs generation ------------------------------
v3.2.1,-- Options for HTML output ----------------------------------------------
v3.2.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.2.1,a list of builtin themes.
v3.2.1,Theme options are theme-specific and customize the look and feel of a theme
v3.2.1,"further.  For a list of options available for each theme, see the"
v3.2.1,documentation.
v3.2.1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.2.1,"relative to this directory. They are copied after the builtin static files,"
v3.2.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.2.1,-- Options for HTMLHelp output ------------------------------------------
v3.2.1,Output file base name for HTML help builder.
v3.2.1,-- Options for LaTeX output ---------------------------------------------
v3.2.1,The name of an image file (relative to this directory) to place at the top of
v3.2.1,the title page.
v3.2.1,Warning! The following code can cause buffer overflows on RTD.
v3.2.1,Consider suppressing output completely if RTD project silently fails.
v3.2.1,Refer to https://github.com/svenevs/exhale
v3.2.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.2.1,Warning! The following code can cause buffer overflows on RTD.
v3.2.1,Consider suppressing output completely if RTD project silently fails.
v3.2.1,Refer to https://github.com/svenevs/exhale
v3.2.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.2.1,coding: utf-8
v3.2.1,Constants
v3.2.1,Start with some content:
v3.2.1,Clear & re-use:
v3.2.1,Output should match new content:
v3.2.1,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.2.1,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.2.1,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.2.1,Compute a new trial address & value & if it is a valid address:
v3.2.1,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.2.1,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.2.1,Check the just-stored value with getitem():
v3.2.1,Also store the just-stored value for future tracking:
v3.2.1,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.2.1,Test in 2 ways that the values are correctly laid out in memory:
v3.2.1,coding: utf-8
v3.2.1,we don't need lib_lightgbm while building docs
v3.2.1,coding: utf-8
v3.2.1,check saved model persistence
v3.2.1,"we need to check the consistency of model file here, so test for exact equal"
v3.2.1,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.2.1,"scores likely to be different, but prediction should still be the same"
v3.2.1,test that shape is checked during prediction
v3.2.1,test that method works even with free_raw_data=True
v3.2.1,test that method works but sets raw data to None in case of immergeable data types
v3.2.1,test that method works for different data types
v3.2.1,"Set extremely harsh penalties, so CEGB will block most splits."
v3.2.1,"Compare pairs of penalties, to ensure scaling works as intended"
v3.2.1,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.2.1,"should resolve duplicate aliases, and prefer the main parameter"
v3.2.1,should choose a value from an alias and set that value on main param
v3.2.1,if only an alias is used
v3.2.1,should use the default if main param and aliases are missing
v3.2.1,all changes should be made on copies and not modify the original
v3.2.1,coding: utf-8
v3.2.1,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.2.1,set_index ensures partitions are based on group id.
v3.2.1,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.2.1,"separate target, weight from features."
v3.2.1,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.2.1,"so that within each partition, sum(g) = n_samples."
v3.2.1,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.2.1,make one categorical feature relevant to the target
v3.2.1,https://github.com/microsoft/LightGBM/issues/4118
v3.2.1,pref_leaf values should have the right shape
v3.2.1,and values that look like valid tree nodes
v3.2.1,"be sure LightGBM actually used at least one categorical column,"
v3.2.1,and that it was correctly treated as a categorical feature
v3.2.1,"be sure LightGBM actually used at least one categorical column,"
v3.2.1,and that it was correctly treated as a categorical feature
v3.2.1,shape depends on whether it is binary or multiclass classification
v3.2.1,* shape depends on whether it is binary or multiclass classification
v3.2.1,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.2.1,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.2.1,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.2.1,that the output has the right shape and base values are in the right position
v3.2.1,check that found ports are different for same address (LocalCluster)
v3.2.1,check that the ports are indeed open
v3.2.1,should handle worker maps without any duplicates
v3.2.1,should handle worker maps with duplicates
v3.2.1,Scores should be the same
v3.2.1,Predictions should be roughly the same.
v3.2.1,pref_leaf values should have the right shape
v3.2.1,and values that look like valid tree nodes
v3.2.1,"be sure LightGBM actually used at least one categorical column,"
v3.2.1,and that it was correctly treated as a categorical feature
v3.2.1,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.2.1,that the output has the right shape and base values are in the right position
v3.2.1,"be sure LightGBM actually used at least one categorical column,"
v3.2.1,and that it was correctly treated as a categorical feature
v3.2.1,Quantiles should be right
v3.2.1,"be sure LightGBM actually used at least one categorical column,"
v3.2.1,and that it was correctly treated as a categorical feature
v3.2.1,rebalance small dask.Array dataset for better performance.
v3.2.1,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.2.1,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.2.1,distributed ranker should be able to rank decently well and should
v3.2.1,have high rank correlation with scores from serial ranker.
v3.2.1,pref_leaf values should have the right shape
v3.2.1,and values that look like valid tree nodes
v3.2.1,"be sure LightGBM actually used at least one categorical column,"
v3.2.1,and that it was correctly treated as a categorical feature
v3.2.1,should be able to use the class without specifying a client
v3.2.1,should be able to set client after construction
v3.2.1,data on cluster1
v3.2.1,create identical data on cluster2
v3.2.1,"at this point, the result of default_client() is client2 since it was the most recently"
v3.2.1,created. So setting client to client1 here to test that you can select a non-default client
v3.2.1,"unfitted model should survive pickling round trip, and pickling"
v3.2.1,shouldn't have side effects on the model object
v3.2.1,client will always be None after unpickling
v3.2.1,"fitted model should survive pickling round trip, and pickling"
v3.2.1,shouldn't have side effects on the model object
v3.2.1,client will always be None after unpickling
v3.2.1,rebalance data to be sure that each worker has a piece of the data
v3.2.1,model 1 - no network parameters given
v3.2.1,model 2 - machines given
v3.2.1,model 3 - local_listen_port given
v3.2.1,training should fail because LightGBM will try to use the same
v3.2.1,port for multiple worker processes on the same machine
v3.2.1,rebalance data to be sure that each worker has a piece of the data
v3.2.1,"test that ""machines"" is actually respected by creating a socket that uses"
v3.2.1,"one of the ports mentioned in ""machines"""
v3.2.1,The above error leaves a worker waiting
v3.2.1,"an informative error should be raised if ""machines"" has duplicates"
v3.2.1,"""client"" should be the only different, and the final argument"
v3.2.1,"init_scores must be a 1D array, even for multiclass classification"
v3.2.1,where you need to provide 1 score per class for each row in X
v3.2.1,https://github.com/microsoft/LightGBM/issues/4046
v3.2.1,value of the root node is 0 when init_score is set
v3.2.1,this test is separate because it takes a not-yet-constructed estimator
v3.2.1,coding: utf-8
v3.2.1,coding: utf-8
v3.2.1,"build target, group ID vectors."
v3.2.1,build y/target and group-id vectors with user-specified group sizes.
v3.2.1,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.2.1,groups should contain > 1 element for pairwise learning objective.
v3.2.1,"build feature data, X. Transform first few into informative features."
v3.2.1,coding: utf-8
v3.2.1,prediction result is actually not transformed (is raw) due to custom objective
v3.2.1,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.2.1,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.2.1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.2.1,sklearn < 0.23 does not have as_frame parameter
v3.2.1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.2.1,sklearn < 0.23 does not have as_frame parameter
v3.2.1,Test if random_state is properly stored
v3.2.1,Test if two random states produce identical models
v3.2.1,Test if subsequent fits sample from random_state object and produce different models
v3.2.1,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.2.1,With default params
v3.2.1,Tests same probabilities
v3.2.1,Tests same predictions
v3.2.1,Tests same raw scores
v3.2.1,Tests same leaf indices
v3.2.1,Tests same feature contributions
v3.2.1,Tests other parameters for the prediction works
v3.2.1,Tests start_iteration
v3.2.1,"Tests same probabilities, starting from iteration 10"
v3.2.1,"Tests same predictions, starting from iteration 10"
v3.2.1,"Tests same raw scores, starting from iteration 10"
v3.2.1,"Tests same leaf indices, starting from iteration 10"
v3.2.1,"Tests same feature contributions, starting from iteration 10"
v3.2.1,"Tests other parameters for the prediction works, starting from iteration 10"
v3.2.1,"no custom objective, no custom metric"
v3.2.1,default metric
v3.2.1,non-default metric
v3.2.1,no metric
v3.2.1,non-default metric in eval_metric
v3.2.1,non-default metric with non-default metric in eval_metric
v3.2.1,non-default metric with multiple metrics in eval_metric
v3.2.1,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.2.1,default metric for non-default objective
v3.2.1,non-default metric for non-default objective
v3.2.1,no metric
v3.2.1,non-default metric in eval_metric for non-default objective
v3.2.1,non-default metric with non-default metric in eval_metric for non-default objective
v3.2.1,non-default metric with multiple metrics in eval_metric for non-default objective
v3.2.1,"custom objective, no custom metric"
v3.2.1,default regression metric for custom objective
v3.2.1,non-default regression metric for custom objective
v3.2.1,multiple regression metrics for custom objective
v3.2.1,no metric
v3.2.1,default regression metric with non-default metric in eval_metric for custom objective
v3.2.1,non-default regression metric with metric in eval_metric for custom objective
v3.2.1,multiple regression metrics with metric in eval_metric for custom objective
v3.2.1,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.2.1,"no custom objective, custom metric"
v3.2.1,default metric with custom metric
v3.2.1,non-default metric with custom metric
v3.2.1,multiple metrics with custom metric
v3.2.1,custom metric (disable default metric)
v3.2.1,default metric for non-default objective with custom metric
v3.2.1,non-default metric for non-default objective with custom metric
v3.2.1,multiple metrics for non-default objective with custom metric
v3.2.1,custom metric (disable default metric for non-default objective)
v3.2.1,"custom objective, custom metric"
v3.2.1,custom metric for custom objective
v3.2.1,non-default regression metric with custom metric for custom objective
v3.2.1,multiple regression metrics with custom metric for custom objective
v3.2.1,default metric and invalid binary metric is replaced with multiclass alternative
v3.2.1,invalid objective is replaced with default multiclass one
v3.2.1,and invalid binary metric is replaced with multiclass alternative
v3.2.1,default metric for non-default multiclass objective
v3.2.1,and invalid binary metric is replaced with multiclass alternative
v3.2.1,default metric and invalid multiclass metric is replaced with binary alternative
v3.2.1,invalid multiclass metric is replaced with binary alternative for custom objective
v3.2.1,"Verify that can receive a list of metrics, only callable"
v3.2.1,Verify that can receive a list of custom and built-in metrics
v3.2.1,Verify that works as expected when eval_metric is empty
v3.2.1,"Verify that can receive a list of metrics, only built-in"
v3.2.1,Verify that eval_metric is robust to receiving a list with None
v3.2.1,training data as eval_set
v3.2.1,feval
v3.2.1,single eval_set
v3.2.1,two eval_set
v3.2.1,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.2.1,Test that estimators are default-constructible
v3.2.1,coding: utf-8
v3.2.1,coding: utf-8
v3.2.1,check that default gives same result as k = 1
v3.2.1,check against independent calculation for k = 1
v3.2.1,check against independent calculation for k = 2
v3.2.1,check against independent calculation for k = 10
v3.2.1,check cases where predictions are equal
v3.2.1,should give same result as binary auc for 2 classes
v3.2.1,test the case where all predictions are equal
v3.2.1,test that weighted data gives different auc_mu
v3.2.1,test that equal data weights give same auc_mu as unweighted data
v3.2.1,should give 1 when accuracy = 1
v3.2.1,test loading class weights
v3.2.1,no early stopping
v3.2.1,early stopping occurs
v3.2.1,test custom eval metrics
v3.2.1,"shuffle = False, override metric in params"
v3.2.1,"shuffle = True, callbacks"
v3.2.1,enable display training loss
v3.2.1,self defined folds
v3.2.1,lambdarank
v3.2.1,... with l2 metric
v3.2.1,... with NDCG (default) metric
v3.2.1,self defined folds with lambdarank
v3.2.1,with early stopping
v3.2.1,predict by each fold booster
v3.2.1,fold averaging
v3.2.1,without early stopping
v3.2.1,test feature_names with whitespaces
v3.2.1,This has non-ascii strings.
v3.2.1,take subsets and train
v3.2.1,generate CSR sparse dataset
v3.2.1,convert data to dense and get back same contribs
v3.2.1,validate the values are the same
v3.2.1,validate using CSC matrix
v3.2.1,validate the values are the same
v3.2.1,generate CSR sparse dataset
v3.2.1,convert data to dense and get back same contribs
v3.2.1,validate the values are the same
v3.2.1,validate using CSC matrix
v3.2.1,validate the values are the same
v3.2.1,Note there is an extra column added to the output for the expected value
v3.2.1,Note output CSC shape should be same as CSR output shape
v3.2.1,test sliced labels
v3.2.1,append some columns
v3.2.1,append some rows
v3.2.1,test sliced 2d matrix
v3.2.1,test sliced CSR
v3.2.1,trees start at position 1.
v3.2.1,split_features are in 4th line.
v3.2.1,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.2.1,The penalization is so high that the first 2 features should not be used here
v3.2.1,Check that a very high penalization is the same as not using the features at all
v3.2.1,"no fobj, no feval"
v3.2.1,default metric
v3.2.1,non-default metric in params
v3.2.1,default metric in args
v3.2.1,non-default metric in args
v3.2.1,metric in args overwrites one in params
v3.2.1,multiple metrics in params
v3.2.1,multiple metrics in args
v3.2.1,remove default metric by 'None' in list
v3.2.1,remove default metric by 'None' aliases
v3.2.1,"fobj, no feval"
v3.2.1,no default metric
v3.2.1,metric in params
v3.2.1,metric in args
v3.2.1,metric in args overwrites its' alias in params
v3.2.1,multiple metrics in params
v3.2.1,multiple metrics in args
v3.2.1,"no fobj, feval"
v3.2.1,default metric with custom one
v3.2.1,non-default metric in params with custom one
v3.2.1,default metric in args with custom one
v3.2.1,non-default metric in args with custom one
v3.2.1,"metric in args overwrites one in params, custom one is evaluated too"
v3.2.1,multiple metrics in params with custom one
v3.2.1,multiple metrics in args with custom one
v3.2.1,custom metric is evaluated despite 'None' is passed
v3.2.1,"fobj, feval"
v3.2.1,"no default metric, only custom one"
v3.2.1,metric in params with custom one
v3.2.1,metric in args with custom one
v3.2.1,"metric in args overwrites one in params, custom one is evaluated too"
v3.2.1,multiple metrics in params with custom one
v3.2.1,multiple metrics in args with custom one
v3.2.1,custom metric is evaluated despite 'None' is passed
v3.2.1,"no fobj, no feval"
v3.2.1,default metric
v3.2.1,default metric in params
v3.2.1,non-default metric in params
v3.2.1,multiple metrics in params
v3.2.1,remove default metric by 'None' aliases
v3.2.1,"fobj, no feval"
v3.2.1,no default metric
v3.2.1,metric in params
v3.2.1,multiple metrics in params
v3.2.1,"no fobj, feval"
v3.2.1,default metric with custom one
v3.2.1,default metric in params with custom one
v3.2.1,non-default metric in params with custom one
v3.2.1,multiple metrics in params with custom one
v3.2.1,custom metric is evaluated despite 'None' is passed
v3.2.1,"fobj, feval"
v3.2.1,"no default metric, only custom one"
v3.2.1,metric in params with custom one
v3.2.1,multiple metrics in params with custom one
v3.2.1,custom metric is evaluated despite 'None' is passed
v3.2.1,multiclass default metric
v3.2.1,multiclass default metric with custom one
v3.2.1,multiclass metric alias with custom one for custom objective
v3.2.1,no metric for invalid class_num
v3.2.1,custom metric for invalid class_num
v3.2.1,multiclass metric alias with custom one with invalid class_num
v3.2.1,multiclass default metric without num_class
v3.2.1,multiclass metric alias
v3.2.1,multiclass metric
v3.2.1,non-valid metric for multiclass objective
v3.2.1,non-default num_class for default objective
v3.2.1,no metric with non-default num_class for custom objective
v3.2.1,multiclass metric alias for custom objective
v3.2.1,multiclass metric for custom objective
v3.2.1,binary metric with non-default num_class for custom objective
v3.2.1,Expect three metrics but mean and stdv for each metric
v3.2.1,test XGBoost-style return value
v3.2.1,test numpy-style return value
v3.2.1,test bins string type
v3.2.1,test histogram is disabled for categorical features
v3.2.1,test for lgb.train
v3.2.1,test feval for lgb.train
v3.2.1,test with two valid data for lgb.train
v3.2.1,test for lgb.cv
v3.2.1,test feval for lgb.cv
v3.2.1,test that binning works properly for features with only positive or only negative values
v3.2.1,decreasing without freeing raw data is allowed
v3.2.1,decreasing before lazy init is allowed
v3.2.1,increasing is allowed
v3.2.1,decreasing with disabled filter is allowed
v3.2.1,decreasing with enabled filter is disallowed;
v3.2.1,also changes of other params are disallowed
v3.2.1,check extra trees increases regularization
v3.2.1,check path smoothing increases regularization
v3.2.1,test edge case with one leaf
v3.2.1,check that constraint containing all features is equivalent to no constraint
v3.2.1,check that constraint partitioning the features reduces train accuracy
v3.2.1,check that constraints consisting of single features reduce accuracy further
v3.2.1,test that interaction constraints work when not all features are used
v3.2.1,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.2.1,test again with nans in data
v3.2.1,test again with bagging
v3.2.1,test with a feature that has only one non-nan value
v3.2.1,test with a categorical feature
v3.2.1,test refit: same results on same data
v3.2.1,test refit with save and load
v3.2.1,test refit: different results training on different data
v3.2.1,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.2.1,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.2.1,"test the case where start_iteration <= 0, and num_iteration is None"
v3.2.1,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.2.1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.2.1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.2.1,test for regression
v3.2.1,test both with and without early stopping
v3.2.1,test for multi-class
v3.2.1,test both with and without early stopping
v3.2.1,test for binary
v3.2.1,test both with and without early stopping
v3.2.1,test against sklearn average precision metric
v3.2.1,test that average precision is 1 where model predicts perfectly
v3.2.1,coding: utf-8
v3.2.1,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.2.1,coding: utf-8
v3.2.1,coding: utf-8
v3.2.1,convert from one-based to  zero-based index
v3.2.1,convert from boundaries to size
v3.2.1,--- start Booster interfaces
v3.2.1,.Call() calls
v3.2.1,coding: utf-8
v3.2.1,alias table
v3.2.1,names
v3.2.1,from strings
v3.2.1,tails
v3.2.1,tails
v3.2.1,coding: utf-8
v3.2.1,Single row predictor to abstract away caching logic
v3.2.1,create boosting
v3.2.1,initialize the boosting
v3.2.1,create objective function
v3.2.1,initialize the objective function
v3.2.1,create training metric
v3.2.1,reset the boosting
v3.2.1,create objective function
v3.2.1,initialize the objective function
v3.2.1,calculate the nonzero data and indices size
v3.2.1,allocate data and indices arrays
v3.2.1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.2.1,aggregated per row feature contribution results
v3.2.1,keep track of the row_vector sizes for parallelization
v3.2.1,copy vector results to output for each row
v3.2.1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.2.1,aggregated per row feature contribution results
v3.2.1,calculate number of elements per column to construct
v3.2.1,the CSC matrix with random access
v3.2.1,keep track of column counts
v3.2.1,keep track of beginning index for each column
v3.2.1,keep track of beginning index for each matrix
v3.2.1,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.2.1,store the row index
v3.2.1,update column count
v3.2.1,explicitly declare symbols from LightGBM namespace
v3.2.1,some help functions used to convert data
v3.2.1,Row iterator of on column for CSC matrix
v3.2.1,"return value at idx, only can access by ascent order"
v3.2.1,"return next non-zero pair, if index < 0, means no more data"
v3.2.1,start of c_api functions
v3.2.1,sample data first
v3.2.1,sample data first
v3.2.1,sample data first
v3.2.1,local buffer to re-use memory
v3.2.1,sample data first
v3.2.1,no more data
v3.2.1,---- start of booster
v3.2.1,Single row in row-major format:
v3.2.1,---- start of some help functions
v3.2.1,data is array of pointers to individual rows
v3.2.1,set number of threads for openmp
v3.2.1,check for alias
v3.2.1,read parameters from config file
v3.2.1,"remove str after ""#"""
v3.2.1,check for alias again
v3.2.1,load configs
v3.2.1,prediction is needed if using input initial model(continued train)
v3.2.1,need to continue training
v3.2.1,sync up random seed for data partition
v3.2.1,load Training data
v3.2.1,load data for distributed training
v3.2.1,load data for single machine
v3.2.1,need save binary file
v3.2.1,create training metric
v3.2.1,only when have metrics then need to construct validation data
v3.2.1,"Add validation data, if it exists"
v3.2.1,add
v3.2.1,need save binary file
v3.2.1,add metric for validation data
v3.2.1,output used time on each iteration
v3.2.1,need init network
v3.2.1,create boosting
v3.2.1,create objective function
v3.2.1,load training data
v3.2.1,initialize the objective function
v3.2.1,initialize the boosting
v3.2.1,add validation data into boosting
v3.2.1,convert model to if-else statement code
v3.2.1,create predictor
v3.2.1,Free memory
v3.2.1,create predictor
v3.2.1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.2.1,counts for all labels
v3.2.1,"start from top label, and accumulate DCG"
v3.2.1,counts for all labels
v3.2.1,calculate k Max DCG by one pass
v3.2.1,get sorted indices by score
v3.2.1,calculate dcg
v3.2.1,get sorted indices by score
v3.2.1,calculate multi dcg by one pass
v3.2.1,wait for all client start up
v3.2.1,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.2.1,Instead we will handle finalize/abort for MPI in main().
v3.2.1,default set to -1
v3.2.1,"distance at k-th communication, distance[k] = 2^k"
v3.2.1,set incoming rank at k-th commuication
v3.2.1,set outgoing rank at k-th commuication
v3.2.1,defalut set as -1
v3.2.1,construct all recursive halving map for all machines
v3.2.1,let 1 << k <= num_machines
v3.2.1,distance of each communication
v3.2.1,"if num_machines = 2^k, don't need to group machines"
v3.2.1,"communication direction, %2 == 0 is positive"
v3.2.1,neighbor at k-th communication
v3.2.1,receive data block at k-th communication
v3.2.1,send data block at k-th communication
v3.2.1,"if num_machines != 2^k, need to group machines"
v3.2.1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.2.1,let left machine as group leader
v3.2.1,"cache block information for groups, group with 2 machines will have double block size"
v3.2.1,convert from group to node leader
v3.2.1,convert from node to group
v3.2.1,meet new group
v3.2.1,add block len for this group
v3.2.1,calculate the group block start
v3.2.1,not need to construct
v3.2.1,get receive block informations
v3.2.1,accumulate block len
v3.2.1,get send block informations
v3.2.1,accumulate block len
v3.2.1,static member definition
v3.2.1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.2.1,assign the blocks to every rank.
v3.2.1,do reduce scatter
v3.2.1,do all gather
v3.2.1,assign blocks
v3.2.1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.2.1,copy back
v3.2.1,assign blocks
v3.2.1,start all gather
v3.2.1,when num_machines is small and data is large
v3.2.1,use output as receive buffer
v3.2.1,get current local block size
v3.2.1,get out rank
v3.2.1,get in rank
v3.2.1,get send information
v3.2.1,get recv information
v3.2.1,send and recv at same time
v3.2.1,rotate in-place
v3.2.1,use output as receive buffer
v3.2.1,get current local block size
v3.2.1,get send information
v3.2.1,get recv information
v3.2.1,send and recv at same time
v3.2.1,use output as receive buffer
v3.2.1,send and recv at same time
v3.2.1,send local data to neighbor first
v3.2.1,receive neighbor data first
v3.2.1,reduce
v3.2.1,get target
v3.2.1,get send information
v3.2.1,get recv information
v3.2.1,send and recv at same time
v3.2.1,reduce
v3.2.1,send result to neighbor
v3.2.1,receive result from neighbor
v3.2.1,copy result
v3.2.1,start up socket
v3.2.1,parse clients from file
v3.2.1,get ip list of local machine
v3.2.1,get local rank
v3.2.1,construct listener
v3.2.1,construct communication topo
v3.2.1,construct linkers
v3.2.1,free listener
v3.2.1,set timeout
v3.2.1,accept incoming socket
v3.2.1,receive rank
v3.2.1,add new socket
v3.2.1,save ranks that need to connect with
v3.2.1,start listener
v3.2.1,start connect
v3.2.1,let smaller rank connect to larger rank
v3.2.1,send local rank
v3.2.1,wait for listener
v3.2.1,print connected linkers
v3.2.1,only need to copy subset
v3.2.1,avoid to copy subset many times
v3.2.1,avoid out of range
v3.2.1,may need to recopy subset
v3.2.1,valid the type
v3.2.1,Constructors
v3.2.1,Get type tag
v3.2.1,Comparisons
v3.2.1,"This has to be separate, not in Statics, because Json() accesses"
v3.2.1,statics().null.
v3.2.1,"advance until next line, or end of input"
v3.2.1,advance until closing tokens
v3.2.1,The usual case: non-escaped characters
v3.2.1,Handle escapes
v3.2.1,Extract 4-byte escape sequence
v3.2.1,Explicitly check length of the substring. The following loop
v3.2.1,relies on std::string returning the terminating NUL when
v3.2.1,accessing str[length]. Checking here reduces brittleness.
v3.2.1,JSON specifies that characters outside the BMP shall be encoded as a
v3.2.1,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.2.1,components. Check whether we're in the middle of such a beast: the
v3.2.1,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.2.1,a trail (low) surrogate.
v3.2.1,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.2.1,per the UTF-16 algorithm.
v3.2.1,Integer part
v3.2.1,Decimal part
v3.2.1,Exponent part
v3.2.1,Check for any trailing garbage
v3.2.1,Documented in json11.hpp
v3.2.1,Check for another object
v3.2.1,get column names
v3.2.1,load label idx first
v3.2.1,erase label column name
v3.2.1,load ignore columns
v3.2.1,load weight idx
v3.2.1,load group idx
v3.2.1,don't support query id in data file when using distributed training
v3.2.1,read data to memory
v3.2.1,sample data
v3.2.1,construct feature bin mappers
v3.2.1,initialize label
v3.2.1,extract features
v3.2.1,sample data from file
v3.2.1,construct feature bin mappers
v3.2.1,initialize label
v3.2.1,extract features
v3.2.1,load data from binary file
v3.2.1,check meta data
v3.2.1,need to check training data
v3.2.1,read data in memory
v3.2.1,initialize label
v3.2.1,extract features
v3.2.1,Get number of lines of data file
v3.2.1,initialize label
v3.2.1,extract features
v3.2.1,load data from binary file
v3.2.1,not need to check validation data
v3.2.1,check meta data
v3.2.1,buffer to read binary file
v3.2.1,check token
v3.2.1,read size of header
v3.2.1,re-allocmate space if not enough
v3.2.1,read header
v3.2.1,get header
v3.2.1,num_groups
v3.2.1,real_feature_idx_
v3.2.1,feature2group
v3.2.1,feature2subfeature
v3.2.1,group_bin_boundaries
v3.2.1,group_feature_start_
v3.2.1,group_feature_cnt_
v3.2.1,get feature names
v3.2.1,write feature names
v3.2.1,get forced_bin_bounds_
v3.2.1,read size of meta data
v3.2.1,re-allocate space if not enough
v3.2.1,read meta data
v3.2.1,load meta data
v3.2.1,sample local used data if need to partition
v3.2.1,"if not contain query file, minimal sample unit is one record"
v3.2.1,"if contain query file, minimal sample unit is one query"
v3.2.1,if is new query
v3.2.1,read feature data
v3.2.1,read feature size
v3.2.1,re-allocate space if not enough
v3.2.1,raw data
v3.2.1,fill feature_names_ if not header
v3.2.1,get forced split
v3.2.1,"if only one machine, find bin locally"
v3.2.1,"if have multi-machines, need to find bin distributed"
v3.2.1,different machines will find bin for different features
v3.2.1,start and len will store the process feature indices for different machines
v3.2.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.2.1,free
v3.2.1,gather global feature bin mappers
v3.2.1,restore features bins from buffer
v3.2.1,---- private functions ----
v3.2.1,"if features are ordered, not need to use hist_buf"
v3.2.1,read all lines
v3.2.1,get query data
v3.2.1,"if not contain query data, minimal sample unit is one record"
v3.2.1,"if contain query data, minimal sample unit is one query"
v3.2.1,if is new query
v3.2.1,get query data
v3.2.1,"if not contain query file, minimal sample unit is one record"
v3.2.1,"if contain query file, minimal sample unit is one query"
v3.2.1,if is new query
v3.2.1,parse features
v3.2.1,get forced split
v3.2.1,"check the range of label_idx, weight_idx and group_idx"
v3.2.1,fill feature_names_ if not header
v3.2.1,start find bins
v3.2.1,"if only one machine, find bin locally"
v3.2.1,start and len will store the process feature indices for different machines
v3.2.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.2.1,free
v3.2.1,gather global feature bin mappers
v3.2.1,restore features bins from buffer
v3.2.1,if doesn't need to prediction with initial model
v3.2.1,parser
v3.2.1,set label
v3.2.1,free processed line:
v3.2.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.2.1,text_reader_->Lines()[i].shrink_to_fit();
v3.2.1,push data
v3.2.1,if is used feature
v3.2.1,if need to prediction with initial model
v3.2.1,parser
v3.2.1,set initial score
v3.2.1,set label
v3.2.1,free processed line:
v3.2.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.2.1,text_reader_->Lines()[i].shrink_to_fit();
v3.2.1,push data
v3.2.1,if is used feature
v3.2.1,metadata_ will manage space of init_score
v3.2.1,text data can be free after loaded feature values
v3.2.1,parser
v3.2.1,set initial score
v3.2.1,set label
v3.2.1,push data
v3.2.1,if is used feature
v3.2.1,only need part of data
v3.2.1,need full data
v3.2.1,metadata_ will manage space of init_score
v3.2.1,read size of token
v3.2.1,remove duplicates
v3.2.1,deep copy function for BinMapper
v3.2.1,mean size for one bin
v3.2.1,need a new bin
v3.2.1,update bin upper bound
v3.2.1,last bin upper bound
v3.2.1,get list of distinct values
v3.2.1,get number of positive and negative distinct values
v3.2.1,include zero bounds and infinity bound
v3.2.1,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.2.1,find remaining bounds
v3.2.1,find distinct_values first
v3.2.1,push zero in the front
v3.2.1,use the large value
v3.2.1,push zero in the back
v3.2.1,convert to int type first
v3.2.1,sort by counts
v3.2.1,will ignore the categorical of small counts
v3.2.1,Push the dummy bin for NaN
v3.2.1,Use MissingType::None to represent this bin contains all categoricals
v3.2.1,fix count of NaN bin
v3.2.1,check trivial(num_bin_ == 1) feature
v3.2.1,check useless bin
v3.2.1,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.2.1,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.2.1,calculate max bin of all features to select the int type in MultiValDenseBin
v3.2.1,"for lambdarank, it needs query data for partition data in distributed learning"
v3.2.1,need convert query_id to boundaries
v3.2.1,check weights
v3.2.1,check query boundries
v3.2.1,contain initial score file
v3.2.1,check weights
v3.2.1,get local weights
v3.2.1,check query boundries
v3.2.1,get local query boundaries
v3.2.1,contain initial score file
v3.2.1,get local initial scores
v3.2.1,re-load query weight
v3.2.1,save to nullptr
v3.2.1,save to nullptr
v3.2.1,save to nullptr
v3.2.1,default weight file name
v3.2.1,default init_score file name
v3.2.1,use first line to count number class
v3.2.1,default query file name
v3.2.1,root is in the depth 0
v3.2.1,non-leaf
v3.2.1,leaf
v3.2.1,use this for the missing value conversion
v3.2.1,Predict func by Map to ifelse
v3.2.1,use this for the missing value conversion
v3.2.1,non-leaf
v3.2.1,left subtree
v3.2.1,right subtree
v3.2.1,leaf
v3.2.1,non-leaf
v3.2.1,left subtree
v3.2.1,right subtree
v3.2.1,leaf
v3.2.1,recursive computation of SHAP values for a decision tree
v3.2.1,extend the unique path
v3.2.1,leaf node
v3.2.1,internal node
v3.2.1,"see if we have already split on this feature,"
v3.2.1,if so we undo that split so we can redo it for this node
v3.2.1,recursive sparse computation of SHAP values for a decision tree
v3.2.1,extend the unique path
v3.2.1,leaf node
v3.2.1,internal node
v3.2.1,"see if we have already split on this feature,"
v3.2.1,if so we undo that split so we can redo it for this node
v3.2.1,add names of objective function if not providing metric
v3.2.1,equal weights for all classes
v3.2.1,generate seeds by seed.
v3.2.1,sort eval_at
v3.2.1,Only push the non-training data
v3.2.1,check for conflicts
v3.2.1,"check if objective, metric, and num_class match"
v3.2.1,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.2.1,Check max_depth and num_leaves
v3.2.1,"Fits in an int, and is more restrictive than the current num_leaves"
v3.2.1,force col-wise for gpu & CUDA
v3.2.1,force gpu_use_dp for CUDA
v3.2.1,linear tree learner must be serial type and run on cpu device
v3.2.1,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.2.1,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.2.1,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.2.1,calculated split gain can be positive even with zero gradient and hessian.
v3.2.1,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.2.1,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.2.1,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.2.1,first round: fill the single val group
v3.2.1,always push the last group
v3.2.1,put dense feature first
v3.2.1,sort by non zero cnt
v3.2.1,"sort by non zero cnt, bigger first"
v3.2.1,shuffle groups
v3.2.1,Using std::swap for vector<bool> will cause the wrong result.
v3.2.1,get num_features
v3.2.1,get bin_mappers
v3.2.1,"for sparse multi value bin, we store the feature bin values with offset added"
v3.2.1,"for dense multi value bin, the feature bin values without offsets are used"
v3.2.1,copy feature bin mapper data
v3.2.1,copy feature bin mapper data
v3.2.1,"if not pass a filename, just append "".bin"" of original file"
v3.2.1,get size of header
v3.2.1,size of feature names
v3.2.1,size of forced bins
v3.2.1,write header
v3.2.1,write feature names
v3.2.1,write forced bins
v3.2.1,get size of meta data
v3.2.1,write meta data
v3.2.1,write feature data
v3.2.1,get size of feature
v3.2.1,write feature
v3.2.1,write raw data; use row-major order so we can read row-by-row
v3.2.1,"explicitly initialize template methods, for cross module call"
v3.2.1,"Only one multi-val group, just simply merge"
v3.2.1,Skip the leading 0 when copying group_bin_boundaries.
v3.2.1,regenerate other fields
v3.2.1,store the importance first
v3.2.1,PredictRaw
v3.2.1,PredictRawByMap
v3.2.1,Predict
v3.2.1,PredictByMap
v3.2.1,PredictLeafIndex
v3.2.1,PredictLeafIndexByMap
v3.2.1,output model type
v3.2.1,output number of class
v3.2.1,output label index
v3.2.1,output max_feature_idx
v3.2.1,output objective
v3.2.1,output tree models
v3.2.1,store the importance first
v3.2.1,sort the importance
v3.2.1,use serialized string to restore this object
v3.2.1,Use first 128 chars to avoid exceed the message buffer.
v3.2.1,get number of classes
v3.2.1,get index of label
v3.2.1,get max_feature_idx first
v3.2.1,get average_output
v3.2.1,get feature names
v3.2.1,get monotone_constraints
v3.2.1,set zero
v3.2.1,predict all the trees for one iteration
v3.2.1,check early stopping
v3.2.1,set zero
v3.2.1,predict all the trees for one iteration
v3.2.1,check early stopping
v3.2.1,margin_threshold will be captured by value
v3.2.1,copy and sort
v3.2.1,margin_threshold will be captured by value
v3.2.1,Fix for compiler warnings about reaching end of control
v3.2.1,load forced_splits file
v3.2.1,init tree learner
v3.2.1,push training metrics
v3.2.1,create buffer for gradients and hessians
v3.2.1,get max feature index
v3.2.1,get label index
v3.2.1,get feature names
v3.2.1,"if need bagging, create buffer"
v3.2.1,"for a validation dataset, we need its score and metric"
v3.2.1,update score
v3.2.1,objective function will calculate gradients and hessians
v3.2.1,"random bagging, minimal unit is one record"
v3.2.1,"random bagging, minimal unit is one record"
v3.2.1,if need bagging
v3.2.1,set bagging data to tree learner
v3.2.1,get subset
v3.2.1,output used time per iteration
v3.2.1,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.2.1,boosting first
v3.2.1,bagging logic
v3.2.1,need to copy gradients for bagging subset.
v3.2.1,shrinkage by learning rate
v3.2.1,update score
v3.2.1,only add default score one-time
v3.2.1,updates scores
v3.2.1,add model
v3.2.1,reset score
v3.2.1,remove model
v3.2.1,print message for metric
v3.2.1,pop last early_stopping_round_ models
v3.2.1,update training score
v3.2.1,we need to predict out-of-bag scores of data for boosting
v3.2.1,update validation score
v3.2.1,print training metric
v3.2.1,print validation metric
v3.2.1,set zero
v3.2.1,predict all the trees for one iteration
v3.2.1,predict all the trees for one iteration
v3.2.1,push training metrics
v3.2.1,"not same training data, need reset score and others"
v3.2.1,create score tracker
v3.2.1,update score
v3.2.1,create buffer for gradients and hessians
v3.2.1,load forced_splits file
v3.2.1,"if need bagging, create buffer"
v3.2.1,Get the max size of pool
v3.2.1,at least need 2 leaves
v3.2.1,push split information for all leaves
v3.2.1,initialize splits for leaf
v3.2.1,initialize data partition
v3.2.1,initialize ordered gradients and hessians
v3.2.1,cannot change is_hist_col_wise during training
v3.2.1,initialize splits for leaf
v3.2.1,initialize data partition
v3.2.1,initialize ordered gradients and hessians
v3.2.1,Get the max size of pool
v3.2.1,at least need 2 leaves
v3.2.1,push split information for all leaves
v3.2.1,some initial works before training
v3.2.1,root leaf
v3.2.1,only root leaf can be splitted on first time
v3.2.1,some initial works before finding best split
v3.2.1,find best threshold for every feature
v3.2.1,Get a leaf with max split gain
v3.2.1,Get split information for best leaf
v3.2.1,"cannot split, quit"
v3.2.1,split tree with best leaf
v3.2.1,reset histogram pool
v3.2.1,initialize data partition
v3.2.1,reset the splits for leaves
v3.2.1,Sumup for root
v3.2.1,use all data
v3.2.1,"use bagging, only use part of data"
v3.2.1,check depth of current leaf
v3.2.1,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.2.1,no enough data to continue
v3.2.1,only have root
v3.2.1,put parent(left) leaf's histograms into larger leaf's histograms
v3.2.1,put parent(left) leaf's histograms to larger leaf's histograms
v3.2.1,construct smaller leaf
v3.2.1,construct larger leaf
v3.2.1,find splits
v3.2.1,only has root leaf
v3.2.1,start at root leaf
v3.2.1,"before processing next node from queue, store info for current left/right leaf"
v3.2.1,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.2.1,"then, compute own splits"
v3.2.1,split info should exist because searching in bfs fashion - should have added from parent
v3.2.1,update before tree split
v3.2.1,don't need to update this in data-based parallel model
v3.2.1,"split tree, will return right leaf"
v3.2.1,store the true split gain in tree model
v3.2.1,don't need to update this in data-based parallel model
v3.2.1,store the true split gain in tree model
v3.2.1,init the leaves that used on next iteration
v3.2.1,update leave outputs if needed
v3.2.1,bag_mapper[index_mapper[i]]
v3.2.1,it is needed to filter the features after the above code.
v3.2.1,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.2.1,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.2.1,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.2.1,find splits
v3.2.1,identify features containing nans
v3.2.1,preallocate the matrix used to calculate linear model coefficients
v3.2.1,"store only upper triangular half of matrix as an array, in row-major order"
v3.2.1,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.2.1,we add another 8 to ensure cache lines are not shared among processors
v3.2.1,some initial works before training
v3.2.1,root leaf
v3.2.1,only root leaf can be splitted on first time
v3.2.1,some initial works before finding best split
v3.2.1,find best threshold for every feature
v3.2.1,Get a leaf with max split gain
v3.2.1,Get split information for best leaf
v3.2.1,"cannot split, quit"
v3.2.1,split tree with best leaf
v3.2.1,map data to leaf number
v3.2.1,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.2.1,the coefficients vector is given by
v3.2.1,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.2.1,where:
v3.2.1,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.2.1,"H is the diagonal matrix of the hessian,"
v3.2.1,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.2.1,g is the vector of gradients
v3.2.1,the subscript _T denotes the transpose
v3.2.1,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.2.1,clear the coefficient matrices
v3.2.1,aggregate results from different threads
v3.2.1,copy into eigen matrices and solve
v3.2.1,update the tree properties
v3.2.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.2.1,get feature partition
v3.2.1,get local used features
v3.2.1,get best split at smaller leaf
v3.2.1,find local best split for larger leaf
v3.2.1,sync global best info
v3.2.1,update best split
v3.2.1,"instantiate template classes, otherwise linker cannot find the code"
v3.2.1,initialize SerialTreeLearner
v3.2.1,Get local rank and global machine size
v3.2.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.2.1,allocate buffer for communication
v3.2.1,generate feature partition for current tree
v3.2.1,get local used feature
v3.2.1,get block start and block len for reduce scatter
v3.2.1,get buffer_write_start_pos_
v3.2.1,get buffer_read_start_pos_
v3.2.1,sync global data sumup info
v3.2.1,global sumup reduce
v3.2.1,copy back
v3.2.1,set global sumup info
v3.2.1,init global data count in leaf
v3.2.1,construct local histograms
v3.2.1,copy to buffer
v3.2.1,Reduce scatter for histogram
v3.2.1,restore global histograms from buffer
v3.2.1,only root leaf
v3.2.1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.2.1,find local best split for larger leaf
v3.2.1,sync global best info
v3.2.1,set best split
v3.2.1,need update global number of data in leaf
v3.2.1,"instantiate template classes, otherwise linker cannot find the code"
v3.2.1,initialize SerialTreeLearner
v3.2.1,some additional variables needed for GPU trainer
v3.2.1,Initialize GPU buffers and kernels
v3.2.1,some functions used for debugging the GPU histogram construction
v3.2.1,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.2.1,goto err;
v3.2.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.2.1,also guarantee that there are at least 2K examples per workgroup
v3.2.1,return 0;
v3.2.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.2.1,decide the best number of workgroups working on one feature4 tuple
v3.2.1,set work group size based on feature size
v3.2.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.2.1,we need to refresh the kernel arguments after reallocating
v3.2.1,The only argument that needs to be changed later is num_data_
v3.2.1,"the GPU kernel will process all features in one call, and each"
v3.2.1,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.2.1,process one feature4 tuple
v3.2.1,"for the root node, indices are not copied"
v3.2.1,"for constant hessian, hessians are not copied except for the root node"
v3.2.1,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.2.1,and we will launch num_feature workgroups for this kernel
v3.2.1,will launch threads for all features
v3.2.1,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.2.1,copy the results asynchronously. Size depends on if double precision is used
v3.2.1,we will wait for this object in WaitAndGetHistograms
v3.2.1,"when the output is ready, the computation is done"
v3.2.1,values of this feature has been redistributed to multiple bins; need a reduction here
v3.2.1,how many feature-group tuples we have
v3.2.1,leave some safe margin for prefetching
v3.2.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.2.1,clear sparse/dense maps
v3.2.1,do nothing if no features can be processed on GPU
v3.2.1,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.2.1,unpin old buffer if necessary before destructing them
v3.2.1,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v3.2.1,allocate space for gradients and hessians on device
v3.2.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.2.1,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.2.1,copy indices to the device
v3.2.1,histogram bin entry size depends on the precision (single/double)
v3.2.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.2.1,each work group generates a sub-histogram of dword_features_ features.
v3.2.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.2.1,create atomic counters for inter-group coordination
v3.2.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.2.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.2.1,looking for dword_features_ non-sparse feature-groups
v3.2.1,decide if we need to redistribute the bin
v3.2.1,multiplier must be a power of 2
v3.2.1,device_bin_mults_.push_back(1);
v3.2.1,found
v3.2.1,for data transfer time
v3.2.1,"Now generate new data structure feature4, and copy data to the device"
v3.2.1,"preallocate arrays for all threads, and pin them"
v3.2.1,building Feature4 bundles; each thread handles dword_features_ features
v3.2.1,one feature datapoint is 4 bits
v3.2.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.2.1,one feature datapoint is one byte
v3.2.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.2.1,Dense bin
v3.2.1,Dense 4-bit bin
v3.2.1,working on the remaining (less than dword_features_) feature groups
v3.2.1,fill the leftover features
v3.2.1,"fill this empty feature with some ""random"" value"
v3.2.1,"fill this empty feature with some ""random"" value"
v3.2.1,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.2.1,deallocate pinned space for feature copying
v3.2.1,data transfer time
v3.2.1,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.2.1,"Something bad happened. Just return ""No log available."""
v3.2.1,"build is okay, log may contain warnings"
v3.2.1,destroy any old kernels
v3.2.1,create OpenCL kernels for different number of workgroups per feature
v3.2.1,currently we don't use constant memory
v3.2.1,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.2.1,kernel with indices in an array
v3.2.1,"kernel with all features enabled, with elimited branches"
v3.2.1,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.2.1,do nothing if no features can be processed on GPU
v3.2.1,The only argument that needs to be changed later is num_data_
v3.2.1,"hessian is passed as a parameter, but it is not available now."
v3.2.1,hessian will be set in BeforeTrain()
v3.2.1,"Get the max bin size, used for selecting best GPU kernel"
v3.2.1,initialize GPU
v3.2.1,determine which kernel to use based on the max number of bins
v3.2.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.2.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.2.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.2.1,setup GPU kernel arguments after we allocating all the buffers
v3.2.1,GPU memory has to been reallocated because data may have been changed
v3.2.1,setup GPU kernel arguments after we allocating all the buffers
v3.2.1,Copy initial full hessians and gradients to GPU.
v3.2.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.2.1,setup hessian parameters only
v3.2.1,hessian is passed as a parameter
v3.2.1,use bagging
v3.2.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.2.1,copy used gradients and hessians to ordered buffer
v3.2.1,transfer the indices to GPU
v3.2.1,transfer hessian to GPU
v3.2.1,setup hessian parameters only
v3.2.1,hessian is passed as a parameter
v3.2.1,transfer gradients to GPU
v3.2.1,only have root
v3.2.1,"Copy indices, gradients and hessians as early as possible"
v3.2.1,only need to initialize for smaller leaf
v3.2.1,Get leaf boundary
v3.2.1,copy indices to the GPU:
v3.2.1,copy ordered hessians to the GPU:
v3.2.1,copy ordered gradients to the GPU:
v3.2.1,do nothing if no features can be processed on GPU
v3.2.1,copy data indices if it is not null
v3.2.1,generate and copy ordered_gradients if gradients is not null
v3.2.1,generate and copy ordered_hessians if hessians is not null
v3.2.1,converted indices in is_feature_used to feature-group indices
v3.2.1,construct the feature masks for dense feature-groups
v3.2.1,"if no feature group is used, just return and do not use GPU"
v3.2.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.2.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.2.1,"All data have been prepared, now run the GPU kernel"
v3.2.1,construct smaller leaf
v3.2.1,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v3.2.1,then construct sparse features on CPU
v3.2.1,"wait for GPU to finish, only if GPU is actually used"
v3.2.1,use double precision
v3.2.1,use single precision
v3.2.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.2.1,#define GPU_DEBUG_COMPARE
v3.2.1,construct larger leaf
v3.2.1,then construct sparse features on CPU
v3.2.1,"wait for GPU to finish, only if GPU is actually used"
v3.2.1,use double precision
v3.2.1,use single precision
v3.2.1,do some sanity check for the GPU algorithm
v3.2.1,limit top k
v3.2.1,get max bin
v3.2.1,calculate buffer size
v3.2.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.2.1,"left and right on same time, so need double size"
v3.2.1,initialize histograms for global
v3.2.1,sync global data sumup info
v3.2.1,set global sumup info
v3.2.1,init global data count in leaf
v3.2.1,get local sumup
v3.2.1,get local sumup
v3.2.1,get mean number on machines
v3.2.1,weighted gain
v3.2.1,get top k
v3.2.1,"Copy histogram to buffer, and Get local aggregate features"
v3.2.1,copy histograms.
v3.2.1,copy smaller leaf histograms first
v3.2.1,mark local aggregated feature
v3.2.1,copy
v3.2.1,then copy larger leaf histograms
v3.2.1,mark local aggregated feature
v3.2.1,copy
v3.2.1,use local data to find local best splits
v3.2.1,find splits
v3.2.1,only has root leaf
v3.2.1,local voting
v3.2.1,gather
v3.2.1,get all top-k from all machines
v3.2.1,global voting
v3.2.1,copy local histgrams to buffer
v3.2.1,Reduce scatter for histogram
v3.2.1,find best split from local aggregated histograms
v3.2.1,restore from buffer
v3.2.1,restore from buffer
v3.2.1,find local best
v3.2.1,find local best split for larger leaf
v3.2.1,sync global best info
v3.2.1,copy back
v3.2.1,set the global number of data for leaves
v3.2.1,init the global sumup info
v3.2.1,"instantiate template classes, otherwise linker cannot find the code"
v3.2.1,launch cuda kernel
v3.2.1,initialize SerialTreeLearner
v3.2.1,some additional variables needed for GPU trainer
v3.2.1,Initialize GPU buffers and kernels: get device info
v3.2.1,some functions used for debugging the GPU histogram construction
v3.2.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.2.1,also guarantee that there are at least 2K examples per workgroup
v3.2.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.2.1,decide the best number of workgroups working on one feature4 tuple
v3.2.1,set work group size based on feature size
v3.2.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.2.1,set thread_data
v3.2.1,copy the results asynchronously. Size depends on if double precision is used
v3.2.1,"when the output is ready, the computation is done"
v3.2.1,how many feature-group tuples we have
v3.2.1,leave some safe margin for prefetching
v3.2.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.2.1,clear sparse/dense maps
v3.2.1,do nothing it there is no dense feature
v3.2.1,calculate number of feature groups per gpu
v3.2.1,histogram bin entry size depends on the precision (single/double)
v3.2.1,allocate GPU memory for each GPU
v3.2.1,do nothing it there is no gpu feature
v3.2.1,allocate memory for all features
v3.2.1,allocate space for gradients and hessians on device
v3.2.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.2.1,copy indices to the device
v3.2.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.2.1,each work group generates a sub-histogram of dword_features_ features.
v3.2.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.2.1,create atomic counters for inter-group coordination
v3.2.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.2.1,clear sparse/dense maps
v3.2.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.2.1,set device info
v3.2.1,looking for dword_features_ non-sparse feature-groups
v3.2.1,reset device info
v3.2.1,InitGPU w/ num_gpu
v3.2.1,"Get the max bin size, used for selecting best GPU kernel"
v3.2.1,get num_dense_feature_groups_
v3.2.1,initialize GPU
v3.2.1,set cpu threads
v3.2.1,resize device memory pointers
v3.2.1,create stream & events to handle multiple GPUs
v3.2.1,check data size
v3.2.1,GPU memory has to been reallocated because data may have been changed
v3.2.1,AllocateGPUMemory only when the number of data increased
v3.2.1,setup GPU kernel arguments after we allocating all the buffers
v3.2.1,Copy initial full hessians and gradients to GPU.
v3.2.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.2.1,use bagging
v3.2.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.2.1,copy used gradients and hessians to ordered buffer
v3.2.1,transfer the indices to GPU
v3.2.1,only have root
v3.2.1,"Copy indices, gradients and hessians as early as possible"
v3.2.1,only need to initialize for smaller leaf
v3.2.1,Get leaf boundary
v3.2.1,do nothing if no features can be processed on GPU
v3.2.1,copy data indices if it is not null
v3.2.1,converted indices in is_feature_used to feature-group indices
v3.2.1,construct the feature masks for dense feature-groups
v3.2.1,"if no feature group is used, just return and do not use GPU"
v3.2.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.2.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.2.1,We now copy even if all features are used.
v3.2.1,"All data have been prepared, now run the GPU kernel"
v3.2.1,construct smaller leaf
v3.2.1,Check workgroups per feature4 tuple..
v3.2.1,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.2.1,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.2.1,then construct sparse features on CPU
v3.2.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.2.1,"wait for GPU to finish, only if GPU is actually used"
v3.2.1,use double precision
v3.2.1,use single precision
v3.2.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.2.1,#define CUDA_DEBUG_COMPARE
v3.2.1,construct larger leaf
v3.2.1,then construct sparse features on CPU
v3.2.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.2.1,"wait for GPU to finish, only if GPU is actually used"
v3.2.1,use double precision
v3.2.1,use single precision
v3.2.1,do some sanity check for the GPU algorithm
v3.2.0,coding: utf-8
v3.2.0,see https://github.com/pypa/distutils/pull/21
v3.2.0,coding: utf-8
v3.2.0,create predictor first
v3.2.0,check dataset
v3.2.0,reduce cost for prediction training data
v3.2.0,process callbacks
v3.2.0,Most of legacy advanced options becomes callbacks
v3.2.0,construct booster
v3.2.0,start training
v3.2.0,check evaluation result.
v3.2.0,"ranking task, split according to groups"
v3.2.0,run preprocessing on the data set if needed
v3.2.0,setup callbacks
v3.2.0,coding: utf-8
v3.2.0,dummy function to support older version of scikit-learn
v3.2.0,coding: utf-8
v3.2.0,documentation templates for LGBMModel methods are shared between the classes in
v3.2.0,this module and those in the ``dask`` module
v3.2.0,"user can set verbose with kwargs, it has higher priority"
v3.2.0,Do not modify original args in fit function
v3.2.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.2.0,Separate built-in from callable evaluation metrics
v3.2.0,register default metric for consistency with callable eval_metric case
v3.2.0,try to deduce from class instance
v3.2.0,overwrite default metric by explicitly set metric
v3.2.0,concatenate metric from params (or default if not provided in params) and eval_metric
v3.2.0,copy for consistency
v3.2.0,reduce cost for prediction training data
v3.2.0,free dataset
v3.2.0,Switch to using a multiclass objective in the underlying LGBM instance
v3.2.0,"do not modify args, as it causes errors in model selection tools"
v3.2.0,check group data
v3.2.0,coding: utf-8
v3.2.0,we don't need lib_lightgbm while building docs
v3.2.0,coding: utf-8
v3.2.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.2.0,coding: utf-8
v3.2.0,coding: utf-8
v3.2.0,TypeError: obj is not a string or a number
v3.2.0,ValueError: invalid literal
v3.2.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.2.0,avoid side effects on passed-in parameters
v3.2.0,"find a value, and remove other aliases with .pop()"
v3.2.0,"prefer the value of 'main_param_name' if it exists, otherwise search the aliases"
v3.2.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.2.0,avoid memory consumption by arrays concatenation operations
v3.2.0,create numpy array from output arrays
v3.2.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.2.0,for CSC there is extra column added
v3.2.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.2.0,same shape as input csr or csc matrix except extra column for expected value
v3.2.0,note: make sure we copy data as it will be deallocated next
v3.2.0,"free the temporary native indptr, indices, and data"
v3.2.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.2.0,avoid memory consumption by arrays concatenation operations
v3.2.0,"no min_data, nthreads and verbose in this function"
v3.2.0,check data has header or not
v3.2.0,need to regroup init_score
v3.2.0,process for args
v3.2.0,"user can set verbose with params, it has higher priority"
v3.2.0,get categorical features
v3.2.0,process for reference dataset
v3.2.0,start construct data
v3.2.0,set feature names
v3.2.0,create valid
v3.2.0,construct subset
v3.2.0,create train
v3.2.0,could be updated if data is not freed
v3.2.0,set to None
v3.2.0,we're done if self and reference share a common upstrem reference
v3.2.0,"group data from LightGBM is boundaries data, need to convert to group size"
v3.2.0,"user can set verbose with params, it has higher priority"
v3.2.0,Training task
v3.2.0,"if ""machines"" is given, assume user wants to do distributed learning, and set up network"
v3.2.0,construct booster object
v3.2.0,copy the parameters from train_set
v3.2.0,save reference to data
v3.2.0,buffer for inner predict
v3.2.0,Prediction task
v3.2.0,if a single node tree it won't have `leaf_index` so return 0
v3.2.0,"Create the node record, and populate universal data members"
v3.2.0,Update values to reflect node type (leaf or split)
v3.2.0,traverse the next level of the tree
v3.2.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.2.0,and we add node to the list.
v3.2.0,need reset training data
v3.2.0,need to push new valid data
v3.2.0,"if buffer length is not long enough, re-allocate a buffer"
v3.2.0,"if buffer length is not long enough, reallocate a buffer"
v3.2.0,Copy models
v3.2.0,Get name of features
v3.2.0,avoid to predict many time in one iteration
v3.2.0,Get num of inner evals
v3.2.0,Get name of evals
v3.2.0,coding: utf-8
v3.2.0,Callback environment used by callbacks
v3.2.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.2.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.2.0,coding: utf-8
v3.2.0,Concatenate many parts into one
v3.2.0,capture whether local_listen_port or its aliases were provided
v3.2.0,capture whether machines or its aliases were provided
v3.2.0,Some passed-in parameters can be removed:
v3.2.0,* 'num_machines': set automatically from Dask worker list
v3.2.0,* 'num_threads': overridden to match nthreads on each Dask process
v3.2.0,Split arrays/dataframes into parts. Arrange parts into dicts to enforce co-locality
v3.2.0,Start computation in the background
v3.2.0,Find locations of all parts and map them to particular Dask workers
v3.2.0,resolve aliases for network parameters and pop the result off params.
v3.2.0,these values are added back in calls to `_train_part()`
v3.2.0,figure out network params
v3.2.0,Tell each worker to train on the parts that it has locally
v3.2.0,"if network parameters were changed during training, remove them from the"
v3.2.0,returned moodel so that they're generated dynamically on every run based
v3.2.0,on the Dask cluster you're connected to and which workers have pieces of
v3.2.0,the training data
v3.2.0,dask.DataFrame.map_partitions() expects each call to return a pandas DataFrame or Series
v3.2.0,the note on custom objective functions in LGBMModel.__init__ is not
v3.2.0,currently relevant for the Dask estimators
v3.2.0,"DaskLGBMClassifier does not support evaluation data, or early stopping"
v3.2.0,DaskLGBMClassifier support for callbacks and init_model is not tested
v3.2.0,the note on custom objective functions in LGBMModel.__init__ is not
v3.2.0,currently relevant for the Dask estimators
v3.2.0,"DaskLGBMRegressor does not support evaluation data, or early stopping"
v3.2.0,DaskLGBMRegressor support for callbacks and init_model is not tested
v3.2.0,the note on custom objective functions in LGBMModel.__init__ is not
v3.2.0,currently relevant for the Dask estimators
v3.2.0,"DaskLGBMRanker does not support evaluation data, or early stopping"
v3.2.0,DaskLGBMRanker support for callbacks and init_model is not tested
v3.2.0,coding: utf-8
v3.2.0,load or create your dataset
v3.2.0,create dataset for lightgbm
v3.2.0,"if you want to re-use data, remember to set free_raw_data=False"
v3.2.0,specify your configurations as a dict
v3.2.0,generate feature names
v3.2.0,feature_name and categorical_feature
v3.2.0,check feature name
v3.2.0,save model to file
v3.2.0,dump model to JSON (and save to file)
v3.2.0,feature names
v3.2.0,feature importances
v3.2.0,load model to predict
v3.2.0,can only predict with the best iteration (or the saving iteration)
v3.2.0,eval with loaded model
v3.2.0,dump model with pickle
v3.2.0,load model with pickle to predict
v3.2.0,can predict with any iteration when loaded in pickle way
v3.2.0,eval with loaded model
v3.2.0,continue training
v3.2.0,init_model accepts:
v3.2.0,1. model file name
v3.2.0,2. Booster()
v3.2.0,decay learning rates
v3.2.0,learning_rates accepts:
v3.2.0,1. list/tuple with length = num_boost_round
v3.2.0,2. function(curr_iter)
v3.2.0,change other parameters during training
v3.2.0,self-defined objective function
v3.2.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.2.0,log likelihood loss
v3.2.0,self-defined eval metric
v3.2.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.0,binary error
v3.2.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.2.0,This may make built-in evalution metric calculate wrong results
v3.2.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.2.0,Keep this in mind when you use the customization
v3.2.0,another self-defined eval metric
v3.2.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.0,accuracy
v3.2.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.2.0,This may make built-in evalution metric calculate wrong results
v3.2.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.2.0,Keep this in mind when you use the customization
v3.2.0,callback
v3.2.0,coding: utf-8
v3.2.0,load or create your dataset
v3.2.0,train
v3.2.0,predict
v3.2.0,eval
v3.2.0,feature importances
v3.2.0,self-defined eval metric
v3.2.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.0,Root Mean Squared Logarithmic Error (RMSLE)
v3.2.0,train
v3.2.0,another self-defined eval metric
v3.2.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.2.0,Relative Absolute Error (RAE)
v3.2.0,train
v3.2.0,predict
v3.2.0,eval
v3.2.0,other scikit-learn modules
v3.2.0,coding: utf-8
v3.2.0,load or create your dataset
v3.2.0,create dataset for lightgbm
v3.2.0,specify your configurations as a dict
v3.2.0,train
v3.2.0,coding: utf-8
v3.2.0,################
v3.2.0,Simulate some binary data with a single categorical and
v3.2.0,single continuous predictor
v3.2.0,################
v3.2.0,Set up a couple of utilities for our experiments
v3.2.0,################
v3.2.0,Observe the behavior of `binary` and `xentropy` objectives
v3.2.0,Trying this throws an error on non-binary values of y:
v3.2.0,"experiment('binary', label_type='probability', DATA)"
v3.2.0,The speed of `binary` is not drastically different than
v3.2.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.2.0,there are reasons to suspect that `binary` should run faster when the
v3.2.0,label is an integer instead of a float
v3.2.0,coding: utf-8
v3.2.0,load or create your dataset
v3.2.0,create dataset for lightgbm
v3.2.0,specify your configurations as a dict
v3.2.0,train
v3.2.0,save model to file
v3.2.0,predict
v3.2.0,eval
v3.2.0,split training data into two partitions
v3.2.0,make this array dense because we're splitting across
v3.2.0,a sparse boundary to partition the data
v3.2.0,"the code below uses sklearn.metrics, but this requires pulling all of the"
v3.2.0,predictions and target values back from workers to the client
v3.2.0,
v3.2.0,"for larger datasets, consider the metrics from dask-ml instead"
v3.2.0,https://ml.dask.org/modules/api.html#dask-ml-metrics-metrics
v3.2.0,coding: utf-8
v3.2.0,!/usr/bin/env python3
v3.2.0,-*- coding: utf-8 -*-
v3.2.0,
v3.2.0,"LightGBM documentation build configuration file, created by"
v3.2.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.2.0,
v3.2.0,This file is execfile()d with the current directory set to its
v3.2.0,containing dir.
v3.2.0,
v3.2.0,Note that not all possible configuration values are present in this
v3.2.0,autogenerated file.
v3.2.0,
v3.2.0,All configuration values have a default; values that are commented out
v3.2.0,serve to show the default.
v3.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.2.0,add these directories to sys.path here. If the directory is relative to the
v3.2.0,"documentation root, use os.path.abspath to make it absolute."
v3.2.0,-- mock out modules
v3.2.0,-- General configuration ------------------------------------------------
v3.2.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.2.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.2.0,ones.
v3.2.0,hide type hints in API docs
v3.2.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.2.0,Only the class' docstring is inserted.
v3.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.2.0,The master toctree document.
v3.2.0,General information about the project.
v3.2.0,The name of an image file (relative to this directory) to place at the top
v3.2.0,of the sidebar.
v3.2.0,The name of an image file (relative to this directory) to use as a favicon of
v3.2.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.2.0,pixels large.
v3.2.0,"The version info for the project you're documenting, acts as replacement for"
v3.2.0,"|version| and |release|, also used in various other places throughout the"
v3.2.0,built documents.
v3.2.0,The short X.Y version.
v3.2.0,"The full version, including alpha/beta/rc tags."
v3.2.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.2.0,for a list of supported languages.
v3.2.0,
v3.2.0,This is also used if you do content translation via gettext catalogs.
v3.2.0,"Usually you set ""language"" from the command line for these cases."
v3.2.0,"List of patterns, relative to source directory, that match files and"
v3.2.0,directories to ignore when looking for source files.
v3.2.0,This patterns also effect to html_static_path and html_extra_path
v3.2.0,The name of the Pygments (syntax highlighting) style to use.
v3.2.0,-- Configuration for C API docs generation ------------------------------
v3.2.0,-- Options for HTML output ----------------------------------------------
v3.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.2.0,a list of builtin themes.
v3.2.0,Theme options are theme-specific and customize the look and feel of a theme
v3.2.0,"further.  For a list of options available for each theme, see the"
v3.2.0,documentation.
v3.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.2.0,"relative to this directory. They are copied after the builtin static files,"
v3.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.2.0,-- Options for HTMLHelp output ------------------------------------------
v3.2.0,Output file base name for HTML help builder.
v3.2.0,-- Options for LaTeX output ---------------------------------------------
v3.2.0,The name of an image file (relative to this directory) to place at the top of
v3.2.0,the title page.
v3.2.0,Warning! The following code can cause buffer overflows on RTD.
v3.2.0,Consider suppressing output completely if RTD project silently fails.
v3.2.0,Refer to https://github.com/svenevs/exhale
v3.2.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.2.0,Warning! The following code can cause buffer overflows on RTD.
v3.2.0,Consider suppressing output completely if RTD project silently fails.
v3.2.0,Refer to https://github.com/svenevs/exhale
v3.2.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.2.0,coding: utf-8
v3.2.0,Constants
v3.2.0,Start with some content:
v3.2.0,Clear & re-use:
v3.2.0,Output should match new content:
v3.2.0,Number of trials for each new ChunkedArray configuration. Pass 100 times over the search space:
v3.2.0,Each outer loop iteration changes the test by adding +1 chunk. We start with 1 chunk only:
v3.2.0,Sweep valid and invalid addresses with a ChunkedArray with `chunks` chunks:
v3.2.0,Compute a new trial address & value & if it is a valid address:
v3.2.0,"Insert item. If at a valid address, 0 is returned, otherwise, -1 is returned:"
v3.2.0,"If at valid address, check that the stored value is correct & remember it for the future:"
v3.2.0,Check the just-stored value with getitem():
v3.2.0,Also store the just-stored value for future tracking:
v3.2.0,"Final check: ensure even with overrides, all valid insertions store the latest value at that address:"
v3.2.0,coding: utf-8
v3.2.0,we don't need lib_lightgbm while building docs
v3.2.0,coding: utf-8
v3.2.0,check saved model persistence
v3.2.0,"we need to check the consistency of model file here, so test for exact equal"
v3.2.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.2.0,"scores likely to be different, but prediction should still be the same"
v3.2.0,test that shape is checked during prediction
v3.2.0,test that method works even with free_raw_data=True
v3.2.0,test that method works but sets raw data to None in case of immergeable data types
v3.2.0,test that method works for different data types
v3.2.0,"Set extremely harsh penalties, so CEGB will block most splits."
v3.2.0,"Compare pairs of penalties, to ensure scaling works as intended"
v3.2.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.2.0,"should resolve duplicate aliases, and prefer the main parameter"
v3.2.0,should choose a value from an alias and set that value on main param
v3.2.0,if only an alias is used
v3.2.0,should use the default if main param and aliases are missing
v3.2.0,all changes should be made on copies and not modify the original
v3.2.0,coding: utf-8
v3.2.0,"time, in seconds, to wait for the Dask client to close. Used to avoid teardown errors"
v3.2.0,see https://distributed.dask.org/en/latest/api.html#distributed.Client.close
v3.2.0,"add target, weight, and group to DataFrame so that partitions abide by group boundaries."
v3.2.0,set_index ensures partitions are based on group id.
v3.2.0,See https://stackoverflow.com/questions/49532824/dask-dataframe-split-partitions-based-on-a-column-or-function.
v3.2.0,"separate target, weight from features."
v3.2.0,"encode group identifiers into run-length encoding, the format LightGBMRanker is expecting"
v3.2.0,"so that within each partition, sum(g) = n_samples."
v3.2.0,ranking arrays: one chunk per group. Each chunk must include all columns.
v3.2.0,"for the small data sizes used in tests, it's hard to get LGBMRegressor to choose"
v3.2.0,"categorical features for splits. So for regression tests with categorical features,"
v3.2.0,_create_data() returns a DataFrame with ONLY categorical features
v3.2.0,pref_leaf values should have the right shape
v3.2.0,and values that look like valid tree nodes
v3.2.0,"be sure LightGBM actually used at least one categorical column,"
v3.2.0,and that it was correctly treated as a categorical feature
v3.2.0,"be sure LightGBM actually used at least one categorical column,"
v3.2.0,and that it was correctly treated as a categorical feature
v3.2.0,shape depends on whether it is binary or multiclass classification
v3.2.0,* shape depends on whether it is binary or multiclass classification
v3.2.0,"* matrix for binary classification is of the form [feature_contrib, base_value],"
v3.2.0,"for multi-class it's [feat_contrib_class1, base_value_class1, feat_contrib_class2, base_value_class2, etc.]"
v3.2.0,"* contrib outputs for distributed training are different than from local training, so we can just test"
v3.2.0,that the output has the right shape and base values are in the right position
v3.2.0,check that found ports are different for same address (LocalCluster)
v3.2.0,check that the ports are indeed open
v3.2.0,Scores should be the same
v3.2.0,Predictions should be roughly the same.
v3.2.0,pref_leaf values should have the right shape
v3.2.0,and values that look like valid tree nodes
v3.2.0,The checks below are skipped
v3.2.0,for the categorical data case because it's difficult to get
v3.2.0,a good fit from just categoricals for a regression problem
v3.2.0,with small data
v3.2.0,"be sure LightGBM actually used at least one categorical column,"
v3.2.0,and that it was correctly treated as a categorical feature
v3.2.0,"contrib outputs for distributed training are different than from local training, so we can just test"
v3.2.0,that the output has the right shape and base values are in the right position
v3.2.0,"be sure LightGBM actually used at least one categorical column,"
v3.2.0,and that it was correctly treated as a categorical feature
v3.2.0,Quantiles should be right
v3.2.0,"be sure LightGBM actually used at least one categorical column,"
v3.2.0,and that it was correctly treated as a categorical feature
v3.2.0,rebalance small dask.Array dataset for better performance.
v3.2.0,"use many trees + leaves to overfit, help ensure that Dask data-parallel strategy matches that of"
v3.2.0,serial learner. See https://github.com/microsoft/LightGBM/issues/3292#issuecomment-671288210.
v3.2.0,distributed ranker should be able to rank decently well and should
v3.2.0,have high rank correlation with scores from serial ranker.
v3.2.0,pref_leaf values should have the right shape
v3.2.0,and values that look like valid tree nodes
v3.2.0,"be sure LightGBM actually used at least one categorical column,"
v3.2.0,and that it was correctly treated as a categorical feature
v3.2.0,should be able to use the class without specifying a client
v3.2.0,should be able to set client after construction
v3.2.0,data on cluster1
v3.2.0,create identical data on cluster2
v3.2.0,"at this point, the result of default_client() is client2 since it was the most recently"
v3.2.0,created. So setting client to client1 here to test that you can select a non-default client
v3.2.0,"unfitted model should survive pickling round trip, and pickling"
v3.2.0,shouldn't have side effects on the model object
v3.2.0,client will always be None after unpickling
v3.2.0,"fitted model should survive pickling round trip, and pickling"
v3.2.0,shouldn't have side effects on the model object
v3.2.0,client will always be None after unpickling
v3.2.0,rebalance data to be sure that each worker has a piece of the data
v3.2.0,model 1 - no network parameters given
v3.2.0,model 2 - machines given
v3.2.0,model 3 - local_listen_port given
v3.2.0,training should fail because LightGBM will try to use the same
v3.2.0,port for multiple worker processes on the same machine
v3.2.0,rebalance data to be sure that each worker has a piece of the data
v3.2.0,"test that ""machines"" is actually respected by creating a socket that uses"
v3.2.0,"one of the ports mentioned in ""machines"""
v3.2.0,"an informative error should be raised if ""machines"" has duplicates"
v3.2.0,"""client"" should be the only different, and the final argument"
v3.2.0,"init_scores must be a 1D array, even for multiclass classification"
v3.2.0,where you need to provide 1 score per class for each row in X
v3.2.0,https://github.com/microsoft/LightGBM/issues/4046
v3.2.0,value of the root node is 0 when init_score is set
v3.2.0,this test is separate because it takes a not-yet-constructed estimator
v3.2.0,coding: utf-8
v3.2.0,coding: utf-8
v3.2.0,"build target, group ID vectors."
v3.2.0,build y/target and group-id vectors with user-specified group sizes.
v3.2.0,"build y/target and group-id vectors according to n_samples, avg_gs, and random_gs."
v3.2.0,groups should contain > 1 element for pairwise learning objective.
v3.2.0,"build feature data, X. Transform first few into informative features."
v3.2.0,coding: utf-8
v3.2.0,prediction result is actually not transformed (is raw) due to custom objective
v3.2.0,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.2.0,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.2.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.2.0,sklearn < 0.23 does not have as_frame parameter
v3.2.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.2.0,sklearn < 0.23 does not have as_frame parameter
v3.2.0,Test if random_state is properly stored
v3.2.0,Test if two random states produce identical models
v3.2.0,Test if subsequent fits sample from random_state object and produce different models
v3.2.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.2.0,With default params
v3.2.0,Tests same probabilities
v3.2.0,Tests same predictions
v3.2.0,Tests same raw scores
v3.2.0,Tests same leaf indices
v3.2.0,Tests same feature contributions
v3.2.0,Tests other parameters for the prediction works
v3.2.0,Tests start_iteration
v3.2.0,"Tests same probabilities, starting from iteration 10"
v3.2.0,"Tests same predictions, starting from iteration 10"
v3.2.0,"Tests same raw scores, starting from iteration 10"
v3.2.0,"Tests same leaf indices, starting from iteration 10"
v3.2.0,"Tests same feature contributions, starting from iteration 10"
v3.2.0,"Tests other parameters for the prediction works, starting from iteration 10"
v3.2.0,"no custom objective, no custom metric"
v3.2.0,default metric
v3.2.0,non-default metric
v3.2.0,no metric
v3.2.0,non-default metric in eval_metric
v3.2.0,non-default metric with non-default metric in eval_metric
v3.2.0,non-default metric with multiple metrics in eval_metric
v3.2.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.2.0,default metric for non-default objective
v3.2.0,non-default metric for non-default objective
v3.2.0,no metric
v3.2.0,non-default metric in eval_metric for non-default objective
v3.2.0,non-default metric with non-default metric in eval_metric for non-default objective
v3.2.0,non-default metric with multiple metrics in eval_metric for non-default objective
v3.2.0,"custom objective, no custom metric"
v3.2.0,default regression metric for custom objective
v3.2.0,non-default regression metric for custom objective
v3.2.0,multiple regression metrics for custom objective
v3.2.0,no metric
v3.2.0,default regression metric with non-default metric in eval_metric for custom objective
v3.2.0,non-default regression metric with metric in eval_metric for custom objective
v3.2.0,multiple regression metrics with metric in eval_metric for custom objective
v3.2.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.2.0,"no custom objective, custom metric"
v3.2.0,default metric with custom metric
v3.2.0,non-default metric with custom metric
v3.2.0,multiple metrics with custom metric
v3.2.0,custom metric (disable default metric)
v3.2.0,default metric for non-default objective with custom metric
v3.2.0,non-default metric for non-default objective with custom metric
v3.2.0,multiple metrics for non-default objective with custom metric
v3.2.0,custom metric (disable default metric for non-default objective)
v3.2.0,"custom objective, custom metric"
v3.2.0,custom metric for custom objective
v3.2.0,non-default regression metric with custom metric for custom objective
v3.2.0,multiple regression metrics with custom metric for custom objective
v3.2.0,default metric and invalid binary metric is replaced with multiclass alternative
v3.2.0,invalid objective is replaced with default multiclass one
v3.2.0,and invalid binary metric is replaced with multiclass alternative
v3.2.0,default metric for non-default multiclass objective
v3.2.0,and invalid binary metric is replaced with multiclass alternative
v3.2.0,default metric and invalid multiclass metric is replaced with binary alternative
v3.2.0,invalid multiclass metric is replaced with binary alternative for custom objective
v3.2.0,"Verify that can receive a list of metrics, only callable"
v3.2.0,Verify that can receive a list of custom and built-in metrics
v3.2.0,Verify that works as expected when eval_metric is empty
v3.2.0,"Verify that can receive a list of metrics, only built-in"
v3.2.0,Verify that eval_metric is robust to receiving a list with None
v3.2.0,training data as eval_set
v3.2.0,feval
v3.2.0,single eval_set
v3.2.0,two eval_set
v3.2.0,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.2.0,Test that estimators are default-constructible
v3.2.0,coding: utf-8
v3.2.0,coding: utf-8
v3.2.0,check that default gives same result as k = 1
v3.2.0,check against independent calculation for k = 1
v3.2.0,check against independent calculation for k = 2
v3.2.0,check against independent calculation for k = 10
v3.2.0,check cases where predictions are equal
v3.2.0,should give same result as binary auc for 2 classes
v3.2.0,test the case where all predictions are equal
v3.2.0,test that weighted data gives different auc_mu
v3.2.0,test that equal data weights give same auc_mu as unweighted data
v3.2.0,should give 1 when accuracy = 1
v3.2.0,test loading class weights
v3.2.0,no early stopping
v3.2.0,early stopping occurs
v3.2.0,test custom eval metrics
v3.2.0,"shuffle = False, override metric in params"
v3.2.0,"shuffle = True, callbacks"
v3.2.0,enable display training loss
v3.2.0,self defined folds
v3.2.0,lambdarank
v3.2.0,... with l2 metric
v3.2.0,... with NDCG (default) metric
v3.2.0,self defined folds with lambdarank
v3.2.0,with early stopping
v3.2.0,predict by each fold booster
v3.2.0,fold averaging
v3.2.0,without early stopping
v3.2.0,test feature_names with whitespaces
v3.2.0,This has non-ascii strings.
v3.2.0,take subsets and train
v3.2.0,generate CSR sparse dataset
v3.2.0,convert data to dense and get back same contribs
v3.2.0,validate the values are the same
v3.2.0,validate using CSC matrix
v3.2.0,validate the values are the same
v3.2.0,generate CSR sparse dataset
v3.2.0,convert data to dense and get back same contribs
v3.2.0,validate the values are the same
v3.2.0,validate using CSC matrix
v3.2.0,validate the values are the same
v3.2.0,Note there is an extra column added to the output for the expected value
v3.2.0,Note output CSC shape should be same as CSR output shape
v3.2.0,test sliced labels
v3.2.0,append some columns
v3.2.0,append some rows
v3.2.0,test sliced 2d matrix
v3.2.0,test sliced CSR
v3.2.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.2.0,The penalization is so high that the first 2 features should not be used here
v3.2.0,Check that a very high penalization is the same as not using the features at all
v3.2.0,"no fobj, no feval"
v3.2.0,default metric
v3.2.0,non-default metric in params
v3.2.0,default metric in args
v3.2.0,non-default metric in args
v3.2.0,metric in args overwrites one in params
v3.2.0,multiple metrics in params
v3.2.0,multiple metrics in args
v3.2.0,remove default metric by 'None' in list
v3.2.0,remove default metric by 'None' aliases
v3.2.0,"fobj, no feval"
v3.2.0,no default metric
v3.2.0,metric in params
v3.2.0,metric in args
v3.2.0,metric in args overwrites its' alias in params
v3.2.0,multiple metrics in params
v3.2.0,multiple metrics in args
v3.2.0,"no fobj, feval"
v3.2.0,default metric with custom one
v3.2.0,non-default metric in params with custom one
v3.2.0,default metric in args with custom one
v3.2.0,non-default metric in args with custom one
v3.2.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.2.0,multiple metrics in params with custom one
v3.2.0,multiple metrics in args with custom one
v3.2.0,custom metric is evaluated despite 'None' is passed
v3.2.0,"fobj, feval"
v3.2.0,"no default metric, only custom one"
v3.2.0,metric in params with custom one
v3.2.0,metric in args with custom one
v3.2.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.2.0,multiple metrics in params with custom one
v3.2.0,multiple metrics in args with custom one
v3.2.0,custom metric is evaluated despite 'None' is passed
v3.2.0,"no fobj, no feval"
v3.2.0,default metric
v3.2.0,default metric in params
v3.2.0,non-default metric in params
v3.2.0,multiple metrics in params
v3.2.0,remove default metric by 'None' aliases
v3.2.0,"fobj, no feval"
v3.2.0,no default metric
v3.2.0,metric in params
v3.2.0,multiple metrics in params
v3.2.0,"no fobj, feval"
v3.2.0,default metric with custom one
v3.2.0,default metric in params with custom one
v3.2.0,non-default metric in params with custom one
v3.2.0,multiple metrics in params with custom one
v3.2.0,custom metric is evaluated despite 'None' is passed
v3.2.0,"fobj, feval"
v3.2.0,"no default metric, only custom one"
v3.2.0,metric in params with custom one
v3.2.0,multiple metrics in params with custom one
v3.2.0,custom metric is evaluated despite 'None' is passed
v3.2.0,multiclass default metric
v3.2.0,multiclass default metric with custom one
v3.2.0,multiclass metric alias with custom one for custom objective
v3.2.0,no metric for invalid class_num
v3.2.0,custom metric for invalid class_num
v3.2.0,multiclass metric alias with custom one with invalid class_num
v3.2.0,multiclass default metric without num_class
v3.2.0,multiclass metric alias
v3.2.0,multiclass metric
v3.2.0,non-valid metric for multiclass objective
v3.2.0,non-default num_class for default objective
v3.2.0,no metric with non-default num_class for custom objective
v3.2.0,multiclass metric alias for custom objective
v3.2.0,multiclass metric for custom objective
v3.2.0,binary metric with non-default num_class for custom objective
v3.2.0,Expect three metrics but mean and stdv for each metric
v3.2.0,test XGBoost-style return value
v3.2.0,test numpy-style return value
v3.2.0,test bins string type
v3.2.0,test histogram is disabled for categorical features
v3.2.0,test for lgb.train
v3.2.0,test feval for lgb.train
v3.2.0,test with two valid data for lgb.train
v3.2.0,test for lgb.cv
v3.2.0,test feval for lgb.cv
v3.2.0,test that binning works properly for features with only positive or only negative values
v3.2.0,decreasing without freeing raw data is allowed
v3.2.0,decreasing before lazy init is allowed
v3.2.0,increasing is allowed
v3.2.0,decreasing with disabled filter is allowed
v3.2.0,decreasing with enabled filter is disallowed;
v3.2.0,also changes of other params are disallowed
v3.2.0,check extra trees increases regularization
v3.2.0,check path smoothing increases regularization
v3.2.0,test edge case with one leaf
v3.2.0,check that constraint containing all features is equivalent to no constraint
v3.2.0,check that constraint partitioning the features reduces train accuracy
v3.2.0,check that constraints consisting of single features reduce accuracy further
v3.2.0,test that interaction constraints work when not all features are used
v3.2.0,check that setting linear_tree=True fits better than ordinary trees when data has linear relationship
v3.2.0,test again with nans in data
v3.2.0,test again with bagging
v3.2.0,test with a feature that has only one non-nan value
v3.2.0,test with a categorical feature
v3.2.0,test refit: same results on same data
v3.2.0,test refit with save and load
v3.2.0,test refit: different results training on different data
v3.2.0,test when num_leaves - 1 < num_features and when num_leaves - 1 > num_features
v3.2.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.2.0,"test the case where start_iteration <= 0, and num_iteration is None"
v3.2.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.2.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.2.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.2.0,test for regression
v3.2.0,test both with and without early stopping
v3.2.0,test for multi-class
v3.2.0,test both with and without early stopping
v3.2.0,test for binary
v3.2.0,test both with and without early stopping
v3.2.0,test against sklearn average precision metric
v3.2.0,test that average precision is 1 where model predicts perfectly
v3.2.0,coding: utf-8
v3.2.0,"If compiled appropriately, the same installation will support both GPU and CPU."
v3.2.0,coding: utf-8
v3.2.0,coding: utf-8
v3.2.0,convert from one-based to  zero-based index
v3.2.0,convert from boundaries to size
v3.2.0,--- start Booster interfaces
v3.2.0,.Call() calls
v3.2.0,coding: utf-8
v3.2.0,alias table
v3.2.0,names
v3.2.0,from strings
v3.2.0,tails
v3.2.0,tails
v3.2.0,coding: utf-8
v3.2.0,Single row predictor to abstract away caching logic
v3.2.0,create boosting
v3.2.0,initialize the boosting
v3.2.0,create objective function
v3.2.0,initialize the objective function
v3.2.0,create training metric
v3.2.0,reset the boosting
v3.2.0,create objective function
v3.2.0,initialize the objective function
v3.2.0,calculate the nonzero data and indices size
v3.2.0,allocate data and indices arrays
v3.2.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.2.0,aggregated per row feature contribution results
v3.2.0,keep track of the row_vector sizes for parallelization
v3.2.0,copy vector results to output for each row
v3.2.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.2.0,aggregated per row feature contribution results
v3.2.0,calculate number of elements per column to construct
v3.2.0,the CSC matrix with random access
v3.2.0,keep track of column counts
v3.2.0,keep track of beginning index for each column
v3.2.0,keep track of beginning index for each matrix
v3.2.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.2.0,store the row index
v3.2.0,update column count
v3.2.0,explicitly declare symbols from LightGBM namespace
v3.2.0,some help functions used to convert data
v3.2.0,Row iterator of on column for CSC matrix
v3.2.0,"return value at idx, only can access by ascent order"
v3.2.0,"return next non-zero pair, if index < 0, means no more data"
v3.2.0,start of c_api functions
v3.2.0,sample data first
v3.2.0,sample data first
v3.2.0,sample data first
v3.2.0,local buffer to re-use memory
v3.2.0,sample data first
v3.2.0,no more data
v3.2.0,---- start of booster
v3.2.0,Single row in row-major format:
v3.2.0,---- start of some help functions
v3.2.0,data is array of pointers to individual rows
v3.2.0,set number of threads for openmp
v3.2.0,check for alias
v3.2.0,read parameters from config file
v3.2.0,"remove str after ""#"""
v3.2.0,check for alias again
v3.2.0,load configs
v3.2.0,prediction is needed if using input initial model(continued train)
v3.2.0,need to continue training
v3.2.0,sync up random seed for data partition
v3.2.0,load Training data
v3.2.0,load data for distributed training
v3.2.0,load data for single machine
v3.2.0,need save binary file
v3.2.0,create training metric
v3.2.0,only when have metrics then need to construct validation data
v3.2.0,"Add validation data, if it exists"
v3.2.0,add
v3.2.0,need save binary file
v3.2.0,add metric for validation data
v3.2.0,output used time on each iteration
v3.2.0,need init network
v3.2.0,create boosting
v3.2.0,create objective function
v3.2.0,load training data
v3.2.0,initialize the objective function
v3.2.0,initialize the boosting
v3.2.0,add validation data into boosting
v3.2.0,convert model to if-else statement code
v3.2.0,create predictor
v3.2.0,Free memory
v3.2.0,create predictor
v3.2.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.2.0,counts for all labels
v3.2.0,"start from top label, and accumulate DCG"
v3.2.0,counts for all labels
v3.2.0,calculate k Max DCG by one pass
v3.2.0,get sorted indices by score
v3.2.0,calculate dcg
v3.2.0,get sorted indices by score
v3.2.0,calculate multi dcg by one pass
v3.2.0,wait for all client start up
v3.2.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.2.0,Instead we will handle finalize/abort for MPI in main().
v3.2.0,default set to -1
v3.2.0,"distance at k-th communication, distance[k] = 2^k"
v3.2.0,set incoming rank at k-th commuication
v3.2.0,set outgoing rank at k-th commuication
v3.2.0,defalut set as -1
v3.2.0,construct all recursive halving map for all machines
v3.2.0,let 1 << k <= num_machines
v3.2.0,distance of each communication
v3.2.0,"if num_machines = 2^k, don't need to group machines"
v3.2.0,"communication direction, %2 == 0 is positive"
v3.2.0,neighbor at k-th communication
v3.2.0,receive data block at k-th communication
v3.2.0,send data block at k-th communication
v3.2.0,"if num_machines != 2^k, need to group machines"
v3.2.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.2.0,let left machine as group leader
v3.2.0,"cache block information for groups, group with 2 machines will have double block size"
v3.2.0,convert from group to node leader
v3.2.0,convert from node to group
v3.2.0,meet new group
v3.2.0,add block len for this group
v3.2.0,calculate the group block start
v3.2.0,not need to construct
v3.2.0,get receive block informations
v3.2.0,accumulate block len
v3.2.0,get send block informations
v3.2.0,accumulate block len
v3.2.0,static member definition
v3.2.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.2.0,assign the blocks to every rank.
v3.2.0,do reduce scatter
v3.2.0,do all gather
v3.2.0,assign blocks
v3.2.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.2.0,copy back
v3.2.0,assign blocks
v3.2.0,start all gather
v3.2.0,when num_machines is small and data is large
v3.2.0,use output as receive buffer
v3.2.0,get current local block size
v3.2.0,get out rank
v3.2.0,get in rank
v3.2.0,get send information
v3.2.0,get recv information
v3.2.0,send and recv at same time
v3.2.0,rotate in-place
v3.2.0,use output as receive buffer
v3.2.0,get current local block size
v3.2.0,get send information
v3.2.0,get recv information
v3.2.0,send and recv at same time
v3.2.0,use output as receive buffer
v3.2.0,send and recv at same time
v3.2.0,send local data to neighbor first
v3.2.0,receive neighbor data first
v3.2.0,reduce
v3.2.0,get target
v3.2.0,get send information
v3.2.0,get recv information
v3.2.0,send and recv at same time
v3.2.0,reduce
v3.2.0,send result to neighbor
v3.2.0,receive result from neighbor
v3.2.0,copy result
v3.2.0,start up socket
v3.2.0,parse clients from file
v3.2.0,get ip list of local machine
v3.2.0,get local rank
v3.2.0,construct listener
v3.2.0,construct communication topo
v3.2.0,construct linkers
v3.2.0,free listener
v3.2.0,set timeout
v3.2.0,accept incoming socket
v3.2.0,receive rank
v3.2.0,add new socket
v3.2.0,save ranks that need to connect with
v3.2.0,start listener
v3.2.0,start connect
v3.2.0,let smaller rank connect to larger rank
v3.2.0,send local rank
v3.2.0,wait for listener
v3.2.0,print connected linkers
v3.2.0,only need to copy subset
v3.2.0,avoid to copy subset many times
v3.2.0,avoid out of range
v3.2.0,may need to recopy subset
v3.2.0,valid the type
v3.2.0,Constructors
v3.2.0,Get type tag
v3.2.0,Comparisons
v3.2.0,"This has to be separate, not in Statics, because Json() accesses"
v3.2.0,statics().null.
v3.2.0,"advance until next line, or end of input"
v3.2.0,advance until closing tokens
v3.2.0,The usual case: non-escaped characters
v3.2.0,Handle escapes
v3.2.0,Extract 4-byte escape sequence
v3.2.0,Explicitly check length of the substring. The following loop
v3.2.0,relies on std::string returning the terminating NUL when
v3.2.0,accessing str[length]. Checking here reduces brittleness.
v3.2.0,JSON specifies that characters outside the BMP shall be encoded as a
v3.2.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.2.0,components. Check whether we're in the middle of such a beast: the
v3.2.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.2.0,a trail (low) surrogate.
v3.2.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.2.0,per the UTF-16 algorithm.
v3.2.0,Integer part
v3.2.0,Decimal part
v3.2.0,Exponent part
v3.2.0,Check for any trailing garbage
v3.2.0,Documented in json11.hpp
v3.2.0,Check for another object
v3.2.0,get column names
v3.2.0,load label idx first
v3.2.0,erase label column name
v3.2.0,load ignore columns
v3.2.0,load weight idx
v3.2.0,load group idx
v3.2.0,don't support query id in data file when using distributed training
v3.2.0,read data to memory
v3.2.0,sample data
v3.2.0,construct feature bin mappers
v3.2.0,initialize label
v3.2.0,extract features
v3.2.0,sample data from file
v3.2.0,construct feature bin mappers
v3.2.0,initialize label
v3.2.0,extract features
v3.2.0,load data from binary file
v3.2.0,check meta data
v3.2.0,need to check training data
v3.2.0,read data in memory
v3.2.0,initialize label
v3.2.0,extract features
v3.2.0,Get number of lines of data file
v3.2.0,initialize label
v3.2.0,extract features
v3.2.0,load data from binary file
v3.2.0,not need to check validation data
v3.2.0,check meta data
v3.2.0,buffer to read binary file
v3.2.0,check token
v3.2.0,read size of header
v3.2.0,re-allocmate space if not enough
v3.2.0,read header
v3.2.0,get header
v3.2.0,num_groups
v3.2.0,real_feature_idx_
v3.2.0,feature2group
v3.2.0,feature2subfeature
v3.2.0,group_bin_boundaries
v3.2.0,group_feature_start_
v3.2.0,group_feature_cnt_
v3.2.0,get feature names
v3.2.0,write feature names
v3.2.0,get forced_bin_bounds_
v3.2.0,read size of meta data
v3.2.0,re-allocate space if not enough
v3.2.0,read meta data
v3.2.0,load meta data
v3.2.0,sample local used data if need to partition
v3.2.0,"if not contain query file, minimal sample unit is one record"
v3.2.0,"if contain query file, minimal sample unit is one query"
v3.2.0,if is new query
v3.2.0,read feature data
v3.2.0,read feature size
v3.2.0,re-allocate space if not enough
v3.2.0,raw data
v3.2.0,fill feature_names_ if not header
v3.2.0,get forced split
v3.2.0,"if only one machine, find bin locally"
v3.2.0,"if have multi-machines, need to find bin distributed"
v3.2.0,different machines will find bin for different features
v3.2.0,start and len will store the process feature indices for different machines
v3.2.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.2.0,free
v3.2.0,gather global feature bin mappers
v3.2.0,restore features bins from buffer
v3.2.0,---- private functions ----
v3.2.0,"if features are ordered, not need to use hist_buf"
v3.2.0,read all lines
v3.2.0,get query data
v3.2.0,"if not contain query data, minimal sample unit is one record"
v3.2.0,"if contain query data, minimal sample unit is one query"
v3.2.0,if is new query
v3.2.0,get query data
v3.2.0,"if not contain query file, minimal sample unit is one record"
v3.2.0,"if contain query file, minimal sample unit is one query"
v3.2.0,if is new query
v3.2.0,parse features
v3.2.0,get forced split
v3.2.0,"check the range of label_idx, weight_idx and group_idx"
v3.2.0,fill feature_names_ if not header
v3.2.0,start find bins
v3.2.0,"if only one machine, find bin locally"
v3.2.0,start and len will store the process feature indices for different machines
v3.2.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.2.0,free
v3.2.0,gather global feature bin mappers
v3.2.0,restore features bins from buffer
v3.2.0,if doesn't need to prediction with initial model
v3.2.0,parser
v3.2.0,set label
v3.2.0,free processed line:
v3.2.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.2.0,text_reader_->Lines()[i].shrink_to_fit();
v3.2.0,push data
v3.2.0,if is used feature
v3.2.0,if need to prediction with initial model
v3.2.0,parser
v3.2.0,set initial score
v3.2.0,set label
v3.2.0,free processed line:
v3.2.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.2.0,text_reader_->Lines()[i].shrink_to_fit();
v3.2.0,push data
v3.2.0,if is used feature
v3.2.0,metadata_ will manage space of init_score
v3.2.0,text data can be free after loaded feature values
v3.2.0,parser
v3.2.0,set initial score
v3.2.0,set label
v3.2.0,push data
v3.2.0,if is used feature
v3.2.0,only need part of data
v3.2.0,need full data
v3.2.0,metadata_ will manage space of init_score
v3.2.0,read size of token
v3.2.0,remove duplicates
v3.2.0,deep copy function for BinMapper
v3.2.0,mean size for one bin
v3.2.0,need a new bin
v3.2.0,update bin upper bound
v3.2.0,last bin upper bound
v3.2.0,get list of distinct values
v3.2.0,get number of positive and negative distinct values
v3.2.0,include zero bounds and infinity bound
v3.2.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.2.0,find remaining bounds
v3.2.0,find distinct_values first
v3.2.0,push zero in the front
v3.2.0,use the large value
v3.2.0,push zero in the back
v3.2.0,convert to int type first
v3.2.0,sort by counts
v3.2.0,will ignore the categorical of small counts
v3.2.0,Push the dummy bin for NaN
v3.2.0,Use MissingType::None to represent this bin contains all categoricals
v3.2.0,fix count of NaN bin
v3.2.0,check trivial(num_bin_ == 1) feature
v3.2.0,check useless bin
v3.2.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.2.0,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.2.0,calculate max bin of all features to select the int type in MultiValDenseBin
v3.2.0,"for lambdarank, it needs query data for partition data in distributed learning"
v3.2.0,need convert query_id to boundaries
v3.2.0,check weights
v3.2.0,check query boundries
v3.2.0,contain initial score file
v3.2.0,check weights
v3.2.0,get local weights
v3.2.0,check query boundries
v3.2.0,get local query boundaries
v3.2.0,contain initial score file
v3.2.0,get local initial scores
v3.2.0,re-load query weight
v3.2.0,save to nullptr
v3.2.0,save to nullptr
v3.2.0,save to nullptr
v3.2.0,default weight file name
v3.2.0,default init_score file name
v3.2.0,use first line to count number class
v3.2.0,default query file name
v3.2.0,root is in the depth 0
v3.2.0,non-leaf
v3.2.0,leaf
v3.2.0,use this for the missing value conversion
v3.2.0,Predict func by Map to ifelse
v3.2.0,use this for the missing value conversion
v3.2.0,non-leaf
v3.2.0,left subtree
v3.2.0,right subtree
v3.2.0,leaf
v3.2.0,non-leaf
v3.2.0,left subtree
v3.2.0,right subtree
v3.2.0,leaf
v3.2.0,recursive computation of SHAP values for a decision tree
v3.2.0,extend the unique path
v3.2.0,leaf node
v3.2.0,internal node
v3.2.0,"see if we have already split on this feature,"
v3.2.0,if so we undo that split so we can redo it for this node
v3.2.0,recursive sparse computation of SHAP values for a decision tree
v3.2.0,extend the unique path
v3.2.0,leaf node
v3.2.0,internal node
v3.2.0,"see if we have already split on this feature,"
v3.2.0,if so we undo that split so we can redo it for this node
v3.2.0,add names of objective function if not providing metric
v3.2.0,equal weights for all classes
v3.2.0,generate seeds by seed.
v3.2.0,sort eval_at
v3.2.0,Only push the non-training data
v3.2.0,check for conflicts
v3.2.0,"check if objective, metric, and num_class match"
v3.2.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.2.0,Check max_depth and num_leaves
v3.2.0,"Fits in an int, and is more restrictive than the current num_leaves"
v3.2.0,force col-wise for gpu & CUDA
v3.2.0,force gpu_use_dp for CUDA
v3.2.0,linear tree learner must be serial type and run on cpu device
v3.2.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.2.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.2.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.2.0,calculated split gain can be positive even with zero gradient and hessian.
v3.2.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.2.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.2.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.2.0,first round: fill the single val group
v3.2.0,always push the last group
v3.2.0,put dense feature first
v3.2.0,sort by non zero cnt
v3.2.0,"sort by non zero cnt, bigger first"
v3.2.0,shuffle groups
v3.2.0,Using std::swap for vector<bool> will cause the wrong result.
v3.2.0,get num_features
v3.2.0,get bin_mappers
v3.2.0,"for sparse multi value bin, we store the feature bin values with offset added"
v3.2.0,"for dense multi value bin, the feature bin values without offsets are used"
v3.2.0,copy feature bin mapper data
v3.2.0,copy feature bin mapper data
v3.2.0,"if not pass a filename, just append "".bin"" of original file"
v3.2.0,get size of header
v3.2.0,size of feature names
v3.2.0,size of forced bins
v3.2.0,write header
v3.2.0,write feature names
v3.2.0,write forced bins
v3.2.0,get size of meta data
v3.2.0,write meta data
v3.2.0,write feature data
v3.2.0,get size of feature
v3.2.0,write feature
v3.2.0,write raw data; use row-major order so we can read row-by-row
v3.2.0,"explicitly initialize template methods, for cross module call"
v3.2.0,"Only one multi-val group, just simply merge"
v3.2.0,Skip the leading 0 when copying group_bin_boundaries.
v3.2.0,regenerate other fields
v3.2.0,store the importance first
v3.2.0,PredictRaw
v3.2.0,PredictRawByMap
v3.2.0,Predict
v3.2.0,PredictByMap
v3.2.0,PredictLeafIndex
v3.2.0,PredictLeafIndexByMap
v3.2.0,output model type
v3.2.0,output number of class
v3.2.0,output label index
v3.2.0,output max_feature_idx
v3.2.0,output objective
v3.2.0,output tree models
v3.2.0,store the importance first
v3.2.0,sort the importance
v3.2.0,use serialized string to restore this object
v3.2.0,Use first 128 chars to avoid exceed the message buffer.
v3.2.0,get number of classes
v3.2.0,get index of label
v3.2.0,get max_feature_idx first
v3.2.0,get average_output
v3.2.0,get feature names
v3.2.0,get monotone_constraints
v3.2.0,set zero
v3.2.0,predict all the trees for one iteration
v3.2.0,check early stopping
v3.2.0,set zero
v3.2.0,predict all the trees for one iteration
v3.2.0,check early stopping
v3.2.0,margin_threshold will be captured by value
v3.2.0,copy and sort
v3.2.0,margin_threshold will be captured by value
v3.2.0,Fix for compiler warnings about reaching end of control
v3.2.0,load forced_splits file
v3.2.0,init tree learner
v3.2.0,push training metrics
v3.2.0,create buffer for gradients and hessians
v3.2.0,get max feature index
v3.2.0,get label index
v3.2.0,get feature names
v3.2.0,"if need bagging, create buffer"
v3.2.0,"for a validation dataset, we need its score and metric"
v3.2.0,update score
v3.2.0,objective function will calculate gradients and hessians
v3.2.0,"random bagging, minimal unit is one record"
v3.2.0,"random bagging, minimal unit is one record"
v3.2.0,if need bagging
v3.2.0,set bagging data to tree learner
v3.2.0,get subset
v3.2.0,output used time per iteration
v3.2.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.2.0,boosting first
v3.2.0,bagging logic
v3.2.0,need to copy gradients for bagging subset.
v3.2.0,shrinkage by learning rate
v3.2.0,update score
v3.2.0,only add default score one-time
v3.2.0,updates scores
v3.2.0,add model
v3.2.0,reset score
v3.2.0,remove model
v3.2.0,print message for metric
v3.2.0,pop last early_stopping_round_ models
v3.2.0,update training score
v3.2.0,we need to predict out-of-bag scores of data for boosting
v3.2.0,update validation score
v3.2.0,print training metric
v3.2.0,print validation metric
v3.2.0,set zero
v3.2.0,predict all the trees for one iteration
v3.2.0,predict all the trees for one iteration
v3.2.0,push training metrics
v3.2.0,"not same training data, need reset score and others"
v3.2.0,create score tracker
v3.2.0,update score
v3.2.0,create buffer for gradients and hessians
v3.2.0,load forced_splits file
v3.2.0,"if need bagging, create buffer"
v3.2.0,Get the max size of pool
v3.2.0,at least need 2 leaves
v3.2.0,push split information for all leaves
v3.2.0,initialize splits for leaf
v3.2.0,initialize data partition
v3.2.0,initialize ordered gradients and hessians
v3.2.0,cannot change is_hist_col_wise during training
v3.2.0,initialize splits for leaf
v3.2.0,initialize data partition
v3.2.0,initialize ordered gradients and hessians
v3.2.0,Get the max size of pool
v3.2.0,at least need 2 leaves
v3.2.0,push split information for all leaves
v3.2.0,some initial works before training
v3.2.0,root leaf
v3.2.0,only root leaf can be splitted on first time
v3.2.0,some initial works before finding best split
v3.2.0,find best threshold for every feature
v3.2.0,Get a leaf with max split gain
v3.2.0,Get split information for best leaf
v3.2.0,"cannot split, quit"
v3.2.0,split tree with best leaf
v3.2.0,reset histogram pool
v3.2.0,initialize data partition
v3.2.0,reset the splits for leaves
v3.2.0,Sumup for root
v3.2.0,use all data
v3.2.0,"use bagging, only use part of data"
v3.2.0,check depth of current leaf
v3.2.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.2.0,no enough data to continue
v3.2.0,only have root
v3.2.0,put parent(left) leaf's histograms into larger leaf's histograms
v3.2.0,put parent(left) leaf's histograms to larger leaf's histograms
v3.2.0,construct smaller leaf
v3.2.0,construct larger leaf
v3.2.0,find splits
v3.2.0,only has root leaf
v3.2.0,start at root leaf
v3.2.0,"before processing next node from queue, store info for current left/right leaf"
v3.2.0,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.2.0,"then, compute own splits"
v3.2.0,split info should exist because searching in bfs fashion - should have added from parent
v3.2.0,update before tree split
v3.2.0,don't need to update this in data-based parallel model
v3.2.0,"split tree, will return right leaf"
v3.2.0,store the true split gain in tree model
v3.2.0,don't need to update this in data-based parallel model
v3.2.0,store the true split gain in tree model
v3.2.0,init the leaves that used on next iteration
v3.2.0,update leave outputs if needed
v3.2.0,bag_mapper[index_mapper[i]]
v3.2.0,it is needed to filter the features after the above code.
v3.2.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.2.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.2.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.2.0,find splits
v3.2.0,identify features containing nans
v3.2.0,preallocate the matrix used to calculate linear model coefficients
v3.2.0,"store only upper triangular half of matrix as an array, in row-major order"
v3.2.0,this requires (max_num_feat + 1) * (max_num_feat + 2) / 2 entries (including the constant terms of the regression)
v3.2.0,we add another 8 to ensure cache lines are not shared among processors
v3.2.0,some initial works before training
v3.2.0,root leaf
v3.2.0,only root leaf can be splitted on first time
v3.2.0,some initial works before finding best split
v3.2.0,find best threshold for every feature
v3.2.0,Get a leaf with max split gain
v3.2.0,Get split information for best leaf
v3.2.0,"cannot split, quit"
v3.2.0,split tree with best leaf
v3.2.0,map data to leaf number
v3.2.0,calculate coefficients using the method described in Eq 3 of https://arxiv.org/pdf/1802.05640.pdf
v3.2.0,the coefficients vector is given by
v3.2.0,- (X_T * H * X + lambda) ^ (-1) * (X_T * g)
v3.2.0,where:
v3.2.0,"X is the matrix where the first column is the feature values and the second is all ones,"
v3.2.0,"H is the diagonal matrix of the hessian,"
v3.2.0,lambda is the diagonal matrix with diagonal entries equal to the regularisation term linear_lambda
v3.2.0,g is the vector of gradients
v3.2.0,the subscript _T denotes the transpose
v3.2.0,"create array of pointers to raw data, and coefficient matrices, for each leaf"
v3.2.0,clear the coefficient matrices
v3.2.0,aggregate results from different threads
v3.2.0,copy into eigen matrices and solve
v3.2.0,update the tree properties
v3.2.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.2.0,get feature partition
v3.2.0,get local used features
v3.2.0,get best split at smaller leaf
v3.2.0,find local best split for larger leaf
v3.2.0,sync global best info
v3.2.0,update best split
v3.2.0,"instantiate template classes, otherwise linker cannot find the code"
v3.2.0,initialize SerialTreeLearner
v3.2.0,Get local rank and global machine size
v3.2.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.2.0,allocate buffer for communication
v3.2.0,generate feature partition for current tree
v3.2.0,get local used feature
v3.2.0,get block start and block len for reduce scatter
v3.2.0,get buffer_write_start_pos_
v3.2.0,get buffer_read_start_pos_
v3.2.0,sync global data sumup info
v3.2.0,global sumup reduce
v3.2.0,copy back
v3.2.0,set global sumup info
v3.2.0,init global data count in leaf
v3.2.0,construct local histograms
v3.2.0,copy to buffer
v3.2.0,Reduce scatter for histogram
v3.2.0,restore global histograms from buffer
v3.2.0,only root leaf
v3.2.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.2.0,find local best split for larger leaf
v3.2.0,sync global best info
v3.2.0,set best split
v3.2.0,need update global number of data in leaf
v3.2.0,"instantiate template classes, otherwise linker cannot find the code"
v3.2.0,initialize SerialTreeLearner
v3.2.0,some additional variables needed for GPU trainer
v3.2.0,Initialize GPU buffers and kernels
v3.2.0,some functions used for debugging the GPU histogram construction
v3.2.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.2.0,goto err;
v3.2.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.2.0,also guarantee that there are at least 2K examples per workgroup
v3.2.0,return 0;
v3.2.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.2.0,decide the best number of workgroups working on one feature4 tuple
v3.2.0,set work group size based on feature size
v3.2.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.2.0,we need to refresh the kernel arguments after reallocating
v3.2.0,The only argument that needs to be changed later is num_data_
v3.2.0,"the GPU kernel will process all features in one call, and each"
v3.2.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.2.0,process one feature4 tuple
v3.2.0,"for the root node, indices are not copied"
v3.2.0,"for constant hessian, hessians are not copied except for the root node"
v3.2.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.2.0,and we will launch num_feature workgroups for this kernel
v3.2.0,will launch threads for all features
v3.2.0,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.2.0,copy the results asynchronously. Size depends on if double precision is used
v3.2.0,we will wait for this object in WaitAndGetHistograms
v3.2.0,"when the output is ready, the computation is done"
v3.2.0,values of this feature has been redistributed to multiple bins; need a reduction here
v3.2.0,how many feature-group tuples we have
v3.2.0,leave some safe margin for prefetching
v3.2.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.2.0,clear sparse/dense maps
v3.2.0,do nothing if no features can be processed on GPU
v3.2.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.2.0,unpin old buffer if necessary before destructing them
v3.2.0,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v3.2.0,allocate space for gradients and hessians on device
v3.2.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.2.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.2.0,copy indices to the device
v3.2.0,histogram bin entry size depends on the precision (single/double)
v3.2.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.2.0,each work group generates a sub-histogram of dword_features_ features.
v3.2.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.2.0,create atomic counters for inter-group coordination
v3.2.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.2.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.2.0,looking for dword_features_ non-sparse feature-groups
v3.2.0,decide if we need to redistribute the bin
v3.2.0,multiplier must be a power of 2
v3.2.0,device_bin_mults_.push_back(1);
v3.2.0,found
v3.2.0,for data transfer time
v3.2.0,"Now generate new data structure feature4, and copy data to the device"
v3.2.0,"preallocate arrays for all threads, and pin them"
v3.2.0,building Feature4 bundles; each thread handles dword_features_ features
v3.2.0,one feature datapoint is 4 bits
v3.2.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.2.0,one feature datapoint is one byte
v3.2.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.2.0,Dense bin
v3.2.0,Dense 4-bit bin
v3.2.0,working on the remaining (less than dword_features_) feature groups
v3.2.0,fill the leftover features
v3.2.0,"fill this empty feature with some ""random"" value"
v3.2.0,"fill this empty feature with some ""random"" value"
v3.2.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.2.0,deallocate pinned space for feature copying
v3.2.0,data transfer time
v3.2.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.2.0,"Something bad happened. Just return ""No log available."""
v3.2.0,"build is okay, log may contain warnings"
v3.2.0,destroy any old kernels
v3.2.0,create OpenCL kernels for different number of workgroups per feature
v3.2.0,currently we don't use constant memory
v3.2.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.2.0,kernel with indices in an array
v3.2.0,"kernel with all features enabled, with elimited branches"
v3.2.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.2.0,do nothing if no features can be processed on GPU
v3.2.0,The only argument that needs to be changed later is num_data_
v3.2.0,"hessian is passed as a parameter, but it is not available now."
v3.2.0,hessian will be set in BeforeTrain()
v3.2.0,"Get the max bin size, used for selecting best GPU kernel"
v3.2.0,initialize GPU
v3.2.0,determine which kernel to use based on the max number of bins
v3.2.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.2.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.2.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.2.0,setup GPU kernel arguments after we allocating all the buffers
v3.2.0,GPU memory has to been reallocated because data may have been changed
v3.2.0,setup GPU kernel arguments after we allocating all the buffers
v3.2.0,Copy initial full hessians and gradients to GPU.
v3.2.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.2.0,setup hessian parameters only
v3.2.0,hessian is passed as a parameter
v3.2.0,use bagging
v3.2.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.2.0,copy used gradients and hessians to ordered buffer
v3.2.0,transfer the indices to GPU
v3.2.0,transfer hessian to GPU
v3.2.0,setup hessian parameters only
v3.2.0,hessian is passed as a parameter
v3.2.0,transfer gradients to GPU
v3.2.0,only have root
v3.2.0,"Copy indices, gradients and hessians as early as possible"
v3.2.0,only need to initialize for smaller leaf
v3.2.0,Get leaf boundary
v3.2.0,copy indices to the GPU:
v3.2.0,copy ordered hessians to the GPU:
v3.2.0,copy ordered gradients to the GPU:
v3.2.0,do nothing if no features can be processed on GPU
v3.2.0,copy data indices if it is not null
v3.2.0,generate and copy ordered_gradients if gradients is not null
v3.2.0,generate and copy ordered_hessians if hessians is not null
v3.2.0,converted indices in is_feature_used to feature-group indices
v3.2.0,construct the feature masks for dense feature-groups
v3.2.0,"if no feature group is used, just return and do not use GPU"
v3.2.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.2.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.2.0,"All data have been prepared, now run the GPU kernel"
v3.2.0,construct smaller leaf
v3.2.0,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v3.2.0,then construct sparse features on CPU
v3.2.0,"wait for GPU to finish, only if GPU is actually used"
v3.2.0,use double precision
v3.2.0,use single precision
v3.2.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.2.0,#define GPU_DEBUG_COMPARE
v3.2.0,construct larger leaf
v3.2.0,then construct sparse features on CPU
v3.2.0,"wait for GPU to finish, only if GPU is actually used"
v3.2.0,use double precision
v3.2.0,use single precision
v3.2.0,do some sanity check for the GPU algorithm
v3.2.0,limit top k
v3.2.0,get max bin
v3.2.0,calculate buffer size
v3.2.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.2.0,"left and right on same time, so need double size"
v3.2.0,initialize histograms for global
v3.2.0,sync global data sumup info
v3.2.0,set global sumup info
v3.2.0,init global data count in leaf
v3.2.0,get local sumup
v3.2.0,get local sumup
v3.2.0,get mean number on machines
v3.2.0,weighted gain
v3.2.0,get top k
v3.2.0,"Copy histogram to buffer, and Get local aggregate features"
v3.2.0,copy histograms.
v3.2.0,copy smaller leaf histograms first
v3.2.0,mark local aggregated feature
v3.2.0,copy
v3.2.0,then copy larger leaf histograms
v3.2.0,mark local aggregated feature
v3.2.0,copy
v3.2.0,use local data to find local best splits
v3.2.0,find splits
v3.2.0,only has root leaf
v3.2.0,local voting
v3.2.0,gather
v3.2.0,get all top-k from all machines
v3.2.0,global voting
v3.2.0,copy local histgrams to buffer
v3.2.0,Reduce scatter for histogram
v3.2.0,find best split from local aggregated histograms
v3.2.0,restore from buffer
v3.2.0,restore from buffer
v3.2.0,find local best
v3.2.0,find local best split for larger leaf
v3.2.0,sync global best info
v3.2.0,copy back
v3.2.0,set the global number of data for leaves
v3.2.0,init the global sumup info
v3.2.0,"instantiate template classes, otherwise linker cannot find the code"
v3.2.0,launch cuda kernel
v3.2.0,initialize SerialTreeLearner
v3.2.0,some additional variables needed for GPU trainer
v3.2.0,Initialize GPU buffers and kernels: get device info
v3.2.0,some functions used for debugging the GPU histogram construction
v3.2.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.2.0,also guarantee that there are at least 2K examples per workgroup
v3.2.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.2.0,decide the best number of workgroups working on one feature4 tuple
v3.2.0,set work group size based on feature size
v3.2.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.2.0,set thread_data
v3.2.0,copy the results asynchronously. Size depends on if double precision is used
v3.2.0,"when the output is ready, the computation is done"
v3.2.0,how many feature-group tuples we have
v3.2.0,leave some safe margin for prefetching
v3.2.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.2.0,clear sparse/dense maps
v3.2.0,do nothing it there is no dense feature
v3.2.0,calculate number of feature groups per gpu
v3.2.0,histogram bin entry size depends on the precision (single/double)
v3.2.0,allocate GPU memory for each GPU
v3.2.0,do nothing it there is no gpu feature
v3.2.0,allocate memory for all features
v3.2.0,allocate space for gradients and hessians on device
v3.2.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.2.0,copy indices to the device
v3.2.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.2.0,each work group generates a sub-histogram of dword_features_ features.
v3.2.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.2.0,create atomic counters for inter-group coordination
v3.2.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.2.0,clear sparse/dense maps
v3.2.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.2.0,set device info
v3.2.0,looking for dword_features_ non-sparse feature-groups
v3.2.0,reset device info
v3.2.0,InitGPU w/ num_gpu
v3.2.0,"Get the max bin size, used for selecting best GPU kernel"
v3.2.0,get num_dense_feature_groups_
v3.2.0,initialize GPU
v3.2.0,set cpu threads
v3.2.0,resize device memory pointers
v3.2.0,create stream & events to handle multiple GPUs
v3.2.0,check data size
v3.2.0,GPU memory has to been reallocated because data may have been changed
v3.2.0,AllocateGPUMemory only when the number of data increased
v3.2.0,setup GPU kernel arguments after we allocating all the buffers
v3.2.0,Copy initial full hessians and gradients to GPU.
v3.2.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.2.0,use bagging
v3.2.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.2.0,copy used gradients and hessians to ordered buffer
v3.2.0,transfer the indices to GPU
v3.2.0,only have root
v3.2.0,"Copy indices, gradients and hessians as early as possible"
v3.2.0,only need to initialize for smaller leaf
v3.2.0,Get leaf boundary
v3.2.0,do nothing if no features can be processed on GPU
v3.2.0,copy data indices if it is not null
v3.2.0,converted indices in is_feature_used to feature-group indices
v3.2.0,construct the feature masks for dense feature-groups
v3.2.0,"if no feature group is used, just return and do not use GPU"
v3.2.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.2.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.2.0,We now copy even if all features are used.
v3.2.0,"All data have been prepared, now run the GPU kernel"
v3.2.0,construct smaller leaf
v3.2.0,Check workgroups per feature4 tuple..
v3.2.0,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.2.0,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.2.0,then construct sparse features on CPU
v3.2.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.2.0,"wait for GPU to finish, only if GPU is actually used"
v3.2.0,use double precision
v3.2.0,use single precision
v3.2.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.2.0,#define CUDA_DEBUG_COMPARE
v3.2.0,construct larger leaf
v3.2.0,then construct sparse features on CPU
v3.2.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.2.0,"wait for GPU to finish, only if GPU is actually used"
v3.2.0,use double precision
v3.2.0,use single precision
v3.2.0,do some sanity check for the GPU algorithm
v3.1.1,coding: utf-8
v3.1.1,see https://github.com/pypa/distutils/pull/21
v3.1.1,coding: utf-8
v3.1.1,create predictor first
v3.1.1,check dataset
v3.1.1,reduce cost for prediction training data
v3.1.1,process callbacks
v3.1.1,Most of legacy advanced options becomes callbacks
v3.1.1,construct booster
v3.1.1,start training
v3.1.1,check evaluation result.
v3.1.1,"ranking task, split according to groups"
v3.1.1,run preprocessing on the data set if needed
v3.1.1,setup callbacks
v3.1.1,coding: utf-8
v3.1.1,"simplejson does not support Python 3.2, it throws a SyntaxError"
v3.1.1,because of u'...' Unicode literals.
v3.1.1,dummy function to support older version of scikit-learn
v3.1.1,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.1.1,coding: utf-8
v3.1.1,"user can set verbose with kwargs, it has higher priority"
v3.1.1,Do not modify original args in fit function
v3.1.1,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.1.1,Separate built-in from callable evaluation metrics
v3.1.1,register default metric for consistency with callable eval_metric case
v3.1.1,try to deduce from class instance
v3.1.1,overwrite default metric by explicitly set metric
v3.1.1,concatenate metric from params (or default if not provided in params) and eval_metric
v3.1.1,copy for consistency
v3.1.1,reduce cost for prediction training data
v3.1.1,free dataset
v3.1.1,Switch to using a multiclass objective in the underlying LGBM instance
v3.1.1,"do not modify args, as it causes errors in model selection tools"
v3.1.1,check group data
v3.1.1,coding: utf-8
v3.1.1,we don't need lib_lightgbm while building docs
v3.1.1,coding: utf-8
v3.1.1,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.1.1,coding: utf-8
v3.1.1,REMOVEME: remove warning after 3.1.0 version release
v3.1.1,coding: utf-8
v3.1.1,TypeError: obj is not a string or a number
v3.1.1,ValueError: invalid literal
v3.1.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.1.1,avoid memory consumption by arrays concatenation operations
v3.1.1,create numpy array from output arrays
v3.1.1,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.1.1,for CSC there is extra column added
v3.1.1,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.1.1,same shape as input csr or csc matrix except extra column for expected value
v3.1.1,note: make sure we copy data as it will be deallocated next
v3.1.1,"free the temporary native indptr, indices, and data"
v3.1.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.1.1,avoid memory consumption by arrays concatenation operations
v3.1.1,"no min_data, nthreads and verbose in this function"
v3.1.1,check data has header or not
v3.1.1,need to regroup init_score
v3.1.1,process for args
v3.1.1,"user can set verbose with params, it has higher priority"
v3.1.1,get categorical features
v3.1.1,process for reference dataset
v3.1.1,start construct data
v3.1.1,set feature names
v3.1.1,create valid
v3.1.1,construct subset
v3.1.1,create train
v3.1.1,could be updated if data is not freed
v3.1.1,set to None
v3.1.1,we're done if self and reference share a common upstrem reference
v3.1.1,"group data from LightGBM is boundaries data, need to convert to group size"
v3.1.1,"user can set verbose with params, it has higher priority"
v3.1.1,Training task
v3.1.1,set network if necessary
v3.1.1,construct booster object
v3.1.1,copy the parameters from train_set
v3.1.1,save reference to data
v3.1.1,buffer for inner predict
v3.1.1,Prediction task
v3.1.1,if a single node tree it won't have `leaf_index` so return 0
v3.1.1,"Create the node record, and populate universal data members"
v3.1.1,Update values to reflect node type (leaf or split)
v3.1.1,traverse the next level of the tree
v3.1.1,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.1.1,and we add node to the list.
v3.1.1,need reset training data
v3.1.1,need to push new valid data
v3.1.1,"if buffer length is not long enough, re-allocate a buffer"
v3.1.1,"if buffer length is not long enough, reallocate a buffer"
v3.1.1,Copy models
v3.1.1,Get name of features
v3.1.1,avoid to predict many time in one iteration
v3.1.1,Get num of inner evals
v3.1.1,Get name of evals
v3.1.1,coding: utf-8
v3.1.1,Callback environment used by callbacks
v3.1.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.1.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.1.1,coding: utf-8
v3.1.1,load or create your dataset
v3.1.1,create dataset for lightgbm
v3.1.1,"if you want to re-use data, remember to set free_raw_data=False"
v3.1.1,specify your configurations as a dict
v3.1.1,generate feature names
v3.1.1,feature_name and categorical_feature
v3.1.1,check feature name
v3.1.1,save model to file
v3.1.1,dump model to JSON (and save to file)
v3.1.1,feature names
v3.1.1,feature importances
v3.1.1,load model to predict
v3.1.1,can only predict with the best iteration (or the saving iteration)
v3.1.1,eval with loaded model
v3.1.1,dump model with pickle
v3.1.1,load model with pickle to predict
v3.1.1,can predict with any iteration when loaded in pickle way
v3.1.1,eval with loaded model
v3.1.1,continue training
v3.1.1,init_model accepts:
v3.1.1,1. model file name
v3.1.1,2. Booster()
v3.1.1,decay learning rates
v3.1.1,learning_rates accepts:
v3.1.1,1. list/tuple with length = num_boost_round
v3.1.1,2. function(curr_iter)
v3.1.1,change other parameters during training
v3.1.1,self-defined objective function
v3.1.1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.1.1,log likelihood loss
v3.1.1,self-defined eval metric
v3.1.1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.1,binary error
v3.1.1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.1.1,This may make built-in evalution metric calculate wrong results
v3.1.1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.1.1,Keep this in mind when you use the customization
v3.1.1,another self-defined eval metric
v3.1.1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.1,accuracy
v3.1.1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.1.1,This may make built-in evalution metric calculate wrong results
v3.1.1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.1.1,Keep this in mind when you use the customization
v3.1.1,callback
v3.1.1,coding: utf-8
v3.1.1,load or create your dataset
v3.1.1,train
v3.1.1,predict
v3.1.1,eval
v3.1.1,feature importances
v3.1.1,self-defined eval metric
v3.1.1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.1,Root Mean Squared Logarithmic Error (RMSLE)
v3.1.1,train
v3.1.1,another self-defined eval metric
v3.1.1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.1,Relative Absolute Error (RAE)
v3.1.1,train
v3.1.1,predict
v3.1.1,eval
v3.1.1,other scikit-learn modules
v3.1.1,coding: utf-8
v3.1.1,load or create your dataset
v3.1.1,create dataset for lightgbm
v3.1.1,specify your configurations as a dict
v3.1.1,train
v3.1.1,coding: utf-8
v3.1.1,################
v3.1.1,Simulate some binary data with a single categorical and
v3.1.1,single continuous predictor
v3.1.1,################
v3.1.1,Set up a couple of utilities for our experiments
v3.1.1,################
v3.1.1,Observe the behavior of `binary` and `xentropy` objectives
v3.1.1,Trying this throws an error on non-binary values of y:
v3.1.1,"experiment('binary', label_type='probability', DATA)"
v3.1.1,The speed of `binary` is not drastically different than
v3.1.1,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.1.1,there are reasons to suspect that `binary` should run faster when the
v3.1.1,label is an integer instead of a float
v3.1.1,coding: utf-8
v3.1.1,load or create your dataset
v3.1.1,create dataset for lightgbm
v3.1.1,specify your configurations as a dict
v3.1.1,train
v3.1.1,save model to file
v3.1.1,predict
v3.1.1,eval
v3.1.1,coding: utf-8
v3.1.1,!/usr/bin/env python3
v3.1.1,-*- coding: utf-8 -*-
v3.1.1,
v3.1.1,"LightGBM documentation build configuration file, created by"
v3.1.1,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.1.1,
v3.1.1,This file is execfile()d with the current directory set to its
v3.1.1,containing dir.
v3.1.1,
v3.1.1,Note that not all possible configuration values are present in this
v3.1.1,autogenerated file.
v3.1.1,
v3.1.1,All configuration values have a default; values that are commented out
v3.1.1,serve to show the default.
v3.1.1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.1.1,add these directories to sys.path here. If the directory is relative to the
v3.1.1,"documentation root, use os.path.abspath to make it absolute."
v3.1.1,-- mock out modules
v3.1.1,-- General configuration ------------------------------------------------
v3.1.1,"If your documentation needs a minimal Sphinx version, state it here."
v3.1.1,"Add any Sphinx extension module names here, as strings. They can be"
v3.1.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.1.1,ones.
v3.1.1,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.1.1,Only the class' docstring is inserted.
v3.1.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.1.1,The master toctree document.
v3.1.1,General information about the project.
v3.1.1,The name of an image file (relative to this directory) to place at the top
v3.1.1,of the sidebar.
v3.1.1,The name of an image file (relative to this directory) to use as a favicon of
v3.1.1,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.1.1,pixels large.
v3.1.1,"The version info for the project you're documenting, acts as replacement for"
v3.1.1,"|version| and |release|, also used in various other places throughout the"
v3.1.1,built documents.
v3.1.1,The short X.Y version.
v3.1.1,"The full version, including alpha/beta/rc tags."
v3.1.1,The language for content autogenerated by Sphinx. Refer to documentation
v3.1.1,for a list of supported languages.
v3.1.1,
v3.1.1,This is also used if you do content translation via gettext catalogs.
v3.1.1,"Usually you set ""language"" from the command line for these cases."
v3.1.1,"List of patterns, relative to source directory, that match files and"
v3.1.1,directories to ignore when looking for source files.
v3.1.1,This patterns also effect to html_static_path and html_extra_path
v3.1.1,The name of the Pygments (syntax highlighting) style to use.
v3.1.1,-- Configuration for C API docs generation ------------------------------
v3.1.1,-- Options for HTML output ----------------------------------------------
v3.1.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.1.1,a list of builtin themes.
v3.1.1,Theme options are theme-specific and customize the look and feel of a theme
v3.1.1,"further.  For a list of options available for each theme, see the"
v3.1.1,documentation.
v3.1.1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.1.1,"relative to this directory. They are copied after the builtin static files,"
v3.1.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.1.1,-- Options for HTMLHelp output ------------------------------------------
v3.1.1,Output file base name for HTML help builder.
v3.1.1,-- Options for LaTeX output ---------------------------------------------
v3.1.1,The name of an image file (relative to this directory) to place at the top of
v3.1.1,the title page.
v3.1.1,Warning! The following code can cause buffer overflows on RTD.
v3.1.1,Consider suppressing output completely if RTD project silently fails.
v3.1.1,Refer to https://github.com/svenevs/exhale
v3.1.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.1.1,Warning! The following code can cause buffer overflows on RTD.
v3.1.1,Consider suppressing output completely if RTD project silently fails.
v3.1.1,Refer to https://github.com/svenevs/exhale
v3.1.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.1.1,coding: utf-8
v3.1.1,coding: utf-8
v3.1.1,we don't need lib_lightgbm while building docs
v3.1.1,coding: utf-8
v3.1.1,check saved model persistence
v3.1.1,"we need to check the consistency of model file here, so test for exact equal"
v3.1.1,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.1.1,"scores likely to be different, but prediction should still be the same"
v3.1.1,test that shape is checked during prediction
v3.1.1,test that method works even with free_raw_data=True
v3.1.1,test that method works but sets raw data to None in case of immergeable data types
v3.1.1,test that method works for different data types
v3.1.1,"Set extremely harsh penalties, so CEGB will block most splits."
v3.1.1,"Compare pairs of penalties, to ensure scaling works as intended"
v3.1.1,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.1.1,coding: utf-8
v3.1.1,coding: utf-8
v3.1.1,prediction result is actually not transformed (is raw) due to custom objective
v3.1.1,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.1.1,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.1.1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.1.1,sklearn < 0.23 does not have as_frame parameter
v3.1.1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.1.1,sklearn < 0.23 does not have as_frame parameter
v3.1.1,Test if random_state is properly stored
v3.1.1,Test if two random states produce identical models
v3.1.1,Test if subsequent fits sample from random_state object and produce different models
v3.1.1,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.1.1,With default params
v3.1.1,Tests same probabilities
v3.1.1,Tests same predictions
v3.1.1,Tests same raw scores
v3.1.1,Tests same leaf indices
v3.1.1,Tests same feature contributions
v3.1.1,Tests other parameters for the prediction works
v3.1.1,Tests start_iteration
v3.1.1,"Tests same probabilities, starting from iteration 10"
v3.1.1,"Tests same predictions, starting from iteration 10"
v3.1.1,"Tests same raw scores, starting from iteration 10"
v3.1.1,"Tests same leaf indices, starting from iteration 10"
v3.1.1,"Tests same feature contributions, starting from iteration 10"
v3.1.1,"Tests other parameters for the prediction works, starting from iteration 10"
v3.1.1,"no custom objective, no custom metric"
v3.1.1,default metric
v3.1.1,non-default metric
v3.1.1,no metric
v3.1.1,non-default metric in eval_metric
v3.1.1,non-default metric with non-default metric in eval_metric
v3.1.1,non-default metric with multiple metrics in eval_metric
v3.1.1,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.1.1,default metric for non-default objective
v3.1.1,non-default metric for non-default objective
v3.1.1,no metric
v3.1.1,non-default metric in eval_metric for non-default objective
v3.1.1,non-default metric with non-default metric in eval_metric for non-default objective
v3.1.1,non-default metric with multiple metrics in eval_metric for non-default objective
v3.1.1,"custom objective, no custom metric"
v3.1.1,default regression metric for custom objective
v3.1.1,non-default regression metric for custom objective
v3.1.1,multiple regression metrics for custom objective
v3.1.1,no metric
v3.1.1,default regression metric with non-default metric in eval_metric for custom objective
v3.1.1,non-default regression metric with metric in eval_metric for custom objective
v3.1.1,multiple regression metrics with metric in eval_metric for custom objective
v3.1.1,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.1.1,"no custom objective, custom metric"
v3.1.1,default metric with custom metric
v3.1.1,non-default metric with custom metric
v3.1.1,multiple metrics with custom metric
v3.1.1,custom metric (disable default metric)
v3.1.1,default metric for non-default objective with custom metric
v3.1.1,non-default metric for non-default objective with custom metric
v3.1.1,multiple metrics for non-default objective with custom metric
v3.1.1,custom metric (disable default metric for non-default objective)
v3.1.1,"custom objective, custom metric"
v3.1.1,custom metric for custom objective
v3.1.1,non-default regression metric with custom metric for custom objective
v3.1.1,multiple regression metrics with custom metric for custom objective
v3.1.1,default metric and invalid binary metric is replaced with multiclass alternative
v3.1.1,invalid objective is replaced with default multiclass one
v3.1.1,and invalid binary metric is replaced with multiclass alternative
v3.1.1,default metric for non-default multiclass objective
v3.1.1,and invalid binary metric is replaced with multiclass alternative
v3.1.1,default metric and invalid multiclass metric is replaced with binary alternative
v3.1.1,invalid multiclass metric is replaced with binary alternative for custom objective
v3.1.1,"Verify that can receive a list of metrics, only callable"
v3.1.1,Verify that can receive a list of custom and built-in metrics
v3.1.1,Verify that works as expected when eval_metric is empty
v3.1.1,"Verify that can receive a list of metrics, only built-in"
v3.1.1,Verify that eval_metric is robust to receiving a list with None
v3.1.1,training data as eval_set
v3.1.1,feval
v3.1.1,single eval_set
v3.1.1,two eval_set
v3.1.1,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.1.1,Test that estimators are default-constructible
v3.1.1,coding: utf-8
v3.1.1,coding: utf-8
v3.1.1,check that default gives same result as k = 1
v3.1.1,check against independent calculation for k = 1
v3.1.1,check against independent calculation for k = 2
v3.1.1,check against independent calculation for k = 10
v3.1.1,check cases where predictions are equal
v3.1.1,should give same result as binary auc for 2 classes
v3.1.1,test the case where all predictions are equal
v3.1.1,test that weighted data gives different auc_mu
v3.1.1,test that equal data weights give same auc_mu as unweighted data
v3.1.1,should give 1 when accuracy = 1
v3.1.1,test loading class weights
v3.1.1,no early stopping
v3.1.1,early stopping occurs
v3.1.1,test custom eval metrics
v3.1.1,"shuffle = False, override metric in params"
v3.1.1,"shuffle = True, callbacks"
v3.1.1,enable display training loss
v3.1.1,self defined folds
v3.1.1,lambdarank
v3.1.1,... with l2 metric
v3.1.1,... with NDCG (default) metric
v3.1.1,self defined folds with lambdarank
v3.1.1,with early stopping
v3.1.1,predict by each fold booster
v3.1.1,fold averaging
v3.1.1,without early stopping
v3.1.1,test feature_names with whitespaces
v3.1.1,This has non-ascii strings.
v3.1.1,take subsets and train
v3.1.1,generate CSR sparse dataset
v3.1.1,convert data to dense and get back same contribs
v3.1.1,validate the values are the same
v3.1.1,validate using CSC matrix
v3.1.1,validate the values are the same
v3.1.1,generate CSR sparse dataset
v3.1.1,convert data to dense and get back same contribs
v3.1.1,validate the values are the same
v3.1.1,validate using CSC matrix
v3.1.1,validate the values are the same
v3.1.1,Note there is an extra column added to the output for the expected value
v3.1.1,Note output CSC shape should be same as CSR output shape
v3.1.1,test sliced labels
v3.1.1,append some columns
v3.1.1,append some rows
v3.1.1,test sliced 2d matrix
v3.1.1,test sliced CSR
v3.1.1,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.1.1,The penalization is so high that the first 2 features should not be used here
v3.1.1,Check that a very high penalization is the same as not using the features at all
v3.1.1,"no fobj, no feval"
v3.1.1,default metric
v3.1.1,non-default metric in params
v3.1.1,default metric in args
v3.1.1,non-default metric in args
v3.1.1,metric in args overwrites one in params
v3.1.1,multiple metrics in params
v3.1.1,multiple metrics in args
v3.1.1,remove default metric by 'None' in list
v3.1.1,remove default metric by 'None' aliases
v3.1.1,"fobj, no feval"
v3.1.1,no default metric
v3.1.1,metric in params
v3.1.1,metric in args
v3.1.1,metric in args overwrites its' alias in params
v3.1.1,multiple metrics in params
v3.1.1,multiple metrics in args
v3.1.1,"no fobj, feval"
v3.1.1,default metric with custom one
v3.1.1,non-default metric in params with custom one
v3.1.1,default metric in args with custom one
v3.1.1,non-default metric in args with custom one
v3.1.1,"metric in args overwrites one in params, custom one is evaluated too"
v3.1.1,multiple metrics in params with custom one
v3.1.1,multiple metrics in args with custom one
v3.1.1,custom metric is evaluated despite 'None' is passed
v3.1.1,"fobj, feval"
v3.1.1,"no default metric, only custom one"
v3.1.1,metric in params with custom one
v3.1.1,metric in args with custom one
v3.1.1,"metric in args overwrites one in params, custom one is evaluated too"
v3.1.1,multiple metrics in params with custom one
v3.1.1,multiple metrics in args with custom one
v3.1.1,custom metric is evaluated despite 'None' is passed
v3.1.1,"no fobj, no feval"
v3.1.1,default metric
v3.1.1,default metric in params
v3.1.1,non-default metric in params
v3.1.1,multiple metrics in params
v3.1.1,remove default metric by 'None' aliases
v3.1.1,"fobj, no feval"
v3.1.1,no default metric
v3.1.1,metric in params
v3.1.1,multiple metrics in params
v3.1.1,"no fobj, feval"
v3.1.1,default metric with custom one
v3.1.1,default metric in params with custom one
v3.1.1,non-default metric in params with custom one
v3.1.1,multiple metrics in params with custom one
v3.1.1,custom metric is evaluated despite 'None' is passed
v3.1.1,"fobj, feval"
v3.1.1,"no default metric, only custom one"
v3.1.1,metric in params with custom one
v3.1.1,multiple metrics in params with custom one
v3.1.1,custom metric is evaluated despite 'None' is passed
v3.1.1,multiclass default metric
v3.1.1,multiclass default metric with custom one
v3.1.1,multiclass metric alias with custom one for custom objective
v3.1.1,no metric for invalid class_num
v3.1.1,custom metric for invalid class_num
v3.1.1,multiclass metric alias with custom one with invalid class_num
v3.1.1,multiclass default metric without num_class
v3.1.1,multiclass metric alias
v3.1.1,multiclass metric
v3.1.1,non-valid metric for multiclass objective
v3.1.1,non-default num_class for default objective
v3.1.1,no metric with non-default num_class for custom objective
v3.1.1,multiclass metric alias for custom objective
v3.1.1,multiclass metric for custom objective
v3.1.1,binary metric with non-default num_class for custom objective
v3.1.1,Expect three metrics but mean and stdv for each metric
v3.1.1,test XGBoost-style return value
v3.1.1,test numpy-style return value
v3.1.1,test bins string type
v3.1.1,test histogram is disabled for categorical features
v3.1.1,test for lgb.train
v3.1.1,test feval for lgb.train
v3.1.1,test with two valid data for lgb.train
v3.1.1,test for lgb.cv
v3.1.1,test feval for lgb.cv
v3.1.1,test that binning works properly for features with only positive or only negative values
v3.1.1,decreasing without freeing raw data is allowed
v3.1.1,decreasing before lazy init is allowed
v3.1.1,increasing is allowed
v3.1.1,decreasing with disabled filter is allowed
v3.1.1,decreasing with enabled filter is disallowed;
v3.1.1,also changes of other params are disallowed
v3.1.1,check extra trees increases regularization
v3.1.1,check path smoothing increases regularization
v3.1.1,test edge case with one leaf
v3.1.1,check that constraint containing all features is equivalent to no constraint
v3.1.1,check that constraint partitioning the features reduces train accuracy
v3.1.1,check that constraints consisting of single features reduce accuracy further
v3.1.1,test that interaction constraints work when not all features are used
v3.1.1,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.1.1,"test the case where start_iteration <= 0, and num_iteration is None"
v3.1.1,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.1.1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.1.1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.1.1,test for regression
v3.1.1,test both with and without early stopping
v3.1.1,test for multi-class
v3.1.1,test both with and without early stopping
v3.1.1,test for binary
v3.1.1,test both with and without early stopping
v3.1.1,test against sklearn average precision metric
v3.1.1,test that average precision is 1 where model predicts perfectly
v3.1.1,coding: utf-8
v3.1.1,convert from one-based to  zero-based index
v3.1.1,convert from boundaries to size
v3.1.1,--- start Booster interfaces
v3.1.1,.Call() calls
v3.1.1,coding: utf-8
v3.1.1,alias table
v3.1.1,names
v3.1.1,from strings
v3.1.1,tails
v3.1.1,tails
v3.1.1,coding: utf-8
v3.1.1,Single row predictor to abstract away caching logic
v3.1.1,create boosting
v3.1.1,initialize the boosting
v3.1.1,create objective function
v3.1.1,initialize the objective function
v3.1.1,create training metric
v3.1.1,reset the boosting
v3.1.1,create objective function
v3.1.1,initialize the objective function
v3.1.1,calculate the nonzero data and indices size
v3.1.1,allocate data and indices arrays
v3.1.1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.1.1,aggregated per row feature contribution results
v3.1.1,keep track of the row_vector sizes for parallelization
v3.1.1,copy vector results to output for each row
v3.1.1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.1.1,aggregated per row feature contribution results
v3.1.1,calculate number of elements per column to construct
v3.1.1,the CSC matrix with random access
v3.1.1,keep track of column counts
v3.1.1,keep track of beginning index for each column
v3.1.1,keep track of beginning index for each matrix
v3.1.1,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.1.1,store the row index
v3.1.1,update column count
v3.1.1,explicitly declare symbols from LightGBM namespace
v3.1.1,some help functions used to convert data
v3.1.1,Row iterator of on column for CSC matrix
v3.1.1,"return value at idx, only can access by ascent order"
v3.1.1,"return next non-zero pair, if index < 0, means no more data"
v3.1.1,start of c_api functions
v3.1.1,sample data first
v3.1.1,sample data first
v3.1.1,sample data first
v3.1.1,local buffer to re-use memory
v3.1.1,sample data first
v3.1.1,no more data
v3.1.1,---- start of booster
v3.1.1,Single row in row-major format:
v3.1.1,---- start of some help functions
v3.1.1,data is array of pointers to individual rows
v3.1.1,set number of threads for openmp
v3.1.1,check for alias
v3.1.1,read parameters from config file
v3.1.1,"remove str after ""#"""
v3.1.1,check for alias again
v3.1.1,load configs
v3.1.1,prediction is needed if using input initial model(continued train)
v3.1.1,need to continue training
v3.1.1,sync up random seed for data partition
v3.1.1,load Training data
v3.1.1,load data for parallel training
v3.1.1,load data for single machine
v3.1.1,need save binary file
v3.1.1,create training metric
v3.1.1,only when have metrics then need to construct validation data
v3.1.1,"Add validation data, if it exists"
v3.1.1,add
v3.1.1,need save binary file
v3.1.1,add metric for validation data
v3.1.1,output used time on each iteration
v3.1.1,need init network
v3.1.1,create boosting
v3.1.1,create objective function
v3.1.1,load training data
v3.1.1,initialize the objective function
v3.1.1,initialize the boosting
v3.1.1,add validation data into boosting
v3.1.1,convert model to if-else statement code
v3.1.1,create predictor
v3.1.1,Free memory
v3.1.1,create predictor
v3.1.1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.1.1,counts for all labels
v3.1.1,"start from top label, and accumulate DCG"
v3.1.1,counts for all labels
v3.1.1,calculate k Max DCG by one pass
v3.1.1,get sorted indices by score
v3.1.1,calculate dcg
v3.1.1,get sorted indices by score
v3.1.1,calculate multi dcg by one pass
v3.1.1,wait for all client start up
v3.1.1,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.1.1,Instead we will handle finalize/abort for MPI in main().
v3.1.1,default set to -1
v3.1.1,"distance at k-th communication, distance[k] = 2^k"
v3.1.1,set incoming rank at k-th commuication
v3.1.1,set outgoing rank at k-th commuication
v3.1.1,defalut set as -1
v3.1.1,construct all recursive halving map for all machines
v3.1.1,let 1 << k <= num_machines
v3.1.1,distance of each communication
v3.1.1,"if num_machines = 2^k, don't need to group machines"
v3.1.1,"communication direction, %2 == 0 is positive"
v3.1.1,neighbor at k-th communication
v3.1.1,receive data block at k-th communication
v3.1.1,send data block at k-th communication
v3.1.1,"if num_machines != 2^k, need to group machines"
v3.1.1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.1.1,let left machine as group leader
v3.1.1,"cache block information for groups, group with 2 machines will have double block size"
v3.1.1,convert from group to node leader
v3.1.1,convert from node to group
v3.1.1,meet new group
v3.1.1,add block len for this group
v3.1.1,calculate the group block start
v3.1.1,not need to construct
v3.1.1,get receive block informations
v3.1.1,accumulate block len
v3.1.1,get send block informations
v3.1.1,accumulate block len
v3.1.1,static member definition
v3.1.1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.1.1,assign the blocks to every rank.
v3.1.1,do reduce scatter
v3.1.1,do all gather
v3.1.1,assign blocks
v3.1.1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.1.1,copy back
v3.1.1,assign blocks
v3.1.1,start all gather
v3.1.1,when num_machines is small and data is large
v3.1.1,use output as receive buffer
v3.1.1,get current local block size
v3.1.1,get out rank
v3.1.1,get in rank
v3.1.1,get send information
v3.1.1,get recv information
v3.1.1,send and recv at same time
v3.1.1,rotate in-place
v3.1.1,use output as receive buffer
v3.1.1,get current local block size
v3.1.1,get send information
v3.1.1,get recv information
v3.1.1,send and recv at same time
v3.1.1,use output as receive buffer
v3.1.1,send and recv at same time
v3.1.1,send local data to neighbor first
v3.1.1,receive neighbor data first
v3.1.1,reduce
v3.1.1,get target
v3.1.1,get send information
v3.1.1,get recv information
v3.1.1,send and recv at same time
v3.1.1,reduce
v3.1.1,send result to neighbor
v3.1.1,receive result from neighbor
v3.1.1,copy result
v3.1.1,start up socket
v3.1.1,parse clients from file
v3.1.1,get ip list of local machine
v3.1.1,get local rank
v3.1.1,construct listener
v3.1.1,construct communication topo
v3.1.1,construct linkers
v3.1.1,free listener
v3.1.1,set timeout
v3.1.1,accept incoming socket
v3.1.1,receive rank
v3.1.1,add new socket
v3.1.1,save ranks that need to connect with
v3.1.1,start listener
v3.1.1,start connect
v3.1.1,let smaller rank connect to larger rank
v3.1.1,send local rank
v3.1.1,wait for listener
v3.1.1,print connected linkers
v3.1.1,only need to copy subset
v3.1.1,avoid to copy subset many times
v3.1.1,avoid out of range
v3.1.1,may need to recopy subset
v3.1.1,valid the type
v3.1.1,Constructors
v3.1.1,Get type tag
v3.1.1,Comparisons
v3.1.1,"This has to be separate, not in Statics, because Json() accesses"
v3.1.1,statics().null.
v3.1.1,"advance until next line, or end of input"
v3.1.1,advance until closing tokens
v3.1.1,The usual case: non-escaped characters
v3.1.1,Handle escapes
v3.1.1,Extract 4-byte escape sequence
v3.1.1,Explicitly check length of the substring. The following loop
v3.1.1,relies on std::string returning the terminating NUL when
v3.1.1,accessing str[length]. Checking here reduces brittleness.
v3.1.1,JSON specifies that characters outside the BMP shall be encoded as a
v3.1.1,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.1.1,components. Check whether we're in the middle of such a beast: the
v3.1.1,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.1.1,a trail (low) surrogate.
v3.1.1,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.1.1,per the UTF-16 algorithm.
v3.1.1,Integer part
v3.1.1,Decimal part
v3.1.1,Exponent part
v3.1.1,Check for any trailing garbage
v3.1.1,Documented in json11.hpp
v3.1.1,Check for another object
v3.1.1,get column names
v3.1.1,load label idx first
v3.1.1,erase label column name
v3.1.1,load ignore columns
v3.1.1,load weight idx
v3.1.1,load group idx
v3.1.1,don't support query id in data file when training in parallel
v3.1.1,read data to memory
v3.1.1,sample data
v3.1.1,construct feature bin mappers
v3.1.1,initialize label
v3.1.1,extract features
v3.1.1,sample data from file
v3.1.1,construct feature bin mappers
v3.1.1,initialize label
v3.1.1,extract features
v3.1.1,load data from binary file
v3.1.1,check meta data
v3.1.1,need to check training data
v3.1.1,read data in memory
v3.1.1,initialize label
v3.1.1,extract features
v3.1.1,Get number of lines of data file
v3.1.1,initialize label
v3.1.1,extract features
v3.1.1,load data from binary file
v3.1.1,not need to check validation data
v3.1.1,check meta data
v3.1.1,buffer to read binary file
v3.1.1,check token
v3.1.1,read size of header
v3.1.1,re-allocmate space if not enough
v3.1.1,read header
v3.1.1,get header
v3.1.1,num_groups
v3.1.1,real_feature_idx_
v3.1.1,feature2group
v3.1.1,feature2subfeature
v3.1.1,group_bin_boundaries
v3.1.1,group_feature_start_
v3.1.1,group_feature_cnt_
v3.1.1,get feature names
v3.1.1,write feature names
v3.1.1,get forced_bin_bounds_
v3.1.1,read size of meta data
v3.1.1,re-allocate space if not enough
v3.1.1,read meta data
v3.1.1,load meta data
v3.1.1,sample local used data if need to partition
v3.1.1,"if not contain query file, minimal sample unit is one record"
v3.1.1,"if contain query file, minimal sample unit is one query"
v3.1.1,if is new query
v3.1.1,read feature data
v3.1.1,read feature size
v3.1.1,re-allocate space if not enough
v3.1.1,fill feature_names_ if not header
v3.1.1,get forced split
v3.1.1,"if only one machine, find bin locally"
v3.1.1,"if have multi-machines, need to find bin distributed"
v3.1.1,different machines will find bin for different features
v3.1.1,start and len will store the process feature indices for different machines
v3.1.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.1.1,free
v3.1.1,gather global feature bin mappers
v3.1.1,restore features bins from buffer
v3.1.1,---- private functions ----
v3.1.1,"if features are ordered, not need to use hist_buf"
v3.1.1,read all lines
v3.1.1,get query data
v3.1.1,"if not contain query data, minimal sample unit is one record"
v3.1.1,"if contain query data, minimal sample unit is one query"
v3.1.1,if is new query
v3.1.1,get query data
v3.1.1,"if not contain query file, minimal sample unit is one record"
v3.1.1,"if contain query file, minimal sample unit is one query"
v3.1.1,if is new query
v3.1.1,parse features
v3.1.1,get forced split
v3.1.1,"check the range of label_idx, weight_idx and group_idx"
v3.1.1,fill feature_names_ if not header
v3.1.1,start find bins
v3.1.1,"if only one machine, find bin locally"
v3.1.1,start and len will store the process feature indices for different machines
v3.1.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.1.1,free
v3.1.1,gather global feature bin mappers
v3.1.1,restore features bins from buffer
v3.1.1,if doesn't need to prediction with initial model
v3.1.1,parser
v3.1.1,set label
v3.1.1,free processed line:
v3.1.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.1.1,text_reader_->Lines()[i].shrink_to_fit();
v3.1.1,push data
v3.1.1,if is used feature
v3.1.1,if need to prediction with initial model
v3.1.1,parser
v3.1.1,set initial score
v3.1.1,set label
v3.1.1,free processed line:
v3.1.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.1.1,text_reader_->Lines()[i].shrink_to_fit();
v3.1.1,push data
v3.1.1,if is used feature
v3.1.1,metadata_ will manage space of init_score
v3.1.1,text data can be free after loaded feature values
v3.1.1,parser
v3.1.1,set initial score
v3.1.1,set label
v3.1.1,push data
v3.1.1,if is used feature
v3.1.1,only need part of data
v3.1.1,need full data
v3.1.1,metadata_ will manage space of init_score
v3.1.1,read size of token
v3.1.1,remove duplicates
v3.1.1,deep copy function for BinMapper
v3.1.1,mean size for one bin
v3.1.1,need a new bin
v3.1.1,update bin upper bound
v3.1.1,last bin upper bound
v3.1.1,get list of distinct values
v3.1.1,get number of positive and negative distinct values
v3.1.1,include zero bounds and infinity bound
v3.1.1,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.1.1,find remaining bounds
v3.1.1,find distinct_values first
v3.1.1,push zero in the front
v3.1.1,use the large value
v3.1.1,push zero in the back
v3.1.1,convert to int type first
v3.1.1,sort by counts
v3.1.1,will ignore the categorical of small counts
v3.1.1,Push the dummy bin for NaN
v3.1.1,Use MissingType::None to represent this bin contains all categoricals
v3.1.1,fix count of NaN bin
v3.1.1,check trivial(num_bin_ == 1) feature
v3.1.1,check useless bin
v3.1.1,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.1.1,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.1.1,calculate max bin of all features to select the int type in MultiValDenseBin
v3.1.1,"for lambdarank, it needs query data for partition data in parallel learning"
v3.1.1,need convert query_id to boundaries
v3.1.1,check weights
v3.1.1,check query boundries
v3.1.1,contain initial score file
v3.1.1,check weights
v3.1.1,get local weights
v3.1.1,check query boundries
v3.1.1,get local query boundaries
v3.1.1,contain initial score file
v3.1.1,get local initial scores
v3.1.1,re-load query weight
v3.1.1,save to nullptr
v3.1.1,save to nullptr
v3.1.1,save to nullptr
v3.1.1,default weight file name
v3.1.1,default init_score file name
v3.1.1,use first line to count number class
v3.1.1,default query file name
v3.1.1,root is in the depth 0
v3.1.1,non-leaf
v3.1.1,leaf
v3.1.1,use this for the missing value conversion
v3.1.1,Predict func by Map to ifelse
v3.1.1,use this for the missing value conversion
v3.1.1,non-leaf
v3.1.1,left subtree
v3.1.1,right subtree
v3.1.1,leaf
v3.1.1,non-leaf
v3.1.1,left subtree
v3.1.1,right subtree
v3.1.1,leaf
v3.1.1,recursive computation of SHAP values for a decision tree
v3.1.1,extend the unique path
v3.1.1,leaf node
v3.1.1,internal node
v3.1.1,"see if we have already split on this feature,"
v3.1.1,if so we undo that split so we can redo it for this node
v3.1.1,recursive sparse computation of SHAP values for a decision tree
v3.1.1,extend the unique path
v3.1.1,leaf node
v3.1.1,internal node
v3.1.1,"see if we have already split on this feature,"
v3.1.1,if so we undo that split so we can redo it for this node
v3.1.1,add names of objective function if not providing metric
v3.1.1,equal weights for all classes
v3.1.1,generate seeds by seed.
v3.1.1,sort eval_at
v3.1.1,Only push the non-training data
v3.1.1,check for conflicts
v3.1.1,"check if objective, metric, and num_class match"
v3.1.1,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.1.1,Check max_depth and num_leaves
v3.1.1,"Fits in an int, and is more restrictive than the current num_leaves"
v3.1.1,force col-wise for gpu & CUDA
v3.1.1,force gpu_use_dp for CUDA
v3.1.1,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.1.1,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.1.1,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.1.1,calculated split gain can be positive even with zero gradient and hessian.
v3.1.1,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.1.1,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.1.1,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.1.1,first round: fill the single val group
v3.1.1,always push the last group
v3.1.1,put dense feature first
v3.1.1,sort by non zero cnt
v3.1.1,"sort by non zero cnt, bigger first"
v3.1.1,shuffle groups
v3.1.1,Using std::swap for vector<bool> will cause the wrong result.
v3.1.1,get num_features
v3.1.1,get bin_mappers
v3.1.1,"for sparse multi value bin, we store the feature bin values with offset added"
v3.1.1,"for dense multi value bin, the feature bin values without offsets are used"
v3.1.1,copy feature bin mapper data
v3.1.1,"if not pass a filename, just append "".bin"" of original file"
v3.1.1,get size of header
v3.1.1,size of feature names
v3.1.1,size of forced bins
v3.1.1,write header
v3.1.1,write feature names
v3.1.1,write forced bins
v3.1.1,get size of meta data
v3.1.1,write meta data
v3.1.1,write feature data
v3.1.1,get size of feature
v3.1.1,write feature
v3.1.1,"explicitly initilize template methods, for cross module call"
v3.1.1,"Only one multi-val group, just simply merge"
v3.1.1,Skip the leading 0 when copying group_bin_boundaries.
v3.1.1,regenerate other fields
v3.1.1,store the importance first
v3.1.1,PredictRaw
v3.1.1,PredictRawByMap
v3.1.1,Predict
v3.1.1,PredictByMap
v3.1.1,PredictLeafIndex
v3.1.1,PredictLeafIndexByMap
v3.1.1,output model type
v3.1.1,output number of class
v3.1.1,output label index
v3.1.1,output max_feature_idx
v3.1.1,output objective
v3.1.1,output tree models
v3.1.1,store the importance first
v3.1.1,sort the importance
v3.1.1,use serialized string to restore this object
v3.1.1,Use first 128 chars to avoid exceed the message buffer.
v3.1.1,get number of classes
v3.1.1,get index of label
v3.1.1,get max_feature_idx first
v3.1.1,get average_output
v3.1.1,get feature names
v3.1.1,get monotone_constraints
v3.1.1,set zero
v3.1.1,predict all the trees for one iteration
v3.1.1,check early stopping
v3.1.1,set zero
v3.1.1,predict all the trees for one iteration
v3.1.1,check early stopping
v3.1.1,margin_threshold will be captured by value
v3.1.1,copy and sort
v3.1.1,margin_threshold will be captured by value
v3.1.1,Fix for compiler warnings about reaching end of control
v3.1.1,load forced_splits file
v3.1.1,init tree learner
v3.1.1,push training metrics
v3.1.1,create buffer for gradients and hessians
v3.1.1,get max feature index
v3.1.1,get label index
v3.1.1,get feature names
v3.1.1,"if need bagging, create buffer"
v3.1.1,"for a validation dataset, we need its score and metric"
v3.1.1,update score
v3.1.1,objective function will calculate gradients and hessians
v3.1.1,"random bagging, minimal unit is one record"
v3.1.1,"random bagging, minimal unit is one record"
v3.1.1,if need bagging
v3.1.1,set bagging data to tree learner
v3.1.1,get subset
v3.1.1,output used time per iteration
v3.1.1,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.1.1,boosting first
v3.1.1,bagging logic
v3.1.1,need to copy gradients for bagging subset.
v3.1.1,shrinkage by learning rate
v3.1.1,update score
v3.1.1,only add default score one-time
v3.1.1,updates scores
v3.1.1,add model
v3.1.1,reset score
v3.1.1,remove model
v3.1.1,print message for metric
v3.1.1,pop last early_stopping_round_ models
v3.1.1,update training score
v3.1.1,we need to predict out-of-bag scores of data for boosting
v3.1.1,update validation score
v3.1.1,print training metric
v3.1.1,print validation metric
v3.1.1,set zero
v3.1.1,predict all the trees for one iteration
v3.1.1,predict all the trees for one iteration
v3.1.1,push training metrics
v3.1.1,"not same training data, need reset score and others"
v3.1.1,create score tracker
v3.1.1,update score
v3.1.1,create buffer for gradients and hessians
v3.1.1,load forced_splits file
v3.1.1,"if need bagging, create buffer"
v3.1.1,Get the max size of pool
v3.1.1,at least need 2 leaves
v3.1.1,push split information for all leaves
v3.1.1,initialize splits for leaf
v3.1.1,initialize data partition
v3.1.1,initialize ordered gradients and hessians
v3.1.1,cannot change is_hist_col_wise during training
v3.1.1,initialize splits for leaf
v3.1.1,initialize data partition
v3.1.1,initialize ordered gradients and hessians
v3.1.1,Get the max size of pool
v3.1.1,at least need 2 leaves
v3.1.1,push split information for all leaves
v3.1.1,some initial works before training
v3.1.1,root leaf
v3.1.1,only root leaf can be splitted on first time
v3.1.1,some initial works before finding best split
v3.1.1,find best threshold for every feature
v3.1.1,Get a leaf with max split gain
v3.1.1,Get split information for best leaf
v3.1.1,"cannot split, quit"
v3.1.1,split tree with best leaf
v3.1.1,reset histogram pool
v3.1.1,initialize data partition
v3.1.1,reset the splits for leaves
v3.1.1,Sumup for root
v3.1.1,use all data
v3.1.1,"use bagging, only use part of data"
v3.1.1,check depth of current leaf
v3.1.1,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.1.1,no enough data to continue
v3.1.1,only have root
v3.1.1,put parent(left) leaf's histograms into larger leaf's histograms
v3.1.1,put parent(left) leaf's histograms to larger leaf's histograms
v3.1.1,construct smaller leaf
v3.1.1,construct larger leaf
v3.1.1,find splits
v3.1.1,only has root leaf
v3.1.1,start at root leaf
v3.1.1,"before processing next node from queue, store info for current left/right leaf"
v3.1.1,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.1.1,"then, compute own splits"
v3.1.1,split info should exist because searching in bfs fashion - should have added from parent
v3.1.1,update before tree split
v3.1.1,don't need to update this in data-based parallel model
v3.1.1,"split tree, will return right leaf"
v3.1.1,store the true split gain in tree model
v3.1.1,don't need to update this in data-based parallel model
v3.1.1,store the true split gain in tree model
v3.1.1,init the leaves that used on next iteration
v3.1.1,update leave outputs if needed
v3.1.1,bag_mapper[index_mapper[i]]
v3.1.1,it is needed to filter the features after the above code.
v3.1.1,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.1.1,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.1.1,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.1.1,find splits
v3.1.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.1.1,get feature partition
v3.1.1,get local used features
v3.1.1,get best split at smaller leaf
v3.1.1,find local best split for larger leaf
v3.1.1,sync global best info
v3.1.1,update best split
v3.1.1,"instantiate template classes, otherwise linker cannot find the code"
v3.1.1,initialize SerialTreeLearner
v3.1.1,Get local rank and global machine size
v3.1.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.1.1,allocate buffer for communication
v3.1.1,generate feature partition for current tree
v3.1.1,get local used feature
v3.1.1,get block start and block len for reduce scatter
v3.1.1,get buffer_write_start_pos_
v3.1.1,get buffer_read_start_pos_
v3.1.1,sync global data sumup info
v3.1.1,global sumup reduce
v3.1.1,copy back
v3.1.1,set global sumup info
v3.1.1,init global data count in leaf
v3.1.1,construct local histograms
v3.1.1,copy to buffer
v3.1.1,Reduce scatter for histogram
v3.1.1,restore global histograms from buffer
v3.1.1,only root leaf
v3.1.1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.1.1,find local best split for larger leaf
v3.1.1,sync global best info
v3.1.1,set best split
v3.1.1,need update global number of data in leaf
v3.1.1,"instantiate template classes, otherwise linker cannot find the code"
v3.1.1,initialize SerialTreeLearner
v3.1.1,some additional variables needed for GPU trainer
v3.1.1,Initialize GPU buffers and kernels
v3.1.1,some functions used for debugging the GPU histogram construction
v3.1.1,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.1.1,goto err;
v3.1.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.1.1,also guarantee that there are at least 2K examples per workgroup
v3.1.1,return 0;
v3.1.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.1.1,decide the best number of workgroups working on one feature4 tuple
v3.1.1,set work group size based on feature size
v3.1.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.1.1,we need to refresh the kernel arguments after reallocating
v3.1.1,The only argument that needs to be changed later is num_data_
v3.1.1,"the GPU kernel will process all features in one call, and each"
v3.1.1,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.1.1,process one feature4 tuple
v3.1.1,"for the root node, indices are not copied"
v3.1.1,"for constant hessian, hessians are not copied except for the root node"
v3.1.1,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.1.1,and we will launch num_feature workgroups for this kernel
v3.1.1,will launch threads for all features
v3.1.1,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.1.1,copy the results asynchronously. Size depends on if double precision is used
v3.1.1,we will wait for this object in WaitAndGetHistograms
v3.1.1,"when the output is ready, the computation is done"
v3.1.1,values of this feature has been redistributed to multiple bins; need a reduction here
v3.1.1,how many feature-group tuples we have
v3.1.1,leave some safe margin for prefetching
v3.1.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.1.1,clear sparse/dense maps
v3.1.1,do nothing if no features can be processed on GPU
v3.1.1,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.1.1,unpin old buffer if necessary before destructing them
v3.1.1,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v3.1.1,allocate space for gradients and hessians on device
v3.1.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.1.1,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.1.1,copy indices to the device
v3.1.1,histogram bin entry size depends on the precision (single/double)
v3.1.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.1.1,each work group generates a sub-histogram of dword_features_ features.
v3.1.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.1.1,create atomic counters for inter-group coordination
v3.1.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.1.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.1.1,looking for dword_features_ non-sparse feature-groups
v3.1.1,decide if we need to redistribute the bin
v3.1.1,multiplier must be a power of 2
v3.1.1,device_bin_mults_.push_back(1);
v3.1.1,found
v3.1.1,for data transfer time
v3.1.1,"Now generate new data structure feature4, and copy data to the device"
v3.1.1,"preallocate arrays for all threads, and pin them"
v3.1.1,building Feature4 bundles; each thread handles dword_features_ features
v3.1.1,one feature datapoint is 4 bits
v3.1.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.1.1,one feature datapoint is one byte
v3.1.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.1.1,Dense bin
v3.1.1,Dense 4-bit bin
v3.1.1,working on the remaining (less than dword_features_) feature groups
v3.1.1,fill the leftover features
v3.1.1,"fill this empty feature with some ""random"" value"
v3.1.1,"fill this empty feature with some ""random"" value"
v3.1.1,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.1.1,deallocate pinned space for feature copying
v3.1.1,data transfer time
v3.1.1,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.1.1,"Something bad happened. Just return ""No log available."""
v3.1.1,"build is okay, log may contain warnings"
v3.1.1,destroy any old kernels
v3.1.1,create OpenCL kernels for different number of workgroups per feature
v3.1.1,currently we don't use constant memory
v3.1.1,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.1.1,kernel with indices in an array
v3.1.1,"kernel with all features enabled, with elimited branches"
v3.1.1,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.1.1,do nothing if no features can be processed on GPU
v3.1.1,The only argument that needs to be changed later is num_data_
v3.1.1,"hessian is passed as a parameter, but it is not available now."
v3.1.1,hessian will be set in BeforeTrain()
v3.1.1,"Get the max bin size, used for selecting best GPU kernel"
v3.1.1,initialize GPU
v3.1.1,determine which kernel to use based on the max number of bins
v3.1.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.1.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.1.1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.1.1,setup GPU kernel arguments after we allocating all the buffers
v3.1.1,GPU memory has to been reallocated because data may have been changed
v3.1.1,setup GPU kernel arguments after we allocating all the buffers
v3.1.1,Copy initial full hessians and gradients to GPU.
v3.1.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.1.1,setup hessian parameters only
v3.1.1,hessian is passed as a parameter
v3.1.1,use bagging
v3.1.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.1.1,copy used gradients and hessians to ordered buffer
v3.1.1,transfer the indices to GPU
v3.1.1,transfer hessian to GPU
v3.1.1,setup hessian parameters only
v3.1.1,hessian is passed as a parameter
v3.1.1,transfer gradients to GPU
v3.1.1,only have root
v3.1.1,"Copy indices, gradients and hessians as early as possible"
v3.1.1,only need to initialize for smaller leaf
v3.1.1,Get leaf boundary
v3.1.1,copy indices to the GPU:
v3.1.1,copy ordered hessians to the GPU:
v3.1.1,copy ordered gradients to the GPU:
v3.1.1,do nothing if no features can be processed on GPU
v3.1.1,copy data indices if it is not null
v3.1.1,generate and copy ordered_gradients if gradients is not null
v3.1.1,generate and copy ordered_hessians if hessians is not null
v3.1.1,converted indices in is_feature_used to feature-group indices
v3.1.1,construct the feature masks for dense feature-groups
v3.1.1,"if no feature group is used, just return and do not use GPU"
v3.1.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.1.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.1.1,"All data have been prepared, now run the GPU kernel"
v3.1.1,construct smaller leaf
v3.1.1,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v3.1.1,then construct sparse features on CPU
v3.1.1,"wait for GPU to finish, only if GPU is actually used"
v3.1.1,use double precision
v3.1.1,use single precision
v3.1.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.1.1,#define GPU_DEBUG_COMPARE
v3.1.1,construct larger leaf
v3.1.1,then construct sparse features on CPU
v3.1.1,"wait for GPU to finish, only if GPU is actually used"
v3.1.1,use double precision
v3.1.1,use single precision
v3.1.1,do some sanity check for the GPU algorithm
v3.1.1,limit top k
v3.1.1,get max bin
v3.1.1,calculate buffer size
v3.1.1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.1.1,"left and right on same time, so need double size"
v3.1.1,initialize histograms for global
v3.1.1,sync global data sumup info
v3.1.1,set global sumup info
v3.1.1,init global data count in leaf
v3.1.1,get local sumup
v3.1.1,get local sumup
v3.1.1,get mean number on machines
v3.1.1,weighted gain
v3.1.1,get top k
v3.1.1,"Copy histogram to buffer, and Get local aggregate features"
v3.1.1,copy histograms.
v3.1.1,copy smaller leaf histograms first
v3.1.1,mark local aggregated feature
v3.1.1,copy
v3.1.1,then copy larger leaf histograms
v3.1.1,mark local aggregated feature
v3.1.1,copy
v3.1.1,use local data to find local best splits
v3.1.1,find splits
v3.1.1,only has root leaf
v3.1.1,local voting
v3.1.1,gather
v3.1.1,get all top-k from all machines
v3.1.1,global voting
v3.1.1,copy local histgrams to buffer
v3.1.1,Reduce scatter for histogram
v3.1.1,find best split from local aggregated histograms
v3.1.1,restore from buffer
v3.1.1,restore from buffer
v3.1.1,find local best
v3.1.1,find local best split for larger leaf
v3.1.1,sync global best info
v3.1.1,copy back
v3.1.1,set the global number of data for leaves
v3.1.1,init the global sumup info
v3.1.1,"instantiate template classes, otherwise linker cannot find the code"
v3.1.1,launch cuda kernel
v3.1.1,initialize SerialTreeLearner
v3.1.1,some additional variables needed for GPU trainer
v3.1.1,Initialize GPU buffers and kernels: get device info
v3.1.1,some functions used for debugging the GPU histogram construction
v3.1.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.1.1,also guarantee that there are at least 2K examples per workgroup
v3.1.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.1.1,decide the best number of workgroups working on one feature4 tuple
v3.1.1,set work group size based on feature size
v3.1.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.1.1,set thread_data
v3.1.1,copy the results asynchronously. Size depends on if double precision is used
v3.1.1,"when the output is ready, the computation is done"
v3.1.1,how many feature-group tuples we have
v3.1.1,leave some safe margin for prefetching
v3.1.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.1.1,clear sparse/dense maps
v3.1.1,do nothing it there is no dense feature
v3.1.1,calculate number of feature groups per gpu
v3.1.1,histogram bin entry size depends on the precision (single/double)
v3.1.1,allocate GPU memory for each GPU
v3.1.1,do nothing it there is no gpu feature
v3.1.1,allocate memory for all features
v3.1.1,allocate space for gradients and hessians on device
v3.1.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.1.1,copy indices to the device
v3.1.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.1.1,each work group generates a sub-histogram of dword_features_ features.
v3.1.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.1.1,create atomic counters for inter-group coordination
v3.1.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.1.1,clear sparse/dense maps
v3.1.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.1.1,set device info
v3.1.1,looking for dword_features_ non-sparse feature-groups
v3.1.1,reset device info
v3.1.1,InitGPU w/ num_gpu
v3.1.1,"Get the max bin size, used for selecting best GPU kernel"
v3.1.1,get num_dense_feature_groups_
v3.1.1,initialize GPU
v3.1.1,set cpu threads
v3.1.1,resize device memory pointers
v3.1.1,create stream & events to handle multiple GPUs
v3.1.1,check data size
v3.1.1,GPU memory has to been reallocated because data may have been changed
v3.1.1,AllocateGPUMemory only when the number of data increased
v3.1.1,setup GPU kernel arguments after we allocating all the buffers
v3.1.1,Copy initial full hessians and gradients to GPU.
v3.1.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.1.1,use bagging
v3.1.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.1.1,copy used gradients and hessians to ordered buffer
v3.1.1,transfer the indices to GPU
v3.1.1,only have root
v3.1.1,"Copy indices, gradients and hessians as early as possible"
v3.1.1,only need to initialize for smaller leaf
v3.1.1,Get leaf boundary
v3.1.1,do nothing if no features can be processed on GPU
v3.1.1,copy data indices if it is not null
v3.1.1,converted indices in is_feature_used to feature-group indices
v3.1.1,construct the feature masks for dense feature-groups
v3.1.1,"if no feature group is used, just return and do not use GPU"
v3.1.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.1.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.1.1,We now copy even if all features are used.
v3.1.1,"All data have been prepared, now run the GPU kernel"
v3.1.1,construct smaller leaf
v3.1.1,Check workgroups per feature4 tuple..
v3.1.1,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.1.1,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.1.1,then construct sparse features on CPU
v3.1.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.1.1,"wait for GPU to finish, only if GPU is actually used"
v3.1.1,use double precision
v3.1.1,use single precision
v3.1.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.1.1,#define CUDA_DEBUG_COMPARE
v3.1.1,construct larger leaf
v3.1.1,then construct sparse features on CPU
v3.1.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.1.1,"wait for GPU to finish, only if GPU is actually used"
v3.1.1,use double precision
v3.1.1,use single precision
v3.1.1,do some sanity check for the GPU algorithm
v3.1.0,coding: utf-8
v3.1.0,coding: utf-8
v3.1.0,create predictor first
v3.1.0,check dataset
v3.1.0,reduce cost for prediction training data
v3.1.0,process callbacks
v3.1.0,Most of legacy advanced options becomes callbacks
v3.1.0,construct booster
v3.1.0,start training
v3.1.0,check evaluation result.
v3.1.0,"ranking task, split according to groups"
v3.1.0,run preprocessing on the data set if needed
v3.1.0,setup callbacks
v3.1.0,coding: utf-8
v3.1.0,"simplejson does not support Python 3.2, it throws a SyntaxError"
v3.1.0,because of u'...' Unicode literals.
v3.1.0,dummy function to support older version of scikit-learn
v3.1.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.1.0,coding: utf-8
v3.1.0,"user can set verbose with kwargs, it has higher priority"
v3.1.0,Do not modify original args in fit function
v3.1.0,Refer to https://github.com/microsoft/LightGBM/pull/2619
v3.1.0,Separate built-in from callable evaluation metrics
v3.1.0,register default metric for consistency with callable eval_metric case
v3.1.0,try to deduce from class instance
v3.1.0,overwrite default metric by explicitly set metric
v3.1.0,concatenate metric from params (or default if not provided in params) and eval_metric
v3.1.0,copy for consistency
v3.1.0,reduce cost for prediction training data
v3.1.0,free dataset
v3.1.0,Switch to using a multiclass objective in the underlying LGBM instance
v3.1.0,"do not modify args, as it causes errors in model selection tools"
v3.1.0,check group data
v3.1.0,coding: utf-8
v3.1.0,we don't need lib_lightgbm while building docs
v3.1.0,coding: utf-8
v3.1.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.1.0,coding: utf-8
v3.1.0,REMOVEME: remove warning after 3.1.0 version release
v3.1.0,coding: utf-8
v3.1.0,TypeError: obj is not a string or a number
v3.1.0,ValueError: invalid literal
v3.1.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.1.0,avoid memory consumption by arrays concatenation operations
v3.1.0,create numpy array from output arrays
v3.1.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.1.0,for CSC there is extra column added
v3.1.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.1.0,same shape as input csr or csc matrix except extra column for expected value
v3.1.0,note: make sure we copy data as it will be deallocated next
v3.1.0,"free the temporary native indptr, indices, and data"
v3.1.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.1.0,avoid memory consumption by arrays concatenation operations
v3.1.0,"no min_data, nthreads and verbose in this function"
v3.1.0,check data has header or not
v3.1.0,need to regroup init_score
v3.1.0,process for args
v3.1.0,"user can set verbose with params, it has higher priority"
v3.1.0,get categorical features
v3.1.0,process for reference dataset
v3.1.0,start construct data
v3.1.0,set feature names
v3.1.0,create valid
v3.1.0,construct subset
v3.1.0,create train
v3.1.0,could be updated if data is not freed
v3.1.0,set to None
v3.1.0,we're done if self and reference share a common upstrem reference
v3.1.0,"group data from LightGBM is boundaries data, need to convert to group size"
v3.1.0,"user can set verbose with params, it has higher priority"
v3.1.0,Training task
v3.1.0,set network if necessary
v3.1.0,construct booster object
v3.1.0,copy the parameters from train_set
v3.1.0,save reference to data
v3.1.0,buffer for inner predict
v3.1.0,Prediction task
v3.1.0,if a single node tree it won't have `leaf_index` so return 0
v3.1.0,"Create the node record, and populate universal data members"
v3.1.0,Update values to reflect node type (leaf or split)
v3.1.0,traverse the next level of the tree
v3.1.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.1.0,and we add node to the list.
v3.1.0,need reset training data
v3.1.0,need to push new valid data
v3.1.0,"if buffer length is not long enough, re-allocate a buffer"
v3.1.0,"if buffer length is not long enough, reallocate a buffer"
v3.1.0,Copy models
v3.1.0,Get name of features
v3.1.0,avoid to predict many time in one iteration
v3.1.0,Get num of inner evals
v3.1.0,Get name of evals
v3.1.0,coding: utf-8
v3.1.0,Callback environment used by callbacks
v3.1.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.1.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.1.0,coding: utf-8
v3.1.0,load or create your dataset
v3.1.0,create dataset for lightgbm
v3.1.0,"if you want to re-use data, remember to set free_raw_data=False"
v3.1.0,specify your configurations as a dict
v3.1.0,generate feature names
v3.1.0,feature_name and categorical_feature
v3.1.0,check feature name
v3.1.0,save model to file
v3.1.0,dump model to JSON (and save to file)
v3.1.0,feature names
v3.1.0,feature importances
v3.1.0,load model to predict
v3.1.0,can only predict with the best iteration (or the saving iteration)
v3.1.0,eval with loaded model
v3.1.0,dump model with pickle
v3.1.0,load model with pickle to predict
v3.1.0,can predict with any iteration when loaded in pickle way
v3.1.0,eval with loaded model
v3.1.0,continue training
v3.1.0,init_model accepts:
v3.1.0,1. model file name
v3.1.0,2. Booster()
v3.1.0,decay learning rates
v3.1.0,learning_rates accepts:
v3.1.0,1. list/tuple with length = num_boost_round
v3.1.0,2. function(curr_iter)
v3.1.0,change other parameters during training
v3.1.0,self-defined objective function
v3.1.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.1.0,log likelihood loss
v3.1.0,self-defined eval metric
v3.1.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.0,binary error
v3.1.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.1.0,This may make built-in evalution metric calculate wrong results
v3.1.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.1.0,Keep this in mind when you use the customization
v3.1.0,another self-defined eval metric
v3.1.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.0,accuracy
v3.1.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.1.0,This may make built-in evalution metric calculate wrong results
v3.1.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.1.0,Keep this in mind when you use the customization
v3.1.0,callback
v3.1.0,coding: utf-8
v3.1.0,load or create your dataset
v3.1.0,train
v3.1.0,predict
v3.1.0,eval
v3.1.0,feature importances
v3.1.0,self-defined eval metric
v3.1.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.0,Root Mean Squared Logarithmic Error (RMSLE)
v3.1.0,train
v3.1.0,another self-defined eval metric
v3.1.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.1.0,Relative Absolute Error (RAE)
v3.1.0,train
v3.1.0,predict
v3.1.0,eval
v3.1.0,other scikit-learn modules
v3.1.0,coding: utf-8
v3.1.0,load or create your dataset
v3.1.0,create dataset for lightgbm
v3.1.0,specify your configurations as a dict
v3.1.0,train
v3.1.0,coding: utf-8
v3.1.0,################
v3.1.0,Simulate some binary data with a single categorical and
v3.1.0,single continuous predictor
v3.1.0,################
v3.1.0,Set up a couple of utilities for our experiments
v3.1.0,################
v3.1.0,Observe the behavior of `binary` and `xentropy` objectives
v3.1.0,Trying this throws an error on non-binary values of y:
v3.1.0,"experiment('binary', label_type='probability', DATA)"
v3.1.0,The speed of `binary` is not drastically different than
v3.1.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.1.0,there are reasons to suspect that `binary` should run faster when the
v3.1.0,label is an integer instead of a float
v3.1.0,coding: utf-8
v3.1.0,load or create your dataset
v3.1.0,create dataset for lightgbm
v3.1.0,specify your configurations as a dict
v3.1.0,train
v3.1.0,save model to file
v3.1.0,predict
v3.1.0,eval
v3.1.0,coding: utf-8
v3.1.0,!/usr/bin/env python3
v3.1.0,-*- coding: utf-8 -*-
v3.1.0,
v3.1.0,"LightGBM documentation build configuration file, created by"
v3.1.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.1.0,
v3.1.0,This file is execfile()d with the current directory set to its
v3.1.0,containing dir.
v3.1.0,
v3.1.0,Note that not all possible configuration values are present in this
v3.1.0,autogenerated file.
v3.1.0,
v3.1.0,All configuration values have a default; values that are commented out
v3.1.0,serve to show the default.
v3.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.1.0,add these directories to sys.path here. If the directory is relative to the
v3.1.0,"documentation root, use os.path.abspath to make it absolute."
v3.1.0,-- mock out modules
v3.1.0,-- General configuration ------------------------------------------------
v3.1.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.1.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.1.0,ones.
v3.1.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.1.0,Only the class' docstring is inserted.
v3.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.1.0,The master toctree document.
v3.1.0,General information about the project.
v3.1.0,The name of an image file (relative to this directory) to place at the top
v3.1.0,of the sidebar.
v3.1.0,The name of an image file (relative to this directory) to use as a favicon of
v3.1.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.1.0,pixels large.
v3.1.0,"The version info for the project you're documenting, acts as replacement for"
v3.1.0,"|version| and |release|, also used in various other places throughout the"
v3.1.0,built documents.
v3.1.0,The short X.Y version.
v3.1.0,"The full version, including alpha/beta/rc tags."
v3.1.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.1.0,for a list of supported languages.
v3.1.0,
v3.1.0,This is also used if you do content translation via gettext catalogs.
v3.1.0,"Usually you set ""language"" from the command line for these cases."
v3.1.0,"List of patterns, relative to source directory, that match files and"
v3.1.0,directories to ignore when looking for source files.
v3.1.0,This patterns also effect to html_static_path and html_extra_path
v3.1.0,The name of the Pygments (syntax highlighting) style to use.
v3.1.0,-- Configuration for C API docs generation ------------------------------
v3.1.0,-- Options for HTML output ----------------------------------------------
v3.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.1.0,a list of builtin themes.
v3.1.0,Theme options are theme-specific and customize the look and feel of a theme
v3.1.0,"further.  For a list of options available for each theme, see the"
v3.1.0,documentation.
v3.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.1.0,"relative to this directory. They are copied after the builtin static files,"
v3.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.1.0,-- Options for HTMLHelp output ------------------------------------------
v3.1.0,Output file base name for HTML help builder.
v3.1.0,-- Options for LaTeX output ---------------------------------------------
v3.1.0,The name of an image file (relative to this directory) to place at the top of
v3.1.0,the title page.
v3.1.0,Warning! The following code can cause buffer overflows on RTD.
v3.1.0,Consider suppressing output completely if RTD project silently fails.
v3.1.0,Refer to https://github.com/svenevs/exhale
v3.1.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.1.0,Warning! The following code can cause buffer overflows on RTD.
v3.1.0,Consider suppressing output completely if RTD project silently fails.
v3.1.0,Refer to https://github.com/svenevs/exhale
v3.1.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.1.0,coding: utf-8
v3.1.0,coding: utf-8
v3.1.0,we don't need lib_lightgbm while building docs
v3.1.0,coding: utf-8
v3.1.0,check saved model persistence
v3.1.0,"we need to check the consistency of model file here, so test for exact equal"
v3.1.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.1.0,"scores likely to be different, but prediction should still be the same"
v3.1.0,test that shape is checked during prediction
v3.1.0,test that method works even with free_raw_data=True
v3.1.0,test that method works but sets raw data to None in case of immergeable data types
v3.1.0,test that method works for different data types
v3.1.0,"Set extremely harsh penalties, so CEGB will block most splits."
v3.1.0,"Compare pairs of penalties, to ensure scaling works as intended"
v3.1.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.1.0,coding: utf-8
v3.1.0,coding: utf-8
v3.1.0,prediction result is actually not transformed (is raw) due to custom objective
v3.1.0,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.1.0,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.1.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.1.0,sklearn < 0.23 does not have as_frame parameter
v3.1.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.1.0,sklearn < 0.23 does not have as_frame parameter
v3.1.0,Test if random_state is properly stored
v3.1.0,Test if two random states produce identical models
v3.1.0,Test if subsequent fits sample from random_state object and produce different models
v3.1.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.1.0,With default params
v3.1.0,Tests same probabilities
v3.1.0,Tests same predictions
v3.1.0,Tests same raw scores
v3.1.0,Tests same leaf indices
v3.1.0,Tests same feature contributions
v3.1.0,Tests other parameters for the prediction works
v3.1.0,Tests start_iteration
v3.1.0,"Tests same probabilities, starting from iteration 10"
v3.1.0,"Tests same predictions, starting from iteration 10"
v3.1.0,"Tests same raw scores, starting from iteration 10"
v3.1.0,"Tests same leaf indices, starting from iteration 10"
v3.1.0,"Tests same feature contributions, starting from iteration 10"
v3.1.0,"Tests other parameters for the prediction works, starting from iteration 10"
v3.1.0,"no custom objective, no custom metric"
v3.1.0,default metric
v3.1.0,non-default metric
v3.1.0,no metric
v3.1.0,non-default metric in eval_metric
v3.1.0,non-default metric with non-default metric in eval_metric
v3.1.0,non-default metric with multiple metrics in eval_metric
v3.1.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.1.0,default metric for non-default objective
v3.1.0,non-default metric for non-default objective
v3.1.0,no metric
v3.1.0,non-default metric in eval_metric for non-default objective
v3.1.0,non-default metric with non-default metric in eval_metric for non-default objective
v3.1.0,non-default metric with multiple metrics in eval_metric for non-default objective
v3.1.0,"custom objective, no custom metric"
v3.1.0,default regression metric for custom objective
v3.1.0,non-default regression metric for custom objective
v3.1.0,multiple regression metrics for custom objective
v3.1.0,no metric
v3.1.0,default regression metric with non-default metric in eval_metric for custom objective
v3.1.0,non-default regression metric with metric in eval_metric for custom objective
v3.1.0,multiple regression metrics with metric in eval_metric for custom objective
v3.1.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.1.0,"no custom objective, custom metric"
v3.1.0,default metric with custom metric
v3.1.0,non-default metric with custom metric
v3.1.0,multiple metrics with custom metric
v3.1.0,custom metric (disable default metric)
v3.1.0,default metric for non-default objective with custom metric
v3.1.0,non-default metric for non-default objective with custom metric
v3.1.0,multiple metrics for non-default objective with custom metric
v3.1.0,custom metric (disable default metric for non-default objective)
v3.1.0,"custom objective, custom metric"
v3.1.0,custom metric for custom objective
v3.1.0,non-default regression metric with custom metric for custom objective
v3.1.0,multiple regression metrics with custom metric for custom objective
v3.1.0,default metric and invalid binary metric is replaced with multiclass alternative
v3.1.0,invalid objective is replaced with default multiclass one
v3.1.0,and invalid binary metric is replaced with multiclass alternative
v3.1.0,default metric for non-default multiclass objective
v3.1.0,and invalid binary metric is replaced with multiclass alternative
v3.1.0,default metric and invalid multiclass metric is replaced with binary alternative
v3.1.0,invalid multiclass metric is replaced with binary alternative for custom objective
v3.1.0,"Verify that can receive a list of metrics, only callable"
v3.1.0,Verify that can receive a list of custom and built-in metrics
v3.1.0,Verify that works as expected when eval_metric is empty
v3.1.0,"Verify that can receive a list of metrics, only built-in"
v3.1.0,Verify that eval_metric is robust to receiving a list with None
v3.1.0,training data as eval_set
v3.1.0,feval
v3.1.0,single eval_set
v3.1.0,two eval_set
v3.1.0,"sklearn < 0.22 requires passing ""attributes"" argument"
v3.1.0,Test that estimators are default-constructible
v3.1.0,coding: utf-8
v3.1.0,coding: utf-8
v3.1.0,check that default gives same result as k = 1
v3.1.0,check against independent calculation for k = 1
v3.1.0,check against independent calculation for k = 2
v3.1.0,check against independent calculation for k = 10
v3.1.0,check cases where predictions are equal
v3.1.0,should give same result as binary auc for 2 classes
v3.1.0,test the case where all predictions are equal
v3.1.0,test that weighted data gives different auc_mu
v3.1.0,test that equal data weights give same auc_mu as unweighted data
v3.1.0,should give 1 when accuracy = 1
v3.1.0,test loading class weights
v3.1.0,no early stopping
v3.1.0,early stopping occurs
v3.1.0,test custom eval metrics
v3.1.0,"shuffle = False, override metric in params"
v3.1.0,"shuffle = True, callbacks"
v3.1.0,enable display training loss
v3.1.0,self defined folds
v3.1.0,lambdarank
v3.1.0,... with l2 metric
v3.1.0,... with NDCG (default) metric
v3.1.0,self defined folds with lambdarank
v3.1.0,with early stopping
v3.1.0,predict by each fold booster
v3.1.0,fold averaging
v3.1.0,without early stopping
v3.1.0,test feature_names with whitespaces
v3.1.0,This has non-ascii strings.
v3.1.0,take subsets and train
v3.1.0,generate CSR sparse dataset
v3.1.0,convert data to dense and get back same contribs
v3.1.0,validate the values are the same
v3.1.0,validate using CSC matrix
v3.1.0,validate the values are the same
v3.1.0,generate CSR sparse dataset
v3.1.0,convert data to dense and get back same contribs
v3.1.0,validate the values are the same
v3.1.0,validate using CSC matrix
v3.1.0,validate the values are the same
v3.1.0,Note there is an extra column added to the output for the expected value
v3.1.0,Note output CSC shape should be same as CSR output shape
v3.1.0,test sliced labels
v3.1.0,append some columns
v3.1.0,append some rows
v3.1.0,test sliced 2d matrix
v3.1.0,test sliced CSR
v3.1.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.1.0,The penalization is so high that the first 2 features should not be used here
v3.1.0,Check that a very high penalization is the same as not using the features at all
v3.1.0,"no fobj, no feval"
v3.1.0,default metric
v3.1.0,non-default metric in params
v3.1.0,default metric in args
v3.1.0,non-default metric in args
v3.1.0,metric in args overwrites one in params
v3.1.0,multiple metrics in params
v3.1.0,multiple metrics in args
v3.1.0,remove default metric by 'None' in list
v3.1.0,remove default metric by 'None' aliases
v3.1.0,"fobj, no feval"
v3.1.0,no default metric
v3.1.0,metric in params
v3.1.0,metric in args
v3.1.0,metric in args overwrites its' alias in params
v3.1.0,multiple metrics in params
v3.1.0,multiple metrics in args
v3.1.0,"no fobj, feval"
v3.1.0,default metric with custom one
v3.1.0,non-default metric in params with custom one
v3.1.0,default metric in args with custom one
v3.1.0,non-default metric in args with custom one
v3.1.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.1.0,multiple metrics in params with custom one
v3.1.0,multiple metrics in args with custom one
v3.1.0,custom metric is evaluated despite 'None' is passed
v3.1.0,"fobj, feval"
v3.1.0,"no default metric, only custom one"
v3.1.0,metric in params with custom one
v3.1.0,metric in args with custom one
v3.1.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.1.0,multiple metrics in params with custom one
v3.1.0,multiple metrics in args with custom one
v3.1.0,custom metric is evaluated despite 'None' is passed
v3.1.0,"no fobj, no feval"
v3.1.0,default metric
v3.1.0,default metric in params
v3.1.0,non-default metric in params
v3.1.0,multiple metrics in params
v3.1.0,remove default metric by 'None' aliases
v3.1.0,"fobj, no feval"
v3.1.0,no default metric
v3.1.0,metric in params
v3.1.0,multiple metrics in params
v3.1.0,"no fobj, feval"
v3.1.0,default metric with custom one
v3.1.0,default metric in params with custom one
v3.1.0,non-default metric in params with custom one
v3.1.0,multiple metrics in params with custom one
v3.1.0,custom metric is evaluated despite 'None' is passed
v3.1.0,"fobj, feval"
v3.1.0,"no default metric, only custom one"
v3.1.0,metric in params with custom one
v3.1.0,multiple metrics in params with custom one
v3.1.0,custom metric is evaluated despite 'None' is passed
v3.1.0,multiclass default metric
v3.1.0,multiclass default metric with custom one
v3.1.0,multiclass metric alias with custom one for custom objective
v3.1.0,no metric for invalid class_num
v3.1.0,custom metric for invalid class_num
v3.1.0,multiclass metric alias with custom one with invalid class_num
v3.1.0,multiclass default metric without num_class
v3.1.0,multiclass metric alias
v3.1.0,multiclass metric
v3.1.0,non-valid metric for multiclass objective
v3.1.0,non-default num_class for default objective
v3.1.0,no metric with non-default num_class for custom objective
v3.1.0,multiclass metric alias for custom objective
v3.1.0,multiclass metric for custom objective
v3.1.0,binary metric with non-default num_class for custom objective
v3.1.0,Expect three metrics but mean and stdv for each metric
v3.1.0,test XGBoost-style return value
v3.1.0,test numpy-style return value
v3.1.0,test bins string type
v3.1.0,test histogram is disabled for categorical features
v3.1.0,test for lgb.train
v3.1.0,test feval for lgb.train
v3.1.0,test with two valid data for lgb.train
v3.1.0,test for lgb.cv
v3.1.0,test feval for lgb.cv
v3.1.0,test that binning works properly for features with only positive or only negative values
v3.1.0,decreasing without freeing raw data is allowed
v3.1.0,decreasing before lazy init is allowed
v3.1.0,increasing is allowed
v3.1.0,decreasing with disabled filter is allowed
v3.1.0,decreasing with enabled filter is disallowed;
v3.1.0,also changes of other params are disallowed
v3.1.0,check extra trees increases regularization
v3.1.0,check path smoothing increases regularization
v3.1.0,test edge case with one leaf
v3.1.0,check that constraint containing all features is equivalent to no constraint
v3.1.0,check that constraint partitioning the features reduces train accuracy
v3.1.0,check that constraints consisting of single features reduce accuracy further
v3.1.0,test that interaction constraints work when not all features are used
v3.1.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.1.0,"test the case where start_iteration <= 0, and num_iteration is None"
v3.1.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.1.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.1.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.1.0,test for regression
v3.1.0,test both with and without early stopping
v3.1.0,test for multi-class
v3.1.0,test both with and without early stopping
v3.1.0,test for binary
v3.1.0,test both with and without early stopping
v3.1.0,test against sklearn average precision metric
v3.1.0,test that average precision is 1 where model predicts perfectly
v3.1.0,coding: utf-8
v3.1.0,convert from one-based to  zero-based index
v3.1.0,convert from boundaries to size
v3.1.0,--- start Booster interfaces
v3.1.0,.Call() calls
v3.1.0,coding: utf-8
v3.1.0,alias table
v3.1.0,names
v3.1.0,from strings
v3.1.0,tails
v3.1.0,tails
v3.1.0,coding: utf-8
v3.1.0,Single row predictor to abstract away caching logic
v3.1.0,create boosting
v3.1.0,initialize the boosting
v3.1.0,create objective function
v3.1.0,initialize the objective function
v3.1.0,create training metric
v3.1.0,reset the boosting
v3.1.0,create objective function
v3.1.0,initialize the objective function
v3.1.0,calculate the nonzero data and indices size
v3.1.0,allocate data and indices arrays
v3.1.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.1.0,aggregated per row feature contribution results
v3.1.0,keep track of the row_vector sizes for parallelization
v3.1.0,copy vector results to output for each row
v3.1.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.1.0,aggregated per row feature contribution results
v3.1.0,calculate number of elements per column to construct
v3.1.0,the CSC matrix with random access
v3.1.0,keep track of column counts
v3.1.0,keep track of beginning index for each column
v3.1.0,keep track of beginning index for each matrix
v3.1.0,Note: we parallelize across matrices instead of rows because of the column_counts[m][col_idx] increment inside the loop
v3.1.0,store the row index
v3.1.0,update column count
v3.1.0,explicitly declare symbols from LightGBM namespace
v3.1.0,some help functions used to convert data
v3.1.0,Row iterator of on column for CSC matrix
v3.1.0,"return value at idx, only can access by ascent order"
v3.1.0,"return next non-zero pair, if index < 0, means no more data"
v3.1.0,start of c_api functions
v3.1.0,sample data first
v3.1.0,sample data first
v3.1.0,sample data first
v3.1.0,local buffer to re-use memory
v3.1.0,sample data first
v3.1.0,no more data
v3.1.0,---- start of booster
v3.1.0,Single row in row-major format:
v3.1.0,---- start of some help functions
v3.1.0,data is array of pointers to individual rows
v3.1.0,set number of threads for openmp
v3.1.0,check for alias
v3.1.0,read parameters from config file
v3.1.0,"remove str after ""#"""
v3.1.0,check for alias again
v3.1.0,load configs
v3.1.0,prediction is needed if using input initial model(continued train)
v3.1.0,need to continue training
v3.1.0,sync up random seed for data partition
v3.1.0,load Training data
v3.1.0,load data for parallel training
v3.1.0,load data for single machine
v3.1.0,need save binary file
v3.1.0,create training metric
v3.1.0,only when have metrics then need to construct validation data
v3.1.0,"Add validation data, if it exists"
v3.1.0,add
v3.1.0,need save binary file
v3.1.0,add metric for validation data
v3.1.0,output used time on each iteration
v3.1.0,need init network
v3.1.0,create boosting
v3.1.0,create objective function
v3.1.0,load training data
v3.1.0,initialize the objective function
v3.1.0,initialize the boosting
v3.1.0,add validation data into boosting
v3.1.0,convert model to if-else statement code
v3.1.0,create predictor
v3.1.0,Free memory
v3.1.0,create predictor
v3.1.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.1.0,counts for all labels
v3.1.0,"start from top label, and accumulate DCG"
v3.1.0,counts for all labels
v3.1.0,calculate k Max DCG by one pass
v3.1.0,get sorted indices by score
v3.1.0,calculate dcg
v3.1.0,get sorted indices by score
v3.1.0,calculate multi dcg by one pass
v3.1.0,wait for all client start up
v3.1.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.1.0,Instead we will handle finalize/abort for MPI in main().
v3.1.0,default set to -1
v3.1.0,"distance at k-th communication, distance[k] = 2^k"
v3.1.0,set incoming rank at k-th commuication
v3.1.0,set outgoing rank at k-th commuication
v3.1.0,defalut set as -1
v3.1.0,construct all recursive halving map for all machines
v3.1.0,let 1 << k <= num_machines
v3.1.0,distance of each communication
v3.1.0,"if num_machines = 2^k, don't need to group machines"
v3.1.0,"communication direction, %2 == 0 is positive"
v3.1.0,neighbor at k-th communication
v3.1.0,receive data block at k-th communication
v3.1.0,send data block at k-th communication
v3.1.0,"if num_machines != 2^k, need to group machines"
v3.1.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.1.0,let left machine as group leader
v3.1.0,"cache block information for groups, group with 2 machines will have double block size"
v3.1.0,convert from group to node leader
v3.1.0,convert from node to group
v3.1.0,meet new group
v3.1.0,add block len for this group
v3.1.0,calculate the group block start
v3.1.0,not need to construct
v3.1.0,get receive block informations
v3.1.0,accumulate block len
v3.1.0,get send block informations
v3.1.0,accumulate block len
v3.1.0,static member definition
v3.1.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.1.0,assign the blocks to every rank.
v3.1.0,do reduce scatter
v3.1.0,do all gather
v3.1.0,assign blocks
v3.1.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.1.0,copy back
v3.1.0,assign blocks
v3.1.0,start all gather
v3.1.0,when num_machines is small and data is large
v3.1.0,use output as receive buffer
v3.1.0,get current local block size
v3.1.0,get out rank
v3.1.0,get in rank
v3.1.0,get send information
v3.1.0,get recv information
v3.1.0,send and recv at same time
v3.1.0,rotate in-place
v3.1.0,use output as receive buffer
v3.1.0,get current local block size
v3.1.0,get send information
v3.1.0,get recv information
v3.1.0,send and recv at same time
v3.1.0,use output as receive buffer
v3.1.0,send and recv at same time
v3.1.0,send local data to neighbor first
v3.1.0,receive neighbor data first
v3.1.0,reduce
v3.1.0,get target
v3.1.0,get send information
v3.1.0,get recv information
v3.1.0,send and recv at same time
v3.1.0,reduce
v3.1.0,send result to neighbor
v3.1.0,receive result from neighbor
v3.1.0,copy result
v3.1.0,start up socket
v3.1.0,parse clients from file
v3.1.0,get ip list of local machine
v3.1.0,get local rank
v3.1.0,construct listener
v3.1.0,construct communication topo
v3.1.0,construct linkers
v3.1.0,free listener
v3.1.0,set timeout
v3.1.0,accept incoming socket
v3.1.0,receive rank
v3.1.0,add new socket
v3.1.0,save ranks that need to connect with
v3.1.0,start listener
v3.1.0,start connect
v3.1.0,let smaller rank connect to larger rank
v3.1.0,send local rank
v3.1.0,wait for listener
v3.1.0,print connected linkers
v3.1.0,only need to copy subset
v3.1.0,avoid to copy subset many times
v3.1.0,avoid out of range
v3.1.0,may need to recopy subset
v3.1.0,valid the type
v3.1.0,Constructors
v3.1.0,Get type tag
v3.1.0,Comparisons
v3.1.0,"This has to be separate, not in Statics, because Json() accesses"
v3.1.0,statics().null.
v3.1.0,"advance until next line, or end of input"
v3.1.0,advance until closing tokens
v3.1.0,The usual case: non-escaped characters
v3.1.0,Handle escapes
v3.1.0,Extract 4-byte escape sequence
v3.1.0,Explicitly check length of the substring. The following loop
v3.1.0,relies on std::string returning the terminating NUL when
v3.1.0,accessing str[length]. Checking here reduces brittleness.
v3.1.0,JSON specifies that characters outside the BMP shall be encoded as a
v3.1.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.1.0,components. Check whether we're in the middle of such a beast: the
v3.1.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.1.0,a trail (low) surrogate.
v3.1.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.1.0,per the UTF-16 algorithm.
v3.1.0,Integer part
v3.1.0,Decimal part
v3.1.0,Exponent part
v3.1.0,Check for any trailing garbage
v3.1.0,Documented in json11.hpp
v3.1.0,Check for another object
v3.1.0,get column names
v3.1.0,load label idx first
v3.1.0,erase label column name
v3.1.0,load ignore columns
v3.1.0,load weight idx
v3.1.0,load group idx
v3.1.0,don't support query id in data file when training in parallel
v3.1.0,read data to memory
v3.1.0,sample data
v3.1.0,construct feature bin mappers
v3.1.0,initialize label
v3.1.0,extract features
v3.1.0,sample data from file
v3.1.0,construct feature bin mappers
v3.1.0,initialize label
v3.1.0,extract features
v3.1.0,load data from binary file
v3.1.0,check meta data
v3.1.0,need to check training data
v3.1.0,read data in memory
v3.1.0,initialize label
v3.1.0,extract features
v3.1.0,Get number of lines of data file
v3.1.0,initialize label
v3.1.0,extract features
v3.1.0,load data from binary file
v3.1.0,not need to check validation data
v3.1.0,check meta data
v3.1.0,buffer to read binary file
v3.1.0,check token
v3.1.0,read size of header
v3.1.0,re-allocmate space if not enough
v3.1.0,read header
v3.1.0,get header
v3.1.0,num_groups
v3.1.0,real_feature_idx_
v3.1.0,feature2group
v3.1.0,feature2subfeature
v3.1.0,group_bin_boundaries
v3.1.0,group_feature_start_
v3.1.0,group_feature_cnt_
v3.1.0,get feature names
v3.1.0,write feature names
v3.1.0,get forced_bin_bounds_
v3.1.0,read size of meta data
v3.1.0,re-allocate space if not enough
v3.1.0,read meta data
v3.1.0,load meta data
v3.1.0,sample local used data if need to partition
v3.1.0,"if not contain query file, minimal sample unit is one record"
v3.1.0,"if contain query file, minimal sample unit is one query"
v3.1.0,if is new query
v3.1.0,read feature data
v3.1.0,read feature size
v3.1.0,re-allocate space if not enough
v3.1.0,fill feature_names_ if not header
v3.1.0,get forced split
v3.1.0,"if only one machine, find bin locally"
v3.1.0,"if have multi-machines, need to find bin distributed"
v3.1.0,different machines will find bin for different features
v3.1.0,start and len will store the process feature indices for different machines
v3.1.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.1.0,free
v3.1.0,gather global feature bin mappers
v3.1.0,restore features bins from buffer
v3.1.0,---- private functions ----
v3.1.0,"if features are ordered, not need to use hist_buf"
v3.1.0,read all lines
v3.1.0,get query data
v3.1.0,"if not contain query data, minimal sample unit is one record"
v3.1.0,"if contain query data, minimal sample unit is one query"
v3.1.0,if is new query
v3.1.0,get query data
v3.1.0,"if not contain query file, minimal sample unit is one record"
v3.1.0,"if contain query file, minimal sample unit is one query"
v3.1.0,if is new query
v3.1.0,parse features
v3.1.0,get forced split
v3.1.0,"check the range of label_idx, weight_idx and group_idx"
v3.1.0,fill feature_names_ if not header
v3.1.0,start find bins
v3.1.0,"if only one machine, find bin locally"
v3.1.0,start and len will store the process feature indices for different machines
v3.1.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.1.0,free
v3.1.0,gather global feature bin mappers
v3.1.0,restore features bins from buffer
v3.1.0,if doesn't need to prediction with initial model
v3.1.0,parser
v3.1.0,set label
v3.1.0,free processed line:
v3.1.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.1.0,text_reader_->Lines()[i].shrink_to_fit();
v3.1.0,push data
v3.1.0,if is used feature
v3.1.0,if need to prediction with initial model
v3.1.0,parser
v3.1.0,set initial score
v3.1.0,set label
v3.1.0,free processed line:
v3.1.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.1.0,text_reader_->Lines()[i].shrink_to_fit();
v3.1.0,push data
v3.1.0,if is used feature
v3.1.0,metadata_ will manage space of init_score
v3.1.0,text data can be free after loaded feature values
v3.1.0,parser
v3.1.0,set initial score
v3.1.0,set label
v3.1.0,push data
v3.1.0,if is used feature
v3.1.0,only need part of data
v3.1.0,need full data
v3.1.0,metadata_ will manage space of init_score
v3.1.0,read size of token
v3.1.0,remove duplicates
v3.1.0,deep copy function for BinMapper
v3.1.0,mean size for one bin
v3.1.0,need a new bin
v3.1.0,update bin upper bound
v3.1.0,last bin upper bound
v3.1.0,get list of distinct values
v3.1.0,get number of positive and negative distinct values
v3.1.0,include zero bounds and infinity bound
v3.1.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.1.0,find remaining bounds
v3.1.0,find distinct_values first
v3.1.0,push zero in the front
v3.1.0,use the large value
v3.1.0,push zero in the back
v3.1.0,convert to int type first
v3.1.0,sort by counts
v3.1.0,will ignore the categorical of small counts
v3.1.0,Push the dummy bin for NaN
v3.1.0,Use MissingType::None to represent this bin contains all categoricals
v3.1.0,fix count of NaN bin
v3.1.0,check trivial(num_bin_ == 1) feature
v3.1.0,check useless bin
v3.1.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.1.0,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.1.0,calculate max bin of all features to select the int type in MultiValDenseBin
v3.1.0,"for lambdarank, it needs query data for partition data in parallel learning"
v3.1.0,need convert query_id to boundaries
v3.1.0,check weights
v3.1.0,check query boundries
v3.1.0,contain initial score file
v3.1.0,check weights
v3.1.0,get local weights
v3.1.0,check query boundries
v3.1.0,get local query boundaries
v3.1.0,contain initial score file
v3.1.0,get local initial scores
v3.1.0,re-load query weight
v3.1.0,save to nullptr
v3.1.0,save to nullptr
v3.1.0,save to nullptr
v3.1.0,default weight file name
v3.1.0,default init_score file name
v3.1.0,use first line to count number class
v3.1.0,default query file name
v3.1.0,root is in the depth 0
v3.1.0,non-leaf
v3.1.0,leaf
v3.1.0,use this for the missing value conversion
v3.1.0,Predict func by Map to ifelse
v3.1.0,use this for the missing value conversion
v3.1.0,non-leaf
v3.1.0,left subtree
v3.1.0,right subtree
v3.1.0,leaf
v3.1.0,non-leaf
v3.1.0,left subtree
v3.1.0,right subtree
v3.1.0,leaf
v3.1.0,recursive computation of SHAP values for a decision tree
v3.1.0,extend the unique path
v3.1.0,leaf node
v3.1.0,internal node
v3.1.0,"see if we have already split on this feature,"
v3.1.0,if so we undo that split so we can redo it for this node
v3.1.0,recursive sparse computation of SHAP values for a decision tree
v3.1.0,extend the unique path
v3.1.0,leaf node
v3.1.0,internal node
v3.1.0,"see if we have already split on this feature,"
v3.1.0,if so we undo that split so we can redo it for this node
v3.1.0,add names of objective function if not providing metric
v3.1.0,equal weights for all classes
v3.1.0,generate seeds by seed.
v3.1.0,sort eval_at
v3.1.0,Only push the non-training data
v3.1.0,check for conflicts
v3.1.0,"check if objective, metric, and num_class match"
v3.1.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.1.0,Check max_depth and num_leaves
v3.1.0,"Fits in an int, and is more restrictive than the current num_leaves"
v3.1.0,force col-wise for gpu & CUDA
v3.1.0,force gpu_use_dp for CUDA
v3.1.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.1.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.1.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.1.0,calculated split gain can be positive even with zero gradient and hessian.
v3.1.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.1.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.1.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.1.0,first round: fill the single val group
v3.1.0,always push the last group
v3.1.0,put dense feature first
v3.1.0,sort by non zero cnt
v3.1.0,"sort by non zero cnt, bigger first"
v3.1.0,shuffle groups
v3.1.0,Using std::swap for vector<bool> will cause the wrong result.
v3.1.0,get num_features
v3.1.0,get bin_mappers
v3.1.0,"for sparse multi value bin, we store the feature bin values with offset added"
v3.1.0,"for dense multi value bin, the feature bin values without offsets are used"
v3.1.0,copy feature bin mapper data
v3.1.0,"if not pass a filename, just append "".bin"" of original file"
v3.1.0,get size of header
v3.1.0,size of feature names
v3.1.0,size of forced bins
v3.1.0,write header
v3.1.0,write feature names
v3.1.0,write forced bins
v3.1.0,get size of meta data
v3.1.0,write meta data
v3.1.0,write feature data
v3.1.0,get size of feature
v3.1.0,write feature
v3.1.0,"explicitly initilize template methods, for cross module call"
v3.1.0,"Only one multi-val group, just simply merge"
v3.1.0,Skip the leading 0 when copying group_bin_boundaries.
v3.1.0,regenerate other fields
v3.1.0,store the importance first
v3.1.0,PredictRaw
v3.1.0,PredictRawByMap
v3.1.0,Predict
v3.1.0,PredictByMap
v3.1.0,PredictLeafIndex
v3.1.0,PredictLeafIndexByMap
v3.1.0,output model type
v3.1.0,output number of class
v3.1.0,output label index
v3.1.0,output max_feature_idx
v3.1.0,output objective
v3.1.0,output tree models
v3.1.0,store the importance first
v3.1.0,sort the importance
v3.1.0,use serialized string to restore this object
v3.1.0,Use first 128 chars to avoid exceed the message buffer.
v3.1.0,get number of classes
v3.1.0,get index of label
v3.1.0,get max_feature_idx first
v3.1.0,get average_output
v3.1.0,get feature names
v3.1.0,get monotone_constraints
v3.1.0,set zero
v3.1.0,predict all the trees for one iteration
v3.1.0,check early stopping
v3.1.0,set zero
v3.1.0,predict all the trees for one iteration
v3.1.0,check early stopping
v3.1.0,margin_threshold will be captured by value
v3.1.0,copy and sort
v3.1.0,margin_threshold will be captured by value
v3.1.0,Fix for compiler warnings about reaching end of control
v3.1.0,load forced_splits file
v3.1.0,init tree learner
v3.1.0,push training metrics
v3.1.0,create buffer for gradients and hessians
v3.1.0,get max feature index
v3.1.0,get label index
v3.1.0,get feature names
v3.1.0,"if need bagging, create buffer"
v3.1.0,"for a validation dataset, we need its score and metric"
v3.1.0,update score
v3.1.0,objective function will calculate gradients and hessians
v3.1.0,"random bagging, minimal unit is one record"
v3.1.0,"random bagging, minimal unit is one record"
v3.1.0,if need bagging
v3.1.0,set bagging data to tree learner
v3.1.0,get subset
v3.1.0,output used time per iteration
v3.1.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.1.0,boosting first
v3.1.0,bagging logic
v3.1.0,need to copy gradients for bagging subset.
v3.1.0,shrinkage by learning rate
v3.1.0,update score
v3.1.0,only add default score one-time
v3.1.0,updates scores
v3.1.0,add model
v3.1.0,reset score
v3.1.0,remove model
v3.1.0,print message for metric
v3.1.0,pop last early_stopping_round_ models
v3.1.0,update training score
v3.1.0,we need to predict out-of-bag scores of data for boosting
v3.1.0,update validation score
v3.1.0,print training metric
v3.1.0,print validation metric
v3.1.0,set zero
v3.1.0,predict all the trees for one iteration
v3.1.0,predict all the trees for one iteration
v3.1.0,push training metrics
v3.1.0,"not same training data, need reset score and others"
v3.1.0,create score tracker
v3.1.0,update score
v3.1.0,create buffer for gradients and hessians
v3.1.0,load forced_splits file
v3.1.0,"if need bagging, create buffer"
v3.1.0,Get the max size of pool
v3.1.0,at least need 2 leaves
v3.1.0,push split information for all leaves
v3.1.0,initialize splits for leaf
v3.1.0,initialize data partition
v3.1.0,initialize ordered gradients and hessians
v3.1.0,cannot change is_hist_col_wise during training
v3.1.0,initialize splits for leaf
v3.1.0,initialize data partition
v3.1.0,initialize ordered gradients and hessians
v3.1.0,Get the max size of pool
v3.1.0,at least need 2 leaves
v3.1.0,push split information for all leaves
v3.1.0,some initial works before training
v3.1.0,root leaf
v3.1.0,only root leaf can be splitted on first time
v3.1.0,some initial works before finding best split
v3.1.0,find best threshold for every feature
v3.1.0,Get a leaf with max split gain
v3.1.0,Get split information for best leaf
v3.1.0,"cannot split, quit"
v3.1.0,split tree with best leaf
v3.1.0,reset histogram pool
v3.1.0,initialize data partition
v3.1.0,reset the splits for leaves
v3.1.0,Sumup for root
v3.1.0,use all data
v3.1.0,"use bagging, only use part of data"
v3.1.0,check depth of current leaf
v3.1.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.1.0,no enough data to continue
v3.1.0,only have root
v3.1.0,put parent(left) leaf's histograms into larger leaf's histograms
v3.1.0,put parent(left) leaf's histograms to larger leaf's histograms
v3.1.0,construct smaller leaf
v3.1.0,construct larger leaf
v3.1.0,find splits
v3.1.0,only has root leaf
v3.1.0,start at root leaf
v3.1.0,"before processing next node from queue, store info for current left/right leaf"
v3.1.0,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.1.0,"then, compute own splits"
v3.1.0,split info should exist because searching in bfs fashion - should have added from parent
v3.1.0,update before tree split
v3.1.0,don't need to update this in data-based parallel model
v3.1.0,"split tree, will return right leaf"
v3.1.0,store the true split gain in tree model
v3.1.0,don't need to update this in data-based parallel model
v3.1.0,store the true split gain in tree model
v3.1.0,init the leaves that used on next iteration
v3.1.0,update leave outputs if needed
v3.1.0,bag_mapper[index_mapper[i]]
v3.1.0,it is needed to filter the features after the above code.
v3.1.0,"Otherwise, the `is_splittable` in `FeatureHistogram` will be wrong, and cause some features being accidentally filtered in the later nodes."
v3.1.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.1.0,can't use GetParentOutput because leaf_splits doesn't have weight property set
v3.1.0,find splits
v3.1.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.1.0,get feature partition
v3.1.0,get local used features
v3.1.0,get best split at smaller leaf
v3.1.0,find local best split for larger leaf
v3.1.0,sync global best info
v3.1.0,update best split
v3.1.0,"instantiate template classes, otherwise linker cannot find the code"
v3.1.0,initialize SerialTreeLearner
v3.1.0,Get local rank and global machine size
v3.1.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.1.0,allocate buffer for communication
v3.1.0,generate feature partition for current tree
v3.1.0,get local used feature
v3.1.0,get block start and block len for reduce scatter
v3.1.0,get buffer_write_start_pos_
v3.1.0,get buffer_read_start_pos_
v3.1.0,sync global data sumup info
v3.1.0,global sumup reduce
v3.1.0,copy back
v3.1.0,set global sumup info
v3.1.0,init global data count in leaf
v3.1.0,construct local histograms
v3.1.0,copy to buffer
v3.1.0,Reduce scatter for histogram
v3.1.0,restore global histograms from buffer
v3.1.0,only root leaf
v3.1.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.1.0,find local best split for larger leaf
v3.1.0,sync global best info
v3.1.0,set best split
v3.1.0,need update global number of data in leaf
v3.1.0,"instantiate template classes, otherwise linker cannot find the code"
v3.1.0,initialize SerialTreeLearner
v3.1.0,some additional variables needed for GPU trainer
v3.1.0,Initialize GPU buffers and kernels
v3.1.0,some functions used for debugging the GPU histogram construction
v3.1.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.1.0,goto err;
v3.1.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.1.0,also guarantee that there are at least 2K examples per workgroup
v3.1.0,return 0;
v3.1.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.1.0,decide the best number of workgroups working on one feature4 tuple
v3.1.0,set work group size based on feature size
v3.1.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.1.0,we need to refresh the kernel arguments after reallocating
v3.1.0,The only argument that needs to be changed later is num_data_
v3.1.0,"the GPU kernel will process all features in one call, and each"
v3.1.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.1.0,process one feature4 tuple
v3.1.0,"for the root node, indices are not copied"
v3.1.0,"for constant hessian, hessians are not copied except for the root node"
v3.1.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.1.0,and we will launch num_feature workgroups for this kernel
v3.1.0,will launch threads for all features
v3.1.0,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.1.0,copy the results asynchronously. Size depends on if double precision is used
v3.1.0,we will wait for this object in WaitAndGetHistograms
v3.1.0,"when the output is ready, the computation is done"
v3.1.0,values of this feature has been redistributed to multiple bins; need a reduction here
v3.1.0,how many feature-group tuples we have
v3.1.0,leave some safe margin for prefetching
v3.1.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.1.0,clear sparse/dense maps
v3.1.0,do nothing if no features can be processed on GPU
v3.1.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.1.0,unpin old buffer if necessary before destructing them
v3.1.0,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v3.1.0,allocate space for gradients and hessians on device
v3.1.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.1.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.1.0,copy indices to the device
v3.1.0,histogram bin entry size depends on the precision (single/double)
v3.1.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.1.0,each work group generates a sub-histogram of dword_features_ features.
v3.1.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.1.0,create atomic counters for inter-group coordination
v3.1.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.1.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.1.0,looking for dword_features_ non-sparse feature-groups
v3.1.0,decide if we need to redistribute the bin
v3.1.0,multiplier must be a power of 2
v3.1.0,device_bin_mults_.push_back(1);
v3.1.0,found
v3.1.0,for data transfer time
v3.1.0,"Now generate new data structure feature4, and copy data to the device"
v3.1.0,"preallocate arrays for all threads, and pin them"
v3.1.0,building Feature4 bundles; each thread handles dword_features_ features
v3.1.0,one feature datapoint is 4 bits
v3.1.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.1.0,one feature datapoint is one byte
v3.1.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.1.0,Dense bin
v3.1.0,Dense 4-bit bin
v3.1.0,working on the remaining (less than dword_features_) feature groups
v3.1.0,fill the leftover features
v3.1.0,"fill this empty feature with some ""random"" value"
v3.1.0,"fill this empty feature with some ""random"" value"
v3.1.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.1.0,deallocate pinned space for feature copying
v3.1.0,data transfer time
v3.1.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.1.0,"Something bad happened. Just return ""No log available."""
v3.1.0,"build is okay, log may contain warnings"
v3.1.0,destroy any old kernels
v3.1.0,create OpenCL kernels for different number of workgroups per feature
v3.1.0,currently we don't use constant memory
v3.1.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.1.0,kernel with indices in an array
v3.1.0,"kernel with all features enabled, with elimited branches"
v3.1.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.1.0,do nothing if no features can be processed on GPU
v3.1.0,The only argument that needs to be changed later is num_data_
v3.1.0,"hessian is passed as a parameter, but it is not available now."
v3.1.0,hessian will be set in BeforeTrain()
v3.1.0,"Get the max bin size, used for selecting best GPU kernel"
v3.1.0,initialize GPU
v3.1.0,determine which kernel to use based on the max number of bins
v3.1.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.1.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.1.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.1.0,setup GPU kernel arguments after we allocating all the buffers
v3.1.0,GPU memory has to been reallocated because data may have been changed
v3.1.0,setup GPU kernel arguments after we allocating all the buffers
v3.1.0,Copy initial full hessians and gradients to GPU.
v3.1.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.1.0,setup hessian parameters only
v3.1.0,hessian is passed as a parameter
v3.1.0,use bagging
v3.1.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.1.0,copy used gradients and hessians to ordered buffer
v3.1.0,transfer the indices to GPU
v3.1.0,transfer hessian to GPU
v3.1.0,setup hessian parameters only
v3.1.0,hessian is passed as a parameter
v3.1.0,transfer gradients to GPU
v3.1.0,only have root
v3.1.0,"Copy indices, gradients and hessians as early as possible"
v3.1.0,only need to initialize for smaller leaf
v3.1.0,Get leaf boundary
v3.1.0,copy indices to the GPU:
v3.1.0,copy ordered hessians to the GPU:
v3.1.0,copy ordered gradients to the GPU:
v3.1.0,do nothing if no features can be processed on GPU
v3.1.0,copy data indices if it is not null
v3.1.0,generate and copy ordered_gradients if gradients is not null
v3.1.0,generate and copy ordered_hessians if hessians is not null
v3.1.0,converted indices in is_feature_used to feature-group indices
v3.1.0,construct the feature masks for dense feature-groups
v3.1.0,"if no feature group is used, just return and do not use GPU"
v3.1.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.1.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.1.0,"All data have been prepared, now run the GPU kernel"
v3.1.0,construct smaller leaf
v3.1.0,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v3.1.0,then construct sparse features on CPU
v3.1.0,"wait for GPU to finish, only if GPU is actually used"
v3.1.0,use double precision
v3.1.0,use single precision
v3.1.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.1.0,#define GPU_DEBUG_COMPARE
v3.1.0,construct larger leaf
v3.1.0,then construct sparse features on CPU
v3.1.0,"wait for GPU to finish, only if GPU is actually used"
v3.1.0,use double precision
v3.1.0,use single precision
v3.1.0,do some sanity check for the GPU algorithm
v3.1.0,limit top k
v3.1.0,get max bin
v3.1.0,calculate buffer size
v3.1.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.1.0,"left and right on same time, so need double size"
v3.1.0,initialize histograms for global
v3.1.0,sync global data sumup info
v3.1.0,set global sumup info
v3.1.0,init global data count in leaf
v3.1.0,get local sumup
v3.1.0,get local sumup
v3.1.0,get mean number on machines
v3.1.0,weighted gain
v3.1.0,get top k
v3.1.0,"Copy histogram to buffer, and Get local aggregate features"
v3.1.0,copy histograms.
v3.1.0,copy smaller leaf histograms first
v3.1.0,mark local aggregated feature
v3.1.0,copy
v3.1.0,then copy larger leaf histograms
v3.1.0,mark local aggregated feature
v3.1.0,copy
v3.1.0,use local data to find local best splits
v3.1.0,find splits
v3.1.0,only has root leaf
v3.1.0,local voting
v3.1.0,gather
v3.1.0,get all top-k from all machines
v3.1.0,global voting
v3.1.0,copy local histgrams to buffer
v3.1.0,Reduce scatter for histogram
v3.1.0,find best split from local aggregated histograms
v3.1.0,restore from buffer
v3.1.0,restore from buffer
v3.1.0,find local best
v3.1.0,find local best split for larger leaf
v3.1.0,sync global best info
v3.1.0,copy back
v3.1.0,set the global number of data for leaves
v3.1.0,init the global sumup info
v3.1.0,"instantiate template classes, otherwise linker cannot find the code"
v3.1.0,launch cuda kernel
v3.1.0,initialize SerialTreeLearner
v3.1.0,some additional variables needed for GPU trainer
v3.1.0,Initialize GPU buffers and kernels: get device info
v3.1.0,some functions used for debugging the GPU histogram construction
v3.1.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.1.0,also guarantee that there are at least 2K examples per workgroup
v3.1.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.1.0,decide the best number of workgroups working on one feature4 tuple
v3.1.0,set work group size based on feature size
v3.1.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.1.0,set thread_data
v3.1.0,copy the results asynchronously. Size depends on if double precision is used
v3.1.0,"when the output is ready, the computation is done"
v3.1.0,how many feature-group tuples we have
v3.1.0,leave some safe margin for prefetching
v3.1.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.1.0,clear sparse/dense maps
v3.1.0,do nothing it there is no dense feature
v3.1.0,calculate number of feature groups per gpu
v3.1.0,histogram bin entry size depends on the precision (single/double)
v3.1.0,allocate GPU memory for each GPU
v3.1.0,do nothing it there is no gpu feature
v3.1.0,allocate memory for all features
v3.1.0,allocate space for gradients and hessians on device
v3.1.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.1.0,copy indices to the device
v3.1.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.1.0,each work group generates a sub-histogram of dword_features_ features.
v3.1.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.1.0,create atomic counters for inter-group coordination
v3.1.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.1.0,clear sparse/dense maps
v3.1.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.1.0,set device info
v3.1.0,looking for dword_features_ non-sparse feature-groups
v3.1.0,reset device info
v3.1.0,InitGPU w/ num_gpu
v3.1.0,"Get the max bin size, used for selecting best GPU kernel"
v3.1.0,get num_dense_feature_groups_
v3.1.0,initialize GPU
v3.1.0,set cpu threads
v3.1.0,resize device memory pointers
v3.1.0,create stream & events to handle multiple GPUs
v3.1.0,check data size
v3.1.0,GPU memory has to been reallocated because data may have been changed
v3.1.0,AllocateGPUMemory only when the number of data increased
v3.1.0,setup GPU kernel arguments after we allocating all the buffers
v3.1.0,Copy initial full hessians and gradients to GPU.
v3.1.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.1.0,use bagging
v3.1.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.1.0,copy used gradients and hessians to ordered buffer
v3.1.0,transfer the indices to GPU
v3.1.0,only have root
v3.1.0,"Copy indices, gradients and hessians as early as possible"
v3.1.0,only need to initialize for smaller leaf
v3.1.0,Get leaf boundary
v3.1.0,do nothing if no features can be processed on GPU
v3.1.0,copy data indices if it is not null
v3.1.0,converted indices in is_feature_used to feature-group indices
v3.1.0,construct the feature masks for dense feature-groups
v3.1.0,"if no feature group is used, just return and do not use GPU"
v3.1.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.1.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.1.0,We now copy even if all features are used.
v3.1.0,"All data have been prepared, now run the GPU kernel"
v3.1.0,construct smaller leaf
v3.1.0,Check workgroups per feature4 tuple..
v3.1.0,"if the workgroup per feature is 1 (2^0), return as the work is too small for a GPU"
v3.1.0,ConstructGPUHistogramsAsync will return true if there are availabe feature groups dispatched to GPU
v3.1.0,then construct sparse features on CPU
v3.1.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.1.0,"wait for GPU to finish, only if GPU is actually used"
v3.1.0,use double precision
v3.1.0,use single precision
v3.1.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.1.0,#define CUDA_DEBUG_COMPARE
v3.1.0,construct larger leaf
v3.1.0,then construct sparse features on CPU
v3.1.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v3.1.0,"wait for GPU to finish, only if GPU is actually used"
v3.1.0,use double precision
v3.1.0,use single precision
v3.1.0,do some sanity check for the GPU algorithm
v3.0.0,coding: utf-8
v3.0.0,coding: utf-8
v3.0.0,create predictor first
v3.0.0,check dataset
v3.0.0,reduce cost for prediction training data
v3.0.0,process callbacks
v3.0.0,Most of legacy advanced options becomes callbacks
v3.0.0,construct booster
v3.0.0,start training
v3.0.0,check evaluation result.
v3.0.0,"ranking task, split according to groups"
v3.0.0,run preprocessing on the data set if needed
v3.0.0,setup callbacks
v3.0.0,coding: utf-8
v3.0.0,"simplejson does not support Python 3.2, it throws a SyntaxError"
v3.0.0,because of u'...' Unicode literals.
v3.0.0,dummy function to support older version of scikit-learn
v3.0.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.0.0,coding: utf-8
v3.0.0,"user can set verbose with kwargs, it has higher priority"
v3.0.0,register default metric for consistency with callable eval_metric case
v3.0.0,try to deduce from class instance
v3.0.0,overwrite default metric by explicitly set metric
v3.0.0,concatenate metric from params (or default if not provided in params) and eval_metric
v3.0.0,copy for consistency
v3.0.0,reduce cost for prediction training data
v3.0.0,free dataset
v3.0.0,Switch to using a multiclass objective in the underlying LGBM instance
v3.0.0,"do not modify args, as it causes errors in model selection tools"
v3.0.0,check group data
v3.0.0,coding: utf-8
v3.0.0,we don't need lib_lightgbm while building docs
v3.0.0,coding: utf-8
v3.0.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.0.0,coding: utf-8
v3.0.0,coding: utf-8
v3.0.0,TypeError: obj is not a string or a number
v3.0.0,ValueError: invalid literal
v3.0.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.0.0,avoid memory consumption by arrays concatenation operations
v3.0.0,create numpy array from output arrays
v3.0.0,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.0.0,for CSC there is extra column added
v3.0.0,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.0.0,same shape as input csr or csc matrix except extra column for expected value
v3.0.0,note: make sure we copy data as it will be deallocated next
v3.0.0,"free the temporary native indptr, indices, and data"
v3.0.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.0.0,avoid memory consumption by arrays concatenation operations
v3.0.0,"no min_data, nthreads and verbose in this function"
v3.0.0,check data has header or not
v3.0.0,need to regroup init_score
v3.0.0,process for args
v3.0.0,"user can set verbose with params, it has higher priority"
v3.0.0,get categorical features
v3.0.0,process for reference dataset
v3.0.0,start construct data
v3.0.0,set feature names
v3.0.0,create valid
v3.0.0,construct subset
v3.0.0,create train
v3.0.0,could be updated if data is not freed
v3.0.0,set to None
v3.0.0,we're done if self and reference share a common upstrem reference
v3.0.0,"group data from LightGBM is boundaries data, need to convert to group size"
v3.0.0,"user can set verbose with params, it has higher priority"
v3.0.0,Training task
v3.0.0,set network if necessary
v3.0.0,construct booster object
v3.0.0,copy the parameters from train_set
v3.0.0,save reference to data
v3.0.0,buffer for inner predict
v3.0.0,Prediction task
v3.0.0,if a single node tree it won't have `leaf_index` so return 0
v3.0.0,"Create the node record, and populate universal data members"
v3.0.0,Update values to reflect node type (leaf or split)
v3.0.0,traverse the next level of the tree
v3.0.0,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.0.0,and we add node to the list.
v3.0.0,need reset training data
v3.0.0,need to push new valid data
v3.0.0,"if buffer length is not long enough, re-allocate a buffer"
v3.0.0,"if buffer length is not long enough, reallocate a buffer"
v3.0.0,Copy models
v3.0.0,Get name of features
v3.0.0,avoid to predict many time in one iteration
v3.0.0,Get num of inner evals
v3.0.0,Get name of evals
v3.0.0,coding: utf-8
v3.0.0,Callback environment used by callbacks
v3.0.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.0.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.0.0,coding: utf-8
v3.0.0,load or create your dataset
v3.0.0,create dataset for lightgbm
v3.0.0,"if you want to re-use data, remember to set free_raw_data=False"
v3.0.0,specify your configurations as a dict
v3.0.0,generate feature names
v3.0.0,feature_name and categorical_feature
v3.0.0,check feature name
v3.0.0,save model to file
v3.0.0,dump model to JSON (and save to file)
v3.0.0,feature names
v3.0.0,feature importances
v3.0.0,load model to predict
v3.0.0,can only predict with the best iteration (or the saving iteration)
v3.0.0,eval with loaded model
v3.0.0,dump model with pickle
v3.0.0,load model with pickle to predict
v3.0.0,can predict with any iteration when loaded in pickle way
v3.0.0,eval with loaded model
v3.0.0,continue training
v3.0.0,init_model accepts:
v3.0.0,1. model file name
v3.0.0,2. Booster()
v3.0.0,decay learning rates
v3.0.0,learning_rates accepts:
v3.0.0,1. list/tuple with length = num_boost_round
v3.0.0,2. function(curr_iter)
v3.0.0,change other parameters during training
v3.0.0,self-defined objective function
v3.0.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.0.0,log likelihood loss
v3.0.0,self-defined eval metric
v3.0.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0,binary error
v3.0.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.0.0,This may make built-in evalution metric calculate wrong results
v3.0.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.0.0,Keep this in mind when you use the customization
v3.0.0,another self-defined eval metric
v3.0.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0,accuracy
v3.0.0,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.0.0,This may make built-in evalution metric calculate wrong results
v3.0.0,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.0.0,Keep this in mind when you use the customization
v3.0.0,callback
v3.0.0,coding: utf-8
v3.0.0,load or create your dataset
v3.0.0,train
v3.0.0,predict
v3.0.0,eval
v3.0.0,feature importances
v3.0.0,self-defined eval metric
v3.0.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0,Root Mean Squared Logarithmic Error (RMSLE)
v3.0.0,train
v3.0.0,another self-defined eval metric
v3.0.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0,Relative Absolute Error (RAE)
v3.0.0,train
v3.0.0,predict
v3.0.0,eval
v3.0.0,other scikit-learn modules
v3.0.0,coding: utf-8
v3.0.0,load or create your dataset
v3.0.0,create dataset for lightgbm
v3.0.0,specify your configurations as a dict
v3.0.0,train
v3.0.0,coding: utf-8
v3.0.0,################
v3.0.0,Simulate some binary data with a single categorical and
v3.0.0,single continuous predictor
v3.0.0,################
v3.0.0,Set up a couple of utilities for our experiments
v3.0.0,################
v3.0.0,Observe the behavior of `binary` and `xentropy` objectives
v3.0.0,Trying this throws an error on non-binary values of y:
v3.0.0,"experiment('binary', label_type='probability', DATA)"
v3.0.0,The speed of `binary` is not drastically different than
v3.0.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.0.0,there are reasons to suspect that `binary` should run faster when the
v3.0.0,label is an integer instead of a float
v3.0.0,coding: utf-8
v3.0.0,load or create your dataset
v3.0.0,create dataset for lightgbm
v3.0.0,specify your configurations as a dict
v3.0.0,train
v3.0.0,save model to file
v3.0.0,predict
v3.0.0,eval
v3.0.0,coding: utf-8
v3.0.0,!/usr/bin/env python3
v3.0.0,-*- coding: utf-8 -*-
v3.0.0,
v3.0.0,"LightGBM documentation build configuration file, created by"
v3.0.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.0.0,
v3.0.0,This file is execfile()d with the current directory set to its
v3.0.0,containing dir.
v3.0.0,
v3.0.0,Note that not all possible configuration values are present in this
v3.0.0,autogenerated file.
v3.0.0,
v3.0.0,All configuration values have a default; values that are commented out
v3.0.0,serve to show the default.
v3.0.0,"If extensions (or modules to document with autodoc) are in another directory,"
v3.0.0,add these directories to sys.path here. If the directory is relative to the
v3.0.0,"documentation root, use os.path.abspath to make it absolute."
v3.0.0,-- mock out modules
v3.0.0,-- General configuration ------------------------------------------------
v3.0.0,"If your documentation needs a minimal Sphinx version, state it here."
v3.0.0,"Add any Sphinx extension module names here, as strings. They can be"
v3.0.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.0.0,ones.
v3.0.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.0.0,Only the class' docstring is inserted.
v3.0.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.0.0,The master toctree document.
v3.0.0,General information about the project.
v3.0.0,The name of an image file (relative to this directory) to place at the top
v3.0.0,of the sidebar.
v3.0.0,The name of an image file (relative to this directory) to use as a favicon of
v3.0.0,the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
v3.0.0,pixels large.
v3.0.0,"The version info for the project you're documenting, acts as replacement for"
v3.0.0,"|version| and |release|, also used in various other places throughout the"
v3.0.0,built documents.
v3.0.0,The short X.Y version.
v3.0.0,"The full version, including alpha/beta/rc tags."
v3.0.0,The language for content autogenerated by Sphinx. Refer to documentation
v3.0.0,for a list of supported languages.
v3.0.0,
v3.0.0,This is also used if you do content translation via gettext catalogs.
v3.0.0,"Usually you set ""language"" from the command line for these cases."
v3.0.0,"List of patterns, relative to source directory, that match files and"
v3.0.0,directories to ignore when looking for source files.
v3.0.0,This patterns also effect to html_static_path and html_extra_path
v3.0.0,The name of the Pygments (syntax highlighting) style to use.
v3.0.0,-- Configuration for C API docs generation ------------------------------
v3.0.0,-- Options for HTML output ----------------------------------------------
v3.0.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.0.0,a list of builtin themes.
v3.0.0,Theme options are theme-specific and customize the look and feel of a theme
v3.0.0,"further.  For a list of options available for each theme, see the"
v3.0.0,documentation.
v3.0.0,"Add any paths that contain custom static files (such as style sheets) here,"
v3.0.0,"relative to this directory. They are copied after the builtin static files,"
v3.0.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.0.0,-- Options for HTMLHelp output ------------------------------------------
v3.0.0,Output file base name for HTML help builder.
v3.0.0,-- Options for LaTeX output ---------------------------------------------
v3.0.0,The name of an image file (relative to this directory) to place at the top of
v3.0.0,the title page.
v3.0.0,Warning! The following code can cause buffer overflows on RTD.
v3.0.0,Consider suppressing output completely if RTD project silently fails.
v3.0.0,Refer to https://github.com/svenevs/exhale
v3.0.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.0.0,Warning! The following code can cause buffer overflows on RTD.
v3.0.0,Consider suppressing output completely if RTD project silently fails.
v3.0.0,Refer to https://github.com/svenevs/exhale
v3.0.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.0.0,coding: utf-8
v3.0.0,coding: utf-8
v3.0.0,we don't need lib_lightgbm while building docs
v3.0.0,coding: utf-8
v3.0.0,check saved model persistence
v3.0.0,"we need to check the consistency of model file here, so test for exact equal"
v3.0.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.0.0,"scores likely to be different, but prediction should still be the same"
v3.0.0,test that shape is checked during prediction
v3.0.0,"Set extremely harsh penalties, so CEGB will block most splits."
v3.0.0,"Compare pairs of penalties, to ensure scaling works as intended"
v3.0.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.0.0,coding: utf-8
v3.0.0,prediction result is actually not transformed (is raw) due to custom objective
v3.0.0,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.0.0,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.0.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.0.0,sklearn < 0.23 does not have as_frame parameter
v3.0.0,sklearn < 0.22 does not have the post fit attribute: classes_
v3.0.0,sklearn < 0.23 does not have as_frame parameter
v3.0.0,Test if random_state is properly stored
v3.0.0,Test if two random states produce identical models
v3.0.0,Test if subsequent fits sample from random_state object and produce different models
v3.0.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.0.0,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v3.0.0,we cannot use `check_estimator` directly since there is no skip test mechanism
v3.0.0,we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)
v3.0.0,skip test because scikit-learn incorrectly asserts that
v3.0.0,private attributes cannot be set in __init__
v3.0.0,(see https://github.com/microsoft/LightGBM/issues/2628)
v3.0.0,With default params
v3.0.0,Tests same probabilities
v3.0.0,Tests same predictions
v3.0.0,Tests same raw scores
v3.0.0,Tests same leaf indices
v3.0.0,Tests same feature contributions
v3.0.0,Tests other parameters for the prediction works
v3.0.0,Tests start_iteration
v3.0.0,"Tests same probabilities, starting from iteration 10"
v3.0.0,"Tests same predictions, starting from iteration 10"
v3.0.0,"Tests same raw scores, starting from iteration 10"
v3.0.0,"Tests same leaf indices, starting from iteration 10"
v3.0.0,"Tests same feature contributions, starting from iteration 10"
v3.0.0,"Tests other parameters for the prediction works, starting from iteration 10"
v3.0.0,"no custom objective, no custom metric"
v3.0.0,default metric
v3.0.0,non-default metric
v3.0.0,no metric
v3.0.0,non-default metric in eval_metric
v3.0.0,non-default metric with non-default metric in eval_metric
v3.0.0,non-default metric with multiple metrics in eval_metric
v3.0.0,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.0.0,default metric for non-default objective
v3.0.0,non-default metric for non-default objective
v3.0.0,no metric
v3.0.0,non-default metric in eval_metric for non-default objective
v3.0.0,non-default metric with non-default metric in eval_metric for non-default objective
v3.0.0,non-default metric with multiple metrics in eval_metric for non-default objective
v3.0.0,"custom objective, no custom metric"
v3.0.0,default regression metric for custom objective
v3.0.0,non-default regression metric for custom objective
v3.0.0,multiple regression metrics for custom objective
v3.0.0,no metric
v3.0.0,default regression metric with non-default metric in eval_metric for custom objective
v3.0.0,non-default regression metric with metric in eval_metric for custom objective
v3.0.0,multiple regression metrics with metric in eval_metric for custom objective
v3.0.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.0.0,"no custom objective, custom metric"
v3.0.0,default metric with custom metric
v3.0.0,non-default metric with custom metric
v3.0.0,multiple metrics with custom metric
v3.0.0,custom metric (disable default metric)
v3.0.0,default metric for non-default objective with custom metric
v3.0.0,non-default metric for non-default objective with custom metric
v3.0.0,multiple metrics for non-default objective with custom metric
v3.0.0,custom metric (disable default metric for non-default objective)
v3.0.0,"custom objective, custom metric"
v3.0.0,custom metric for custom objective
v3.0.0,non-default regression metric with custom metric for custom objective
v3.0.0,multiple regression metrics with custom metric for custom objective
v3.0.0,default metric and invalid binary metric is replaced with multiclass alternative
v3.0.0,invalid objective is replaced with default multiclass one
v3.0.0,and invalid binary metric is replaced with multiclass alternative
v3.0.0,default metric for non-default multiclass objective
v3.0.0,and invalid binary metric is replaced with multiclass alternative
v3.0.0,default metric and invalid multiclass metric is replaced with binary alternative
v3.0.0,invalid multiclass metric is replaced with binary alternative for custom objective
v3.0.0,training data as eval_set
v3.0.0,feval
v3.0.0,single eval_set
v3.0.0,two eval_set
v3.0.0,coding: utf-8
v3.0.0,coding: utf-8
v3.0.0,check that default gives same result as k = 1
v3.0.0,check against independent calculation for k = 1
v3.0.0,check against independent calculation for k = 2
v3.0.0,check against independent calculation for k = 10
v3.0.0,check cases where predictions are equal
v3.0.0,should give same result as binary auc for 2 classes
v3.0.0,test the case where all predictions are equal
v3.0.0,should give 1 when accuracy = 1
v3.0.0,test loading weights
v3.0.0,no early stopping
v3.0.0,early stopping occurs
v3.0.0,test custom eval metrics
v3.0.0,"shuffle = False, override metric in params"
v3.0.0,"shuffle = True, callbacks"
v3.0.0,enable display training loss
v3.0.0,self defined folds
v3.0.0,lambdarank
v3.0.0,... with l2 metric
v3.0.0,... with NDCG (default) metric
v3.0.0,self defined folds with lambdarank
v3.0.0,with early stopping
v3.0.0,predict by each fold booster
v3.0.0,fold averaging
v3.0.0,without early stopping
v3.0.0,test feature_names with whitespaces
v3.0.0,This has non-ascii strings.
v3.0.0,take subsets and train
v3.0.0,generate CSR sparse dataset
v3.0.0,convert data to dense and get back same contribs
v3.0.0,validate the values are the same
v3.0.0,validate using CSC matrix
v3.0.0,validate the values are the same
v3.0.0,Note there is an extra column added to the output for the expected value
v3.0.0,Note output CSC shape should be same as CSR output shape
v3.0.0,test sliced labels
v3.0.0,append some columns
v3.0.0,append some rows
v3.0.0,test sliced 2d matrix
v3.0.0,test sliced CSR
v3.0.0,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.0.0,The penalization is so high that the first 2 features should not be used here
v3.0.0,Check that a very high penalization is the same as not using the features at all
v3.0.0,"no fobj, no feval"
v3.0.0,default metric
v3.0.0,non-default metric in params
v3.0.0,default metric in args
v3.0.0,non-default metric in args
v3.0.0,metric in args overwrites one in params
v3.0.0,multiple metrics in params
v3.0.0,multiple metrics in args
v3.0.0,remove default metric by 'None' in list
v3.0.0,remove default metric by 'None' aliases
v3.0.0,"fobj, no feval"
v3.0.0,no default metric
v3.0.0,metric in params
v3.0.0,metric in args
v3.0.0,metric in args overwrites its' alias in params
v3.0.0,multiple metrics in params
v3.0.0,multiple metrics in args
v3.0.0,"no fobj, feval"
v3.0.0,default metric with custom one
v3.0.0,non-default metric in params with custom one
v3.0.0,default metric in args with custom one
v3.0.0,non-default metric in args with custom one
v3.0.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.0.0,multiple metrics in params with custom one
v3.0.0,multiple metrics in args with custom one
v3.0.0,custom metric is evaluated despite 'None' is passed
v3.0.0,"fobj, feval"
v3.0.0,"no default metric, only custom one"
v3.0.0,metric in params with custom one
v3.0.0,metric in args with custom one
v3.0.0,"metric in args overwrites one in params, custom one is evaluated too"
v3.0.0,multiple metrics in params with custom one
v3.0.0,multiple metrics in args with custom one
v3.0.0,custom metric is evaluated despite 'None' is passed
v3.0.0,"no fobj, no feval"
v3.0.0,default metric
v3.0.0,default metric in params
v3.0.0,non-default metric in params
v3.0.0,multiple metrics in params
v3.0.0,remove default metric by 'None' aliases
v3.0.0,"fobj, no feval"
v3.0.0,no default metric
v3.0.0,metric in params
v3.0.0,multiple metrics in params
v3.0.0,"no fobj, feval"
v3.0.0,default metric with custom one
v3.0.0,default metric in params with custom one
v3.0.0,non-default metric in params with custom one
v3.0.0,multiple metrics in params with custom one
v3.0.0,custom metric is evaluated despite 'None' is passed
v3.0.0,"fobj, feval"
v3.0.0,"no default metric, only custom one"
v3.0.0,metric in params with custom one
v3.0.0,multiple metrics in params with custom one
v3.0.0,custom metric is evaluated despite 'None' is passed
v3.0.0,multiclass default metric
v3.0.0,multiclass default metric with custom one
v3.0.0,multiclass metric alias with custom one for custom objective
v3.0.0,no metric for invalid class_num
v3.0.0,custom metric for invalid class_num
v3.0.0,multiclass metric alias with custom one with invalid class_num
v3.0.0,multiclass default metric without num_class
v3.0.0,multiclass metric alias
v3.0.0,multiclass metric
v3.0.0,non-valid metric for multiclass objective
v3.0.0,non-default num_class for default objective
v3.0.0,no metric with non-default num_class for custom objective
v3.0.0,multiclass metric alias for custom objective
v3.0.0,multiclass metric for custom objective
v3.0.0,binary metric with non-default num_class for custom objective
v3.0.0,test XGBoost-style return value
v3.0.0,test numpy-style return value
v3.0.0,test bins string type
v3.0.0,test histogram is disabled for categorical features
v3.0.0,test for lgb.train
v3.0.0,test feval for lgb.train
v3.0.0,test with two valid data for lgb.train
v3.0.0,test for lgb.cv
v3.0.0,test feval for lgb.cv
v3.0.0,test that binning works properly for features with only positive or only negative values
v3.0.0,decreasing without freeing raw data is allowed
v3.0.0,decreasing before lazy init is allowed
v3.0.0,increasing is allowed
v3.0.0,decreasing with disabled filter is allowed
v3.0.0,decreasing with enabled filter is disallowed;
v3.0.0,also changes of other params are disallowed
v3.0.0,check extra trees increases regularization
v3.0.0,check path smoothing increases regularization
v3.0.0,test edge case with one leaf
v3.0.0,check that constraint containing all features is equivalent to no constraint
v3.0.0,check that constraint partitioning the features reduces train accuracy
v3.0.0,check that constraints consisting of single features reduce accuracy further
v3.0.0,test that interaction constraints work when not all features are used
v3.0.0,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.0.0,"test the case where start_iteration <= 0, and num_iteration is None"
v3.0.0,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.0.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.0.0,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.0.0,test for regression
v3.0.0,test both with and without early stopping
v3.0.0,test for multi-class
v3.0.0,test both with and without early stopping
v3.0.0,test for binary
v3.0.0,test both with and without early stopping
v3.0.0,coding: utf-8
v3.0.0,convert from one-based to  zero-based index
v3.0.0,convert from boundaries to size
v3.0.0,--- start Booster interfaces
v3.0.0,.Call() calls
v3.0.0,coding: utf-8
v3.0.0,alias table
v3.0.0,names
v3.0.0,from strings
v3.0.0,tails
v3.0.0,tails
v3.0.0,coding: utf-8
v3.0.0,Single row predictor to abstract away caching logic
v3.0.0,create boosting
v3.0.0,initialize the boosting
v3.0.0,create objective function
v3.0.0,initialize the objective function
v3.0.0,create training metric
v3.0.0,reset the boosting
v3.0.0,create objective function
v3.0.0,initialize the objective function
v3.0.0,calculate the nonzero data and indices size
v3.0.0,allocate data and indices arrays
v3.0.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.0.0,aggregated per row feature contribution results
v3.0.0,keep track of the row_vector sizes for parallelization
v3.0.0,copy vector results to output for each row
v3.0.0,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.0.0,aggregated per row feature contribution results
v3.0.0,calculate number of elements per column to construct
v3.0.0,the CSC matrix with random access
v3.0.0,keep track of column counts
v3.0.0,keep track of beginning index for each column
v3.0.0,keep track of beginning index for each matrix
v3.0.0,store the row index
v3.0.0,update column count
v3.0.0,explicitly declare symbols from LightGBM namespace
v3.0.0,some help functions used to convert data
v3.0.0,Row iterator of on column for CSC matrix
v3.0.0,"return value at idx, only can access by ascent order"
v3.0.0,"return next non-zero pair, if index < 0, means no more data"
v3.0.0,start of c_api functions
v3.0.0,sample data first
v3.0.0,sample data first
v3.0.0,sample data first
v3.0.0,local buffer to re-use memory
v3.0.0,sample data first
v3.0.0,no more data
v3.0.0,---- start of booster
v3.0.0,Single row in row-major format:
v3.0.0,---- start of some help functions
v3.0.0,data is array of pointers to individual rows
v3.0.0,set number of threads for openmp
v3.0.0,check for alias
v3.0.0,read parameters from config file
v3.0.0,"remove str after ""#"""
v3.0.0,check for alias again
v3.0.0,load configs
v3.0.0,prediction is needed if using input initial model(continued train)
v3.0.0,need to continue training
v3.0.0,sync up random seed for data partition
v3.0.0,load Training data
v3.0.0,load data for parallel training
v3.0.0,load data for single machine
v3.0.0,need save binary file
v3.0.0,create training metric
v3.0.0,only when have metrics then need to construct validation data
v3.0.0,"Add validation data, if it exists"
v3.0.0,add
v3.0.0,need save binary file
v3.0.0,add metric for validation data
v3.0.0,output used time on each iteration
v3.0.0,need init network
v3.0.0,create boosting
v3.0.0,create objective function
v3.0.0,load training data
v3.0.0,initialize the objective function
v3.0.0,initialize the boosting
v3.0.0,add validation data into boosting
v3.0.0,convert model to if-else statement code
v3.0.0,create predictor
v3.0.0,Free memory
v3.0.0,create predictor
v3.0.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.0.0,counts for all labels
v3.0.0,"start from top label, and accumulate DCG"
v3.0.0,counts for all labels
v3.0.0,calculate k Max DCG by one pass
v3.0.0,get sorted indices by score
v3.0.0,calculate dcg
v3.0.0,get sorted indices by score
v3.0.0,calculate multi dcg by one pass
v3.0.0,wait for all client start up
v3.0.0,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.0.0,Instead we will handle finalize/abort for MPI in main().
v3.0.0,default set to -1
v3.0.0,"distance at k-th communication, distance[k] = 2^k"
v3.0.0,set incoming rank at k-th commuication
v3.0.0,set outgoing rank at k-th commuication
v3.0.0,defalut set as -1
v3.0.0,construct all recursive halving map for all machines
v3.0.0,let 1 << k <= num_machines
v3.0.0,distance of each communication
v3.0.0,"if num_machines = 2^k, don't need to group machines"
v3.0.0,"communication direction, %2 == 0 is positive"
v3.0.0,neighbor at k-th communication
v3.0.0,receive data block at k-th communication
v3.0.0,send data block at k-th communication
v3.0.0,"if num_machines != 2^k, need to group machines"
v3.0.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.0.0,let left machine as group leader
v3.0.0,"cache block information for groups, group with 2 machines will have double block size"
v3.0.0,convert from group to node leader
v3.0.0,convert from node to group
v3.0.0,meet new group
v3.0.0,add block len for this group
v3.0.0,calculate the group block start
v3.0.0,not need to construct
v3.0.0,get receive block informations
v3.0.0,accumulate block len
v3.0.0,get send block informations
v3.0.0,accumulate block len
v3.0.0,static member definition
v3.0.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.0.0,assign the blocks to every rank.
v3.0.0,do reduce scatter
v3.0.0,do all gather
v3.0.0,assign blocks
v3.0.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.0.0,copy back
v3.0.0,assign blocks
v3.0.0,start all gather
v3.0.0,when num_machines is small and data is large
v3.0.0,use output as receive buffer
v3.0.0,get current local block size
v3.0.0,get out rank
v3.0.0,get in rank
v3.0.0,get send information
v3.0.0,get recv information
v3.0.0,send and recv at same time
v3.0.0,rotate in-place
v3.0.0,use output as receive buffer
v3.0.0,get current local block size
v3.0.0,get send information
v3.0.0,get recv information
v3.0.0,send and recv at same time
v3.0.0,use output as receive buffer
v3.0.0,send and recv at same time
v3.0.0,send local data to neighbor first
v3.0.0,receive neighbor data first
v3.0.0,reduce
v3.0.0,get target
v3.0.0,get send information
v3.0.0,get recv information
v3.0.0,send and recv at same time
v3.0.0,reduce
v3.0.0,send result to neighbor
v3.0.0,receive result from neighbor
v3.0.0,copy result
v3.0.0,start up socket
v3.0.0,parse clients from file
v3.0.0,get ip list of local machine
v3.0.0,get local rank
v3.0.0,construct listener
v3.0.0,construct communication topo
v3.0.0,construct linkers
v3.0.0,free listener
v3.0.0,set timeout
v3.0.0,accept incoming socket
v3.0.0,receive rank
v3.0.0,add new socket
v3.0.0,save ranks that need to connect with
v3.0.0,start listener
v3.0.0,start connect
v3.0.0,let smaller rank connect to larger rank
v3.0.0,send local rank
v3.0.0,wait for listener
v3.0.0,print connected linkers
v3.0.0,valid the type
v3.0.0,Constructors
v3.0.0,Get type tag
v3.0.0,Comparisons
v3.0.0,"This has to be separate, not in Statics, because Json() accesses"
v3.0.0,statics().null.
v3.0.0,"advance until next line, or end of input"
v3.0.0,advance until closing tokens
v3.0.0,The usual case: non-escaped characters
v3.0.0,Handle escapes
v3.0.0,Extract 4-byte escape sequence
v3.0.0,Explicitly check length of the substring. The following loop
v3.0.0,relies on std::string returning the terminating NUL when
v3.0.0,accessing str[length]. Checking here reduces brittleness.
v3.0.0,JSON specifies that characters outside the BMP shall be encoded as a
v3.0.0,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.0.0,components. Check whether we're in the middle of such a beast: the
v3.0.0,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.0.0,a trail (low) surrogate.
v3.0.0,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.0.0,per the UTF-16 algorithm.
v3.0.0,Integer part
v3.0.0,Decimal part
v3.0.0,Exponent part
v3.0.0,Check for any trailing garbage
v3.0.0,Documented in json11.hpp
v3.0.0,Check for another object
v3.0.0,get column names
v3.0.0,load label idx first
v3.0.0,erase label column name
v3.0.0,load ignore columns
v3.0.0,load weight idx
v3.0.0,load group idx
v3.0.0,don't support query id in data file when training in parallel
v3.0.0,read data to memory
v3.0.0,sample data
v3.0.0,construct feature bin mappers
v3.0.0,initialize label
v3.0.0,extract features
v3.0.0,sample data from file
v3.0.0,construct feature bin mappers
v3.0.0,initialize label
v3.0.0,extract features
v3.0.0,load data from binary file
v3.0.0,check meta data
v3.0.0,need to check training data
v3.0.0,read data in memory
v3.0.0,initialize label
v3.0.0,extract features
v3.0.0,Get number of lines of data file
v3.0.0,initialize label
v3.0.0,extract features
v3.0.0,load data from binary file
v3.0.0,not need to check validation data
v3.0.0,check meta data
v3.0.0,buffer to read binary file
v3.0.0,check token
v3.0.0,read size of header
v3.0.0,re-allocmate space if not enough
v3.0.0,read header
v3.0.0,get header
v3.0.0,num_groups
v3.0.0,real_feature_idx_
v3.0.0,feature2group
v3.0.0,feature2subfeature
v3.0.0,group_bin_boundaries
v3.0.0,group_feature_start_
v3.0.0,group_feature_cnt_
v3.0.0,get feature names
v3.0.0,write feature names
v3.0.0,get forced_bin_bounds_
v3.0.0,read size of meta data
v3.0.0,re-allocate space if not enough
v3.0.0,read meta data
v3.0.0,load meta data
v3.0.0,sample local used data if need to partition
v3.0.0,"if not contain query file, minimal sample unit is one record"
v3.0.0,"if contain query file, minimal sample unit is one query"
v3.0.0,if is new query
v3.0.0,read feature data
v3.0.0,read feature size
v3.0.0,re-allocate space if not enough
v3.0.0,fill feature_names_ if not header
v3.0.0,get forced split
v3.0.0,"if only one machine, find bin locally"
v3.0.0,"if have multi-machines, need to find bin distributed"
v3.0.0,different machines will find bin for different features
v3.0.0,start and len will store the process feature indices for different machines
v3.0.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.0.0,free
v3.0.0,gather global feature bin mappers
v3.0.0,restore features bins from buffer
v3.0.0,---- private functions ----
v3.0.0,"if features are ordered, not need to use hist_buf"
v3.0.0,read all lines
v3.0.0,get query data
v3.0.0,"if not contain query data, minimal sample unit is one record"
v3.0.0,"if contain query data, minimal sample unit is one query"
v3.0.0,if is new query
v3.0.0,get query data
v3.0.0,"if not contain query file, minimal sample unit is one record"
v3.0.0,"if contain query file, minimal sample unit is one query"
v3.0.0,if is new query
v3.0.0,parse features
v3.0.0,get forced split
v3.0.0,"check the range of label_idx, weight_idx and group_idx"
v3.0.0,fill feature_names_ if not header
v3.0.0,start find bins
v3.0.0,"if only one machine, find bin locally"
v3.0.0,start and len will store the process feature indices for different machines
v3.0.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.0.0,free
v3.0.0,gather global feature bin mappers
v3.0.0,restore features bins from buffer
v3.0.0,if doesn't need to prediction with initial model
v3.0.0,parser
v3.0.0,set label
v3.0.0,free processed line:
v3.0.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.0.0,text_reader_->Lines()[i].shrink_to_fit();
v3.0.0,push data
v3.0.0,if is used feature
v3.0.0,if need to prediction with initial model
v3.0.0,parser
v3.0.0,set initial score
v3.0.0,set label
v3.0.0,free processed line:
v3.0.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.0.0,text_reader_->Lines()[i].shrink_to_fit();
v3.0.0,push data
v3.0.0,if is used feature
v3.0.0,metadata_ will manage space of init_score
v3.0.0,text data can be free after loaded feature values
v3.0.0,parser
v3.0.0,set initial score
v3.0.0,set label
v3.0.0,push data
v3.0.0,if is used feature
v3.0.0,only need part of data
v3.0.0,need full data
v3.0.0,metadata_ will manage space of init_score
v3.0.0,read size of token
v3.0.0,remove duplicates
v3.0.0,deep copy function for BinMapper
v3.0.0,mean size for one bin
v3.0.0,need a new bin
v3.0.0,update bin upper bound
v3.0.0,last bin upper bound
v3.0.0,get list of distinct values
v3.0.0,get number of positive and negative distinct values
v3.0.0,include zero bounds and infinity bound
v3.0.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.0.0,find remaining bounds
v3.0.0,find distinct_values first
v3.0.0,push zero in the front
v3.0.0,use the large value
v3.0.0,push zero in the back
v3.0.0,convert to int type first
v3.0.0,sort by counts
v3.0.0,will ignore the categorical of small counts
v3.0.0,Push the dummy bin for NaN
v3.0.0,Use MissingType::None to represent this bin contains all categoricals
v3.0.0,fix count of NaN bin
v3.0.0,check trivial(num_bin_ == 1) feature
v3.0.0,check useless bin
v3.0.0,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.0.0,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.0.0,"for lambdarank, it needs query data for partition data in parallel learning"
v3.0.0,need convert query_id to boundaries
v3.0.0,check weights
v3.0.0,check query boundries
v3.0.0,contain initial score file
v3.0.0,check weights
v3.0.0,get local weights
v3.0.0,check query boundries
v3.0.0,get local query boundaries
v3.0.0,contain initial score file
v3.0.0,get local initial scores
v3.0.0,re-load query weight
v3.0.0,save to nullptr
v3.0.0,save to nullptr
v3.0.0,save to nullptr
v3.0.0,default weight file name
v3.0.0,default init_score file name
v3.0.0,use first line to count number class
v3.0.0,default query file name
v3.0.0,root is in the depth 0
v3.0.0,non-leaf
v3.0.0,leaf
v3.0.0,use this for the missing value conversion
v3.0.0,Predict func by Map to ifelse
v3.0.0,use this for the missing value conversion
v3.0.0,non-leaf
v3.0.0,left subtree
v3.0.0,right subtree
v3.0.0,leaf
v3.0.0,non-leaf
v3.0.0,left subtree
v3.0.0,right subtree
v3.0.0,leaf
v3.0.0,recursive computation of SHAP values for a decision tree
v3.0.0,extend the unique path
v3.0.0,leaf node
v3.0.0,internal node
v3.0.0,"see if we have already split on this feature,"
v3.0.0,if so we undo that split so we can redo it for this node
v3.0.0,recursive sparse computation of SHAP values for a decision tree
v3.0.0,extend the unique path
v3.0.0,leaf node
v3.0.0,internal node
v3.0.0,"see if we have already split on this feature,"
v3.0.0,if so we undo that split so we can redo it for this node
v3.0.0,add names of objective function if not providing metric
v3.0.0,equal weights for all classes
v3.0.0,generate seeds by seed.
v3.0.0,sort eval_at
v3.0.0,Only push the non-training data
v3.0.0,check for conflicts
v3.0.0,"check if objective, metric, and num_class match"
v3.0.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.0.0,Check max_depth and num_leaves
v3.0.0,"Fits in an int, and is more restrictive than the current num_leaves"
v3.0.0,force col-wise for gpu
v3.0.0,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.0.0,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.0.0,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.0.0,calculated split gain can be positive even with zero gradient and hessian.
v3.0.0,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.0.0,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.0.0,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.0.0,first round: fill the single val group
v3.0.0,always push the last group
v3.0.0,put dense feature first
v3.0.0,sort by non zero cnt
v3.0.0,"sort by non zero cnt, bigger first"
v3.0.0,shuffle groups
v3.0.0,Using std::swap for vector<bool> will cause the wrong result.
v3.0.0,get num_features
v3.0.0,get bin_mappers
v3.0.0,copy feature bin mapper data
v3.0.0,copy feature bin mapper data
v3.0.0,"if not pass a filename, just append "".bin"" of original file"
v3.0.0,get size of header
v3.0.0,size of feature names
v3.0.0,size of forced bins
v3.0.0,write header
v3.0.0,write feature names
v3.0.0,write forced bins
v3.0.0,get size of meta data
v3.0.0,write meta data
v3.0.0,write feature data
v3.0.0,get size of feature
v3.0.0,write feature
v3.0.0,only need to copy subset
v3.0.0,avoid to copy subset many times
v3.0.0,avoid out of range
v3.0.0,may need to recopy subset
v3.0.0,"explicitly initilize template methods, for cross module call"
v3.0.0,"FIXME: fix the multiple multi-val feature groups, they need to be merged"
v3.0.0,into one multi-val group
v3.0.0,Skip the leading 0 when copying group_bin_boundaries.
v3.0.0,store the importance first
v3.0.0,PredictRaw
v3.0.0,PredictRawByMap
v3.0.0,Predict
v3.0.0,PredictByMap
v3.0.0,PredictLeafIndex
v3.0.0,PredictLeafIndexByMap
v3.0.0,output model type
v3.0.0,output number of class
v3.0.0,output label index
v3.0.0,output max_feature_idx
v3.0.0,output objective
v3.0.0,output tree models
v3.0.0,store the importance first
v3.0.0,sort the importance
v3.0.0,use serialized string to restore this object
v3.0.0,Use first 128 chars to avoid exceed the message buffer.
v3.0.0,get number of classes
v3.0.0,get index of label
v3.0.0,get max_feature_idx first
v3.0.0,get average_output
v3.0.0,get feature names
v3.0.0,get monotone_constraints
v3.0.0,set zero
v3.0.0,predict all the trees for one iteration
v3.0.0,check early stopping
v3.0.0,set zero
v3.0.0,predict all the trees for one iteration
v3.0.0,check early stopping
v3.0.0,margin_threshold will be captured by value
v3.0.0,copy and sort
v3.0.0,margin_threshold will be captured by value
v3.0.0,Fix for compiler warnings about reaching end of control
v3.0.0,load forced_splits file
v3.0.0,init tree learner
v3.0.0,push training metrics
v3.0.0,create buffer for gradients and hessians
v3.0.0,get max feature index
v3.0.0,get label index
v3.0.0,get feature names
v3.0.0,"if need bagging, create buffer"
v3.0.0,"for a validation dataset, we need its score and metric"
v3.0.0,update score
v3.0.0,objective function will calculate gradients and hessians
v3.0.0,"random bagging, minimal unit is one record"
v3.0.0,"random bagging, minimal unit is one record"
v3.0.0,if need bagging
v3.0.0,set bagging data to tree learner
v3.0.0,get subset
v3.0.0,output used time per iteration
v3.0.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.0.0,boosting first
v3.0.0,bagging logic
v3.0.0,need to copy gradients for bagging subset.
v3.0.0,shrinkage by learning rate
v3.0.0,update score
v3.0.0,only add default score one-time
v3.0.0,updates scores
v3.0.0,add model
v3.0.0,reset score
v3.0.0,remove model
v3.0.0,print message for metric
v3.0.0,pop last early_stopping_round_ models
v3.0.0,update training score
v3.0.0,we need to predict out-of-bag scores of data for boosting
v3.0.0,update validation score
v3.0.0,print training metric
v3.0.0,print validation metric
v3.0.0,set zero
v3.0.0,predict all the trees for one iteration
v3.0.0,predict all the trees for one iteration
v3.0.0,push training metrics
v3.0.0,"not same training data, need reset score and others"
v3.0.0,create score tracker
v3.0.0,update score
v3.0.0,create buffer for gradients and hessians
v3.0.0,load forced_splits file
v3.0.0,"if need bagging, create buffer"
v3.0.0,Get the max size of pool
v3.0.0,at least need 2 leaves
v3.0.0,push split information for all leaves
v3.0.0,initialize splits for leaf
v3.0.0,initialize data partition
v3.0.0,initialize ordered gradients and hessians
v3.0.0,cannot change is_hist_col_wise during training
v3.0.0,initialize splits for leaf
v3.0.0,initialize data partition
v3.0.0,initialize ordered gradients and hessians
v3.0.0,Get the max size of pool
v3.0.0,at least need 2 leaves
v3.0.0,push split information for all leaves
v3.0.0,some initial works before training
v3.0.0,root leaf
v3.0.0,only root leaf can be splitted on first time
v3.0.0,some initial works before finding best split
v3.0.0,find best threshold for every feature
v3.0.0,Get a leaf with max split gain
v3.0.0,Get split information for best leaf
v3.0.0,"cannot split, quit"
v3.0.0,split tree with best leaf
v3.0.0,reset histogram pool
v3.0.0,initialize data partition
v3.0.0,reset the splits for leaves
v3.0.0,Sumup for root
v3.0.0,use all data
v3.0.0,"use bagging, only use part of data"
v3.0.0,check depth of current leaf
v3.0.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.0.0,no enough data to continue
v3.0.0,only have root
v3.0.0,put parent(left) leaf's histograms into larger leaf's histograms
v3.0.0,put parent(left) leaf's histograms to larger leaf's histograms
v3.0.0,construct smaller leaf
v3.0.0,construct larger leaf
v3.0.0,find splits
v3.0.0,only has root leaf
v3.0.0,start at root leaf
v3.0.0,"before processing next node from queue, store info for current left/right leaf"
v3.0.0,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.0.0,"then, compute own splits"
v3.0.0,split info should exist because searching in bfs fashion - should have added from parent
v3.0.0,update before tree split
v3.0.0,don't need to update this in data-based parallel model
v3.0.0,"split tree, will return right leaf"
v3.0.0,store the true split gain in tree model
v3.0.0,don't need to update this in data-based parallel model
v3.0.0,store the true split gain in tree model
v3.0.0,init the leaves that used on next iteration
v3.0.0,update leave outputs if needed
v3.0.0,bag_mapper[index_mapper[i]]
v3.0.0,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.0.0,find splits
v3.0.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.0.0,get feature partition
v3.0.0,get local used features
v3.0.0,get best split at smaller leaf
v3.0.0,find local best split for larger leaf
v3.0.0,sync global best info
v3.0.0,update best split
v3.0.0,"instantiate template classes, otherwise linker cannot find the code"
v3.0.0,initialize SerialTreeLearner
v3.0.0,Get local rank and global machine size
v3.0.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.0.0,allocate buffer for communication
v3.0.0,generate feature partition for current tree
v3.0.0,get local used feature
v3.0.0,get block start and block len for reduce scatter
v3.0.0,get buffer_write_start_pos_
v3.0.0,get buffer_read_start_pos_
v3.0.0,sync global data sumup info
v3.0.0,global sumup reduce
v3.0.0,copy back
v3.0.0,set global sumup info
v3.0.0,init global data count in leaf
v3.0.0,construct local histograms
v3.0.0,copy to buffer
v3.0.0,Reduce scatter for histogram
v3.0.0,restore global histograms from buffer
v3.0.0,only root leaf
v3.0.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.0.0,find local best split for larger leaf
v3.0.0,sync global best info
v3.0.0,set best split
v3.0.0,need update global number of data in leaf
v3.0.0,"instantiate template classes, otherwise linker cannot find the code"
v3.0.0,initialize SerialTreeLearner
v3.0.0,some additional variables needed for GPU trainer
v3.0.0,Initialize GPU buffers and kernels
v3.0.0,some functions used for debugging the GPU histogram construction
v3.0.0,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.0.0,goto err;
v3.0.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.0.0,also guarantee that there are at least 2K examples per workgroup
v3.0.0,return 0;
v3.0.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.0.0,decide the best number of workgroups working on one feature4 tuple
v3.0.0,set work group size based on feature size
v3.0.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.0.0,we need to refresh the kernel arguments after reallocating
v3.0.0,The only argument that needs to be changed later is num_data_
v3.0.0,"the GPU kernel will process all features in one call, and each"
v3.0.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.0.0,process one feature4 tuple
v3.0.0,"for the root node, indices are not copied"
v3.0.0,"for constant hessian, hessians are not copied except for the root node"
v3.0.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.0.0,and we will launch num_feature workgroups for this kernel
v3.0.0,will launch threads for all features
v3.0.0,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.0.0,copy the results asynchronously. Size depends on if double precision is used
v3.0.0,we will wait for this object in WaitAndGetHistograms
v3.0.0,"when the output is ready, the computation is done"
v3.0.0,values of this feature has been redistributed to multiple bins; need a reduction here
v3.0.0,how many feature-group tuples we have
v3.0.0,leave some safe margin for prefetching
v3.0.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.0.0,clear sparse/dense maps
v3.0.0,do nothing if no features can be processed on GPU
v3.0.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.0.0,unpin old buffer if necessary before destructing them
v3.0.0,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v3.0.0,allocate space for gradients and hessians on device
v3.0.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.0.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.0.0,copy indices to the device
v3.0.0,histogram bin entry size depends on the precision (single/double)
v3.0.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.0.0,each work group generates a sub-histogram of dword_features_ features.
v3.0.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.0.0,create atomic counters for inter-group coordination
v3.0.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.0.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.0.0,looking for dword_features_ non-sparse feature-groups
v3.0.0,decide if we need to redistribute the bin
v3.0.0,multiplier must be a power of 2
v3.0.0,device_bin_mults_.push_back(1);
v3.0.0,found
v3.0.0,for data transfer time
v3.0.0,"Now generate new data structure feature4, and copy data to the device"
v3.0.0,"preallocate arrays for all threads, and pin them"
v3.0.0,building Feature4 bundles; each thread handles dword_features_ features
v3.0.0,one feature datapoint is 4 bits
v3.0.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.0.0,one feature datapoint is one byte
v3.0.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.0.0,Dense bin
v3.0.0,Dense 4-bit bin
v3.0.0,working on the remaining (less than dword_features_) feature groups
v3.0.0,fill the leftover features
v3.0.0,"fill this empty feature with some ""random"" value"
v3.0.0,"fill this empty feature with some ""random"" value"
v3.0.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.0.0,deallocate pinned space for feature copying
v3.0.0,data transfer time
v3.0.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.0.0,"Something bad happened. Just return ""No log available."""
v3.0.0,"build is okay, log may contain warnings"
v3.0.0,destroy any old kernels
v3.0.0,create OpenCL kernels for different number of workgroups per feature
v3.0.0,currently we don't use constant memory
v3.0.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.0.0,kernel with indices in an array
v3.0.0,"kernel with all features enabled, with elimited branches"
v3.0.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.0.0,do nothing if no features can be processed on GPU
v3.0.0,The only argument that needs to be changed later is num_data_
v3.0.0,"hessian is passed as a parameter, but it is not available now."
v3.0.0,hessian will be set in BeforeTrain()
v3.0.0,"Get the max bin size, used for selecting best GPU kernel"
v3.0.0,initialize GPU
v3.0.0,determine which kernel to use based on the max number of bins
v3.0.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.0.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.0.0,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.0.0,setup GPU kernel arguments after we allocating all the buffers
v3.0.0,GPU memory has to been reallocated because data may have been changed
v3.0.0,setup GPU kernel arguments after we allocating all the buffers
v3.0.0,Copy initial full hessians and gradients to GPU.
v3.0.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.0.0,setup hessian parameters only
v3.0.0,hessian is passed as a parameter
v3.0.0,use bagging
v3.0.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.0.0,copy used gradients and hessians to ordered buffer
v3.0.0,transfer the indices to GPU
v3.0.0,transfer hessian to GPU
v3.0.0,setup hessian parameters only
v3.0.0,hessian is passed as a parameter
v3.0.0,transfer gradients to GPU
v3.0.0,only have root
v3.0.0,"Copy indices, gradients and hessians as early as possible"
v3.0.0,only need to initialize for smaller leaf
v3.0.0,Get leaf boundary
v3.0.0,copy indices to the GPU:
v3.0.0,copy ordered hessians to the GPU:
v3.0.0,copy ordered gradients to the GPU:
v3.0.0,do nothing if no features can be processed on GPU
v3.0.0,copy data indices if it is not null
v3.0.0,generate and copy ordered_gradients if gradients is not null
v3.0.0,generate and copy ordered_hessians if hessians is not null
v3.0.0,converted indices in is_feature_used to feature-group indices
v3.0.0,construct the feature masks for dense feature-groups
v3.0.0,"if no feature group is used, just return and do not use GPU"
v3.0.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.0.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.0.0,"All data have been prepared, now run the GPU kernel"
v3.0.0,construct smaller leaf
v3.0.0,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v3.0.0,then construct sparse features on CPU
v3.0.0,"wait for GPU to finish, only if GPU is actually used"
v3.0.0,use double precision
v3.0.0,use single precision
v3.0.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.0.0,#define GPU_DEBUG_COMPARE
v3.0.0,construct larger leaf
v3.0.0,then construct sparse features on CPU
v3.0.0,"wait for GPU to finish, only if GPU is actually used"
v3.0.0,use double precision
v3.0.0,use single precision
v3.0.0,do some sanity check for the GPU algorithm
v3.0.0,limit top k
v3.0.0,get max bin
v3.0.0,calculate buffer size
v3.0.0,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.0.0,"left and right on same time, so need double size"
v3.0.0,initialize histograms for global
v3.0.0,sync global data sumup info
v3.0.0,set global sumup info
v3.0.0,init global data count in leaf
v3.0.0,get local sumup
v3.0.0,get local sumup
v3.0.0,get mean number on machines
v3.0.0,weighted gain
v3.0.0,get top k
v3.0.0,"Copy histogram to buffer, and Get local aggregate features"
v3.0.0,copy histograms.
v3.0.0,copy smaller leaf histograms first
v3.0.0,mark local aggregated feature
v3.0.0,copy
v3.0.0,then copy larger leaf histograms
v3.0.0,mark local aggregated feature
v3.0.0,copy
v3.0.0,use local data to find local best splits
v3.0.0,find splits
v3.0.0,only has root leaf
v3.0.0,local voting
v3.0.0,gather
v3.0.0,get all top-k from all machines
v3.0.0,global voting
v3.0.0,copy local histgrams to buffer
v3.0.0,Reduce scatter for histogram
v3.0.0,find best split from local aggregated histograms
v3.0.0,restore from buffer
v3.0.0,restore from buffer
v3.0.0,find local best
v3.0.0,find local best split for larger leaf
v3.0.0,sync global best info
v3.0.0,copy back
v3.0.0,set the global number of data for leaves
v3.0.0,init the global sumup info
v3.0.0,"instantiate template classes, otherwise linker cannot find the code"
v3.0.0rc1,coding: utf-8
v3.0.0rc1,coding: utf-8
v3.0.0rc1,create predictor first
v3.0.0rc1,check dataset
v3.0.0rc1,reduce cost for prediction training data
v3.0.0rc1,process callbacks
v3.0.0rc1,Most of legacy advanced options becomes callbacks
v3.0.0rc1,construct booster
v3.0.0rc1,start training
v3.0.0rc1,check evaluation result.
v3.0.0rc1,"ranking task, split according to groups"
v3.0.0rc1,run preprocessing on the data set if needed
v3.0.0rc1,setup callbacks
v3.0.0rc1,coding: utf-8
v3.0.0rc1,"simplejson does not support Python 3.2, it throws a SyntaxError"
v3.0.0rc1,because of u'...' Unicode literals.
v3.0.0rc1,dummy function to support older version of scikit-learn
v3.0.0rc1,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v3.0.0rc1,coding: utf-8
v3.0.0rc1,"user can set verbose with kwargs, it has higher priority"
v3.0.0rc1,register default metric for consistency with callable eval_metric case
v3.0.0rc1,try to deduce from class instance
v3.0.0rc1,overwrite default metric by explicitly set metric
v3.0.0rc1,concatenate metric from params (or default if not provided in params) and eval_metric
v3.0.0rc1,copy for consistency
v3.0.0rc1,reduce cost for prediction training data
v3.0.0rc1,free dataset
v3.0.0rc1,Switch to using a multiclass objective in the underlying LGBM instance
v3.0.0rc1,"do not modify args, as it causes errors in model selection tools"
v3.0.0rc1,check group data
v3.0.0rc1,coding: utf-8
v3.0.0rc1,we don't need lib_lightgbm while building docs
v3.0.0rc1,coding: utf-8
v3.0.0rc1,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v3.0.0rc1,coding: utf-8
v3.0.0rc1,coding: utf-8
v3.0.0rc1,TypeError: obj is not a string or a number
v3.0.0rc1,ValueError: invalid literal
v3.0.0rc1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.0.0rc1,avoid memory consumption by arrays concatenation operations
v3.0.0rc1,create numpy array from output arrays
v3.0.0rc1,break up indptr based on number of rows (note more than one matrix in multiclass case)
v3.0.0rc1,for CSC there is extra column added
v3.0.0rc1,reformat output into a csr or csc matrix or list of csr or csc matrices
v3.0.0rc1,same shape as input csr or csc matrix except extra column for expected value
v3.0.0rc1,note: make sure we copy data as it will be deallocated next
v3.0.0rc1,"free the temporary native indptr, indices, and data"
v3.0.0rc1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v3.0.0rc1,avoid memory consumption by arrays concatenation operations
v3.0.0rc1,"no min_data, nthreads and verbose in this function"
v3.0.0rc1,check data has header or not
v3.0.0rc1,need to regroup init_score
v3.0.0rc1,process for args
v3.0.0rc1,"user can set verbose with params, it has higher priority"
v3.0.0rc1,get categorical features
v3.0.0rc1,process for reference dataset
v3.0.0rc1,start construct data
v3.0.0rc1,set feature names
v3.0.0rc1,create valid
v3.0.0rc1,construct subset
v3.0.0rc1,create train
v3.0.0rc1,could be updated if data is not freed
v3.0.0rc1,set to None
v3.0.0rc1,we're done if self and reference share a common upstrem reference
v3.0.0rc1,"group data from LightGBM is boundaries data, need to convert to group size"
v3.0.0rc1,"user can set verbose with params, it has higher priority"
v3.0.0rc1,Training task
v3.0.0rc1,set network if necessary
v3.0.0rc1,construct booster object
v3.0.0rc1,copy the parameters from train_set
v3.0.0rc1,save reference to data
v3.0.0rc1,buffer for inner predict
v3.0.0rc1,Prediction task
v3.0.0rc1,if a single node tree it won't have `leaf_index` so return 0
v3.0.0rc1,"Create the node record, and populate universal data members"
v3.0.0rc1,Update values to reflect node type (leaf or split)
v3.0.0rc1,traverse the next level of the tree
v3.0.0rc1,"In tree format, ""subtree_list"" is a list of node records (dicts),"
v3.0.0rc1,and we add node to the list.
v3.0.0rc1,need reset training data
v3.0.0rc1,need to push new valid data
v3.0.0rc1,"if buffer length is not long enough, re-allocate a buffer"
v3.0.0rc1,"if buffer length is not long enough, reallocate a buffer"
v3.0.0rc1,Copy models
v3.0.0rc1,Get name of features
v3.0.0rc1,avoid to predict many time in one iteration
v3.0.0rc1,Get num of inner evals
v3.0.0rc1,Get name of evals
v3.0.0rc1,coding: utf-8
v3.0.0rc1,Callback environment used by callbacks
v3.0.0rc1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.0.0rc1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v3.0.0rc1,coding: utf-8
v3.0.0rc1,load or create your dataset
v3.0.0rc1,create dataset for lightgbm
v3.0.0rc1,"if you want to re-use data, remember to set free_raw_data=False"
v3.0.0rc1,specify your configurations as a dict
v3.0.0rc1,generate feature names
v3.0.0rc1,feature_name and categorical_feature
v3.0.0rc1,check feature name
v3.0.0rc1,save model to file
v3.0.0rc1,dump model to JSON (and save to file)
v3.0.0rc1,feature names
v3.0.0rc1,feature importances
v3.0.0rc1,load model to predict
v3.0.0rc1,can only predict with the best iteration (or the saving iteration)
v3.0.0rc1,eval with loaded model
v3.0.0rc1,dump model with pickle
v3.0.0rc1,load model with pickle to predict
v3.0.0rc1,can predict with any iteration when loaded in pickle way
v3.0.0rc1,eval with loaded model
v3.0.0rc1,continue training
v3.0.0rc1,init_model accepts:
v3.0.0rc1,1. model file name
v3.0.0rc1,2. Booster()
v3.0.0rc1,decay learning rates
v3.0.0rc1,learning_rates accepts:
v3.0.0rc1,1. list/tuple with length = num_boost_round
v3.0.0rc1,2. function(curr_iter)
v3.0.0rc1,change other parameters during training
v3.0.0rc1,self-defined objective function
v3.0.0rc1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v3.0.0rc1,log likelihood loss
v3.0.0rc1,self-defined eval metric
v3.0.0rc1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0rc1,binary error
v3.0.0rc1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.0.0rc1,This may make built-in evalution metric calculate wrong results
v3.0.0rc1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.0.0rc1,Keep this in mind when you use the customization
v3.0.0rc1,another self-defined eval metric
v3.0.0rc1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0rc1,accuracy
v3.0.0rc1,"NOTE: when you do customized loss function, the default prediction value is margin"
v3.0.0rc1,This may make built-in evalution metric calculate wrong results
v3.0.0rc1,"For example, we are doing log likelihood loss, the prediction is score before logistic transformation"
v3.0.0rc1,Keep this in mind when you use the customization
v3.0.0rc1,callback
v3.0.0rc1,coding: utf-8
v3.0.0rc1,load or create your dataset
v3.0.0rc1,train
v3.0.0rc1,predict
v3.0.0rc1,eval
v3.0.0rc1,feature importances
v3.0.0rc1,self-defined eval metric
v3.0.0rc1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0rc1,Root Mean Squared Logarithmic Error (RMSLE)
v3.0.0rc1,train
v3.0.0rc1,another self-defined eval metric
v3.0.0rc1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v3.0.0rc1,Relative Absolute Error (RAE)
v3.0.0rc1,train
v3.0.0rc1,predict
v3.0.0rc1,eval
v3.0.0rc1,other scikit-learn modules
v3.0.0rc1,coding: utf-8
v3.0.0rc1,load or create your dataset
v3.0.0rc1,create dataset for lightgbm
v3.0.0rc1,specify your configurations as a dict
v3.0.0rc1,train
v3.0.0rc1,coding: utf-8
v3.0.0rc1,################
v3.0.0rc1,Simulate some binary data with a single categorical and
v3.0.0rc1,single continuous predictor
v3.0.0rc1,################
v3.0.0rc1,Set up a couple of utilities for our experiments
v3.0.0rc1,################
v3.0.0rc1,Observe the behavior of `binary` and `xentropy` objectives
v3.0.0rc1,Trying this throws an error on non-binary values of y:
v3.0.0rc1,"experiment('binary', label_type='probability', DATA)"
v3.0.0rc1,The speed of `binary` is not drastically different than
v3.0.0rc1,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v3.0.0rc1,there are reasons to suspect that `binary` should run faster when the
v3.0.0rc1,label is an integer instead of a float
v3.0.0rc1,coding: utf-8
v3.0.0rc1,load or create your dataset
v3.0.0rc1,create dataset for lightgbm
v3.0.0rc1,specify your configurations as a dict
v3.0.0rc1,train
v3.0.0rc1,save model to file
v3.0.0rc1,predict
v3.0.0rc1,eval
v3.0.0rc1,coding: utf-8
v3.0.0rc1,!/usr/bin/env python3
v3.0.0rc1,-*- coding: utf-8 -*-
v3.0.0rc1,
v3.0.0rc1,"LightGBM documentation build configuration file, created by"
v3.0.0rc1,sphinx-quickstart on Thu May  4 14:30:58 2017.
v3.0.0rc1,
v3.0.0rc1,This file is execfile()d with the current directory set to its
v3.0.0rc1,containing dir.
v3.0.0rc1,
v3.0.0rc1,Note that not all possible configuration values are present in this
v3.0.0rc1,autogenerated file.
v3.0.0rc1,
v3.0.0rc1,All configuration values have a default; values that are commented out
v3.0.0rc1,serve to show the default.
v3.0.0rc1,"If extensions (or modules to document with autodoc) are in another directory,"
v3.0.0rc1,add these directories to sys.path here. If the directory is relative to the
v3.0.0rc1,"documentation root, use os.path.abspath to make it absolute."
v3.0.0rc1,-- mock out modules
v3.0.0rc1,-- General configuration ------------------------------------------------
v3.0.0rc1,"If your documentation needs a minimal Sphinx version, state it here."
v3.0.0rc1,"Add any Sphinx extension module names here, as strings. They can be"
v3.0.0rc1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v3.0.0rc1,ones.
v3.0.0rc1,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v3.0.0rc1,Only the class' docstring is inserted.
v3.0.0rc1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v3.0.0rc1,The master toctree document.
v3.0.0rc1,General information about the project.
v3.0.0rc1,"The version info for the project you're documenting, acts as replacement for"
v3.0.0rc1,"|version| and |release|, also used in various other places throughout the"
v3.0.0rc1,built documents.
v3.0.0rc1,The short X.Y version.
v3.0.0rc1,"The full version, including alpha/beta/rc tags."
v3.0.0rc1,The language for content autogenerated by Sphinx. Refer to documentation
v3.0.0rc1,for a list of supported languages.
v3.0.0rc1,
v3.0.0rc1,This is also used if you do content translation via gettext catalogs.
v3.0.0rc1,"Usually you set ""language"" from the command line for these cases."
v3.0.0rc1,"List of patterns, relative to source directory, that match files and"
v3.0.0rc1,directories to ignore when looking for source files.
v3.0.0rc1,This patterns also effect to html_static_path and html_extra_path
v3.0.0rc1,The name of the Pygments (syntax highlighting) style to use.
v3.0.0rc1,-- Configuration for C API docs generation ------------------------------
v3.0.0rc1,-- Options for HTML output ----------------------------------------------
v3.0.0rc1,The theme to use for HTML and HTML Help pages.  See the documentation for
v3.0.0rc1,a list of builtin themes.
v3.0.0rc1,Theme options are theme-specific and customize the look and feel of a theme
v3.0.0rc1,"further.  For a list of options available for each theme, see the"
v3.0.0rc1,documentation.
v3.0.0rc1,"Add any paths that contain custom static files (such as style sheets) here,"
v3.0.0rc1,"relative to this directory. They are copied after the builtin static files,"
v3.0.0rc1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v3.0.0rc1,-- Options for HTMLHelp output ------------------------------------------
v3.0.0rc1,Output file base name for HTML help builder.
v3.0.0rc1,Warning! The following code can cause buffer overflows on RTD.
v3.0.0rc1,Consider suppressing output completely if RTD project silently fails.
v3.0.0rc1,Refer to https://github.com/svenevs/exhale
v3.0.0rc1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.0.0rc1,Warning! The following code can cause buffer overflows on RTD.
v3.0.0rc1,Consider suppressing output completely if RTD project silently fails.
v3.0.0rc1,Refer to https://github.com/svenevs/exhale
v3.0.0rc1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v3.0.0rc1,coding: utf-8
v3.0.0rc1,coding: utf-8
v3.0.0rc1,we don't need lib_lightgbm while building docs
v3.0.0rc1,coding: utf-8
v3.0.0rc1,check saved model persistence
v3.0.0rc1,"we need to check the consistency of model file here, so test for exact equal"
v3.0.0rc1,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v3.0.0rc1,"scores likely to be different, but prediction should still be the same"
v3.0.0rc1,test that shape is checked during prediction
v3.0.0rc1,"Set extremely harsh penalties, so CEGB will block most splits."
v3.0.0rc1,"Compare pairs of penalties, to ensure scaling works as intended"
v3.0.0rc1,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v3.0.0rc1,coding: utf-8
v3.0.0rc1,prediction result is actually not transformed (is raw) due to custom objective
v3.0.0rc1,sklearn <0.23 does not have a stacking classifier and n_features_in_ property
v3.0.0rc1,sklearn <0.23 does not have a stacking regressor and n_features_in_ property
v3.0.0rc1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.0.0rc1,sklearn < 0.23 does not have as_frame parameter
v3.0.0rc1,sklearn < 0.22 does not have the post fit attribute: classes_
v3.0.0rc1,sklearn < 0.23 does not have as_frame parameter
v3.0.0rc1,Test if random_state is properly stored
v3.0.0rc1,Test if two random states produce identical models
v3.0.0rc1,Test if subsequent fits sample from random_state object and produce different models
v3.0.0rc1,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v3.0.0rc1,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v3.0.0rc1,we cannot use `check_estimator` directly since there is no skip test mechanism
v3.0.0rc1,we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)
v3.0.0rc1,skip test because scikit-learn incorrectly asserts that
v3.0.0rc1,private attributes cannot be set in __init__
v3.0.0rc1,(see https://github.com/microsoft/LightGBM/issues/2628)
v3.0.0rc1,With default params
v3.0.0rc1,Tests same probabilities
v3.0.0rc1,Tests same predictions
v3.0.0rc1,Tests same raw scores
v3.0.0rc1,Tests same leaf indices
v3.0.0rc1,Tests same feature contributions
v3.0.0rc1,Tests other parameters for the prediction works
v3.0.0rc1,Tests start_iteration
v3.0.0rc1,"Tests same probabilities, starting from iteration 10"
v3.0.0rc1,"Tests same predictions, starting from iteration 10"
v3.0.0rc1,"Tests same raw scores, starting from iteration 10"
v3.0.0rc1,"Tests same leaf indices, starting from iteration 10"
v3.0.0rc1,"Tests same feature contributions, starting from iteration 10"
v3.0.0rc1,"Tests other parameters for the prediction works, starting from iteration 10"
v3.0.0rc1,"no custom objective, no custom metric"
v3.0.0rc1,default metric
v3.0.0rc1,non-default metric
v3.0.0rc1,no metric
v3.0.0rc1,non-default metric in eval_metric
v3.0.0rc1,non-default metric with non-default metric in eval_metric
v3.0.0rc1,non-default metric with multiple metrics in eval_metric
v3.0.0rc1,non-default metric with multiple metrics in eval_metric for LGBMClassifier
v3.0.0rc1,default metric for non-default objective
v3.0.0rc1,non-default metric for non-default objective
v3.0.0rc1,no metric
v3.0.0rc1,non-default metric in eval_metric for non-default objective
v3.0.0rc1,non-default metric with non-default metric in eval_metric for non-default objective
v3.0.0rc1,non-default metric with multiple metrics in eval_metric for non-default objective
v3.0.0rc1,"custom objective, no custom metric"
v3.0.0rc1,default regression metric for custom objective
v3.0.0rc1,non-default regression metric for custom objective
v3.0.0rc1,multiple regression metrics for custom objective
v3.0.0rc1,no metric
v3.0.0rc1,default regression metric with non-default metric in eval_metric for custom objective
v3.0.0rc1,non-default regression metric with metric in eval_metric for custom objective
v3.0.0rc1,multiple regression metrics with metric in eval_metric for custom objective
v3.0.0rc1,multiple regression metrics with multiple metrics in eval_metric for custom objective
v3.0.0rc1,"no custom objective, custom metric"
v3.0.0rc1,default metric with custom metric
v3.0.0rc1,non-default metric with custom metric
v3.0.0rc1,multiple metrics with custom metric
v3.0.0rc1,custom metric (disable default metric)
v3.0.0rc1,default metric for non-default objective with custom metric
v3.0.0rc1,non-default metric for non-default objective with custom metric
v3.0.0rc1,multiple metrics for non-default objective with custom metric
v3.0.0rc1,custom metric (disable default metric for non-default objective)
v3.0.0rc1,"custom objective, custom metric"
v3.0.0rc1,custom metric for custom objective
v3.0.0rc1,non-default regression metric with custom metric for custom objective
v3.0.0rc1,multiple regression metrics with custom metric for custom objective
v3.0.0rc1,default metric and invalid binary metric is replaced with multiclass alternative
v3.0.0rc1,invalid objective is replaced with default multiclass one
v3.0.0rc1,and invalid binary metric is replaced with multiclass alternative
v3.0.0rc1,default metric for non-default multiclass objective
v3.0.0rc1,and invalid binary metric is replaced with multiclass alternative
v3.0.0rc1,default metric and invalid multiclass metric is replaced with binary alternative
v3.0.0rc1,invalid multiclass metric is replaced with binary alternative for custom objective
v3.0.0rc1,training data as eval_set
v3.0.0rc1,feval
v3.0.0rc1,single eval_set
v3.0.0rc1,two eval_set
v3.0.0rc1,coding: utf-8
v3.0.0rc1,coding: utf-8
v3.0.0rc1,check that default gives same result as k = 1
v3.0.0rc1,check against independent calculation for k = 1
v3.0.0rc1,check against independent calculation for k = 2
v3.0.0rc1,check against independent calculation for k = 10
v3.0.0rc1,check cases where predictions are equal
v3.0.0rc1,should give same result as binary auc for 2 classes
v3.0.0rc1,test the case where all predictions are equal
v3.0.0rc1,should give 1 when accuracy = 1
v3.0.0rc1,test loading weights
v3.0.0rc1,no early stopping
v3.0.0rc1,early stopping occurs
v3.0.0rc1,test custom eval metrics
v3.0.0rc1,"shuffle = False, override metric in params"
v3.0.0rc1,"shuffle = True, callbacks"
v3.0.0rc1,enable display training loss
v3.0.0rc1,self defined folds
v3.0.0rc1,lambdarank
v3.0.0rc1,... with l2 metric
v3.0.0rc1,... with NDCG (default) metric
v3.0.0rc1,self defined folds with lambdarank
v3.0.0rc1,with early stopping
v3.0.0rc1,predict by each fold booster
v3.0.0rc1,fold averaging
v3.0.0rc1,without early stopping
v3.0.0rc1,test feature_names with whitespaces
v3.0.0rc1,This has non-ascii strings.
v3.0.0rc1,take subsets and train
v3.0.0rc1,generate CSR sparse dataset
v3.0.0rc1,convert data to dense and get back same contribs
v3.0.0rc1,validate the values are the same
v3.0.0rc1,validate using CSC matrix
v3.0.0rc1,validate the values are the same
v3.0.0rc1,Note there is an extra column added to the output for the expected value
v3.0.0rc1,Note output CSC shape should be same as CSR output shape
v3.0.0rc1,test sliced labels
v3.0.0rc1,append some columns
v3.0.0rc1,append some rows
v3.0.0rc1,test sliced 2d matrix
v3.0.0rc1,test sliced CSR
v3.0.0rc1,test if a penalty as high as the depth indeed prohibits all monotone splits
v3.0.0rc1,The penalization is so high that the first 2 features should not be used here
v3.0.0rc1,Check that a very high penalization is the same as not using the features at all
v3.0.0rc1,"no fobj, no feval"
v3.0.0rc1,default metric
v3.0.0rc1,non-default metric in params
v3.0.0rc1,default metric in args
v3.0.0rc1,non-default metric in args
v3.0.0rc1,metric in args overwrites one in params
v3.0.0rc1,multiple metrics in params
v3.0.0rc1,multiple metrics in args
v3.0.0rc1,remove default metric by 'None' in list
v3.0.0rc1,remove default metric by 'None' aliases
v3.0.0rc1,"fobj, no feval"
v3.0.0rc1,no default metric
v3.0.0rc1,metric in params
v3.0.0rc1,metric in args
v3.0.0rc1,metric in args overwrites its' alias in params
v3.0.0rc1,multiple metrics in params
v3.0.0rc1,multiple metrics in args
v3.0.0rc1,"no fobj, feval"
v3.0.0rc1,default metric with custom one
v3.0.0rc1,non-default metric in params with custom one
v3.0.0rc1,default metric in args with custom one
v3.0.0rc1,non-default metric in args with custom one
v3.0.0rc1,"metric in args overwrites one in params, custom one is evaluated too"
v3.0.0rc1,multiple metrics in params with custom one
v3.0.0rc1,multiple metrics in args with custom one
v3.0.0rc1,custom metric is evaluated despite 'None' is passed
v3.0.0rc1,"fobj, feval"
v3.0.0rc1,"no default metric, only custom one"
v3.0.0rc1,metric in params with custom one
v3.0.0rc1,metric in args with custom one
v3.0.0rc1,"metric in args overwrites one in params, custom one is evaluated too"
v3.0.0rc1,multiple metrics in params with custom one
v3.0.0rc1,multiple metrics in args with custom one
v3.0.0rc1,custom metric is evaluated despite 'None' is passed
v3.0.0rc1,"no fobj, no feval"
v3.0.0rc1,default metric
v3.0.0rc1,default metric in params
v3.0.0rc1,non-default metric in params
v3.0.0rc1,multiple metrics in params
v3.0.0rc1,remove default metric by 'None' aliases
v3.0.0rc1,"fobj, no feval"
v3.0.0rc1,no default metric
v3.0.0rc1,metric in params
v3.0.0rc1,multiple metrics in params
v3.0.0rc1,"no fobj, feval"
v3.0.0rc1,default metric with custom one
v3.0.0rc1,default metric in params with custom one
v3.0.0rc1,non-default metric in params with custom one
v3.0.0rc1,multiple metrics in params with custom one
v3.0.0rc1,custom metric is evaluated despite 'None' is passed
v3.0.0rc1,"fobj, feval"
v3.0.0rc1,"no default metric, only custom one"
v3.0.0rc1,metric in params with custom one
v3.0.0rc1,multiple metrics in params with custom one
v3.0.0rc1,custom metric is evaluated despite 'None' is passed
v3.0.0rc1,multiclass default metric
v3.0.0rc1,multiclass default metric with custom one
v3.0.0rc1,multiclass metric alias with custom one for custom objective
v3.0.0rc1,no metric for invalid class_num
v3.0.0rc1,custom metric for invalid class_num
v3.0.0rc1,multiclass metric alias with custom one with invalid class_num
v3.0.0rc1,multiclass default metric without num_class
v3.0.0rc1,multiclass metric alias
v3.0.0rc1,multiclass metric
v3.0.0rc1,non-valid metric for multiclass objective
v3.0.0rc1,non-default num_class for default objective
v3.0.0rc1,no metric with non-default num_class for custom objective
v3.0.0rc1,multiclass metric alias for custom objective
v3.0.0rc1,multiclass metric for custom objective
v3.0.0rc1,binary metric with non-default num_class for custom objective
v3.0.0rc1,test XGBoost-style return value
v3.0.0rc1,test numpy-style return value
v3.0.0rc1,test bins string type
v3.0.0rc1,test histogram is disabled for categorical features
v3.0.0rc1,test for lgb.train
v3.0.0rc1,test feval for lgb.train
v3.0.0rc1,test with two valid data for lgb.train
v3.0.0rc1,test for lgb.cv
v3.0.0rc1,test feval for lgb.cv
v3.0.0rc1,test that binning works properly for features with only positive or only negative values
v3.0.0rc1,decreasing without freeing raw data is allowed
v3.0.0rc1,decreasing before lazy init is allowed
v3.0.0rc1,increasing is allowed
v3.0.0rc1,decreasing with disabled filter is allowed
v3.0.0rc1,decreasing with enabled filter is disallowed;
v3.0.0rc1,also changes of other params are disallowed
v3.0.0rc1,check extra trees increases regularization
v3.0.0rc1,check path smoothing increases regularization
v3.0.0rc1,test edge case with one leaf
v3.0.0rc1,check that constraint containing all features is equivalent to no constraint
v3.0.0rc1,check that constraint partitioning the features reduces train accuracy
v3.0.0rc1,check that constraints consisting of single features reduce accuracy further
v3.0.0rc1,test that interaction constraints work when not all features are used
v3.0.0rc1,test that the predict once with all iterations equals summed results with start_iteration and num_iteration
v3.0.0rc1,"test the case where start_iteration <= 0, and num_iteration is None"
v3.0.0rc1,"test the case where start_iteration > 0, and num_iteration <= 0"
v3.0.0rc1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_leaf=True"
v3.0.0rc1,"test the case where start_iteration > 0, and num_iteration <= 0, with pred_contrib=True"
v3.0.0rc1,test for regression
v3.0.0rc1,test both with and without early stopping
v3.0.0rc1,test for multi-class
v3.0.0rc1,test both with and without early stopping
v3.0.0rc1,test for binary
v3.0.0rc1,test both with and without early stopping
v3.0.0rc1,coding: utf-8
v3.0.0rc1,convert from one-based to  zero-based index
v3.0.0rc1,convert from boundaries to size
v3.0.0rc1,--- start Booster interfaces
v3.0.0rc1,.Call() calls
v3.0.0rc1,coding: utf-8
v3.0.0rc1,alias table
v3.0.0rc1,names
v3.0.0rc1,from strings
v3.0.0rc1,tails
v3.0.0rc1,tails
v3.0.0rc1,coding: utf-8
v3.0.0rc1,Single row predictor to abstract away caching logic
v3.0.0rc1,create boosting
v3.0.0rc1,initialize the boosting
v3.0.0rc1,create objective function
v3.0.0rc1,initialize the objective function
v3.0.0rc1,create training metric
v3.0.0rc1,reset the boosting
v3.0.0rc1,create objective function
v3.0.0rc1,initialize the objective function
v3.0.0rc1,calculate the nonzero data and indices size
v3.0.0rc1,allocate data and indices arrays
v3.0.0rc1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.0.0rc1,aggregated per row feature contribution results
v3.0.0rc1,keep track of the row_vector sizes for parallelization
v3.0.0rc1,copy vector results to output for each row
v3.0.0rc1,Get the number of trees per iteration (for multiclass scenario we output multiple sparse matrices)
v3.0.0rc1,aggregated per row feature contribution results
v3.0.0rc1,calculate number of elements per column to construct
v3.0.0rc1,the CSC matrix with random access
v3.0.0rc1,keep track of column counts
v3.0.0rc1,keep track of beginning index for each column
v3.0.0rc1,keep track of beginning index for each matrix
v3.0.0rc1,store the row index
v3.0.0rc1,update column count
v3.0.0rc1,explicitly declare symbols from LightGBM namespace
v3.0.0rc1,some help functions used to convert data
v3.0.0rc1,Row iterator of on column for CSC matrix
v3.0.0rc1,"return value at idx, only can access by ascent order"
v3.0.0rc1,"return next non-zero pair, if index < 0, means no more data"
v3.0.0rc1,start of c_api functions
v3.0.0rc1,sample data first
v3.0.0rc1,sample data first
v3.0.0rc1,sample data first
v3.0.0rc1,local buffer to re-use memory
v3.0.0rc1,sample data first
v3.0.0rc1,no more data
v3.0.0rc1,---- start of booster
v3.0.0rc1,Single row in row-major format:
v3.0.0rc1,---- start of some help functions
v3.0.0rc1,data is array of pointers to individual rows
v3.0.0rc1,set number of threads for openmp
v3.0.0rc1,check for alias
v3.0.0rc1,read parameters from config file
v3.0.0rc1,"remove str after ""#"""
v3.0.0rc1,check for alias again
v3.0.0rc1,load configs
v3.0.0rc1,prediction is needed if using input initial model(continued train)
v3.0.0rc1,need to continue training
v3.0.0rc1,sync up random seed for data partition
v3.0.0rc1,load Training data
v3.0.0rc1,load data for parallel training
v3.0.0rc1,load data for single machine
v3.0.0rc1,need save binary file
v3.0.0rc1,create training metric
v3.0.0rc1,only when have metrics then need to construct validation data
v3.0.0rc1,"Add validation data, if it exists"
v3.0.0rc1,add
v3.0.0rc1,need save binary file
v3.0.0rc1,add metric for validation data
v3.0.0rc1,output used time on each iteration
v3.0.0rc1,need init network
v3.0.0rc1,create boosting
v3.0.0rc1,create objective function
v3.0.0rc1,load training data
v3.0.0rc1,initialize the objective function
v3.0.0rc1,initialize the boosting
v3.0.0rc1,add validation data into boosting
v3.0.0rc1,convert model to if-else statement code
v3.0.0rc1,create predictor
v3.0.0rc1,Free memory
v3.0.0rc1,create predictor
v3.0.0rc1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v3.0.0rc1,counts for all labels
v3.0.0rc1,"start from top label, and accumulate DCG"
v3.0.0rc1,counts for all labels
v3.0.0rc1,calculate k Max DCG by one pass
v3.0.0rc1,get sorted indices by score
v3.0.0rc1,calculate dcg
v3.0.0rc1,get sorted indices by score
v3.0.0rc1,calculate multi dcg by one pass
v3.0.0rc1,wait for all client start up
v3.0.0rc1,"Don't call MPI_Finalize() here: If the destructor was called because only this node had an exception, calling MPI_Finalize() will cause all nodes to hang."
v3.0.0rc1,Instead we will handle finalize/abort for MPI in main().
v3.0.0rc1,default set to -1
v3.0.0rc1,"distance at k-th communication, distance[k] = 2^k"
v3.0.0rc1,set incoming rank at k-th commuication
v3.0.0rc1,set outgoing rank at k-th commuication
v3.0.0rc1,defalut set as -1
v3.0.0rc1,construct all recursive halving map for all machines
v3.0.0rc1,let 1 << k <= num_machines
v3.0.0rc1,distance of each communication
v3.0.0rc1,"if num_machines = 2^k, don't need to group machines"
v3.0.0rc1,"communication direction, %2 == 0 is positive"
v3.0.0rc1,neighbor at k-th communication
v3.0.0rc1,receive data block at k-th communication
v3.0.0rc1,send data block at k-th communication
v3.0.0rc1,"if num_machines != 2^k, need to group machines"
v3.0.0rc1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v3.0.0rc1,let left machine as group leader
v3.0.0rc1,"cache block information for groups, group with 2 machines will have double block size"
v3.0.0rc1,convert from group to node leader
v3.0.0rc1,convert from node to group
v3.0.0rc1,meet new group
v3.0.0rc1,add block len for this group
v3.0.0rc1,calculate the group block start
v3.0.0rc1,not need to construct
v3.0.0rc1,get receive block informations
v3.0.0rc1,accumulate block len
v3.0.0rc1,get send block informations
v3.0.0rc1,accumulate block len
v3.0.0rc1,static member definition
v3.0.0rc1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v3.0.0rc1,assign the blocks to every rank.
v3.0.0rc1,do reduce scatter
v3.0.0rc1,do all gather
v3.0.0rc1,assign blocks
v3.0.0rc1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v3.0.0rc1,copy back
v3.0.0rc1,assign blocks
v3.0.0rc1,start all gather
v3.0.0rc1,when num_machines is small and data is large
v3.0.0rc1,use output as receive buffer
v3.0.0rc1,get current local block size
v3.0.0rc1,get out rank
v3.0.0rc1,get in rank
v3.0.0rc1,get send information
v3.0.0rc1,get recv information
v3.0.0rc1,send and recv at same time
v3.0.0rc1,rotate in-place
v3.0.0rc1,use output as receive buffer
v3.0.0rc1,get current local block size
v3.0.0rc1,get send information
v3.0.0rc1,get recv information
v3.0.0rc1,send and recv at same time
v3.0.0rc1,use output as receive buffer
v3.0.0rc1,send and recv at same time
v3.0.0rc1,send local data to neighbor first
v3.0.0rc1,receive neighbor data first
v3.0.0rc1,reduce
v3.0.0rc1,get target
v3.0.0rc1,get send information
v3.0.0rc1,get recv information
v3.0.0rc1,send and recv at same time
v3.0.0rc1,reduce
v3.0.0rc1,send result to neighbor
v3.0.0rc1,receive result from neighbor
v3.0.0rc1,copy result
v3.0.0rc1,start up socket
v3.0.0rc1,parse clients from file
v3.0.0rc1,get ip list of local machine
v3.0.0rc1,get local rank
v3.0.0rc1,construct listener
v3.0.0rc1,construct communication topo
v3.0.0rc1,construct linkers
v3.0.0rc1,free listener
v3.0.0rc1,set timeout
v3.0.0rc1,accept incoming socket
v3.0.0rc1,receive rank
v3.0.0rc1,add new socket
v3.0.0rc1,save ranks that need to connect with
v3.0.0rc1,start listener
v3.0.0rc1,start connect
v3.0.0rc1,let smaller rank connect to larger rank
v3.0.0rc1,send local rank
v3.0.0rc1,wait for listener
v3.0.0rc1,print connected linkers
v3.0.0rc1,valid the type
v3.0.0rc1,Constructors
v3.0.0rc1,Get type tag
v3.0.0rc1,Comparisons
v3.0.0rc1,"This has to be separate, not in Statics, because Json() accesses"
v3.0.0rc1,statics().null.
v3.0.0rc1,"advance until next line, or end of input"
v3.0.0rc1,advance until closing tokens
v3.0.0rc1,The usual case: non-escaped characters
v3.0.0rc1,Handle escapes
v3.0.0rc1,Extract 4-byte escape sequence
v3.0.0rc1,Explicitly check length of the substring. The following loop
v3.0.0rc1,relies on std::string returning the terminating NUL when
v3.0.0rc1,accessing str[length]. Checking here reduces brittleness.
v3.0.0rc1,JSON specifies that characters outside the BMP shall be encoded as a
v3.0.0rc1,pair of 4-hex-digit \u escapes encoding their surrogate pair
v3.0.0rc1,components. Check whether we're in the middle of such a beast: the
v3.0.0rc1,"previous codepoint was an escaped lead (high) surrogate, and this is"
v3.0.0rc1,a trail (low) surrogate.
v3.0.0rc1,"Reassemble the two surrogate pairs into one astral-plane character,"
v3.0.0rc1,per the UTF-16 algorithm.
v3.0.0rc1,Integer part
v3.0.0rc1,Decimal part
v3.0.0rc1,Exponent part
v3.0.0rc1,Check for any trailing garbage
v3.0.0rc1,Documented in json11.hpp
v3.0.0rc1,Check for another object
v3.0.0rc1,get column names
v3.0.0rc1,load label idx first
v3.0.0rc1,erase label column name
v3.0.0rc1,load ignore columns
v3.0.0rc1,load weight idx
v3.0.0rc1,load group idx
v3.0.0rc1,don't support query id in data file when training in parallel
v3.0.0rc1,read data to memory
v3.0.0rc1,sample data
v3.0.0rc1,construct feature bin mappers
v3.0.0rc1,initialize label
v3.0.0rc1,extract features
v3.0.0rc1,sample data from file
v3.0.0rc1,construct feature bin mappers
v3.0.0rc1,initialize label
v3.0.0rc1,extract features
v3.0.0rc1,load data from binary file
v3.0.0rc1,check meta data
v3.0.0rc1,need to check training data
v3.0.0rc1,read data in memory
v3.0.0rc1,initialize label
v3.0.0rc1,extract features
v3.0.0rc1,Get number of lines of data file
v3.0.0rc1,initialize label
v3.0.0rc1,extract features
v3.0.0rc1,load data from binary file
v3.0.0rc1,not need to check validation data
v3.0.0rc1,check meta data
v3.0.0rc1,buffer to read binary file
v3.0.0rc1,check token
v3.0.0rc1,read size of header
v3.0.0rc1,re-allocmate space if not enough
v3.0.0rc1,read header
v3.0.0rc1,get header
v3.0.0rc1,num_groups
v3.0.0rc1,real_feature_idx_
v3.0.0rc1,feature2group
v3.0.0rc1,feature2subfeature
v3.0.0rc1,group_bin_boundaries
v3.0.0rc1,group_feature_start_
v3.0.0rc1,group_feature_cnt_
v3.0.0rc1,get feature names
v3.0.0rc1,write feature names
v3.0.0rc1,get forced_bin_bounds_
v3.0.0rc1,read size of meta data
v3.0.0rc1,re-allocate space if not enough
v3.0.0rc1,read meta data
v3.0.0rc1,load meta data
v3.0.0rc1,sample local used data if need to partition
v3.0.0rc1,"if not contain query file, minimal sample unit is one record"
v3.0.0rc1,"if contain query file, minimal sample unit is one query"
v3.0.0rc1,if is new query
v3.0.0rc1,read feature data
v3.0.0rc1,read feature size
v3.0.0rc1,re-allocate space if not enough
v3.0.0rc1,fill feature_names_ if not header
v3.0.0rc1,get forced split
v3.0.0rc1,"if only one machine, find bin locally"
v3.0.0rc1,"if have multi-machines, need to find bin distributed"
v3.0.0rc1,different machines will find bin for different features
v3.0.0rc1,start and len will store the process feature indices for different machines
v3.0.0rc1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.0.0rc1,free
v3.0.0rc1,gather global feature bin mappers
v3.0.0rc1,restore features bins from buffer
v3.0.0rc1,---- private functions ----
v3.0.0rc1,"if features are ordered, not need to use hist_buf"
v3.0.0rc1,read all lines
v3.0.0rc1,get query data
v3.0.0rc1,"if not contain query data, minimal sample unit is one record"
v3.0.0rc1,"if contain query data, minimal sample unit is one query"
v3.0.0rc1,if is new query
v3.0.0rc1,get query data
v3.0.0rc1,"if not contain query file, minimal sample unit is one record"
v3.0.0rc1,"if contain query file, minimal sample unit is one query"
v3.0.0rc1,if is new query
v3.0.0rc1,parse features
v3.0.0rc1,get forced split
v3.0.0rc1,"check the range of label_idx, weight_idx and group_idx"
v3.0.0rc1,fill feature_names_ if not header
v3.0.0rc1,start find bins
v3.0.0rc1,"if only one machine, find bin locally"
v3.0.0rc1,start and len will store the process feature indices for different machines
v3.0.0rc1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v3.0.0rc1,free
v3.0.0rc1,gather global feature bin mappers
v3.0.0rc1,restore features bins from buffer
v3.0.0rc1,if doesn't need to prediction with initial model
v3.0.0rc1,parser
v3.0.0rc1,set label
v3.0.0rc1,free processed line:
v3.0.0rc1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.0.0rc1,text_reader_->Lines()[i].shrink_to_fit();
v3.0.0rc1,push data
v3.0.0rc1,if is used feature
v3.0.0rc1,if need to prediction with initial model
v3.0.0rc1,parser
v3.0.0rc1,set initial score
v3.0.0rc1,set label
v3.0.0rc1,free processed line:
v3.0.0rc1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v3.0.0rc1,text_reader_->Lines()[i].shrink_to_fit();
v3.0.0rc1,push data
v3.0.0rc1,if is used feature
v3.0.0rc1,metadata_ will manage space of init_score
v3.0.0rc1,text data can be free after loaded feature values
v3.0.0rc1,parser
v3.0.0rc1,set initial score
v3.0.0rc1,set label
v3.0.0rc1,push data
v3.0.0rc1,if is used feature
v3.0.0rc1,only need part of data
v3.0.0rc1,need full data
v3.0.0rc1,metadata_ will manage space of init_score
v3.0.0rc1,read size of token
v3.0.0rc1,remove duplicates
v3.0.0rc1,deep copy function for BinMapper
v3.0.0rc1,mean size for one bin
v3.0.0rc1,need a new bin
v3.0.0rc1,update bin upper bound
v3.0.0rc1,last bin upper bound
v3.0.0rc1,get list of distinct values
v3.0.0rc1,get number of positive and negative distinct values
v3.0.0rc1,include zero bounds and infinity bound
v3.0.0rc1,"add forced bounds, excluding zeros since we have already added zero bounds"
v3.0.0rc1,find remaining bounds
v3.0.0rc1,find distinct_values first
v3.0.0rc1,push zero in the front
v3.0.0rc1,use the large value
v3.0.0rc1,push zero in the back
v3.0.0rc1,convert to int type first
v3.0.0rc1,sort by counts
v3.0.0rc1,avoid first bin is zero
v3.0.0rc1,will ignore the categorical of small counts
v3.0.0rc1,need an additional bin for NaN
v3.0.0rc1,use -1 to represent NaN
v3.0.0rc1,Use MissingType::None to represent this bin contains all categoricals
v3.0.0rc1,check trivial(num_bin_ == 1) feature
v3.0.0rc1,check useless bin
v3.0.0rc1,FIXME: how to enable `most_freq_bin_ = 0` for categorical features
v3.0.0rc1,"When most_freq_bin_ != default_bin_, there are some additional data loading costs."
v3.0.0rc1,so use most_freq_bin_  = default_bin_ when there is not so sparse
v3.0.0rc1,"for lambdarank, it needs query data for partition data in parallel learning"
v3.0.0rc1,need convert query_id to boundaries
v3.0.0rc1,check weights
v3.0.0rc1,check query boundries
v3.0.0rc1,contain initial score file
v3.0.0rc1,check weights
v3.0.0rc1,get local weights
v3.0.0rc1,check query boundries
v3.0.0rc1,get local query boundaries
v3.0.0rc1,contain initial score file
v3.0.0rc1,get local initial scores
v3.0.0rc1,re-load query weight
v3.0.0rc1,save to nullptr
v3.0.0rc1,save to nullptr
v3.0.0rc1,save to nullptr
v3.0.0rc1,default weight file name
v3.0.0rc1,default init_score file name
v3.0.0rc1,use first line to count number class
v3.0.0rc1,default query file name
v3.0.0rc1,root is in the depth 0
v3.0.0rc1,non-leaf
v3.0.0rc1,leaf
v3.0.0rc1,use this for the missing value conversion
v3.0.0rc1,Predict func by Map to ifelse
v3.0.0rc1,use this for the missing value conversion
v3.0.0rc1,non-leaf
v3.0.0rc1,left subtree
v3.0.0rc1,right subtree
v3.0.0rc1,leaf
v3.0.0rc1,non-leaf
v3.0.0rc1,left subtree
v3.0.0rc1,right subtree
v3.0.0rc1,leaf
v3.0.0rc1,recursive computation of SHAP values for a decision tree
v3.0.0rc1,extend the unique path
v3.0.0rc1,leaf node
v3.0.0rc1,internal node
v3.0.0rc1,"see if we have already split on this feature,"
v3.0.0rc1,if so we undo that split so we can redo it for this node
v3.0.0rc1,recursive sparse computation of SHAP values for a decision tree
v3.0.0rc1,extend the unique path
v3.0.0rc1,leaf node
v3.0.0rc1,internal node
v3.0.0rc1,"see if we have already split on this feature,"
v3.0.0rc1,if so we undo that split so we can redo it for this node
v3.0.0rc1,add names of objective function if not providing metric
v3.0.0rc1,equal weights for all classes
v3.0.0rc1,generate seeds by seed.
v3.0.0rc1,sort eval_at
v3.0.0rc1,Only push the non-training data
v3.0.0rc1,check for conflicts
v3.0.0rc1,"check if objective, metric, and num_class match"
v3.0.0rc1,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v3.0.0rc1,Check max_depth and num_leaves
v3.0.0rc1,"Fits in an int, and is more restrictive than the current num_leaves"
v3.0.0rc1,force col-wise for gpu
v3.0.0rc1,min_data_in_leaf must be at least 2 if path smoothing is active. This is because when the split is calculated
v3.0.0rc1,"the count is calculated using the proportion of hessian in the leaf which is rounded up to nearest int, so it can"
v3.0.0rc1,be 1 when there is actually no data in the leaf. In rare cases this can cause a bug because with path smoothing the
v3.0.0rc1,calculated split gain can be positive even with zero gradient and hessian.
v3.0.0rc1,"In distributed mode, local node doesn't have histograms on all features, cannot perform ""intermediate"" monotone constraints."
v3.0.0rc1,"""intermediate"" monotone constraints need to recompute splits. If the features are sampled when computing the"
v3.0.0rc1,"split initially, then the sampling needs to be recorded or done once again, which is currently not supported"
v3.0.0rc1,first round: fill the single val group
v3.0.0rc1,always push the last group
v3.0.0rc1,put dense feature first
v3.0.0rc1,sort by non zero cnt
v3.0.0rc1,"sort by non zero cnt, bigger first"
v3.0.0rc1,shuffle groups
v3.0.0rc1,Using std::swap for vector<bool> will cause the wrong result.
v3.0.0rc1,get num_features
v3.0.0rc1,get bin_mappers
v3.0.0rc1,copy feature bin mapper data
v3.0.0rc1,copy feature bin mapper data
v3.0.0rc1,"if not pass a filename, just append "".bin"" of original file"
v3.0.0rc1,get size of header
v3.0.0rc1,size of feature names
v3.0.0rc1,size of forced bins
v3.0.0rc1,write header
v3.0.0rc1,write feature names
v3.0.0rc1,write forced bins
v3.0.0rc1,get size of meta data
v3.0.0rc1,write meta data
v3.0.0rc1,write feature data
v3.0.0rc1,get size of feature
v3.0.0rc1,write feature
v3.0.0rc1,only need to copy subset
v3.0.0rc1,avoid to copy subset many times
v3.0.0rc1,avoid out of range
v3.0.0rc1,may need to recopy subset
v3.0.0rc1,"explicitly initilize template methods, for cross module call"
v3.0.0rc1,"FIXME: fix the multiple multi-val feature groups, they need to be merged"
v3.0.0rc1,into one multi-val group
v3.0.0rc1,Skip the leading 0 when copying group_bin_boundaries.
v3.0.0rc1,store the importance first
v3.0.0rc1,PredictRaw
v3.0.0rc1,PredictRawByMap
v3.0.0rc1,Predict
v3.0.0rc1,PredictByMap
v3.0.0rc1,PredictLeafIndex
v3.0.0rc1,PredictLeafIndexByMap
v3.0.0rc1,output model type
v3.0.0rc1,output number of class
v3.0.0rc1,output label index
v3.0.0rc1,output max_feature_idx
v3.0.0rc1,output objective
v3.0.0rc1,output tree models
v3.0.0rc1,store the importance first
v3.0.0rc1,sort the importance
v3.0.0rc1,use serialized string to restore this object
v3.0.0rc1,Use first 128 chars to avoid exceed the message buffer.
v3.0.0rc1,get number of classes
v3.0.0rc1,get index of label
v3.0.0rc1,get max_feature_idx first
v3.0.0rc1,get average_output
v3.0.0rc1,get feature names
v3.0.0rc1,get monotone_constraints
v3.0.0rc1,set zero
v3.0.0rc1,predict all the trees for one iteration
v3.0.0rc1,check early stopping
v3.0.0rc1,set zero
v3.0.0rc1,predict all the trees for one iteration
v3.0.0rc1,check early stopping
v3.0.0rc1,margin_threshold will be captured by value
v3.0.0rc1,copy and sort
v3.0.0rc1,margin_threshold will be captured by value
v3.0.0rc1,Fix for compiler warnings about reaching end of control
v3.0.0rc1,load forced_splits file
v3.0.0rc1,init tree learner
v3.0.0rc1,push training metrics
v3.0.0rc1,create buffer for gradients and hessians
v3.0.0rc1,get max feature index
v3.0.0rc1,get label index
v3.0.0rc1,get feature names
v3.0.0rc1,"if need bagging, create buffer"
v3.0.0rc1,"for a validation dataset, we need its score and metric"
v3.0.0rc1,update score
v3.0.0rc1,objective function will calculate gradients and hessians
v3.0.0rc1,"random bagging, minimal unit is one record"
v3.0.0rc1,"random bagging, minimal unit is one record"
v3.0.0rc1,if need bagging
v3.0.0rc1,set bagging data to tree learner
v3.0.0rc1,get subset
v3.0.0rc1,output used time per iteration
v3.0.0rc1,"boosting from average label; or customized ""average"" if implemented for the current objective"
v3.0.0rc1,boosting first
v3.0.0rc1,bagging logic
v3.0.0rc1,need to copy gradients for bagging subset.
v3.0.0rc1,shrinkage by learning rate
v3.0.0rc1,update score
v3.0.0rc1,only add default score one-time
v3.0.0rc1,updates scores
v3.0.0rc1,add model
v3.0.0rc1,reset score
v3.0.0rc1,remove model
v3.0.0rc1,print message for metric
v3.0.0rc1,pop last early_stopping_round_ models
v3.0.0rc1,update training score
v3.0.0rc1,we need to predict out-of-bag scores of data for boosting
v3.0.0rc1,update validation score
v3.0.0rc1,print training metric
v3.0.0rc1,print validation metric
v3.0.0rc1,set zero
v3.0.0rc1,predict all the trees for one iteration
v3.0.0rc1,predict all the trees for one iteration
v3.0.0rc1,push training metrics
v3.0.0rc1,"not same training data, need reset score and others"
v3.0.0rc1,create score tracker
v3.0.0rc1,update score
v3.0.0rc1,create buffer for gradients and hessians
v3.0.0rc1,load forced_splits file
v3.0.0rc1,"if need bagging, create buffer"
v3.0.0rc1,Get the max size of pool
v3.0.0rc1,at least need 2 leaves
v3.0.0rc1,push split information for all leaves
v3.0.0rc1,initialize splits for leaf
v3.0.0rc1,initialize data partition
v3.0.0rc1,initialize ordered gradients and hessians
v3.0.0rc1,cannot change is_hist_col_wise during training
v3.0.0rc1,initialize splits for leaf
v3.0.0rc1,initialize data partition
v3.0.0rc1,initialize ordered gradients and hessians
v3.0.0rc1,Get the max size of pool
v3.0.0rc1,at least need 2 leaves
v3.0.0rc1,push split information for all leaves
v3.0.0rc1,some initial works before training
v3.0.0rc1,root leaf
v3.0.0rc1,only root leaf can be splitted on first time
v3.0.0rc1,some initial works before finding best split
v3.0.0rc1,find best threshold for every feature
v3.0.0rc1,Get a leaf with max split gain
v3.0.0rc1,Get split information for best leaf
v3.0.0rc1,"cannot split, quit"
v3.0.0rc1,split tree with best leaf
v3.0.0rc1,reset histogram pool
v3.0.0rc1,initialize data partition
v3.0.0rc1,reset the splits for leaves
v3.0.0rc1,Sumup for root
v3.0.0rc1,use all data
v3.0.0rc1,"use bagging, only use part of data"
v3.0.0rc1,check depth of current leaf
v3.0.0rc1,"only need to check left leaf, since right leaf is in same level of left leaf"
v3.0.0rc1,no enough data to continue
v3.0.0rc1,only have root
v3.0.0rc1,put parent(left) leaf's histograms into larger leaf's histograms
v3.0.0rc1,put parent(left) leaf's histograms to larger leaf's histograms
v3.0.0rc1,construct smaller leaf
v3.0.0rc1,construct larger leaf
v3.0.0rc1,find splits
v3.0.0rc1,only has root leaf
v3.0.0rc1,start at root leaf
v3.0.0rc1,"before processing next node from queue, store info for current left/right leaf"
v3.0.0rc1,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v3.0.0rc1,"then, compute own splits"
v3.0.0rc1,split info should exist because searching in bfs fashion - should have added from parent
v3.0.0rc1,update before tree split
v3.0.0rc1,don't need to update this in data-based parallel model
v3.0.0rc1,"split tree, will return right leaf"
v3.0.0rc1,store the true split gain in tree model
v3.0.0rc1,don't need to update this in data-based parallel model
v3.0.0rc1,store the true split gain in tree model
v3.0.0rc1,init the leaves that used on next iteration
v3.0.0rc1,update leave outputs if needed
v3.0.0rc1,bag_mapper[index_mapper[i]]
v3.0.0rc1,"for root leaf the ""parent"" output is its own output because we don't apply any smoothing to the root"
v3.0.0rc1,find splits
v3.0.0rc1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.0.0rc1,get feature partition
v3.0.0rc1,get local used features
v3.0.0rc1,get best split at smaller leaf
v3.0.0rc1,find local best split for larger leaf
v3.0.0rc1,sync global best info
v3.0.0rc1,update best split
v3.0.0rc1,"instantiate template classes, otherwise linker cannot find the code"
v3.0.0rc1,initialize SerialTreeLearner
v3.0.0rc1,Get local rank and global machine size
v3.0.0rc1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.0.0rc1,allocate buffer for communication
v3.0.0rc1,generate feature partition for current tree
v3.0.0rc1,get local used feature
v3.0.0rc1,get block start and block len for reduce scatter
v3.0.0rc1,get buffer_write_start_pos_
v3.0.0rc1,get buffer_read_start_pos_
v3.0.0rc1,sync global data sumup info
v3.0.0rc1,global sumup reduce
v3.0.0rc1,copy back
v3.0.0rc1,set global sumup info
v3.0.0rc1,init global data count in leaf
v3.0.0rc1,construct local histograms
v3.0.0rc1,copy to buffer
v3.0.0rc1,Reduce scatter for histogram
v3.0.0rc1,restore global histograms from buffer
v3.0.0rc1,only root leaf
v3.0.0rc1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v3.0.0rc1,find local best split for larger leaf
v3.0.0rc1,sync global best info
v3.0.0rc1,set best split
v3.0.0rc1,need update global number of data in leaf
v3.0.0rc1,"instantiate template classes, otherwise linker cannot find the code"
v3.0.0rc1,initialize SerialTreeLearner
v3.0.0rc1,some additional variables needed for GPU trainer
v3.0.0rc1,Initialize GPU buffers and kernels
v3.0.0rc1,some functions used for debugging the GPU histogram construction
v3.0.0rc1,"printf(""grad %g != %g (%d ULPs)\n"", GET_GRAD(h1, i), GET_GRAD(h2, i), ulps);"
v3.0.0rc1,goto err;
v3.0.0rc1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v3.0.0rc1,also guarantee that there are at least 2K examples per workgroup
v3.0.0rc1,return 0;
v3.0.0rc1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v3.0.0rc1,decide the best number of workgroups working on one feature4 tuple
v3.0.0rc1,set work group size based on feature size
v3.0.0rc1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v3.0.0rc1,we need to refresh the kernel arguments after reallocating
v3.0.0rc1,The only argument that needs to be changed later is num_data_
v3.0.0rc1,"the GPU kernel will process all features in one call, and each"
v3.0.0rc1,2^exp_workgroups_per_feature (compile time constant) workgroup will
v3.0.0rc1,process one feature4 tuple
v3.0.0rc1,"for the root node, indices are not copied"
v3.0.0rc1,"for constant hessian, hessians are not copied except for the root node"
v3.0.0rc1,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v3.0.0rc1,and we will launch num_feature workgroups for this kernel
v3.0.0rc1,will launch threads for all features
v3.0.0rc1,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v3.0.0rc1,copy the results asynchronously. Size depends on if double precision is used
v3.0.0rc1,we will wait for this object in WaitAndGetHistograms
v3.0.0rc1,"when the output is ready, the computation is done"
v3.0.0rc1,values of this feature has been redistributed to multiple bins; need a reduction here
v3.0.0rc1,how many feature-group tuples we have
v3.0.0rc1,leave some safe margin for prefetching
v3.0.0rc1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v3.0.0rc1,clear sparse/dense maps
v3.0.0rc1,do nothing if no features can be processed on GPU
v3.0.0rc1,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v3.0.0rc1,unpin old buffer if necessary before destructing them
v3.0.0rc1,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v3.0.0rc1,allocate space for gradients and hessians on device
v3.0.0rc1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v3.0.0rc1,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v3.0.0rc1,copy indices to the device
v3.0.0rc1,histogram bin entry size depends on the precision (single/double)
v3.0.0rc1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v3.0.0rc1,each work group generates a sub-histogram of dword_features_ features.
v3.0.0rc1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v3.0.0rc1,create atomic counters for inter-group coordination
v3.0.0rc1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v3.0.0rc1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v3.0.0rc1,looking for dword_features_ non-sparse feature-groups
v3.0.0rc1,decide if we need to redistribute the bin
v3.0.0rc1,multiplier must be a power of 2
v3.0.0rc1,device_bin_mults_.push_back(1);
v3.0.0rc1,found
v3.0.0rc1,for data transfer time
v3.0.0rc1,"Now generate new data structure feature4, and copy data to the device"
v3.0.0rc1,"preallocate arrays for all threads, and pin them"
v3.0.0rc1,building Feature4 bundles; each thread handles dword_features_ features
v3.0.0rc1,one feature datapoint is 4 bits
v3.0.0rc1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.0.0rc1,one feature datapoint is one byte
v3.0.0rc1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v3.0.0rc1,Dense bin
v3.0.0rc1,Dense 4-bit bin
v3.0.0rc1,working on the remaining (less than dword_features_) feature groups
v3.0.0rc1,fill the leftover features
v3.0.0rc1,"fill this empty feature with some ""random"" value"
v3.0.0rc1,"fill this empty feature with some ""random"" value"
v3.0.0rc1,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v3.0.0rc1,deallocate pinned space for feature copying
v3.0.0rc1,data transfer time
v3.0.0rc1,"for other types of failure, build log might not be available; program.build_log() can crash"
v3.0.0rc1,"Something bad happened. Just return ""No log available."""
v3.0.0rc1,"build is okay, log may contain warnings"
v3.0.0rc1,destroy any old kernels
v3.0.0rc1,create OpenCL kernels for different number of workgroups per feature
v3.0.0rc1,currently we don't use constant memory
v3.0.0rc1,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v3.0.0rc1,kernel with indices in an array
v3.0.0rc1,"kernel with all features enabled, with elimited branches"
v3.0.0rc1,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v3.0.0rc1,do nothing if no features can be processed on GPU
v3.0.0rc1,The only argument that needs to be changed later is num_data_
v3.0.0rc1,"hessian is passed as a parameter, but it is not available now."
v3.0.0rc1,hessian will be set in BeforeTrain()
v3.0.0rc1,"Get the max bin size, used for selecting best GPU kernel"
v3.0.0rc1,initialize GPU
v3.0.0rc1,determine which kernel to use based on the max number of bins
v3.0.0rc1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.0.0rc1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.0.0rc1,"the +9 skips extra characters "")"", newline, ""#endif"" and newline at the beginning"
v3.0.0rc1,setup GPU kernel arguments after we allocating all the buffers
v3.0.0rc1,GPU memory has to been reallocated because data may have been changed
v3.0.0rc1,setup GPU kernel arguments after we allocating all the buffers
v3.0.0rc1,Copy initial full hessians and gradients to GPU.
v3.0.0rc1,"We start copying as early as possible, instead of at ConstructHistogram()."
v3.0.0rc1,setup hessian parameters only
v3.0.0rc1,hessian is passed as a parameter
v3.0.0rc1,use bagging
v3.0.0rc1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v3.0.0rc1,copy used gradients and hessians to ordered buffer
v3.0.0rc1,transfer the indices to GPU
v3.0.0rc1,transfer hessian to GPU
v3.0.0rc1,setup hessian parameters only
v3.0.0rc1,hessian is passed as a parameter
v3.0.0rc1,transfer gradients to GPU
v3.0.0rc1,only have root
v3.0.0rc1,"Copy indices, gradients and hessians as early as possible"
v3.0.0rc1,only need to initialize for smaller leaf
v3.0.0rc1,Get leaf boundary
v3.0.0rc1,copy indices to the GPU:
v3.0.0rc1,copy ordered hessians to the GPU:
v3.0.0rc1,copy ordered gradients to the GPU:
v3.0.0rc1,do nothing if no features can be processed on GPU
v3.0.0rc1,copy data indices if it is not null
v3.0.0rc1,generate and copy ordered_gradients if gradients is not null
v3.0.0rc1,generate and copy ordered_hessians if hessians is not null
v3.0.0rc1,converted indices in is_feature_used to feature-group indices
v3.0.0rc1,construct the feature masks for dense feature-groups
v3.0.0rc1,"if no feature group is used, just return and do not use GPU"
v3.0.0rc1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v3.0.0rc1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v3.0.0rc1,"All data have been prepared, now run the GPU kernel"
v3.0.0rc1,construct smaller leaf
v3.0.0rc1,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v3.0.0rc1,then construct sparse features on CPU
v3.0.0rc1,"wait for GPU to finish, only if GPU is actually used"
v3.0.0rc1,use double precision
v3.0.0rc1,use single precision
v3.0.0rc1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v3.0.0rc1,#define GPU_DEBUG_COMPARE
v3.0.0rc1,construct larger leaf
v3.0.0rc1,then construct sparse features on CPU
v3.0.0rc1,"wait for GPU to finish, only if GPU is actually used"
v3.0.0rc1,use double precision
v3.0.0rc1,use single precision
v3.0.0rc1,do some sanity check for the GPU algorithm
v3.0.0rc1,limit top k
v3.0.0rc1,get max bin
v3.0.0rc1,calculate buffer size
v3.0.0rc1,need to be able to hold smaller and larger best splits in SyncUpGlobalBestSplit
v3.0.0rc1,"left and right on same time, so need double size"
v3.0.0rc1,initialize histograms for global
v3.0.0rc1,sync global data sumup info
v3.0.0rc1,set global sumup info
v3.0.0rc1,init global data count in leaf
v3.0.0rc1,get local sumup
v3.0.0rc1,get local sumup
v3.0.0rc1,get mean number on machines
v3.0.0rc1,weighted gain
v3.0.0rc1,get top k
v3.0.0rc1,"Copy histogram to buffer, and Get local aggregate features"
v3.0.0rc1,copy histograms.
v3.0.0rc1,copy smaller leaf histograms first
v3.0.0rc1,mark local aggregated feature
v3.0.0rc1,copy
v3.0.0rc1,then copy larger leaf histograms
v3.0.0rc1,mark local aggregated feature
v3.0.0rc1,copy
v3.0.0rc1,use local data to find local best splits
v3.0.0rc1,find splits
v3.0.0rc1,only has root leaf
v3.0.0rc1,local voting
v3.0.0rc1,gather
v3.0.0rc1,get all top-k from all machines
v3.0.0rc1,global voting
v3.0.0rc1,copy local histgrams to buffer
v3.0.0rc1,Reduce scatter for histogram
v3.0.0rc1,find best split from local aggregated histograms
v3.0.0rc1,restore from buffer
v3.0.0rc1,restore from buffer
v3.0.0rc1,find local best
v3.0.0rc1,find local best split for larger leaf
v3.0.0rc1,sync global best info
v3.0.0rc1,copy back
v3.0.0rc1,set the global number of data for leaves
v3.0.0rc1,init the global sumup info
v3.0.0rc1,"instantiate template classes, otherwise linker cannot find the code"
v2.3.1,coding: utf-8
v2.3.1,Apple Clang with OpenMP
v2.3.1,coding: utf-8
v2.3.1,create predictor first
v2.3.1,check dataset
v2.3.1,reduce cost for prediction training data
v2.3.1,process callbacks
v2.3.1,Most of legacy advanced options becomes callbacks
v2.3.1,construct booster
v2.3.1,start training
v2.3.1,check evaluation result.
v2.3.1,"lambdarank task, split according to groups"
v2.3.1,run preprocessing on the data set if needed
v2.3.1,setup callbacks
v2.3.1,coding: utf-8
v2.3.1,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.3.1,because of u'...' Unicode literals.
v2.3.1,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.3.1,coding: utf-8
v2.3.1,minor change to support `**kwargs`
v2.3.1,"user can set verbose with kwargs, it has higher priority"
v2.3.1,register default metric for consistency with callable eval_metric case
v2.3.1,try to deduce from class instance
v2.3.1,overwrite default metric by explicitly set metric
v2.3.1,concatenate metric from params (or default if not provided in params) and eval_metric
v2.3.1,reduce cost for prediction training data
v2.3.1,free dataset
v2.3.1,Switch to using a multiclass objective in the underlying LGBM instance
v2.3.1,check group data
v2.3.1,coding: utf-8
v2.3.1,we don't need lib_lightgbm while building docs
v2.3.1,coding: utf-8
v2.3.1,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v2.3.1,coding: utf-8
v2.3.1,coding: utf-8
v2.3.1,TypeError: obj is not a string or a number
v2.3.1,ValueError: invalid literal
v2.3.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.3.1,avoid memory consumption by arrays concatenation operations
v2.3.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.3.1,avoid memory consumption by arrays concatenation operations
v2.3.1,check data has header or not
v2.3.1,need to regroup init_score
v2.3.1,process for args
v2.3.1,"user can set verbose with params, it has higher priority"
v2.3.1,get categorical features
v2.3.1,process for reference dataset
v2.3.1,start construct data
v2.3.1,set feature names
v2.3.1,create valid
v2.3.1,construct subset
v2.3.1,create train
v2.3.1,set to None
v2.3.1,we're done if self and reference share a common upstrem reference
v2.3.1,"group data from LightGBM is boundaries data, need to convert to group size"
v2.3.1,"user can set verbose with params, it has higher priority"
v2.3.1,Training task
v2.3.1,set network if necessary
v2.3.1,construct booster object
v2.3.1,save reference to data
v2.3.1,buffer for inner predict
v2.3.1,Prediction task
v2.3.1,need reset training data
v2.3.1,need to push new valid data
v2.3.1,"if buffer length is not long enough, re-allocate a buffer"
v2.3.1,"if buffer length is not long enough, reallocate a buffer"
v2.3.1,Copy models
v2.3.1,Get name of features
v2.3.1,avoid to predict many time in one iteration
v2.3.1,Get num of inner evals
v2.3.1,Get name of evals
v2.3.1,coding: utf-8
v2.3.1,Callback environment used by callbacks
v2.3.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v2.3.1,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v2.3.1,coding: utf-8
v2.3.1,load or create your dataset
v2.3.1,create dataset for lightgbm
v2.3.1,"if you want to re-use data, remember to set free_raw_data=False"
v2.3.1,specify your configurations as a dict
v2.3.1,generate feature names
v2.3.1,feature_name and categorical_feature
v2.3.1,check feature name
v2.3.1,save model to file
v2.3.1,dump model to JSON (and save to file)
v2.3.1,feature names
v2.3.1,feature importances
v2.3.1,load model to predict
v2.3.1,can only predict with the best iteration (or the saving iteration)
v2.3.1,eval with loaded model
v2.3.1,dump model with pickle
v2.3.1,load model with pickle to predict
v2.3.1,can predict with any iteration when loaded in pickle way
v2.3.1,eval with loaded model
v2.3.1,continue training
v2.3.1,init_model accepts:
v2.3.1,1. model file name
v2.3.1,2. Booster()
v2.3.1,decay learning rates
v2.3.1,learning_rates accepts:
v2.3.1,1. list/tuple with length = num_boost_round
v2.3.1,2. function(curr_iter)
v2.3.1,change other parameters during training
v2.3.1,self-defined objective function
v2.3.1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.3.1,log likelihood loss
v2.3.1,self-defined eval metric
v2.3.1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.1,binary error
v2.3.1,another self-defined eval metric
v2.3.1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.1,accuracy
v2.3.1,callback
v2.3.1,coding: utf-8
v2.3.1,load or create your dataset
v2.3.1,train
v2.3.1,predict
v2.3.1,eval
v2.3.1,feature importances
v2.3.1,self-defined eval metric
v2.3.1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.1,Root Mean Squared Logarithmic Error (RMSLE)
v2.3.1,train
v2.3.1,another self-defined eval metric
v2.3.1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.1,Relative Absolute Error (RAE)
v2.3.1,train
v2.3.1,predict
v2.3.1,eval
v2.3.1,other scikit-learn modules
v2.3.1,coding: utf-8
v2.3.1,load or create your dataset
v2.3.1,create dataset for lightgbm
v2.3.1,specify your configurations as a dict
v2.3.1,train
v2.3.1,coding: utf-8
v2.3.1,################
v2.3.1,Simulate some binary data with a single categorical and
v2.3.1,single continuous predictor
v2.3.1,################
v2.3.1,Set up a couple of utilities for our experiments
v2.3.1,################
v2.3.1,Observe the behavior of `binary` and `xentropy` objectives
v2.3.1,Trying this throws an error on non-binary values of y:
v2.3.1,"experiment('binary', label_type='probability', DATA)"
v2.3.1,The speed of `binary` is not drastically different than
v2.3.1,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v2.3.1,there are reasons to suspect that `binary` should run faster when the
v2.3.1,label is an integer instead of a float
v2.3.1,coding: utf-8
v2.3.1,load or create your dataset
v2.3.1,create dataset for lightgbm
v2.3.1,specify your configurations as a dict
v2.3.1,train
v2.3.1,save model to file
v2.3.1,predict
v2.3.1,eval
v2.3.1,coding: utf-8
v2.3.1,!/usr/bin/env python3
v2.3.1,-*- coding: utf-8 -*-
v2.3.1,
v2.3.1,"LightGBM documentation build configuration file, created by"
v2.3.1,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.3.1,
v2.3.1,This file is execfile()d with the current directory set to its
v2.3.1,containing dir.
v2.3.1,
v2.3.1,Note that not all possible configuration values are present in this
v2.3.1,autogenerated file.
v2.3.1,
v2.3.1,All configuration values have a default; values that are commented out
v2.3.1,serve to show the default.
v2.3.1,"If extensions (or modules to document with autodoc) are in another directory,"
v2.3.1,add these directories to sys.path here. If the directory is relative to the
v2.3.1,"documentation root, use os.path.abspath to make it absolute."
v2.3.1,-- mock out modules
v2.3.1,-- General configuration ------------------------------------------------
v2.3.1,"If your documentation needs a minimal Sphinx version, state it here."
v2.3.1,"Add any Sphinx extension module names here, as strings. They can be"
v2.3.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.3.1,ones.
v2.3.1,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v2.3.1,Only the class' docstring is inserted.
v2.3.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.3.1,The master toctree document.
v2.3.1,General information about the project.
v2.3.1,"The version info for the project you're documenting, acts as replacement for"
v2.3.1,"|version| and |release|, also used in various other places throughout the"
v2.3.1,built documents.
v2.3.1,The short X.Y version.
v2.3.1,"The full version, including alpha/beta/rc tags."
v2.3.1,The language for content autogenerated by Sphinx. Refer to documentation
v2.3.1,for a list of supported languages.
v2.3.1,
v2.3.1,This is also used if you do content translation via gettext catalogs.
v2.3.1,"Usually you set ""language"" from the command line for these cases."
v2.3.1,"List of patterns, relative to source directory, that match files and"
v2.3.1,directories to ignore when looking for source files.
v2.3.1,This patterns also effect to html_static_path and html_extra_path
v2.3.1,The name of the Pygments (syntax highlighting) style to use.
v2.3.1,-- Configuration for C API docs generation ------------------------------
v2.3.1,-- Options for HTML output ----------------------------------------------
v2.3.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.3.1,a list of builtin themes.
v2.3.1,Theme options are theme-specific and customize the look and feel of a theme
v2.3.1,"further.  For a list of options available for each theme, see the"
v2.3.1,documentation.
v2.3.1,"Add any paths that contain custom static files (such as style sheets) here,"
v2.3.1,"relative to this directory. They are copied after the builtin static files,"
v2.3.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.3.1,-- Options for HTMLHelp output ------------------------------------------
v2.3.1,Output file base name for HTML help builder.
v2.3.1,Warning! The following code can cause buffer overflows on RTD.
v2.3.1,Consider suppressing output completely if RTD project silently fails.
v2.3.1,Refer to https://github.com/svenevs/exhale
v2.3.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v2.3.1,Warning! The following code can cause buffer overflows on RTD.
v2.3.1,Consider suppressing output completely if RTD project silently fails.
v2.3.1,Refer to https://github.com/svenevs/exhale
v2.3.1,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v2.3.1,coding: utf-8
v2.3.1,coding: utf-8
v2.3.1,we don't need lib_lightgbm while building docs
v2.3.1,coding: utf-8
v2.3.1,check saved model persistence
v2.3.1,"we need to check the consistency of model file here, so test for exact equal"
v2.3.1,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.3.1,"scores likely to be different, but prediction should still be the same"
v2.3.1,test that shape is checked during prediction
v2.3.1,"Set extremely harsh penalties, so CEGB will block most splits."
v2.3.1,"Compare pairs of penalties, to ensure scaling works as intended"
v2.3.1,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v2.3.1,coding: utf-8
v2.3.1,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v2.3.1,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.3.1,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.3.1,we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)
v2.3.1,With default params
v2.3.1,Tests same probabilities
v2.3.1,Tests same predictions
v2.3.1,Tests same raw scores
v2.3.1,Tests same leaf indices
v2.3.1,Tests same feature contributions
v2.3.1,Tests other parameters for the prediction works
v2.3.1,"no custom objective, no custom metric"
v2.3.1,default metric
v2.3.1,non-default metric
v2.3.1,no metric
v2.3.1,non-default metric in eval_metric
v2.3.1,non-default metric with non-default metric in eval_metric
v2.3.1,non-default metric with multiple metrics in eval_metric
v2.3.1,default metric for non-default objective
v2.3.1,non-default metric for non-default objective
v2.3.1,no metric
v2.3.1,non-default metric in eval_metric for non-default objective
v2.3.1,non-default metric with non-default metric in eval_metric for non-default objective
v2.3.1,non-default metric with multiple metrics in eval_metric for non-default objective
v2.3.1,"custom objective, no custom metric"
v2.3.1,default regression metric for custom objective
v2.3.1,non-default regression metric for custom objective
v2.3.1,multiple regression metrics for custom objective
v2.3.1,no metric
v2.3.1,default regression metric with non-default metric in eval_metric for custom objective
v2.3.1,non-default regression metric with metric in eval_metric for custom objective
v2.3.1,multiple regression metrics with metric in eval_metric for custom objective
v2.3.1,multiple regression metrics with multiple metrics in eval_metric for custom objective
v2.3.1,"no custom objective, custom metric"
v2.3.1,default metric with custom metric
v2.3.1,non-default metric with custom metric
v2.3.1,multiple metrics with custom metric
v2.3.1,custom metric (disable default metric)
v2.3.1,default metric for non-default objective with custom metric
v2.3.1,non-default metric for non-default objective with custom metric
v2.3.1,multiple metrics for non-default objective with custom metric
v2.3.1,custom metric (disable default metric for non-default objective)
v2.3.1,"custom objective, custom metric"
v2.3.1,custom metric for custom objective
v2.3.1,non-default regression metric with custom metric for custom objective
v2.3.1,multiple regression metrics with custom metric for custom objective
v2.3.1,default metric and invalid binary metric is replaced with multiclass alternative
v2.3.1,invalid objective is replaced with default multiclass one
v2.3.1,and invalid binary metric is replaced with multiclass alternative
v2.3.1,default metric for non-default multiclass objective
v2.3.1,and invalid binary metric is replaced with multiclass alternative
v2.3.1,default metric and invalid multiclass metric is replaced with binary alternative
v2.3.1,invalid multiclass metric is replaced with binary alternative for custom objective
v2.3.1,training data as eval_set
v2.3.1,feval
v2.3.1,single eval_set
v2.3.1,two eval_set
v2.3.1,coding: utf-8
v2.3.1,coding: utf-8
v2.3.1,check that default gives same result as k = 1
v2.3.1,check against independent calculation for k = 1
v2.3.1,check against independent calculation for k = 2
v2.3.1,check against independent calculation for k = 10
v2.3.1,check cases where predictions are equal
v2.3.1,no early stopping
v2.3.1,early stopping occurs
v2.3.1,test custom eval metrics
v2.3.1,"shuffle = False, override metric in params"
v2.3.1,"shuffle = True, callbacks"
v2.3.1,enable display training loss
v2.3.1,self defined folds
v2.3.1,lambdarank
v2.3.1,... with l2 metric
v2.3.1,... with NDCG (default) metric
v2.3.1,self defined folds with lambdarank
v2.3.1,test feature_names with whitespaces
v2.3.1,take subsets and train
v2.3.1,test sliced labels
v2.3.1,append some columns
v2.3.1,append some rows
v2.3.1,test sliced 2d matrix
v2.3.1,test sliced CSR
v2.3.1,"no fobj, no feval"
v2.3.1,default metric
v2.3.1,non-default metric in params
v2.3.1,default metric in args
v2.3.1,non-default metric in args
v2.3.1,metric in args overwrites one in params
v2.3.1,multiple metrics in params
v2.3.1,multiple metrics in args
v2.3.1,remove default metric by 'None' in list
v2.3.1,remove default metric by 'None' aliases
v2.3.1,"fobj, no feval"
v2.3.1,no default metric
v2.3.1,metric in params
v2.3.1,metric in args
v2.3.1,metric in args overwrites its' alias in params
v2.3.1,multiple metrics in params
v2.3.1,multiple metrics in args
v2.3.1,"no fobj, feval"
v2.3.1,default metric with custom one
v2.3.1,non-default metric in params with custom one
v2.3.1,default metric in args with custom one
v2.3.1,non-default metric in args with custom one
v2.3.1,"metric in args overwrites one in params, custom one is evaluated too"
v2.3.1,multiple metrics in params with custom one
v2.3.1,multiple metrics in args with custom one
v2.3.1,custom metric is evaluated despite 'None' is passed
v2.3.1,"fobj, feval"
v2.3.1,"no default metric, only custom one"
v2.3.1,metric in params with custom one
v2.3.1,metric in args with custom one
v2.3.1,"metric in args overwrites one in params, custom one is evaluated too"
v2.3.1,multiple metrics in params with custom one
v2.3.1,multiple metrics in args with custom one
v2.3.1,custom metric is evaluated despite 'None' is passed
v2.3.1,"no fobj, no feval"
v2.3.1,default metric
v2.3.1,default metric in params
v2.3.1,non-default metric in params
v2.3.1,multiple metrics in params
v2.3.1,remove default metric by 'None' aliases
v2.3.1,"fobj, no feval"
v2.3.1,no default metric
v2.3.1,metric in params
v2.3.1,multiple metrics in params
v2.3.1,"no fobj, feval"
v2.3.1,default metric with custom one
v2.3.1,default metric in params with custom one
v2.3.1,non-default metric in params with custom one
v2.3.1,multiple metrics in params with custom one
v2.3.1,custom metric is evaluated despite 'None' is passed
v2.3.1,"fobj, feval"
v2.3.1,"no default metric, only custom one"
v2.3.1,metric in params with custom one
v2.3.1,multiple metrics in params with custom one
v2.3.1,custom metric is evaluated despite 'None' is passed
v2.3.1,multiclass default metric
v2.3.1,multiclass default metric with custom one
v2.3.1,multiclass metric alias with custom one for custom objective
v2.3.1,no metric for invalid class_num
v2.3.1,custom metric for invalid class_num
v2.3.1,multiclass metric alias with custom one with invalid class_num
v2.3.1,multiclass default metric without num_class
v2.3.1,multiclass metric alias
v2.3.1,multiclass metric
v2.3.1,non-valid metric for multiclass objective
v2.3.1,non-default num_class for default objective
v2.3.1,no metric with non-default num_class for custom objective
v2.3.1,multiclass metric alias for custom objective
v2.3.1,multiclass metric for custom objective
v2.3.1,binary metric with non-default num_class for custom objective
v2.3.1,test XGBoost-style return value
v2.3.1,test numpy-style return value
v2.3.1,test bins string type
v2.3.1,test histogram is disabled for categorical features
v2.3.1,test for lgb.train
v2.3.1,test feval for lgb.train
v2.3.1,test with two valid data for lgb.train
v2.3.1,test for lgb.cv
v2.3.1,test feval for lgb.cv
v2.3.1,test that binning works properly for features with only positive or only negative values
v2.3.1,coding: utf-8
v2.3.1,coding: utf-8
v2.3.1,alias table
v2.3.1,names
v2.3.1,from strings
v2.3.1,tails
v2.3.1,tails
v2.3.1,coding: utf-8
v2.3.1,convert from one-based to  zero-based index
v2.3.1,convert from boundaries to size
v2.3.1,--- start Booster interfaces
v2.3.1,Single row predictor to abstract away caching logic
v2.3.1,create boosting
v2.3.1,initialize the boosting
v2.3.1,create objective function
v2.3.1,initialize the objective function
v2.3.1,create training metric
v2.3.1,reset the boosting
v2.3.1,create objective function
v2.3.1,initialize the objective function
v2.3.1,some help functions used to convert data
v2.3.1,Row iterator of on column for CSC matrix
v2.3.1,"return value at idx, only can access by ascent order"
v2.3.1,"return next non-zero pair, if index < 0, means no more data"
v2.3.1,start of c_api functions
v2.3.1,sample data first
v2.3.1,sample data first
v2.3.1,sample data first
v2.3.1,local buffer to re-use memory
v2.3.1,sample data first
v2.3.1,no more data
v2.3.1,---- start of booster
v2.3.1,---- start of some help functions
v2.3.1,data is array of pointers to individual rows
v2.3.1,set number of threads for openmp
v2.3.1,check for alias
v2.3.1,read parameters from config file
v2.3.1,"remove str after ""#"""
v2.3.1,check for alias again
v2.3.1,load configs
v2.3.1,prediction is needed if using input initial model(continued train)
v2.3.1,need to continue training
v2.3.1,sync up random seed for data partition
v2.3.1,load Training data
v2.3.1,load data for parallel training
v2.3.1,load data for single machine
v2.3.1,need save binary file
v2.3.1,create training metric
v2.3.1,only when have metrics then need to construct validation data
v2.3.1,"Add validation data, if it exists"
v2.3.1,add
v2.3.1,need save binary file
v2.3.1,add metric for validation data
v2.3.1,output used time on each iteration
v2.3.1,need init network
v2.3.1,create boosting
v2.3.1,create objective function
v2.3.1,load training data
v2.3.1,initialize the objective function
v2.3.1,initialize the boosting
v2.3.1,add validation data into boosting
v2.3.1,convert model to if-else statement code
v2.3.1,create predictor
v2.3.1,Free memory
v2.3.1,create predictor
v2.3.1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.3.1,counts for all labels
v2.3.1,"start from top label, and accumulate DCG"
v2.3.1,counts for all labels
v2.3.1,calculate k Max DCG by one pass
v2.3.1,get sorted indices by score
v2.3.1,calculate dcg
v2.3.1,get sorted indices by score
v2.3.1,calculate multi dcg by one pass
v2.3.1,wait for all client start up
v2.3.1,default set to -1
v2.3.1,"distance at k-th communication, distance[k] = 2^k"
v2.3.1,set incoming rank at k-th commuication
v2.3.1,set outgoing rank at k-th commuication
v2.3.1,defalut set as -1
v2.3.1,construct all recursive halving map for all machines
v2.3.1,let 1 << k <= num_machines
v2.3.1,distance of each communication
v2.3.1,"if num_machines = 2^k, don't need to group machines"
v2.3.1,"communication direction, %2 == 0 is positive"
v2.3.1,neighbor at k-th communication
v2.3.1,receive data block at k-th communication
v2.3.1,send data block at k-th communication
v2.3.1,"if num_machines != 2^k, need to group machines"
v2.3.1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.3.1,let left machine as group leader
v2.3.1,"cache block information for groups, group with 2 machines will have double block size"
v2.3.1,convert from group to node leader
v2.3.1,convert from node to group
v2.3.1,meet new group
v2.3.1,add block len for this group
v2.3.1,calculate the group block start
v2.3.1,not need to construct
v2.3.1,get receive block informations
v2.3.1,accumulate block len
v2.3.1,get send block informations
v2.3.1,accumulate block len
v2.3.1,static member definition
v2.3.1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.3.1,assign the blocks to every rank.
v2.3.1,do reduce scatter
v2.3.1,do all gather
v2.3.1,assign blocks
v2.3.1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.3.1,copy back
v2.3.1,assign blocks
v2.3.1,start all gather
v2.3.1,when num_machines is small and data is large
v2.3.1,use output as receive buffer
v2.3.1,get current local block size
v2.3.1,get out rank
v2.3.1,get in rank
v2.3.1,get send information
v2.3.1,get recv information
v2.3.1,send and recv at same time
v2.3.1,rotate in-place
v2.3.1,use output as receive buffer
v2.3.1,get current local block size
v2.3.1,get send information
v2.3.1,get recv information
v2.3.1,send and recv at same time
v2.3.1,use output as receive buffer
v2.3.1,send and recv at same time
v2.3.1,send local data to neighbor first
v2.3.1,receive neighbor data first
v2.3.1,reduce
v2.3.1,get target
v2.3.1,get send information
v2.3.1,get recv information
v2.3.1,send and recv at same time
v2.3.1,reduce
v2.3.1,send result to neighbor
v2.3.1,receive result from neighbor
v2.3.1,copy result
v2.3.1,start up socket
v2.3.1,parse clients from file
v2.3.1,get ip list of local machine
v2.3.1,get local rank
v2.3.1,construct listener
v2.3.1,construct communication topo
v2.3.1,construct linkers
v2.3.1,free listener
v2.3.1,set timeout
v2.3.1,accept incoming socket
v2.3.1,receive rank
v2.3.1,add new socket
v2.3.1,save ranks that need to connect with
v2.3.1,start listener
v2.3.1,start connect
v2.3.1,let smaller rank connect to larger rank
v2.3.1,send local rank
v2.3.1,wait for listener
v2.3.1,print connected linkers
v2.3.1,valid the type
v2.3.1,Constructors
v2.3.1,Get type tag
v2.3.1,Comparisons
v2.3.1,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.3.1,"advance until next line, or end of input"
v2.3.1,advance until closing tokens
v2.3.1,The usual case: non-escaped characters
v2.3.1,Handle escapes
v2.3.1,Extract 4-byte escape sequence
v2.3.1,Explicitly check length of the substring. The following loop
v2.3.1,relies on std::string returning the terminating NUL when
v2.3.1,accessing str[length]. Checking here reduces brittleness.
v2.3.1,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.3.1,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.3.1,whether we're in the middle of such a beast: the previous codepoint was an
v2.3.1,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.3.1,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.3.1,the UTF-16 algorithm.
v2.3.1,Integer part
v2.3.1,Decimal part
v2.3.1,Exponent part
v2.3.1,Check for any trailing garbage
v2.3.1,Documented in json11.hpp
v2.3.1,Check for another object
v2.3.1,get column names
v2.3.1,load label idx first
v2.3.1,erase label column name
v2.3.1,load ignore columns
v2.3.1,load weight idx
v2.3.1,load group idx
v2.3.1,don't support query id in data file when training in parallel
v2.3.1,read data to memory
v2.3.1,sample data
v2.3.1,construct feature bin mappers
v2.3.1,initialize label
v2.3.1,extract features
v2.3.1,sample data from file
v2.3.1,construct feature bin mappers
v2.3.1,initialize label
v2.3.1,extract features
v2.3.1,load data from binary file
v2.3.1,check meta data
v2.3.1,need to check training data
v2.3.1,read data in memory
v2.3.1,initialize label
v2.3.1,extract features
v2.3.1,Get number of lines of data file
v2.3.1,initialize label
v2.3.1,extract features
v2.3.1,load data from binary file
v2.3.1,not need to check validation data
v2.3.1,check meta data
v2.3.1,buffer to read binary file
v2.3.1,check token
v2.3.1,read size of header
v2.3.1,re-allocmate space if not enough
v2.3.1,read header
v2.3.1,get header
v2.3.1,num_groups
v2.3.1,real_feature_idx_
v2.3.1,feature2group
v2.3.1,feature2subfeature
v2.3.1,group_bin_boundaries
v2.3.1,group_feature_start_
v2.3.1,group_feature_cnt_
v2.3.1,get feature names
v2.3.1,write feature names
v2.3.1,get forced_bin_bounds_
v2.3.1,read size of meta data
v2.3.1,re-allocate space if not enough
v2.3.1,read meta data
v2.3.1,load meta data
v2.3.1,sample local used data if need to partition
v2.3.1,"if not contain query file, minimal sample unit is one record"
v2.3.1,"if contain query file, minimal sample unit is one query"
v2.3.1,if is new query
v2.3.1,read feature data
v2.3.1,read feature size
v2.3.1,re-allocate space if not enough
v2.3.1,fill feature_names_ if not header
v2.3.1,get forced split
v2.3.1,"if only one machine, find bin locally"
v2.3.1,"if have multi-machines, need to find bin distributed"
v2.3.1,different machines will find bin for different features
v2.3.1,start and len will store the process feature indices for different machines
v2.3.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.3.1,free
v2.3.1,gather global feature bin mappers
v2.3.1,restore features bins from buffer
v2.3.1,---- private functions ----
v2.3.1,"if features are ordered, not need to use hist_buf"
v2.3.1,read all lines
v2.3.1,get query data
v2.3.1,"if not contain query data, minimal sample unit is one record"
v2.3.1,"if contain query data, minimal sample unit is one query"
v2.3.1,if is new query
v2.3.1,get query data
v2.3.1,"if not contain query file, minimal sample unit is one record"
v2.3.1,"if contain query file, minimal sample unit is one query"
v2.3.1,if is new query
v2.3.1,parse features
v2.3.1,get forced split
v2.3.1,"check the range of label_idx, weight_idx and group_idx"
v2.3.1,fill feature_names_ if not header
v2.3.1,start find bins
v2.3.1,"if only one machine, find bin locally"
v2.3.1,start and len will store the process feature indices for different machines
v2.3.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.3.1,free
v2.3.1,gather global feature bin mappers
v2.3.1,restore features bins from buffer
v2.3.1,if doesn't need to prediction with initial model
v2.3.1,parser
v2.3.1,set label
v2.3.1,free processed line:
v2.3.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.3.1,text_reader_->Lines()[i].shrink_to_fit();
v2.3.1,push data
v2.3.1,if is used feature
v2.3.1,if need to prediction with initial model
v2.3.1,parser
v2.3.1,set initial score
v2.3.1,set label
v2.3.1,free processed line:
v2.3.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.3.1,text_reader_->Lines()[i].shrink_to_fit();
v2.3.1,push data
v2.3.1,if is used feature
v2.3.1,metadata_ will manage space of init_score
v2.3.1,text data can be free after loaded feature values
v2.3.1,parser
v2.3.1,set initial score
v2.3.1,set label
v2.3.1,push data
v2.3.1,if is used feature
v2.3.1,only need part of data
v2.3.1,need full data
v2.3.1,metadata_ will manage space of init_score
v2.3.1,read size of token
v2.3.1,remove duplicates
v2.3.1,deep copy function for BinMapper
v2.3.1,mean size for one bin
v2.3.1,need a new bin
v2.3.1,update bin upper bound
v2.3.1,last bin upper bound
v2.3.1,get list of distinct values
v2.3.1,get number of positive and negative distinct values
v2.3.1,include zero bounds and infinity bound
v2.3.1,"add forced bounds, excluding zeros since we have already added zero bounds"
v2.3.1,find remaining bounds
v2.3.1,find distinct_values first
v2.3.1,push zero in the front
v2.3.1,use the large value
v2.3.1,push zero in the back
v2.3.1,convert to int type first
v2.3.1,sort by counts
v2.3.1,avoid first bin is zero
v2.3.1,will ignore the categorical of small counts
v2.3.1,need an additional bin for NaN
v2.3.1,use -1 to represent NaN
v2.3.1,Use MissingType::None to represent this bin contains all categoricals
v2.3.1,check trivial(num_bin_ == 1) feature
v2.3.1,check useless bin
v2.3.1,calculate sparse rate
v2.3.1,sparse threshold
v2.3.1,"for lambdarank, it needs query data for partition data in parallel learning"
v2.3.1,need convert query_id to boundaries
v2.3.1,check weights
v2.3.1,check query boundries
v2.3.1,contain initial score file
v2.3.1,check weights
v2.3.1,get local weights
v2.3.1,check query boundries
v2.3.1,get local query boundaries
v2.3.1,contain initial score file
v2.3.1,get local initial scores
v2.3.1,re-load query weight
v2.3.1,save to nullptr
v2.3.1,save to nullptr
v2.3.1,save to nullptr
v2.3.1,default weight file name
v2.3.1,default weight file name
v2.3.1,use first line to count number class
v2.3.1,default query file name
v2.3.1,root is in the depth 0
v2.3.1,non-leaf
v2.3.1,leaf
v2.3.1,use this for the missing value conversion
v2.3.1,Predict func by Map to ifelse
v2.3.1,use this for the missing value conversion
v2.3.1,non-leaf
v2.3.1,left subtree
v2.3.1,right subtree
v2.3.1,leaf
v2.3.1,non-leaf
v2.3.1,left subtree
v2.3.1,right subtree
v2.3.1,leaf
v2.3.1,recursive computation of SHAP values for a decision tree
v2.3.1,extend the unique path
v2.3.1,leaf node
v2.3.1,internal node
v2.3.1,"see if we have already split on this feature,"
v2.3.1,if so we undo that split so we can redo it for this node
v2.3.1,add names of objective function if not providing metric
v2.3.1,generate seeds by seed.
v2.3.1,sort eval_at
v2.3.1,Only push the non-training data
v2.3.1,check for conflicts
v2.3.1,"check if objective, metric, and num_class match"
v2.3.1,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.3.1,Check max_depth and num_leaves
v2.3.1,"Fits in an int, and is more restrictive than the current num_leaves"
v2.3.1,"filter is based on sampling data, so decrease its range"
v2.3.1,put dense feature first
v2.3.1,sort by non zero cnt
v2.3.1,"sort by non zero cnt, bigger first"
v2.3.1,"take apart small sparse group, due it will not gain on speed"
v2.3.1,shuffle groups
v2.3.1,get num_features
v2.3.1,get bin_mappers
v2.3.1,copy feature bin mapper data
v2.3.1,copy feature bin mapper data
v2.3.1,"if not pass a filename, just append "".bin"" of original file"
v2.3.1,get size of header
v2.3.1,size of feature names
v2.3.1,size of forced bins
v2.3.1,write header
v2.3.1,write feature names
v2.3.1,write forced bins
v2.3.1,get size of meta data
v2.3.1,write meta data
v2.3.1,write feature data
v2.3.1,get size of feature
v2.3.1,write feature
v2.3.1,feature is not used
v2.3.1,construct histograms for smaller leaf
v2.3.1,if not use ordered bin
v2.3.1,used ordered bin
v2.3.1,feature is not used
v2.3.1,construct histograms for smaller leaf
v2.3.1,if not use ordered bin
v2.3.1,used ordered bin
v2.3.1,fixed hessian.
v2.3.1,feature is not used
v2.3.1,construct histograms for smaller leaf
v2.3.1,if not use ordered bin
v2.3.1,used ordered bin
v2.3.1,feature is not used
v2.3.1,construct histograms for smaller leaf
v2.3.1,if not use ordered bin
v2.3.1,used ordered bin
v2.3.1,fixed hessian.
v2.3.1,Skip the leading 0 when copying group_bin_boundaries.
v2.3.1,PredictRaw
v2.3.1,PredictRawByMap
v2.3.1,Predict
v2.3.1,PredictByMap
v2.3.1,PredictLeafIndex
v2.3.1,PredictLeafIndexByMap
v2.3.1,output model type
v2.3.1,output number of class
v2.3.1,output label index
v2.3.1,output max_feature_idx
v2.3.1,output objective
v2.3.1,output tree models
v2.3.1,store the importance first
v2.3.1,sort the importance
v2.3.1,use serialized string to restore this object
v2.3.1,Use first 128 chars to avoid exceed the message buffer.
v2.3.1,get number of classes
v2.3.1,get index of label
v2.3.1,get max_feature_idx first
v2.3.1,get average_output
v2.3.1,get feature names
v2.3.1,get monotone_constraints
v2.3.1,set zero
v2.3.1,predict all the trees for one iteration
v2.3.1,check early stopping
v2.3.1,set zero
v2.3.1,predict all the trees for one iteration
v2.3.1,check early stopping
v2.3.1,margin_threshold will be captured by value
v2.3.1,copy and sort
v2.3.1,margin_threshold will be captured by value
v2.3.1,load forced_splits file
v2.3.1,init tree learner
v2.3.1,push training metrics
v2.3.1,create buffer for gradients and hessians
v2.3.1,get max feature index
v2.3.1,get label index
v2.3.1,get feature names
v2.3.1,"if need bagging, create buffer"
v2.3.1,"for a validation dataset, we need its score and metric"
v2.3.1,update score
v2.3.1,objective function will calculate gradients and hessians
v2.3.1,"random bagging, minimal unit is one record"
v2.3.1,from right to left
v2.3.1,"random bagging, minimal unit is one record"
v2.3.1,reverse right buffer
v2.3.1,if need bagging
v2.3.1,set bagging data to tree learner
v2.3.1,get subset
v2.3.1,output used time per iteration
v2.3.1,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.3.1,boosting first
v2.3.1,bagging logic
v2.3.1,need to copy gradients for bagging subset.
v2.3.1,shrinkage by learning rate
v2.3.1,update score
v2.3.1,only add default score one-time
v2.3.1,updates scores
v2.3.1,add model
v2.3.1,reset score
v2.3.1,remove model
v2.3.1,print message for metric
v2.3.1,pop last early_stopping_round_ models
v2.3.1,update training score
v2.3.1,we need to predict out-of-bag scores of data for boosting
v2.3.1,update validation score
v2.3.1,print training metric
v2.3.1,print validation metric
v2.3.1,set zero
v2.3.1,predict all the trees for one iteration
v2.3.1,check early stopping
v2.3.1,push training metrics
v2.3.1,"not same training data, need reset score and others"
v2.3.1,create score tracker
v2.3.1,update score
v2.3.1,create buffer for gradients and hessians
v2.3.1,"if need bagging, create buffer"
v2.3.1,Get the max size of pool
v2.3.1,at least need 2 leaves
v2.3.1,push split information for all leaves
v2.3.1,get ordered bin
v2.3.1,check existing for ordered bin
v2.3.1,initialize splits for leaf
v2.3.1,initialize data partition
v2.3.1,initialize ordered gradients and hessians
v2.3.1,"if has ordered bin, need to allocate a buffer to fast split"
v2.3.1,get ordered bin
v2.3.1,initialize splits for leaf
v2.3.1,initialize data partition
v2.3.1,initialize ordered gradients and hessians
v2.3.1,"if has ordered bin, need to allocate a buffer to fast split"
v2.3.1,Get the max size of pool
v2.3.1,at least need 2 leaves
v2.3.1,push split information for all leaves
v2.3.1,some initial works before training
v2.3.1,root leaf
v2.3.1,only root leaf can be splitted on first time
v2.3.1,some initial works before finding best split
v2.3.1,find best threshold for every feature
v2.3.1,Get a leaf with max split gain
v2.3.1,Get split information for best leaf
v2.3.1,"cannot split, quit"
v2.3.1,split tree with best leaf
v2.3.1,reset histogram pool
v2.3.1,initialize data partition
v2.3.1,reset the splits for leaves
v2.3.1,Sumup for root
v2.3.1,use all data
v2.3.1,"use bagging, only use part of data"
v2.3.1,"if has ordered bin, need to initialize the ordered bin"
v2.3.1,"use all data, pass nullptr"
v2.3.1,"bagging, only use part of data"
v2.3.1,mark used data
v2.3.1,initialize ordered bin
v2.3.1,check depth of current leaf
v2.3.1,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.3.1,no enough data to continue
v2.3.1,only have root
v2.3.1,put parent(left) leaf's histograms into larger leaf's histograms
v2.3.1,put parent(left) leaf's histograms to larger leaf's histograms
v2.3.1,split for the ordered bin
v2.3.1,mark data that at left-leaf
v2.3.1,split the ordered bin
v2.3.1,construct smaller leaf
v2.3.1,construct larger leaf
v2.3.1,find splits
v2.3.1,only has root leaf
v2.3.1,find best threshold for larger child
v2.3.1,start at root leaf
v2.3.1,"before processing next node from queue, store info for current left/right leaf"
v2.3.1,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.3.1,"then, compute own splits"
v2.3.1,split info should exist because searching in bfs fashion - should have added from parent
v2.3.1,"split tree, will return right leaf"
v2.3.1,left = parent
v2.3.1,"split tree, will return right leaf"
v2.3.1,init the leaves that used on next iteration
v2.3.1,bag_mapper[index_mapper[i]]
v2.3.1,get feature partition
v2.3.1,get local used features
v2.3.1,get best split at smaller leaf
v2.3.1,find local best split for larger leaf
v2.3.1,sync global best info
v2.3.1,update best split
v2.3.1,"instantiate template classes, otherwise linker cannot find the code"
v2.3.1,initialize SerialTreeLearner
v2.3.1,Get local rank and global machine size
v2.3.1,allocate buffer for communication
v2.3.1,generate feature partition for current tree
v2.3.1,get local used feature
v2.3.1,get block start and block len for reduce scatter
v2.3.1,get buffer_write_start_pos_
v2.3.1,get buffer_read_start_pos_
v2.3.1,sync global data sumup info
v2.3.1,global sumup reduce
v2.3.1,copy back
v2.3.1,set global sumup info
v2.3.1,init global data count in leaf
v2.3.1,construct local histograms
v2.3.1,copy to buffer
v2.3.1,Reduce scatter for histogram
v2.3.1,restore global histograms from buffer
v2.3.1,find best threshold for smaller child
v2.3.1,only root leaf
v2.3.1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.3.1,find best threshold for larger child
v2.3.1,find local best split for larger leaf
v2.3.1,sync global best info
v2.3.1,set best split
v2.3.1,need update global number of data in leaf
v2.3.1,"instantiate template classes, otherwise linker cannot find the code"
v2.3.1,initialize SerialTreeLearner
v2.3.1,some additional variables needed for GPU trainer
v2.3.1,Initialize GPU buffers and kernels
v2.3.1,some functions used for debugging the GPU histogram construction
v2.3.1,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.3.1,goto err;
v2.3.1,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.3.1,goto err;
v2.3.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.3.1,also guarantee that there are at least 2K examples per workgroup
v2.3.1,return 0;
v2.3.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.3.1,decide the best number of workgroups working on one feature4 tuple
v2.3.1,set work group size based on feature size
v2.3.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.3.1,we need to refresh the kernel arguments after reallocating
v2.3.1,The only argument that needs to be changed later is num_data_
v2.3.1,"the GPU kernel will process all features in one call, and each"
v2.3.1,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.3.1,process one feature4 tuple
v2.3.1,"for the root node, indices are not copied"
v2.3.1,"for constant hessian, hessians are not copied except for the root node"
v2.3.1,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.3.1,and we will launch num_feature workgroups for this kernel
v2.3.1,will launch threads for all features
v2.3.1,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.3.1,copy the results asynchronously. Size depends on if double precision is used
v2.3.1,we will wait for this object in WaitAndGetHistograms
v2.3.1,"when the output is ready, the computation is done"
v2.3.1,values of this feature has been redistributed to multiple bins; need a reduction here
v2.3.1,how many feature-group tuples we have
v2.3.1,leave some safe margin for prefetching
v2.3.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.3.1,clear sparse/dense maps
v2.3.1,do nothing if no features can be processed on GPU
v2.3.1,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.3.1,unpin old buffer if necessary before destructing them
v2.3.1,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.3.1,allocate space for gradients and hessians on device
v2.3.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.3.1,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.3.1,copy indices to the device
v2.3.1,histogram bin entry size depends on the precision (single/double)
v2.3.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.3.1,each work group generates a sub-histogram of dword_features_ features.
v2.3.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.3.1,create atomic counters for inter-group coordination
v2.3.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.3.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.3.1,looking for dword_features_ non-sparse feature-groups
v2.3.1,decide if we need to redistribute the bin
v2.3.1,multiplier must be a power of 2
v2.3.1,device_bin_mults_.push_back(1);
v2.3.1,found
v2.3.1,for data transfer time
v2.3.1,"Now generate new data structure feature4, and copy data to the device"
v2.3.1,"preallocate arrays for all threads, and pin them"
v2.3.1,building Feature4 bundles; each thread handles dword_features_ features
v2.3.1,one feature datapoint is 4 bits
v2.3.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.3.1,one feature datapoint is one byte
v2.3.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.3.1,Dense bin
v2.3.1,Dense 4-bit bin
v2.3.1,working on the remaining (less than dword_features_) feature groups
v2.3.1,fill the leftover features
v2.3.1,"fill this empty feature with some ""random"" value"
v2.3.1,"fill this empty feature with some ""random"" value"
v2.3.1,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.3.1,deallocate pinned space for feature copying
v2.3.1,data transfer time
v2.3.1,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.3.1,"Something bad happened. Just return ""No log available."""
v2.3.1,"build is okay, log may contain warnings"
v2.3.1,destroy any old kernels
v2.3.1,create OpenCL kernels for different number of workgroups per feature
v2.3.1,currently we don't use constant memory
v2.3.1,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v2.3.1,kernel with indices in an array
v2.3.1,"kernel with all features enabled, with elimited branches"
v2.3.1,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.3.1,do nothing if no features can be processed on GPU
v2.3.1,The only argument that needs to be changed later is num_data_
v2.3.1,"hessian is passed as a parameter, but it is not available now."
v2.3.1,hessian will be set in BeforeTrain()
v2.3.1,"Get the max bin size, used for selecting best GPU kernel"
v2.3.1,initialize GPU
v2.3.1,determine which kernel to use based on the max number of bins
v2.3.1,setup GPU kernel arguments after we allocating all the buffers
v2.3.1,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.3.1,this should rarely occur
v2.3.1,GPU memory has to been reallocated because data may have been changed
v2.3.1,setup GPU kernel arguments after we allocating all the buffers
v2.3.1,Copy initial full hessians and gradients to GPU.
v2.3.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.3.1,setup hessian parameters only
v2.3.1,hessian is passed as a parameter
v2.3.1,use bagging
v2.3.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.3.1,copy used gradients and hessians to ordered buffer
v2.3.1,transfer the indices to GPU
v2.3.1,transfer hessian to GPU
v2.3.1,setup hessian parameters only
v2.3.1,hessian is passed as a parameter
v2.3.1,transfer gradients to GPU
v2.3.1,only have root
v2.3.1,"Copy indices, gradients and hessians as early as possible"
v2.3.1,only need to initialize for smaller leaf
v2.3.1,Get leaf boundary
v2.3.1,copy indices to the GPU:
v2.3.1,copy ordered hessians to the GPU:
v2.3.1,copy ordered gradients to the GPU:
v2.3.1,do nothing if no features can be processed on GPU
v2.3.1,copy data indices if it is not null
v2.3.1,generate and copy ordered_gradients if gradients is not null
v2.3.1,generate and copy ordered_hessians if hessians is not null
v2.3.1,converted indices in is_feature_used to feature-group indices
v2.3.1,construct the feature masks for dense feature-groups
v2.3.1,"if no feature group is used, just return and do not use GPU"
v2.3.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.3.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.3.1,"All data have been prepared, now run the GPU kernel"
v2.3.1,construct smaller leaf
v2.3.1,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.3.1,then construct sparse features on CPU
v2.3.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.3.1,"wait for GPU to finish, only if GPU is actually used"
v2.3.1,use double precision
v2.3.1,use single precision
v2.3.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.3.1,#define GPU_DEBUG_COMPARE
v2.3.1,construct larger leaf
v2.3.1,then construct sparse features on CPU
v2.3.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.3.1,"wait for GPU to finish, only if GPU is actually used"
v2.3.1,use double precision
v2.3.1,use single precision
v2.3.1,do some sanity check for the GPU algorithm
v2.3.1,limit top k
v2.3.1,get max bin
v2.3.1,calculate buffer size
v2.3.1,"left and right on same time, so need double size"
v2.3.1,initialize histograms for global
v2.3.1,sync global data sumup info
v2.3.1,set global sumup info
v2.3.1,init global data count in leaf
v2.3.1,get local sumup
v2.3.1,get local sumup
v2.3.1,get mean number on machines
v2.3.1,weighted gain
v2.3.1,get top k
v2.3.1,"Copy histogram to buffer, and Get local aggregate features"
v2.3.1,copy histograms.
v2.3.1,copy smaller leaf histograms first
v2.3.1,mark local aggregated feature
v2.3.1,copy
v2.3.1,then copy larger leaf histograms
v2.3.1,mark local aggregated feature
v2.3.1,copy
v2.3.1,use local data to find local best splits
v2.3.1,find splits
v2.3.1,only has root leaf
v2.3.1,find best threshold for larger child
v2.3.1,local voting
v2.3.1,gather
v2.3.1,get all top-k from all machines
v2.3.1,global voting
v2.3.1,copy local histgrams to buffer
v2.3.1,Reduce scatter for histogram
v2.3.1,find best split from local aggregated histograms
v2.3.1,restore from buffer
v2.3.1,find best threshold
v2.3.1,restore from buffer
v2.3.1,find best threshold
v2.3.1,find local best
v2.3.1,find local best split for larger leaf
v2.3.1,sync global best info
v2.3.1,copy back
v2.3.1,set the global number of data for leaves
v2.3.1,init the global sumup info
v2.3.1,"instantiate template classes, otherwise linker cannot find the code"
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable=invalid-name, exec-used, C0111"
v2.3.0,Apple Clang with OpenMP
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, W0105"
v2.3.0,create predictor first
v2.3.0,check dataset
v2.3.0,reduce cost for prediction training data
v2.3.0,process callbacks
v2.3.0,Most of legacy advanced options becomes callbacks
v2.3.0,construct booster
v2.3.0,start training
v2.3.0,check evaluation result.
v2.3.0,"lambdarank task, split according to groups"
v2.3.0,run preprocessing on the data set if needed
v2.3.0,setup callbacks
v2.3.0,coding: utf-8
v2.3.0,pylint: disable = C0103
v2.3.0,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.3.0,because of u'...' Unicode literals.
v2.3.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.3.0,minor change to support `**kwargs`
v2.3.0,"user can set verbose with kwargs, it has higher priority"
v2.3.0,register default metric for consistency with callable eval_metric case
v2.3.0,try to deduce from class instance
v2.3.0,overwrite default metric by explicitly set metric
v2.3.0,concatenate metric from params (or default if not provided in params) and eval_metric
v2.3.0,reduce cost for prediction training data
v2.3.0,free dataset
v2.3.0,Switch to using a multiclass objective in the underlying LGBM instance
v2.3.0,check group data
v2.3.0,coding: utf-8
v2.3.0,we don't need lib_lightgbm while building docs
v2.3.0,coding: utf-8
v2.3.0,pylint: disable = C0103
v2.3.0,"""#ddffdd"" is light green, ""#ffdddd"" is light red"
v2.3.0,coding: utf-8
v2.3.0,REMOVEME: remove warning after 2.3.0 version release
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, C0111, C0301"
v2.3.0,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.3.0,TypeError: obj is not a string or a number
v2.3.0,ValueError: invalid literal
v2.3.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.3.0,avoid memory consumption by arrays concatenation operations
v2.3.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.3.0,avoid memory consumption by arrays concatenation operations
v2.3.0,check data has header or not
v2.3.0,need to regroup init_score
v2.3.0,process for args
v2.3.0,"user can set verbose with params, it has higher priority"
v2.3.0,get categorical features
v2.3.0,process for reference dataset
v2.3.0,start construct data
v2.3.0,set feature names
v2.3.0,create valid
v2.3.0,construct subset
v2.3.0,create train
v2.3.0,set to None
v2.3.0,we're done if self and reference share a common upstrem reference
v2.3.0,"group data from LightGBM is boundaries data, need to convert to group size"
v2.3.0,"user can set verbose with params, it has higher priority"
v2.3.0,Training task
v2.3.0,set network if necessary
v2.3.0,construct booster object
v2.3.0,save reference to data
v2.3.0,buffer for inner predict
v2.3.0,Prediction task
v2.3.0,need reset training data
v2.3.0,need to push new valid data
v2.3.0,"if buffer length is not long enough, re-allocate a buffer"
v2.3.0,"if buffer length is not long enough, reallocate a buffer"
v2.3.0,Copy models
v2.3.0,Get name of features
v2.3.0,avoid to predict many time in one iteration
v2.3.0,Get num of inner evals
v2.3.0,Get name of evals
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, W0105, C0301"
v2.3.0,Callback environment used by callbacks
v2.3.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v2.3.0,"split is needed for ""<dataset type> <metric>"" case (e.g. ""train l1"")"
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, C0111"
v2.3.0,load or create your dataset
v2.3.0,create dataset for lightgbm
v2.3.0,"if you want to re-use data, remember to set free_raw_data=False"
v2.3.0,specify your configurations as a dict
v2.3.0,generate feature names
v2.3.0,feature_name and categorical_feature
v2.3.0,check feature name
v2.3.0,save model to file
v2.3.0,dump model to JSON (and save to file)
v2.3.0,feature names
v2.3.0,feature importances
v2.3.0,load model to predict
v2.3.0,can only predict with the best iteration (or the saving iteration)
v2.3.0,eval with loaded model
v2.3.0,dump model with pickle
v2.3.0,load model with pickle to predict
v2.3.0,can predict with any iteration when loaded in pickle way
v2.3.0,eval with loaded model
v2.3.0,continue training
v2.3.0,init_model accepts:
v2.3.0,1. model file name
v2.3.0,2. Booster()
v2.3.0,decay learning rates
v2.3.0,learning_rates accepts:
v2.3.0,1. list/tuple with length = num_boost_round
v2.3.0,2. function(curr_iter)
v2.3.0,change other parameters during training
v2.3.0,self-defined objective function
v2.3.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.3.0,log likelihood loss
v2.3.0,self-defined eval metric
v2.3.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.0,binary error
v2.3.0,another self-defined eval metric
v2.3.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.0,accuracy
v2.3.0,callback
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, C0111"
v2.3.0,load or create your dataset
v2.3.0,train
v2.3.0,predict
v2.3.0,eval
v2.3.0,feature importances
v2.3.0,self-defined eval metric
v2.3.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.0,Root Mean Squared Logarithmic Error (RMSLE)
v2.3.0,train
v2.3.0,another self-defined eval metric
v2.3.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.3.0,Relative Absolute Error (RAE)
v2.3.0,train
v2.3.0,predict
v2.3.0,eval
v2.3.0,other scikit-learn modules
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, C0111"
v2.3.0,load or create your dataset
v2.3.0,create dataset for lightgbm
v2.3.0,specify your configurations as a dict
v2.3.0,train
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, C0111"
v2.3.0,################
v2.3.0,Simulate some binary data with a single categorical and
v2.3.0,single continuous predictor
v2.3.0,################
v2.3.0,Set up a couple of utilities for our experiments
v2.3.0,################
v2.3.0,Observe the behavior of `binary` and `xentropy` objectives
v2.3.0,Trying this throws an error on non-binary values of y:
v2.3.0,"experiment('binary', label_type='probability', DATA)"
v2.3.0,The speed of `binary` is not drastically different than
v2.3.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v2.3.0,there are reasons to suspect that `binary` should run faster when the
v2.3.0,label is an integer instead of a float
v2.3.0,coding: utf-8
v2.3.0,"pylint: disable = invalid-name, C0111"
v2.3.0,load or create your dataset
v2.3.0,create dataset for lightgbm
v2.3.0,specify your configurations as a dict
v2.3.0,train
v2.3.0,save model to file
v2.3.0,predict
v2.3.0,eval
v2.3.0,coding: utf-8
v2.3.0,!/usr/bin/env python3
v2.3.0,-*- coding: utf-8 -*-
v2.3.0,
v2.3.0,"LightGBM documentation build configuration file, created by"
v2.3.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.3.0,
v2.3.0,This file is execfile()d with the current directory set to its
v2.3.0,containing dir.
v2.3.0,
v2.3.0,Note that not all possible configuration values are present in this
v2.3.0,autogenerated file.
v2.3.0,
v2.3.0,All configuration values have a default; values that are commented out
v2.3.0,serve to show the default.
v2.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
v2.3.0,add these directories to sys.path here. If the directory is relative to the
v2.3.0,"documentation root, use os.path.abspath to make it absolute."
v2.3.0,-- mock out modules
v2.3.0,-- General configuration ------------------------------------------------
v2.3.0,"If your documentation needs a minimal Sphinx version, state it here."
v2.3.0,"Add any Sphinx extension module names here, as strings. They can be"
v2.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.3.0,ones.
v2.3.0,Generate autosummary pages. Output should be set with: `:toctree: pythonapi/`
v2.3.0,Only the class' docstring is inserted.
v2.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.3.0,The master toctree document.
v2.3.0,General information about the project.
v2.3.0,"The version info for the project you're documenting, acts as replacement for"
v2.3.0,"|version| and |release|, also used in various other places throughout the"
v2.3.0,built documents.
v2.3.0,The short X.Y version.
v2.3.0,"The full version, including alpha/beta/rc tags."
v2.3.0,The language for content autogenerated by Sphinx. Refer to documentation
v2.3.0,for a list of supported languages.
v2.3.0,
v2.3.0,This is also used if you do content translation via gettext catalogs.
v2.3.0,"Usually you set ""language"" from the command line for these cases."
v2.3.0,"List of patterns, relative to source directory, that match files and"
v2.3.0,directories to ignore when looking for source files.
v2.3.0,This patterns also effect to html_static_path and html_extra_path
v2.3.0,The name of the Pygments (syntax highlighting) style to use.
v2.3.0,-- Configuration for C API docs generation ------------------------------
v2.3.0,-- Options for HTML output ----------------------------------------------
v2.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.3.0,a list of builtin themes.
v2.3.0,Theme options are theme-specific and customize the look and feel of a theme
v2.3.0,"further.  For a list of options available for each theme, see the"
v2.3.0,documentation.
v2.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
v2.3.0,"relative to this directory. They are copied after the builtin static files,"
v2.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.3.0,-- Options for HTMLHelp output ------------------------------------------
v2.3.0,Output file base name for HTML help builder.
v2.3.0,Warning! The following code can cause buffer overflows on RTD.
v2.3.0,Consider suppressing output completely if RTD project silently fails.
v2.3.0,Refer to https://github.com/svenevs/exhale
v2.3.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v2.3.0,Warning! The following code can cause buffer overflows on RTD.
v2.3.0,Consider suppressing output completely if RTD project silently fails.
v2.3.0,Refer to https://github.com/svenevs/exhale
v2.3.0,/blob/fe7644829057af622e467bb529db6c03a830da99/exhale/deploy.py#L99-L111
v2.3.0,coding: utf-8
v2.3.0,coding: utf-8
v2.3.0,pylint: skip-file
v2.3.0,we don't need lib_lightgbm while building docs
v2.3.0,coding: utf-8
v2.3.0,pylint: skip-file
v2.3.0,check saved model persistence
v2.3.0,"we need to check the consistency of model file here, so test for exact equal"
v2.3.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.3.0,"scores likely to be different, but prediction should still be the same"
v2.3.0,"Set extremely harsh penalties, so CEGB will block most splits."
v2.3.0,"Compare pairs of penalties, to ensure scaling works as intended"
v2.3.0,"Reset booster1's parameters to p2, so the parameter section of the file matches."
v2.3.0,coding: utf-8
v2.3.0,pylint: skip-file
v2.3.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v2.3.0,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.3.0,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.3.0,we cannot leave default params (see https://github.com/microsoft/LightGBM/issues/833)
v2.3.0,Tests same probabilities
v2.3.0,Tests same predictions
v2.3.0,Tests same raw scores
v2.3.0,Tests same leaf indices
v2.3.0,Tests same feature contributions
v2.3.0,Tests other parameters for the prediction works
v2.3.0,"no custom objective, no custom metric"
v2.3.0,default metric
v2.3.0,non-default metric
v2.3.0,no metric
v2.3.0,non-default metric in eval_metric
v2.3.0,non-default metric with non-default metric in eval_metric
v2.3.0,non-default metric with multiple metrics in eval_metric
v2.3.0,default metric for non-default objective
v2.3.0,non-default metric for non-default objective
v2.3.0,no metric
v2.3.0,non-default metric in eval_metric for non-default objective
v2.3.0,non-default metric with non-default metric in eval_metric for non-default objective
v2.3.0,non-default metric with multiple metrics in eval_metric for non-default objective
v2.3.0,"custom objective, no custom metric"
v2.3.0,default regression metric for custom objective
v2.3.0,non-default regression metric for custom objective
v2.3.0,multiple regression metrics for custom objective
v2.3.0,no metric
v2.3.0,default regression metric with non-default metric in eval_metric for custom objective
v2.3.0,non-default regression metric with metric in eval_metric for custom objective
v2.3.0,multiple regression metrics with metric in eval_metric for custom objective
v2.3.0,multiple regression metrics with multiple metrics in eval_metric for custom objective
v2.3.0,"no custom objective, custom metric"
v2.3.0,default metric with custom metric
v2.3.0,non-default metric with custom metric
v2.3.0,multiple metrics with custom metric
v2.3.0,custom metric (disable default metric)
v2.3.0,default metric for non-default objective with custom metric
v2.3.0,non-default metric for non-default objective with custom metric
v2.3.0,multiple metrics for non-default objective with custom metric
v2.3.0,custom metric (disable default metric for non-default objective)
v2.3.0,"custom objective, custom metric"
v2.3.0,custom metric for custom objective
v2.3.0,non-default regression metric with custom metric for custom objective
v2.3.0,multiple regression metrics with custom metric for custom objective
v2.3.0,default metric and invalid binary metric is replaced with multiclass alternative
v2.3.0,invalid objective is replaced with default multiclass one
v2.3.0,and invalid binary metric is replaced with multiclass alternative
v2.3.0,default metric for non-default multiclass objective
v2.3.0,and invalid binary metric is replaced with multiclass alternative
v2.3.0,default metric and invalid multiclass metric is replaced with binary alternative
v2.3.0,invalid multiclass metric is replaced with binary alternative for custom objective
v2.3.0,training data as eval_set
v2.3.0,feval
v2.3.0,single eval_set
v2.3.0,two eval_set
v2.3.0,coding: utf-8
v2.3.0,pylint: skip-file
v2.3.0,coding: utf-8
v2.3.0,pylint: skip-file
v2.3.0,check that default gives same result as k = 1
v2.3.0,check against independent calculation for k = 1
v2.3.0,check against independent calculation for k = 2
v2.3.0,check against independent calculation for k = 10
v2.3.0,check cases where predictions are equal
v2.3.0,no early stopping
v2.3.0,early stopping occurs
v2.3.0,test custom eval metrics
v2.3.0,"shuffle = False, override metric in params"
v2.3.0,"shuffle = True, callbacks"
v2.3.0,enable display training loss
v2.3.0,self defined folds
v2.3.0,lambdarank
v2.3.0,... with l2 metric
v2.3.0,... with NDCG (default) metric
v2.3.0,self defined folds with lambdarank
v2.3.0,test feature_names with whitespaces
v2.3.0,take subsets and train
v2.3.0,test sliced labels
v2.3.0,append some columns
v2.3.0,append some rows
v2.3.0,test sliced 2d matrix
v2.3.0,test sliced CSR
v2.3.0,"no fobj, no feval"
v2.3.0,default metric
v2.3.0,non-default metric in params
v2.3.0,default metric in args
v2.3.0,non-default metric in args
v2.3.0,metric in args overwrites one in params
v2.3.0,multiple metrics in params
v2.3.0,multiple metrics in args
v2.3.0,remove default metric by 'None' in list
v2.3.0,remove default metric by 'None' aliases
v2.3.0,"fobj, no feval"
v2.3.0,no default metric
v2.3.0,metric in params
v2.3.0,metric in args
v2.3.0,metric in args overwrites its' alias in params
v2.3.0,multiple metrics in params
v2.3.0,multiple metrics in args
v2.3.0,"no fobj, feval"
v2.3.0,default metric with custom one
v2.3.0,non-default metric in params with custom one
v2.3.0,default metric in args with custom one
v2.3.0,non-default metric in args with custom one
v2.3.0,"metric in args overwrites one in params, custom one is evaluated too"
v2.3.0,multiple metrics in params with custom one
v2.3.0,multiple metrics in args with custom one
v2.3.0,custom metric is evaluated despite 'None' is passed
v2.3.0,"fobj, feval"
v2.3.0,"no default metric, only custom one"
v2.3.0,metric in params with custom one
v2.3.0,metric in args with custom one
v2.3.0,"metric in args overwrites one in params, custom one is evaluated too"
v2.3.0,multiple metrics in params with custom one
v2.3.0,multiple metrics in args with custom one
v2.3.0,custom metric is evaluated despite 'None' is passed
v2.3.0,"no fobj, no feval"
v2.3.0,default metric
v2.3.0,default metric in params
v2.3.0,non-default metric in params
v2.3.0,multiple metrics in params
v2.3.0,remove default metric by 'None' aliases
v2.3.0,"fobj, no feval"
v2.3.0,no default metric
v2.3.0,metric in params
v2.3.0,multiple metrics in params
v2.3.0,"no fobj, feval"
v2.3.0,default metric with custom one
v2.3.0,default metric in params with custom one
v2.3.0,non-default metric in params with custom one
v2.3.0,multiple metrics in params with custom one
v2.3.0,custom metric is evaluated despite 'None' is passed
v2.3.0,"fobj, feval"
v2.3.0,"no default metric, only custom one"
v2.3.0,metric in params with custom one
v2.3.0,multiple metrics in params with custom one
v2.3.0,custom metric is evaluated despite 'None' is passed
v2.3.0,multiclass default metric
v2.3.0,multiclass default metric with custom one
v2.3.0,multiclass metric alias with custom one for custom objective
v2.3.0,no metric for invalid class_num
v2.3.0,custom metric for invalid class_num
v2.3.0,multiclass metric alias with custom one with invalid class_num
v2.3.0,multiclass default metric without num_class
v2.3.0,multiclass metric alias
v2.3.0,multiclass metric
v2.3.0,non-valid metric for multiclass objective
v2.3.0,non-default num_class for default objective
v2.3.0,no metric with non-default num_class for custom objective
v2.3.0,multiclass metric alias for custom objective
v2.3.0,multiclass metric for custom objective
v2.3.0,binary metric with non-default num_class for custom objective
v2.3.0,test XGBoost-style return value
v2.3.0,test numpy-style return value
v2.3.0,test bins string type
v2.3.0,test histogram is disabled for categorical features
v2.3.0,test for lgb.train
v2.3.0,test feval for lgb.train
v2.3.0,test with two valid data for lgb.train
v2.3.0,test for lgb.cv
v2.3.0,test feval for lgb.cv
v2.3.0,test that binning works properly for features with only positive or only negative values
v2.3.0,coding: utf-8
v2.3.0,pylint: skip-file
v2.3.0,coding: utf-8
v2.3.0,alias table
v2.3.0,names
v2.3.0,from strings
v2.3.0,tails
v2.3.0,tails
v2.3.0,coding: utf-8
v2.3.0,convert from one-based to  zero-based index
v2.3.0,convert from boundaries to size
v2.3.0,--- start Booster interfaces
v2.3.0,Single row predictor to abstract away caching logic
v2.3.0,create boosting
v2.3.0,initialize the boosting
v2.3.0,create objective function
v2.3.0,initialize the objective function
v2.3.0,create training metric
v2.3.0,reset the boosting
v2.3.0,create objective function
v2.3.0,initialize the objective function
v2.3.0,some help functions used to convert data
v2.3.0,Row iterator of on column for CSC matrix
v2.3.0,"return value at idx, only can access by ascent order"
v2.3.0,"return next non-zero pair, if index < 0, means no more data"
v2.3.0,start of c_api functions
v2.3.0,sample data first
v2.3.0,sample data first
v2.3.0,sample data first
v2.3.0,local buffer to re-use memory
v2.3.0,sample data first
v2.3.0,no more data
v2.3.0,---- start of booster
v2.3.0,---- start of some help functions
v2.3.0,data is array of pointers to individual rows
v2.3.0,set number of threads for openmp
v2.3.0,check for alias
v2.3.0,read parameters from config file
v2.3.0,"remove str after ""#"""
v2.3.0,check for alias again
v2.3.0,load configs
v2.3.0,prediction is needed if using input initial model(continued train)
v2.3.0,need to continue training
v2.3.0,sync up random seed for data partition
v2.3.0,load Training data
v2.3.0,load data for parallel training
v2.3.0,load data for single machine
v2.3.0,need save binary file
v2.3.0,create training metric
v2.3.0,only when have metrics then need to construct validation data
v2.3.0,"Add validation data, if it exists"
v2.3.0,add
v2.3.0,need save binary file
v2.3.0,add metric for validation data
v2.3.0,output used time on each iteration
v2.3.0,need init network
v2.3.0,create boosting
v2.3.0,create objective function
v2.3.0,load training data
v2.3.0,initialize the objective function
v2.3.0,initialize the boosting
v2.3.0,add validation data into boosting
v2.3.0,convert model to if-else statement code
v2.3.0,create predictor
v2.3.0,Free memory
v2.3.0,create predictor
v2.3.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.3.0,counts for all labels
v2.3.0,"start from top label, and accumulate DCG"
v2.3.0,counts for all labels
v2.3.0,calculate k Max DCG by one pass
v2.3.0,get sorted indices by score
v2.3.0,calculate dcg
v2.3.0,get sorted indices by score
v2.3.0,calculate multi dcg by one pass
v2.3.0,wait for all client start up
v2.3.0,default set to -1
v2.3.0,"distance at k-th communication, distance[k] = 2^k"
v2.3.0,set incoming rank at k-th commuication
v2.3.0,set outgoing rank at k-th commuication
v2.3.0,defalut set as -1
v2.3.0,construct all recursive halving map for all machines
v2.3.0,let 1 << k <= num_machines
v2.3.0,distance of each communication
v2.3.0,"if num_machines = 2^k, don't need to group machines"
v2.3.0,"communication direction, %2 == 0 is positive"
v2.3.0,neighbor at k-th communication
v2.3.0,receive data block at k-th communication
v2.3.0,send data block at k-th communication
v2.3.0,"if num_machines != 2^k, need to group machines"
v2.3.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.3.0,let left machine as group leader
v2.3.0,"cache block information for groups, group with 2 machines will have double block size"
v2.3.0,convert from group to node leader
v2.3.0,convert from node to group
v2.3.0,meet new group
v2.3.0,add block len for this group
v2.3.0,calculate the group block start
v2.3.0,not need to construct
v2.3.0,get receive block informations
v2.3.0,accumulate block len
v2.3.0,get send block informations
v2.3.0,accumulate block len
v2.3.0,static member definition
v2.3.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.3.0,assign the blocks to every rank.
v2.3.0,do reduce scatter
v2.3.0,do all gather
v2.3.0,assign blocks
v2.3.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.3.0,copy back
v2.3.0,assign blocks
v2.3.0,start all gather
v2.3.0,when num_machines is small and data is large
v2.3.0,use output as receive buffer
v2.3.0,get current local block size
v2.3.0,get out rank
v2.3.0,get in rank
v2.3.0,get send information
v2.3.0,get recv information
v2.3.0,send and recv at same time
v2.3.0,rotate in-place
v2.3.0,use output as receive buffer
v2.3.0,get current local block size
v2.3.0,get send information
v2.3.0,get recv information
v2.3.0,send and recv at same time
v2.3.0,use output as receive buffer
v2.3.0,send and recv at same time
v2.3.0,send local data to neighbor first
v2.3.0,receive neighbor data first
v2.3.0,reduce
v2.3.0,get target
v2.3.0,get send information
v2.3.0,get recv information
v2.3.0,send and recv at same time
v2.3.0,reduce
v2.3.0,send result to neighbor
v2.3.0,receive result from neighbor
v2.3.0,copy result
v2.3.0,start up socket
v2.3.0,parse clients from file
v2.3.0,get ip list of local machine
v2.3.0,get local rank
v2.3.0,construct listener
v2.3.0,construct communication topo
v2.3.0,construct linkers
v2.3.0,free listener
v2.3.0,set timeout
v2.3.0,accept incoming socket
v2.3.0,receive rank
v2.3.0,add new socket
v2.3.0,save ranks that need to connect with
v2.3.0,start listener
v2.3.0,start connect
v2.3.0,let smaller rank connect to larger rank
v2.3.0,send local rank
v2.3.0,wait for listener
v2.3.0,print connected linkers
v2.3.0,Get some statistic from 2 line
v2.3.0,if only have one line on file
v2.3.0,Constructors
v2.3.0,Get type tag
v2.3.0,Comparisons
v2.3.0,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.3.0,"advance until next line, or end of input"
v2.3.0,advance until closing tokens
v2.3.0,The usual case: non-escaped characters
v2.3.0,Handle escapes
v2.3.0,Extract 4-byte escape sequence
v2.3.0,Explicitly check length of the substring. The following loop
v2.3.0,relies on std::string returning the terminating NUL when
v2.3.0,accessing str[length]. Checking here reduces brittleness.
v2.3.0,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.3.0,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.3.0,whether we're in the middle of such a beast: the previous codepoint was an
v2.3.0,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.3.0,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.3.0,the UTF-16 algorithm.
v2.3.0,Integer part
v2.3.0,Decimal part
v2.3.0,Exponent part
v2.3.0,Check for any trailing garbage
v2.3.0,Documented in json11.hpp
v2.3.0,Check for another object
v2.3.0,get column names
v2.3.0,load label idx first
v2.3.0,erase label column name
v2.3.0,load ignore columns
v2.3.0,load weight idx
v2.3.0,load group idx
v2.3.0,don't support query id in data file when training in parallel
v2.3.0,read data to memory
v2.3.0,sample data
v2.3.0,construct feature bin mappers
v2.3.0,initialize label
v2.3.0,extract features
v2.3.0,sample data from file
v2.3.0,construct feature bin mappers
v2.3.0,initialize label
v2.3.0,extract features
v2.3.0,load data from binary file
v2.3.0,check meta data
v2.3.0,need to check training data
v2.3.0,read data in memory
v2.3.0,initialize label
v2.3.0,extract features
v2.3.0,Get number of lines of data file
v2.3.0,initialize label
v2.3.0,extract features
v2.3.0,load data from binary file
v2.3.0,not need to check validation data
v2.3.0,check meta data
v2.3.0,buffer to read binary file
v2.3.0,check token
v2.3.0,read size of header
v2.3.0,re-allocmate space if not enough
v2.3.0,read header
v2.3.0,get header
v2.3.0,num_groups
v2.3.0,real_feature_idx_
v2.3.0,feature2group
v2.3.0,feature2subfeature
v2.3.0,group_bin_boundaries
v2.3.0,group_feature_start_
v2.3.0,group_feature_cnt_
v2.3.0,get feature names
v2.3.0,write feature names
v2.3.0,get forced_bin_bounds_
v2.3.0,read size of meta data
v2.3.0,re-allocate space if not enough
v2.3.0,read meta data
v2.3.0,load meta data
v2.3.0,sample local used data if need to partition
v2.3.0,"if not contain query file, minimal sample unit is one record"
v2.3.0,"if contain query file, minimal sample unit is one query"
v2.3.0,if is new query
v2.3.0,read feature data
v2.3.0,read feature size
v2.3.0,re-allocate space if not enough
v2.3.0,fill feature_names_ if not header
v2.3.0,get forced split
v2.3.0,"if only one machine, find bin locally"
v2.3.0,"if have multi-machines, need to find bin distributed"
v2.3.0,different machines will find bin for different features
v2.3.0,start and len will store the process feature indices for different machines
v2.3.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.3.0,get size of bin mapper with max_bin size
v2.3.0,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.3.0,find local feature bins and copy to buffer
v2.3.0,free
v2.3.0,convert to binary size
v2.3.0,gather global feature bin mappers
v2.3.0,restore features bins from buffer
v2.3.0,---- private functions ----
v2.3.0,"if features are ordered, not need to use hist_buf"
v2.3.0,read all lines
v2.3.0,get query data
v2.3.0,"if not contain query data, minimal sample unit is one record"
v2.3.0,"if contain query data, minimal sample unit is one query"
v2.3.0,if is new query
v2.3.0,get query data
v2.3.0,"if not contain query file, minimal sample unit is one record"
v2.3.0,"if contain query file, minimal sample unit is one query"
v2.3.0,if is new query
v2.3.0,parse features
v2.3.0,-1 means doesn't use this feature
v2.3.0,get forced split
v2.3.0,"check the range of label_idx, weight_idx and group_idx"
v2.3.0,fill feature_names_ if not header
v2.3.0,start find bins
v2.3.0,"if only one machine, find bin locally"
v2.3.0,"if have multi-machines, need to find bin distributed"
v2.3.0,different machines will find bin for different features
v2.3.0,start and len will store the process feature indices for different machines
v2.3.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.3.0,get size of bin mapper with max_bin size
v2.3.0,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.3.0,find local feature bins and copy to buffer
v2.3.0,free
v2.3.0,convert to binary size
v2.3.0,gather global feature bin mappers
v2.3.0,restore features bins from buffer
v2.3.0,if doesn't need to prediction with initial model
v2.3.0,parser
v2.3.0,set label
v2.3.0,free processed line:
v2.3.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.3.0,text_reader_->Lines()[i].shrink_to_fit();
v2.3.0,push data
v2.3.0,if is used feature
v2.3.0,if need to prediction with initial model
v2.3.0,parser
v2.3.0,set initial score
v2.3.0,set label
v2.3.0,free processed line:
v2.3.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.3.0,text_reader_->Lines()[i].shrink_to_fit();
v2.3.0,push data
v2.3.0,if is used feature
v2.3.0,metadata_ will manage space of init_score
v2.3.0,text data can be free after loaded feature values
v2.3.0,parser
v2.3.0,set initial score
v2.3.0,set label
v2.3.0,push data
v2.3.0,if is used feature
v2.3.0,only need part of data
v2.3.0,need full data
v2.3.0,metadata_ will manage space of init_score
v2.3.0,read size of token
v2.3.0,remove duplicates
v2.3.0,deep copy function for BinMapper
v2.3.0,mean size for one bin
v2.3.0,need a new bin
v2.3.0,update bin upper bound
v2.3.0,last bin upper bound
v2.3.0,get list of distinct values
v2.3.0,get number of positive and negative distinct values
v2.3.0,include zero bounds and infinity bound
v2.3.0,"add forced bounds, excluding zeros since we have already added zero bounds"
v2.3.0,find remaining bounds
v2.3.0,find distinct_values first
v2.3.0,push zero in the front
v2.3.0,use the large value
v2.3.0,push zero in the back
v2.3.0,convert to int type first
v2.3.0,sort by counts
v2.3.0,avoid first bin is zero
v2.3.0,will ignore the categorical of small counts
v2.3.0,need an additional bin for NaN
v2.3.0,use -1 to represent NaN
v2.3.0,Use MissingType::None to represent this bin contains all categoricals
v2.3.0,check trivial(num_bin_ == 1) feature
v2.3.0,check useless bin
v2.3.0,calculate sparse rate
v2.3.0,sparse threshold
v2.3.0,"for lambdarank, it needs query data for partition data in parallel learning"
v2.3.0,need convert query_id to boundaries
v2.3.0,check weights
v2.3.0,check query boundries
v2.3.0,contain initial score file
v2.3.0,check weights
v2.3.0,get local weights
v2.3.0,check query boundries
v2.3.0,get local query boundaries
v2.3.0,contain initial score file
v2.3.0,get local initial scores
v2.3.0,re-load query weight
v2.3.0,save to nullptr
v2.3.0,save to nullptr
v2.3.0,save to nullptr
v2.3.0,default weight file name
v2.3.0,default weight file name
v2.3.0,use first line to count number class
v2.3.0,default query file name
v2.3.0,root is in the depth 0
v2.3.0,non-leaf
v2.3.0,leaf
v2.3.0,use this for the missing value conversion
v2.3.0,Predict func by Map to ifelse
v2.3.0,use this for the missing value conversion
v2.3.0,non-leaf
v2.3.0,left subtree
v2.3.0,right subtree
v2.3.0,leaf
v2.3.0,non-leaf
v2.3.0,left subtree
v2.3.0,right subtree
v2.3.0,leaf
v2.3.0,recursive computation of SHAP values for a decision tree
v2.3.0,extend the unique path
v2.3.0,leaf node
v2.3.0,internal node
v2.3.0,"see if we have already split on this feature,"
v2.3.0,if so we undo that split so we can redo it for this node
v2.3.0,add names of objective function if not providing metric
v2.3.0,generate seeds by seed.
v2.3.0,sort eval_at
v2.3.0,Only push the non-training data
v2.3.0,check for conflicts
v2.3.0,"check if objective, metric, and num_class match"
v2.3.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.3.0,Check max_depth and num_leaves
v2.3.0,"Fits in an int, and is more restrictive than the current num_leaves"
v2.3.0,"filter is based on sampling data, so decrease its range"
v2.3.0,put dense feature first
v2.3.0,sort by non zero cnt
v2.3.0,"sort by non zero cnt, bigger first"
v2.3.0,"take apart small sparse group, due it will not gain on speed"
v2.3.0,shuffle groups
v2.3.0,get num_features
v2.3.0,get bin_mappers
v2.3.0,copy feature bin mapper data
v2.3.0,copy feature bin mapper data
v2.3.0,"if not pass a filename, just append "".bin"" of original file"
v2.3.0,get size of header
v2.3.0,size of feature names
v2.3.0,size of forced bins
v2.3.0,write header
v2.3.0,write feature names
v2.3.0,write forced bins
v2.3.0,get size of meta data
v2.3.0,write meta data
v2.3.0,write feature data
v2.3.0,get size of feature
v2.3.0,write feature
v2.3.0,feature is not used
v2.3.0,construct histograms for smaller leaf
v2.3.0,if not use ordered bin
v2.3.0,used ordered bin
v2.3.0,feature is not used
v2.3.0,construct histograms for smaller leaf
v2.3.0,if not use ordered bin
v2.3.0,used ordered bin
v2.3.0,fixed hessian.
v2.3.0,feature is not used
v2.3.0,construct histograms for smaller leaf
v2.3.0,if not use ordered bin
v2.3.0,used ordered bin
v2.3.0,feature is not used
v2.3.0,construct histograms for smaller leaf
v2.3.0,if not use ordered bin
v2.3.0,used ordered bin
v2.3.0,fixed hessian.
v2.3.0,Skip the leading 0 when copying group_bin_boundaries.
v2.3.0,PredictRaw
v2.3.0,PredictRawByMap
v2.3.0,Predict
v2.3.0,PredictByMap
v2.3.0,PredictLeafIndex
v2.3.0,PredictLeafIndexByMap
v2.3.0,output model type
v2.3.0,output number of class
v2.3.0,output label index
v2.3.0,output max_feature_idx
v2.3.0,output objective
v2.3.0,output tree models
v2.3.0,store the importance first
v2.3.0,sort the importance
v2.3.0,use serialized string to restore this object
v2.3.0,Use first 128 chars to avoid exceed the message buffer.
v2.3.0,get number of classes
v2.3.0,get index of label
v2.3.0,get max_feature_idx first
v2.3.0,get average_output
v2.3.0,get feature names
v2.3.0,get monotone_constraints
v2.3.0,set zero
v2.3.0,predict all the trees for one iteration
v2.3.0,check early stopping
v2.3.0,set zero
v2.3.0,predict all the trees for one iteration
v2.3.0,check early stopping
v2.3.0,margin_threshold will be captured by value
v2.3.0,copy and sort
v2.3.0,margin_threshold will be captured by value
v2.3.0,load forced_splits file
v2.3.0,init tree learner
v2.3.0,push training metrics
v2.3.0,create buffer for gradients and hessians
v2.3.0,get max feature index
v2.3.0,get label index
v2.3.0,get feature names
v2.3.0,"if need bagging, create buffer"
v2.3.0,"for a validation dataset, we need its score and metric"
v2.3.0,update score
v2.3.0,objective function will calculate gradients and hessians
v2.3.0,"random bagging, minimal unit is one record"
v2.3.0,from right to left
v2.3.0,"random bagging, minimal unit is one record"
v2.3.0,reverse right buffer
v2.3.0,if need bagging
v2.3.0,set bagging data to tree learner
v2.3.0,get subset
v2.3.0,output used time per iteration
v2.3.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.3.0,boosting first
v2.3.0,bagging logic
v2.3.0,need to copy gradients for bagging subset.
v2.3.0,shrinkage by learning rate
v2.3.0,update score
v2.3.0,only add default score one-time
v2.3.0,updates scores
v2.3.0,add model
v2.3.0,reset score
v2.3.0,remove model
v2.3.0,print message for metric
v2.3.0,pop last early_stopping_round_ models
v2.3.0,update training score
v2.3.0,we need to predict out-of-bag scores of data for boosting
v2.3.0,update validation score
v2.3.0,print training metric
v2.3.0,print validation metric
v2.3.0,set zero
v2.3.0,predict all the trees for one iteration
v2.3.0,check early stopping
v2.3.0,push training metrics
v2.3.0,"not same training data, need reset score and others"
v2.3.0,create score tracker
v2.3.0,update score
v2.3.0,create buffer for gradients and hessians
v2.3.0,"if need bagging, create buffer"
v2.3.0,Get the max size of pool
v2.3.0,at least need 2 leaves
v2.3.0,push split information for all leaves
v2.3.0,get ordered bin
v2.3.0,check existing for ordered bin
v2.3.0,initialize splits for leaf
v2.3.0,initialize data partition
v2.3.0,initialize ordered gradients and hessians
v2.3.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.3.0,get ordered bin
v2.3.0,initialize splits for leaf
v2.3.0,initialize data partition
v2.3.0,initialize ordered gradients and hessians
v2.3.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.3.0,Get the max size of pool
v2.3.0,at least need 2 leaves
v2.3.0,push split information for all leaves
v2.3.0,some initial works before training
v2.3.0,root leaf
v2.3.0,only root leaf can be splitted on first time
v2.3.0,some initial works before finding best split
v2.3.0,find best threshold for every feature
v2.3.0,Get a leaf with max split gain
v2.3.0,Get split information for best leaf
v2.3.0,"cannot split, quit"
v2.3.0,split tree with best leaf
v2.3.0,reset histogram pool
v2.3.0,initialize data partition
v2.3.0,reset the splits for leaves
v2.3.0,Sumup for root
v2.3.0,use all data
v2.3.0,"use bagging, only use part of data"
v2.3.0,"if has ordered bin, need to initialize the ordered bin"
v2.3.0,"use all data, pass nullptr"
v2.3.0,"bagging, only use part of data"
v2.3.0,mark used data
v2.3.0,initialize ordered bin
v2.3.0,check depth of current leaf
v2.3.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.3.0,no enough data to continue
v2.3.0,only have root
v2.3.0,put parent(left) leaf's histograms into larger leaf's histograms
v2.3.0,put parent(left) leaf's histograms to larger leaf's histograms
v2.3.0,split for the ordered bin
v2.3.0,mark data that at left-leaf
v2.3.0,split the ordered bin
v2.3.0,construct smaller leaf
v2.3.0,construct larger leaf
v2.3.0,find splits
v2.3.0,only has root leaf
v2.3.0,find best threshold for larger child
v2.3.0,start at root leaf
v2.3.0,"before processing next node from queue, store info for current left/right leaf"
v2.3.0,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.3.0,"then, compute own splits"
v2.3.0,split info should exist because searching in bfs fashion - should have added from parent
v2.3.0,"split tree, will return right leaf"
v2.3.0,left = parent
v2.3.0,"split tree, will return right leaf"
v2.3.0,init the leaves that used on next iteration
v2.3.0,bag_mapper[index_mapper[i]]
v2.3.0,get feature partition
v2.3.0,get local used features
v2.3.0,get best split at smaller leaf
v2.3.0,find local best split for larger leaf
v2.3.0,sync global best info
v2.3.0,update best split
v2.3.0,"instantiate template classes, otherwise linker cannot find the code"
v2.3.0,initialize SerialTreeLearner
v2.3.0,Get local rank and global machine size
v2.3.0,allocate buffer for communication
v2.3.0,generate feature partition for current tree
v2.3.0,get local used feature
v2.3.0,get block start and block len for reduce scatter
v2.3.0,get buffer_write_start_pos_
v2.3.0,get buffer_read_start_pos_
v2.3.0,sync global data sumup info
v2.3.0,global sumup reduce
v2.3.0,copy back
v2.3.0,set global sumup info
v2.3.0,init global data count in leaf
v2.3.0,construct local histograms
v2.3.0,copy to buffer
v2.3.0,Reduce scatter for histogram
v2.3.0,restore global histograms from buffer
v2.3.0,find best threshold for smaller child
v2.3.0,only root leaf
v2.3.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.3.0,find best threshold for larger child
v2.3.0,find local best split for larger leaf
v2.3.0,sync global best info
v2.3.0,set best split
v2.3.0,need update global number of data in leaf
v2.3.0,"instantiate template classes, otherwise linker cannot find the code"
v2.3.0,initialize SerialTreeLearner
v2.3.0,some additional variables needed for GPU trainer
v2.3.0,Initialize GPU buffers and kernels
v2.3.0,some functions used for debugging the GPU histogram construction
v2.3.0,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.3.0,goto err;
v2.3.0,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.3.0,goto err;
v2.3.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.3.0,also guarantee that there are at least 2K examples per workgroup
v2.3.0,return 0;
v2.3.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.3.0,decide the best number of workgroups working on one feature4 tuple
v2.3.0,set work group size based on feature size
v2.3.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.3.0,we need to refresh the kernel arguments after reallocating
v2.3.0,The only argument that needs to be changed later is num_data_
v2.3.0,"the GPU kernel will process all features in one call, and each"
v2.3.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.3.0,process one feature4 tuple
v2.3.0,"for the root node, indices are not copied"
v2.3.0,"for constant hessian, hessians are not copied except for the root node"
v2.3.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.3.0,and we will launch num_feature workgroups for this kernel
v2.3.0,will launch threads for all features
v2.3.0,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.3.0,copy the results asynchronously. Size depends on if double precision is used
v2.3.0,we will wait for this object in WaitAndGetHistograms
v2.3.0,"when the output is ready, the computation is done"
v2.3.0,values of this feature has been redistributed to multiple bins; need a reduction here
v2.3.0,how many feature-group tuples we have
v2.3.0,leave some safe margin for prefetching
v2.3.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.3.0,clear sparse/dense maps
v2.3.0,do nothing if no features can be processed on GPU
v2.3.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.3.0,unpin old buffer if necessary before destructing them
v2.3.0,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.3.0,allocate space for gradients and hessians on device
v2.3.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.3.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.3.0,copy indices to the device
v2.3.0,histogram bin entry size depends on the precision (single/double)
v2.3.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.3.0,each work group generates a sub-histogram of dword_features_ features.
v2.3.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.3.0,create atomic counters for inter-group coordination
v2.3.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.3.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.3.0,looking for dword_features_ non-sparse feature-groups
v2.3.0,decide if we need to redistribute the bin
v2.3.0,multiplier must be a power of 2
v2.3.0,device_bin_mults_.push_back(1);
v2.3.0,found
v2.3.0,for data transfer time
v2.3.0,"Now generate new data structure feature4, and copy data to the device"
v2.3.0,"preallocate arrays for all threads, and pin them"
v2.3.0,building Feature4 bundles; each thread handles dword_features_ features
v2.3.0,one feature datapoint is 4 bits
v2.3.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.3.0,one feature datapoint is one byte
v2.3.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.3.0,Dense bin
v2.3.0,Dense 4-bit bin
v2.3.0,working on the remaining (less than dword_features_) feature groups
v2.3.0,fill the leftover features
v2.3.0,"fill this empty feature with some ""random"" value"
v2.3.0,"fill this empty feature with some ""random"" value"
v2.3.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.3.0,deallocate pinned space for feature copying
v2.3.0,data transfer time
v2.3.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.3.0,"Something bad happened. Just return ""No log available."""
v2.3.0,"build is okay, log may contain warnings"
v2.3.0,destroy any old kernels
v2.3.0,create OpenCL kernels for different number of workgroups per feature
v2.3.0,currently we don't use constant memory
v2.3.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v2.3.0,kernel with indices in an array
v2.3.0,"kernel with all features enabled, with elimited branches"
v2.3.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.3.0,do nothing if no features can be processed on GPU
v2.3.0,The only argument that needs to be changed later is num_data_
v2.3.0,"hessian is passed as a parameter, but it is not available now."
v2.3.0,hessian will be set in BeforeTrain()
v2.3.0,"Get the max bin size, used for selecting best GPU kernel"
v2.3.0,initialize GPU
v2.3.0,determine which kernel to use based on the max number of bins
v2.3.0,setup GPU kernel arguments after we allocating all the buffers
v2.3.0,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.3.0,this should rarely occur
v2.3.0,GPU memory has to been reallocated because data may have been changed
v2.3.0,setup GPU kernel arguments after we allocating all the buffers
v2.3.0,Copy initial full hessians and gradients to GPU.
v2.3.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.3.0,setup hessian parameters only
v2.3.0,hessian is passed as a parameter
v2.3.0,use bagging
v2.3.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.3.0,copy used gradients and hessians to ordered buffer
v2.3.0,transfer the indices to GPU
v2.3.0,transfer hessian to GPU
v2.3.0,setup hessian parameters only
v2.3.0,hessian is passed as a parameter
v2.3.0,transfer gradients to GPU
v2.3.0,only have root
v2.3.0,"Copy indices, gradients and hessians as early as possible"
v2.3.0,only need to initialize for smaller leaf
v2.3.0,Get leaf boundary
v2.3.0,copy indices to the GPU:
v2.3.0,copy ordered hessians to the GPU:
v2.3.0,copy ordered gradients to the GPU:
v2.3.0,do nothing if no features can be processed on GPU
v2.3.0,copy data indices if it is not null
v2.3.0,generate and copy ordered_gradients if gradients is not null
v2.3.0,generate and copy ordered_hessians if hessians is not null
v2.3.0,converted indices in is_feature_used to feature-group indices
v2.3.0,construct the feature masks for dense feature-groups
v2.3.0,"if no feature group is used, just return and do not use GPU"
v2.3.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.3.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.3.0,"All data have been prepared, now run the GPU kernel"
v2.3.0,construct smaller leaf
v2.3.0,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.3.0,then construct sparse features on CPU
v2.3.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.3.0,"wait for GPU to finish, only if GPU is actually used"
v2.3.0,use double precision
v2.3.0,use single precision
v2.3.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.3.0,#define GPU_DEBUG_COMPARE
v2.3.0,construct larger leaf
v2.3.0,then construct sparse features on CPU
v2.3.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.3.0,"wait for GPU to finish, only if GPU is actually used"
v2.3.0,use double precision
v2.3.0,use single precision
v2.3.0,do some sanity check for the GPU algorithm
v2.3.0,limit top k
v2.3.0,get max bin
v2.3.0,calculate buffer size
v2.3.0,"left and right on same time, so need double size"
v2.3.0,initialize histograms for global
v2.3.0,sync global data sumup info
v2.3.0,set global sumup info
v2.3.0,init global data count in leaf
v2.3.0,get local sumup
v2.3.0,get local sumup
v2.3.0,get mean number on machines
v2.3.0,weighted gain
v2.3.0,get top k
v2.3.0,"Copy histogram to buffer, and Get local aggregate features"
v2.3.0,copy histograms.
v2.3.0,copy smaller leaf histograms first
v2.3.0,mark local aggregated feature
v2.3.0,copy
v2.3.0,then copy larger leaf histograms
v2.3.0,mark local aggregated feature
v2.3.0,copy
v2.3.0,use local data to find local best splits
v2.3.0,find splits
v2.3.0,only has root leaf
v2.3.0,find best threshold for larger child
v2.3.0,local voting
v2.3.0,gather
v2.3.0,get all top-k from all machines
v2.3.0,global voting
v2.3.0,copy local histgrams to buffer
v2.3.0,Reduce scatter for histogram
v2.3.0,find best split from local aggregated histograms
v2.3.0,restore from buffer
v2.3.0,find best threshold
v2.3.0,restore from buffer
v2.3.0,find best threshold
v2.3.0,find local best
v2.3.0,find local best split for larger leaf
v2.3.0,sync global best info
v2.3.0,copy back
v2.3.0,set the global number of data for leaves
v2.3.0,init the global sumup info
v2.3.0,"instantiate template classes, otherwise linker cannot find the code"
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable=invalid-name, exec-used, C0111"
v2.2.3,Apple Clang with OpenMP
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, W0105"
v2.2.3,create predictor first
v2.2.3,check dataset
v2.2.3,reduce cost for prediction training data
v2.2.3,process callbacks
v2.2.3,Most of legacy advanced options becomes callbacks
v2.2.3,construct booster
v2.2.3,start training
v2.2.3,check evaluation result.
v2.2.3,"lambdarank task, split according to groups"
v2.2.3,run preprocessing on the data set if needed
v2.2.3,setup callbacks
v2.2.3,coding: utf-8
v2.2.3,pylint: disable = C0103
v2.2.3,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.2.3,because of u'...' Unicode literals.
v2.2.3,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.2.3,minor change to support `**kwargs`
v2.2.3,"user can set verbose with kwargs, it has higher priority"
v2.2.3,register default metric for consistency with callable eval_metric case
v2.2.3,try to deduce from class instance
v2.2.3,overwrite default metric by explicitly set metric
v2.2.3,concatenate metric from params (or default if not provided in params) and eval_metric
v2.2.3,reduce cost for prediction training data
v2.2.3,free dataset
v2.2.3,Switch to using a multiclass objective in the underlying LGBM instance
v2.2.3,check group data
v2.2.3,coding: utf-8
v2.2.3,we don't need lib_lightgbm while building docs
v2.2.3,coding: utf-8
v2.2.3,pylint: disable = C0103
v2.2.3,coding: utf-8
v2.2.3,REMOVEME: remove warning after 2.3.0 version release
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, C0111, C0301"
v2.2.3,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.2.3,TypeError: obj is not a string or a number
v2.2.3,ValueError: invalid literal
v2.2.3,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.3,avoid memory consumption by arrays concatenation operations
v2.2.3,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.3,avoid memory consumption by arrays concatenation operations
v2.2.3,process for args
v2.2.3,"user can set verbose with params, it has higher priority"
v2.2.3,get categorical features
v2.2.3,process for reference dataset
v2.2.3,start construct data
v2.2.3,check data has header or not
v2.2.3,load init score
v2.2.3,need re group init score
v2.2.3,set feature names
v2.2.3,"change non-float data to float data, need to copy"
v2.2.3,"change non-float data to float data, need to copy"
v2.2.3,create valid
v2.2.3,construct subset
v2.2.3,create train
v2.2.3,set to None
v2.2.3,we're done if self and reference share a common upstrem reference
v2.2.3,"group data from LightGBM is boundaries data, need to convert to group size"
v2.2.3,"user can set verbose with params, it has higher priority"
v2.2.3,Training task
v2.2.3,set network if necessary
v2.2.3,construct booster object
v2.2.3,save reference to data
v2.2.3,buffer for inner predict
v2.2.3,Prediction task
v2.2.3,need reset training data
v2.2.3,need to push new valid data
v2.2.3,"if buffer length is not long enough, re-allocate a buffer"
v2.2.3,"if buffer length is not long enough, reallocate a buffer"
v2.2.3,Copy models
v2.2.3,Get name of features
v2.2.3,avoid to predict many time in one iteration
v2.2.3,Get num of inner evals
v2.2.3,Get name of evals
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, W0105, C0301"
v2.2.3,Callback environment used by callbacks
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, C0111"
v2.2.3,load or create your dataset
v2.2.3,create dataset for lightgbm
v2.2.3,"if you want to re-use data, remember to set free_raw_data=False"
v2.2.3,specify your configurations as a dict
v2.2.3,generate feature names
v2.2.3,feature_name and categorical_feature
v2.2.3,check feature name
v2.2.3,save model to file
v2.2.3,dump model to JSON (and save to file)
v2.2.3,feature names
v2.2.3,feature importances
v2.2.3,load model to predict
v2.2.3,can only predict with the best iteration (or the saving iteration)
v2.2.3,eval with loaded model
v2.2.3,dump model with pickle
v2.2.3,load model with pickle to predict
v2.2.3,can predict with any iteration when loaded in pickle way
v2.2.3,eval with loaded model
v2.2.3,continue training
v2.2.3,init_model accepts:
v2.2.3,1. model file name
v2.2.3,2. Booster()
v2.2.3,decay learning rates
v2.2.3,learning_rates accepts:
v2.2.3,1. list/tuple with length = num_boost_round
v2.2.3,2. function(curr_iter)
v2.2.3,change other parameters during training
v2.2.3,self-defined objective function
v2.2.3,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.2.3,log likelihood loss
v2.2.3,self-defined eval metric
v2.2.3,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.3,binary error
v2.2.3,callback
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, C0111"
v2.2.3,load or create your dataset
v2.2.3,train
v2.2.3,predict
v2.2.3,eval
v2.2.3,feature importances
v2.2.3,self-defined eval metric
v2.2.3,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.3,Root Mean Squared Logarithmic Error (RMSLE)
v2.2.3,train
v2.2.3,predict
v2.2.3,eval
v2.2.3,other scikit-learn modules
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, C0111"
v2.2.3,load or create your dataset
v2.2.3,create dataset for lightgbm
v2.2.3,specify your configurations as a dict
v2.2.3,train
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, C0111"
v2.2.3,################
v2.2.3,Simulate some binary data with a single categorical and
v2.2.3,single continuous predictor
v2.2.3,################
v2.2.3,Set up a couple of utilities for our experiments
v2.2.3,################
v2.2.3,Observe the behavior of `binary` and `xentropy` objectives
v2.2.3,Trying this throws an error on non-binary values of y:
v2.2.3,"experiment('binary', label_type='probability', DATA)"
v2.2.3,The speed of `binary` is not drastically different than
v2.2.3,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v2.2.3,there are reasons to suspect that `binary` should run faster when the
v2.2.3,label is an integer instead of a float
v2.2.3,coding: utf-8
v2.2.3,"pylint: disable = invalid-name, C0111"
v2.2.3,load or create your dataset
v2.2.3,create dataset for lightgbm
v2.2.3,specify your configurations as a dict
v2.2.3,train
v2.2.3,save model to file
v2.2.3,predict
v2.2.3,eval
v2.2.3,coding: utf-8
v2.2.3,!/usr/bin/env python3
v2.2.3,-*- coding: utf-8 -*-
v2.2.3,
v2.2.3,"LightGBM documentation build configuration file, created by"
v2.2.3,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.2.3,
v2.2.3,This file is execfile()d with the current directory set to its
v2.2.3,containing dir.
v2.2.3,
v2.2.3,Note that not all possible configuration values are present in this
v2.2.3,autogenerated file.
v2.2.3,
v2.2.3,All configuration values have a default; values that are commented out
v2.2.3,serve to show the default.
v2.2.3,"If extensions (or modules to document with autodoc) are in another directory,"
v2.2.3,add these directories to sys.path here. If the directory is relative to the
v2.2.3,"documentation root, use os.path.abspath to make it absolute."
v2.2.3,-- mock out modules
v2.2.3,-- General configuration ------------------------------------------------
v2.2.3,"If your documentation needs a minimal Sphinx version, state it here."
v2.2.3,"Add any Sphinx extension module names here, as strings. They can be"
v2.2.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.2.3,ones.
v2.2.3,"Add any paths that contain templates here, relative to this directory."
v2.2.3,The suffix(es) of source filenames.
v2.2.3,You can specify multiple suffix as a list of string:
v2.2.3,"source_suffix = ['.rst', '.md']"
v2.2.3,The master toctree document.
v2.2.3,General information about the project.
v2.2.3,"The version info for the project you're documenting, acts as replacement for"
v2.2.3,"|version| and |release|, also used in various other places throughout the"
v2.2.3,built documents.
v2.2.3,
v2.2.3,The short X.Y version.
v2.2.3,"The full version, including alpha/beta/rc tags."
v2.2.3,The language for content autogenerated by Sphinx. Refer to documentation
v2.2.3,for a list of supported languages.
v2.2.3,
v2.2.3,This is also used if you do content translation via gettext catalogs.
v2.2.3,"Usually you set ""language"" from the command line for these cases."
v2.2.3,"List of patterns, relative to source directory, that match files and"
v2.2.3,directories to ignore when looking for source files.
v2.2.3,This patterns also effect to html_static_path and html_extra_path
v2.2.3,The name of the Pygments (syntax highlighting) style to use.
v2.2.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.2.3,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.2.3,-- Options for HTML output ----------------------------------------------
v2.2.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.2.3,a list of builtin themes.
v2.2.3,Theme options are theme-specific and customize the look and feel of a theme
v2.2.3,"further.  For a list of options available for each theme, see the"
v2.2.3,documentation.
v2.2.3,"Add any paths that contain custom static files (such as style sheets) here,"
v2.2.3,"relative to this directory. They are copied after the builtin static files,"
v2.2.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.2.3,-- Options for HTMLHelp output ------------------------------------------
v2.2.3,Output file base name for HTML help builder.
v2.2.3,coding: utf-8
v2.2.3,coding: utf-8
v2.2.3,pylint: skip-file
v2.2.3,we don't need lib_lightgbm while building docs
v2.2.3,coding: utf-8
v2.2.3,pylint: skip-file
v2.2.3,check saved model persistence
v2.2.3,"we need to check the consistency of model file here, so test for exact equal"
v2.2.3,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.2.3,"scores likely to be different, but prediction should still be the same"
v2.2.3,coding: utf-8
v2.2.3,pylint: skip-file
v2.2.3,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v2.2.3,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.2.3,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.2.3,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.2.3,Tests same probabilities
v2.2.3,Tests same predictions
v2.2.3,Tests same raw scores
v2.2.3,Tests same leaf indices
v2.2.3,Tests same feature contributions
v2.2.3,Tests other parameters for the prediction works
v2.2.3,"no custom objective, no custom metric"
v2.2.3,default metric
v2.2.3,non-default metric
v2.2.3,no metric
v2.2.3,non-default metric in eval_metric
v2.2.3,non-default metric with non-default metric in eval_metric
v2.2.3,non-default metric with multiple metrics in eval_metric
v2.2.3,default metric for non-default objective
v2.2.3,non-default metric for non-default objective
v2.2.3,no metric
v2.2.3,non-default metric in eval_metric for non-default objective
v2.2.3,non-default metric with non-default metric in eval_metric for non-default objective
v2.2.3,non-default metric with multiple metrics in eval_metric for non-default objective
v2.2.3,"custom objective, no custom metric"
v2.2.3,default regression metric for custom objective
v2.2.3,non-default regression metric for custom objective
v2.2.3,multiple regression metrics for custom objective
v2.2.3,no metric
v2.2.3,default regression metric with non-default metric in eval_metric for custom objective
v2.2.3,non-default regression metric with metric in eval_metric for custom objective
v2.2.3,multiple regression metrics with metric in eval_metric for custom objective
v2.2.3,multiple regression metrics with multiple metrics in eval_metric for custom objective
v2.2.3,"no custom objective, custom metric"
v2.2.3,default metric with custom metric
v2.2.3,non-default metric with custom metric
v2.2.3,multiple metrics with custom metric
v2.2.3,custom metric (disable default metric)
v2.2.3,default metric for non-default objective with custom metric
v2.2.3,non-default metric for non-default objective with custom metric
v2.2.3,multiple metrics for non-default objective with custom metric
v2.2.3,custom metric (disable default metric for non-default objective)
v2.2.3,"custom objective, custom metric"
v2.2.3,custom metric for custom objective
v2.2.3,non-default regression metric with custom metric for custom objective
v2.2.3,multiple regression metrics with custom metric for custom objective
v2.2.3,default metric and invalid binary metric is replaced with multiclass alternative
v2.2.3,invalid objective is replaced with default multiclass one
v2.2.3,and invalid binary metric is replaced with multiclass alternative
v2.2.3,default metric for non-default multiclass objective
v2.2.3,and invalid binary metric is replaced with multiclass alternative
v2.2.3,default metric and invalid multiclass metric is replaced with binary alternative
v2.2.3,invalid multiclass metric is replaced with binary alternative for custom objective
v2.2.3,coding: utf-8
v2.2.3,pylint: skip-file
v2.2.3,coding: utf-8
v2.2.3,pylint: skip-file
v2.2.3,no early stopping
v2.2.3,early stopping occurs
v2.2.3,test custom eval metrics
v2.2.3,"shuffle = False, override metric in params"
v2.2.3,"shuffle = True, callbacks"
v2.2.3,self defined folds
v2.2.3,lambdarank
v2.2.3,... with l2 metric
v2.2.3,... with NDCG (default) metric
v2.2.3,self defined folds with lambdarank
v2.2.3,test feature_names with whitespaces
v2.2.3,take subsets and train
v2.2.3,test sliced labels
v2.2.3,append some columns
v2.2.3,append some rows
v2.2.3,test sliced 2d matrix
v2.2.3,test sliced CSR
v2.2.3,"no fobj, no feval"
v2.2.3,default metric
v2.2.3,non-default metric in params
v2.2.3,default metric in args
v2.2.3,non-default metric in args
v2.2.3,metric in args overwrites one in params
v2.2.3,multiple metrics in params
v2.2.3,multiple metrics in args
v2.2.3,remove default metric by 'None' in list
v2.2.3,remove default metric by 'None' aliases
v2.2.3,"fobj, no feval"
v2.2.3,no default metric
v2.2.3,metric in params
v2.2.3,metric in args
v2.2.3,metric in args overwrites its' alias in params
v2.2.3,multiple metrics in params
v2.2.3,multiple metrics in args
v2.2.3,"no fobj, feval"
v2.2.3,default metric with custom one
v2.2.3,non-default metric in params with custom one
v2.2.3,default metric in args with custom one
v2.2.3,non-default metric in args with custom one
v2.2.3,"metric in args overwrites one in params, custom one is evaluated too"
v2.2.3,multiple metrics in params with custom one
v2.2.3,multiple metrics in args with custom one
v2.2.3,custom metric is evaluated despite 'None' is passed
v2.2.3,"fobj, feval"
v2.2.3,"no default metric, only custom one"
v2.2.3,metric in params with custom one
v2.2.3,metric in args with custom one
v2.2.3,"metric in args overwrites one in params, custom one is evaluated too"
v2.2.3,multiple metrics in params with custom one
v2.2.3,multiple metrics in args with custom one
v2.2.3,custom metric is evaluated despite 'None' is passed
v2.2.3,"no fobj, no feval"
v2.2.3,default metric
v2.2.3,default metric in params
v2.2.3,non-default metric in params
v2.2.3,multiple metrics in params
v2.2.3,remove default metric by 'None' aliases
v2.2.3,"fobj, no feval"
v2.2.3,no default metric
v2.2.3,metric in params
v2.2.3,multiple metrics in params
v2.2.3,"no fobj, feval"
v2.2.3,default metric with custom one
v2.2.3,default metric in params with custom one
v2.2.3,non-default metric in params with custom one
v2.2.3,multiple metrics in params with custom one
v2.2.3,custom metric is evaluated despite 'None' is passed
v2.2.3,"fobj, feval"
v2.2.3,"no default metric, only custom one"
v2.2.3,metric in params with custom one
v2.2.3,multiple metrics in params with custom one
v2.2.3,custom metric is evaluated despite 'None' is passed
v2.2.3,multiclass default metric
v2.2.3,multiclass default metric with custom one
v2.2.3,multiclass metric alias with custom one for custom objective
v2.2.3,no metric for invalid class_num
v2.2.3,custom metric for invalid class_num
v2.2.3,multiclass metric alias with custom one with invalid class_num
v2.2.3,multiclass default metric without num_class
v2.2.3,multiclass metric alias
v2.2.3,multiclass metric
v2.2.3,non-valid metric for multiclass objective
v2.2.3,non-default num_class for default objective
v2.2.3,no metric with non-default num_class for custom objective
v2.2.3,multiclass metric alias for custom objective
v2.2.3,multiclass metric for custom objective
v2.2.3,binary metric with non-default num_class for custom objective
v2.2.3,coding: utf-8
v2.2.3,pylint: skip-file
v2.2.3,Register Dynamic Symbols
v2.2.3,coding: utf-8
v2.2.3,alias table
v2.2.3,names
v2.2.3,from strings
v2.2.3,tails
v2.2.3,tails
v2.2.3,coding: utf-8
v2.2.3,convert from one-based to  zero-based index
v2.2.3,convert from boundaries to size
v2.2.3,--- start Booster interfaces
v2.2.3,create boosting
v2.2.3,initialize the boosting
v2.2.3,create objective function
v2.2.3,initialize the objective function
v2.2.3,create training metric
v2.2.3,reset the boosting
v2.2.3,create objective function
v2.2.3,initialize the objective function
v2.2.3,some help functions used to convert data
v2.2.3,Row iterator of on column for CSC matrix
v2.2.3,"return value at idx, only can access by ascent order"
v2.2.3,"return next non-zero pair, if index < 0, means no more data"
v2.2.3,start of c_api functions
v2.2.3,sample data first
v2.2.3,sample data first
v2.2.3,sample data first
v2.2.3,no more data
v2.2.3,---- start of booster
v2.2.3,---- start of some help functions
v2.2.3,set number of threads for openmp
v2.2.3,check for alias
v2.2.3,read parameters from config file
v2.2.3,"remove str after ""#"""
v2.2.3,check for alias again
v2.2.3,load configs
v2.2.3,prediction is needed if using input initial model(continued train)
v2.2.3,need to continue training
v2.2.3,sync up random seed for data partition
v2.2.3,load Training data
v2.2.3,load data for parallel training
v2.2.3,load data for single machine
v2.2.3,need save binary file
v2.2.3,create training metric
v2.2.3,only when have metrics then need to construct validation data
v2.2.3,"Add validation data, if it exists"
v2.2.3,add
v2.2.3,need save binary file
v2.2.3,add metric for validation data
v2.2.3,output used time on each iteration
v2.2.3,need init network
v2.2.3,create boosting
v2.2.3,create objective function
v2.2.3,load training data
v2.2.3,initialize the objective function
v2.2.3,initialize the boosting
v2.2.3,add validation data into boosting
v2.2.3,convert model to if-else statement code
v2.2.3,create predictor
v2.2.3,Free memory
v2.2.3,create predictor
v2.2.3,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.2.3,counts for all labels
v2.2.3,"start from top label, and accumulate DCG"
v2.2.3,counts for all labels
v2.2.3,calculate k Max DCG by one pass
v2.2.3,get sorted indices by score
v2.2.3,calculate dcg
v2.2.3,get sorted indices by score
v2.2.3,calculate multi dcg by one pass
v2.2.3,wait for all client start up
v2.2.3,default set to -1
v2.2.3,"distance at k-th communication, distance[k] = 2^k"
v2.2.3,set incoming rank at k-th commuication
v2.2.3,set outgoing rank at k-th commuication
v2.2.3,defalut set as -1
v2.2.3,construct all recursive halving map for all machines
v2.2.3,let 1 << k <= num_machines
v2.2.3,distance of each communication
v2.2.3,"if num_machines = 2^k, don't need to group machines"
v2.2.3,"communication direction, %2 == 0 is positive"
v2.2.3,neighbor at k-th communication
v2.2.3,receive data block at k-th communication
v2.2.3,send data block at k-th communication
v2.2.3,"if num_machines != 2^k, need to group machines"
v2.2.3,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.2.3,let left machine as group leader
v2.2.3,"cache block information for groups, group with 2 machines will have double block size"
v2.2.3,convert from group to node leader
v2.2.3,convert from node to group
v2.2.3,meet new group
v2.2.3,add block len for this group
v2.2.3,calculate the group block start
v2.2.3,not need to construct
v2.2.3,get receive block informations
v2.2.3,accumulate block len
v2.2.3,get send block informations
v2.2.3,accumulate block len
v2.2.3,static member definition
v2.2.3,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.2.3,assign the blocks to every rank.
v2.2.3,do reduce scatter
v2.2.3,do all gather
v2.2.3,assign blocks
v2.2.3,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.2.3,copy back
v2.2.3,assign blocks
v2.2.3,start all gather
v2.2.3,when num_machines is small and data is large
v2.2.3,use output as receive buffer
v2.2.3,get current local block size
v2.2.3,get out rank
v2.2.3,get in rank
v2.2.3,get send information
v2.2.3,get recv information
v2.2.3,send and recv at same time
v2.2.3,rotate in-place
v2.2.3,use output as receive buffer
v2.2.3,get current local block size
v2.2.3,get send information
v2.2.3,get recv information
v2.2.3,send and recv at same time
v2.2.3,use output as receive buffer
v2.2.3,send and recv at same time
v2.2.3,send local data to neighbor first
v2.2.3,receive neighbor data first
v2.2.3,reduce
v2.2.3,get target
v2.2.3,get send information
v2.2.3,get recv information
v2.2.3,send and recv at same time
v2.2.3,reduce
v2.2.3,send result to neighbor
v2.2.3,receive result from neighbor
v2.2.3,copy result
v2.2.3,start up socket
v2.2.3,parse clients from file
v2.2.3,get ip list of local machine
v2.2.3,get local rank
v2.2.3,construct listener
v2.2.3,construct communication topo
v2.2.3,construct linkers
v2.2.3,free listener
v2.2.3,set timeout
v2.2.3,accept incoming socket
v2.2.3,receive rank
v2.2.3,add new socket
v2.2.3,save ranks that need to connect with
v2.2.3,start listener
v2.2.3,start connect
v2.2.3,let smaller rank connect to larger rank
v2.2.3,send local rank
v2.2.3,wait for listener
v2.2.3,print connected linkers
v2.2.3,Get some statistic from 2 line
v2.2.3,if only have one line on file
v2.2.3,Constructors
v2.2.3,Get type tag
v2.2.3,Comparisons
v2.2.3,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.2.3,"advance until next line, or end of input"
v2.2.3,advance until closing tokens
v2.2.3,The usual case: non-escaped characters
v2.2.3,Handle escapes
v2.2.3,Extract 4-byte escape sequence
v2.2.3,Explicitly check length of the substring. The following loop
v2.2.3,relies on std::string returning the terminating NUL when
v2.2.3,accessing str[length]. Checking here reduces brittleness.
v2.2.3,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.2.3,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.2.3,whether we're in the middle of such a beast: the previous codepoint was an
v2.2.3,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.2.3,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.2.3,the UTF-16 algorithm.
v2.2.3,Integer part
v2.2.3,Decimal part
v2.2.3,Exponent part
v2.2.3,Check for any trailing garbage
v2.2.3,Documented in json11.hpp
v2.2.3,Check for another object
v2.2.3,get column names
v2.2.3,load label idx first
v2.2.3,erase label column name
v2.2.3,load ignore columns
v2.2.3,load weight idx
v2.2.3,load group idx
v2.2.3,don't support query id in data file when training in parallel
v2.2.3,read data to memory
v2.2.3,sample data
v2.2.3,construct feature bin mappers
v2.2.3,initialize label
v2.2.3,extract features
v2.2.3,sample data from file
v2.2.3,construct feature bin mappers
v2.2.3,initialize label
v2.2.3,extract features
v2.2.3,load data from binary file
v2.2.3,check meta data
v2.2.3,need to check training data
v2.2.3,read data in memory
v2.2.3,initialize label
v2.2.3,extract features
v2.2.3,Get number of lines of data file
v2.2.3,initialize label
v2.2.3,extract features
v2.2.3,load data from binary file
v2.2.3,not need to check validation data
v2.2.3,check meta data
v2.2.3,buffer to read binary file
v2.2.3,check token
v2.2.3,read size of header
v2.2.3,re-allocmate space if not enough
v2.2.3,read header
v2.2.3,get header
v2.2.3,num_groups
v2.2.3,real_feature_idx_
v2.2.3,feature2group
v2.2.3,feature2subfeature
v2.2.3,group_bin_boundaries
v2.2.3,group_feature_start_
v2.2.3,group_feature_cnt_
v2.2.3,get feature names
v2.2.3,write feature names
v2.2.3,read size of meta data
v2.2.3,re-allocate space if not enough
v2.2.3,read meta data
v2.2.3,load meta data
v2.2.3,sample local used data if need to partition
v2.2.3,"if not contain query file, minimal sample unit is one record"
v2.2.3,"if contain query file, minimal sample unit is one query"
v2.2.3,if is new query
v2.2.3,read feature data
v2.2.3,read feature size
v2.2.3,re-allocate space if not enough
v2.2.3,fill feature_names_ if not header
v2.2.3,"if only one machine, find bin locally"
v2.2.3,"if have multi-machines, need to find bin distributed"
v2.2.3,different machines will find bin for different features
v2.2.3,start and len will store the process feature indices for different machines
v2.2.3,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.3,get size of bin mapper with max_bin size
v2.2.3,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.3,find local feature bins and copy to buffer
v2.2.3,free
v2.2.3,convert to binary size
v2.2.3,gather global feature bin mappers
v2.2.3,restore features bins from buffer
v2.2.3,---- private functions ----
v2.2.3,"if features are ordered, not need to use hist_buf"
v2.2.3,read all lines
v2.2.3,get query data
v2.2.3,"if not contain query data, minimal sample unit is one record"
v2.2.3,"if contain query data, minimal sample unit is one query"
v2.2.3,if is new query
v2.2.3,get query data
v2.2.3,"if not contain query file, minimal sample unit is one record"
v2.2.3,"if contain query file, minimal sample unit is one query"
v2.2.3,if is new query
v2.2.3,parse features
v2.2.3,-1 means doesn't use this feature
v2.2.3,"check the range of label_idx, weight_idx and group_idx"
v2.2.3,fill feature_names_ if not header
v2.2.3,start find bins
v2.2.3,"if only one machine, find bin locally"
v2.2.3,"if have multi-machines, need to find bin distributed"
v2.2.3,different machines will find bin for different features
v2.2.3,start and len will store the process feature indices for different machines
v2.2.3,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.3,get size of bin mapper with max_bin size
v2.2.3,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.3,find local feature bins and copy to buffer
v2.2.3,free
v2.2.3,convert to binary size
v2.2.3,gather global feature bin mappers
v2.2.3,restore features bins from buffer
v2.2.3,if doesn't need to prediction with initial model
v2.2.3,parser
v2.2.3,set label
v2.2.3,free processed line:
v2.2.3,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.3,text_reader_->Lines()[i].shrink_to_fit();
v2.2.3,push data
v2.2.3,if is used feature
v2.2.3,if need to prediction with initial model
v2.2.3,parser
v2.2.3,set initial score
v2.2.3,set label
v2.2.3,free processed line:
v2.2.3,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.3,text_reader_->Lines()[i].shrink_to_fit();
v2.2.3,push data
v2.2.3,if is used feature
v2.2.3,metadata_ will manage space of init_score
v2.2.3,text data can be free after loaded feature values
v2.2.3,parser
v2.2.3,set initial score
v2.2.3,set label
v2.2.3,push data
v2.2.3,if is used feature
v2.2.3,only need part of data
v2.2.3,need full data
v2.2.3,metadata_ will manage space of init_score
v2.2.3,read size of token
v2.2.3,deep copy function for BinMapper
v2.2.3,mean size for one bin
v2.2.3,need a new bin
v2.2.3,update bin upper bound
v2.2.3,last bin upper bound
v2.2.3,find distinct_values first
v2.2.3,push zero in the front
v2.2.3,use the large value
v2.2.3,push zero in the back
v2.2.3,convert to int type first
v2.2.3,sort by counts
v2.2.3,avoid first bin is zero
v2.2.3,will ignore the categorical of small counts
v2.2.3,need an additional bin for NaN
v2.2.3,use -1 to represent NaN
v2.2.3,Use MissingType::None to represent this bin contains all categoricals
v2.2.3,check trivial(num_bin_ == 1) feature
v2.2.3,check useless bin
v2.2.3,calculate sparse rate
v2.2.3,sparse threshold
v2.2.3,"for lambdarank, it needs query data for partition data in parallel learning"
v2.2.3,need convert query_id to boundaries
v2.2.3,check weights
v2.2.3,check query boundries
v2.2.3,contain initial score file
v2.2.3,check weights
v2.2.3,get local weights
v2.2.3,check query boundries
v2.2.3,get local query boundaries
v2.2.3,contain initial score file
v2.2.3,get local initial scores
v2.2.3,re-load query weight
v2.2.3,save to nullptr
v2.2.3,save to nullptr
v2.2.3,save to nullptr
v2.2.3,default weight file name
v2.2.3,default weight file name
v2.2.3,use first line to count number class
v2.2.3,default query file name
v2.2.3,/ This file is auto generated by LightGBM\helpers\parameter_generator.py from LightGBM\include\LightGBM\config.h file.
v2.2.3,root is in the depth 0
v2.2.3,non-leaf
v2.2.3,leaf
v2.2.3,use this for the missing value conversion
v2.2.3,Predict func by Map to ifelse
v2.2.3,use this for the missing value conversion
v2.2.3,non-leaf
v2.2.3,left subtree
v2.2.3,right subtree
v2.2.3,leaf
v2.2.3,non-leaf
v2.2.3,left subtree
v2.2.3,right subtree
v2.2.3,leaf
v2.2.3,recursive computation of SHAP values for a decision tree
v2.2.3,extend the unique path
v2.2.3,leaf node
v2.2.3,internal node
v2.2.3,"see if we have already split on this feature,"
v2.2.3,if so we undo that split so we can redo it for this node
v2.2.3,clear old metrics
v2.2.3,to lower
v2.2.3,split
v2.2.3,remove duplicate
v2.2.3,add names of objective function if not providing metric
v2.2.3,generate seeds by seed.
v2.2.3,check for conflicts
v2.2.3,"check if objective, metric, and num_class match"
v2.2.3,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.2.3,Check max_depth and num_leaves
v2.2.3,"filter is based on sampling data, so decrease its range"
v2.2.3,put dense feature first
v2.2.3,sort by non zero cnt
v2.2.3,"sort by non zero cnt, bigger first"
v2.2.3,"take apart small sparse group, due it will not gain on speed"
v2.2.3,shuffle groups
v2.2.3,get num_features
v2.2.3,get bin_mappers
v2.2.3,copy feature bin mapper data
v2.2.3,copy feature bin mapper data
v2.2.3,"if not pass a filename, just append "".bin"" of original file"
v2.2.3,get size of header
v2.2.3,size of feature names
v2.2.3,write header
v2.2.3,write feature names
v2.2.3,get size of meta data
v2.2.3,write meta data
v2.2.3,write feature data
v2.2.3,get size of feature
v2.2.3,write feature
v2.2.3,feature is not used
v2.2.3,construct histograms for smaller leaf
v2.2.3,if not use ordered bin
v2.2.3,used ordered bin
v2.2.3,feature is not used
v2.2.3,construct histograms for smaller leaf
v2.2.3,if not use ordered bin
v2.2.3,used ordered bin
v2.2.3,fixed hessian.
v2.2.3,feature is not used
v2.2.3,construct histograms for smaller leaf
v2.2.3,if not use ordered bin
v2.2.3,used ordered bin
v2.2.3,feature is not used
v2.2.3,construct histograms for smaller leaf
v2.2.3,if not use ordered bin
v2.2.3,used ordered bin
v2.2.3,fixed hessian.
v2.2.3,PredictRaw
v2.2.3,PredictRawByMap
v2.2.3,Predict
v2.2.3,PredictByMap
v2.2.3,PredictLeafIndex
v2.2.3,PredictLeafIndexByMap
v2.2.3,output model type
v2.2.3,output number of class
v2.2.3,output label index
v2.2.3,output max_feature_idx
v2.2.3,output objective
v2.2.3,output tree models
v2.2.3,store the importance first
v2.2.3,sort the importance
v2.2.3,use serialized string to restore this object
v2.2.3,Use first 128 chars to avoid exceed the message buffer.
v2.2.3,get number of classes
v2.2.3,get index of label
v2.2.3,get max_feature_idx first
v2.2.3,get average_output
v2.2.3,get feature names
v2.2.3,set zero
v2.2.3,predict all the trees for one iteration
v2.2.3,check early stopping
v2.2.3,set zero
v2.2.3,predict all the trees for one iteration
v2.2.3,check early stopping
v2.2.3,margin_threshold will be captured by value
v2.2.3,copy and sort
v2.2.3,margin_threshold will be captured by value
v2.2.3,load forced_splits file
v2.2.3,init tree learner
v2.2.3,push training metrics
v2.2.3,create buffer for gradients and hessians
v2.2.3,get max feature index
v2.2.3,get label index
v2.2.3,get feature names
v2.2.3,"if need bagging, create buffer"
v2.2.3,"for a validation dataset, we need its score and metric"
v2.2.3,update score
v2.2.3,objective function will calculate gradients and hessians
v2.2.3,"random bagging, minimal unit is one record"
v2.2.3,if need bagging
v2.2.3,set bagging data to tree learner
v2.2.3,get subset
v2.2.3,output used time per iteration
v2.2.3,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.2.3,boosting first
v2.2.3,bagging logic
v2.2.3,need to copy gradients for bagging subset.
v2.2.3,shrinkage by learning rate
v2.2.3,update score
v2.2.3,only add default score one-time
v2.2.3,updates scores
v2.2.3,add model
v2.2.3,reset score
v2.2.3,remove model
v2.2.3,print message for metric
v2.2.3,pop last early_stopping_round_ models
v2.2.3,update training score
v2.2.3,we need to predict out-of-bag scores of data for boosting
v2.2.3,update validation score
v2.2.3,print training metric
v2.2.3,print validation metric
v2.2.3,set zero
v2.2.3,predict all the trees for one iteration
v2.2.3,check early stopping
v2.2.3,push training metrics
v2.2.3,"not same training data, need reset score and others"
v2.2.3,create score tracker
v2.2.3,update score
v2.2.3,create buffer for gradients and hessians
v2.2.3,"if need bagging, create buffer"
v2.2.3,Get the max size of pool
v2.2.3,at least need 2 leaves
v2.2.3,push split information for all leaves
v2.2.3,get ordered bin
v2.2.3,check existing for ordered bin
v2.2.3,initialize splits for leaf
v2.2.3,initialize data partition
v2.2.3,initialize ordered gradients and hessians
v2.2.3,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.3,get ordered bin
v2.2.3,initialize splits for leaf
v2.2.3,initialize data partition
v2.2.3,initialize ordered gradients and hessians
v2.2.3,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.3,Get the max size of pool
v2.2.3,at least need 2 leaves
v2.2.3,push split information for all leaves
v2.2.3,some initial works before training
v2.2.3,root leaf
v2.2.3,only root leaf can be splitted on first time
v2.2.3,some initial works before finding best split
v2.2.3,find best threshold for every feature
v2.2.3,Get a leaf with max split gain
v2.2.3,Get split information for best leaf
v2.2.3,"cannot split, quit"
v2.2.3,split tree with best leaf
v2.2.3,reset histogram pool
v2.2.3,at least use one feature
v2.2.3,initialize used features
v2.2.3,Get used feature at current tree
v2.2.3,initialize data partition
v2.2.3,reset the splits for leaves
v2.2.3,Sumup for root
v2.2.3,use all data
v2.2.3,"use bagging, only use part of data"
v2.2.3,"if has ordered bin, need to initialize the ordered bin"
v2.2.3,"use all data, pass nullptr"
v2.2.3,"bagging, only use part of data"
v2.2.3,mark used data
v2.2.3,initialize ordered bin
v2.2.3,check depth of current leaf
v2.2.3,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.2.3,no enough data to continue
v2.2.3,only have root
v2.2.3,put parent(left) leaf's histograms into larger leaf's histograms
v2.2.3,put parent(left) leaf's histograms to larger leaf's histograms
v2.2.3,split for the ordered bin
v2.2.3,mark data that at left-leaf
v2.2.3,split the ordered bin
v2.2.3,construct smaller leaf
v2.2.3,construct larger leaf
v2.2.3,find splits
v2.2.3,only has root leaf
v2.2.3,find best threshold for larger child
v2.2.3,start at root leaf
v2.2.3,"before processing next node from queue, store info for current left/right leaf"
v2.2.3,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.2.3,"then, compute own splits"
v2.2.3,split info should exist because searching in bfs fashion - should have added from parent
v2.2.3,"split tree, will return right leaf"
v2.2.3,left = parent
v2.2.3,"split tree, will return right leaf"
v2.2.3,init the leaves that used on next iteration
v2.2.3,bag_mapper[index_mapper[i]]
v2.2.3,bag_mapper[index_mapper[i]]
v2.2.3,get feature partition
v2.2.3,get local used features
v2.2.3,get best split at smaller leaf
v2.2.3,find local best split for larger leaf
v2.2.3,sync global best info
v2.2.3,update best split
v2.2.3,"instantiate template classes, otherwise linker cannot find the code"
v2.2.3,initialize SerialTreeLearner
v2.2.3,Get local rank and global machine size
v2.2.3,allocate buffer for communication
v2.2.3,generate feature partition for current tree
v2.2.3,get local used feature
v2.2.3,get block start and block len for reduce scatter
v2.2.3,get buffer_write_start_pos_
v2.2.3,get buffer_read_start_pos_
v2.2.3,sync global data sumup info
v2.2.3,global sumup reduce
v2.2.3,copy back
v2.2.3,set global sumup info
v2.2.3,init global data count in leaf
v2.2.3,construct local histograms
v2.2.3,copy to buffer
v2.2.3,Reduce scatter for histogram
v2.2.3,restore global histograms from buffer
v2.2.3,find best threshold for smaller child
v2.2.3,only root leaf
v2.2.3,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.2.3,find best threshold for larger child
v2.2.3,find local best split for larger leaf
v2.2.3,sync global best info
v2.2.3,set best split
v2.2.3,need update global number of data in leaf
v2.2.3,"instantiate template classes, otherwise linker cannot find the code"
v2.2.3,initialize SerialTreeLearner
v2.2.3,some additional variables needed for GPU trainer
v2.2.3,Initialize GPU buffers and kernels
v2.2.3,some functions used for debugging the GPU histogram construction
v2.2.3,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.2.3,goto err;
v2.2.3,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.2.3,goto err;
v2.2.3,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.2.3,also guarantee that there are at least 2K examples per workgroup
v2.2.3,return 0;
v2.2.3,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.2.3,decide the best number of workgroups working on one feature4 tuple
v2.2.3,set work group size based on feature size
v2.2.3,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.2.3,we need to refresh the kernel arguments after reallocating
v2.2.3,The only argument that needs to be changed later is num_data_
v2.2.3,"the GPU kernel will process all features in one call, and each"
v2.2.3,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.2.3,process one feature4 tuple
v2.2.3,"for the root node, indices are not copied"
v2.2.3,"for constant hessian, hessians are not copied except for the root node"
v2.2.3,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.2.3,and we will launch num_feature workgroups for this kernel
v2.2.3,will launch threads for all features
v2.2.3,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.2.3,copy the results asynchronously. Size depends on if double precision is used
v2.2.3,we will wait for this object in WaitAndGetHistograms
v2.2.3,"when the output is ready, the computation is done"
v2.2.3,values of this feature has been redistributed to multiple bins; need a reduction here
v2.2.3,how many feature-group tuples we have
v2.2.3,leave some safe margin for prefetching
v2.2.3,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.2.3,clear sparse/dense maps
v2.2.3,do nothing if no features can be processed on GPU
v2.2.3,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.2.3,unpin old buffer if necessary before destructing them
v2.2.3,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.2.3,allocate space for gradients and hessians on device
v2.2.3,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.2.3,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.2.3,copy indices to the device
v2.2.3,histogram bin entry size depends on the precision (single/double)
v2.2.3,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.2.3,each work group generates a sub-histogram of dword_features_ features.
v2.2.3,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.2.3,create atomic counters for inter-group coordination
v2.2.3,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.2.3,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.2.3,looking for dword_features_ non-sparse feature-groups
v2.2.3,decide if we need to redistribute the bin
v2.2.3,multiplier must be a power of 2
v2.2.3,device_bin_mults_.push_back(1);
v2.2.3,found
v2.2.3,for data transfer time
v2.2.3,"Now generate new data structure feature4, and copy data to the device"
v2.2.3,"preallocate arrays for all threads, and pin them"
v2.2.3,building Feature4 bundles; each thread handles dword_features_ features
v2.2.3,one feature datapoint is 4 bits
v2.2.3,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.3,one feature datapoint is one byte
v2.2.3,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.3,Dense bin
v2.2.3,Dense 4-bit bin
v2.2.3,working on the remaining (less than dword_features_) feature groups
v2.2.3,fill the leftover features
v2.2.3,"fill this empty feature with some ""random"" value"
v2.2.3,"fill this empty feature with some ""random"" value"
v2.2.3,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.2.3,deallocate pinned space for feature copying
v2.2.3,data transfer time
v2.2.3,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.2.3,"Something bad happened. Just return ""No log available."""
v2.2.3,"build is okay, log may contain warnings"
v2.2.3,destroy any old kernels
v2.2.3,create OpenCL kernels for different number of workgroups per feature
v2.2.3,currently we don't use constant memory
v2.2.3,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc."
v2.2.3,kernel with indices in an array
v2.2.3,"kernel with all features enabled, with elimited branches"
v2.2.3,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.2.3,do nothing if no features can be processed on GPU
v2.2.3,The only argument that needs to be changed later is num_data_
v2.2.3,"hessian is passed as a parameter, but it is not available now."
v2.2.3,hessian will be set in BeforeTrain()
v2.2.3,"Get the max bin size, used for selecting best GPU kernel"
v2.2.3,initialize GPU
v2.2.3,determine which kernel to use based on the max number of bins
v2.2.3,setup GPU kernel arguments after we allocating all the buffers
v2.2.3,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.2.3,this should rarely occur
v2.2.3,GPU memory has to been reallocated because data may have been changed
v2.2.3,setup GPU kernel arguments after we allocating all the buffers
v2.2.3,Copy initial full hessians and gradients to GPU.
v2.2.3,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.2.3,setup hessian parameters only
v2.2.3,hessian is passed as a parameter
v2.2.3,use bagging
v2.2.3,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.2.3,copy used gradients and hessians to ordered buffer
v2.2.3,transfer the indices to GPU
v2.2.3,transfer hessian to GPU
v2.2.3,setup hessian parameters only
v2.2.3,hessian is passed as a parameter
v2.2.3,transfer gradients to GPU
v2.2.3,only have root
v2.2.3,"Copy indices, gradients and hessians as early as possible"
v2.2.3,only need to initialize for smaller leaf
v2.2.3,Get leaf boundary
v2.2.3,copy indices to the GPU:
v2.2.3,copy ordered hessians to the GPU:
v2.2.3,copy ordered gradients to the GPU:
v2.2.3,do nothing if no features can be processed on GPU
v2.2.3,copy data indices if it is not null
v2.2.3,generate and copy ordered_gradients if gradients is not null
v2.2.3,generate and copy ordered_hessians if hessians is not null
v2.2.3,converted indices in is_feature_used to feature-group indices
v2.2.3,construct the feature masks for dense feature-groups
v2.2.3,"if no feature group is used, just return and do not use GPU"
v2.2.3,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.2.3,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.2.3,"All data have been prepared, now run the GPU kernel"
v2.2.3,construct smaller leaf
v2.2.3,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.2.3,then construct sparse features on CPU
v2.2.3,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.3,"wait for GPU to finish, only if GPU is actually used"
v2.2.3,use double precision
v2.2.3,use single precision
v2.2.3,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.2.3,#define GPU_DEBUG_COMPARE
v2.2.3,construct larger leaf
v2.2.3,then construct sparse features on CPU
v2.2.3,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.3,"wait for GPU to finish, only if GPU is actually used"
v2.2.3,use double precision
v2.2.3,use single precision
v2.2.3,do some sanity check for the GPU algorithm
v2.2.3,limit top k
v2.2.3,get max bin
v2.2.3,calculate buffer size
v2.2.3,"left and right on same time, so need double size"
v2.2.3,initialize histograms for global
v2.2.3,sync global data sumup info
v2.2.3,set global sumup info
v2.2.3,init global data count in leaf
v2.2.3,get local sumup
v2.2.3,get local sumup
v2.2.3,get mean number on machines
v2.2.3,weighted gain
v2.2.3,get top k
v2.2.3,"Copy histogram to buffer, and Get local aggregate features"
v2.2.3,copy histograms.
v2.2.3,copy smaller leaf histograms first
v2.2.3,mark local aggregated feature
v2.2.3,copy
v2.2.3,then copy larger leaf histograms
v2.2.3,mark local aggregated feature
v2.2.3,copy
v2.2.3,use local data to find local best splits
v2.2.3,find splits
v2.2.3,only has root leaf
v2.2.3,find best threshold for larger child
v2.2.3,local voting
v2.2.3,gather
v2.2.3,get all top-k from all machines
v2.2.3,global voting
v2.2.3,copy local histgrams to buffer
v2.2.3,Reduce scatter for histogram
v2.2.3,find best split from local aggregated histograms
v2.2.3,restore from buffer
v2.2.3,find best threshold
v2.2.3,restore from buffer
v2.2.3,find best threshold
v2.2.3,find local best
v2.2.3,find local best split for larger leaf
v2.2.3,sync global best info
v2.2.3,copy back
v2.2.3,set the global number of data for leaves
v2.2.3,init the global sumup info
v2.2.3,"instantiate template classes, otherwise linker cannot find the code"
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable=invalid-name, exec-used, C0111"
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, W0105"
v2.2.2,create predictor first
v2.2.2,check dataset
v2.2.2,reduce cost for prediction training data
v2.2.2,process callbacks
v2.2.2,Most of legacy advanced options becomes callbacks
v2.2.2,construct booster
v2.2.2,start training
v2.2.2,check evaluation result.
v2.2.2,"lambdarank task, split according to groups"
v2.2.2,run preprocessing on the data set if needed
v2.2.2,setup callbacks
v2.2.2,coding: utf-8
v2.2.2,pylint: disable = C0103
v2.2.2,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.2.2,because of u'...' Unicode literals.
v2.2.2,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.2.2,minor change to support `**kwargs`
v2.2.2,"user can set verbose with kwargs, it has higher priority"
v2.2.2,register default metric for consistency with callable eval_metric case
v2.2.2,try to deduce from class instance
v2.2.2,overwrite default metric by explicitly set metric
v2.2.2,concatenate metric from params (or default if not provided in params) and eval_metric
v2.2.2,reduce cost for prediction training data
v2.2.2,free dataset
v2.2.2,Switch to using a multiclass objective in the underlying LGBM instance
v2.2.2,check group data
v2.2.2,coding: utf-8
v2.2.2,we don't need lib_lightgbm while building docs
v2.2.2,coding: utf-8
v2.2.2,pylint: disable = C0103
v2.2.2,coding: utf-8
v2.2.2,REMOVEME: remove warning after 2.3.0 version release
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, C0111, C0301"
v2.2.2,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.2.2,TypeError: obj is not a string or a number
v2.2.2,ValueError: invalid literal
v2.2.2,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.2,avoid memory consumption by arrays concatenation operations
v2.2.2,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.2,avoid memory consumption by arrays concatenation operations
v2.2.2,process for args
v2.2.2,"user can set verbose with params, it has higher priority"
v2.2.2,get categorical features
v2.2.2,process for reference dataset
v2.2.2,start construct data
v2.2.2,check data has header or not
v2.2.2,load init score
v2.2.2,need re group init score
v2.2.2,set feature names
v2.2.2,"change non-float data to float data, need to copy"
v2.2.2,"change non-float data to float data, need to copy"
v2.2.2,create valid
v2.2.2,construct subset
v2.2.2,create train
v2.2.2,set to None
v2.2.2,we're done if self and reference share a common upstrem reference
v2.2.2,"group data from LightGBM is boundaries data, need to convert to group size"
v2.2.2,"user can set verbose with params, it has higher priority"
v2.2.2,Training task
v2.2.2,set network if necessary
v2.2.2,construct booster object
v2.2.2,save reference to data
v2.2.2,buffer for inner predict
v2.2.2,Prediction task
v2.2.2,need reset training data
v2.2.2,need to push new valid data
v2.2.2,"if buffer length is not long enough, re-allocate a buffer"
v2.2.2,"if buffer length is not long enough, reallocate a buffer"
v2.2.2,Copy models
v2.2.2,Get name of features
v2.2.2,avoid to predict many time in one iteration
v2.2.2,Get num of inner evals
v2.2.2,Get name of evals
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, W0105, C0301"
v2.2.2,Callback environment used by callbacks
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, C0111"
v2.2.2,load or create your dataset
v2.2.2,create dataset for lightgbm
v2.2.2,"if you want to re-use data, remember to set free_raw_data=False"
v2.2.2,specify your configurations as a dict
v2.2.2,generate feature names
v2.2.2,feature_name and categorical_feature
v2.2.2,check feature name
v2.2.2,save model to file
v2.2.2,dump model to JSON (and save to file)
v2.2.2,feature names
v2.2.2,feature importances
v2.2.2,load model to predict
v2.2.2,can only predict with the best iteration (or the saving iteration)
v2.2.2,eval with loaded model
v2.2.2,dump model with pickle
v2.2.2,load model with pickle to predict
v2.2.2,can predict with any iteration when loaded in pickle way
v2.2.2,eval with loaded model
v2.2.2,continue training
v2.2.2,init_model accepts:
v2.2.2,1. model file name
v2.2.2,2. Booster()
v2.2.2,decay learning rates
v2.2.2,learning_rates accepts:
v2.2.2,1. list/tuple with length = num_boost_round
v2.2.2,2. function(curr_iter)
v2.2.2,change other parameters during training
v2.2.2,self-defined objective function
v2.2.2,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.2.2,log likelihood loss
v2.2.2,self-defined eval metric
v2.2.2,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.2,binary error
v2.2.2,callback
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, C0111"
v2.2.2,load or create your dataset
v2.2.2,train
v2.2.2,predict
v2.2.2,eval
v2.2.2,feature importances
v2.2.2,self-defined eval metric
v2.2.2,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.2,Root Mean Squared Logarithmic Error (RMSLE)
v2.2.2,train
v2.2.2,predict
v2.2.2,eval
v2.2.2,other scikit-learn modules
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, C0111"
v2.2.2,load or create your dataset
v2.2.2,create dataset for lightgbm
v2.2.2,specify your configurations as a dict
v2.2.2,train
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, C0111"
v2.2.2,################
v2.2.2,Simulate some binary data with a single categorical and
v2.2.2,single continuous predictor
v2.2.2,################
v2.2.2,Set up a couple of utilities for our experiments
v2.2.2,################
v2.2.2,Observe the behavior of `binary` and `xentropy` objectives
v2.2.2,Trying this throws an error on non-binary values of y:
v2.2.2,"experiment('binary', label_type='probability', DATA)"
v2.2.2,The speed of `binary` is not drastically different than
v2.2.2,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v2.2.2,there are reasons to suspect that `binary` should run faster when the
v2.2.2,label is an integer instead of a float
v2.2.2,coding: utf-8
v2.2.2,"pylint: disable = invalid-name, C0111"
v2.2.2,load or create your dataset
v2.2.2,create dataset for lightgbm
v2.2.2,specify your configurations as a dict
v2.2.2,train
v2.2.2,save model to file
v2.2.2,predict
v2.2.2,eval
v2.2.2,coding: utf-8
v2.2.2,!/usr/bin/env python3
v2.2.2,-*- coding: utf-8 -*-
v2.2.2,
v2.2.2,"LightGBM documentation build configuration file, created by"
v2.2.2,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.2.2,
v2.2.2,This file is execfile()d with the current directory set to its
v2.2.2,containing dir.
v2.2.2,
v2.2.2,Note that not all possible configuration values are present in this
v2.2.2,autogenerated file.
v2.2.2,
v2.2.2,All configuration values have a default; values that are commented out
v2.2.2,serve to show the default.
v2.2.2,"If extensions (or modules to document with autodoc) are in another directory,"
v2.2.2,add these directories to sys.path here. If the directory is relative to the
v2.2.2,"documentation root, use os.path.abspath to make it absolute."
v2.2.2,-- mock out modules
v2.2.2,-- General configuration ------------------------------------------------
v2.2.2,"If your documentation needs a minimal Sphinx version, state it here."
v2.2.2,"Add any Sphinx extension module names here, as strings. They can be"
v2.2.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.2.2,ones.
v2.2.2,"Add any paths that contain templates here, relative to this directory."
v2.2.2,The suffix(es) of source filenames.
v2.2.2,You can specify multiple suffix as a list of string:
v2.2.2,"source_suffix = ['.rst', '.md']"
v2.2.2,The master toctree document.
v2.2.2,General information about the project.
v2.2.2,"The version info for the project you're documenting, acts as replacement for"
v2.2.2,"|version| and |release|, also used in various other places throughout the"
v2.2.2,built documents.
v2.2.2,
v2.2.2,The short X.Y version.
v2.2.2,"The full version, including alpha/beta/rc tags."
v2.2.2,The language for content autogenerated by Sphinx. Refer to documentation
v2.2.2,for a list of supported languages.
v2.2.2,
v2.2.2,This is also used if you do content translation via gettext catalogs.
v2.2.2,"Usually you set ""language"" from the command line for these cases."
v2.2.2,"List of patterns, relative to source directory, that match files and"
v2.2.2,directories to ignore when looking for source files.
v2.2.2,This patterns also effect to html_static_path and html_extra_path
v2.2.2,The name of the Pygments (syntax highlighting) style to use.
v2.2.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.2.2,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.2.2,-- Options for HTML output ----------------------------------------------
v2.2.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.2.2,a list of builtin themes.
v2.2.2,Theme options are theme-specific and customize the look and feel of a theme
v2.2.2,"further.  For a list of options available for each theme, see the"
v2.2.2,documentation.
v2.2.2,"Add any paths that contain custom static files (such as style sheets) here,"
v2.2.2,"relative to this directory. They are copied after the builtin static files,"
v2.2.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.2.2,-- Options for HTMLHelp output ------------------------------------------
v2.2.2,Output file base name for HTML help builder.
v2.2.2,coding: utf-8
v2.2.2,coding: utf-8
v2.2.2,pylint: skip-file
v2.2.2,we don't need lib_lightgbm while building docs
v2.2.2,coding: utf-8
v2.2.2,pylint: skip-file
v2.2.2,check saved model persistence
v2.2.2,"we need to check the consistency of model file here, so test for exact equal"
v2.2.2,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.2.2,"scores likely to be different, but prediction should still be the same"
v2.2.2,coding: utf-8
v2.2.2,pylint: skip-file
v2.2.2,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v2.2.2,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.2.2,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.2.2,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.2.2,Tests same probabilities
v2.2.2,Tests same predictions
v2.2.2,Tests same raw scores
v2.2.2,Tests same leaf indices
v2.2.2,Tests same feature contributions
v2.2.2,Tests other parameters for the prediction works
v2.2.2,coding: utf-8
v2.2.2,pylint: skip-file
v2.2.2,coding: utf-8
v2.2.2,pylint: skip-file
v2.2.2,no early stopping
v2.2.2,early stopping occurs
v2.2.2,test custom eval metrics
v2.2.2,"shuffle = False, override metric in params"
v2.2.2,"shuffle = True, callbacks"
v2.2.2,self defined folds
v2.2.2,lambdarank
v2.2.2,... with l2 metric
v2.2.2,... with NDCG (default) metric
v2.2.2,self defined folds with lambdarank
v2.2.2,test feature_names with whitespaces
v2.2.2,take subsets and train
v2.2.2,test sliced labels
v2.2.2,append some columns
v2.2.2,append some rows
v2.2.2,test sliced 2d matrix
v2.2.2,test sliced CSR
v2.2.2,coding: utf-8
v2.2.2,pylint: skip-file
v2.2.2,Register Dynamic Symbols
v2.2.2,coding: utf-8
v2.2.2,alias table
v2.2.2,names
v2.2.2,from strings
v2.2.2,tails
v2.2.2,tails
v2.2.2,coding: utf-8
v2.2.2,convert from one-based to  zero-based index
v2.2.2,convert from boundaries to size
v2.2.2,--- start Booster interfaces
v2.2.2,create boosting
v2.2.2,initialize the boosting
v2.2.2,create objective function
v2.2.2,initialize the objective function
v2.2.2,create training metric
v2.2.2,reset the boosting
v2.2.2,create objective function
v2.2.2,initialize the objective function
v2.2.2,some help functions used to convert data
v2.2.2,Row iterator of on column for CSC matrix
v2.2.2,"return value at idx, only can access by ascent order"
v2.2.2,"return next non-zero pair, if index < 0, means no more data"
v2.2.2,start of c_api functions
v2.2.2,sample data first
v2.2.2,sample data first
v2.2.2,sample data first
v2.2.2,no more data
v2.2.2,---- start of booster
v2.2.2,---- start of some help functions
v2.2.2,set number of threads for openmp
v2.2.2,check for alias
v2.2.2,read parameters from config file
v2.2.2,"remove str after ""#"""
v2.2.2,check for alias again
v2.2.2,load configs
v2.2.2,prediction is needed if using input initial model(continued train)
v2.2.2,need to continue training
v2.2.2,sync up random seed for data partition
v2.2.2,load Training data
v2.2.2,load data for parallel training
v2.2.2,load data for single machine
v2.2.2,need save binary file
v2.2.2,create training metric
v2.2.2,only when have metrics then need to construct validation data
v2.2.2,"Add validation data, if it exists"
v2.2.2,add
v2.2.2,need save binary file
v2.2.2,add metric for validation data
v2.2.2,output used time on each iteration
v2.2.2,need init network
v2.2.2,create boosting
v2.2.2,create objective function
v2.2.2,load training data
v2.2.2,initialize the objective function
v2.2.2,initialize the boosting
v2.2.2,add validation data into boosting
v2.2.2,convert model to if-else statement code
v2.2.2,create predictor
v2.2.2,Free memory
v2.2.2,create predictor
v2.2.2,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.2.2,counts for all labels
v2.2.2,"start from top label, and accumulate DCG"
v2.2.2,counts for all labels
v2.2.2,calculate k Max DCG by one pass
v2.2.2,get sorted indices by score
v2.2.2,calculate dcg
v2.2.2,get sorted indices by score
v2.2.2,calculate multi dcg by one pass
v2.2.2,wait for all client start up
v2.2.2,default set to -1
v2.2.2,"distance at k-th communication, distance[k] = 2^k"
v2.2.2,set incoming rank at k-th commuication
v2.2.2,set outgoing rank at k-th commuication
v2.2.2,defalut set as -1
v2.2.2,construct all recursive halving map for all machines
v2.2.2,let 1 << k <= num_machines
v2.2.2,distance of each communication
v2.2.2,"if num_machines = 2^k, don't need to group machines"
v2.2.2,"communication direction, %2 == 0 is positive"
v2.2.2,neighbor at k-th communication
v2.2.2,receive data block at k-th communication
v2.2.2,send data block at k-th communication
v2.2.2,"if num_machines != 2^k, need to group machines"
v2.2.2,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.2.2,let left machine as group leader
v2.2.2,"cache block information for groups, group with 2 machines will have double block size"
v2.2.2,convert from group to node leader
v2.2.2,convert from node to group
v2.2.2,meet new group
v2.2.2,add block len for this group
v2.2.2,calculate the group block start
v2.2.2,not need to construct
v2.2.2,get receive block informations
v2.2.2,accumulate block len
v2.2.2,get send block informations
v2.2.2,accumulate block len
v2.2.2,static member definition
v2.2.2,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.2.2,assign the blocks to every rank.
v2.2.2,do reduce scatter
v2.2.2,do all gather
v2.2.2,assign blocks
v2.2.2,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.2.2,copy back
v2.2.2,assign blocks
v2.2.2,start all gather
v2.2.2,when num_machines is small and data is large
v2.2.2,use output as receive buffer
v2.2.2,get current local block size
v2.2.2,get out rank
v2.2.2,get in rank
v2.2.2,get send information
v2.2.2,get recv information
v2.2.2,send and recv at same time
v2.2.2,rotate in-place
v2.2.2,use output as receive buffer
v2.2.2,get current local block size
v2.2.2,get send information
v2.2.2,get recv information
v2.2.2,send and recv at same time
v2.2.2,use output as receive buffer
v2.2.2,send and recv at same time
v2.2.2,send local data to neighbor first
v2.2.2,receive neighbor data first
v2.2.2,reduce
v2.2.2,get target
v2.2.2,get send information
v2.2.2,get recv information
v2.2.2,send and recv at same time
v2.2.2,reduce
v2.2.2,send result to neighbor
v2.2.2,receive result from neighbor
v2.2.2,copy result
v2.2.2,start up socket
v2.2.2,parse clients from file
v2.2.2,get ip list of local machine
v2.2.2,get local rank
v2.2.2,construct listener
v2.2.2,construct communication topo
v2.2.2,construct linkers
v2.2.2,free listener
v2.2.2,set timeout
v2.2.2,accept incoming socket
v2.2.2,receive rank
v2.2.2,add new socket
v2.2.2,save ranks that need to connect with
v2.2.2,start listener
v2.2.2,start connect
v2.2.2,let smaller rank connect to larger rank
v2.2.2,send local rank
v2.2.2,wait for listener
v2.2.2,print connected linkers
v2.2.2,Get some statistic from 2 line
v2.2.2,if only have one line on file
v2.2.2,Constructors
v2.2.2,Get type tag
v2.2.2,Comparisons
v2.2.2,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.2.2,"advance until next line, or end of input"
v2.2.2,advance until closing tokens
v2.2.2,The usual case: non-escaped characters
v2.2.2,Handle escapes
v2.2.2,Extract 4-byte escape sequence
v2.2.2,Explicitly check length of the substring. The following loop
v2.2.2,relies on std::string returning the terminating NUL when
v2.2.2,accessing str[length]. Checking here reduces brittleness.
v2.2.2,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.2.2,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.2.2,whether we're in the middle of such a beast: the previous codepoint was an
v2.2.2,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.2.2,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.2.2,the UTF-16 algorithm.
v2.2.2,Integer part
v2.2.2,Decimal part
v2.2.2,Exponent part
v2.2.2,Check for any trailing garbage
v2.2.2,Documented in json11.hpp
v2.2.2,Check for another object
v2.2.2,get column names
v2.2.2,load label idx first
v2.2.2,erase label column name
v2.2.2,load ignore columns
v2.2.2,load weight idx
v2.2.2,load group idx
v2.2.2,don't support query id in data file when training in parallel
v2.2.2,read data to memory
v2.2.2,sample data
v2.2.2,construct feature bin mappers
v2.2.2,initialize label
v2.2.2,extract features
v2.2.2,sample data from file
v2.2.2,construct feature bin mappers
v2.2.2,initialize label
v2.2.2,extract features
v2.2.2,load data from binary file
v2.2.2,check meta data
v2.2.2,need to check training data
v2.2.2,read data in memory
v2.2.2,initialize label
v2.2.2,extract features
v2.2.2,Get number of lines of data file
v2.2.2,initialize label
v2.2.2,extract features
v2.2.2,load data from binary file
v2.2.2,not need to check validation data
v2.2.2,check meta data
v2.2.2,buffer to read binary file
v2.2.2,check token
v2.2.2,read size of header
v2.2.2,re-allocmate space if not enough
v2.2.2,read header
v2.2.2,get header
v2.2.2,num_groups
v2.2.2,real_feature_idx_
v2.2.2,feature2group
v2.2.2,feature2subfeature
v2.2.2,group_bin_boundaries
v2.2.2,group_feature_start_
v2.2.2,group_feature_cnt_
v2.2.2,get feature names
v2.2.2,write feature names
v2.2.2,read size of meta data
v2.2.2,re-allocate space if not enough
v2.2.2,read meta data
v2.2.2,load meta data
v2.2.2,sample local used data if need to partition
v2.2.2,"if not contain query file, minimal sample unit is one record"
v2.2.2,"if contain query file, minimal sample unit is one query"
v2.2.2,if is new query
v2.2.2,read feature data
v2.2.2,read feature size
v2.2.2,re-allocate space if not enough
v2.2.2,fill feature_names_ if not header
v2.2.2,"if only one machine, find bin locally"
v2.2.2,"if have multi-machines, need to find bin distributed"
v2.2.2,different machines will find bin for different features
v2.2.2,start and len will store the process feature indices for different machines
v2.2.2,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.2,get size of bin mapper with max_bin size
v2.2.2,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.2,find local feature bins and copy to buffer
v2.2.2,free
v2.2.2,convert to binary size
v2.2.2,gather global feature bin mappers
v2.2.2,restore features bins from buffer
v2.2.2,---- private functions ----
v2.2.2,"if features are ordered, not need to use hist_buf"
v2.2.2,read all lines
v2.2.2,get query data
v2.2.2,"if not contain query data, minimal sample unit is one record"
v2.2.2,"if contain query data, minimal sample unit is one query"
v2.2.2,if is new query
v2.2.2,get query data
v2.2.2,"if not contain query file, minimal sample unit is one record"
v2.2.2,"if contain query file, minimal sample unit is one query"
v2.2.2,if is new query
v2.2.2,parse features
v2.2.2,-1 means doesn't use this feature
v2.2.2,"check the range of label_idx, weight_idx and group_idx"
v2.2.2,fill feature_names_ if not header
v2.2.2,start find bins
v2.2.2,"if only one machine, find bin locally"
v2.2.2,"if have multi-machines, need to find bin distributed"
v2.2.2,different machines will find bin for different features
v2.2.2,start and len will store the process feature indices for different machines
v2.2.2,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.2,get size of bin mapper with max_bin size
v2.2.2,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.2,find local feature bins and copy to buffer
v2.2.2,free
v2.2.2,convert to binary size
v2.2.2,gather global feature bin mappers
v2.2.2,restore features bins from buffer
v2.2.2,if doesn't need to prediction with initial model
v2.2.2,parser
v2.2.2,set label
v2.2.2,free processed line:
v2.2.2,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.2,text_reader_->Lines()[i].shrink_to_fit();
v2.2.2,push data
v2.2.2,if is used feature
v2.2.2,if need to prediction with initial model
v2.2.2,parser
v2.2.2,set initial score
v2.2.2,set label
v2.2.2,free processed line:
v2.2.2,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.2,text_reader_->Lines()[i].shrink_to_fit();
v2.2.2,push data
v2.2.2,if is used feature
v2.2.2,metadata_ will manage space of init_score
v2.2.2,text data can be free after loaded feature values
v2.2.2,parser
v2.2.2,set initial score
v2.2.2,set label
v2.2.2,push data
v2.2.2,if is used feature
v2.2.2,only need part of data
v2.2.2,need full data
v2.2.2,metadata_ will manage space of init_score
v2.2.2,read size of token
v2.2.2,deep copy function for BinMapper
v2.2.2,mean size for one bin
v2.2.2,need a new bin
v2.2.2,update bin upper bound
v2.2.2,last bin upper bound
v2.2.2,find distinct_values first
v2.2.2,push zero in the front
v2.2.2,use the large value
v2.2.2,push zero in the back
v2.2.2,convert to int type first
v2.2.2,sort by counts
v2.2.2,avoid first bin is zero
v2.2.2,will ignore the categorical of small counts
v2.2.2,need an additional bin for NaN
v2.2.2,use -1 to represent NaN
v2.2.2,Use MissingType::None to represent this bin contains all categoricals
v2.2.2,check trival(num_bin_ == 1) feature
v2.2.2,check useless bin
v2.2.2,calculate sparse rate
v2.2.2,sparse threshold
v2.2.2,"for lambdarank, it needs query data for partition data in parallel learning"
v2.2.2,need convert query_id to boundaries
v2.2.2,check weights
v2.2.2,check query boundries
v2.2.2,contain initial score file
v2.2.2,check weights
v2.2.2,get local weights
v2.2.2,check query boundries
v2.2.2,get local query boundaries
v2.2.2,contain initial score file
v2.2.2,get local initial scores
v2.2.2,re-load query weight
v2.2.2,save to nullptr
v2.2.2,save to nullptr
v2.2.2,save to nullptr
v2.2.2,default weight file name
v2.2.2,default weight file name
v2.2.2,use first line to count number class
v2.2.2,default query file name
v2.2.2,/ This file is auto generated by LightGBM\helpers\parameter_generator.py from LightGBM\include\LightGBM\config.h file.
v2.2.2,root is in the depth 0
v2.2.2,non-leaf
v2.2.2,leaf
v2.2.2,use this for the missing value conversion
v2.2.2,Predict func by Map to ifelse
v2.2.2,use this for the missing value conversion
v2.2.2,non-leaf
v2.2.2,left subtree
v2.2.2,right subtree
v2.2.2,leaf
v2.2.2,non-leaf
v2.2.2,left subtree
v2.2.2,right subtree
v2.2.2,leaf
v2.2.2,recursive computation of SHAP values for a decision tree
v2.2.2,extend the unique path
v2.2.2,leaf node
v2.2.2,internal node
v2.2.2,"see if we have already split on this feature,"
v2.2.2,if so we undo that split so we can redo it for this node
v2.2.2,clear old metrics
v2.2.2,to lower
v2.2.2,split
v2.2.2,remove duplicate
v2.2.2,add names of objective function if not providing metric
v2.2.2,generate seeds by seed.
v2.2.2,check for conflicts
v2.2.2,"check if objective, metric, and num_class match"
v2.2.2,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.2.2,Check max_depth and num_leaves
v2.2.2,"filter is based on sampling data, so decrease its range"
v2.2.2,put dense feature first
v2.2.2,sort by non zero cnt
v2.2.2,"sort by non zero cnt, bigger first"
v2.2.2,"take apart small sparse group, due it will not gain on speed"
v2.2.2,shuffle groups
v2.2.2,get num_features
v2.2.2,get bin_mappers
v2.2.2,copy feature bin mapper data
v2.2.2,copy feature bin mapper data
v2.2.2,"if not pass a filename, just append "".bin"" of original file"
v2.2.2,get size of header
v2.2.2,size of feature names
v2.2.2,write header
v2.2.2,write feature names
v2.2.2,get size of meta data
v2.2.2,write meta data
v2.2.2,write feature data
v2.2.2,get size of feature
v2.2.2,write feature
v2.2.2,feature is not used
v2.2.2,construct histograms for smaller leaf
v2.2.2,if not use ordered bin
v2.2.2,used ordered bin
v2.2.2,feature is not used
v2.2.2,construct histograms for smaller leaf
v2.2.2,if not use ordered bin
v2.2.2,used ordered bin
v2.2.2,fixed hessian.
v2.2.2,feature is not used
v2.2.2,construct histograms for smaller leaf
v2.2.2,if not use ordered bin
v2.2.2,used ordered bin
v2.2.2,feature is not used
v2.2.2,construct histograms for smaller leaf
v2.2.2,if not use ordered bin
v2.2.2,used ordered bin
v2.2.2,fixed hessian.
v2.2.2,PredictRaw
v2.2.2,PredictRawByMap
v2.2.2,Predict
v2.2.2,PredictByMap
v2.2.2,PredictLeafIndex
v2.2.2,PredictLeafIndexByMap
v2.2.2,output model type
v2.2.2,output number of class
v2.2.2,output label index
v2.2.2,output max_feature_idx
v2.2.2,output objective
v2.2.2,output tree models
v2.2.2,store the importance first
v2.2.2,sort the importance
v2.2.2,use serialized string to restore this object
v2.2.2,Use first 128 chars to avoid exceed the message buffer.
v2.2.2,get number of classes
v2.2.2,get index of label
v2.2.2,get max_feature_idx first
v2.2.2,get average_output
v2.2.2,get feature names
v2.2.2,set zero
v2.2.2,predict all the trees for one iteration
v2.2.2,check early stopping
v2.2.2,set zero
v2.2.2,predict all the trees for one iteration
v2.2.2,check early stopping
v2.2.2,margin_threshold will be captured by value
v2.2.2,copy and sort
v2.2.2,margin_threshold will be captured by value
v2.2.2,load forced_splits file
v2.2.2,init tree learner
v2.2.2,push training metrics
v2.2.2,create buffer for gradients and hessians
v2.2.2,get max feature index
v2.2.2,get label index
v2.2.2,get feature names
v2.2.2,"if need bagging, create buffer"
v2.2.2,"for a validation dataset, we need its score and metric"
v2.2.2,update score
v2.2.2,objective function will calculate gradients and hessians
v2.2.2,"random bagging, minimal unit is one record"
v2.2.2,if need bagging
v2.2.2,set bagging data to tree learner
v2.2.2,get subset
v2.2.2,output used time per iteration
v2.2.2,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.2.2,boosting first
v2.2.2,bagging logic
v2.2.2,need to copy gradients for bagging subset.
v2.2.2,shrinkage by learning rate
v2.2.2,update score
v2.2.2,only add default score one-time
v2.2.2,updates scores
v2.2.2,add model
v2.2.2,reset score
v2.2.2,remove model
v2.2.2,print message for metric
v2.2.2,pop last early_stopping_round_ models
v2.2.2,update training score
v2.2.2,we need to predict out-of-bag scores of data for boosting
v2.2.2,update validation score
v2.2.2,print training metric
v2.2.2,print validation metric
v2.2.2,set zero
v2.2.2,predict all the trees for one iteration
v2.2.2,check early stopping
v2.2.2,push training metrics
v2.2.2,"not same training data, need reset score and others"
v2.2.2,create score tracker
v2.2.2,update score
v2.2.2,create buffer for gradients and hessians
v2.2.2,"if need bagging, create buffer"
v2.2.2,Get the max size of pool
v2.2.2,at least need 2 leaves
v2.2.2,push split information for all leaves
v2.2.2,get ordered bin
v2.2.2,check existing for ordered bin
v2.2.2,initialize splits for leaf
v2.2.2,initialize data partition
v2.2.2,initialize ordered gradients and hessians
v2.2.2,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.2,get ordered bin
v2.2.2,initialize splits for leaf
v2.2.2,initialize data partition
v2.2.2,initialize ordered gradients and hessians
v2.2.2,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.2,Get the max size of pool
v2.2.2,at least need 2 leaves
v2.2.2,push split information for all leaves
v2.2.2,some initial works before training
v2.2.2,root leaf
v2.2.2,only root leaf can be splitted on first time
v2.2.2,some initial works before finding best split
v2.2.2,find best threshold for every feature
v2.2.2,Get a leaf with max split gain
v2.2.2,Get split information for best leaf
v2.2.2,"cannot split, quit"
v2.2.2,split tree with best leaf
v2.2.2,reset histogram pool
v2.2.2,at least use one feature
v2.2.2,initialize used features
v2.2.2,Get used feature at current tree
v2.2.2,initialize data partition
v2.2.2,reset the splits for leaves
v2.2.2,Sumup for root
v2.2.2,use all data
v2.2.2,"use bagging, only use part of data"
v2.2.2,"if has ordered bin, need to initialize the ordered bin"
v2.2.2,"use all data, pass nullptr"
v2.2.2,"bagging, only use part of data"
v2.2.2,mark used data
v2.2.2,initialize ordered bin
v2.2.2,check depth of current leaf
v2.2.2,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.2.2,no enough data to continue
v2.2.2,only have root
v2.2.2,put parent(left) leaf's histograms into larger leaf's histograms
v2.2.2,put parent(left) leaf's histograms to larger leaf's histograms
v2.2.2,split for the ordered bin
v2.2.2,mark data that at left-leaf
v2.2.2,split the ordered bin
v2.2.2,construct smaller leaf
v2.2.2,construct larger leaf
v2.2.2,find splits
v2.2.2,only has root leaf
v2.2.2,find best threshold for larger child
v2.2.2,start at root leaf
v2.2.2,"before processing next node from queue, store info for current left/right leaf"
v2.2.2,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.2.2,"then, compute own splits"
v2.2.2,split info should exist because searching in bfs fashion - should have added from parent
v2.2.2,"split tree, will return right leaf"
v2.2.2,left = parent
v2.2.2,"split tree, will return right leaf"
v2.2.2,init the leaves that used on next iteration
v2.2.2,bag_mapper[index_mapper[i]]
v2.2.2,get feature partition
v2.2.2,get local used features
v2.2.2,get best split at smaller leaf
v2.2.2,find local best split for larger leaf
v2.2.2,sync global best info
v2.2.2,update best split
v2.2.2,"instantiate template classes, otherwise linker cannot find the code"
v2.2.2,initialize SerialTreeLearner
v2.2.2,Get local rank and global machine size
v2.2.2,allocate buffer for communication
v2.2.2,generate feature partition for current tree
v2.2.2,get local used feature
v2.2.2,get block start and block len for reduce scatter
v2.2.2,get buffer_write_start_pos_
v2.2.2,get buffer_read_start_pos_
v2.2.2,sync global data sumup info
v2.2.2,global sumup reduce
v2.2.2,copy back
v2.2.2,set global sumup info
v2.2.2,init global data count in leaf
v2.2.2,construct local histograms
v2.2.2,copy to buffer
v2.2.2,Reduce scatter for histogram
v2.2.2,restore global histograms from buffer
v2.2.2,find best threshold for smaller child
v2.2.2,only root leaf
v2.2.2,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.2.2,find best threshold for larger child
v2.2.2,find local best split for larger leaf
v2.2.2,sync global best info
v2.2.2,set best split
v2.2.2,need update global number of data in leaf
v2.2.2,"instantiate template classes, otherwise linker cannot find the code"
v2.2.2,initialize SerialTreeLearner
v2.2.2,some additional variables needed for GPU trainer
v2.2.2,Initialize GPU buffers and kernels
v2.2.2,some functions used for debugging the GPU histogram construction
v2.2.2,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.2.2,goto err;
v2.2.2,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.2.2,goto err;
v2.2.2,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.2.2,also guarantee that there are at least 2K examples per workgroup
v2.2.2,return 0;
v2.2.2,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.2.2,decide the best number of workgroups working on one feature4 tuple
v2.2.2,set work group size based on feature size
v2.2.2,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.2.2,we need to refresh the kernel arguments after reallocating
v2.2.2,The only argument that needs to be changed later is num_data_
v2.2.2,"the GPU kernel will process all features in one call, and each"
v2.2.2,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.2.2,process one feature4 tuple
v2.2.2,"for the root node, indices are not copied"
v2.2.2,"for constant hessian, hessians are not copied except for the root node"
v2.2.2,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.2.2,and we will launch num_feature workgroups for this kernel
v2.2.2,will launch threads for all features
v2.2.2,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.2.2,copy the results asynchronously. Size depends on if double precision is used
v2.2.2,we will wait for this object in WaitAndGetHistograms
v2.2.2,"when the output is ready, the computation is done"
v2.2.2,values of this feature has been redistributed to multiple bins; need a reduction here
v2.2.2,how many feature-group tuples we have
v2.2.2,leave some safe margin for prefetching
v2.2.2,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.2.2,clear sparse/dense maps
v2.2.2,do nothing if no features can be processed on GPU
v2.2.2,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.2.2,unpin old buffer if necessary before destructing them
v2.2.2,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.2.2,allocate space for gradients and hessians on device
v2.2.2,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.2.2,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.2.2,copy indices to the device
v2.2.2,histogram bin entry size depends on the precision (single/double)
v2.2.2,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.2.2,each work group generates a sub-histogram of dword_features_ features.
v2.2.2,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.2.2,create atomic counters for inter-group coordination
v2.2.2,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.2.2,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.2.2,looking for dword_features_ non-sparse feature-groups
v2.2.2,decide if we need to redistribute the bin
v2.2.2,multiplier must be a power of 2
v2.2.2,device_bin_mults_.push_back(1);
v2.2.2,found
v2.2.2,for data transfer time
v2.2.2,"Now generate new data structure feature4, and copy data to the device"
v2.2.2,"preallocate arrays for all threads, and pin them"
v2.2.2,building Feature4 bundles; each thread handles dword_features_ features
v2.2.2,one feature datapoint is 4 bits
v2.2.2,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.2,one feature datapoint is one byte
v2.2.2,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.2,Dense bin
v2.2.2,Dense 4-bit bin
v2.2.2,working on the remaining (less than dword_features_) feature groups
v2.2.2,fill the leftover features
v2.2.2,"fill this empty feature with some ""random"" value"
v2.2.2,"fill this empty feature with some ""random"" value"
v2.2.2,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.2.2,deallocate pinned space for feature copying
v2.2.2,data transfer time
v2.2.2,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.2.2,"Something bad happened. Just return ""No log available."""
v2.2.2,"build is okay, log may contain warnings"
v2.2.2,destroy any old kernels
v2.2.2,create OpenCL kernels for different number of workgroups per feature
v2.2.2,currently we don't use constant memory
v2.2.2,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.2.2,kernel with indices in an array
v2.2.2,"kernel with all features enabled, with elimited branches"
v2.2.2,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.2.2,do nothing if no features can be processed on GPU
v2.2.2,The only argument that needs to be changed later is num_data_
v2.2.2,"hessian is passed as a parameter, but it is not available now."
v2.2.2,hessian will be set in BeforeTrain()
v2.2.2,"Get the max bin size, used for selecting best GPU kernel"
v2.2.2,initialize GPU
v2.2.2,determine which kernel to use based on the max number of bins
v2.2.2,setup GPU kernel arguments after we allocating all the buffers
v2.2.2,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.2.2,this should rarely occur
v2.2.2,GPU memory has to been reallocated because data may have been changed
v2.2.2,setup GPU kernel arguments after we allocating all the buffers
v2.2.2,Copy initial full hessians and gradients to GPU.
v2.2.2,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.2.2,setup hessian parameters only
v2.2.2,hessian is passed as a parameter
v2.2.2,use bagging
v2.2.2,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.2.2,copy used gradients and hessians to ordered buffer
v2.2.2,transfer the indices to GPU
v2.2.2,transfer hessian to GPU
v2.2.2,setup hessian parameters only
v2.2.2,hessian is passed as a parameter
v2.2.2,transfer gradients to GPU
v2.2.2,only have root
v2.2.2,"Copy indices, gradients and hessians as early as possible"
v2.2.2,only need to initialize for smaller leaf
v2.2.2,Get leaf boundary
v2.2.2,copy indices to the GPU:
v2.2.2,copy ordered hessians to the GPU:
v2.2.2,copy ordered gradients to the GPU:
v2.2.2,do nothing if no features can be processed on GPU
v2.2.2,copy data indices if it is not null
v2.2.2,generate and copy ordered_gradients if gradients is not null
v2.2.2,generate and copy ordered_hessians if hessians is not null
v2.2.2,converted indices in is_feature_used to feature-group indices
v2.2.2,construct the feature masks for dense feature-groups
v2.2.2,"if no feature group is used, just return and do not use GPU"
v2.2.2,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.2.2,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.2.2,"All data have been prepared, now run the GPU kernel"
v2.2.2,construct smaller leaf
v2.2.2,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.2.2,then construct sparse features on CPU
v2.2.2,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.2,"wait for GPU to finish, only if GPU is actually used"
v2.2.2,use double precision
v2.2.2,use single precision
v2.2.2,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.2.2,#define GPU_DEBUG_COMPARE
v2.2.2,construct larger leaf
v2.2.2,then construct sparse features on CPU
v2.2.2,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.2,"wait for GPU to finish, only if GPU is actually used"
v2.2.2,use double precision
v2.2.2,use single precision
v2.2.2,do some sanity check for the GPU algorithm
v2.2.2,limit top k
v2.2.2,get max bin
v2.2.2,calculate buffer size
v2.2.2,"left and right on same time, so need double size"
v2.2.2,initialize histograms for global
v2.2.2,sync global data sumup info
v2.2.2,set global sumup info
v2.2.2,init global data count in leaf
v2.2.2,get local sumup
v2.2.2,get local sumup
v2.2.2,get mean number on machines
v2.2.2,weighted gain
v2.2.2,get top k
v2.2.2,"Copy histogram to buffer, and Get local aggregate features"
v2.2.2,copy histograms.
v2.2.2,copy smaller leaf histograms first
v2.2.2,mark local aggregated feature
v2.2.2,copy
v2.2.2,then copy larger leaf histograms
v2.2.2,mark local aggregated feature
v2.2.2,copy
v2.2.2,use local data to find local best splits
v2.2.2,find splits
v2.2.2,only has root leaf
v2.2.2,find best threshold for larger child
v2.2.2,local voting
v2.2.2,gather
v2.2.2,get all top-k from all machines
v2.2.2,global voting
v2.2.2,copy local histgrams to buffer
v2.2.2,Reduce scatter for histogram
v2.2.2,find best split from local aggregated histograms
v2.2.2,restore from buffer
v2.2.2,find best threshold
v2.2.2,restore from buffer
v2.2.2,find best threshold
v2.2.2,find local best
v2.2.2,find local best split for larger leaf
v2.2.2,sync global best info
v2.2.2,copy back
v2.2.2,set the global number of data for leaves
v2.2.2,init the global sumup info
v2.2.2,"instantiate template classes, otherwise linker cannot find the code"
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable=invalid-name, exec-used, C0111"
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, W0105"
v2.2.1,create predictor first
v2.2.1,check dataset
v2.2.1,reduce cost for prediction training data
v2.2.1,process callbacks
v2.2.1,Most of legacy advanced options becomes callbacks
v2.2.1,construct booster
v2.2.1,start training
v2.2.1,check evaluation result.
v2.2.1,"lambdarank task, split according to groups"
v2.2.1,run preprocessing on the data set if needed
v2.2.1,setup callbacks
v2.2.1,coding: utf-8
v2.2.1,pylint: disable = C0103
v2.2.1,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.2.1,because of u'...' Unicode literals.
v2.2.1,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.2.1,minor change to support `**kwargs`
v2.2.1,"user can set verbose with kwargs, it has higher priority"
v2.2.1,register default metric for consistency with callable eval_metric case
v2.2.1,try to deduce from class instance
v2.2.1,overwrite default metric by explicitly set metric
v2.2.1,concatenate metric from params (or default if not provided in params) and eval_metric
v2.2.1,reduce cost for prediction training data
v2.2.1,free dataset
v2.2.1,Switch to using a multiclass objective in the underlying LGBM instance
v2.2.1,check group data
v2.2.1,coding: utf-8
v2.2.1,we don't need lib_lightgbm while building docs
v2.2.1,coding: utf-8
v2.2.1,pylint: disable = C0103
v2.2.1,coding: utf-8
v2.2.1,REMOVEME: remove warning after 2.3.0 version release
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, C0111, C0301"
v2.2.1,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.2.1,TypeError: obj is not a string or a number
v2.2.1,ValueError: invalid literal
v2.2.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.1,avoid memory consumption by arrays concatenation operations
v2.2.1,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.1,avoid memory consumption by arrays concatenation operations
v2.2.1,process for args
v2.2.1,"user can set verbose with params, it has higher priority"
v2.2.1,get categorical features
v2.2.1,process for reference dataset
v2.2.1,start construct data
v2.2.1,check data has header or not
v2.2.1,load init score
v2.2.1,need re group init score
v2.2.1,set feature names
v2.2.1,"change non-float data to float data, need to copy"
v2.2.1,"change non-float data to float data, need to copy"
v2.2.1,create valid
v2.2.1,construct subset
v2.2.1,create train
v2.2.1,set to None
v2.2.1,we're done if self and reference share a common upstrem reference
v2.2.1,"group data from LightGBM is boundaries data, need to convert to group size"
v2.2.1,"user can set verbose with params, it has higher priority"
v2.2.1,Training task
v2.2.1,construct booster object
v2.2.1,save reference to data
v2.2.1,buffer for inner predict
v2.2.1,set network if necessary
v2.2.1,Prediction task
v2.2.1,need reset training data
v2.2.1,need to push new valid data
v2.2.1,Copy models
v2.2.1,Get name of features
v2.2.1,avoid to predict many time in one iteration
v2.2.1,Get num of inner evals
v2.2.1,Get name of evals
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, W0105, C0301"
v2.2.1,Callback environment used by callbacks
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, C0111"
v2.2.1,load or create your dataset
v2.2.1,create dataset for lightgbm
v2.2.1,"if you want to re-use data, remember to set free_raw_data=False"
v2.2.1,specify your configurations as a dict
v2.2.1,generate a feature name
v2.2.1,feature_name and categorical_feature
v2.2.1,check feature name
v2.2.1,save model to file
v2.2.1,dump model to JSON (and save to file)
v2.2.1,feature names
v2.2.1,feature importances
v2.2.1,load model to predict
v2.2.1,can only predict with the best iteration (or the saving iteration)
v2.2.1,eval with loaded model
v2.2.1,dump model with pickle
v2.2.1,load model with pickle to predict
v2.2.1,can predict with any iteration when loaded in pickle way
v2.2.1,eval with loaded model
v2.2.1,continue training
v2.2.1,init_model accepts:
v2.2.1,1. model file name
v2.2.1,2. Booster()
v2.2.1,decay learning rates
v2.2.1,learning_rates accepts:
v2.2.1,1. list/tuple with length = num_boost_round
v2.2.1,2. function(curr_iter)
v2.2.1,change other parameters during training
v2.2.1,self-defined objective function
v2.2.1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.2.1,log likelihood loss
v2.2.1,self-defined eval metric
v2.2.1,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.1,binary error
v2.2.1,callback
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, C0111"
v2.2.1,load or create your dataset
v2.2.1,train
v2.2.1,predict
v2.2.1,eval
v2.2.1,feature importances
v2.2.1,self-defined eval metric
v2.2.1,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.1,Root Mean Squared Logarithmic Error (RMSLE)
v2.2.1,train
v2.2.1,predict
v2.2.1,eval
v2.2.1,other scikit-learn modules
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, C0111"
v2.2.1,load or create your dataset
v2.2.1,create dataset for lightgbm
v2.2.1,specify your configurations as a dict
v2.2.1,train
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, C0111"
v2.2.1,################
v2.2.1,Simulate some binary data with a single categorical and
v2.2.1,single continuous predictor
v2.2.1,################
v2.2.1,Set up a couple of utilities for our experiments
v2.2.1,################
v2.2.1,Observe the behavior of `binary` and `xentropy` objectives
v2.2.1,Trying this throws an error on non-binary values of y:
v2.2.1,"experiment('binary', label_type='probability', DATA)"
v2.2.1,The speed of `binary` is not drastically different than
v2.2.1,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v2.2.1,there are reasons to suspect that `binary` should run faster when the
v2.2.1,label is an integer instead of a float
v2.2.1,coding: utf-8
v2.2.1,"pylint: disable = invalid-name, C0111"
v2.2.1,load or create your dataset
v2.2.1,create dataset for lightgbm
v2.2.1,specify your configurations as a dict
v2.2.1,train
v2.2.1,save model to file
v2.2.1,predict
v2.2.1,eval
v2.2.1,!/usr/bin/env python3
v2.2.1,-*- coding: utf-8 -*-
v2.2.1,
v2.2.1,"LightGBM documentation build configuration file, created by"
v2.2.1,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.2.1,
v2.2.1,This file is execfile()d with the current directory set to its
v2.2.1,containing dir.
v2.2.1,
v2.2.1,Note that not all possible configuration values are present in this
v2.2.1,autogenerated file.
v2.2.1,
v2.2.1,All configuration values have a default; values that are commented out
v2.2.1,serve to show the default.
v2.2.1,"If extensions (or modules to document with autodoc) are in another directory,"
v2.2.1,add these directories to sys.path here. If the directory is relative to the
v2.2.1,"documentation root, use os.path.abspath to make it absolute."
v2.2.1,-- mock out modules
v2.2.1,-- General configuration ------------------------------------------------
v2.2.1,"If your documentation needs a minimal Sphinx version, state it here."
v2.2.1,"Add any Sphinx extension module names here, as strings. They can be"
v2.2.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.2.1,ones.
v2.2.1,"Add any paths that contain templates here, relative to this directory."
v2.2.1,The suffix(es) of source filenames.
v2.2.1,You can specify multiple suffix as a list of string:
v2.2.1,"source_suffix = ['.rst', '.md']"
v2.2.1,The master toctree document.
v2.2.1,General information about the project.
v2.2.1,"The version info for the project you're documenting, acts as replacement for"
v2.2.1,"|version| and |release|, also used in various other places throughout the"
v2.2.1,built documents.
v2.2.1,
v2.2.1,The short X.Y version.
v2.2.1,"The full version, including alpha/beta/rc tags."
v2.2.1,The language for content autogenerated by Sphinx. Refer to documentation
v2.2.1,for a list of supported languages.
v2.2.1,
v2.2.1,This is also used if you do content translation via gettext catalogs.
v2.2.1,"Usually you set ""language"" from the command line for these cases."
v2.2.1,"List of patterns, relative to source directory, that match files and"
v2.2.1,directories to ignore when looking for source files.
v2.2.1,This patterns also effect to html_static_path and html_extra_path
v2.2.1,The name of the Pygments (syntax highlighting) style to use.
v2.2.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.2.1,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.2.1,-- Options for HTML output ----------------------------------------------
v2.2.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.2.1,a list of builtin themes.
v2.2.1,Theme options are theme-specific and customize the look and feel of a theme
v2.2.1,"further.  For a list of options available for each theme, see the"
v2.2.1,documentation.
v2.2.1,"Add any paths that contain custom static files (such as style sheets) here,"
v2.2.1,"relative to this directory. They are copied after the builtin static files,"
v2.2.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.2.1,-- Options for HTMLHelp output ------------------------------------------
v2.2.1,Output file base name for HTML help builder.
v2.2.1,coding: utf-8
v2.2.1,coding: utf-8
v2.2.1,pylint: skip-file
v2.2.1,we don't need lib_lightgbm while building docs
v2.2.1,coding: utf-8
v2.2.1,pylint: skip-file
v2.2.1,check saved model persistence
v2.2.1,"we need to check the consistency of model file here, so test for exact equal"
v2.2.1,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.2.1,"scores likely to be different, but prediction should still be the same"
v2.2.1,coding: utf-8
v2.2.1,pylint: skip-file
v2.2.1,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v2.2.1,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.2.1,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.2.1,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.2.1,Tests same probabilities
v2.2.1,Tests same predictions
v2.2.1,Tests same raw scores
v2.2.1,Tests same leaf indices
v2.2.1,Tests same feature contributions
v2.2.1,Tests other parameters for the prediction works
v2.2.1,coding: utf-8
v2.2.1,pylint: skip-file
v2.2.1,coding: utf-8
v2.2.1,pylint: skip-file
v2.2.1,no early stopping
v2.2.1,early stopping occurs
v2.2.1,test custom eval metrics
v2.2.1,"shuffle = False, override metric in params"
v2.2.1,"shuffle = True, callbacks"
v2.2.1,self defined folds
v2.2.1,lambdarank
v2.2.1,... with l2 metric
v2.2.1,... with NDCG (default) metric
v2.2.1,self defined folds with lambdarank
v2.2.1,test feature_names with whitespaces
v2.2.1,take subsets and train
v2.2.1,test sliced labels
v2.2.1,append some columns
v2.2.1,append some rows
v2.2.1,test sliced 2d matrix
v2.2.1,test sliced CSR
v2.2.1,coding: utf-8
v2.2.1,pylint: skip-file
v2.2.1,Register Dynamic Symbols
v2.2.1,convert from one-based to  zero-based index
v2.2.1,convert from boundaries to size
v2.2.1,--- start Booster interfaces
v2.2.1,create boosting
v2.2.1,initialize the boosting
v2.2.1,create objective function
v2.2.1,initialize the objective function
v2.2.1,create training metric
v2.2.1,reset the boosting
v2.2.1,create objective function
v2.2.1,initialize the objective function
v2.2.1,some help functions used to convert data
v2.2.1,Row iterator of on column for CSC matrix
v2.2.1,"return value at idx, only can access by ascent order"
v2.2.1,"return next non-zero pair, if index < 0, means no more data"
v2.2.1,start of c_api functions
v2.2.1,sample data first
v2.2.1,sample data first
v2.2.1,sample data first
v2.2.1,no more data
v2.2.1,---- start of booster
v2.2.1,---- start of some help functions
v2.2.1,set number of threads for openmp
v2.2.1,check for alias
v2.2.1,read parameters from config file
v2.2.1,"remove str after ""#"""
v2.2.1,check for alias again
v2.2.1,load configs
v2.2.1,prediction is needed if using input initial model(continued train)
v2.2.1,need to continue training
v2.2.1,sync up random seed for data partition
v2.2.1,load Training data
v2.2.1,load data for parallel training
v2.2.1,load data for single machine
v2.2.1,need save binary file
v2.2.1,create training metric
v2.2.1,only when have metrics then need to construct validation data
v2.2.1,"Add validation data, if it exists"
v2.2.1,add
v2.2.1,need save binary file
v2.2.1,add metric for validation data
v2.2.1,output used time on each iteration
v2.2.1,need init network
v2.2.1,create boosting
v2.2.1,create objective function
v2.2.1,load training data
v2.2.1,initialize the objective function
v2.2.1,initialize the boosting
v2.2.1,add validation data into boosting
v2.2.1,convert model to if-else statement code
v2.2.1,create predictor
v2.2.1,Free memory
v2.2.1,create predictor
v2.2.1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.2.1,counts for all labels
v2.2.1,"start from top label, and accumulate DCG"
v2.2.1,counts for all labels
v2.2.1,calculate k Max DCG by one pass
v2.2.1,get sorted indices by score
v2.2.1,calculate dcg
v2.2.1,get sorted indices by score
v2.2.1,calculate multi dcg by one pass
v2.2.1,wait for all client start up
v2.2.1,default set to -1
v2.2.1,"distance at k-th communication, distance[k] = 2^k"
v2.2.1,set incoming rank at k-th commuication
v2.2.1,set outgoing rank at k-th commuication
v2.2.1,defalut set as -1
v2.2.1,construct all recursive halving map for all machines
v2.2.1,let 1 << k <= num_machines
v2.2.1,distance of each communication
v2.2.1,"if num_machines = 2^k, don't need to group machines"
v2.2.1,"communication direction, %2 == 0 is positive"
v2.2.1,neighbor at k-th communication
v2.2.1,receive data block at k-th communication
v2.2.1,send data block at k-th communication
v2.2.1,"if num_machines != 2^k, need to group machines"
v2.2.1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.2.1,let left machine as group leader
v2.2.1,"cache block information for groups, group with 2 machines will have double block size"
v2.2.1,convert from group to node leader
v2.2.1,convert from node to group
v2.2.1,meet new group
v2.2.1,add block len for this group
v2.2.1,calculate the group block start
v2.2.1,not need to construct
v2.2.1,get receive block informations
v2.2.1,accumulate block len
v2.2.1,get send block informations
v2.2.1,accumulate block len
v2.2.1,static member definition
v2.2.1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.2.1,assign the blocks to every rank.
v2.2.1,do reduce scatter
v2.2.1,do all gather
v2.2.1,assign blocks
v2.2.1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.2.1,copy back
v2.2.1,assign blocks
v2.2.1,start all gather
v2.2.1,when num_machines is small and data is large
v2.2.1,use output as receive buffer
v2.2.1,get current local block size
v2.2.1,get out rank
v2.2.1,get in rank
v2.2.1,get send information
v2.2.1,get recv information
v2.2.1,send and recv at same time
v2.2.1,rotate in-place
v2.2.1,use output as receive buffer
v2.2.1,get current local block size
v2.2.1,get send information
v2.2.1,get recv information
v2.2.1,send and recv at same time
v2.2.1,use output as receive buffer
v2.2.1,send and recv at same time
v2.2.1,send local data to neighbor first
v2.2.1,receive neighbor data first
v2.2.1,reduce
v2.2.1,get target
v2.2.1,get send information
v2.2.1,get recv information
v2.2.1,send and recv at same time
v2.2.1,reduce
v2.2.1,send result to neighbor
v2.2.1,receive result from neighbor
v2.2.1,copy result
v2.2.1,start up socket
v2.2.1,parse clients from file
v2.2.1,get ip list of local machine
v2.2.1,get local rank
v2.2.1,construct listener
v2.2.1,construct communication topo
v2.2.1,construct linkers
v2.2.1,free listener
v2.2.1,set timeout
v2.2.1,accept incoming socket
v2.2.1,receive rank
v2.2.1,add new socket
v2.2.1,save ranks that need to connect with
v2.2.1,start listener
v2.2.1,start connect
v2.2.1,let smaller rank connect to larger rank
v2.2.1,send local rank
v2.2.1,wait for listener
v2.2.1,print connected linkers
v2.2.1,Get some statistic from 2 line
v2.2.1,if only have one line on file
v2.2.1,Constructors
v2.2.1,Get type tag
v2.2.1,Comparisons
v2.2.1,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.2.1,"advance until next line, or end of input"
v2.2.1,advance until closing tokens
v2.2.1,The usual case: non-escaped characters
v2.2.1,Handle escapes
v2.2.1,Extract 4-byte escape sequence
v2.2.1,Explicitly check length of the substring. The following loop
v2.2.1,relies on std::string returning the terminating NUL when
v2.2.1,accessing str[length]. Checking here reduces brittleness.
v2.2.1,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.2.1,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.2.1,whether we're in the middle of such a beast: the previous codepoint was an
v2.2.1,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.2.1,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.2.1,the UTF-16 algorithm.
v2.2.1,Integer part
v2.2.1,Decimal part
v2.2.1,Exponent part
v2.2.1,Check for any trailing garbage
v2.2.1,Documented in json11.hpp
v2.2.1,Check for another object
v2.2.1,get column names
v2.2.1,load label idx first
v2.2.1,erase label column name
v2.2.1,load ignore columns
v2.2.1,load weight idx
v2.2.1,load group idx
v2.2.1,don't support query id in data file when training in parallel
v2.2.1,read data to memory
v2.2.1,sample data
v2.2.1,construct feature bin mappers
v2.2.1,initialize label
v2.2.1,extract features
v2.2.1,sample data from file
v2.2.1,construct feature bin mappers
v2.2.1,initialize label
v2.2.1,extract features
v2.2.1,load data from binary file
v2.2.1,check meta data
v2.2.1,need to check training data
v2.2.1,read data in memory
v2.2.1,initialize label
v2.2.1,extract features
v2.2.1,Get number of lines of data file
v2.2.1,initialize label
v2.2.1,extract features
v2.2.1,load data from binary file
v2.2.1,not need to check validation data
v2.2.1,check meta data
v2.2.1,buffer to read binary file
v2.2.1,check token
v2.2.1,read size of header
v2.2.1,re-allocmate space if not enough
v2.2.1,read header
v2.2.1,get header
v2.2.1,num_groups
v2.2.1,real_feature_idx_
v2.2.1,feature2group
v2.2.1,feature2subfeature
v2.2.1,group_bin_boundaries
v2.2.1,group_feature_start_
v2.2.1,group_feature_cnt_
v2.2.1,get feature names
v2.2.1,write feature names
v2.2.1,read size of meta data
v2.2.1,re-allocate space if not enough
v2.2.1,read meta data
v2.2.1,load meta data
v2.2.1,sample local used data if need to partition
v2.2.1,"if not contain query file, minimal sample unit is one record"
v2.2.1,"if contain query file, minimal sample unit is one query"
v2.2.1,if is new query
v2.2.1,read feature data
v2.2.1,read feature size
v2.2.1,re-allocate space if not enough
v2.2.1,fill feature_names_ if not header
v2.2.1,"if only one machine, find bin locally"
v2.2.1,"if have multi-machines, need to find bin distributed"
v2.2.1,different machines will find bin for different features
v2.2.1,start and len will store the process feature indices for different machines
v2.2.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.1,get size of bin mapper with max_bin size
v2.2.1,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.1,find local feature bins and copy to buffer
v2.2.1,free
v2.2.1,convert to binary size
v2.2.1,gather global feature bin mappers
v2.2.1,restore features bins from buffer
v2.2.1,---- private functions ----
v2.2.1,"if features are ordered, not need to use hist_buf"
v2.2.1,read all lines
v2.2.1,get query data
v2.2.1,"if not contain query data, minimal sample unit is one record"
v2.2.1,"if contain query data, minimal sample unit is one query"
v2.2.1,if is new query
v2.2.1,get query data
v2.2.1,"if not contain query file, minimal sample unit is one record"
v2.2.1,"if contain query file, minimal sample unit is one query"
v2.2.1,if is new query
v2.2.1,parse features
v2.2.1,-1 means doesn't use this feature
v2.2.1,"check the range of label_idx, weight_idx and group_idx"
v2.2.1,fill feature_names_ if not header
v2.2.1,start find bins
v2.2.1,"if only one machine, find bin locally"
v2.2.1,"if have multi-machines, need to find bin distributed"
v2.2.1,different machines will find bin for different features
v2.2.1,start and len will store the process feature indices for different machines
v2.2.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.1,get size of bin mapper with max_bin size
v2.2.1,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.1,find local feature bins and copy to buffer
v2.2.1,free
v2.2.1,convert to binary size
v2.2.1,gather global feature bin mappers
v2.2.1,restore features bins from buffer
v2.2.1,if doesn't need to prediction with initial model
v2.2.1,parser
v2.2.1,set label
v2.2.1,free processed line:
v2.2.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.1,text_reader_->Lines()[i].shrink_to_fit();
v2.2.1,push data
v2.2.1,if is used feature
v2.2.1,if need to prediction with initial model
v2.2.1,parser
v2.2.1,set initial score
v2.2.1,set label
v2.2.1,free processed line:
v2.2.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.1,text_reader_->Lines()[i].shrink_to_fit();
v2.2.1,push data
v2.2.1,if is used feature
v2.2.1,metadata_ will manage space of init_score
v2.2.1,text data can be free after loaded feature values
v2.2.1,parser
v2.2.1,set initial score
v2.2.1,set label
v2.2.1,push data
v2.2.1,if is used feature
v2.2.1,only need part of data
v2.2.1,need full data
v2.2.1,metadata_ will manage space of init_score
v2.2.1,read size of token
v2.2.1,deep copy function for BinMapper
v2.2.1,mean size for one bin
v2.2.1,need a new bin
v2.2.1,update bin upper bound
v2.2.1,last bin upper bound
v2.2.1,find distinct_values first
v2.2.1,push zero in the front
v2.2.1,use the large value
v2.2.1,push zero in the back
v2.2.1,convert to int type first
v2.2.1,sort by counts
v2.2.1,avoid first bin is zero
v2.2.1,will ignore the categorical of small counts
v2.2.1,need an additional bin for NaN
v2.2.1,use -1 to represent NaN
v2.2.1,Use MissingType::None to represent this bin contains all categoricals
v2.2.1,check trival(num_bin_ == 1) feature
v2.2.1,check useless bin
v2.2.1,calculate sparse rate
v2.2.1,sparse threshold
v2.2.1,"for lambdarank, it needs query data for partition data in parallel learning"
v2.2.1,need convert query_id to boundaries
v2.2.1,check weights
v2.2.1,check query boundries
v2.2.1,contain initial score file
v2.2.1,check weights
v2.2.1,get local weights
v2.2.1,check query boundries
v2.2.1,get local query boundaries
v2.2.1,contain initial score file
v2.2.1,get local initial scores
v2.2.1,re-load query weight
v2.2.1,save to nullptr
v2.2.1,save to nullptr
v2.2.1,save to nullptr
v2.2.1,default weight file name
v2.2.1,default weight file name
v2.2.1,use first line to count number class
v2.2.1,default query file name
v2.2.1,/ This file is auto generated by LightGBM\helper\parameter_generator.py from LightGBM\include\LightGBM\config.h file.
v2.2.1,root is in the depth 0
v2.2.1,non-leaf
v2.2.1,leaf
v2.2.1,use this for the missing value conversion
v2.2.1,Predict func by Map to ifelse
v2.2.1,use this for the missing value conversion
v2.2.1,non-leaf
v2.2.1,left subtree
v2.2.1,right subtree
v2.2.1,leaf
v2.2.1,non-leaf
v2.2.1,left subtree
v2.2.1,right subtree
v2.2.1,leaf
v2.2.1,recursive computation of SHAP values for a decision tree
v2.2.1,extend the unique path
v2.2.1,leaf node
v2.2.1,internal node
v2.2.1,"see if we have already split on this feature,"
v2.2.1,if so we undo that split so we can redo it for this node
v2.2.1,clear old metrics
v2.2.1,to lower
v2.2.1,split
v2.2.1,remove duplicate
v2.2.1,add names of objective function if not providing metric
v2.2.1,generate seeds by seed.
v2.2.1,check for conflicts
v2.2.1,"check if objective, metric, and num_class match"
v2.2.1,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.2.1,Check max_depth and num_leaves
v2.2.1,"filter is based on sampling data, so decrease its range"
v2.2.1,put dense feature first
v2.2.1,sort by non zero cnt
v2.2.1,"sort by non zero cnt, bigger first"
v2.2.1,"take apart small sparse group, due it will not gain on speed"
v2.2.1,shuffle groups
v2.2.1,get num_features
v2.2.1,get bin_mappers
v2.2.1,copy feature bin mapper data
v2.2.1,copy feature bin mapper data
v2.2.1,"if not pass a filename, just append "".bin"" of original file"
v2.2.1,get size of header
v2.2.1,size of feature names
v2.2.1,write header
v2.2.1,write feature names
v2.2.1,get size of meta data
v2.2.1,write meta data
v2.2.1,write feature data
v2.2.1,get size of feature
v2.2.1,write feature
v2.2.1,feature is not used
v2.2.1,construct histograms for smaller leaf
v2.2.1,if not use ordered bin
v2.2.1,used ordered bin
v2.2.1,feature is not used
v2.2.1,construct histograms for smaller leaf
v2.2.1,if not use ordered bin
v2.2.1,used ordered bin
v2.2.1,fixed hessian.
v2.2.1,feature is not used
v2.2.1,construct histograms for smaller leaf
v2.2.1,if not use ordered bin
v2.2.1,used ordered bin
v2.2.1,feature is not used
v2.2.1,construct histograms for smaller leaf
v2.2.1,if not use ordered bin
v2.2.1,used ordered bin
v2.2.1,fixed hessian.
v2.2.1,PredictRaw
v2.2.1,PredictRawByMap
v2.2.1,Predict
v2.2.1,PredictByMap
v2.2.1,PredictLeafIndex
v2.2.1,PredictLeafIndexByMap
v2.2.1,output model type
v2.2.1,output number of class
v2.2.1,output label index
v2.2.1,output max_feature_idx
v2.2.1,output objective
v2.2.1,output tree models
v2.2.1,store the importance first
v2.2.1,sort the importance
v2.2.1,use serialized string to restore this object
v2.2.1,Use first 128 chars to avoid exceed the message buffer.
v2.2.1,get number of classes
v2.2.1,get index of label
v2.2.1,get max_feature_idx first
v2.2.1,get average_output
v2.2.1,get feature names
v2.2.1,set zero
v2.2.1,predict all the trees for one iteration
v2.2.1,check early stopping
v2.2.1,set zero
v2.2.1,predict all the trees for one iteration
v2.2.1,check early stopping
v2.2.1,margin_threshold will be captured by value
v2.2.1,copy and sort
v2.2.1,margin_threshold will be captured by value
v2.2.1,load forced_splits file
v2.2.1,init tree learner
v2.2.1,push training metrics
v2.2.1,create buffer for gradients and hessians
v2.2.1,get max feature index
v2.2.1,get label index
v2.2.1,get feature names
v2.2.1,"if need bagging, create buffer"
v2.2.1,reset config for tree learner
v2.2.1,multi-class
v2.2.1,binary class
v2.2.1,"for a validation dataset, we need its score and metric"
v2.2.1,update score
v2.2.1,objective function will calculate gradients and hessians
v2.2.1,"random bagging, minimal unit is one record"
v2.2.1,if need bagging
v2.2.1,set bagging data to tree learner
v2.2.1,get subset
v2.2.1,output used time per iteration
v2.2.1,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.2.1,boosting first
v2.2.1,bagging logic
v2.2.1,need to copy gradients for bagging subset.
v2.2.1,shrinkage by learning rate
v2.2.1,update score
v2.2.1,only add default score one-time
v2.2.1,updates scores
v2.2.1,add model
v2.2.1,reset score
v2.2.1,remove model
v2.2.1,print message for metric
v2.2.1,pop last early_stopping_round_ models
v2.2.1,update training score
v2.2.1,we need to predict out-of-bag scores of data for boosting
v2.2.1,update validation score
v2.2.1,print training metric
v2.2.1,print validation metric
v2.2.1,set zero
v2.2.1,predict all the trees for one iteration
v2.2.1,check early stopping
v2.2.1,push training metrics
v2.2.1,"not same training data, need reset score and others"
v2.2.1,create score tracker
v2.2.1,update score
v2.2.1,create buffer for gradients and hessians
v2.2.1,"if need bagging, create buffer"
v2.2.1,Get the max size of pool
v2.2.1,at least need 2 leaves
v2.2.1,push split information for all leaves
v2.2.1,get ordered bin
v2.2.1,check existing for ordered bin
v2.2.1,initialize splits for leaf
v2.2.1,initialize data partition
v2.2.1,initialize ordered gradients and hessians
v2.2.1,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.1,get ordered bin
v2.2.1,initialize splits for leaf
v2.2.1,initialize data partition
v2.2.1,initialize ordered gradients and hessians
v2.2.1,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.1,Get the max size of pool
v2.2.1,at least need 2 leaves
v2.2.1,push split information for all leaves
v2.2.1,some initial works before training
v2.2.1,root leaf
v2.2.1,only root leaf can be splitted on first time
v2.2.1,some initial works before finding best split
v2.2.1,find best threshold for every feature
v2.2.1,Get a leaf with max split gain
v2.2.1,Get split information for best leaf
v2.2.1,"cannot split, quit"
v2.2.1,split tree with best leaf
v2.2.1,reset histogram pool
v2.2.1,at least use one feature
v2.2.1,initialize used features
v2.2.1,Get used feature at current tree
v2.2.1,initialize data partition
v2.2.1,reset the splits for leaves
v2.2.1,Sumup for root
v2.2.1,use all data
v2.2.1,"use bagging, only use part of data"
v2.2.1,"if has ordered bin, need to initialize the ordered bin"
v2.2.1,"use all data, pass nullptr"
v2.2.1,"bagging, only use part of data"
v2.2.1,mark used data
v2.2.1,initialize ordered bin
v2.2.1,check depth of current leaf
v2.2.1,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.2.1,no enough data to continue
v2.2.1,only have root
v2.2.1,put parent(left) leaf's histograms into larger leaf's histograms
v2.2.1,put parent(left) leaf's histograms to larger leaf's histograms
v2.2.1,split for the ordered bin
v2.2.1,mark data that at left-leaf
v2.2.1,split the ordered bin
v2.2.1,construct smaller leaf
v2.2.1,construct larger leaf
v2.2.1,find splits
v2.2.1,only has root leaf
v2.2.1,find best threshold for larger child
v2.2.1,start at root leaf
v2.2.1,"before processing next node from queue, store info for current left/right leaf"
v2.2.1,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.2.1,"then, compute own splits"
v2.2.1,split info should exist because searching in bfs fashion - should have added from parent
v2.2.1,"split tree, will return right leaf"
v2.2.1,left = parent
v2.2.1,"split tree, will return right leaf"
v2.2.1,init the leaves that used on next iteration
v2.2.1,bag_mapper[index_mapper[i]]
v2.2.1,get feature partition
v2.2.1,get local used features
v2.2.1,get best split at smaller leaf
v2.2.1,find local best split for larger leaf
v2.2.1,sync global best info
v2.2.1,update best split
v2.2.1,"instantiate template classes, otherwise linker cannot find the code"
v2.2.1,initialize SerialTreeLearner
v2.2.1,Get local rank and global machine size
v2.2.1,allocate buffer for communication
v2.2.1,generate feature partition for current tree
v2.2.1,get local used feature
v2.2.1,get block start and block len for reduce scatter
v2.2.1,get buffer_write_start_pos_
v2.2.1,get buffer_read_start_pos_
v2.2.1,sync global data sumup info
v2.2.1,global sumup reduce
v2.2.1,copy back
v2.2.1,set global sumup info
v2.2.1,init global data count in leaf
v2.2.1,construct local histograms
v2.2.1,copy to buffer
v2.2.1,Reduce scatter for histogram
v2.2.1,restore global histograms from buffer
v2.2.1,find best threshold for smaller child
v2.2.1,only root leaf
v2.2.1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.2.1,find best threshold for larger child
v2.2.1,find local best split for larger leaf
v2.2.1,sync global best info
v2.2.1,set best split
v2.2.1,need update global number of data in leaf
v2.2.1,"instantiate template classes, otherwise linker cannot find the code"
v2.2.1,initialize SerialTreeLearner
v2.2.1,some additional variables needed for GPU trainer
v2.2.1,Initialize GPU buffers and kernels
v2.2.1,some functions used for debugging the GPU histogram construction
v2.2.1,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.2.1,goto err;
v2.2.1,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.2.1,goto err;
v2.2.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.2.1,also guarantee that there are at least 2K examples per workgroup
v2.2.1,return 0;
v2.2.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.2.1,decide the best number of workgroups working on one feature4 tuple
v2.2.1,set work group size based on feature size
v2.2.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.2.1,we need to refresh the kernel arguments after reallocating
v2.2.1,The only argument that needs to be changed later is num_data_
v2.2.1,"the GPU kernel will process all features in one call, and each"
v2.2.1,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.2.1,process one feature4 tuple
v2.2.1,"for the root node, indices are not copied"
v2.2.1,"for constant hessian, hessians are not copied except for the root node"
v2.2.1,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.2.1,and we will launch num_feature workgroups for this kernel
v2.2.1,will launch threads for all features
v2.2.1,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.2.1,copy the results asynchronously. Size depends on if double precision is used
v2.2.1,we will wait for this object in WaitAndGetHistograms
v2.2.1,"when the output is ready, the computation is done"
v2.2.1,values of this feature has been redistributed to multiple bins; need a reduction here
v2.2.1,how many feature-group tuples we have
v2.2.1,leave some safe margin for prefetching
v2.2.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.2.1,clear sparse/dense maps
v2.2.1,do nothing if no features can be processed on GPU
v2.2.1,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.2.1,unpin old buffer if necessary before destructing them
v2.2.1,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.2.1,allocate space for gradients and hessians on device
v2.2.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.2.1,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.2.1,copy indices to the device
v2.2.1,histogram bin entry size depends on the precision (single/double)
v2.2.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.2.1,each work group generates a sub-histogram of dword_features_ features.
v2.2.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.2.1,create atomic counters for inter-group coordination
v2.2.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.2.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.2.1,looking for dword_features_ non-sparse feature-groups
v2.2.1,decide if we need to redistribute the bin
v2.2.1,multiplier must be a power of 2
v2.2.1,device_bin_mults_.push_back(1);
v2.2.1,found
v2.2.1,for data transfer time
v2.2.1,"Now generate new data structure feature4, and copy data to the device"
v2.2.1,"preallocate arrays for all threads, and pin them"
v2.2.1,building Feature4 bundles; each thread handles dword_features_ features
v2.2.1,one feature datapoint is 4 bits
v2.2.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.1,one feature datapoint is one byte
v2.2.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.1,Dense bin
v2.2.1,Dense 4-bit bin
v2.2.1,working on the remaining (less than dword_features_) feature groups
v2.2.1,fill the leftover features
v2.2.1,"fill this empty feature with some ""random"" value"
v2.2.1,"fill this empty feature with some ""random"" value"
v2.2.1,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.2.1,deallocate pinned space for feature copying
v2.2.1,data transfer time
v2.2.1,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.2.1,"Something bad happened. Just return ""No log available."""
v2.2.1,"build is okay, log may contain warnings"
v2.2.1,destroy any old kernels
v2.2.1,create OpenCL kernels for different number of workgroups per feature
v2.2.1,currently we don't use constant memory
v2.2.1,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.2.1,kernel with indices in an array
v2.2.1,"kernel with all features enabled, with elimited branches"
v2.2.1,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.2.1,do nothing if no features can be processed on GPU
v2.2.1,The only argument that needs to be changed later is num_data_
v2.2.1,"hessian is passed as a parameter, but it is not available now."
v2.2.1,hessian will be set in BeforeTrain()
v2.2.1,"Get the max bin size, used for selecting best GPU kernel"
v2.2.1,initialize GPU
v2.2.1,determine which kernel to use based on the max number of bins
v2.2.1,setup GPU kernel arguments after we allocating all the buffers
v2.2.1,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.2.1,this should rarely occur
v2.2.1,GPU memory has to been reallocated because data may have been changed
v2.2.1,setup GPU kernel arguments after we allocating all the buffers
v2.2.1,Copy initial full hessians and gradients to GPU.
v2.2.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.2.1,setup hessian parameters only
v2.2.1,hessian is passed as a parameter
v2.2.1,use bagging
v2.2.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.2.1,copy used gradients and hessians to ordered buffer
v2.2.1,transfer the indices to GPU
v2.2.1,transfer hessian to GPU
v2.2.1,setup hessian parameters only
v2.2.1,hessian is passed as a parameter
v2.2.1,transfer gradients to GPU
v2.2.1,only have root
v2.2.1,"Copy indices, gradients and hessians as early as possible"
v2.2.1,only need to initialize for smaller leaf
v2.2.1,Get leaf boundary
v2.2.1,copy indices to the GPU:
v2.2.1,copy ordered hessians to the GPU:
v2.2.1,copy ordered gradients to the GPU:
v2.2.1,do nothing if no features can be processed on GPU
v2.2.1,copy data indices if it is not null
v2.2.1,generate and copy ordered_gradients if gradients is not null
v2.2.1,generate and copy ordered_hessians if hessians is not null
v2.2.1,converted indices in is_feature_used to feature-group indices
v2.2.1,construct the feature masks for dense feature-groups
v2.2.1,"if no feature group is used, just return and do not use GPU"
v2.2.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.2.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.2.1,"All data have been prepared, now run the GPU kernel"
v2.2.1,construct smaller leaf
v2.2.1,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.2.1,then construct sparse features on CPU
v2.2.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.1,"wait for GPU to finish, only if GPU is actually used"
v2.2.1,use double precision
v2.2.1,use single precision
v2.2.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.2.1,#define GPU_DEBUG_COMPARE
v2.2.1,construct larger leaf
v2.2.1,then construct sparse features on CPU
v2.2.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.1,"wait for GPU to finish, only if GPU is actually used"
v2.2.1,use double precision
v2.2.1,use single precision
v2.2.1,do some sanity check for the GPU algorithm
v2.2.1,limit top k
v2.2.1,get max bin
v2.2.1,calculate buffer size
v2.2.1,"left and right on same time, so need double size"
v2.2.1,initialize histograms for global
v2.2.1,sync global data sumup info
v2.2.1,set global sumup info
v2.2.1,init global data count in leaf
v2.2.1,get local sumup
v2.2.1,get local sumup
v2.2.1,get mean number on machines
v2.2.1,weighted gain
v2.2.1,get top k
v2.2.1,"Copy histogram to buffer, and Get local aggregate features"
v2.2.1,copy histograms.
v2.2.1,copy smaller leaf histograms first
v2.2.1,mark local aggregated feature
v2.2.1,copy
v2.2.1,then copy larger leaf histograms
v2.2.1,mark local aggregated feature
v2.2.1,copy
v2.2.1,use local data to find local best splits
v2.2.1,find splits
v2.2.1,only has root leaf
v2.2.1,find best threshold for larger child
v2.2.1,local voting
v2.2.1,gather
v2.2.1,get all top-k from all machines
v2.2.1,global voting
v2.2.1,copy local histgrams to buffer
v2.2.1,Reduce scatter for histogram
v2.2.1,find best split from local aggregated histograms
v2.2.1,restore from buffer
v2.2.1,find best threshold
v2.2.1,restore from buffer
v2.2.1,find best threshold
v2.2.1,find local best
v2.2.1,find local best split for larger leaf
v2.2.1,sync global best info
v2.2.1,copy back
v2.2.1,set the global number of data for leaves
v2.2.1,init the global sumup info
v2.2.1,"instantiate template classes, otherwise linker cannot find the code"
v2.2.1,coding: utf-8
v2.2.1,alias table
v2.2.1,names
v2.2.1,from strings
v2.2.1,tails
v2.2.1,tails
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable=invalid-name, exec-used, C0111"
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, W0105"
v2.2.0,create predictor first
v2.2.0,check dataset
v2.2.0,reduce cost for prediction training data
v2.2.0,process callbacks
v2.2.0,Most of legacy advanced options becomes callbacks
v2.2.0,construct booster
v2.2.0,start training
v2.2.0,check evaluation result.
v2.2.0,"lambdarank task, split according to groups"
v2.2.0,run preprocessing on the data set if needed
v2.2.0,setup callbacks
v2.2.0,coding: utf-8
v2.2.0,pylint: disable = C0103
v2.2.0,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.2.0,because of u'...' Unicode literals.
v2.2.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.2.0,minor change to support `**kwargs`
v2.2.0,"user can set verbose with kwargs, it has higher priority"
v2.2.0,register default metric for consistency with callable eval_metric case
v2.2.0,try to deduce from class instance
v2.2.0,overwrite default metric by explicitly set metric
v2.2.0,concatenate metric from params (or default if not provided in params) and eval_metric
v2.2.0,reduce cost for prediction training data
v2.2.0,free dataset
v2.2.0,Switch to using a multiclass objective in the underlying LGBM instance
v2.2.0,check group data
v2.2.0,coding: utf-8
v2.2.0,we don't need lib_lightgbm while building docs
v2.2.0,coding: utf-8
v2.2.0,pylint: disable = C0103
v2.2.0,coding: utf-8
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, C0111, C0301"
v2.2.0,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.2.0,TypeError: obj is not a string or a number
v2.2.0,ValueError: invalid literal
v2.2.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.0,avoid memory consumption by arrays concatenation operations
v2.2.0,"__get_num_preds() cannot work with nrow > MAX_INT32, so calculate overall number of predictions piecemeal"
v2.2.0,avoid memory consumption by arrays concatenation operations
v2.2.0,process for args
v2.2.0,"user can set verbose with params, it has higher priority"
v2.2.0,get categorical features
v2.2.0,process for reference dataset
v2.2.0,start construct data
v2.2.0,check data has header or not
v2.2.0,load init score
v2.2.0,need re group init score
v2.2.0,set feature names
v2.2.0,"change non-float data to float data, need to copy"
v2.2.0,"change non-float data to float data, need to copy"
v2.2.0,create valid
v2.2.0,construct subset
v2.2.0,create train
v2.2.0,set to None
v2.2.0,we're done if self and reference share a common upstrem reference
v2.2.0,"group data from LightGBM is boundaries data, need to convert to group size"
v2.2.0,"user can set verbose with params, it has higher priority"
v2.2.0,Training task
v2.2.0,construct booster object
v2.2.0,save reference to data
v2.2.0,buffer for inner predict
v2.2.0,set network if necessary
v2.2.0,Prediction task
v2.2.0,need reset training data
v2.2.0,need to push new valid data
v2.2.0,Copy models
v2.2.0,Get name of features
v2.2.0,avoid to predict many time in one iteration
v2.2.0,Get num of inner evals
v2.2.0,Get name of evals
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, W0105, C0301"
v2.2.0,Callback environment used by callbacks
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, C0111"
v2.2.0,load or create your dataset
v2.2.0,create dataset for lightgbm
v2.2.0,"if you want to re-use data, remember to set free_raw_data=False"
v2.2.0,specify your configurations as a dict
v2.2.0,generate a feature name
v2.2.0,feature_name and categorical_feature
v2.2.0,check feature name
v2.2.0,save model to file
v2.2.0,dump model to JSON (and save to file)
v2.2.0,feature names
v2.2.0,feature importances
v2.2.0,load model to predict
v2.2.0,can only predict with the best iteration (or the saving iteration)
v2.2.0,eval with loaded model
v2.2.0,dump model with pickle
v2.2.0,load model with pickle to predict
v2.2.0,can predict with any iteration when loaded in pickle way
v2.2.0,eval with loaded model
v2.2.0,continue training
v2.2.0,init_model accepts:
v2.2.0,1. model file name
v2.2.0,2. Booster()
v2.2.0,decay learning rates
v2.2.0,learning_rates accepts:
v2.2.0,1. list/tuple with length = num_boost_round
v2.2.0,2. function(curr_iter)
v2.2.0,change other parameters during training
v2.2.0,self-defined objective function
v2.2.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.2.0,log likelihood loss
v2.2.0,self-defined eval metric
v2.2.0,"f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.0,binary error
v2.2.0,callback
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, C0111"
v2.2.0,load or create your dataset
v2.2.0,train
v2.2.0,predict
v2.2.0,eval
v2.2.0,feature importances
v2.2.0,self-defined eval metric
v2.2.0,"f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool"
v2.2.0,Root Mean Squared Logarithmic Error (RMSLE)
v2.2.0,train
v2.2.0,predict
v2.2.0,eval
v2.2.0,other scikit-learn modules
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, C0111"
v2.2.0,load or create your dataset
v2.2.0,create dataset for lightgbm
v2.2.0,specify your configurations as a dict
v2.2.0,train
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, C0111"
v2.2.0,################
v2.2.0,Simulate some binary data with a single categorical and
v2.2.0,single continuous predictor
v2.2.0,################
v2.2.0,Set up a couple of utilities for our experiments
v2.2.0,################
v2.2.0,Observe the behavior of `binary` and `xentropy` objectives
v2.2.0,Trying this throws an error on non-binary values of y:
v2.2.0,"experiment('binary', label_type='probability', DATA)"
v2.2.0,The speed of `binary` is not drastically different than
v2.2.0,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v2.2.0,there are reasons to suspect that `binary` should run faster when the
v2.2.0,label is an integer instead of a float
v2.2.0,coding: utf-8
v2.2.0,"pylint: disable = invalid-name, C0111"
v2.2.0,load or create your dataset
v2.2.0,create dataset for lightgbm
v2.2.0,specify your configurations as a dict
v2.2.0,train
v2.2.0,save model to file
v2.2.0,predict
v2.2.0,eval
v2.2.0,!/usr/bin/env python3
v2.2.0,-*- coding: utf-8 -*-
v2.2.0,
v2.2.0,"LightGBM documentation build configuration file, created by"
v2.2.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.2.0,
v2.2.0,This file is execfile()d with the current directory set to its
v2.2.0,containing dir.
v2.2.0,
v2.2.0,Note that not all possible configuration values are present in this
v2.2.0,autogenerated file.
v2.2.0,
v2.2.0,All configuration values have a default; values that are commented out
v2.2.0,serve to show the default.
v2.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
v2.2.0,add these directories to sys.path here. If the directory is relative to the
v2.2.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.2.0,
v2.2.0,-- mock out modules
v2.2.0,-- General configuration ------------------------------------------------
v2.2.0,"If your documentation needs a minimal Sphinx version, state it here."
v2.2.0,"Add any Sphinx extension module names here, as strings. They can be"
v2.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.2.0,ones.
v2.2.0,"Add any paths that contain templates here, relative to this directory."
v2.2.0,The suffix(es) of source filenames.
v2.2.0,You can specify multiple suffix as a list of string:
v2.2.0,"source_suffix = ['.rst', '.md']"
v2.2.0,The master toctree document.
v2.2.0,General information about the project.
v2.2.0,"The version info for the project you're documenting, acts as replacement for"
v2.2.0,"|version| and |release|, also used in various other places throughout the"
v2.2.0,built documents.
v2.2.0,
v2.2.0,The short X.Y version.
v2.2.0,"The full version, including alpha/beta/rc tags."
v2.2.0,The language for content autogenerated by Sphinx. Refer to documentation
v2.2.0,for a list of supported languages.
v2.2.0,
v2.2.0,This is also used if you do content translation via gettext catalogs.
v2.2.0,"Usually you set ""language"" from the command line for these cases."
v2.2.0,"List of patterns, relative to source directory, that match files and"
v2.2.0,directories to ignore when looking for source files.
v2.2.0,This patterns also effect to html_static_path and html_extra_path
v2.2.0,The name of the Pygments (syntax highlighting) style to use.
v2.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.2.0,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.2.0,-- Options for HTML output ----------------------------------------------
v2.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.2.0,a list of builtin themes.
v2.2.0,
v2.2.0,Theme options are theme-specific and customize the look and feel of a theme
v2.2.0,"further.  For a list of options available for each theme, see the"
v2.2.0,documentation.
v2.2.0,
v2.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
v2.2.0,"relative to this directory. They are copied after the builtin static files,"
v2.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.2.0,-- Options for HTMLHelp output ------------------------------------------
v2.2.0,Output file base name for HTML help builder.
v2.2.0,coding: utf-8
v2.2.0,coding: utf-8
v2.2.0,pylint: skip-file
v2.2.0,we don't need lib_lightgbm while building docs
v2.2.0,coding: utf-8
v2.2.0,pylint: skip-file
v2.2.0,check saved model persistence
v2.2.0,"we need to check the consistency of model file here, so test for exact equal"
v2.2.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.2.0,"scores likely to be different, but prediction should still be the same"
v2.2.0,coding: utf-8
v2.2.0,pylint: skip-file
v2.2.0,"Test that the largest element is NOT the same, the smallest can be the same, i.e. zero"
v2.2.0,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.2.0,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.2.0,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.2.0,Tests same probabilities
v2.2.0,Tests same predictions
v2.2.0,Tests same raw scores
v2.2.0,Tests same leaf indices
v2.2.0,Tests same feature contributions
v2.2.0,Tests other parameters for the prediction works
v2.2.0,coding: utf-8
v2.2.0,pylint: skip-file
v2.2.0,coding: utf-8
v2.2.0,pylint: skip-file
v2.2.0,no early stopping
v2.2.0,early stopping occurs
v2.2.0,test custom eval metrics
v2.2.0,"shuffle = False, override metric in params"
v2.2.0,"shuffle = True, callbacks"
v2.2.0,self defined folds
v2.2.0,lambdarank
v2.2.0,... with NDCG (default) metric
v2.2.0,... with l2 metric
v2.2.0,test feature_names with whitespaces
v2.2.0,take subsets and train
v2.2.0,test sliced labels
v2.2.0,append some columns
v2.2.0,append some rows
v2.2.0,test sliced 2d matrix
v2.2.0,test sliced CSR
v2.2.0,coding: utf-8
v2.2.0,pylint: skip-file
v2.2.0,Register Dynamic Symbols
v2.2.0,convert from one-based to  zero-based index
v2.2.0,convert from boundaries to size
v2.2.0,--- start Booster interfaces
v2.2.0,create boosting
v2.2.0,initialize the boosting
v2.2.0,create objective function
v2.2.0,initialize the objective function
v2.2.0,create training metric
v2.2.0,reset the boosting
v2.2.0,create objective function
v2.2.0,initialize the objective function
v2.2.0,some help functions used to convert data
v2.2.0,Row iterator of on column for CSC matrix
v2.2.0,"return value at idx, only can access by ascent order"
v2.2.0,"return next non-zero pair, if index < 0, means no more data"
v2.2.0,start of c_api functions
v2.2.0,sample data first
v2.2.0,sample data first
v2.2.0,sample data first
v2.2.0,no more data
v2.2.0,---- start of booster
v2.2.0,---- start of some help functions
v2.2.0,set number of threads for openmp
v2.2.0,check for alias
v2.2.0,read parameters from config file
v2.2.0,"remove str after ""#"""
v2.2.0,check for alias again
v2.2.0,load configs
v2.2.0,prediction is needed if using input initial model(continued train)
v2.2.0,need to continue training
v2.2.0,sync up random seed for data partition
v2.2.0,load Training data
v2.2.0,load data for parallel training
v2.2.0,load data for single machine
v2.2.0,need save binary file
v2.2.0,create training metric
v2.2.0,only when have metrics then need to construct validation data
v2.2.0,"Add validation data, if it exists"
v2.2.0,add
v2.2.0,need save binary file
v2.2.0,add metric for validation data
v2.2.0,output used time on each iteration
v2.2.0,need init network
v2.2.0,create boosting
v2.2.0,create objective function
v2.2.0,load training data
v2.2.0,initialize the objective function
v2.2.0,initialize the boosting
v2.2.0,add validation data into boosting
v2.2.0,convert model to if-else statement code
v2.2.0,create predictor
v2.2.0,Free memory
v2.2.0,create predictor
v2.2.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.2.0,counts for all labels
v2.2.0,"start from top label, and accumulate DCG"
v2.2.0,counts for all labels
v2.2.0,calculate k Max DCG by one pass
v2.2.0,get sorted indices by score
v2.2.0,calculate dcg
v2.2.0,get sorted indices by score
v2.2.0,calculate multi dcg by one pass
v2.2.0,wait for all client start up
v2.2.0,default set to -1
v2.2.0,"distance at k-th communication, distance[k] = 2^k"
v2.2.0,set incoming rank at k-th commuication
v2.2.0,set outgoing rank at k-th commuication
v2.2.0,defalut set as -1
v2.2.0,construct all recursive halving map for all machines
v2.2.0,let 1 << k <= num_machines
v2.2.0,distance of each communication
v2.2.0,"if num_machines = 2^k, don't need to group machines"
v2.2.0,"communication direction, %2 == 0 is positive"
v2.2.0,neighbor at k-th communication
v2.2.0,receive data block at k-th communication
v2.2.0,send data block at k-th communication
v2.2.0,"if num_machines != 2^k, need to group machines"
v2.2.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.2.0,let left machine as group leader
v2.2.0,"cache block information for groups, group with 2 machines will have double block size"
v2.2.0,convert from group to node leader
v2.2.0,convert from node to group
v2.2.0,meet new group
v2.2.0,add block len for this group
v2.2.0,calculate the group block start
v2.2.0,not need to construct
v2.2.0,get receive block informations
v2.2.0,accumulate block len
v2.2.0,get send block informations
v2.2.0,accumulate block len
v2.2.0,static member definition
v2.2.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.2.0,assign the blocks to every rank.
v2.2.0,do reduce scatter
v2.2.0,do all gather
v2.2.0,assign blocks
v2.2.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.2.0,copy back
v2.2.0,assign blocks
v2.2.0,start all gather
v2.2.0,when num_machines is small and data is large
v2.2.0,use output as receive buffer
v2.2.0,get current local block size
v2.2.0,get out rank
v2.2.0,get in rank
v2.2.0,get send information
v2.2.0,get recv information
v2.2.0,send and recv at same time
v2.2.0,rotate in-place
v2.2.0,use output as receive buffer
v2.2.0,get current local block size
v2.2.0,get send information
v2.2.0,get recv information
v2.2.0,send and recv at same time
v2.2.0,use output as receive buffer
v2.2.0,send and recv at same time
v2.2.0,send local data to neighbor first
v2.2.0,receive neighbor data first
v2.2.0,reduce
v2.2.0,get target
v2.2.0,get send information
v2.2.0,get recv information
v2.2.0,send and recv at same time
v2.2.0,reduce
v2.2.0,send result to neighbor
v2.2.0,receive result from neighbor
v2.2.0,copy result
v2.2.0,start up socket
v2.2.0,parse clients from file
v2.2.0,get ip list of local machine
v2.2.0,get local rank
v2.2.0,construct listener
v2.2.0,construct communication topo
v2.2.0,construct linkers
v2.2.0,free listener
v2.2.0,set timeout
v2.2.0,accept incoming socket
v2.2.0,receive rank
v2.2.0,add new socket
v2.2.0,save ranks that need to connect with
v2.2.0,start listener
v2.2.0,start connect
v2.2.0,let smaller rank connect to larger rank
v2.2.0,send local rank
v2.2.0,wait for listener
v2.2.0,print connected linkers
v2.2.0,Get some statistic from 2 line
v2.2.0,if only have one line on file
v2.2.0,Constructors
v2.2.0,Get type tag
v2.2.0,Comparisons
v2.2.0,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.2.0,"advance until next line, or end of input"
v2.2.0,advance until closing tokens
v2.2.0,The usual case: non-escaped characters
v2.2.0,Handle escapes
v2.2.0,Extract 4-byte escape sequence
v2.2.0,Explicitly check length of the substring. The following loop
v2.2.0,relies on std::string returning the terminating NUL when
v2.2.0,accessing str[length]. Checking here reduces brittleness.
v2.2.0,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.2.0,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.2.0,whether we're in the middle of such a beast: the previous codepoint was an
v2.2.0,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.2.0,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.2.0,the UTF-16 algorithm.
v2.2.0,Integer part
v2.2.0,Decimal part
v2.2.0,Exponent part
v2.2.0,Check for any trailing garbage
v2.2.0,Documented in json11.hpp
v2.2.0,Check for another object
v2.2.0,get column names
v2.2.0,load label idx first
v2.2.0,erase label column name
v2.2.0,load ignore columns
v2.2.0,load weight idx
v2.2.0,load group idx
v2.2.0,don't support query id in data file when training in parallel
v2.2.0,read data to memory
v2.2.0,sample data
v2.2.0,construct feature bin mappers
v2.2.0,initialize label
v2.2.0,extract features
v2.2.0,sample data from file
v2.2.0,construct feature bin mappers
v2.2.0,initialize label
v2.2.0,extract features
v2.2.0,load data from binary file
v2.2.0,check meta data
v2.2.0,need to check training data
v2.2.0,read data in memory
v2.2.0,initialize label
v2.2.0,extract features
v2.2.0,Get number of lines of data file
v2.2.0,initialize label
v2.2.0,extract features
v2.2.0,load data from binary file
v2.2.0,not need to check validation data
v2.2.0,check meta data
v2.2.0,buffer to read binary file
v2.2.0,check token
v2.2.0,read size of header
v2.2.0,re-allocmate space if not enough
v2.2.0,read header
v2.2.0,get header
v2.2.0,num_groups
v2.2.0,real_feature_idx_
v2.2.0,feature2group
v2.2.0,feature2subfeature
v2.2.0,group_bin_boundaries
v2.2.0,group_feature_start_
v2.2.0,group_feature_cnt_
v2.2.0,get feature names
v2.2.0,write feature names
v2.2.0,read size of meta data
v2.2.0,re-allocate space if not enough
v2.2.0,read meta data
v2.2.0,load meta data
v2.2.0,sample local used data if need to partition
v2.2.0,"if not contain query file, minimal sample unit is one record"
v2.2.0,"if contain query file, minimal sample unit is one query"
v2.2.0,if is new query
v2.2.0,read feature data
v2.2.0,read feature size
v2.2.0,re-allocate space if not enough
v2.2.0,fill feature_names_ if not header
v2.2.0,"if only one machine, find bin locally"
v2.2.0,"if have multi-machines, need to find bin distributed"
v2.2.0,different machines will find bin for different features
v2.2.0,start and len will store the process feature indices for different machines
v2.2.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.0,get size of bin mapper with max_bin size
v2.2.0,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.0,find local feature bins and copy to buffer
v2.2.0,free
v2.2.0,convert to binary size
v2.2.0,gather global feature bin mappers
v2.2.0,restore features bins from buffer
v2.2.0,---- private functions ----
v2.2.0,"if features are ordered, not need to use hist_buf"
v2.2.0,read all lines
v2.2.0,get query data
v2.2.0,"if not contain query data, minimal sample unit is one record"
v2.2.0,"if contain query data, minimal sample unit is one query"
v2.2.0,if is new query
v2.2.0,get query data
v2.2.0,"if not contain query file, minimal sample unit is one record"
v2.2.0,"if contain query file, minimal sample unit is one query"
v2.2.0,if is new query
v2.2.0,parse features
v2.2.0,-1 means doesn't use this feature
v2.2.0,"check the range of label_idx, weight_idx and group_idx"
v2.2.0,fill feature_names_ if not header
v2.2.0,start find bins
v2.2.0,"if only one machine, find bin locally"
v2.2.0,"if have multi-machines, need to find bin distributed"
v2.2.0,different machines will find bin for different features
v2.2.0,start and len will store the process feature indices for different machines
v2.2.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.2.0,get size of bin mapper with max_bin size
v2.2.0,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.2.0,find local feature bins and copy to buffer
v2.2.0,free
v2.2.0,convert to binary size
v2.2.0,gather global feature bin mappers
v2.2.0,restore features bins from buffer
v2.2.0,if doesn't need to prediction with initial model
v2.2.0,parser
v2.2.0,set label
v2.2.0,free processed line:
v2.2.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.0,text_reader_->Lines()[i].shrink_to_fit();
v2.2.0,push data
v2.2.0,if is used feature
v2.2.0,if need to prediction with initial model
v2.2.0,parser
v2.2.0,set initial score
v2.2.0,set label
v2.2.0,free processed line:
v2.2.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.2.0,text_reader_->Lines()[i].shrink_to_fit();
v2.2.0,push data
v2.2.0,if is used feature
v2.2.0,metadata_ will manage space of init_score
v2.2.0,text data can be free after loaded feature values
v2.2.0,parser
v2.2.0,set initial score
v2.2.0,set label
v2.2.0,push data
v2.2.0,if is used feature
v2.2.0,only need part of data
v2.2.0,need full data
v2.2.0,metadata_ will manage space of init_score
v2.2.0,read size of token
v2.2.0,deep copy function for BinMapper
v2.2.0,mean size for one bin
v2.2.0,need a new bin
v2.2.0,update bin upper bound
v2.2.0,last bin upper bound
v2.2.0,find distinct_values first
v2.2.0,push zero in the front
v2.2.0,use the large value
v2.2.0,push zero in the back
v2.2.0,convert to int type first
v2.2.0,sort by counts
v2.2.0,avoid first bin is zero
v2.2.0,will ignore the categorical of small counts
v2.2.0,need an additional bin for NaN
v2.2.0,use -1 to represent NaN
v2.2.0,Use MissingType::None to represent this bin contains all categoricals
v2.2.0,check trival(num_bin_ == 1) feature
v2.2.0,check useless bin
v2.2.0,calculate sparse rate
v2.2.0,sparse threshold
v2.2.0,"for lambdarank, it needs query data for partition data in parallel learning"
v2.2.0,need convert query_id to boundaries
v2.2.0,check weights
v2.2.0,check query boundries
v2.2.0,contain initial score file
v2.2.0,check weights
v2.2.0,get local weights
v2.2.0,check query boundries
v2.2.0,get local query boundaries
v2.2.0,contain initial score file
v2.2.0,get local initial scores
v2.2.0,re-load query weight
v2.2.0,save to nullptr
v2.2.0,save to nullptr
v2.2.0,save to nullptr
v2.2.0,default weight file name
v2.2.0,default weight file name
v2.2.0,use first line to count number class
v2.2.0,default query file name
v2.2.0,/ This file is auto generated by LightGBM\helper\parameter_generator.py from LightGBM\include\LightGBM\config.h file.
v2.2.0,root is in the depth 0
v2.2.0,non-leaf
v2.2.0,leaf
v2.2.0,use this for the missing value conversion
v2.2.0,Predict func by Map to ifelse
v2.2.0,use this for the missing value conversion
v2.2.0,non-leaf
v2.2.0,left subtree
v2.2.0,right subtree
v2.2.0,leaf
v2.2.0,non-leaf
v2.2.0,left subtree
v2.2.0,right subtree
v2.2.0,leaf
v2.2.0,recursive computation of SHAP values for a decision tree
v2.2.0,extend the unique path
v2.2.0,leaf node
v2.2.0,internal node
v2.2.0,"see if we have already split on this feature,"
v2.2.0,if so we undo that split so we can redo it for this node
v2.2.0,clear old metrics
v2.2.0,to lower
v2.2.0,split
v2.2.0,remove duplicate
v2.2.0,add names of objective function if not providing metric
v2.2.0,generate seeds by seed.
v2.2.0,check for conflicts
v2.2.0,"check if objective, metric, and num_class match"
v2.2.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.2.0,Check max_depth and num_leaves
v2.2.0,"filter is based on sampling data, so decrease its range"
v2.2.0,put dense feature first
v2.2.0,sort by non zero cnt
v2.2.0,"sort by non zero cnt, bigger first"
v2.2.0,"take apart small sparse group, due it will not gain on speed"
v2.2.0,shuffle groups
v2.2.0,get num_features
v2.2.0,get bin_mappers
v2.2.0,copy feature bin mapper data
v2.2.0,copy feature bin mapper data
v2.2.0,"if not pass a filename, just append "".bin"" of original file"
v2.2.0,get size of header
v2.2.0,size of feature names
v2.2.0,write header
v2.2.0,write feature names
v2.2.0,get size of meta data
v2.2.0,write meta data
v2.2.0,write feature data
v2.2.0,get size of feature
v2.2.0,write feature
v2.2.0,feature is not used
v2.2.0,construct histograms for smaller leaf
v2.2.0,if not use ordered bin
v2.2.0,used ordered bin
v2.2.0,feature is not used
v2.2.0,construct histograms for smaller leaf
v2.2.0,if not use ordered bin
v2.2.0,used ordered bin
v2.2.0,fixed hessian.
v2.2.0,feature is not used
v2.2.0,construct histograms for smaller leaf
v2.2.0,if not use ordered bin
v2.2.0,used ordered bin
v2.2.0,feature is not used
v2.2.0,construct histograms for smaller leaf
v2.2.0,if not use ordered bin
v2.2.0,used ordered bin
v2.2.0,fixed hessian.
v2.2.0,PredictRaw
v2.2.0,PredictRawByMap
v2.2.0,Predict
v2.2.0,PredictByMap
v2.2.0,PredictLeafIndex
v2.2.0,PredictLeafIndexByMap
v2.2.0,output model type
v2.2.0,output number of class
v2.2.0,output label index
v2.2.0,output max_feature_idx
v2.2.0,output objective
v2.2.0,output tree models
v2.2.0,store the importance first
v2.2.0,sort the importance
v2.2.0,use serialized string to restore this object
v2.2.0,Use first 128 chars to avoid exceed the message buffer.
v2.2.0,get number of classes
v2.2.0,get index of label
v2.2.0,get max_feature_idx first
v2.2.0,get average_output
v2.2.0,get feature names
v2.2.0,set zero
v2.2.0,predict all the trees for one iteration
v2.2.0,check early stopping
v2.2.0,set zero
v2.2.0,predict all the trees for one iteration
v2.2.0,check early stopping
v2.2.0,margin_threshold will be captured by value
v2.2.0,copy and sort
v2.2.0,margin_threshold will be captured by value
v2.2.0,load forced_splits file
v2.2.0,init tree learner
v2.2.0,push training metrics
v2.2.0,create buffer for gradients and hessians
v2.2.0,get max feature index
v2.2.0,get label index
v2.2.0,get feature names
v2.2.0,"if need bagging, create buffer"
v2.2.0,reset config for tree learner
v2.2.0,multi-class
v2.2.0,binary class
v2.2.0,"for a validation dataset, we need its score and metric"
v2.2.0,update score
v2.2.0,objective function will calculate gradients and hessians
v2.2.0,"random bagging, minimal unit is one record"
v2.2.0,if need bagging
v2.2.0,set bagging data to tree learner
v2.2.0,get subset
v2.2.0,output used time per iteration
v2.2.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.2.0,boosting first
v2.2.0,bagging logic
v2.2.0,need to copy gradients for bagging subset.
v2.2.0,shrinkage by learning rate
v2.2.0,update score
v2.2.0,only add default score one-time
v2.2.0,updates scores
v2.2.0,add model
v2.2.0,reset score
v2.2.0,remove model
v2.2.0,print message for metric
v2.2.0,pop last early_stopping_round_ models
v2.2.0,update training score
v2.2.0,we need to predict out-of-bag scores of data for boosting
v2.2.0,update validation score
v2.2.0,print training metric
v2.2.0,print validation metric
v2.2.0,set zero
v2.2.0,predict all the trees for one iteration
v2.2.0,check early stopping
v2.2.0,push training metrics
v2.2.0,"not same training data, need reset score and others"
v2.2.0,create score tracker
v2.2.0,update score
v2.2.0,create buffer for gradients and hessians
v2.2.0,"if need bagging, create buffer"
v2.2.0,Get the max size of pool
v2.2.0,at least need 2 leaves
v2.2.0,push split information for all leaves
v2.2.0,get ordered bin
v2.2.0,check existing for ordered bin
v2.2.0,initialize splits for leaf
v2.2.0,initialize data partition
v2.2.0,initialize ordered gradients and hessians
v2.2.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.0,get ordered bin
v2.2.0,initialize splits for leaf
v2.2.0,initialize data partition
v2.2.0,initialize ordered gradients and hessians
v2.2.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.2.0,Get the max size of pool
v2.2.0,at least need 2 leaves
v2.2.0,push split information for all leaves
v2.2.0,some initial works before training
v2.2.0,root leaf
v2.2.0,only root leaf can be splitted on first time
v2.2.0,some initial works before finding best split
v2.2.0,find best threshold for every feature
v2.2.0,Get a leaf with max split gain
v2.2.0,Get split information for best leaf
v2.2.0,"cannot split, quit"
v2.2.0,split tree with best leaf
v2.2.0,reset histogram pool
v2.2.0,at least use one feature
v2.2.0,initialize used features
v2.2.0,Get used feature at current tree
v2.2.0,initialize data partition
v2.2.0,reset the splits for leaves
v2.2.0,Sumup for root
v2.2.0,use all data
v2.2.0,"use bagging, only use part of data"
v2.2.0,"if has ordered bin, need to initialize the ordered bin"
v2.2.0,"use all data, pass nullptr"
v2.2.0,"bagging, only use part of data"
v2.2.0,mark used data
v2.2.0,initialize ordered bin
v2.2.0,check depth of current leaf
v2.2.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.2.0,no enough data to continue
v2.2.0,only have root
v2.2.0,put parent(left) leaf's histograms into larger leaf's histograms
v2.2.0,put parent(left) leaf's histograms to larger leaf's histograms
v2.2.0,split for the ordered bin
v2.2.0,mark data that at left-leaf
v2.2.0,split the ordered bin
v2.2.0,construct smaller leaf
v2.2.0,construct larger leaf
v2.2.0,find splits
v2.2.0,only has root leaf
v2.2.0,find best threshold for larger child
v2.2.0,start at root leaf
v2.2.0,"before processing next node from queue, store info for current left/right leaf"
v2.2.0,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.2.0,"then, compute own splits"
v2.2.0,split info should exist because searching in bfs fashion - should have added from parent
v2.2.0,"split tree, will return right leaf"
v2.2.0,left = parent
v2.2.0,"split tree, will return right leaf"
v2.2.0,init the leaves that used on next iteration
v2.2.0,bag_mapper[index_mapper[i]]
v2.2.0,get feature partition
v2.2.0,get local used features
v2.2.0,get best split at smaller leaf
v2.2.0,find local best split for larger leaf
v2.2.0,sync global best info
v2.2.0,update best split
v2.2.0,"instantiate template classes, otherwise linker cannot find the code"
v2.2.0,initialize SerialTreeLearner
v2.2.0,Get local rank and global machine size
v2.2.0,allocate buffer for communication
v2.2.0,generate feature partition for current tree
v2.2.0,get local used feature
v2.2.0,get block start and block len for reduce scatter
v2.2.0,get buffer_write_start_pos_
v2.2.0,get buffer_read_start_pos_
v2.2.0,sync global data sumup info
v2.2.0,global sumup reduce
v2.2.0,copy back
v2.2.0,set global sumup info
v2.2.0,init global data count in leaf
v2.2.0,construct local histograms
v2.2.0,copy to buffer
v2.2.0,Reduce scatter for histogram
v2.2.0,restore global histograms from buffer
v2.2.0,find best threshold for smaller child
v2.2.0,only root leaf
v2.2.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.2.0,find best threshold for larger child
v2.2.0,find local best split for larger leaf
v2.2.0,sync global best info
v2.2.0,set best split
v2.2.0,need update global number of data in leaf
v2.2.0,"instantiate template classes, otherwise linker cannot find the code"
v2.2.0,initialize SerialTreeLearner
v2.2.0,some additional variables needed for GPU trainer
v2.2.0,Initialize GPU buffers and kernels
v2.2.0,some functions used for debugging the GPU histogram construction
v2.2.0,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.2.0,goto err;
v2.2.0,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.2.0,goto err;
v2.2.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.2.0,also guarantee that there are at least 2K examples per workgroup
v2.2.0,return 0;
v2.2.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.2.0,decide the best number of workgroups working on one feature4 tuple
v2.2.0,set work group size based on feature size
v2.2.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.2.0,we need to refresh the kernel arguments after reallocating
v2.2.0,The only argument that needs to be changed later is num_data_
v2.2.0,"the GPU kernel will process all features in one call, and each"
v2.2.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.2.0,process one feature4 tuple
v2.2.0,"for the root node, indices are not copied"
v2.2.0,"for constant hessian, hessians are not copied except for the root node"
v2.2.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.2.0,and we will launch num_feature workgroups for this kernel
v2.2.0,will launch threads for all features
v2.2.0,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.2.0,copy the results asynchronously. Size depends on if double precision is used
v2.2.0,we will wait for this object in WaitAndGetHistograms
v2.2.0,"when the output is ready, the computation is done"
v2.2.0,values of this feature has been redistributed to multiple bins; need a reduction here
v2.2.0,how many feature-group tuples we have
v2.2.0,leave some safe margin for prefetching
v2.2.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.2.0,clear sparse/dense maps
v2.2.0,do nothing if no features can be processed on GPU
v2.2.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.2.0,unpin old buffer if necessary before destructing them
v2.2.0,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.2.0,allocate space for gradients and hessians on device
v2.2.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.2.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.2.0,copy indices to the device
v2.2.0,histogram bin entry size depends on the precision (single/double)
v2.2.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.2.0,each work group generates a sub-histogram of dword_features_ features.
v2.2.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.2.0,create atomic counters for inter-group coordination
v2.2.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.2.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.2.0,looking for dword_features_ non-sparse feature-groups
v2.2.0,decide if we need to redistribute the bin
v2.2.0,multiplier must be a power of 2
v2.2.0,device_bin_mults_.push_back(1);
v2.2.0,found
v2.2.0,for data transfer time
v2.2.0,"Now generate new data structure feature4, and copy data to the device"
v2.2.0,"preallocate arrays for all threads, and pin them"
v2.2.0,building Feature4 bundles; each thread handles dword_features_ features
v2.2.0,one feature datapoint is 4 bits
v2.2.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.0,one feature datapoint is one byte
v2.2.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.2.0,Dense bin
v2.2.0,Dense 4-bit bin
v2.2.0,working on the remaining (less than dword_features_) feature groups
v2.2.0,fill the leftover features
v2.2.0,"fill this empty feature with some ""random"" value"
v2.2.0,"fill this empty feature with some ""random"" value"
v2.2.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.2.0,deallocate pinned space for feature copying
v2.2.0,data transfer time
v2.2.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.2.0,"Something bad happened. Just return ""No log available."""
v2.2.0,"build is okay, log may contain warnings"
v2.2.0,destroy any old kernels
v2.2.0,create OpenCL kernels for different number of workgroups per feature
v2.2.0,currently we don't use constant memory
v2.2.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.2.0,kernel with indices in an array
v2.2.0,"kernel with all features enabled, with elimited branches"
v2.2.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.2.0,do nothing if no features can be processed on GPU
v2.2.0,The only argument that needs to be changed later is num_data_
v2.2.0,"hessian is passed as a parameter, but it is not available now."
v2.2.0,hessian will be set in BeforeTrain()
v2.2.0,"Get the max bin size, used for selecting best GPU kernel"
v2.2.0,initialize GPU
v2.2.0,determine which kernel to use based on the max number of bins
v2.2.0,setup GPU kernel arguments after we allocating all the buffers
v2.2.0,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.2.0,this should rarely occur
v2.2.0,GPU memory has to been reallocated because data may have been changed
v2.2.0,setup GPU kernel arguments after we allocating all the buffers
v2.2.0,Copy initial full hessians and gradients to GPU.
v2.2.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.2.0,setup hessian parameters only
v2.2.0,hessian is passed as a parameter
v2.2.0,use bagging
v2.2.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.2.0,copy used gradients and hessians to ordered buffer
v2.2.0,transfer the indices to GPU
v2.2.0,transfer hessian to GPU
v2.2.0,setup hessian parameters only
v2.2.0,hessian is passed as a parameter
v2.2.0,transfer gradients to GPU
v2.2.0,only have root
v2.2.0,"Copy indices, gradients and hessians as early as possible"
v2.2.0,only need to initialize for smaller leaf
v2.2.0,Get leaf boundary
v2.2.0,copy indices to the GPU:
v2.2.0,copy ordered hessians to the GPU:
v2.2.0,copy ordered gradients to the GPU:
v2.2.0,do nothing if no features can be processed on GPU
v2.2.0,copy data indices if it is not null
v2.2.0,generate and copy ordered_gradients if gradients is not null
v2.2.0,generate and copy ordered_hessians if hessians is not null
v2.2.0,converted indices in is_feature_used to feature-group indices
v2.2.0,construct the feature masks for dense feature-groups
v2.2.0,"if no feature group is used, just return and do not use GPU"
v2.2.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.2.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.2.0,"All data have been prepared, now run the GPU kernel"
v2.2.0,construct smaller leaf
v2.2.0,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.2.0,then construct sparse features on CPU
v2.2.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.0,"wait for GPU to finish, only if GPU is actually used"
v2.2.0,use double precision
v2.2.0,use single precision
v2.2.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.2.0,#define GPU_DEBUG_COMPARE
v2.2.0,construct larger leaf
v2.2.0,then construct sparse features on CPU
v2.2.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.2.0,"wait for GPU to finish, only if GPU is actually used"
v2.2.0,use double precision
v2.2.0,use single precision
v2.2.0,do some sanity check for the GPU algorithm
v2.2.0,limit top k
v2.2.0,get max bin
v2.2.0,calculate buffer size
v2.2.0,"left and right on same time, so need double size"
v2.2.0,initialize histograms for global
v2.2.0,sync global data sumup info
v2.2.0,set global sumup info
v2.2.0,init global data count in leaf
v2.2.0,get local sumup
v2.2.0,get local sumup
v2.2.0,get mean number on machines
v2.2.0,weighted gain
v2.2.0,get top k
v2.2.0,"Copy histogram to buffer, and Get local aggregate features"
v2.2.0,copy histograms.
v2.2.0,copy smaller leaf histograms first
v2.2.0,mark local aggregated feature
v2.2.0,copy
v2.2.0,then copy larger leaf histograms
v2.2.0,mark local aggregated feature
v2.2.0,copy
v2.2.0,use local data to find local best splits
v2.2.0,find splits
v2.2.0,only has root leaf
v2.2.0,find best threshold for larger child
v2.2.0,local voting
v2.2.0,gather
v2.2.0,get all top-k from all machines
v2.2.0,global voting
v2.2.0,copy local histgrams to buffer
v2.2.0,Reduce scatter for histogram
v2.2.0,find best split from local aggregated histograms
v2.2.0,restore from buffer
v2.2.0,find best threshold
v2.2.0,restore from buffer
v2.2.0,find best threshold
v2.2.0,find local best
v2.2.0,find local best split for larger leaf
v2.2.0,sync global best info
v2.2.0,copy back
v2.2.0,set the global number of data for leaves
v2.2.0,init the global sumup info
v2.2.0,"instantiate template classes, otherwise linker cannot find the code"
v2.2.0,coding: utf-8
v2.2.0,alias table
v2.2.0,names
v2.2.0,from strings
v2.2.0,tails
v2.2.0,tails
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable=invalid-name, exec-used, C0111"
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, W0105"
v2.1.2,create predictor first
v2.1.2,check dataset
v2.1.2,reduce cost for prediction training data
v2.1.2,process callbacks
v2.1.2,Most of legacy advanced options becomes callbacks
v2.1.2,construct booster
v2.1.2,start training
v2.1.2,check evaluation result.
v2.1.2,"lambdarank task, split according to groups"
v2.1.2,run preprocessing on the data set if needed
v2.1.2,setup callbacks
v2.1.2,coding: utf-8
v2.1.2,pylint: disable = C0103
v2.1.2,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.1.2,because of u'...' Unicode literals.
v2.1.2,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.1.2,minor change to support `**kwargs`
v2.1.2,sklearn interface has another naming convention
v2.1.2,"user can set verbose with kwargs, it has higher priority"
v2.1.2,reduce cost for prediction training data
v2.1.2,free dataset
v2.1.2,Switch to using a multiclass objective in the underlying LGBM instance
v2.1.2,check group data
v2.1.2,coding: utf-8
v2.1.2,we don't need lib_lightgbm while building docs
v2.1.2,coding: utf-8
v2.1.2,pylint: disable = C0103
v2.1.2,coding: utf-8
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, C0111, C0301"
v2.1.2,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.1.2,TypeError: obj is not a string or a number
v2.1.2,ValueError: invalid literal
v2.1.2,process for args
v2.1.2,get categorical features
v2.1.2,process for reference dataset
v2.1.2,start construct data
v2.1.2,check data has header or not
v2.1.2,load init score
v2.1.2,need re group init score
v2.1.2,set feature names
v2.1.2,"change non-float data to float data, need to copy"
v2.1.2,create valid
v2.1.2,construct subset
v2.1.2,create train
v2.1.2,set to None
v2.1.2,we're done if self and reference share a common upstrem reference
v2.1.2,"group data from LightGBM is boundaries data, need to convert to group size"
v2.1.2,Training task
v2.1.2,construct booster object
v2.1.2,save reference to data
v2.1.2,buffer for inner predict
v2.1.2,set network if necessary
v2.1.2,Prediction task
v2.1.2,need reset training data
v2.1.2,need to push new valid data
v2.1.2,Get name of features
v2.1.2,avoid to predict many time in one iteration
v2.1.2,Get num of inner evals
v2.1.2,Get name of evals
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, W0105, C0301"
v2.1.2,Callback environment used by callbacks
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, C0111"
v2.1.2,load or create your dataset
v2.1.2,create dataset for lightgbm
v2.1.2,"if you want to re-use data, remember to set free_raw_data=False"
v2.1.2,specify your configurations as a dict
v2.1.2,generate a feature name
v2.1.2,feature_name and categorical_feature
v2.1.2,check feature name
v2.1.2,save model to file
v2.1.2,dump model to json (and save to file)
v2.1.2,feature names
v2.1.2,feature importances
v2.1.2,load model to predict
v2.1.2,can only predict with the best iteration (or the saving iteration)
v2.1.2,eval with loaded model
v2.1.2,dump model with pickle
v2.1.2,load model with pickle to predict
v2.1.2,can predict with any iteration when loaded in pickle way
v2.1.2,eval with loaded model
v2.1.2,continue training
v2.1.2,init_model accepts:
v2.1.2,1. model file name
v2.1.2,2. Booster()
v2.1.2,decay learning rates
v2.1.2,learning_rates accepts:
v2.1.2,1. list/tuple with length = num_boost_round
v2.1.2,2. function(curr_iter)
v2.1.2,change other parameters during training
v2.1.2,self-defined objective function
v2.1.2,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.1.2,log likelihood loss
v2.1.2,self-defined eval metric
v2.1.2,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.1.2,binary error
v2.1.2,callback
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, C0111"
v2.1.2,load or create your dataset
v2.1.2,train
v2.1.2,predict
v2.1.2,eval
v2.1.2,feature importances
v2.1.2,other scikit-learn modules
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, C0111"
v2.1.2,load or create your dataset
v2.1.2,create dataset for lightgbm
v2.1.2,specify your configurations as a dict
v2.1.2,train
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, C0111"
v2.1.2,################
v2.1.2,Simulate some binary data with a single categorical and
v2.1.2,single continuous predictor
v2.1.2,################
v2.1.2,Set up a couple of utilities for our experiments
v2.1.2,################
v2.1.2,Observe the behavior of `binary` and `xentropy` objectives
v2.1.2,Trying this throws an error on non-binary values of y:
v2.1.2,"experiment('binary', label_type='probability', DATA)"
v2.1.2,The speed of `binary` is not drastically different than
v2.1.2,"`xentropy`. `xentropy` runs faster than `binary` in many cases, although"
v2.1.2,there are reasons to suspect that `binary` should run faster when the
v2.1.2,label is an integer instead of a float
v2.1.2,coding: utf-8
v2.1.2,"pylint: disable = invalid-name, C0111"
v2.1.2,load or create your dataset
v2.1.2,create dataset for lightgbm
v2.1.2,specify your configurations as a dict
v2.1.2,train
v2.1.2,save model to file
v2.1.2,predict
v2.1.2,eval
v2.1.2,!/usr/bin/env python3
v2.1.2,-*- coding: utf-8 -*-
v2.1.2,
v2.1.2,"LightGBM documentation build configuration file, created by"
v2.1.2,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.1.2,
v2.1.2,This file is execfile()d with the current directory set to its
v2.1.2,containing dir.
v2.1.2,
v2.1.2,Note that not all possible configuration values are present in this
v2.1.2,autogenerated file.
v2.1.2,
v2.1.2,All configuration values have a default; values that are commented out
v2.1.2,serve to show the default.
v2.1.2,"If extensions (or modules to document with autodoc) are in another directory,"
v2.1.2,add these directories to sys.path here. If the directory is relative to the
v2.1.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.1.2,
v2.1.2,-- mock out modules
v2.1.2,-- General configuration ------------------------------------------------
v2.1.2,"If your documentation needs a minimal Sphinx version, state it here."
v2.1.2,"Add any Sphinx extension module names here, as strings. They can be"
v2.1.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.1.2,ones.
v2.1.2,"Add any paths that contain templates here, relative to this directory."
v2.1.2,The suffix(es) of source filenames.
v2.1.2,You can specify multiple suffix as a list of string:
v2.1.2,"source_suffix = ['.rst', '.md']"
v2.1.2,The master toctree document.
v2.1.2,General information about the project.
v2.1.2,"The version info for the project you're documenting, acts as replacement for"
v2.1.2,"|version| and |release|, also used in various other places throughout the"
v2.1.2,built documents.
v2.1.2,
v2.1.2,The short X.Y version.
v2.1.2,"The full version, including alpha/beta/rc tags."
v2.1.2,The language for content autogenerated by Sphinx. Refer to documentation
v2.1.2,for a list of supported languages.
v2.1.2,
v2.1.2,This is also used if you do content translation via gettext catalogs.
v2.1.2,"Usually you set ""language"" from the command line for these cases."
v2.1.2,"List of patterns, relative to source directory, that match files and"
v2.1.2,directories to ignore when looking for source files.
v2.1.2,This patterns also effect to html_static_path and html_extra_path
v2.1.2,The name of the Pygments (syntax highlighting) style to use.
v2.1.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.1.2,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.1.2,-- Options for HTML output ----------------------------------------------
v2.1.2,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.1.2,a list of builtin themes.
v2.1.2,
v2.1.2,Theme options are theme-specific and customize the look and feel of a theme
v2.1.2,"further.  For a list of options available for each theme, see the"
v2.1.2,documentation.
v2.1.2,
v2.1.2,"Add any paths that contain custom static files (such as style sheets) here,"
v2.1.2,"relative to this directory. They are copied after the builtin static files,"
v2.1.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.1.2,-- Options for HTMLHelp output ------------------------------------------
v2.1.2,Output file base name for HTML help builder.
v2.1.2,coding: utf-8
v2.1.2,coding: utf-8
v2.1.2,pylint: skip-file
v2.1.2,we don't need lib_lightgbm while building docs
v2.1.2,coding: utf-8
v2.1.2,pylint: skip-file
v2.1.2,check saved model persistence
v2.1.2,"we need to check the consistency of model file here, so test for exact equal"
v2.1.2,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.1.2,"scores likely to be different, but prediction should still be the same"
v2.1.2,coding: utf-8
v2.1.2,pylint: skip-file
v2.1.2,Tests that `seed` is the same as `random_state`
v2.1.2,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.1.2,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.1.2,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.1.2,Tests same probabilities
v2.1.2,Tests same predictions
v2.1.2,Tests same raw scores
v2.1.2,Tests same leaf indices
v2.1.2,Tests same feature contributions
v2.1.2,Tests other parameters for the prediction works
v2.1.2,coding: utf-8
v2.1.2,pylint: skip-file
v2.1.2,coding: utf-8
v2.1.2,pylint: skip-file
v2.1.2,no early stopping
v2.1.2,early stopping occurs
v2.1.2,test custom eval metrics
v2.1.2,"shuffle = False, override metric in params"
v2.1.2,"shuffle = True, callbacks"
v2.1.2,self defined folds
v2.1.2,lambdarank
v2.1.2,test feature_names with whitespaces
v2.1.2,take subsets and train
v2.1.2,test sliced labels
v2.1.2,append some columns
v2.1.2,append some rows
v2.1.2,test sliced 2d matrix
v2.1.2,test sliced CSR
v2.1.2,coding: utf-8
v2.1.2,pylint: skip-file
v2.1.2,convert from one-based to  zero-based index
v2.1.2,convert from boundaries to size
v2.1.2,--- start Booster interfaces
v2.1.2,create boosting
v2.1.2,initialize the boosting
v2.1.2,create objective function
v2.1.2,initialize the objective function
v2.1.2,create training metric
v2.1.2,reset the boosting
v2.1.2,create objective function
v2.1.2,initialize the objective function
v2.1.2,some help functions used to convert data
v2.1.2,Row iterator of on column for CSC matrix
v2.1.2,"return value at idx, only can access by ascent order"
v2.1.2,"return next non-zero pair, if index < 0, means no more data"
v2.1.2,start of c_api functions
v2.1.2,sample data first
v2.1.2,sample data first
v2.1.2,sample data first
v2.1.2,no more data
v2.1.2,---- start of booster
v2.1.2,---- start of some help functions
v2.1.2,set number of threads for openmp
v2.1.2,check for alias
v2.1.2,read parameters from config file
v2.1.2,"remove str after ""#"""
v2.1.2,check for alias again
v2.1.2,load configs
v2.1.2,prediction is needed if using input initial model(continued train)
v2.1.2,need to continue training
v2.1.2,sync up random seed for data partition
v2.1.2,load Training data
v2.1.2,load data for parallel training
v2.1.2,load data for single machine
v2.1.2,need save binary file
v2.1.2,create training metric
v2.1.2,only when have metrics then need to construct validation data
v2.1.2,"Add validation data, if it exists"
v2.1.2,add
v2.1.2,need save binary file
v2.1.2,add metric for validation data
v2.1.2,output used time on each iteration
v2.1.2,need init network
v2.1.2,create boosting
v2.1.2,create objective function
v2.1.2,load training data
v2.1.2,initialize the objective function
v2.1.2,initialize the boosting
v2.1.2,add validation data into boosting
v2.1.2,convert model to if-else statement code
v2.1.2,create predictor
v2.1.2,Free memory
v2.1.2,create predictor
v2.1.2,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.1.2,counts for all labels
v2.1.2,"start from top label, and accumulate DCG"
v2.1.2,counts for all labels
v2.1.2,calculate k Max DCG by one pass
v2.1.2,get sorted indices by score
v2.1.2,calculate dcg
v2.1.2,get sorted indices by score
v2.1.2,calculate multi dcg by one pass
v2.1.2,wait for all client start up
v2.1.2,default set to -1
v2.1.2,"distance at k-th communication, distance[k] = 2^k"
v2.1.2,set incoming rank at k-th commuication
v2.1.2,set outgoing rank at k-th commuication
v2.1.2,defalut set as -1
v2.1.2,construct all recursive halving map for all machines
v2.1.2,let 1 << k <= num_machines
v2.1.2,distance of each communication
v2.1.2,"if num_machines = 2^k, don't need to group machines"
v2.1.2,"communication direction, %2 == 0 is positive"
v2.1.2,neighbor at k-th communication
v2.1.2,receive data block at k-th communication
v2.1.2,send data block at k-th communication
v2.1.2,"if num_machines != 2^k, need to group machines"
v2.1.2,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.1.2,let left machine as group leader
v2.1.2,"cache block information for groups, group with 2 machines will have double block size"
v2.1.2,convert from group to node leader
v2.1.2,convert from node to group
v2.1.2,meet new group
v2.1.2,add block len for this group
v2.1.2,calculate the group block start
v2.1.2,not need to construct
v2.1.2,get receive block informations
v2.1.2,accumulate block len
v2.1.2,get send block informations
v2.1.2,accumulate block len
v2.1.2,static member definition
v2.1.2,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.1.2,assign the blocks to every rank.
v2.1.2,do reduce scatter
v2.1.2,do all gather
v2.1.2,assign blocks
v2.1.2,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.1.2,copy back
v2.1.2,assign blocks
v2.1.2,start all gather
v2.1.2,when num_machines is small and data is large
v2.1.2,use output as receive buffer
v2.1.2,get current local block size
v2.1.2,get out rank
v2.1.2,get in rank
v2.1.2,get send information
v2.1.2,get recv information
v2.1.2,send and recv at same time
v2.1.2,rotate in-place
v2.1.2,use output as receive buffer
v2.1.2,get current local block size
v2.1.2,get send information
v2.1.2,get recv information
v2.1.2,send and recv at same time
v2.1.2,use output as receive buffer
v2.1.2,send and recv at same time
v2.1.2,send local data to neighbor first
v2.1.2,receive neighbor data first
v2.1.2,reduce
v2.1.2,get target
v2.1.2,get send information
v2.1.2,get recv information
v2.1.2,send and recv at same time
v2.1.2,reduce
v2.1.2,send result to neighbor
v2.1.2,receive result from neighbor
v2.1.2,copy result
v2.1.2,start up socket
v2.1.2,parse clients from file
v2.1.2,get ip list of local machine
v2.1.2,get local rank
v2.1.2,construct listener
v2.1.2,construct communication topo
v2.1.2,construct linkers
v2.1.2,free listener
v2.1.2,set timeout
v2.1.2,accept incoming socket
v2.1.2,receive rank
v2.1.2,add new socket
v2.1.2,save ranks that need to connect with
v2.1.2,start listener
v2.1.2,start connect
v2.1.2,let smaller rank connect to larger rank
v2.1.2,send local rank
v2.1.2,wait for listener
v2.1.2,print connected linkers
v2.1.2,Get some statistic from 2 line
v2.1.2,if only have one line on file
v2.1.2,Constructors
v2.1.2,Get type tag
v2.1.2,Comparisons
v2.1.2,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.1.2,"advance until next line, or end of input"
v2.1.2,advance until closing tokens
v2.1.2,The usual case: non-escaped characters
v2.1.2,Handle escapes
v2.1.2,Extract 4-byte escape sequence
v2.1.2,Explicitly check length of the substring. The following loop
v2.1.2,relies on std::string returning the terminating NUL when
v2.1.2,accessing str[length]. Checking here reduces brittleness.
v2.1.2,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.1.2,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.1.2,whether we're in the middle of such a beast: the previous codepoint was an
v2.1.2,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.1.2,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.1.2,the UTF-16 algorithm.
v2.1.2,Integer part
v2.1.2,Decimal part
v2.1.2,Exponent part
v2.1.2,Check for any trailing garbage
v2.1.2,Documented in json11.hpp
v2.1.2,Check for another object
v2.1.2,get column names
v2.1.2,load label idx first
v2.1.2,erase label column name
v2.1.2,load ignore columns
v2.1.2,load weight idx
v2.1.2,load group idx
v2.1.2,don't support query id in data file when training in parallel
v2.1.2,read data to memory
v2.1.2,sample data
v2.1.2,construct feature bin mappers
v2.1.2,initialize label
v2.1.2,extract features
v2.1.2,sample data from file
v2.1.2,construct feature bin mappers
v2.1.2,initialize label
v2.1.2,extract features
v2.1.2,load data from binary file
v2.1.2,check meta data
v2.1.2,need to check training data
v2.1.2,read data in memory
v2.1.2,initialize label
v2.1.2,extract features
v2.1.2,Get number of lines of data file
v2.1.2,initialize label
v2.1.2,extract features
v2.1.2,load data from binary file
v2.1.2,not need to check validation data
v2.1.2,check meta data
v2.1.2,buffer to read binary file
v2.1.2,check token
v2.1.2,read size of header
v2.1.2,re-allocmate space if not enough
v2.1.2,read header
v2.1.2,get header
v2.1.2,num_groups
v2.1.2,real_feature_idx_
v2.1.2,feature2group
v2.1.2,feature2subfeature
v2.1.2,group_bin_boundaries
v2.1.2,group_feature_start_
v2.1.2,group_feature_cnt_
v2.1.2,get feature names
v2.1.2,write feature names
v2.1.2,read size of meta data
v2.1.2,re-allocate space if not enough
v2.1.2,read meta data
v2.1.2,load meta data
v2.1.2,sample local used data if need to partition
v2.1.2,"if not contain query file, minimal sample unit is one record"
v2.1.2,"if contain query file, minimal sample unit is one query"
v2.1.2,if is new query
v2.1.2,read feature data
v2.1.2,read feature size
v2.1.2,re-allocate space if not enough
v2.1.2,fill feature_names_ if not header
v2.1.2,"if only one machine, find bin locally"
v2.1.2,"if have multi-machines, need to find bin distributed"
v2.1.2,different machines will find bin for different features
v2.1.2,start and len will store the process feature indices for different machines
v2.1.2,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.1.2,get size of bin mapper with max_bin size
v2.1.2,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.1.2,find local feature bins and copy to buffer
v2.1.2,free
v2.1.2,convert to binary size
v2.1.2,gather global feature bin mappers
v2.1.2,restore features bins from buffer
v2.1.2,---- private functions ----
v2.1.2,"if features are ordered, not need to use hist_buf"
v2.1.2,read all lines
v2.1.2,get query data
v2.1.2,"if not contain query data, minimal sample unit is one record"
v2.1.2,"if contain query data, minimal sample unit is one query"
v2.1.2,if is new query
v2.1.2,get query data
v2.1.2,"if not contain query file, minimal sample unit is one record"
v2.1.2,"if contain query file, minimal sample unit is one query"
v2.1.2,if is new query
v2.1.2,parse features
v2.1.2,-1 means doesn't use this feature
v2.1.2,"check the range of label_idx, weight_idx and group_idx"
v2.1.2,fill feature_names_ if not header
v2.1.2,start find bins
v2.1.2,"if only one machine, find bin locally"
v2.1.2,"if have multi-machines, need to find bin distributed"
v2.1.2,different machines will find bin for different features
v2.1.2,start and len will store the process feature indices for different machines
v2.1.2,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.1.2,get size of bin mapper with max_bin size
v2.1.2,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.1.2,find local feature bins and copy to buffer
v2.1.2,free
v2.1.2,convert to binary size
v2.1.2,gather global feature bin mappers
v2.1.2,restore features bins from buffer
v2.1.2,if doesn't need to prediction with initial model
v2.1.2,parser
v2.1.2,set label
v2.1.2,free processed line:
v2.1.2,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.1.2,text_reader_->Lines()[i].shrink_to_fit();
v2.1.2,push data
v2.1.2,if is used feature
v2.1.2,if need to prediction with initial model
v2.1.2,parser
v2.1.2,set initial score
v2.1.2,set label
v2.1.2,free processed line:
v2.1.2,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.1.2,text_reader_->Lines()[i].shrink_to_fit();
v2.1.2,push data
v2.1.2,if is used feature
v2.1.2,metadata_ will manage space of init_score
v2.1.2,text data can be free after loaded feature values
v2.1.2,parser
v2.1.2,set initial score
v2.1.2,set label
v2.1.2,push data
v2.1.2,if is used feature
v2.1.2,only need part of data
v2.1.2,need full data
v2.1.2,metadata_ will manage space of init_score
v2.1.2,read size of token
v2.1.2,deep copy function for BinMapper
v2.1.2,mean size for one bin
v2.1.2,need a new bin
v2.1.2,update bin upper bound
v2.1.2,last bin upper bound
v2.1.2,find distinct_values first
v2.1.2,push zero in the front
v2.1.2,use the large value
v2.1.2,push zero in the back
v2.1.2,convert to int type first
v2.1.2,sort by counts
v2.1.2,avoid first bin is zero
v2.1.2,will ignore the categorical of small counts
v2.1.2,need an additional bin for NaN
v2.1.2,use -1 to represent NaN
v2.1.2,Use MissingType::None to represent this bin contains all categoricals
v2.1.2,check trival(num_bin_ == 1) feature
v2.1.2,check useless bin
v2.1.2,calculate sparse rate
v2.1.2,sparse threshold
v2.1.2,"for lambdarank, it needs query data for partition data in parallel learning"
v2.1.2,need convert query_id to boundaries
v2.1.2,check weights
v2.1.2,check query boundries
v2.1.2,contain initial score file
v2.1.2,check weights
v2.1.2,get local weights
v2.1.2,check query boundries
v2.1.2,get local query boundaries
v2.1.2,contain initial score file
v2.1.2,get local initial scores
v2.1.2,re-load query weight
v2.1.2,save to nullptr
v2.1.2,save to nullptr
v2.1.2,save to nullptr
v2.1.2,default weight file name
v2.1.2,default weight file name
v2.1.2,use first line to count number class
v2.1.2,default query file name
v2.1.2,/ This file is auto generated by LightGBM\helper\parameter_generator.py from LightGBM\include\LightGBM\config.h file.
v2.1.2,root is in the depth 0
v2.1.2,non-leaf
v2.1.2,leaf
v2.1.2,use this for the missing value conversion
v2.1.2,Predict func by Map to ifelse
v2.1.2,use this for the missing value conversion
v2.1.2,non-leaf
v2.1.2,left subtree
v2.1.2,right subtree
v2.1.2,leaf
v2.1.2,non-leaf
v2.1.2,left subtree
v2.1.2,right subtree
v2.1.2,leaf
v2.1.2,recursive computation of SHAP values for a decision tree
v2.1.2,extend the unique path
v2.1.2,leaf node
v2.1.2,internal node
v2.1.2,"see if we have already split on this feature,"
v2.1.2,if so we undo that split so we can redo it for this node
v2.1.2,clear old metrics
v2.1.2,to lower
v2.1.2,split
v2.1.2,remove duplicate
v2.1.2,add names of objective function if not providing metric
v2.1.2,generate seeds by seed.
v2.1.2,check for conflicts
v2.1.2,"check if objective, metric, and num_class match"
v2.1.2,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.1.2,Check max_depth and num_leaves
v2.1.2,"filter is based on sampling data, so decrease its range"
v2.1.2,put dense feature first
v2.1.2,sort by non zero cnt
v2.1.2,"sort by non zero cnt, bigger first"
v2.1.2,"take apart small sparse group, due it will not gain on speed"
v2.1.2,shuffle groups
v2.1.2,get num_features
v2.1.2,get bin_mappers
v2.1.2,copy feature bin mapper data
v2.1.2,copy feature bin mapper data
v2.1.2,"if not pass a filename, just append "".bin"" of original file"
v2.1.2,get size of header
v2.1.2,size of feature names
v2.1.2,write header
v2.1.2,write feature names
v2.1.2,get size of meta data
v2.1.2,write meta data
v2.1.2,write feature data
v2.1.2,get size of feature
v2.1.2,write feature
v2.1.2,feature is not used
v2.1.2,construct histograms for smaller leaf
v2.1.2,if not use ordered bin
v2.1.2,used ordered bin
v2.1.2,feature is not used
v2.1.2,construct histograms for smaller leaf
v2.1.2,if not use ordered bin
v2.1.2,used ordered bin
v2.1.2,fixed hessian.
v2.1.2,feature is not used
v2.1.2,construct histograms for smaller leaf
v2.1.2,if not use ordered bin
v2.1.2,used ordered bin
v2.1.2,feature is not used
v2.1.2,construct histograms for smaller leaf
v2.1.2,if not use ordered bin
v2.1.2,used ordered bin
v2.1.2,fixed hessian.
v2.1.2,PredictRaw
v2.1.2,PredictRawByMap
v2.1.2,Predict
v2.1.2,PredictByMap
v2.1.2,PredictLeafIndex
v2.1.2,PredictLeafIndexByMap
v2.1.2,output model type
v2.1.2,output number of class
v2.1.2,output label index
v2.1.2,output max_feature_idx
v2.1.2,output objective
v2.1.2,output tree models
v2.1.2,store the importance first
v2.1.2,sort the importance
v2.1.2,use serialized string to restore this object
v2.1.2,Use first 128 chars to avoid exceed the message buffer.
v2.1.2,get number of classes
v2.1.2,get index of label
v2.1.2,get max_feature_idx first
v2.1.2,get average_output
v2.1.2,get feature names
v2.1.2,set zero
v2.1.2,predict all the trees for one iteration
v2.1.2,check early stopping
v2.1.2,set zero
v2.1.2,predict all the trees for one iteration
v2.1.2,check early stopping
v2.1.2,margin_threshold will be captured by value
v2.1.2,copy and sort
v2.1.2,margin_threshold will be captured by value
v2.1.2,load forced_splits file
v2.1.2,init tree learner
v2.1.2,push training metrics
v2.1.2,create buffer for gradients and hessians
v2.1.2,get max feature index
v2.1.2,get label index
v2.1.2,get feature names
v2.1.2,"if need bagging, create buffer"
v2.1.2,reset config for tree learner
v2.1.2,multi-class
v2.1.2,binary class
v2.1.2,"for a validation dataset, we need its score and metric"
v2.1.2,update score
v2.1.2,objective function will calculate gradients and hessians
v2.1.2,"random bagging, minimal unit is one record"
v2.1.2,if need bagging
v2.1.2,set bagging data to tree learner
v2.1.2,get subset
v2.1.2,output used time per iteration
v2.1.2,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.1.2,boosting first
v2.1.2,bagging logic
v2.1.2,need to copy gradients for bagging subset.
v2.1.2,shrinkage by learning rate
v2.1.2,update score
v2.1.2,only add default score one-time
v2.1.2,updates scores
v2.1.2,add model
v2.1.2,reset score
v2.1.2,remove model
v2.1.2,print message for metric
v2.1.2,pop last early_stopping_round_ models
v2.1.2,update training score
v2.1.2,we need to predict out-of-bag scores of data for boosting
v2.1.2,update validation score
v2.1.2,print training metric
v2.1.2,print validation metric
v2.1.2,set zero
v2.1.2,predict all the trees for one iteration
v2.1.2,check early stopping
v2.1.2,push training metrics
v2.1.2,"not same training data, need reset score and others"
v2.1.2,create score tracker
v2.1.2,update score
v2.1.2,create buffer for gradients and hessians
v2.1.2,"if need bagging, create buffer"
v2.1.2,Get the max size of pool
v2.1.2,at least need 2 leaves
v2.1.2,push split information for all leaves
v2.1.2,get ordered bin
v2.1.2,check existing for ordered bin
v2.1.2,initialize splits for leaf
v2.1.2,initialize data partition
v2.1.2,initialize ordered gradients and hessians
v2.1.2,"if has ordered bin, need to allocate a buffer to fast split"
v2.1.2,get ordered bin
v2.1.2,initialize splits for leaf
v2.1.2,initialize data partition
v2.1.2,initialize ordered gradients and hessians
v2.1.2,"if has ordered bin, need to allocate a buffer to fast split"
v2.1.2,Get the max size of pool
v2.1.2,at least need 2 leaves
v2.1.2,push split information for all leaves
v2.1.2,some initial works before training
v2.1.2,root leaf
v2.1.2,only root leaf can be splitted on first time
v2.1.2,some initial works before finding best split
v2.1.2,find best threshold for every feature
v2.1.2,Get a leaf with max split gain
v2.1.2,Get split information for best leaf
v2.1.2,"cannot split, quit"
v2.1.2,split tree with best leaf
v2.1.2,reset histogram pool
v2.1.2,at least use one feature
v2.1.2,initialize used features
v2.1.2,Get used feature at current tree
v2.1.2,initialize data partition
v2.1.2,reset the splits for leaves
v2.1.2,Sumup for root
v2.1.2,use all data
v2.1.2,"use bagging, only use part of data"
v2.1.2,"if has ordered bin, need to initialize the ordered bin"
v2.1.2,"use all data, pass nullptr"
v2.1.2,"bagging, only use part of data"
v2.1.2,mark used data
v2.1.2,initialize ordered bin
v2.1.2,check depth of current leaf
v2.1.2,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.1.2,no enough data to continue
v2.1.2,only have root
v2.1.2,put parent(left) leaf's histograms into larger leaf's histograms
v2.1.2,put parent(left) leaf's histograms to larger leaf's histograms
v2.1.2,split for the ordered bin
v2.1.2,mark data that at left-leaf
v2.1.2,split the ordered bin
v2.1.2,construct smaller leaf
v2.1.2,construct larger leaf
v2.1.2,find splits
v2.1.2,only has root leaf
v2.1.2,find best threshold for larger child
v2.1.2,start at root leaf
v2.1.2,"before processing next node from queue, store info for current left/right leaf"
v2.1.2,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.1.2,"then, compute own splits"
v2.1.2,split info should exist because searching in bfs fashion - should have added from parent
v2.1.2,"split tree, will return right leaf"
v2.1.2,left = parent
v2.1.2,"split tree, will return right leaf"
v2.1.2,init the leaves that used on next iteration
v2.1.2,bag_mapper[index_mapper[i]]
v2.1.2,get feature partition
v2.1.2,get local used features
v2.1.2,get best split at smaller leaf
v2.1.2,find local best split for larger leaf
v2.1.2,sync global best info
v2.1.2,update best split
v2.1.2,"instantiate template classes, otherwise linker cannot find the code"
v2.1.2,initialize SerialTreeLearner
v2.1.2,Get local rank and global machine size
v2.1.2,allocate buffer for communication
v2.1.2,generate feature partition for current tree
v2.1.2,get local used feature
v2.1.2,get block start and block len for reduce scatter
v2.1.2,get buffer_write_start_pos_
v2.1.2,get buffer_read_start_pos_
v2.1.2,sync global data sumup info
v2.1.2,global sumup reduce
v2.1.2,copy back
v2.1.2,set global sumup info
v2.1.2,init global data count in leaf
v2.1.2,construct local histograms
v2.1.2,copy to buffer
v2.1.2,Reduce scatter for histogram
v2.1.2,restore global histograms from buffer
v2.1.2,find best threshold for smaller child
v2.1.2,only root leaf
v2.1.2,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.1.2,find best threshold for larger child
v2.1.2,find local best split for larger leaf
v2.1.2,sync global best info
v2.1.2,set best split
v2.1.2,need update global number of data in leaf
v2.1.2,"instantiate template classes, otherwise linker cannot find the code"
v2.1.2,initialize SerialTreeLearner
v2.1.2,some additional variables needed for GPU trainer
v2.1.2,Initialize GPU buffers and kernels
v2.1.2,some functions used for debugging the GPU histogram construction
v2.1.2,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.1.2,goto err;
v2.1.2,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.1.2,goto err;
v2.1.2,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.1.2,also guarantee that there are at least 2K examples per workgroup
v2.1.2,return 0;
v2.1.2,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.1.2,decide the best number of workgroups working on one feature4 tuple
v2.1.2,set work group size based on feature size
v2.1.2,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.1.2,we need to refresh the kernel arguments after reallocating
v2.1.2,The only argument that needs to be changed later is num_data_
v2.1.2,"the GPU kernel will process all features in one call, and each"
v2.1.2,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.1.2,process one feature4 tuple
v2.1.2,"for the root node, indices are not copied"
v2.1.2,"for constant hessian, hessians are not copied except for the root node"
v2.1.2,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.1.2,and we will launch num_feature workgroups for this kernel
v2.1.2,will launch threads for all features
v2.1.2,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.1.2,copy the results asynchronously. Size depends on if double precision is used
v2.1.2,we will wait for this object in WaitAndGetHistograms
v2.1.2,"when the output is ready, the computation is done"
v2.1.2,values of this feature has been redistributed to multiple bins; need a reduction here
v2.1.2,how many feature-group tuples we have
v2.1.2,leave some safe margin for prefetching
v2.1.2,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.1.2,clear sparse/dense maps
v2.1.2,do nothing if no features can be processed on GPU
v2.1.2,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.1.2,unpin old buffer if necessary before destructing them
v2.1.2,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.1.2,allocate space for gradients and hessians on device
v2.1.2,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.1.2,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.1.2,copy indices to the device
v2.1.2,histogram bin entry size depends on the precision (single/double)
v2.1.2,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.1.2,each work group generates a sub-histogram of dword_features_ features.
v2.1.2,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.1.2,create atomic counters for inter-group coordination
v2.1.2,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.1.2,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.1.2,looking for dword_features_ non-sparse feature-groups
v2.1.2,decide if we need to redistribute the bin
v2.1.2,multiplier must be a power of 2
v2.1.2,device_bin_mults_.push_back(1);
v2.1.2,found
v2.1.2,for data transfer time
v2.1.2,"Now generate new data structure feature4, and copy data to the device"
v2.1.2,"preallocate arrays for all threads, and pin them"
v2.1.2,building Feature4 bundles; each thread handles dword_features_ features
v2.1.2,one feature datapoint is 4 bits
v2.1.2,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.1.2,one feature datapoint is one byte
v2.1.2,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.1.2,Dense bin
v2.1.2,Dense 4-bit bin
v2.1.2,working on the remaining (less than dword_features_) feature groups
v2.1.2,fill the leftover features
v2.1.2,"fill this empty feature with some ""random"" value"
v2.1.2,"fill this empty feature with some ""random"" value"
v2.1.2,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.1.2,deallocate pinned space for feature copying
v2.1.2,data transfer time
v2.1.2,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.1.2,"Something bad happened. Just return ""No log available."""
v2.1.2,"build is okay, log may contain warnings"
v2.1.2,destroy any old kernels
v2.1.2,create OpenCL kernels for different number of workgroups per feature
v2.1.2,currently we don't use constant memory
v2.1.2,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.1.2,kernel with indices in an array
v2.1.2,"kernel with all features enabled, with elimited branches"
v2.1.2,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.1.2,do nothing if no features can be processed on GPU
v2.1.2,The only argument that needs to be changed later is num_data_
v2.1.2,"hessian is passed as a parameter, but it is not available now."
v2.1.2,hessian will be set in BeforeTrain()
v2.1.2,"Get the max bin size, used for selecting best GPU kernel"
v2.1.2,initialize GPU
v2.1.2,determine which kernel to use based on the max number of bins
v2.1.2,setup GPU kernel arguments after we allocating all the buffers
v2.1.2,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.1.2,this should rarely occur
v2.1.2,GPU memory has to been reallocated because data may have been changed
v2.1.2,setup GPU kernel arguments after we allocating all the buffers
v2.1.2,Copy initial full hessians and gradients to GPU.
v2.1.2,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.1.2,setup hessian parameters only
v2.1.2,hessian is passed as a parameter
v2.1.2,use bagging
v2.1.2,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.1.2,copy used gradients and hessians to ordered buffer
v2.1.2,transfer the indices to GPU
v2.1.2,transfer hessian to GPU
v2.1.2,setup hessian parameters only
v2.1.2,hessian is passed as a parameter
v2.1.2,transfer gradients to GPU
v2.1.2,only have root
v2.1.2,"Copy indices, gradients and hessians as early as possible"
v2.1.2,only need to initialize for smaller leaf
v2.1.2,Get leaf boundary
v2.1.2,copy indices to the GPU:
v2.1.2,copy ordered hessians to the GPU:
v2.1.2,copy ordered gradients to the GPU:
v2.1.2,do nothing if no features can be processed on GPU
v2.1.2,copy data indices if it is not null
v2.1.2,generate and copy ordered_gradients if gradients is not null
v2.1.2,generate and copy ordered_hessians if hessians is not null
v2.1.2,converted indices in is_feature_used to feature-group indices
v2.1.2,construct the feature masks for dense feature-groups
v2.1.2,"if no feature group is used, just return and do not use GPU"
v2.1.2,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.1.2,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.1.2,"All data have been prepared, now run the GPU kernel"
v2.1.2,construct smaller leaf
v2.1.2,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.1.2,then construct sparse features on CPU
v2.1.2,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.1.2,"wait for GPU to finish, only if GPU is actually used"
v2.1.2,use double precision
v2.1.2,use single precision
v2.1.2,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.1.2,#define GPU_DEBUG_COMPARE
v2.1.2,construct larger leaf
v2.1.2,then construct sparse features on CPU
v2.1.2,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.1.2,"wait for GPU to finish, only if GPU is actually used"
v2.1.2,use double precision
v2.1.2,use single precision
v2.1.2,do some sanity check for the GPU algorithm
v2.1.2,limit top k
v2.1.2,get max bin
v2.1.2,calculate buffer size
v2.1.2,"left and right on same time, so need double size"
v2.1.2,initialize histograms for global
v2.1.2,sync global data sumup info
v2.1.2,set global sumup info
v2.1.2,init global data count in leaf
v2.1.2,get local sumup
v2.1.2,get local sumup
v2.1.2,get mean number on machines
v2.1.2,weighted gain
v2.1.2,get top k
v2.1.2,"Copy histogram to buffer, and Get local aggregate features"
v2.1.2,copy histograms.
v2.1.2,copy smaller leaf histograms first
v2.1.2,mark local aggregated feature
v2.1.2,copy
v2.1.2,then copy larger leaf histograms
v2.1.2,mark local aggregated feature
v2.1.2,copy
v2.1.2,use local data to find local best splits
v2.1.2,find splits
v2.1.2,only has root leaf
v2.1.2,find best threshold for larger child
v2.1.2,local voting
v2.1.2,gather
v2.1.2,get all top-k from all machines
v2.1.2,global voting
v2.1.2,copy local histgrams to buffer
v2.1.2,Reduce scatter for histogram
v2.1.2,find best split from local aggregated histograms
v2.1.2,restore from buffer
v2.1.2,find best threshold
v2.1.2,restore from buffer
v2.1.2,find best threshold
v2.1.2,find local best
v2.1.2,find local best split for larger leaf
v2.1.2,sync global best info
v2.1.2,copy back
v2.1.2,set the global number of data for leaves
v2.1.2,init the global sumup info
v2.1.2,"instantiate template classes, otherwise linker cannot find the code"
v2.1.2,coding: utf-8
v2.1.2,alias table
v2.1.2,names
v2.1.2,from strings
v2.1.2,tails
v2.1.2,tails
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable=invalid-name, exec-used, C0111"
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, W0105"
v2.1.1,create predictor first
v2.1.1,check dataset
v2.1.1,reduce cost for prediction training data
v2.1.1,process callbacks
v2.1.1,Most of legacy advanced options becomes callbacks
v2.1.1,construct booster
v2.1.1,start training
v2.1.1,check evaluation result.
v2.1.1,"lambdarank task, split according to groups"
v2.1.1,run preprocessing on the data set if needed
v2.1.1,setup callbacks
v2.1.1,coding: utf-8
v2.1.1,pylint: disable = C0103
v2.1.1,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.1.1,because of u'...' Unicode literals.
v2.1.1,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.1.1,minor change to support `**kwargs`
v2.1.1,sklearn interface has another naming convention
v2.1.1,"user can set verbose with kwargs, it has higher priority"
v2.1.1,reduce cost for prediction training data
v2.1.1,free dataset
v2.1.1,Switch to using a multiclass objective in the underlying LGBM instance
v2.1.1,check group data
v2.1.1,coding: utf-8
v2.1.1,we don't need lib_lightgbm while building docs
v2.1.1,coding: utf-8
v2.1.1,pylint: disable = C0103
v2.1.1,coding: utf-8
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, C0111, C0301"
v2.1.1,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.1.1,TypeError: obj is not a string or a number
v2.1.1,ValueError: invalid literal
v2.1.1,process for args
v2.1.1,get categorical features
v2.1.1,process for reference dataset
v2.1.1,start construct data
v2.1.1,check data has header or not
v2.1.1,load init score
v2.1.1,need re group init score
v2.1.1,set feature names
v2.1.1,"change non-float data to float data, need to copy"
v2.1.1,create valid
v2.1.1,construct subset
v2.1.1,create train
v2.1.1,set to None
v2.1.1,we're done if self and reference share a common upstrem reference
v2.1.1,"group data from LightGBM is boundaries data, need to convert to group size"
v2.1.1,Training task
v2.1.1,construct booster object
v2.1.1,save reference to data
v2.1.1,buffer for inner predict
v2.1.1,set network if necessary
v2.1.1,Prediction task
v2.1.1,need reset training data
v2.1.1,need to push new valid data
v2.1.1,Get name of features
v2.1.1,avoid to predict many time in one iteration
v2.1.1,Get num of inner evals
v2.1.1,Get name of evals
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, W0105, C0301"
v2.1.1,Callback environment used by callbacks
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, C0111"
v2.1.1,load or create your dataset
v2.1.1,create dataset for lightgbm
v2.1.1,"if you want to re-use data, remember to set free_raw_data=False"
v2.1.1,specify your configurations as a dict
v2.1.1,generate a feature name
v2.1.1,feature_name and categorical_feature
v2.1.1,check feature name
v2.1.1,save model to file
v2.1.1,dump model to json (and save to file)
v2.1.1,feature names
v2.1.1,feature importances
v2.1.1,load model to predict
v2.1.1,can only predict with the best iteration (or the saving iteration)
v2.1.1,eval with loaded model
v2.1.1,dump model with pickle
v2.1.1,load model with pickle to predict
v2.1.1,can predict with any iteration when loaded in pickle way
v2.1.1,eval with loaded model
v2.1.1,continue training
v2.1.1,init_model accepts:
v2.1.1,1. model file name
v2.1.1,2. Booster()
v2.1.1,decay learning rates
v2.1.1,learning_rates accepts:
v2.1.1,1. list/tuple with length = num_boost_round
v2.1.1,2. function(curr_iter)
v2.1.1,change other parameters during training
v2.1.1,self-defined objective function
v2.1.1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.1.1,log likelihood loss
v2.1.1,self-defined eval metric
v2.1.1,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.1.1,binary error
v2.1.1,callback
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, C0111"
v2.1.1,load or create your dataset
v2.1.1,train
v2.1.1,predict
v2.1.1,eval
v2.1.1,feature importances
v2.1.1,other scikit-learn modules
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, C0111"
v2.1.1,load or create your dataset
v2.1.1,create dataset for lightgbm
v2.1.1,specify your configurations as a dict
v2.1.1,train
v2.1.1,coding: utf-8
v2.1.1,"pylint: disable = invalid-name, C0111"
v2.1.1,load or create your dataset
v2.1.1,create dataset for lightgbm
v2.1.1,specify your configurations as a dict
v2.1.1,train
v2.1.1,save model to file
v2.1.1,predict
v2.1.1,eval
v2.1.1,!/usr/bin/env python3
v2.1.1,-*- coding: utf-8 -*-
v2.1.1,
v2.1.1,"LightGBM documentation build configuration file, created by"
v2.1.1,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.1.1,
v2.1.1,This file is execfile()d with the current directory set to its
v2.1.1,containing dir.
v2.1.1,
v2.1.1,Note that not all possible configuration values are present in this
v2.1.1,autogenerated file.
v2.1.1,
v2.1.1,All configuration values have a default; values that are commented out
v2.1.1,serve to show the default.
v2.1.1,"If extensions (or modules to document with autodoc) are in another directory,"
v2.1.1,add these directories to sys.path here. If the directory is relative to the
v2.1.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.1.1,
v2.1.1,-- mock out modules
v2.1.1,-- General configuration ------------------------------------------------
v2.1.1,"If your documentation needs a minimal Sphinx version, state it here."
v2.1.1,"Add any Sphinx extension module names here, as strings. They can be"
v2.1.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.1.1,ones.
v2.1.1,"Add any paths that contain templates here, relative to this directory."
v2.1.1,The suffix(es) of source filenames.
v2.1.1,You can specify multiple suffix as a list of string:
v2.1.1,"source_suffix = ['.rst', '.md']"
v2.1.1,The master toctree document.
v2.1.1,General information about the project.
v2.1.1,"The version info for the project you're documenting, acts as replacement for"
v2.1.1,"|version| and |release|, also used in various other places throughout the"
v2.1.1,built documents.
v2.1.1,
v2.1.1,The short X.Y version.
v2.1.1,"The full version, including alpha/beta/rc tags."
v2.1.1,The language for content autogenerated by Sphinx. Refer to documentation
v2.1.1,for a list of supported languages.
v2.1.1,
v2.1.1,This is also used if you do content translation via gettext catalogs.
v2.1.1,"Usually you set ""language"" from the command line for these cases."
v2.1.1,"List of patterns, relative to source directory, that match files and"
v2.1.1,directories to ignore when looking for source files.
v2.1.1,This patterns also effect to html_static_path and html_extra_path
v2.1.1,The name of the Pygments (syntax highlighting) style to use.
v2.1.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.1.1,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.1.1,-- Options for HTML output ----------------------------------------------
v2.1.1,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.1.1,a list of builtin themes.
v2.1.1,
v2.1.1,Theme options are theme-specific and customize the look and feel of a theme
v2.1.1,"further.  For a list of options available for each theme, see the"
v2.1.1,documentation.
v2.1.1,
v2.1.1,html_theme_options = {}
v2.1.1,"Add any paths that contain custom static files (such as style sheets) here,"
v2.1.1,"relative to this directory. They are copied after the builtin static files,"
v2.1.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.1.1,-- Options for HTMLHelp output ------------------------------------------
v2.1.1,Output file base name for HTML help builder.
v2.1.1,coding: utf-8
v2.1.1,coding: utf-8
v2.1.1,pylint: skip-file
v2.1.1,we don't need lib_lightgbm while building docs
v2.1.1,coding: utf-8
v2.1.1,pylint: skip-file
v2.1.1,check saved model persistence
v2.1.1,"we need to check the consistency of model file here, so test for exact equal"
v2.1.1,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.1.1,"scores likely to be different, but prediction should still be the same"
v2.1.1,coding: utf-8
v2.1.1,pylint: skip-file
v2.1.1,Tests that `seed` is the same as `random_state`
v2.1.1,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.1.1,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.1.1,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.1.1,coding: utf-8
v2.1.1,pylint: skip-file
v2.1.1,coding: utf-8
v2.1.1,pylint: skip-file
v2.1.1,no early stopping
v2.1.1,early stopping occurs
v2.1.1,test custom eval metrics
v2.1.1,"shuffle = False, override metric in params"
v2.1.1,"shuffle = True, callbacks"
v2.1.1,self defined folds
v2.1.1,lambdarank
v2.1.1,test feature_names with whitespaces
v2.1.1,take subsets and train
v2.1.1,test sliced labels
v2.1.1,append some columns
v2.1.1,append some rows
v2.1.1,test sliced 2d matrix
v2.1.1,test sliced CSR
v2.1.1,coding: utf-8
v2.1.1,pylint: skip-file
v2.1.1,convert from one-based to  zero-based index
v2.1.1,convert from boundaries to size
v2.1.1,--- start Booster interfaces
v2.1.1,create boosting
v2.1.1,initialize the boosting
v2.1.1,create objective function
v2.1.1,initialize the objective function
v2.1.1,create training metric
v2.1.1,reset the boosting
v2.1.1,create objective function
v2.1.1,initialize the objective function
v2.1.1,some help functions used to convert data
v2.1.1,Row iterator of on column for CSC matrix
v2.1.1,"return value at idx, only can access by ascent order"
v2.1.1,"return next non-zero pair, if index < 0, means no more data"
v2.1.1,start of c_api functions
v2.1.1,sample data first
v2.1.1,sample data first
v2.1.1,sample data first
v2.1.1,no more data
v2.1.1,---- start of booster
v2.1.1,---- start of some help functions
v2.1.1,set number of threads for openmp
v2.1.1,check for alias
v2.1.1,read parameters from config file
v2.1.1,"remove str after ""#"""
v2.1.1,check for alias again
v2.1.1,load configs
v2.1.1,prediction is needed if using input initial model(continued train)
v2.1.1,need to continue training
v2.1.1,sync up random seed for data partition
v2.1.1,load Training data
v2.1.1,load data for parallel training
v2.1.1,load data for single machine
v2.1.1,need save binary file
v2.1.1,create training metric
v2.1.1,only when have metrics then need to construct validation data
v2.1.1,"Add validation data, if it exists"
v2.1.1,add
v2.1.1,need save binary file
v2.1.1,add metric for validation data
v2.1.1,output used time on each iteration
v2.1.1,need init network
v2.1.1,create boosting
v2.1.1,create objective function
v2.1.1,load training data
v2.1.1,initialize the objective function
v2.1.1,initialize the boosting
v2.1.1,add validation data into boosting
v2.1.1,convert model to if-else statement code
v2.1.1,create predictor
v2.1.1,Free memory
v2.1.1,create predictor
v2.1.1,counts for all labels
v2.1.1,"start from top label, and accumulate DCG"
v2.1.1,counts for all labels
v2.1.1,calculate k Max DCG by one pass
v2.1.1,get sorted indices by score
v2.1.1,calculate dcg
v2.1.1,get sorted indices by score
v2.1.1,calculate multi dcg by one pass
v2.1.1,wait for all client start up
v2.1.1,default set to -1
v2.1.1,"distance at k-th communication, distance[k] = 2^k"
v2.1.1,set incoming rank at k-th commuication
v2.1.1,set outgoing rank at k-th commuication
v2.1.1,defalut set as -1
v2.1.1,construct all recursive halving map for all machines
v2.1.1,let 1 << k <= num_machines
v2.1.1,distance of each communication
v2.1.1,"if num_machines = 2^k, don't need to group machines"
v2.1.1,"communication direction, %2 == 0 is positive"
v2.1.1,neighbor at k-th communication
v2.1.1,receive data block at k-th communication
v2.1.1,send data block at k-th communication
v2.1.1,"if num_machines != 2^k, need to group machines"
v2.1.1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.1.1,let left machine as group leader
v2.1.1,"cache block information for groups, group with 2 machines will have double block size"
v2.1.1,convert from group to node leader
v2.1.1,convert from node to group
v2.1.1,meet new group
v2.1.1,add block len for this group
v2.1.1,calculate the group block start
v2.1.1,not need to construct
v2.1.1,get receive block informations
v2.1.1,accumulate block len
v2.1.1,get send block informations
v2.1.1,accumulate block len
v2.1.1,static member definition
v2.1.1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.1.1,assign the blocks to every rank.
v2.1.1,do reduce scatter
v2.1.1,do all gather
v2.1.1,assign blocks
v2.1.1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.1.1,copy back
v2.1.1,assign blocks
v2.1.1,start all gather
v2.1.1,when num_machines is small and data is large
v2.1.1,use output as receive buffer
v2.1.1,get current local block size
v2.1.1,get out rank
v2.1.1,get in rank
v2.1.1,get send information
v2.1.1,get recv information
v2.1.1,send and recv at same time
v2.1.1,rotate in-place
v2.1.1,use output as receive buffer
v2.1.1,get current local block size
v2.1.1,get send information
v2.1.1,get recv information
v2.1.1,send and recv at same time
v2.1.1,use output as receive buffer
v2.1.1,send and recv at same time
v2.1.1,send local data to neighbor first
v2.1.1,receive neighbor data first
v2.1.1,reduce
v2.1.1,get target
v2.1.1,get send information
v2.1.1,get recv information
v2.1.1,send and recv at same time
v2.1.1,reduce
v2.1.1,send result to neighbor
v2.1.1,receive result from neighbor
v2.1.1,copy result
v2.1.1,start up socket
v2.1.1,parse clients from file
v2.1.1,get ip list of local machine
v2.1.1,get local rank
v2.1.1,construct listener
v2.1.1,construct communication topo
v2.1.1,construct linkers
v2.1.1,free listener
v2.1.1,set timeout
v2.1.1,accept incoming socket
v2.1.1,receive rank
v2.1.1,add new socket
v2.1.1,save ranks that need to connect with
v2.1.1,start listener
v2.1.1,start connect
v2.1.1,let smaller rank connect to larger rank
v2.1.1,send local rank
v2.1.1,wait for listener
v2.1.1,print connected linkers
v2.1.1,Get some statistic from 2 line
v2.1.1,if only have one line on file
v2.1.1,Constructors
v2.1.1,Get type tag
v2.1.1,Comparisons
v2.1.1,"This has to be separate, not in Statics, because Json() accesses statics().null."
v2.1.1,"advance until next line, or end of input"
v2.1.1,advance until closing tokens
v2.1.1,The usual case: non-escaped characters
v2.1.1,Handle escapes
v2.1.1,Extract 4-byte escape sequence
v2.1.1,Explicitly check length of the substring. The following loop
v2.1.1,relies on std::string returning the terminating NUL when
v2.1.1,accessing str[length]. Checking here reduces brittleness.
v2.1.1,JSON specifies that characters outside the BMP shall be encoded as a pair
v2.1.1,of 4-hex-digit \u escapes encoding their surrogate pair components. Check
v2.1.1,whether we're in the middle of such a beast: the previous codepoint was an
v2.1.1,"escaped lead (high) surrogate, and this is a trail (low) surrogate."
v2.1.1,"Reassemble the two surrogate pairs into one astral-plane character, per"
v2.1.1,the UTF-16 algorithm.
v2.1.1,Integer part
v2.1.1,Decimal part
v2.1.1,Exponent part
v2.1.1,Check for any trailing garbage
v2.1.1,Documented in json11.hpp
v2.1.1,Check for another object
v2.1.1,get column names
v2.1.1,load label idx first
v2.1.1,erase label column name
v2.1.1,load ignore columns
v2.1.1,load weight idx
v2.1.1,load group idx
v2.1.1,don't support query id in data file when training in parallel
v2.1.1,read data to memory
v2.1.1,sample data
v2.1.1,construct feature bin mappers
v2.1.1,initialize label
v2.1.1,extract features
v2.1.1,sample data from file
v2.1.1,construct feature bin mappers
v2.1.1,initialize label
v2.1.1,extract features
v2.1.1,load data from binary file
v2.1.1,check meta data
v2.1.1,need to check training data
v2.1.1,read data in memory
v2.1.1,initialize label
v2.1.1,extract features
v2.1.1,Get number of lines of data file
v2.1.1,initialize label
v2.1.1,extract features
v2.1.1,load data from binary file
v2.1.1,not need to check validation data
v2.1.1,check meta data
v2.1.1,buffer to read binary file
v2.1.1,check token
v2.1.1,read size of header
v2.1.1,re-allocmate space if not enough
v2.1.1,read header
v2.1.1,get header
v2.1.1,num_groups
v2.1.1,real_feature_idx_
v2.1.1,feature2group
v2.1.1,feature2subfeature
v2.1.1,group_bin_boundaries
v2.1.1,group_feature_start_
v2.1.1,group_feature_cnt_
v2.1.1,get feature names
v2.1.1,write feature names
v2.1.1,read size of meta data
v2.1.1,re-allocate space if not enough
v2.1.1,read meta data
v2.1.1,load meta data
v2.1.1,sample local used data if need to partition
v2.1.1,"if not contain query file, minimal sample unit is one record"
v2.1.1,"if contain query file, minimal sample unit is one query"
v2.1.1,if is new query
v2.1.1,read feature data
v2.1.1,read feature size
v2.1.1,re-allocate space if not enough
v2.1.1,fill feature_names_ if not header
v2.1.1,"if only one machine, find bin locally"
v2.1.1,"if have multi-machines, need to find bin distributed"
v2.1.1,different machines will find bin for different features
v2.1.1,start and len will store the process feature indices for different machines
v2.1.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.1.1,get size of bin mapper with max_bin size
v2.1.1,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.1.1,find local feature bins and copy to buffer
v2.1.1,free
v2.1.1,convert to binary size
v2.1.1,gather global feature bin mappers
v2.1.1,restore features bins from buffer
v2.1.1,---- private functions ----
v2.1.1,"if features are ordered, not need to use hist_buf"
v2.1.1,read all lines
v2.1.1,get query data
v2.1.1,"if not contain query data, minimal sample unit is one record"
v2.1.1,"if contain query data, minimal sample unit is one query"
v2.1.1,if is new query
v2.1.1,get query data
v2.1.1,"if not contain query file, minimal sample unit is one record"
v2.1.1,"if contain query file, minimal sample unit is one query"
v2.1.1,if is new query
v2.1.1,parse features
v2.1.1,-1 means doesn't use this feature
v2.1.1,"check the range of label_idx, weight_idx and group_idx"
v2.1.1,fill feature_names_ if not header
v2.1.1,start find bins
v2.1.1,"if only one machine, find bin locally"
v2.1.1,"if have multi-machines, need to find bin distributed"
v2.1.1,different machines will find bin for different features
v2.1.1,start and len will store the process feature indices for different machines
v2.1.1,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.1.1,get size of bin mapper with max_bin size
v2.1.1,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.1.1,find local feature bins and copy to buffer
v2.1.1,free
v2.1.1,convert to binary size
v2.1.1,gather global feature bin mappers
v2.1.1,restore features bins from buffer
v2.1.1,if doesn't need to prediction with initial model
v2.1.1,parser
v2.1.1,set label
v2.1.1,free processed line:
v2.1.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.1.1,text_reader_->Lines()[i].shrink_to_fit();
v2.1.1,push data
v2.1.1,if is used feature
v2.1.1,if need to prediction with initial model
v2.1.1,parser
v2.1.1,set initial score
v2.1.1,set label
v2.1.1,free processed line:
v2.1.1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.1.1,text_reader_->Lines()[i].shrink_to_fit();
v2.1.1,push data
v2.1.1,if is used feature
v2.1.1,metadata_ will manage space of init_score
v2.1.1,text data can be free after loaded feature values
v2.1.1,parser
v2.1.1,set initial score
v2.1.1,set label
v2.1.1,push data
v2.1.1,if is used feature
v2.1.1,only need part of data
v2.1.1,need full data
v2.1.1,metadata_ will manage space of init_score
v2.1.1,read size of token
v2.1.1,deep copy function for BinMapper
v2.1.1,mean size for one bin
v2.1.1,need a new bin
v2.1.1,update bin upper bound
v2.1.1,last bin upper bound
v2.1.1,find distinct_values first
v2.1.1,push zero in the front
v2.1.1,use the large value
v2.1.1,push zero in the back
v2.1.1,convert to int type first
v2.1.1,sort by counts
v2.1.1,avoid first bin is zero
v2.1.1,will ignore the categorical of small counts
v2.1.1,need an additional bin for NaN
v2.1.1,use -1 to represent NaN
v2.1.1,Use MissingType::None to represent this bin contains all categoricals
v2.1.1,check trival(num_bin_ == 1) feature
v2.1.1,check useless bin
v2.1.1,calculate sparse rate
v2.1.1,sparse threshold
v2.1.1,"for lambdarank, it needs query data for partition data in parallel learning"
v2.1.1,need convert query_id to boundaries
v2.1.1,check weights
v2.1.1,check query boundries
v2.1.1,contain initial score file
v2.1.1,check weights
v2.1.1,get local weights
v2.1.1,check query boundries
v2.1.1,get local query boundaries
v2.1.1,contain initial score file
v2.1.1,get local initial scores
v2.1.1,re-load query weight
v2.1.1,save to nullptr
v2.1.1,save to nullptr
v2.1.1,save to nullptr
v2.1.1,default weight file name
v2.1.1,default weight file name
v2.1.1,use first line to count number class
v2.1.1,default query file name
v2.1.1,root is in the depth 0
v2.1.1,non-leaf
v2.1.1,leaf
v2.1.1,use this for the missing value conversion
v2.1.1,Predict func by Map to ifelse
v2.1.1,use this for the missing value conversion
v2.1.1,non-leaf
v2.1.1,left subtree
v2.1.1,right subtree
v2.1.1,leaf
v2.1.1,non-leaf
v2.1.1,left subtree
v2.1.1,right subtree
v2.1.1,leaf
v2.1.1,recursive computation of SHAP values for a decision tree
v2.1.1,extend the unique path
v2.1.1,leaf node
v2.1.1,internal node
v2.1.1,"see if we have already split on this feature,"
v2.1.1,if so we undo that split so we can redo it for this node
v2.1.1,clear old metrics
v2.1.1,to lower
v2.1.1,split
v2.1.1,remove duplicate
v2.1.1,add names of objective function if not providing metric
v2.1.1,load main config types
v2.1.1,generate seeds by seed.
v2.1.1,sub-config setup
v2.1.1,check for conflicts
v2.1.1,"check if objective_type, metric_type, and num_class match"
v2.1.1,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.1.1,Check max_depth and num_leaves
v2.1.1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.1.1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.1.1,default eval ndcg @[1-5]
v2.1.1,"filter is based on sampling data, so decrease its range"
v2.1.1,put dense feature first
v2.1.1,sort by non zero cnt
v2.1.1,"sort by non zero cnt, bigger first"
v2.1.1,"take apart small sparse group, due it will not gain on speed"
v2.1.1,shuffle groups
v2.1.1,get num_features
v2.1.1,get bin_mappers
v2.1.1,copy feature bin mapper data
v2.1.1,copy feature bin mapper data
v2.1.1,"if not pass a filename, just append "".bin"" of original file"
v2.1.1,get size of header
v2.1.1,size of feature names
v2.1.1,write header
v2.1.1,write feature names
v2.1.1,get size of meta data
v2.1.1,write meta data
v2.1.1,write feature data
v2.1.1,get size of feature
v2.1.1,write feature
v2.1.1,feature is not used
v2.1.1,construct histograms for smaller leaf
v2.1.1,if not use ordered bin
v2.1.1,used ordered bin
v2.1.1,feature is not used
v2.1.1,construct histograms for smaller leaf
v2.1.1,if not use ordered bin
v2.1.1,used ordered bin
v2.1.1,fixed hessian.
v2.1.1,feature is not used
v2.1.1,construct histograms for smaller leaf
v2.1.1,if not use ordered bin
v2.1.1,used ordered bin
v2.1.1,feature is not used
v2.1.1,construct histograms for smaller leaf
v2.1.1,if not use ordered bin
v2.1.1,used ordered bin
v2.1.1,fixed hessian.
v2.1.1,PredictRaw
v2.1.1,PredictRawByMap
v2.1.1,Predict
v2.1.1,PredictByMap
v2.1.1,PredictLeafIndex
v2.1.1,PredictLeafIndexByMap
v2.1.1,output model type
v2.1.1,output number of class
v2.1.1,output label index
v2.1.1,output max_feature_idx
v2.1.1,output objective
v2.1.1,output tree models
v2.1.1,store the importance first
v2.1.1,sort the importance
v2.1.1,use serialized string to restore this object
v2.1.1,Use first 128 chars to avoid exceed the message buffer.
v2.1.1,get number of classes
v2.1.1,get index of label
v2.1.1,get max_feature_idx first
v2.1.1,get average_output
v2.1.1,get feature names
v2.1.1,set zero
v2.1.1,predict all the trees for one iteration
v2.1.1,check early stopping
v2.1.1,set zero
v2.1.1,predict all the trees for one iteration
v2.1.1,check early stopping
v2.1.1,margin_threshold will be captured by value
v2.1.1,copy and sort
v2.1.1,margin_threshold will be captured by value
v2.1.1,load forced_splits file
v2.1.1,init tree learner
v2.1.1,push training metrics
v2.1.1,create buffer for gradients and hessians
v2.1.1,get max feature index
v2.1.1,get label index
v2.1.1,get feature names
v2.1.1,"if need bagging, create buffer"
v2.1.1,reset config for tree learner
v2.1.1,multi-class
v2.1.1,binary class
v2.1.1,"for a validation dataset, we need its score and metric"
v2.1.1,update score
v2.1.1,objective function will calculate gradients and hessians
v2.1.1,"random bagging, minimal unit is one record"
v2.1.1,if need bagging
v2.1.1,set bagging data to tree learner
v2.1.1,get subset
v2.1.1,output used time per iteration
v2.1.1,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.1.1,boosting first
v2.1.1,bagging logic
v2.1.1,need to copy gradients for bagging subset.
v2.1.1,shrinkage by learning rate
v2.1.1,update score
v2.1.1,only add default score one-time
v2.1.1,updates scores
v2.1.1,add model
v2.1.1,reset score
v2.1.1,remove model
v2.1.1,print message for metric
v2.1.1,pop last early_stopping_round_ models
v2.1.1,update training score
v2.1.1,we need to predict out-of-bag scores of data for boosting
v2.1.1,update validation score
v2.1.1,print training metric
v2.1.1,print validation metric
v2.1.1,set zero
v2.1.1,predict all the trees for one iteration
v2.1.1,check early stopping
v2.1.1,push training metrics
v2.1.1,"not same training data, need reset score and others"
v2.1.1,create score tracker
v2.1.1,update score
v2.1.1,create buffer for gradients and hessians
v2.1.1,"if need bagging, create buffer"
v2.1.1,Get the max size of pool
v2.1.1,at least need 2 leaves
v2.1.1,push split information for all leaves
v2.1.1,get ordered bin
v2.1.1,check existing for ordered bin
v2.1.1,initialize splits for leaf
v2.1.1,initialize data partition
v2.1.1,initialize ordered gradients and hessians
v2.1.1,"if has ordered bin, need to allocate a buffer to fast split"
v2.1.1,get ordered bin
v2.1.1,initialize splits for leaf
v2.1.1,initialize data partition
v2.1.1,initialize ordered gradients and hessians
v2.1.1,"if has ordered bin, need to allocate a buffer to fast split"
v2.1.1,Get the max size of pool
v2.1.1,at least need 2 leaves
v2.1.1,push split information for all leaves
v2.1.1,some initial works before training
v2.1.1,root leaf
v2.1.1,only root leaf can be splitted on first time
v2.1.1,some initial works before finding best split
v2.1.1,find best threshold for every feature
v2.1.1,Get a leaf with max split gain
v2.1.1,Get split information for best leaf
v2.1.1,"cannot split, quit"
v2.1.1,split tree with best leaf
v2.1.1,reset histogram pool
v2.1.1,at least use one feature
v2.1.1,initialize used features
v2.1.1,Get used feature at current tree
v2.1.1,initialize data partition
v2.1.1,reset the splits for leaves
v2.1.1,Sumup for root
v2.1.1,use all data
v2.1.1,"use bagging, only use part of data"
v2.1.1,"if has ordered bin, need to initialize the ordered bin"
v2.1.1,"use all data, pass nullptr"
v2.1.1,"bagging, only use part of data"
v2.1.1,mark used data
v2.1.1,initialize ordered bin
v2.1.1,check depth of current leaf
v2.1.1,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.1.1,no enough data to continue
v2.1.1,only have root
v2.1.1,put parent(left) leaf's histograms into larger leaf's histograms
v2.1.1,put parent(left) leaf's histograms to larger leaf's histograms
v2.1.1,split for the ordered bin
v2.1.1,mark data that at left-leaf
v2.1.1,split the ordered bin
v2.1.1,construct smaller leaf
v2.1.1,construct larger leaf
v2.1.1,find splits
v2.1.1,only has root leaf
v2.1.1,find best threshold for larger child
v2.1.1,start at root leaf
v2.1.1,"before processing next node from queue, store info for current left/right leaf"
v2.1.1,"store ""best split"" for left and right, even if they might be overwritten by forced split"
v2.1.1,"then, compute own splits"
v2.1.1,split info should exist because searching in bfs fashion - should have added from parent
v2.1.1,"split tree, will return right leaf"
v2.1.1,left = parent
v2.1.1,"split tree, will return right leaf"
v2.1.1,init the leaves that used on next iteration
v2.1.1,bag_mapper[index_mapper[i]]
v2.1.1,get feature partition
v2.1.1,get local used features
v2.1.1,get best split at smaller leaf
v2.1.1,find local best split for larger leaf
v2.1.1,sync global best info
v2.1.1,update best split
v2.1.1,"instantiate template classes, otherwise linker cannot find the code"
v2.1.1,initialize SerialTreeLearner
v2.1.1,Get local rank and global machine size
v2.1.1,allocate buffer for communication
v2.1.1,generate feature partition for current tree
v2.1.1,get local used feature
v2.1.1,get block start and block len for reduce scatter
v2.1.1,get buffer_write_start_pos_
v2.1.1,get buffer_read_start_pos_
v2.1.1,sync global data sumup info
v2.1.1,global sumup reduce
v2.1.1,copy back
v2.1.1,set global sumup info
v2.1.1,init global data count in leaf
v2.1.1,construct local histograms
v2.1.1,copy to buffer
v2.1.1,Reduce scatter for histogram
v2.1.1,restore global histograms from buffer
v2.1.1,find best threshold for smaller child
v2.1.1,only root leaf
v2.1.1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.1.1,find best threshold for larger child
v2.1.1,find local best split for larger leaf
v2.1.1,sync global best info
v2.1.1,set best split
v2.1.1,need update global number of data in leaf
v2.1.1,"instantiate template classes, otherwise linker cannot find the code"
v2.1.1,initialize SerialTreeLearner
v2.1.1,some additional variables needed for GPU trainer
v2.1.1,Initialize GPU buffers and kernels
v2.1.1,some functions used for debugging the GPU histogram construction
v2.1.1,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.1.1,goto err;
v2.1.1,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.1.1,goto err;
v2.1.1,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.1.1,also guarantee that there are at least 2K examples per workgroup
v2.1.1,return 0;
v2.1.1,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.1.1,decide the best number of workgroups working on one feature4 tuple
v2.1.1,set work group size based on feature size
v2.1.1,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.1.1,we need to refresh the kernel arguments after reallocating
v2.1.1,The only argument that needs to be changed later is num_data_
v2.1.1,"the GPU kernel will process all features in one call, and each"
v2.1.1,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.1.1,process one feature4 tuple
v2.1.1,"for the root node, indices are not copied"
v2.1.1,"for constant hessian, hessians are not copied except for the root node"
v2.1.1,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.1.1,and we will launch num_feature workgroups for this kernel
v2.1.1,will launch threads for all features
v2.1.1,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.1.1,copy the results asynchronously. Size depends on if double precision is used
v2.1.1,we will wait for this object in WaitAndGetHistograms
v2.1.1,"when the output is ready, the computation is done"
v2.1.1,values of this feature has been redistributed to multiple bins; need a reduction here
v2.1.1,how many feature-group tuples we have
v2.1.1,leave some safe margin for prefetching
v2.1.1,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.1.1,clear sparse/dense maps
v2.1.1,do nothing if no features can be processed on GPU
v2.1.1,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.1.1,unpin old buffer if necessary before destructing them
v2.1.1,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.1.1,allocate space for gradients and hessians on device
v2.1.1,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.1.1,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.1.1,copy indices to the device
v2.1.1,histogram bin entry size depends on the precision (single/double)
v2.1.1,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.1.1,each work group generates a sub-histogram of dword_features_ features.
v2.1.1,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.1.1,create atomic counters for inter-group coordination
v2.1.1,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.1.1,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.1.1,looking for dword_features_ non-sparse feature-groups
v2.1.1,decide if we need to redistribute the bin
v2.1.1,multiplier must be a power of 2
v2.1.1,device_bin_mults_.push_back(1);
v2.1.1,found
v2.1.1,for data transfer time
v2.1.1,"Now generate new data structure feature4, and copy data to the device"
v2.1.1,"preallocate arrays for all threads, and pin them"
v2.1.1,building Feature4 bundles; each thread handles dword_features_ features
v2.1.1,one feature datapoint is 4 bits
v2.1.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.1.1,one feature datapoint is one byte
v2.1.1,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.1.1,Dense bin
v2.1.1,Dense 4-bit bin
v2.1.1,working on the remaining (less than dword_features_) feature groups
v2.1.1,fill the leftover features
v2.1.1,"fill this empty feature with some ""random"" value"
v2.1.1,"fill this empty feature with some ""random"" value"
v2.1.1,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.1.1,deallocate pinned space for feature copying
v2.1.1,data transfer time
v2.1.1,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.1.1,"Something bad happened. Just return ""No log available."""
v2.1.1,"build is okay, log may contain warnings"
v2.1.1,destroy any old kernels
v2.1.1,create OpenCL kernels for different number of workgroups per feature
v2.1.1,currently we don't use constant memory
v2.1.1,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.1.1,kernel with indices in an array
v2.1.1,"kernel with all features enabled, with elimited branches"
v2.1.1,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.1.1,do nothing if no features can be processed on GPU
v2.1.1,The only argument that needs to be changed later is num_data_
v2.1.1,"hessian is passed as a parameter, but it is not available now."
v2.1.1,hessian will be set in BeforeTrain()
v2.1.1,"Get the max bin size, used for selecting best GPU kernel"
v2.1.1,initialize GPU
v2.1.1,determine which kernel to use based on the max number of bins
v2.1.1,setup GPU kernel arguments after we allocating all the buffers
v2.1.1,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.1.1,this should rarely occur
v2.1.1,GPU memory has to been reallocated because data may have been changed
v2.1.1,setup GPU kernel arguments after we allocating all the buffers
v2.1.1,Copy initial full hessians and gradients to GPU.
v2.1.1,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.1.1,setup hessian parameters only
v2.1.1,hessian is passed as a parameter
v2.1.1,use bagging
v2.1.1,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.1.1,copy used gradients and hessians to ordered buffer
v2.1.1,transfer the indices to GPU
v2.1.1,transfer hessian to GPU
v2.1.1,setup hessian parameters only
v2.1.1,hessian is passed as a parameter
v2.1.1,transfer gradients to GPU
v2.1.1,only have root
v2.1.1,"Copy indices, gradients and hessians as early as possible"
v2.1.1,only need to initialize for smaller leaf
v2.1.1,Get leaf boundary
v2.1.1,copy indices to the GPU:
v2.1.1,copy ordered hessians to the GPU:
v2.1.1,copy ordered gradients to the GPU:
v2.1.1,do nothing if no features can be processed on GPU
v2.1.1,copy data indices if it is not null
v2.1.1,generate and copy ordered_gradients if gradients is not null
v2.1.1,generate and copy ordered_hessians if hessians is not null
v2.1.1,converted indices in is_feature_used to feature-group indices
v2.1.1,construct the feature masks for dense feature-groups
v2.1.1,"if no feature group is used, just return and do not use GPU"
v2.1.1,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.1.1,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.1.1,"All data have been prepared, now run the GPU kernel"
v2.1.1,construct smaller leaf
v2.1.1,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.1.1,then construct sparse features on CPU
v2.1.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.1.1,"wait for GPU to finish, only if GPU is actually used"
v2.1.1,use double precision
v2.1.1,use single precision
v2.1.1,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.1.1,#define GPU_DEBUG_COMPARE
v2.1.1,construct larger leaf
v2.1.1,then construct sparse features on CPU
v2.1.1,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.1.1,"wait for GPU to finish, only if GPU is actually used"
v2.1.1,use double precision
v2.1.1,use single precision
v2.1.1,do some sanity check for the GPU algorithm
v2.1.1,limit top k
v2.1.1,get max bin
v2.1.1,calculate buffer size
v2.1.1,"left and right on same time, so need double size"
v2.1.1,initialize histograms for global
v2.1.1,sync global data sumup info
v2.1.1,set global sumup info
v2.1.1,init global data count in leaf
v2.1.1,get local sumup
v2.1.1,get local sumup
v2.1.1,get mean number on machines
v2.1.1,weighted gain
v2.1.1,get top k
v2.1.1,"Copy histogram to buffer, and Get local aggregate features"
v2.1.1,copy histograms.
v2.1.1,copy smaller leaf histograms first
v2.1.1,mark local aggregated feature
v2.1.1,copy
v2.1.1,then copy larger leaf histograms
v2.1.1,mark local aggregated feature
v2.1.1,copy
v2.1.1,use local data to find local best splits
v2.1.1,find splits
v2.1.1,only has root leaf
v2.1.1,find best threshold for larger child
v2.1.1,local voting
v2.1.1,gather
v2.1.1,get all top-k from all machines
v2.1.1,global voting
v2.1.1,copy local histgrams to buffer
v2.1.1,Reduce scatter for histogram
v2.1.1,find best split from local aggregated histograms
v2.1.1,restore from buffer
v2.1.1,find best threshold
v2.1.1,restore from buffer
v2.1.1,find best threshold
v2.1.1,find local best
v2.1.1,find local best split for larger leaf
v2.1.1,sync global best info
v2.1.1,copy back
v2.1.1,set the global number of data for leaves
v2.1.1,init the global sumup info
v2.1.1,"instantiate template classes, otherwise linker cannot find the code"
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable=invalid-name, exec-used, C0111"
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, W0105"
v2.1.0,create predictor first
v2.1.0,check dataset
v2.1.0,reduce cost for prediction training data
v2.1.0,process callbacks
v2.1.0,Most of legacy advanced options becomes callbacks
v2.1.0,construct booster
v2.1.0,start training
v2.1.0,check evaluation result.
v2.1.0,"lambdarank task, split according to groups"
v2.1.0,run preprocessing on the data set if needed
v2.1.0,setup callbacks
v2.1.0,coding: utf-8
v2.1.0,pylint: disable = C0103
v2.1.0,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.1.0,because of u'...' Unicode literals.
v2.1.0,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.1.0,minor change to support `**kwargs`
v2.1.0,sklearn interface has another naming convention
v2.1.0,"user can set verbose with kwargs, it has higher priority"
v2.1.0,reduce cost for prediction training data
v2.1.0,free dataset
v2.1.0,Switch to using a multiclass objective in the underlying LGBM instance
v2.1.0,check group data
v2.1.0,coding: utf-8
v2.1.0,we don't need lib_lightgbm while building docs
v2.1.0,coding: utf-8
v2.1.0,pylint: disable = C0103
v2.1.0,coding: utf-8
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, C0111, C0301"
v2.1.0,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.1.0,TypeError: obj is not a string or a number
v2.1.0,ValueError: invalid literal
v2.1.0,process for args
v2.1.0,get categorical features
v2.1.0,process for reference dataset
v2.1.0,start construct data
v2.1.0,check data has header or not
v2.1.0,load init score
v2.1.0,need re group init score
v2.1.0,set feature names
v2.1.0,"change non-float data to float data, need to copy"
v2.1.0,create valid
v2.1.0,construct subset
v2.1.0,create train
v2.1.0,set to None
v2.1.0,we're done if self and reference share a common upstrem reference
v2.1.0,"group data from LightGBM is boundaries data, need to convert to group size"
v2.1.0,Training task
v2.1.0,construct booster object
v2.1.0,save reference to data
v2.1.0,buffer for inner predict
v2.1.0,set network if necessary
v2.1.0,Prediction task
v2.1.0,need reset training data
v2.1.0,need to push new valid data
v2.1.0,Get name of features
v2.1.0,avoid to predict many time in one iteration
v2.1.0,Get num of inner evals
v2.1.0,Get name of evals
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, W0105, C0301"
v2.1.0,Callback environment used by callbacks
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, C0111"
v2.1.0,load or create your dataset
v2.1.0,create dataset for lightgbm
v2.1.0,"if you want to re-use data, remember to set free_raw_data=False"
v2.1.0,specify your configurations as a dict
v2.1.0,generate a feature name
v2.1.0,feature_name and categorical_feature
v2.1.0,check feature name
v2.1.0,save model to file
v2.1.0,dump model to json (and save to file)
v2.1.0,feature names
v2.1.0,feature importances
v2.1.0,load model to predict
v2.1.0,can only predict with the best iteration (or the saving iteration)
v2.1.0,eval with loaded model
v2.1.0,dump model with pickle
v2.1.0,load model with pickle to predict
v2.1.0,can predict with any iteration when loaded in pickle way
v2.1.0,eval with loaded model
v2.1.0,continue training
v2.1.0,init_model accepts:
v2.1.0,1. model file name
v2.1.0,2. Booster()
v2.1.0,decay learning rates
v2.1.0,learning_rates accepts:
v2.1.0,1. list/tuple with length = num_boost_round
v2.1.0,2. function(curr_iter)
v2.1.0,change other parameters during training
v2.1.0,self-defined objective function
v2.1.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.1.0,log likelihood loss
v2.1.0,self-defined eval metric
v2.1.0,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.1.0,binary error
v2.1.0,callback
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, C0111"
v2.1.0,load or create your dataset
v2.1.0,train
v2.1.0,predict
v2.1.0,eval
v2.1.0,feature importances
v2.1.0,other scikit-learn modules
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, C0111"
v2.1.0,load or create your dataset
v2.1.0,create dataset for lightgbm
v2.1.0,specify your configurations as a dict
v2.1.0,train
v2.1.0,coding: utf-8
v2.1.0,"pylint: disable = invalid-name, C0111"
v2.1.0,load or create your dataset
v2.1.0,create dataset for lightgbm
v2.1.0,specify your configurations as a dict
v2.1.0,train
v2.1.0,save model to file
v2.1.0,predict
v2.1.0,eval
v2.1.0,!/usr/bin/env python3
v2.1.0,-*- coding: utf-8 -*-
v2.1.0,
v2.1.0,"LightGBM documentation build configuration file, created by"
v2.1.0,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.1.0,
v2.1.0,This file is execfile()d with the current directory set to its
v2.1.0,containing dir.
v2.1.0,
v2.1.0,Note that not all possible configuration values are present in this
v2.1.0,autogenerated file.
v2.1.0,
v2.1.0,All configuration values have a default; values that are commented out
v2.1.0,serve to show the default.
v2.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
v2.1.0,add these directories to sys.path here. If the directory is relative to the
v2.1.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.1.0,
v2.1.0,-- mock out modules
v2.1.0,-- General configuration ------------------------------------------------
v2.1.0,"If your documentation needs a minimal Sphinx version, state it here."
v2.1.0,"Add any Sphinx extension module names here, as strings. They can be"
v2.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.1.0,ones.
v2.1.0,"Add any paths that contain templates here, relative to this directory."
v2.1.0,The suffix(es) of source filenames.
v2.1.0,You can specify multiple suffix as a list of string:
v2.1.0,"source_suffix = ['.rst', '.md']"
v2.1.0,The master toctree document.
v2.1.0,General information about the project.
v2.1.0,"The version info for the project you're documenting, acts as replacement for"
v2.1.0,"|version| and |release|, also used in various other places throughout the"
v2.1.0,built documents.
v2.1.0,
v2.1.0,The short X.Y version.
v2.1.0,"The full version, including alpha/beta/rc tags."
v2.1.0,The language for content autogenerated by Sphinx. Refer to documentation
v2.1.0,for a list of supported languages.
v2.1.0,
v2.1.0,This is also used if you do content translation via gettext catalogs.
v2.1.0,"Usually you set ""language"" from the command line for these cases."
v2.1.0,"List of patterns, relative to source directory, that match files and"
v2.1.0,directories to ignore when looking for source files.
v2.1.0,This patterns also effect to html_static_path and html_extra_path
v2.1.0,The name of the Pygments (syntax highlighting) style to use.
v2.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.1.0,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.1.0,-- Options for HTML output ----------------------------------------------
v2.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.1.0,a list of builtin themes.
v2.1.0,
v2.1.0,Theme options are theme-specific and customize the look and feel of a theme
v2.1.0,"further.  For a list of options available for each theme, see the"
v2.1.0,documentation.
v2.1.0,
v2.1.0,html_theme_options = {}
v2.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
v2.1.0,"relative to this directory. They are copied after the builtin static files,"
v2.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.1.0,-- Options for HTMLHelp output ------------------------------------------
v2.1.0,Output file base name for HTML help builder.
v2.1.0,coding: utf-8
v2.1.0,coding: utf-8
v2.1.0,pylint: skip-file
v2.1.0,we don't need lib_lightgbm while building docs
v2.1.0,coding: utf-8
v2.1.0,pylint: skip-file
v2.1.0,check saved model persistence
v2.1.0,"we need to check the consistency of model file here, so test for exact equal"
v2.1.0,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.1.0,"scores likely to be different, but prediction should still be the same"
v2.1.0,coding: utf-8
v2.1.0,pylint: skip-file
v2.1.0,Tests that `seed` is the same as `random_state`
v2.1.0,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.1.0,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.1.0,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.1.0,coding: utf-8
v2.1.0,pylint: skip-file
v2.1.0,coding: utf-8
v2.1.0,pylint: skip-file
v2.1.0,no early stopping
v2.1.0,early stopping occurs
v2.1.0,test custom eval metrics
v2.1.0,"shuffle = False, override metric in params"
v2.1.0,"shuffle = True, callbacks"
v2.1.0,self defined folds
v2.1.0,lambdarank
v2.1.0,test feature_names with whitespaces
v2.1.0,take subsets and train
v2.1.0,test sliced labels
v2.1.0,append some columns
v2.1.0,append some rows
v2.1.0,test sliced 2d matrix
v2.1.0,test sliced CSR
v2.1.0,coding: utf-8
v2.1.0,pylint: skip-file
v2.1.0,convert from one-based to  zero-based index
v2.1.0,convert from boundaries to size
v2.1.0,--- start Booster interfaces
v2.1.0,create boosting
v2.1.0,initialize the boosting
v2.1.0,create objective function
v2.1.0,initialize the objective function
v2.1.0,create training metric
v2.1.0,reset the boosting
v2.1.0,create objective function
v2.1.0,initialize the objective function
v2.1.0,some help functions used to convert data
v2.1.0,Row iterator of on column for CSC matrix
v2.1.0,"return value at idx, only can access by ascent order"
v2.1.0,"return next non-zero pair, if index < 0, means no more data"
v2.1.0,start of c_api functions
v2.1.0,sample data first
v2.1.0,sample data first
v2.1.0,sample data first
v2.1.0,no more data
v2.1.0,---- start of booster
v2.1.0,---- start of some help functions
v2.1.0,set number of threads for openmp
v2.1.0,check for alias
v2.1.0,read parameters from config file
v2.1.0,"remove str after ""#"""
v2.1.0,check for alias again
v2.1.0,load configs
v2.1.0,prediction is needed if using input initial model(continued train)
v2.1.0,need to continue training
v2.1.0,sync up random seed for data partition
v2.1.0,load Training data
v2.1.0,load data for parallel training
v2.1.0,load data for single machine
v2.1.0,need save binary file
v2.1.0,create training metric
v2.1.0,only when have metrics then need to construct validation data
v2.1.0,"Add validation data, if it exists"
v2.1.0,add
v2.1.0,need save binary file
v2.1.0,add metric for validation data
v2.1.0,output used time on each iteration
v2.1.0,need init network
v2.1.0,create boosting
v2.1.0,create objective function
v2.1.0,load training data
v2.1.0,initialize the objective function
v2.1.0,initialize the boosting
v2.1.0,add validation data into boosting
v2.1.0,convert model to if-else statement code
v2.1.0,create predictor
v2.1.0,Free memory
v2.1.0,create predictor
v2.1.0,counts for all labels
v2.1.0,"start from top label, and accumulate DCG"
v2.1.0,counts for all labels
v2.1.0,calculate k Max DCG by one pass
v2.1.0,get sorted indices by score
v2.1.0,calculate dcg
v2.1.0,get sorted indices by score
v2.1.0,calculate multi dcg by one pass
v2.1.0,wait for all client start up
v2.1.0,default set to -1
v2.1.0,"distance at k-th communication, distance[k] = 2^k"
v2.1.0,set incoming rank at k-th commuication
v2.1.0,set outgoing rank at k-th commuication
v2.1.0,defalut set as -1
v2.1.0,construct all recursive halving map for all machines
v2.1.0,let 1 << k <= num_machines
v2.1.0,distance of each communication
v2.1.0,"if num_machines = 2^k, don't need to group machines"
v2.1.0,"communication direction, %2 == 0 is positive"
v2.1.0,neighbor at k-th communication
v2.1.0,receive data block at k-th communication
v2.1.0,send data block at k-th communication
v2.1.0,"if num_machines != 2^k, need to group machines"
v2.1.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.1.0,let left machine as group leader
v2.1.0,"cache block information for groups, group with 2 machines will have double block size"
v2.1.0,convert from group to node leader
v2.1.0,convert from node to group
v2.1.0,meet new group
v2.1.0,add block len for this group
v2.1.0,calculate the group block start
v2.1.0,not need to construct
v2.1.0,get receive block informations
v2.1.0,accumulate block len
v2.1.0,get send block informations
v2.1.0,accumulate block len
v2.1.0,static member definition
v2.1.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.1.0,assign the blocks to every rank.
v2.1.0,do reduce scatter
v2.1.0,do all gather
v2.1.0,assign blocks
v2.1.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.1.0,copy back
v2.1.0,assign blocks
v2.1.0,start all gather
v2.1.0,when num_machines is small and data is large
v2.1.0,use output as receive buffer
v2.1.0,get current local block size
v2.1.0,get out rank
v2.1.0,get in rank
v2.1.0,get send information
v2.1.0,get recv information
v2.1.0,send and recv at same time
v2.1.0,rotate in-place
v2.1.0,use output as receive buffer
v2.1.0,get current local block size
v2.1.0,get send information
v2.1.0,get recv information
v2.1.0,send and recv at same time
v2.1.0,use output as receive buffer
v2.1.0,send and recv at same time
v2.1.0,send local data to neighbor first
v2.1.0,receive neighbor data first
v2.1.0,reduce
v2.1.0,get target
v2.1.0,get send information
v2.1.0,get recv information
v2.1.0,send and recv at same time
v2.1.0,reduce
v2.1.0,send result to neighbor
v2.1.0,receive result from neighbor
v2.1.0,copy result
v2.1.0,start up socket
v2.1.0,parse clients from file
v2.1.0,get ip list of local machine
v2.1.0,get local rank
v2.1.0,construct listener
v2.1.0,construct communication topo
v2.1.0,construct linkers
v2.1.0,free listener
v2.1.0,set timeout
v2.1.0,accept incoming socket
v2.1.0,receive rank
v2.1.0,add new socket
v2.1.0,save ranks that need to connect with
v2.1.0,start listener
v2.1.0,start connect
v2.1.0,let smaller rank connect to larger rank
v2.1.0,send local rank
v2.1.0,wait for listener
v2.1.0,print connected linkers
v2.1.0,Get some statistic from 2 line
v2.1.0,if only have one line on file
v2.1.0,get column names
v2.1.0,load label idx first
v2.1.0,erase label column name
v2.1.0,load ignore columns
v2.1.0,load weight idx
v2.1.0,load group idx
v2.1.0,don't support query id in data file when training in parallel
v2.1.0,read data to memory
v2.1.0,sample data
v2.1.0,construct feature bin mappers
v2.1.0,initialize label
v2.1.0,extract features
v2.1.0,sample data from file
v2.1.0,construct feature bin mappers
v2.1.0,initialize label
v2.1.0,extract features
v2.1.0,load data from binary file
v2.1.0,check meta data
v2.1.0,need to check training data
v2.1.0,read data in memory
v2.1.0,initialize label
v2.1.0,extract features
v2.1.0,Get number of lines of data file
v2.1.0,initialize label
v2.1.0,extract features
v2.1.0,load data from binary file
v2.1.0,not need to check validation data
v2.1.0,check meta data
v2.1.0,buffer to read binary file
v2.1.0,check token
v2.1.0,read size of header
v2.1.0,re-allocmate space if not enough
v2.1.0,read header
v2.1.0,get header
v2.1.0,num_groups
v2.1.0,real_feature_idx_
v2.1.0,feature2group
v2.1.0,feature2subfeature
v2.1.0,group_bin_boundaries
v2.1.0,group_feature_start_
v2.1.0,group_feature_cnt_
v2.1.0,get feature names
v2.1.0,write feature names
v2.1.0,read size of meta data
v2.1.0,re-allocate space if not enough
v2.1.0,read meta data
v2.1.0,load meta data
v2.1.0,sample local used data if need to partition
v2.1.0,"if not contain query file, minimal sample unit is one record"
v2.1.0,"if contain query file, minimal sample unit is one query"
v2.1.0,if is new query
v2.1.0,read feature data
v2.1.0,read feature size
v2.1.0,re-allocate space if not enough
v2.1.0,fill feature_names_ if not header
v2.1.0,"if only one machine, find bin locally"
v2.1.0,"if have multi-machines, need to find bin distributed"
v2.1.0,different machines will find bin for different features
v2.1.0,start and len will store the process feature indices for different machines
v2.1.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.1.0,get size of bin mapper with max_bin size
v2.1.0,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.1.0,find local feature bins and copy to buffer
v2.1.0,free
v2.1.0,convert to binary size
v2.1.0,gather global feature bin mappers
v2.1.0,restore features bins from buffer
v2.1.0,---- private functions ----
v2.1.0,"if features are ordered, not need to use hist_buf"
v2.1.0,read all lines
v2.1.0,get query data
v2.1.0,"if not contain query data, minimal sample unit is one record"
v2.1.0,"if contain query data, minimal sample unit is one query"
v2.1.0,if is new query
v2.1.0,get query data
v2.1.0,"if not contain query file, minimal sample unit is one record"
v2.1.0,"if contain query file, minimal sample unit is one query"
v2.1.0,if is new query
v2.1.0,parse features
v2.1.0,-1 means doesn't use this feature
v2.1.0,"check the range of label_idx, weight_idx and group_idx"
v2.1.0,fill feature_names_ if not header
v2.1.0,start find bins
v2.1.0,"if only one machine, find bin locally"
v2.1.0,"if have multi-machines, need to find bin distributed"
v2.1.0,different machines will find bin for different features
v2.1.0,start and len will store the process feature indices for different machines
v2.1.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.1.0,get size of bin mapper with max_bin size
v2.1.0,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.1.0,find local feature bins and copy to buffer
v2.1.0,free
v2.1.0,convert to binary size
v2.1.0,gather global feature bin mappers
v2.1.0,restore features bins from buffer
v2.1.0,if doesn't need to prediction with initial model
v2.1.0,parser
v2.1.0,set label
v2.1.0,free processed line:
v2.1.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.1.0,text_reader_->Lines()[i].shrink_to_fit();
v2.1.0,push data
v2.1.0,if is used feature
v2.1.0,if need to prediction with initial model
v2.1.0,parser
v2.1.0,set initial score
v2.1.0,set label
v2.1.0,free processed line:
v2.1.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.1.0,text_reader_->Lines()[i].shrink_to_fit();
v2.1.0,push data
v2.1.0,if is used feature
v2.1.0,metadata_ will manage space of init_score
v2.1.0,text data can be free after loaded feature values
v2.1.0,parser
v2.1.0,set initial score
v2.1.0,set label
v2.1.0,push data
v2.1.0,if is used feature
v2.1.0,only need part of data
v2.1.0,need full data
v2.1.0,metadata_ will manage space of init_score
v2.1.0,read size of token
v2.1.0,deep copy function for BinMapper
v2.1.0,mean size for one bin
v2.1.0,need a new bin
v2.1.0,update bin upper bound
v2.1.0,last bin upper bound
v2.1.0,find distinct_values first
v2.1.0,push zero in the front
v2.1.0,use the large value
v2.1.0,push zero in the back
v2.1.0,convert to int type first
v2.1.0,sort by counts
v2.1.0,avoid first bin is zero
v2.1.0,will ignore the categorical of small counts
v2.1.0,need an additional bin for NaN
v2.1.0,use -1 to represent NaN
v2.1.0,Use MissingType::None to represent this bin contains all categoricals
v2.1.0,check trival(num_bin_ == 1) feature
v2.1.0,check useless bin
v2.1.0,calculate sparse rate
v2.1.0,sparse threshold
v2.1.0,"for lambdarank, it needs query data for partition data in parallel learning"
v2.1.0,need convert query_id to boundaries
v2.1.0,check weights
v2.1.0,check query boundries
v2.1.0,contain initial score file
v2.1.0,check weights
v2.1.0,get local weights
v2.1.0,check query boundries
v2.1.0,get local query boundaries
v2.1.0,contain initial score file
v2.1.0,get local initial scores
v2.1.0,re-load query weight
v2.1.0,save to nullptr
v2.1.0,save to nullptr
v2.1.0,save to nullptr
v2.1.0,default weight file name
v2.1.0,default weight file name
v2.1.0,use first line to count number class
v2.1.0,default query file name
v2.1.0,root is in the depth 0
v2.1.0,non-leaf
v2.1.0,leaf
v2.1.0,use this for the missing value conversion
v2.1.0,Predict func by Map to ifelse
v2.1.0,use this for the missing value conversion
v2.1.0,non-leaf
v2.1.0,left subtree
v2.1.0,right subtree
v2.1.0,leaf
v2.1.0,non-leaf
v2.1.0,left subtree
v2.1.0,right subtree
v2.1.0,leaf
v2.1.0,recursive computation of SHAP values for a decision tree
v2.1.0,extend the unique path
v2.1.0,leaf node
v2.1.0,internal node
v2.1.0,"see if we have already split on this feature,"
v2.1.0,if so we undo that split so we can redo it for this node
v2.1.0,clear old metrics
v2.1.0,to lower
v2.1.0,split
v2.1.0,remove duplicate
v2.1.0,add names of objective function if not providing metric
v2.1.0,load main config types
v2.1.0,generate seeds by seed.
v2.1.0,sub-config setup
v2.1.0,check for conflicts
v2.1.0,"check if objective_type, metric_type, and num_class match"
v2.1.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.1.0,Check max_depth and num_leaves
v2.1.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.1.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.1.0,default eval ndcg @[1-5]
v2.1.0,"filter is based on sampling data, so decrease its range"
v2.1.0,put dense feature first
v2.1.0,sort by non zero cnt
v2.1.0,"sort by non zero cnt, bigger first"
v2.1.0,"take apart small sparse group, due it will not gain on speed"
v2.1.0,shuffle groups
v2.1.0,get num_features
v2.1.0,get bin_mappers
v2.1.0,copy feature bin mapper data
v2.1.0,copy feature bin mapper data
v2.1.0,"if not pass a filename, just append "".bin"" of original file"
v2.1.0,get size of header
v2.1.0,size of feature names
v2.1.0,write header
v2.1.0,write feature names
v2.1.0,get size of meta data
v2.1.0,write meta data
v2.1.0,write feature data
v2.1.0,get size of feature
v2.1.0,write feature
v2.1.0,feature is not used
v2.1.0,construct histograms for smaller leaf
v2.1.0,if not use ordered bin
v2.1.0,used ordered bin
v2.1.0,feature is not used
v2.1.0,construct histograms for smaller leaf
v2.1.0,if not use ordered bin
v2.1.0,used ordered bin
v2.1.0,fixed hessian.
v2.1.0,feature is not used
v2.1.0,construct histograms for smaller leaf
v2.1.0,if not use ordered bin
v2.1.0,used ordered bin
v2.1.0,feature is not used
v2.1.0,construct histograms for smaller leaf
v2.1.0,if not use ordered bin
v2.1.0,used ordered bin
v2.1.0,fixed hessian.
v2.1.0,PredictRaw
v2.1.0,PredictRawByMap
v2.1.0,Predict
v2.1.0,PredictByMap
v2.1.0,PredictLeafIndex
v2.1.0,PredictLeafIndexByMap
v2.1.0,output model type
v2.1.0,output number of class
v2.1.0,output label index
v2.1.0,output max_feature_idx
v2.1.0,output objective
v2.1.0,output tree models
v2.1.0,store the importance first
v2.1.0,sort the importance
v2.1.0,use serialized string to restore this object
v2.1.0,Use first 128 chars to avoid exceed the message buffer.
v2.1.0,get number of classes
v2.1.0,get index of label
v2.1.0,get max_feature_idx first
v2.1.0,get average_output
v2.1.0,get feature names
v2.1.0,set zero
v2.1.0,predict all the trees for one iteration
v2.1.0,check early stopping
v2.1.0,set zero
v2.1.0,predict all the trees for one iteration
v2.1.0,check early stopping
v2.1.0,margin_threshold will be captured by value
v2.1.0,copy and sort
v2.1.0,margin_threshold will be captured by value
v2.1.0,init tree learner
v2.1.0,push training metrics
v2.1.0,create buffer for gradients and hessians
v2.1.0,get max feature index
v2.1.0,get label index
v2.1.0,get feature names
v2.1.0,"if need bagging, create buffer"
v2.1.0,reset config for tree learner
v2.1.0,multi-class
v2.1.0,binary class
v2.1.0,"for a validation dataset, we need its score and metric"
v2.1.0,update score
v2.1.0,objective function will calculate gradients and hessians
v2.1.0,"random bagging, minimal unit is one record"
v2.1.0,if need bagging
v2.1.0,set bagging data to tree learner
v2.1.0,get subset
v2.1.0,output used time per iteration
v2.1.0,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.1.0,boosting first
v2.1.0,bagging logic
v2.1.0,need to copy gradients for bagging subset.
v2.1.0,shrinkage by learning rate
v2.1.0,update score
v2.1.0,only add default score one-time
v2.1.0,updates scores
v2.1.0,add model
v2.1.0,reset score
v2.1.0,remove model
v2.1.0,print message for metric
v2.1.0,pop last early_stopping_round_ models
v2.1.0,update training score
v2.1.0,we need to predict out-of-bag scores of data for boosting
v2.1.0,update validation score
v2.1.0,print training metric
v2.1.0,print validation metric
v2.1.0,set zero
v2.1.0,predict all the trees for one iteration
v2.1.0,check early stopping
v2.1.0,push training metrics
v2.1.0,"not same training data, need reset score and others"
v2.1.0,create score tracker
v2.1.0,update score
v2.1.0,create buffer for gradients and hessians
v2.1.0,"if need bagging, create buffer"
v2.1.0,Get the max size of pool
v2.1.0,at least need 2 leaves
v2.1.0,push split information for all leaves
v2.1.0,get ordered bin
v2.1.0,check existing for ordered bin
v2.1.0,initialize splits for leaf
v2.1.0,initialize data partition
v2.1.0,initialize ordered gradients and hessians
v2.1.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.1.0,get ordered bin
v2.1.0,initialize splits for leaf
v2.1.0,initialize data partition
v2.1.0,initialize ordered gradients and hessians
v2.1.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.1.0,Get the max size of pool
v2.1.0,at least need 2 leaves
v2.1.0,push split information for all leaves
v2.1.0,some initial works before training
v2.1.0,root leaf
v2.1.0,only root leaf can be splitted on first time
v2.1.0,some initial works before finding best split
v2.1.0,find best threshold for every feature
v2.1.0,Get a leaf with max split gain
v2.1.0,Get split information for best leaf
v2.1.0,"cannot split, quit"
v2.1.0,split tree with best leaf
v2.1.0,reset histogram pool
v2.1.0,at least use one feature
v2.1.0,initialize used features
v2.1.0,Get used feature at current tree
v2.1.0,initialize data partition
v2.1.0,reset the splits for leaves
v2.1.0,Sumup for root
v2.1.0,use all data
v2.1.0,"use bagging, only use part of data"
v2.1.0,"if has ordered bin, need to initialize the ordered bin"
v2.1.0,"use all data, pass nullptr"
v2.1.0,"bagging, only use part of data"
v2.1.0,mark used data
v2.1.0,initialize ordered bin
v2.1.0,check depth of current leaf
v2.1.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.1.0,no enough data to continue
v2.1.0,only have root
v2.1.0,put parent(left) leaf's histograms into larger leaf's histograms
v2.1.0,put parent(left) leaf's histograms to larger leaf's histograms
v2.1.0,split for the ordered bin
v2.1.0,mark data that at left-leaf
v2.1.0,split the ordered bin
v2.1.0,construct smaller leaf
v2.1.0,construct larger leaf
v2.1.0,find splits
v2.1.0,only has root leaf
v2.1.0,find best threshold for larger child
v2.1.0,left = parent
v2.1.0,"split tree, will return right leaf"
v2.1.0,init the leaves that used on next iteration
v2.1.0,bag_mapper[index_mapper[i]]
v2.1.0,get feature partition
v2.1.0,get local used features
v2.1.0,get best split at smaller leaf
v2.1.0,find local best split for larger leaf
v2.1.0,sync global best info
v2.1.0,update best split
v2.1.0,"instantiate template classes, otherwise linker cannot find the code"
v2.1.0,initialize SerialTreeLearner
v2.1.0,Get local rank and global machine size
v2.1.0,allocate buffer for communication
v2.1.0,generate feature partition for current tree
v2.1.0,get local used feature
v2.1.0,get block start and block len for reduce scatter
v2.1.0,get buffer_write_start_pos_
v2.1.0,get buffer_read_start_pos_
v2.1.0,sync global data sumup info
v2.1.0,global sumup reduce
v2.1.0,copy back
v2.1.0,set global sumup info
v2.1.0,init global data count in leaf
v2.1.0,construct local histograms
v2.1.0,copy to buffer
v2.1.0,Reduce scatter for histogram
v2.1.0,restore global histograms from buffer
v2.1.0,find best threshold for smaller child
v2.1.0,only root leaf
v2.1.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.1.0,find best threshold for larger child
v2.1.0,find local best split for larger leaf
v2.1.0,sync global best info
v2.1.0,set best split
v2.1.0,need update global number of data in leaf
v2.1.0,"instantiate template classes, otherwise linker cannot find the code"
v2.1.0,initialize SerialTreeLearner
v2.1.0,some additional variables needed for GPU trainer
v2.1.0,Initialize GPU buffers and kernels
v2.1.0,some functions used for debugging the GPU histogram construction
v2.1.0,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.1.0,goto err;
v2.1.0,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.1.0,goto err;
v2.1.0,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.1.0,also guarantee that there are at least 2K examples per workgroup
v2.1.0,return 0;
v2.1.0,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.1.0,decide the best number of workgroups working on one feature4 tuple
v2.1.0,set work group size based on feature size
v2.1.0,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.1.0,we need to refresh the kernel arguments after reallocating
v2.1.0,The only argument that needs to be changed later is num_data_
v2.1.0,"the GPU kernel will process all features in one call, and each"
v2.1.0,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.1.0,process one feature4 tuple
v2.1.0,"for the root node, indices are not copied"
v2.1.0,"for constant hessian, hessians are not copied except for the root node"
v2.1.0,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.1.0,and we will launch num_feature workgroups for this kernel
v2.1.0,will launch threads for all features
v2.1.0,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.1.0,copy the results asynchronously. Size depends on if double precision is used
v2.1.0,we will wait for this object in WaitAndGetHistograms
v2.1.0,"when the output is ready, the computation is done"
v2.1.0,values of this feature has been redistributed to multiple bins; need a reduction here
v2.1.0,how many feature-group tuples we have
v2.1.0,leave some safe margin for prefetching
v2.1.0,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.1.0,clear sparse/dense maps
v2.1.0,do nothing if no features can be processed on GPU
v2.1.0,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.1.0,unpin old buffer if necessary before destructing them
v2.1.0,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.1.0,allocate space for gradients and hessians on device
v2.1.0,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.1.0,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.1.0,copy indices to the device
v2.1.0,histogram bin entry size depends on the precision (single/double)
v2.1.0,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.1.0,each work group generates a sub-histogram of dword_features_ features.
v2.1.0,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.1.0,create atomic counters for inter-group coordination
v2.1.0,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.1.0,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.1.0,looking for dword_features_ non-sparse feature-groups
v2.1.0,decide if we need to redistribute the bin
v2.1.0,multiplier must be a power of 2
v2.1.0,device_bin_mults_.push_back(1);
v2.1.0,found
v2.1.0,for data transfer time
v2.1.0,"Now generate new data structure feature4, and copy data to the device"
v2.1.0,"preallocate arrays for all threads, and pin them"
v2.1.0,building Feature4 bundles; each thread handles dword_features_ features
v2.1.0,one feature datapoint is 4 bits
v2.1.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.1.0,one feature datapoint is one byte
v2.1.0,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.1.0,Dense bin
v2.1.0,Dense 4-bit bin
v2.1.0,working on the remaining (less than dword_features_) feature groups
v2.1.0,fill the leftover features
v2.1.0,"fill this empty feature with some ""random"" value"
v2.1.0,"fill this empty feature with some ""random"" value"
v2.1.0,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.1.0,deallocate pinned space for feature copying
v2.1.0,data transfer time
v2.1.0,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.1.0,"Something bad happened. Just return ""No log available."""
v2.1.0,"build is okay, log may contain warnings"
v2.1.0,destroy any old kernels
v2.1.0,create OpenCL kernels for different number of workgroups per feature
v2.1.0,currently we don't use constant memory
v2.1.0,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.1.0,kernel with indices in an array
v2.1.0,"kernel with all features enabled, with elimited branches"
v2.1.0,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.1.0,do nothing if no features can be processed on GPU
v2.1.0,The only argument that needs to be changed later is num_data_
v2.1.0,"hessian is passed as a parameter, but it is not available now."
v2.1.0,hessian will be set in BeforeTrain()
v2.1.0,"Get the max bin size, used for selecting best GPU kernel"
v2.1.0,initialize GPU
v2.1.0,determine which kernel to use based on the max number of bins
v2.1.0,setup GPU kernel arguments after we allocating all the buffers
v2.1.0,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.1.0,this should rarely occur
v2.1.0,GPU memory has to been reallocated because data may have been changed
v2.1.0,setup GPU kernel arguments after we allocating all the buffers
v2.1.0,Copy initial full hessians and gradients to GPU.
v2.1.0,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.1.0,setup hessian parameters only
v2.1.0,hessian is passed as a parameter
v2.1.0,use bagging
v2.1.0,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.1.0,copy used gradients and hessians to ordered buffer
v2.1.0,transfer the indices to GPU
v2.1.0,transfer hessian to GPU
v2.1.0,setup hessian parameters only
v2.1.0,hessian is passed as a parameter
v2.1.0,transfer gradients to GPU
v2.1.0,only have root
v2.1.0,"Copy indices, gradients and hessians as early as possible"
v2.1.0,only need to initialize for smaller leaf
v2.1.0,Get leaf boundary
v2.1.0,copy indices to the GPU:
v2.1.0,copy ordered hessians to the GPU:
v2.1.0,copy ordered gradients to the GPU:
v2.1.0,do nothing if no features can be processed on GPU
v2.1.0,copy data indices if it is not null
v2.1.0,generate and copy ordered_gradients if gradients is not null
v2.1.0,generate and copy ordered_hessians if hessians is not null
v2.1.0,converted indices in is_feature_used to feature-group indices
v2.1.0,construct the feature masks for dense feature-groups
v2.1.0,"if no feature group is used, just return and do not use GPU"
v2.1.0,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.1.0,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.1.0,"All data have been prepared, now run the GPU kernel"
v2.1.0,construct smaller leaf
v2.1.0,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.1.0,then construct sparse features on CPU
v2.1.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.1.0,"wait for GPU to finish, only if GPU is actually used"
v2.1.0,use double precision
v2.1.0,use single precision
v2.1.0,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.1.0,#define GPU_DEBUG_COMPARE
v2.1.0,construct larger leaf
v2.1.0,then construct sparse features on CPU
v2.1.0,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.1.0,"wait for GPU to finish, only if GPU is actually used"
v2.1.0,use double precision
v2.1.0,use single precision
v2.1.0,do some sanity check for the GPU algorithm
v2.1.0,limit top k
v2.1.0,get max bin
v2.1.0,calculate buffer size
v2.1.0,"left and right on same time, so need double size"
v2.1.0,initialize histograms for global
v2.1.0,sync global data sumup info
v2.1.0,set global sumup info
v2.1.0,init global data count in leaf
v2.1.0,get local sumup
v2.1.0,get local sumup
v2.1.0,get mean number on machines
v2.1.0,weighted gain
v2.1.0,get top k
v2.1.0,"Copy histogram to buffer, and Get local aggregate features"
v2.1.0,copy histograms.
v2.1.0,copy smaller leaf histograms first
v2.1.0,mark local aggregated feature
v2.1.0,copy
v2.1.0,then copy larger leaf histograms
v2.1.0,mark local aggregated feature
v2.1.0,copy
v2.1.0,use local data to find local best splits
v2.1.0,find splits
v2.1.0,only has root leaf
v2.1.0,find best threshold for larger child
v2.1.0,local voting
v2.1.0,gather
v2.1.0,get all top-k from all machines
v2.1.0,global voting
v2.1.0,copy local histgrams to buffer
v2.1.0,Reduce scatter for histogram
v2.1.0,find best split from local aggregated histograms
v2.1.0,restore from buffer
v2.1.0,find best threshold
v2.1.0,restore from buffer
v2.1.0,find best threshold
v2.1.0,find local best
v2.1.0,find local best split for larger leaf
v2.1.0,sync global best info
v2.1.0,copy back
v2.1.0,set the global number of data for leaves
v2.1.0,init the global sumup info
v2.1.0,"instantiate template classes, otherwise linker cannot find the code"
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable=invalid-name, exec-used, C0111"
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, W0105"
v2.0.12,create predictor first
v2.0.12,check dataset
v2.0.12,reduce cost for prediction training data
v2.0.12,process callbacks
v2.0.12,Most of legacy advanced options becomes callbacks
v2.0.12,construct booster
v2.0.12,start training
v2.0.12,check evaluation result.
v2.0.12,"lambdarank task, split according to groups"
v2.0.12,run preprocessing on the data set if needed
v2.0.12,setup callbacks
v2.0.12,coding: utf-8
v2.0.12,pylint: disable = C0103
v2.0.12,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.0.12,because of u'...' Unicode literals.
v2.0.12,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.0.12,minor change to support `**kwargs`
v2.0.12,sklearn interface has another naming convention
v2.0.12,"user can set verbose with kwargs, it has higher priority"
v2.0.12,reduce cost for prediction training data
v2.0.12,free dataset
v2.0.12,Switch to using a multiclass objective in the underlying LGBM instance
v2.0.12,check group data
v2.0.12,coding: utf-8
v2.0.12,we don't need lib_lightgbm while building docs
v2.0.12,coding: utf-8
v2.0.12,pylint: disable = C0103
v2.0.12,coding: utf-8
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, C0111, C0301"
v2.0.12,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.0.12,TypeError: obj is not a string or a number
v2.0.12,ValueError: invalid literal
v2.0.12,process for args
v2.0.12,get categorical features
v2.0.12,process for reference dataset
v2.0.12,start construct data
v2.0.12,check data has header or not
v2.0.12,load init score
v2.0.12,need re group init score
v2.0.12,set feature names
v2.0.12,"change non-float data to float data, need to copy"
v2.0.12,create valid
v2.0.12,construct subset
v2.0.12,create train
v2.0.12,set to None
v2.0.12,we're done if self and reference share a common upstrem reference
v2.0.12,"group data from LightGBM is boundaries data, need to convert to group size"
v2.0.12,Training task
v2.0.12,construct booster object
v2.0.12,save reference to data
v2.0.12,buffer for inner predict
v2.0.12,set network if necessary
v2.0.12,Prediction task
v2.0.12,need reset training data
v2.0.12,need to push new valid data
v2.0.12,Get name of features
v2.0.12,avoid to predict many time in one iteration
v2.0.12,Get num of inner evals
v2.0.12,Get name of evals
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, W0105, C0301"
v2.0.12,Callback environment used by callbacks
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, C0111"
v2.0.12,load or create your dataset
v2.0.12,create dataset for lightgbm
v2.0.12,"if you want to re-use data, remember to set free_raw_data=False"
v2.0.12,specify your configurations as a dict
v2.0.12,generate a feature name
v2.0.12,feature_name and categorical_feature
v2.0.12,check feature name
v2.0.12,save model to file
v2.0.12,dump model to json (and save to file)
v2.0.12,feature names
v2.0.12,feature importances
v2.0.12,load model to predict
v2.0.12,can only predict with the best iteration (or the saving iteration)
v2.0.12,eval with loaded model
v2.0.12,dump model with pickle
v2.0.12,load model with pickle to predict
v2.0.12,can predict with any iteration when loaded in pickle way
v2.0.12,eval with loaded model
v2.0.12,continue training
v2.0.12,init_model accepts:
v2.0.12,1. model file name
v2.0.12,2. Booster()
v2.0.12,decay learning rates
v2.0.12,learning_rates accepts:
v2.0.12,1. list/tuple with length = num_boost_round
v2.0.12,2. function(curr_iter)
v2.0.12,change other parameters during training
v2.0.12,self-defined objective function
v2.0.12,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.0.12,log likelihood loss
v2.0.12,self-defined eval metric
v2.0.12,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.0.12,binary error
v2.0.12,callback
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, C0111"
v2.0.12,load or create your dataset
v2.0.12,train
v2.0.12,predict
v2.0.12,eval
v2.0.12,feature importances
v2.0.12,other scikit-learn modules
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, C0111"
v2.0.12,load or create your dataset
v2.0.12,create dataset for lightgbm
v2.0.12,specify your configurations as a dict
v2.0.12,train
v2.0.12,coding: utf-8
v2.0.12,"pylint: disable = invalid-name, C0111"
v2.0.12,load or create your dataset
v2.0.12,create dataset for lightgbm
v2.0.12,specify your configurations as a dict
v2.0.12,train
v2.0.12,save model to file
v2.0.12,predict
v2.0.12,eval
v2.0.12,!/usr/bin/env python3
v2.0.12,-*- coding: utf-8 -*-
v2.0.12,
v2.0.12,"LightGBM documentation build configuration file, created by"
v2.0.12,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.0.12,
v2.0.12,This file is execfile()d with the current directory set to its
v2.0.12,containing dir.
v2.0.12,
v2.0.12,Note that not all possible configuration values are present in this
v2.0.12,autogenerated file.
v2.0.12,
v2.0.12,All configuration values have a default; values that are commented out
v2.0.12,serve to show the default.
v2.0.12,"If extensions (or modules to document with autodoc) are in another directory,"
v2.0.12,add these directories to sys.path here. If the directory is relative to the
v2.0.12,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.0.12,
v2.0.12,-- mock out modules
v2.0.12,-- General configuration ------------------------------------------------
v2.0.12,"If your documentation needs a minimal Sphinx version, state it here."
v2.0.12,"Add any Sphinx extension module names here, as strings. They can be"
v2.0.12,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.0.12,ones.
v2.0.12,"Add any paths that contain templates here, relative to this directory."
v2.0.12,The suffix(es) of source filenames.
v2.0.12,You can specify multiple suffix as a list of string:
v2.0.12,"source_suffix = ['.rst', '.md']"
v2.0.12,The master toctree document.
v2.0.12,General information about the project.
v2.0.12,"The version info for the project you're documenting, acts as replacement for"
v2.0.12,"|version| and |release|, also used in various other places throughout the"
v2.0.12,built documents.
v2.0.12,
v2.0.12,The short X.Y version.
v2.0.12,"The full version, including alpha/beta/rc tags."
v2.0.12,The language for content autogenerated by Sphinx. Refer to documentation
v2.0.12,for a list of supported languages.
v2.0.12,
v2.0.12,This is also used if you do content translation via gettext catalogs.
v2.0.12,"Usually you set ""language"" from the command line for these cases."
v2.0.12,"List of patterns, relative to source directory, that match files and"
v2.0.12,directories to ignore when looking for source files.
v2.0.12,This patterns also effect to html_static_path and html_extra_path
v2.0.12,The name of the Pygments (syntax highlighting) style to use.
v2.0.12,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.0.12,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.0.12,-- Options for HTML output ----------------------------------------------
v2.0.12,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.0.12,a list of builtin themes.
v2.0.12,
v2.0.12,Theme options are theme-specific and customize the look and feel of a theme
v2.0.12,"further.  For a list of options available for each theme, see the"
v2.0.12,documentation.
v2.0.12,
v2.0.12,html_theme_options = {}
v2.0.12,"Add any paths that contain custom static files (such as style sheets) here,"
v2.0.12,"relative to this directory. They are copied after the builtin static files,"
v2.0.12,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.0.12,-- Options for HTMLHelp output ------------------------------------------
v2.0.12,Output file base name for HTML help builder.
v2.0.12,coding: utf-8
v2.0.12,coding: utf-8
v2.0.12,pylint: skip-file
v2.0.12,we don't need lib_lightgbm while building docs
v2.0.12,coding: utf-8
v2.0.12,pylint: skip-file
v2.0.12,check saved model persistence
v2.0.12,"we need to check the consistency of model file here, so test for exact equal"
v2.0.12,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.0.12,"scores likely to be different, but prediction should still be the same"
v2.0.12,coding: utf-8
v2.0.12,pylint: skip-file
v2.0.12,Tests that `seed` is the same as `random_state`
v2.0.12,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.0.12,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.0.12,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.0.12,coding: utf-8
v2.0.12,pylint: skip-file
v2.0.12,coding: utf-8
v2.0.12,pylint: skip-file
v2.0.12,no early stopping
v2.0.12,early stopping occurs
v2.0.12,test custom eval metrics
v2.0.12,"shuffle = False, override metric in params"
v2.0.12,"shuffle = True, callbacks"
v2.0.12,self defined folds
v2.0.12,lambdarank
v2.0.12,test feature_names with whitespaces
v2.0.12,take subsets and train
v2.0.12,coding: utf-8
v2.0.12,pylint: skip-file
v2.0.12,convert from one-based to  zero-based index
v2.0.12,convert from boundaries to size
v2.0.12,--- start Booster interfaces
v2.0.12,create boosting
v2.0.12,initialize the boosting
v2.0.12,create objective function
v2.0.12,initialize the objective function
v2.0.12,create training metric
v2.0.12,reset the boosting
v2.0.12,create objective function
v2.0.12,initialize the objective function
v2.0.12,some help functions used to convert data
v2.0.12,Row iterator of on column for CSC matrix
v2.0.12,"return value at idx, only can access by ascent order"
v2.0.12,"return next non-zero pair, if index < 0, means no more data"
v2.0.12,start of c_api functions
v2.0.12,sample data first
v2.0.12,sample data first
v2.0.12,sample data first
v2.0.12,no more data
v2.0.12,---- start of booster
v2.0.12,---- start of some help functions
v2.0.12,set number of threads for openmp
v2.0.12,check for alias
v2.0.12,read parameters from config file
v2.0.12,"remove str after ""#"""
v2.0.12,check for alias again
v2.0.12,load configs
v2.0.12,prediction is needed if using input initial model(continued train)
v2.0.12,need to continue training
v2.0.12,sync up random seed for data partition
v2.0.12,load Training data
v2.0.12,load data for parallel training
v2.0.12,load data for single machine
v2.0.12,need save binary file
v2.0.12,create training metric
v2.0.12,only when have metrics then need to construct validation data
v2.0.12,"Add validation data, if it exists"
v2.0.12,add
v2.0.12,need save binary file
v2.0.12,add metric for validation data
v2.0.12,output used time on each iteration
v2.0.12,need init network
v2.0.12,create boosting
v2.0.12,create objective function
v2.0.12,load training data
v2.0.12,initialize the objective function
v2.0.12,initialize the boosting
v2.0.12,add validation data into boosting
v2.0.12,convert model to if-else statement code
v2.0.12,create predictor
v2.0.12,Free memory
v2.0.12,create predictor
v2.0.12,counts for all labels
v2.0.12,"start from top label, and accumulate DCG"
v2.0.12,counts for all labels
v2.0.12,calculate k Max DCG by one pass
v2.0.12,get sorted indices by score
v2.0.12,calculate dcg
v2.0.12,get sorted indices by score
v2.0.12,calculate multi dcg by one pass
v2.0.12,wait for all client start up
v2.0.12,default set to -1
v2.0.12,"distance at k-th communication, distance[k] = 2^k"
v2.0.12,set incoming rank at k-th commuication
v2.0.12,set outgoing rank at k-th commuication
v2.0.12,defalut set as -1
v2.0.12,construct all recursive halving map for all machines
v2.0.12,let 1 << k <= num_machines
v2.0.12,distance of each communication
v2.0.12,"if num_machines = 2^k, don't need to group machines"
v2.0.12,"communication direction, %2 == 0 is positive"
v2.0.12,neighbor at k-th communication
v2.0.12,receive data block at k-th communication
v2.0.12,send data block at k-th communication
v2.0.12,static member definition
v2.0.12,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.0.12,assign the blocks to every rank.
v2.0.12,do reduce scatter
v2.0.12,do all gather
v2.0.12,assign blocks
v2.0.12,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.0.12,copy back
v2.0.12,assign blocks
v2.0.12,start all gather
v2.0.12,use output as receive buffer
v2.0.12,get current local block size
v2.0.12,get out rank
v2.0.12,get in rank
v2.0.12,get send information
v2.0.12,get recv information
v2.0.12,send and recv at same time
v2.0.12,rotate in-place
v2.0.12,get target
v2.0.12,get send information
v2.0.12,get recv information
v2.0.12,send and recv at same time
v2.0.12,reduce
v2.0.12,copy result
v2.0.12,start up socket
v2.0.12,parse clients from file
v2.0.12,get ip list of local machine
v2.0.12,get local rank
v2.0.12,construct listener
v2.0.12,construct communication topo
v2.0.12,construct linkers
v2.0.12,free listener
v2.0.12,set timeout
v2.0.12,accept incoming socket
v2.0.12,receive rank
v2.0.12,add new socket
v2.0.12,save ranks that need to connect with
v2.0.12,start listener
v2.0.12,start connect
v2.0.12,let smaller rank connect to larger rank
v2.0.12,send local rank
v2.0.12,wait for listener
v2.0.12,print connected linkers
v2.0.12,Get some statistic from 2 line
v2.0.12,if only have one line on file
v2.0.12,get column names
v2.0.12,load label idx first
v2.0.12,erase label column name
v2.0.12,load ignore columns
v2.0.12,load weight idx
v2.0.12,load group idx
v2.0.12,don't support query id in data file when training in parallel
v2.0.12,read data to memory
v2.0.12,sample data
v2.0.12,construct feature bin mappers
v2.0.12,initialize label
v2.0.12,extract features
v2.0.12,sample data from file
v2.0.12,construct feature bin mappers
v2.0.12,initialize label
v2.0.12,extract features
v2.0.12,load data from binary file
v2.0.12,check meta data
v2.0.12,need to check training data
v2.0.12,read data in memory
v2.0.12,initialize label
v2.0.12,extract features
v2.0.12,Get number of lines of data file
v2.0.12,initialize label
v2.0.12,extract features
v2.0.12,load data from binary file
v2.0.12,not need to check validation data
v2.0.12,check meta data
v2.0.12,buffer to read binary file
v2.0.12,check token
v2.0.12,read size of header
v2.0.12,re-allocmate space if not enough
v2.0.12,read header
v2.0.12,get header
v2.0.12,num_groups
v2.0.12,real_feature_idx_
v2.0.12,feature2group
v2.0.12,feature2subfeature
v2.0.12,group_bin_boundaries
v2.0.12,group_feature_start_
v2.0.12,group_feature_cnt_
v2.0.12,get feature names
v2.0.12,write feature names
v2.0.12,read size of meta data
v2.0.12,re-allocate space if not enough
v2.0.12,read meta data
v2.0.12,load meta data
v2.0.12,sample local used data if need to partition
v2.0.12,"if not contain query file, minimal sample unit is one record"
v2.0.12,"if contain query file, minimal sample unit is one query"
v2.0.12,if is new query
v2.0.12,read feature data
v2.0.12,read feature size
v2.0.12,re-allocate space if not enough
v2.0.12,fill feature_names_ if not header
v2.0.12,"if only one machine, find bin locally"
v2.0.12,"if have multi-machines, need to find bin distributed"
v2.0.12,different machines will find bin for different features
v2.0.12,start and len will store the process feature indices for different machines
v2.0.12,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.0.12,get size of bin mapper with max_bin size
v2.0.12,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.0.12,find local feature bins and copy to buffer
v2.0.12,free
v2.0.12,convert to binary size
v2.0.12,gather global feature bin mappers
v2.0.12,restore features bins from buffer
v2.0.12,---- private functions ----
v2.0.12,"if features are ordered, not need to use hist_buf"
v2.0.12,read all lines
v2.0.12,get query data
v2.0.12,"if not contain query data, minimal sample unit is one record"
v2.0.12,"if contain query data, minimal sample unit is one query"
v2.0.12,if is new query
v2.0.12,get query data
v2.0.12,"if not contain query file, minimal sample unit is one record"
v2.0.12,"if contain query file, minimal sample unit is one query"
v2.0.12,if is new query
v2.0.12,parse features
v2.0.12,-1 means doesn't use this feature
v2.0.12,"check the range of label_idx, weight_idx and group_idx"
v2.0.12,fill feature_names_ if not header
v2.0.12,start find bins
v2.0.12,"if only one machine, find bin locally"
v2.0.12,"if have multi-machines, need to find bin distributed"
v2.0.12,different machines will find bin for different features
v2.0.12,start and len will store the process feature indices for different machines
v2.0.12,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.0.12,get size of bin mapper with max_bin size
v2.0.12,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.0.12,find local feature bins and copy to buffer
v2.0.12,free
v2.0.12,convert to binary size
v2.0.12,gather global feature bin mappers
v2.0.12,restore features bins from buffer
v2.0.12,if doesn't need to prediction with initial model
v2.0.12,parser
v2.0.12,set label
v2.0.12,free processed line:
v2.0.12,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.12,text_reader_->Lines()[i].shrink_to_fit();
v2.0.12,push data
v2.0.12,if is used feature
v2.0.12,if need to prediction with initial model
v2.0.12,parser
v2.0.12,set initial score
v2.0.12,set label
v2.0.12,free processed line:
v2.0.12,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.12,text_reader_->Lines()[i].shrink_to_fit();
v2.0.12,push data
v2.0.12,if is used feature
v2.0.12,metadata_ will manage space of init_score
v2.0.12,text data can be free after loaded feature values
v2.0.12,parser
v2.0.12,set initial score
v2.0.12,set label
v2.0.12,push data
v2.0.12,if is used feature
v2.0.12,only need part of data
v2.0.12,need full data
v2.0.12,metadata_ will manage space of init_score
v2.0.12,read size of token
v2.0.12,deep copy function for BinMapper
v2.0.12,mean size for one bin
v2.0.12,need a new bin
v2.0.12,update bin upper bound
v2.0.12,last bin upper bound
v2.0.12,find distinct_values first
v2.0.12,push zero in the front
v2.0.12,use the large value
v2.0.12,push zero in the back
v2.0.12,convert to int type first
v2.0.12,sort by counts
v2.0.12,avoid first bin is zero
v2.0.12,will ignore the categorical of small counts
v2.0.12,need an additional bin for NaN
v2.0.12,use -1 to represent NaN
v2.0.12,Use MissingType::None to represent this bin contains all categoricals
v2.0.12,check trival(num_bin_ == 1) feature
v2.0.12,check useless bin
v2.0.12,calculate sparse rate
v2.0.12,sparse threshold
v2.0.12,"for lambdarank, it needs query data for partition data in parallel learning"
v2.0.12,need convert query_id to boundaries
v2.0.12,check weights
v2.0.12,check query boundries
v2.0.12,contain initial score file
v2.0.12,check weights
v2.0.12,get local weights
v2.0.12,check query boundries
v2.0.12,get local query boundaries
v2.0.12,contain initial score file
v2.0.12,get local initial scores
v2.0.12,re-load query weight
v2.0.12,save to nullptr
v2.0.12,save to nullptr
v2.0.12,save to nullptr
v2.0.12,default weight file name
v2.0.12,default weight file name
v2.0.12,use first line to count number class
v2.0.12,default query file name
v2.0.12,root is in the depth 0
v2.0.12,non-leaf
v2.0.12,leaf
v2.0.12,use this for the missing value conversion
v2.0.12,Predict func by Map to ifelse
v2.0.12,use this for the missing value conversion
v2.0.12,non-leaf
v2.0.12,left subtree
v2.0.12,right subtree
v2.0.12,leaf
v2.0.12,non-leaf
v2.0.12,left subtree
v2.0.12,right subtree
v2.0.12,leaf
v2.0.12,recursive computation of SHAP values for a decision tree
v2.0.12,extend the unique path
v2.0.12,leaf node
v2.0.12,internal node
v2.0.12,"see if we have already split on this feature,"
v2.0.12,if so we undo that split so we can redo it for this node
v2.0.12,clear old metrics
v2.0.12,to lower
v2.0.12,split
v2.0.12,remove duplicate
v2.0.12,load main config types
v2.0.12,generate seeds by seed.
v2.0.12,sub-config setup
v2.0.12,check for conflicts
v2.0.12,"check if objective_type, metric_type, and num_class match"
v2.0.12,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.0.12,Check max_depth and num_leaves
v2.0.12,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.12,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.12,default eval ndcg @[1-5]
v2.0.12,"filter is based on sampling data, so decrease its range"
v2.0.12,put dense feature first
v2.0.12,sort by non zero cnt
v2.0.12,"sort by non zero cnt, bigger first"
v2.0.12,"take apart small sparse group, due it will not gain on speed"
v2.0.12,shuffle groups
v2.0.12,get num_features
v2.0.12,get bin_mappers
v2.0.12,copy feature bin mapper data
v2.0.12,copy feature bin mapper data
v2.0.12,"if not pass a filename, just append "".bin"" of original file"
v2.0.12,get size of header
v2.0.12,size of feature names
v2.0.12,write header
v2.0.12,write feature names
v2.0.12,get size of meta data
v2.0.12,write meta data
v2.0.12,write feature data
v2.0.12,get size of feature
v2.0.12,write feature
v2.0.12,feature is not used
v2.0.12,construct histograms for smaller leaf
v2.0.12,if not use ordered bin
v2.0.12,used ordered bin
v2.0.12,feature is not used
v2.0.12,construct histograms for smaller leaf
v2.0.12,if not use ordered bin
v2.0.12,used ordered bin
v2.0.12,fixed hessian.
v2.0.12,feature is not used
v2.0.12,construct histograms for smaller leaf
v2.0.12,if not use ordered bin
v2.0.12,used ordered bin
v2.0.12,feature is not used
v2.0.12,construct histograms for smaller leaf
v2.0.12,if not use ordered bin
v2.0.12,used ordered bin
v2.0.12,fixed hessian.
v2.0.12,PredictRaw
v2.0.12,PredictRawByMap
v2.0.12,Predict
v2.0.12,PredictByMap
v2.0.12,PredictLeafIndex
v2.0.12,PredictLeafIndexByMap
v2.0.12,output model type
v2.0.12,output number of class
v2.0.12,output label index
v2.0.12,output max_feature_idx
v2.0.12,output objective
v2.0.12,output tree models
v2.0.12,store the importance first
v2.0.12,sort the importance
v2.0.12,use serialized string to restore this object
v2.0.12,Use first 128 chars to avoid exceed the message buffer.
v2.0.12,get number of classes
v2.0.12,get index of label
v2.0.12,get max_feature_idx first
v2.0.12,get average_output
v2.0.12,get feature names
v2.0.12,set zero
v2.0.12,predict all the trees for one iteration
v2.0.12,check early stopping
v2.0.12,set zero
v2.0.12,predict all the trees for one iteration
v2.0.12,check early stopping
v2.0.12,margin_threshold will be captured by value
v2.0.12,copy and sort
v2.0.12,margin_threshold will be captured by value
v2.0.12,init tree learner
v2.0.12,push training metrics
v2.0.12,create buffer for gradients and hessians
v2.0.12,get max feature index
v2.0.12,get label index
v2.0.12,get feature names
v2.0.12,"if need bagging, create buffer"
v2.0.12,reset config for tree learner
v2.0.12,multi-class
v2.0.12,binary class
v2.0.12,"for a validation dataset, we need its score and metric"
v2.0.12,update score
v2.0.12,objective function will calculate gradients and hessians
v2.0.12,"random bagging, minimal unit is one record"
v2.0.12,if need bagging
v2.0.12,set bagging data to tree learner
v2.0.12,get subset
v2.0.12,output used time per iteration
v2.0.12,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.0.12,boosting first
v2.0.12,bagging logic
v2.0.12,need to copy gradients for bagging subset.
v2.0.12,shrinkage by learning rate
v2.0.12,update score
v2.0.12,only add default score one-time
v2.0.12,updates scores
v2.0.12,add model
v2.0.12,reset score
v2.0.12,remove model
v2.0.12,print message for metric
v2.0.12,pop last early_stopping_round_ models
v2.0.12,update training score
v2.0.12,we need to predict out-of-bag scores of data for boosting
v2.0.12,update validation score
v2.0.12,print training metric
v2.0.12,print validation metric
v2.0.12,set zero
v2.0.12,predict all the trees for one iteration
v2.0.12,check early stopping
v2.0.12,push training metrics
v2.0.12,"not same training data, need reset score and others"
v2.0.12,create score tracker
v2.0.12,update score
v2.0.12,create buffer for gradients and hessians
v2.0.12,"if need bagging, create buffer"
v2.0.12,Get the max size of pool
v2.0.12,at least need 2 leaves
v2.0.12,push split information for all leaves
v2.0.12,get ordered bin
v2.0.12,check existing for ordered bin
v2.0.12,initialize splits for leaf
v2.0.12,initialize data partition
v2.0.12,initialize ordered gradients and hessians
v2.0.12,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.12,get ordered bin
v2.0.12,initialize splits for leaf
v2.0.12,initialize data partition
v2.0.12,initialize ordered gradients and hessians
v2.0.12,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.12,Get the max size of pool
v2.0.12,at least need 2 leaves
v2.0.12,push split information for all leaves
v2.0.12,some initial works before training
v2.0.12,root leaf
v2.0.12,only root leaf can be splitted on first time
v2.0.12,some initial works before finding best split
v2.0.12,find best threshold for every feature
v2.0.12,Get a leaf with max split gain
v2.0.12,Get split information for best leaf
v2.0.12,"cannot split, quit"
v2.0.12,split tree with best leaf
v2.0.12,reset histogram pool
v2.0.12,at least use one feature
v2.0.12,initialize used features
v2.0.12,Get used feature at current tree
v2.0.12,initialize data partition
v2.0.12,reset the splits for leaves
v2.0.12,Sumup for root
v2.0.12,use all data
v2.0.12,"use bagging, only use part of data"
v2.0.12,"if has ordered bin, need to initialize the ordered bin"
v2.0.12,"use all data, pass nullptr"
v2.0.12,"bagging, only use part of data"
v2.0.12,mark used data
v2.0.12,initialize ordered bin
v2.0.12,check depth of current leaf
v2.0.12,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.0.12,no enough data to continue
v2.0.12,only have root
v2.0.12,put parent(left) leaf's histograms into larger leaf's histograms
v2.0.12,put parent(left) leaf's histograms to larger leaf's histograms
v2.0.12,split for the ordered bin
v2.0.12,mark data that at left-leaf
v2.0.12,split the ordered bin
v2.0.12,construct smaller leaf
v2.0.12,construct larger leaf
v2.0.12,find splits
v2.0.12,only has root leaf
v2.0.12,find best threshold for larger child
v2.0.12,left = parent
v2.0.12,"split tree, will return right leaf"
v2.0.12,init the leaves that used on next iteration
v2.0.12,get feature partition
v2.0.12,get local used features
v2.0.12,get best split at smaller leaf
v2.0.12,find local best split for larger leaf
v2.0.12,sync global best info
v2.0.12,update best split
v2.0.12,"instantiate template classes, otherwise linker cannot find the code"
v2.0.12,initialize SerialTreeLearner
v2.0.12,Get local rank and global machine size
v2.0.12,allocate buffer for communication
v2.0.12,generate feature partition for current tree
v2.0.12,get local used feature
v2.0.12,get block start and block len for reduce scatter
v2.0.12,get buffer_write_start_pos_
v2.0.12,get buffer_read_start_pos_
v2.0.12,sync global data sumup info
v2.0.12,global sumup reduce
v2.0.12,copy back
v2.0.12,set global sumup info
v2.0.12,init global data count in leaf
v2.0.12,construct local histograms
v2.0.12,copy to buffer
v2.0.12,Reduce scatter for histogram
v2.0.12,restore global histograms from buffer
v2.0.12,find best threshold for smaller child
v2.0.12,only root leaf
v2.0.12,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.0.12,find best threshold for larger child
v2.0.12,find local best split for larger leaf
v2.0.12,sync global best info
v2.0.12,set best split
v2.0.12,need update global number of data in leaf
v2.0.12,"instantiate template classes, otherwise linker cannot find the code"
v2.0.12,initialize SerialTreeLearner
v2.0.12,some additional variables needed for GPU trainer
v2.0.12,Initialize GPU buffers and kernels
v2.0.12,some functions used for debugging the GPU histogram construction
v2.0.12,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.0.12,goto err;
v2.0.12,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.0.12,goto err;
v2.0.12,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.0.12,also guarantee that there are at least 2K examples per workgroup
v2.0.12,return 0;
v2.0.12,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.0.12,decide the best number of workgroups working on one feature4 tuple
v2.0.12,set work group size based on feature size
v2.0.12,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.0.12,we need to refresh the kernel arguments after reallocating
v2.0.12,The only argument that needs to be changed later is num_data_
v2.0.12,"the GPU kernel will process all features in one call, and each"
v2.0.12,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.0.12,process one feature4 tuple
v2.0.12,"for the root node, indices are not copied"
v2.0.12,"for constant hessian, hessians are not copied except for the root node"
v2.0.12,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.0.12,and we will launch num_feature workgroups for this kernel
v2.0.12,will launch threads for all features
v2.0.12,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.0.12,copy the results asynchronously. Size depends on if double precision is used
v2.0.12,we will wait for this object in WaitAndGetHistograms
v2.0.12,"when the output is ready, the computation is done"
v2.0.12,values of this feature has been redistributed to multiple bins; need a reduction here
v2.0.12,how many feature-group tuples we have
v2.0.12,leave some safe margin for prefetching
v2.0.12,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.0.12,clear sparse/dense maps
v2.0.12,do nothing if no features can be processed on GPU
v2.0.12,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.0.12,unpin old buffer if necessary before destructing them
v2.0.12,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.0.12,allocate space for gradients and hessians on device
v2.0.12,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.0.12,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.0.12,copy indices to the device
v2.0.12,histogram bin entry size depends on the precision (single/double)
v2.0.12,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.0.12,each work group generates a sub-histogram of dword_features_ features.
v2.0.12,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.0.12,create atomic counters for inter-group coordination
v2.0.12,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.0.12,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.0.12,looking for dword_features_ non-sparse feature-groups
v2.0.12,decide if we need to redistribute the bin
v2.0.12,multiplier must be a power of 2
v2.0.12,device_bin_mults_.push_back(1);
v2.0.12,found
v2.0.12,for data transfer time
v2.0.12,"Now generate new data structure feature4, and copy data to the device"
v2.0.12,"preallocate arrays for all threads, and pin them"
v2.0.12,building Feature4 bundles; each thread handles dword_features_ features
v2.0.12,one feature datapoint is 4 bits
v2.0.12,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.12,one feature datapoint is one byte
v2.0.12,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.12,Dense bin
v2.0.12,Dense 4-bit bin
v2.0.12,working on the remaining (less than dword_features_) feature groups
v2.0.12,fill the leftover features
v2.0.12,"fill this empty feature with some ""random"" value"
v2.0.12,"fill this empty feature with some ""random"" value"
v2.0.12,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.0.12,deallocate pinned space for feature copying
v2.0.12,data transfer time
v2.0.12,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.0.12,"Something bad happened. Just return ""No log available."""
v2.0.12,"build is okay, log may contain warnings"
v2.0.12,destroy any old kernels
v2.0.12,create OpenCL kernels for different number of workgroups per feature
v2.0.12,currently we don't use constant memory
v2.0.12,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.0.12,kernel with indices in an array
v2.0.12,"kernel with all features enabled, with elimited branches"
v2.0.12,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.0.12,do nothing if no features can be processed on GPU
v2.0.12,The only argument that needs to be changed later is num_data_
v2.0.12,"hessian is passed as a parameter, but it is not available now."
v2.0.12,hessian will be set in BeforeTrain()
v2.0.12,"Get the max bin size, used for selecting best GPU kernel"
v2.0.12,initialize GPU
v2.0.12,determine which kernel to use based on the max number of bins
v2.0.12,setup GPU kernel arguments after we allocating all the buffers
v2.0.12,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.0.12,this should rarely occur
v2.0.12,GPU memory has to been reallocated because data may have been changed
v2.0.12,setup GPU kernel arguments after we allocating all the buffers
v2.0.12,Copy initial full hessians and gradients to GPU.
v2.0.12,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.0.12,setup hessian parameters only
v2.0.12,hessian is passed as a parameter
v2.0.12,use bagging
v2.0.12,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.0.12,copy used gradients and hessians to ordered buffer
v2.0.12,transfer the indices to GPU
v2.0.12,transfer hessian to GPU
v2.0.12,setup hessian parameters only
v2.0.12,hessian is passed as a parameter
v2.0.12,transfer gradients to GPU
v2.0.12,only have root
v2.0.12,"Copy indices, gradients and hessians as early as possible"
v2.0.12,only need to initialize for smaller leaf
v2.0.12,Get leaf boundary
v2.0.12,copy indices to the GPU:
v2.0.12,copy ordered hessians to the GPU:
v2.0.12,copy ordered gradients to the GPU:
v2.0.12,do nothing if no features can be processed on GPU
v2.0.12,copy data indices if it is not null
v2.0.12,generate and copy ordered_gradients if gradients is not null
v2.0.12,generate and copy ordered_hessians if hessians is not null
v2.0.12,converted indices in is_feature_used to feature-group indices
v2.0.12,construct the feature masks for dense feature-groups
v2.0.12,"if no feature group is used, just return and do not use GPU"
v2.0.12,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.0.12,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.0.12,"All data have been prepared, now run the GPU kernel"
v2.0.12,construct smaller leaf
v2.0.12,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.0.12,then construct sparse features on CPU
v2.0.12,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.12,"wait for GPU to finish, only if GPU is actually used"
v2.0.12,use double precision
v2.0.12,use single precision
v2.0.12,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.0.12,#define GPU_DEBUG_COMPARE
v2.0.12,construct larger leaf
v2.0.12,then construct sparse features on CPU
v2.0.12,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.12,"wait for GPU to finish, only if GPU is actually used"
v2.0.12,use double precision
v2.0.12,use single precision
v2.0.12,do some sanity check for the GPU algorithm
v2.0.12,limit top k
v2.0.12,get max bin
v2.0.12,calculate buffer size
v2.0.12,"left and right on same time, so need double size"
v2.0.12,initialize histograms for global
v2.0.12,sync global data sumup info
v2.0.12,set global sumup info
v2.0.12,init global data count in leaf
v2.0.12,get local sumup
v2.0.12,get local sumup
v2.0.12,get mean number on machines
v2.0.12,weighted gain
v2.0.12,get top k
v2.0.12,"Copy histogram to buffer, and Get local aggregate features"
v2.0.12,copy histograms.
v2.0.12,copy smaller leaf histograms first
v2.0.12,mark local aggregated feature
v2.0.12,copy
v2.0.12,then copy larger leaf histograms
v2.0.12,mark local aggregated feature
v2.0.12,copy
v2.0.12,use local data to find local best splits
v2.0.12,find splits
v2.0.12,only has root leaf
v2.0.12,find best threshold for larger child
v2.0.12,local voting
v2.0.12,gather
v2.0.12,get all top-k from all machines
v2.0.12,global voting
v2.0.12,copy local histgrams to buffer
v2.0.12,Reduce scatter for histogram
v2.0.12,find best split from local aggregated histograms
v2.0.12,restore from buffer
v2.0.12,find best threshold
v2.0.12,restore from buffer
v2.0.12,find best threshold
v2.0.12,find local best
v2.0.12,find local best split for larger leaf
v2.0.12,sync global best info
v2.0.12,copy back
v2.0.12,set the global number of data for leaves
v2.0.12,init the global sumup info
v2.0.12,"instantiate template classes, otherwise linker cannot find the code"
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable=invalid-name, exec-used, C0111"
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, W0105"
v2.0.10,Most of legacy advanced options becomes callbacks
v2.0.10,check evaluation result.
v2.0.10,"lambdarank task, split according to groups"
v2.0.10,run preprocessing on the data set if needed
v2.0.10,setup callbacks
v2.0.10,coding: utf-8
v2.0.10,pylint: disable = C0103
v2.0.10,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.0.10,because of u'...' Unicode literals.
v2.0.10,LGBMDeprecated = None  Don't uncomment it because it causes error without installed sklearn
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.0.10,"DeprecationWarning is not shown by default, so let's create our own with higher level"
v2.0.10,minor change to support `**kwargs`
v2.0.10,sklearn interface has another naming convention
v2.0.10,"user can set verbose with kwargs, it has higher priority"
v2.0.10,free dataset
v2.0.10,Switch to using a multiclass objective in the underlying LGBM instance
v2.0.10,check group data
v2.0.10,coding: utf-8
v2.0.10,we don't need lib_lightgbm while building docs
v2.0.10,coding: utf-8
v2.0.10,pylint: disable = C0103
v2.0.10,coding: utf-8
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, C0111, C0301"
v2.0.10,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.0.10,TypeError: obj is not a string or a number
v2.0.10,ValueError: invalid literal
v2.0.10,load init score
v2.0.10,need re group init score
v2.0.10,set feature names
v2.0.10,we're done if self and reference share a common upstrem reference
v2.0.10,"group data from LightGBM is boundaries data, need to convert to group size"
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, W0105, C0301"
v2.0.10,Callback environment used by callbacks
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, C0111"
v2.0.10,load or create your dataset
v2.0.10,create dataset for lightgbm
v2.0.10,"if you want to re-use data, remember to set free_raw_data=False"
v2.0.10,specify your configurations as a dict
v2.0.10,generate a feature name
v2.0.10,feature_name and categorical_feature
v2.0.10,check feature name
v2.0.10,save model to file
v2.0.10,dump model to json (and save to file)
v2.0.10,feature names
v2.0.10,feature importances
v2.0.10,load model to predict
v2.0.10,can only predict with the best iteration (or the saving iteration)
v2.0.10,eval with loaded model
v2.0.10,dump model with pickle
v2.0.10,load model with pickle to predict
v2.0.10,can predict with any iteration when loaded in pickle way
v2.0.10,eval with loaded model
v2.0.10,continue training
v2.0.10,init_model accepts:
v2.0.10,1. model file name
v2.0.10,2. Booster()
v2.0.10,decay learning rates
v2.0.10,learning_rates accepts:
v2.0.10,1. list/tuple with length = num_boost_round
v2.0.10,2. function(curr_iter)
v2.0.10,change other parameters during training
v2.0.10,self-defined objective function
v2.0.10,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.0.10,log likelihood loss
v2.0.10,self-defined eval metric
v2.0.10,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.0.10,binary error
v2.0.10,callback
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, C0111"
v2.0.10,load or create your dataset
v2.0.10,train
v2.0.10,predict
v2.0.10,eval
v2.0.10,feature importances
v2.0.10,other scikit-learn modules
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, C0111"
v2.0.10,load or create your dataset
v2.0.10,create dataset for lightgbm
v2.0.10,specify your configurations as a dict
v2.0.10,train
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = invalid-name, C0111"
v2.0.10,load or create your dataset
v2.0.10,create dataset for lightgbm
v2.0.10,specify your configurations as a dict
v2.0.10,train
v2.0.10,save model to file
v2.0.10,predict
v2.0.10,eval
v2.0.10,coding: utf-8
v2.0.10,"pylint: disable = C0111, C0103"
v2.0.10,print out the pmml for a decision tree
v2.0.10,specify the objective as function name and binarySplit for
v2.0.10,splitCharacteristic because each node has 2 children
v2.0.10,"list each feature name as a mining field, and treat all outliers as is,"
v2.0.10,unless specified
v2.0.10,begin printing out the decision tree
v2.0.10,open the model file and then process it
v2.0.10,ignore first 6 and empty lines
v2.0.10,print out data dictionary entries for each column
v2.0.10,"not adding any interval definition, all values are currently"
v2.0.10,valid
v2.0.10,"list each feature name as a mining field, and treat all outliers"
v2.0.10,"as is, unless specified"
v2.0.10,read each array that contains pertinent information for the pmml
v2.0.10,these arrays will be used to recreate the traverse the decision tree
v2.0.10,!/usr/bin/env python3
v2.0.10,-*- coding: utf-8 -*-
v2.0.10,
v2.0.10,"LightGBM documentation build configuration file, created by"
v2.0.10,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.0.10,
v2.0.10,This file is execfile()d with the current directory set to its
v2.0.10,containing dir.
v2.0.10,
v2.0.10,Note that not all possible configuration values are present in this
v2.0.10,autogenerated file.
v2.0.10,
v2.0.10,All configuration values have a default; values that are commented out
v2.0.10,serve to show the default.
v2.0.10,"If extensions (or modules to document with autodoc) are in another directory,"
v2.0.10,add these directories to sys.path here. If the directory is relative to the
v2.0.10,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.0.10,
v2.0.10,-- mock out modules
v2.0.10,-- General configuration ------------------------------------------------
v2.0.10,"If your documentation needs a minimal Sphinx version, state it here."
v2.0.10,"Add any Sphinx extension module names here, as strings. They can be"
v2.0.10,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.0.10,ones.
v2.0.10,"Add any paths that contain templates here, relative to this directory."
v2.0.10,The suffix(es) of source filenames.
v2.0.10,You can specify multiple suffix as a list of string:
v2.0.10,"source_suffix = ['.rst', '.md']"
v2.0.10,The master toctree document.
v2.0.10,General information about the project.
v2.0.10,"The version info for the project you're documenting, acts as replacement for"
v2.0.10,"|version| and |release|, also used in various other places throughout the"
v2.0.10,built documents.
v2.0.10,
v2.0.10,The short X.Y version.
v2.0.10,"The full version, including alpha/beta/rc tags."
v2.0.10,The language for content autogenerated by Sphinx. Refer to documentation
v2.0.10,for a list of supported languages.
v2.0.10,
v2.0.10,This is also used if you do content translation via gettext catalogs.
v2.0.10,"Usually you set ""language"" from the command line for these cases."
v2.0.10,"List of patterns, relative to source directory, that match files and"
v2.0.10,directories to ignore when looking for source files.
v2.0.10,This patterns also effect to html_static_path and html_extra_path
v2.0.10,The name of the Pygments (syntax highlighting) style to use.
v2.0.10,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.0.10,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.0.10,-- Options for HTML output ----------------------------------------------
v2.0.10,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.0.10,a list of builtin themes.
v2.0.10,
v2.0.10,Theme options are theme-specific and customize the look and feel of a theme
v2.0.10,"further.  For a list of options available for each theme, see the"
v2.0.10,documentation.
v2.0.10,
v2.0.10,html_theme_options = {}
v2.0.10,"Add any paths that contain custom static files (such as style sheets) here,"
v2.0.10,"relative to this directory. They are copied after the builtin static files,"
v2.0.10,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.0.10,-- Options for HTMLHelp output ------------------------------------------
v2.0.10,Output file base name for HTML help builder.
v2.0.10,-- Options for LaTeX output ---------------------------------------------
v2.0.10,The paper size ('letterpaper' or 'a4paper').
v2.0.10,
v2.0.10,"'papersize': 'letterpaper',"
v2.0.10,"The font size ('10pt', '11pt' or '12pt')."
v2.0.10,
v2.0.10,"'pointsize': '10pt',"
v2.0.10,Additional stuff for the LaTeX preamble.
v2.0.10,
v2.0.10,"'preamble': '',"
v2.0.10,Latex figure (float) alignment
v2.0.10,
v2.0.10,"'figure_align': 'htbp',"
v2.0.10,Grouping the document tree into LaTeX files. List of tuples
v2.0.10,"(source start file, target name, title,"
v2.0.10,"author, documentclass [howto, manual, or own class])."
v2.0.10,latex_documents = [
v2.0.10,"(master_doc, 'LightGBM.tex', 'LightGBM Documentation',"
v2.0.10,"'Microsoft Corporation', 'manual'),"
v2.0.10,]
v2.0.10,-- Options for manual page output ---------------------------------------
v2.0.10,One entry per manual page. List of tuples
v2.0.10,"(source start file, name, description, authors, manual section)."
v2.0.10,man_pages = [
v2.0.10,"(master_doc, 'lightgbm', 'LightGBM Documentation',"
v2.0.10,"[author], 1)"
v2.0.10,]
v2.0.10,-- Options for Texinfo output -------------------------------------------
v2.0.10,Grouping the document tree into Texinfo files. List of tuples
v2.0.10,"(source start file, target name, title, author,"
v2.0.10,"dir menu entry, description, category)"
v2.0.10,texinfo_documents = [
v2.0.10,"(master_doc, 'LightGBM', 'LightGBM Documentation',"
v2.0.10,"author, 'LightGBM', 'One line description of project.',"
v2.0.10,"'Miscellaneous'),"
v2.0.10,]
v2.0.10,coding: utf-8
v2.0.10,coding: utf-8
v2.0.10,pylint: skip-file
v2.0.10,we don't need lib_lightgbm while building docs
v2.0.10,coding: utf-8
v2.0.10,pylint: skip-file
v2.0.10,check saved model persistence
v2.0.10,"we need to check the consistency of model file here, so test for exact equal"
v2.0.10,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.0.10,"scores likely to be different, but prediction should still be the same"
v2.0.10,coding: utf-8
v2.0.10,pylint: skip-file
v2.0.10,Tests that `seed` is the same as `random_state`
v2.0.10,"sklearn <0.19 cannot accept instance, but many tests could be passed only with min_data=1 and min_data_in_bin=1"
v2.0.10,we cannot use `check_estimator` directly since there is no skip test mechanism
v2.0.10,we cannot leave default params (see https://github.com/Microsoft/LightGBM/issues/833)
v2.0.10,coding: utf-8
v2.0.10,pylint: skip-file
v2.0.10,coding: utf-8
v2.0.10,pylint: skip-file
v2.0.10,no early stopping
v2.0.10,early stopping occurs
v2.0.10,test custom eval metrics
v2.0.10,"shuffle = False, override metric in params"
v2.0.10,"shuffle = True, callbacks"
v2.0.10,self defined folds
v2.0.10,lambdarank
v2.0.10,test feature_names with whitespaces
v2.0.10,take subsets and train
v2.0.10,convert from one-based to  zero-based index
v2.0.10,convert from boundaries to size
v2.0.10,--- start Booster interfaces
v2.0.10,create boosting
v2.0.10,initialize the boosting
v2.0.10,create objective function
v2.0.10,initialize the objective function
v2.0.10,create training metric
v2.0.10,reset the boosting
v2.0.10,create objective function
v2.0.10,initialize the objective function
v2.0.10,some help functions used to convert data
v2.0.10,Row iterator of on column for CSC matrix
v2.0.10,"return value at idx, only can access by ascent order"
v2.0.10,"return next non-zero pair, if index < 0, means no more data"
v2.0.10,start of c_api functions
v2.0.10,sample data first
v2.0.10,sample data first
v2.0.10,sample data first
v2.0.10,no more data
v2.0.10,---- start of booster
v2.0.10,---- start of some help functions
v2.0.10,set number of threads for openmp
v2.0.10,check for alias
v2.0.10,read parameters from config file
v2.0.10,"remove str after ""#"""
v2.0.10,check for alias again
v2.0.10,load configs
v2.0.10,prediction is needed if using input initial model(continued train)
v2.0.10,need to continue training
v2.0.10,sync up random seed for data partition
v2.0.10,load Training data
v2.0.10,load data for parallel training
v2.0.10,load data for single machine
v2.0.10,need save binary file
v2.0.10,create training metric
v2.0.10,only when have metrics then need to construct validation data
v2.0.10,"Add validation data, if it exists"
v2.0.10,add
v2.0.10,need save binary file
v2.0.10,add metric for validation data
v2.0.10,output used time on each iteration
v2.0.10,need init network
v2.0.10,create boosting
v2.0.10,create objective function
v2.0.10,load training data
v2.0.10,initialize the objective function
v2.0.10,initialize the boosting
v2.0.10,add validation data into boosting
v2.0.10,convert model to if-else statement code
v2.0.10,create predictor
v2.0.10,counts for all labels
v2.0.10,"start from top label, and accumulate DCG"
v2.0.10,counts for all labels
v2.0.10,calculate k Max DCG by one pass
v2.0.10,get sorted indices by score
v2.0.10,calculate dcg
v2.0.10,get sorted indices by score
v2.0.10,calculate multi dcg by one pass
v2.0.10,wait for all client start up
v2.0.10,default set to -1
v2.0.10,"distance at k-th communication, distance[k] = 2^k"
v2.0.10,set incoming rank at k-th commuication
v2.0.10,set outgoing rank at k-th commuication
v2.0.10,defalut set as -1
v2.0.10,construct all recursive halving map for all machines
v2.0.10,let 1 << k <= num_machines
v2.0.10,distance of each communication
v2.0.10,"if num_machines = 2^k, don't need to group machines"
v2.0.10,"communication direction, %2 == 0 is positive"
v2.0.10,neighbor at k-th communication
v2.0.10,receive data block at k-th communication
v2.0.10,send data block at k-th communication
v2.0.10,static member definition
v2.0.10,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.0.10,assign the blocks to every rank.
v2.0.10,do reduce scatter
v2.0.10,do all gather
v2.0.10,assign blocks
v2.0.10,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.0.10,copy back
v2.0.10,assign blocks
v2.0.10,start all gather
v2.0.10,use output as receive buffer
v2.0.10,get current local block size
v2.0.10,get out rank
v2.0.10,get in rank
v2.0.10,get send information
v2.0.10,get recv information
v2.0.10,send and recv at same time
v2.0.10,rotate in-place
v2.0.10,get target
v2.0.10,get send information
v2.0.10,get recv information
v2.0.10,send and recv at same time
v2.0.10,reduce
v2.0.10,copy result
v2.0.10,start up socket
v2.0.10,parse clients from file
v2.0.10,get ip list of local machine
v2.0.10,get local rank
v2.0.10,construct listener
v2.0.10,construct communication topo
v2.0.10,construct linkers
v2.0.10,free listener
v2.0.10,set timeout
v2.0.10,accept incoming socket
v2.0.10,receive rank
v2.0.10,add new socket
v2.0.10,save ranks that need to connect with
v2.0.10,start listener
v2.0.10,start connect
v2.0.10,let smaller rank connect to larger rank
v2.0.10,send local rank
v2.0.10,wait for listener
v2.0.10,print connected linkers
v2.0.10,Get some statistic from 2 line
v2.0.10,if only have one line on file
v2.0.10,get column names
v2.0.10,load label idx first
v2.0.10,erase label column name
v2.0.10,load ignore columns
v2.0.10,load weight idx
v2.0.10,load group idx
v2.0.10,don't support query id in data file when training in parallel
v2.0.10,read data to memory
v2.0.10,sample data
v2.0.10,construct feature bin mappers
v2.0.10,initialize label
v2.0.10,extract features
v2.0.10,sample data from file
v2.0.10,construct feature bin mappers
v2.0.10,initialize label
v2.0.10,extract features
v2.0.10,load data from binary file
v2.0.10,check meta data
v2.0.10,need to check training data
v2.0.10,read data in memory
v2.0.10,initialize label
v2.0.10,extract features
v2.0.10,Get number of lines of data file
v2.0.10,initialize label
v2.0.10,extract features
v2.0.10,load data from binary file
v2.0.10,not need to check validation data
v2.0.10,check meta data
v2.0.10,buffer to read binary file
v2.0.10,check token
v2.0.10,read size of header
v2.0.10,re-allocmate space if not enough
v2.0.10,read header
v2.0.10,get header
v2.0.10,num_groups
v2.0.10,real_feature_idx_
v2.0.10,feature2group
v2.0.10,feature2subfeature
v2.0.10,group_bin_boundaries
v2.0.10,group_feature_start_
v2.0.10,group_feature_cnt_
v2.0.10,get feature names
v2.0.10,write feature names
v2.0.10,read size of meta data
v2.0.10,re-allocate space if not enough
v2.0.10,read meta data
v2.0.10,load meta data
v2.0.10,sample local used data if need to partition
v2.0.10,"if not contain query file, minimal sample unit is one record"
v2.0.10,"if contain query file, minimal sample unit is one query"
v2.0.10,if is new query
v2.0.10,read feature data
v2.0.10,read feature size
v2.0.10,re-allocate space if not enough
v2.0.10,fill feature_names_ if not header
v2.0.10,"if only one machine, find bin locally"
v2.0.10,"if have multi-machines, need to find bin distributed"
v2.0.10,different machines will find bin for different features
v2.0.10,start and len will store the process feature indices for different machines
v2.0.10,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.0.10,get size of bin mapper with max_bin size
v2.0.10,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.0.10,find local feature bins and copy to buffer
v2.0.10,free
v2.0.10,convert to binary size
v2.0.10,gather global feature bin mappers
v2.0.10,restore features bins from buffer
v2.0.10,---- private functions ----
v2.0.10,"if features are ordered, not need to use hist_buf"
v2.0.10,read all lines
v2.0.10,get query data
v2.0.10,"if not contain query data, minimal sample unit is one record"
v2.0.10,"if contain query data, minimal sample unit is one query"
v2.0.10,if is new query
v2.0.10,get query data
v2.0.10,"if not contain query file, minimal sample unit is one record"
v2.0.10,"if contain query file, minimal sample unit is one query"
v2.0.10,if is new query
v2.0.10,parse features
v2.0.10,-1 means doesn't use this feature
v2.0.10,"check the range of label_idx, weight_idx and group_idx"
v2.0.10,fill feature_names_ if not header
v2.0.10,start find bins
v2.0.10,"if only one machine, find bin locally"
v2.0.10,"if have multi-machines, need to find bin distributed"
v2.0.10,different machines will find bin for different features
v2.0.10,start and len will store the process feature indices for different machines
v2.0.10,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.0.10,get size of bin mapper with max_bin size
v2.0.10,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.0.10,find local feature bins and copy to buffer
v2.0.10,free
v2.0.10,convert to binary size
v2.0.10,gather global feature bin mappers
v2.0.10,restore features bins from buffer
v2.0.10,if doesn't need to prediction with initial model
v2.0.10,parser
v2.0.10,set label
v2.0.10,free processed line:
v2.0.10,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.10,text_reader_->Lines()[i].shrink_to_fit();
v2.0.10,push data
v2.0.10,if is used feature
v2.0.10,if need to prediction with initial model
v2.0.10,parser
v2.0.10,set initial score
v2.0.10,set label
v2.0.10,free processed line:
v2.0.10,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.10,text_reader_->Lines()[i].shrink_to_fit();
v2.0.10,push data
v2.0.10,if is used feature
v2.0.10,metadata_ will manage space of init_score
v2.0.10,text data can be free after loaded feature values
v2.0.10,parser
v2.0.10,set initial score
v2.0.10,set label
v2.0.10,push data
v2.0.10,if is used feature
v2.0.10,only need part of data
v2.0.10,need full data
v2.0.10,metadata_ will manage space of init_score
v2.0.10,read size of token
v2.0.10,deep copy function for BinMapper
v2.0.10,mean size for one bin
v2.0.10,need a new bin
v2.0.10,update bin upper bound
v2.0.10,last bin upper bound
v2.0.10,find distinct_values first
v2.0.10,push zero in the front
v2.0.10,push zero in the back
v2.0.10,convert to int type first
v2.0.10,sort by counts
v2.0.10,avoid first bin is zero
v2.0.10,will ignore the categorical of small counts
v2.0.10,need an additional bin for NaN
v2.0.10,use -1 to represent NaN
v2.0.10,Use MissingType::None to represent this bin contains all categoricals
v2.0.10,check trival(num_bin_ == 1) feature
v2.0.10,check useless bin
v2.0.10,calculate sparse rate
v2.0.10,sparse threshold
v2.0.10,"for lambdarank, it needs query data for partition data in parallel learning"
v2.0.10,need convert query_id to boundaries
v2.0.10,check weights
v2.0.10,check query boundries
v2.0.10,contain initial score file
v2.0.10,check weights
v2.0.10,get local weights
v2.0.10,check query boundries
v2.0.10,get local query boundaries
v2.0.10,contain initial score file
v2.0.10,get local initial scores
v2.0.10,re-load query weight
v2.0.10,save to nullptr
v2.0.10,save to nullptr
v2.0.10,save to nullptr
v2.0.10,default weight file name
v2.0.10,default weight file name
v2.0.10,use first line to count number class
v2.0.10,default query file name
v2.0.10,root is in the depth 0
v2.0.10,non-leaf
v2.0.10,leaf
v2.0.10,use this for the missing value conversion
v2.0.10,non-leaf
v2.0.10,left subtree
v2.0.10,right subtree
v2.0.10,leaf
v2.0.10,recursive computation of SHAP values for a decision tree
v2.0.10,extend the unique path
v2.0.10,leaf node
v2.0.10,internal node
v2.0.10,"see if we have already split on this feature,"
v2.0.10,if so we undo that split so we can redo it for this node
v2.0.10,clear old metrics
v2.0.10,to lower
v2.0.10,split
v2.0.10,remove duplicate
v2.0.10,load main config types
v2.0.10,generate seeds by seed.
v2.0.10,sub-config setup
v2.0.10,check for conflicts
v2.0.10,"check if objective_type, metric_type, and num_class match"
v2.0.10,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.0.10,Check max_depth and num_leaves
v2.0.10,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.10,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.10,default eval ndcg @[1-5]
v2.0.10,"filter is based on sampling data, so decrease its range"
v2.0.10,put dense feature first
v2.0.10,sort by non zero cnt
v2.0.10,"sort by non zero cnt, bigger first"
v2.0.10,"take apart small sparse group, due it will not gain on speed"
v2.0.10,shuffle groups
v2.0.10,get num_features
v2.0.10,get bin_mappers
v2.0.10,copy feature bin mapper data
v2.0.10,copy feature bin mapper data
v2.0.10,"if not pass a filename, just append "".bin"" of original file"
v2.0.10,get size of header
v2.0.10,size of feature names
v2.0.10,write header
v2.0.10,write feature names
v2.0.10,get size of meta data
v2.0.10,write meta data
v2.0.10,write feature data
v2.0.10,get size of feature
v2.0.10,write feature
v2.0.10,feature is not used
v2.0.10,construct histograms for smaller leaf
v2.0.10,if not use ordered bin
v2.0.10,used ordered bin
v2.0.10,feature is not used
v2.0.10,construct histograms for smaller leaf
v2.0.10,if not use ordered bin
v2.0.10,used ordered bin
v2.0.10,fixed hessian.
v2.0.10,feature is not used
v2.0.10,construct histograms for smaller leaf
v2.0.10,if not use ordered bin
v2.0.10,used ordered bin
v2.0.10,feature is not used
v2.0.10,construct histograms for smaller leaf
v2.0.10,if not use ordered bin
v2.0.10,used ordered bin
v2.0.10,fixed hessian.
v2.0.10,set zero
v2.0.10,predict all the trees for one iteration
v2.0.10,check early stopping
v2.0.10,PredictRaw
v2.0.10,Predict
v2.0.10,PredictLeafIndex
v2.0.10,output model type
v2.0.10,output number of class
v2.0.10,output label index
v2.0.10,output max_feature_idx
v2.0.10,output objective
v2.0.10,output tree models
v2.0.10,store the importance first
v2.0.10,sort the importance
v2.0.10,use serialized string to restore this object
v2.0.10,get number of classes
v2.0.10,get index of label
v2.0.10,get max_feature_idx first
v2.0.10,get average_output
v2.0.10,get feature names
v2.0.10,get tree models
v2.0.10,margin_threshold will be captured by value
v2.0.10,copy and sort
v2.0.10,margin_threshold will be captured by value
v2.0.10,init tree learner
v2.0.10,push training metrics
v2.0.10,create buffer for gradients and hessians
v2.0.10,get max feature index
v2.0.10,get label index
v2.0.10,get feature names
v2.0.10,"if need bagging, create buffer"
v2.0.10,reset config for tree learner
v2.0.10,multi-class
v2.0.10,binary class
v2.0.10,"for a validation dataset, we need its score and metric"
v2.0.10,update score
v2.0.10,objective function will calculate gradients and hessians
v2.0.10,"random bagging, minimal unit is one record"
v2.0.10,if need bagging
v2.0.10,set bagging data to tree learner
v2.0.10,get subset
v2.0.10,output used time per iteration
v2.0.10,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.0.10,boosting first
v2.0.10,bagging logic
v2.0.10,need to copy gradients for bagging subset.
v2.0.10,shrinkage by learning rate
v2.0.10,update score
v2.0.10,only add default score one-time
v2.0.10,updates scores
v2.0.10,add model
v2.0.10,reset score
v2.0.10,remove model
v2.0.10,print message for metric
v2.0.10,pop last early_stopping_round_ models
v2.0.10,update training score
v2.0.10,we need to predict out-of-bag scores of data for boosting
v2.0.10,update validation score
v2.0.10,print training metric
v2.0.10,print validation metric
v2.0.10,set zero
v2.0.10,predict all the trees for one iteration
v2.0.10,check early stopping
v2.0.10,push training metrics
v2.0.10,"not same training data, need reset score and others"
v2.0.10,create score tracker
v2.0.10,update score
v2.0.10,create buffer for gradients and hessians
v2.0.10,"if need bagging, create buffer"
v2.0.10,Get the max size of pool
v2.0.10,at least need 2 leaves
v2.0.10,push split information for all leaves
v2.0.10,get ordered bin
v2.0.10,check existing for ordered bin
v2.0.10,initialize splits for leaf
v2.0.10,initialize data partition
v2.0.10,initialize ordered gradients and hessians
v2.0.10,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.10,get ordered bin
v2.0.10,initialize splits for leaf
v2.0.10,initialize data partition
v2.0.10,initialize ordered gradients and hessians
v2.0.10,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.10,Get the max size of pool
v2.0.10,at least need 2 leaves
v2.0.10,push split information for all leaves
v2.0.10,some initial works before training
v2.0.10,root leaf
v2.0.10,only root leaf can be splitted on first time
v2.0.10,some initial works before finding best split
v2.0.10,find best threshold for every feature
v2.0.10,Get a leaf with max split gain
v2.0.10,Get split information for best leaf
v2.0.10,"cannot split, quit"
v2.0.10,split tree with best leaf
v2.0.10,avoid zero hessians.
v2.0.10,reset histogram pool
v2.0.10,initialize used features
v2.0.10,Get used feature at current tree
v2.0.10,initialize data partition
v2.0.10,reset the splits for leaves
v2.0.10,Sumup for root
v2.0.10,use all data
v2.0.10,"use bagging, only use part of data"
v2.0.10,"if has ordered bin, need to initialize the ordered bin"
v2.0.10,"use all data, pass nullptr"
v2.0.10,"bagging, only use part of data"
v2.0.10,mark used data
v2.0.10,initialize ordered bin
v2.0.10,check depth of current leaf
v2.0.10,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.0.10,no enough data to continue
v2.0.10,only have root
v2.0.10,put parent(left) leaf's histograms into larger leaf's histograms
v2.0.10,put parent(left) leaf's histograms to larger leaf's histograms
v2.0.10,split for the ordered bin
v2.0.10,mark data that at left-leaf
v2.0.10,split the ordered bin
v2.0.10,construct smaller leaf
v2.0.10,construct larger leaf
v2.0.10,find splits
v2.0.10,only has root leaf
v2.0.10,find best threshold for larger child
v2.0.10,left = parent
v2.0.10,"split tree, will return right leaf"
v2.0.10,init the leaves that used on next iteration
v2.0.10,get feature partition
v2.0.10,get local used features
v2.0.10,get best split at smaller leaf
v2.0.10,find local best split for larger leaf
v2.0.10,sync global best info
v2.0.10,update best split
v2.0.10,"instantiate template classes, otherwise linker cannot find the code"
v2.0.10,initialize SerialTreeLearner
v2.0.10,Get local rank and global machine size
v2.0.10,allocate buffer for communication
v2.0.10,generate feature partition for current tree
v2.0.10,get local used feature
v2.0.10,get block start and block len for reduce scatter
v2.0.10,get buffer_write_start_pos_
v2.0.10,get buffer_read_start_pos_
v2.0.10,sync global data sumup info
v2.0.10,global sumup reduce
v2.0.10,copy back
v2.0.10,set global sumup info
v2.0.10,init global data count in leaf
v2.0.10,construct local histograms
v2.0.10,copy to buffer
v2.0.10,Reduce scatter for histogram
v2.0.10,restore global histograms from buffer
v2.0.10,find best threshold for smaller child
v2.0.10,only root leaf
v2.0.10,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.0.10,find best threshold for larger child
v2.0.10,find local best split for larger leaf
v2.0.10,sync global best info
v2.0.10,set best split
v2.0.10,need update global number of data in leaf
v2.0.10,"instantiate template classes, otherwise linker cannot find the code"
v2.0.10,initialize SerialTreeLearner
v2.0.10,some additional variables needed for GPU trainer
v2.0.10,Initialize GPU buffers and kernels
v2.0.10,some functions used for debugging the GPU histogram construction
v2.0.10,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.0.10,goto err;
v2.0.10,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.0.10,goto err;
v2.0.10,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.0.10,also guarantee that there are at least 2K examples per workgroup
v2.0.10,return 0;
v2.0.10,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.0.10,decide the best number of workgroups working on one feature4 tuple
v2.0.10,set work group size based on feature size
v2.0.10,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.0.10,we need to refresh the kernel arguments after reallocating
v2.0.10,The only argument that needs to be changed later is num_data_
v2.0.10,"the GPU kernel will process all features in one call, and each"
v2.0.10,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.0.10,process one feature4 tuple
v2.0.10,"for the root node, indices are not copied"
v2.0.10,"for constant hessian, hessians are not copied except for the root node"
v2.0.10,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.0.10,and we will launch num_feature workgroups for this kernel
v2.0.10,will launch threads for all features
v2.0.10,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.0.10,copy the results asynchronously. Size depends on if double precision is used
v2.0.10,we will wait for this object in WaitAndGetHistograms
v2.0.10,"when the output is ready, the computation is done"
v2.0.10,values of this feature has been redistributed to multiple bins; need a reduction here
v2.0.10,how many feature-group tuples we have
v2.0.10,leave some safe margin for prefetching
v2.0.10,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.0.10,clear sparse/dense maps
v2.0.10,do nothing if no features can be processed on GPU
v2.0.10,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.0.10,unpin old buffer if necessary before destructing them
v2.0.10,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.0.10,allocate space for gradients and hessians on device
v2.0.10,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.0.10,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.0.10,copy indices to the device
v2.0.10,histogram bin entry size depends on the precision (single/double)
v2.0.10,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.0.10,each work group generates a sub-histogram of dword_features_ features.
v2.0.10,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.0.10,create atomic counters for inter-group coordination
v2.0.10,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.0.10,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.0.10,looking for dword_features_ non-sparse feature-groups
v2.0.10,decide if we need to redistribute the bin
v2.0.10,multiplier must be a power of 2
v2.0.10,device_bin_mults_.push_back(1);
v2.0.10,found
v2.0.10,for data transfer time
v2.0.10,"Now generate new data structure feature4, and copy data to the device"
v2.0.10,"preallocate arrays for all threads, and pin them"
v2.0.10,building Feature4 bundles; each thread handles dword_features_ features
v2.0.10,one feature datapoint is 4 bits
v2.0.10,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.10,one feature datapoint is one byte
v2.0.10,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.10,Dense bin
v2.0.10,Dense 4-bit bin
v2.0.10,working on the remaining (less than dword_features_) feature groups
v2.0.10,fill the leftover features
v2.0.10,"fill this empty feature with some ""random"" value"
v2.0.10,"fill this empty feature with some ""random"" value"
v2.0.10,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.0.10,deallocate pinned space for feature copying
v2.0.10,data transfer time
v2.0.10,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.0.10,"Something bad happened. Just return ""No log available."""
v2.0.10,"build is okay, log may contain warnings"
v2.0.10,destroy any old kernels
v2.0.10,create OpenCL kernels for different number of workgroups per feature
v2.0.10,currently we don't use constant memory
v2.0.10,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.0.10,kernel with indices in an array
v2.0.10,"kernel with all features enabled, with elimited branches"
v2.0.10,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.0.10,do nothing if no features can be processed on GPU
v2.0.10,The only argument that needs to be changed later is num_data_
v2.0.10,"hessian is passed as a parameter, but it is not available now."
v2.0.10,hessian will be set in BeforeTrain()
v2.0.10,"Get the max bin size, used for selecting best GPU kernel"
v2.0.10,initialize GPU
v2.0.10,determine which kernel to use based on the max number of bins
v2.0.10,setup GPU kernel arguments after we allocating all the buffers
v2.0.10,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.0.10,this should rarely occur
v2.0.10,GPU memory has to been reallocated because data may have been changed
v2.0.10,setup GPU kernel arguments after we allocating all the buffers
v2.0.10,Copy initial full hessians and gradients to GPU.
v2.0.10,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.0.10,setup hessian parameters only
v2.0.10,hessian is passed as a parameter
v2.0.10,use bagging
v2.0.10,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.0.10,copy used gradients and hessians to ordered buffer
v2.0.10,transfer the indices to GPU
v2.0.10,transfer hessian to GPU
v2.0.10,setup hessian parameters only
v2.0.10,hessian is passed as a parameter
v2.0.10,transfer gradients to GPU
v2.0.10,only have root
v2.0.10,"Copy indices, gradients and hessians as early as possible"
v2.0.10,only need to initialize for smaller leaf
v2.0.10,Get leaf boundary
v2.0.10,copy indices to the GPU:
v2.0.10,copy ordered hessians to the GPU:
v2.0.10,copy ordered gradients to the GPU:
v2.0.10,do nothing if no features can be processed on GPU
v2.0.10,copy data indices if it is not null
v2.0.10,generate and copy ordered_gradients if gradients is not null
v2.0.10,generate and copy ordered_hessians if hessians is not null
v2.0.10,converted indices in is_feature_used to feature-group indices
v2.0.10,construct the feature masks for dense feature-groups
v2.0.10,"if no feature group is used, just return and do not use GPU"
v2.0.10,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.0.10,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.0.10,"All data have been prepared, now run the GPU kernel"
v2.0.10,construct smaller leaf
v2.0.10,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.0.10,then construct sparse features on CPU
v2.0.10,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.10,"wait for GPU to finish, only if GPU is actually used"
v2.0.10,use double precision
v2.0.10,use single precision
v2.0.10,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.0.10,#define GPU_DEBUG_COMPARE
v2.0.10,construct larger leaf
v2.0.10,then construct sparse features on CPU
v2.0.10,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.10,"wait for GPU to finish, only if GPU is actually used"
v2.0.10,use double precision
v2.0.10,use single precision
v2.0.10,do some sanity check for the GPU algorithm
v2.0.10,limit top k
v2.0.10,get max bin
v2.0.10,calculate buffer size
v2.0.10,"left and right on same time, so need double size"
v2.0.10,initialize histograms for global
v2.0.10,sync global data sumup info
v2.0.10,set global sumup info
v2.0.10,init global data count in leaf
v2.0.10,get local sumup
v2.0.10,get local sumup
v2.0.10,get mean number on machines
v2.0.10,weighted gain
v2.0.10,get top k
v2.0.10,"Copy histogram to buffer, and Get local aggregate features"
v2.0.10,copy histograms.
v2.0.10,copy smaller leaf histograms first
v2.0.10,mark local aggregated feature
v2.0.10,copy
v2.0.10,then copy larger leaf histograms
v2.0.10,mark local aggregated feature
v2.0.10,copy
v2.0.10,use local data to find local best splits
v2.0.10,find splits
v2.0.10,only has root leaf
v2.0.10,find best threshold for larger child
v2.0.10,local voting
v2.0.10,gather
v2.0.10,get all top-k from all machines
v2.0.10,global voting
v2.0.10,copy local histgrams to buffer
v2.0.10,Reduce scatter for histogram
v2.0.10,find best split from local aggregated histograms
v2.0.10,restore from buffer
v2.0.10,find best threshold
v2.0.10,restore from buffer
v2.0.10,find best threshold
v2.0.10,find local best
v2.0.10,find local best split for larger leaf
v2.0.10,sync global best info
v2.0.10,copy back
v2.0.10,set the global number of data for leaves
v2.0.10,init the global sumup info
v2.0.10,"instantiate template classes, otherwise linker cannot find the code"
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable=invalid-name, exec-used, C0111"
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, W0105"
v2.0.6,Most of legacy advanced options becomes callbacks
v2.0.6,check evaluation result.
v2.0.6,"lambdarank task, split according to groups"
v2.0.6,run preprocessing on the data set if needed
v2.0.6,setup callbacks
v2.0.6,coding: utf-8
v2.0.6,pylint: disable = C0103
v2.0.6,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.0.6,because of u'...' Unicode literals.
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.0.6,minor change to support `**kwargs`
v2.0.6,"user can set verbose with kwargs, it has higher priority"
v2.0.6,free dataset
v2.0.6,Switch to using a multiclass objective in the underlying LGBM instance
v2.0.6,coding: utf-8
v2.0.6,we don't need lib_lightgbm while building docs
v2.0.6,coding: utf-8
v2.0.6,pylint: disable = C0103
v2.0.6,coding: utf-8
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, C0111, C0301"
v2.0.6,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.0.6,TypeError: obj is not a string or a number
v2.0.6,ValueError: invalid literal
v2.0.6,load init score
v2.0.6,need re group init score
v2.0.6,set feature names
v2.0.6,we're done if self and reference share a common upstrem reference
v2.0.6,"group data from LightGBM is boundaries data, need to convert to group size"
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, W0105, C0301"
v2.0.6,Callback environment used by callbacks
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, C0111"
v2.0.6,load or create your dataset
v2.0.6,create dataset for lightgbm
v2.0.6,"if you want to re-use data, remember to set free_raw_data=False"
v2.0.6,specify your configurations as a dict
v2.0.6,generate a feature name
v2.0.6,feature_name and categorical_feature
v2.0.6,check feature name
v2.0.6,save model to file
v2.0.6,load model to predict
v2.0.6,can only predict with the best iteration (or the saving iteration)
v2.0.6,eval with loaded model
v2.0.6,dump model with pickle
v2.0.6,load model with pickle to predict
v2.0.6,can predict with any iteration when loaded in pickle way
v2.0.6,eval with loaded model
v2.0.6,continue training
v2.0.6,init_model accepts:
v2.0.6,1. model file name
v2.0.6,2. Booster()
v2.0.6,decay learning rates
v2.0.6,learning_rates accepts:
v2.0.6,1. list/tuple with length = num_boost_round
v2.0.6,2. function(curr_iter)
v2.0.6,change other parameters during training
v2.0.6,self-defined objective function
v2.0.6,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.0.6,log likelihood loss
v2.0.6,self-defined eval metric
v2.0.6,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.0.6,binary error
v2.0.6,callback
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, C0111"
v2.0.6,load or create your dataset
v2.0.6,train
v2.0.6,predict
v2.0.6,eval
v2.0.6,feature importances
v2.0.6,other scikit-learn modules
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, C0111"
v2.0.6,load or create your dataset
v2.0.6,create dataset for lightgbm
v2.0.6,specify your configurations as a dict
v2.0.6,train
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = invalid-name, C0111"
v2.0.6,load or create your dataset
v2.0.6,create dataset for lightgbm
v2.0.6,specify your configurations as a dict
v2.0.6,train
v2.0.6,save model to file
v2.0.6,predict
v2.0.6,eval
v2.0.6,dump model to json (and save to file)
v2.0.6,feature importances
v2.0.6,coding: utf-8
v2.0.6,"pylint: disable = C0111, C0103"
v2.0.6,print out the pmml for a decision tree
v2.0.6,specify the objective as function name and binarySplit for
v2.0.6,splitCharacteristic because each node has 2 children
v2.0.6,"list each feature name as a mining field, and treat all outliers as is,"
v2.0.6,unless specified
v2.0.6,begin printing out the decision tree
v2.0.6,open the model file and then process it
v2.0.6,ignore first 6 and empty lines
v2.0.6,print out data dictionary entries for each column
v2.0.6,"not adding any interval definition, all values are currently"
v2.0.6,valid
v2.0.6,"list each feature name as a mining field, and treat all outliers"
v2.0.6,"as is, unless specified"
v2.0.6,read each array that contains pertinent information for the pmml
v2.0.6,these arrays will be used to recreate the traverse the decision tree
v2.0.6,!/usr/bin/env python3
v2.0.6,-*- coding: utf-8 -*-
v2.0.6,
v2.0.6,"LightGBM documentation build configuration file, created by"
v2.0.6,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.0.6,
v2.0.6,This file is execfile()d with the current directory set to its
v2.0.6,containing dir.
v2.0.6,
v2.0.6,Note that not all possible configuration values are present in this
v2.0.6,autogenerated file.
v2.0.6,
v2.0.6,All configuration values have a default; values that are commented out
v2.0.6,serve to show the default.
v2.0.6,"If extensions (or modules to document with autodoc) are in another directory,"
v2.0.6,add these directories to sys.path here. If the directory is relative to the
v2.0.6,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.0.6,
v2.0.6,-- mock out modules
v2.0.6,-- General configuration ------------------------------------------------
v2.0.6,"If your documentation needs a minimal Sphinx version, state it here."
v2.0.6,
v2.0.6,needs_sphinx = '1.0'
v2.0.6,"Add any Sphinx extension module names here, as strings. They can be"
v2.0.6,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.0.6,ones.
v2.0.6,"Add any paths that contain templates here, relative to this directory."
v2.0.6,The suffix(es) of source filenames.
v2.0.6,You can specify multiple suffix as a list of string:
v2.0.6,The master toctree document.
v2.0.6,General information about the project.
v2.0.6,"The version info for the project you're documenting, acts as replacement for"
v2.0.6,"|version| and |release|, also used in various other places throughout the"
v2.0.6,built documents.
v2.0.6,
v2.0.6,The short X.Y version.
v2.0.6,"The full version, including alpha/beta/rc tags."
v2.0.6,The language for content autogenerated by Sphinx. Refer to documentation
v2.0.6,for a list of supported languages.
v2.0.6,
v2.0.6,This is also used if you do content translation via gettext catalogs.
v2.0.6,"Usually you set ""language"" from the command line for these cases."
v2.0.6,"List of patterns, relative to source directory, that match files and"
v2.0.6,directories to ignore when looking for source files.
v2.0.6,This patterns also effect to html_static_path and html_extra_path
v2.0.6,The name of the Pygments (syntax highlighting) style to use.
v2.0.6,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.0.6,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.0.6,-- Options for HTML output ----------------------------------------------
v2.0.6,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.0.6,a list of builtin themes.
v2.0.6,
v2.0.6,Theme options are theme-specific and customize the look and feel of a theme
v2.0.6,"further.  For a list of options available for each theme, see the"
v2.0.6,documentation.
v2.0.6,
v2.0.6,html_theme_options = {}
v2.0.6,"Add any paths that contain custom static files (such as style sheets) here,"
v2.0.6,"relative to this directory. They are copied after the builtin static files,"
v2.0.6,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.0.6,-- Options for HTMLHelp output ------------------------------------------
v2.0.6,Output file base name for HTML help builder.
v2.0.6,-- Options for LaTeX output ---------------------------------------------
v2.0.6,The paper size ('letterpaper' or 'a4paper').
v2.0.6,
v2.0.6,"'papersize': 'letterpaper',"
v2.0.6,"The font size ('10pt', '11pt' or '12pt')."
v2.0.6,
v2.0.6,"'pointsize': '10pt',"
v2.0.6,Additional stuff for the LaTeX preamble.
v2.0.6,
v2.0.6,"'preamble': '',"
v2.0.6,Latex figure (float) alignment
v2.0.6,
v2.0.6,"'figure_align': 'htbp',"
v2.0.6,Grouping the document tree into LaTeX files. List of tuples
v2.0.6,"(source start file, target name, title,"
v2.0.6,"author, documentclass [howto, manual, or own class])."
v2.0.6,-- Options for manual page output ---------------------------------------
v2.0.6,One entry per manual page. List of tuples
v2.0.6,"(source start file, name, description, authors, manual section)."
v2.0.6,-- Options for Texinfo output -------------------------------------------
v2.0.6,Grouping the document tree into Texinfo files. List of tuples
v2.0.6,"(source start file, target name, title, author,"
v2.0.6,"dir menu entry, description, category)"
v2.0.6,https://recommonmark.readthedocs.io/en/latest/
v2.0.6,coding: utf-8
v2.0.6,coding: utf-8
v2.0.6,pylint: skip-file
v2.0.6,we don't need lib_lightgbm while building docs
v2.0.6,coding: utf-8
v2.0.6,pylint: skip-file
v2.0.6,check saved model persistence
v2.0.6,"we need to check the consistency of model file here, so test for exact equal"
v2.0.6,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.0.6,"scores likely to be different, but prediction should still be the same"
v2.0.6,coding: utf-8
v2.0.6,pylint: skip-file
v2.0.6,coding: utf-8
v2.0.6,pylint: skip-file
v2.0.6,coding: utf-8
v2.0.6,pylint: skip-file
v2.0.6,no early stopping
v2.0.6,early stopping occurs
v2.0.6,test custom eval metrics
v2.0.6,test dump model
v2.0.6,"shuffle = False, override metric in params"
v2.0.6,"shuffle = True, callbacks"
v2.0.6,self defined folds
v2.0.6,lambdarank
v2.0.6,test feature_names with whitespaces
v2.0.6,take subsets and train
v2.0.6,convert from one-based to  zero-based index
v2.0.6,convert from boundaries to size
v2.0.6,--- start Booster interfaces
v2.0.6,create boosting
v2.0.6,initialize the boosting
v2.0.6,create objective function
v2.0.6,initialize the objective function
v2.0.6,create training metric
v2.0.6,reset the boosting
v2.0.6,create objective function
v2.0.6,initialize the objective function
v2.0.6,some help functions used to convert data
v2.0.6,Row iterator of on column for CSC matrix
v2.0.6,"return value at idx, only can access by ascent order"
v2.0.6,"return next non-zero pair, if index < 0, means no more data"
v2.0.6,start of c_api functions
v2.0.6,sample data first
v2.0.6,sample data first
v2.0.6,sample data first
v2.0.6,no more data
v2.0.6,---- start of booster
v2.0.6,---- start of some help functions
v2.0.6,set number of threads for openmp
v2.0.6,check for alias
v2.0.6,read parameters from config file
v2.0.6,"remove str after ""#"""
v2.0.6,Command-line has higher priority
v2.0.6,check for alias again
v2.0.6,load configs
v2.0.6,prediction is needed if using input initial model(continued train)
v2.0.6,need to continue training
v2.0.6,sync up random seed for data partition
v2.0.6,load Training data
v2.0.6,load data for parallel training
v2.0.6,load data for single machine
v2.0.6,need save binary file
v2.0.6,create training metric
v2.0.6,only when have metrics then need to construct validation data
v2.0.6,"Add validation data, if it exists"
v2.0.6,add
v2.0.6,need save binary file
v2.0.6,add metric for validation data
v2.0.6,output used time on each iteration
v2.0.6,need init network
v2.0.6,create boosting
v2.0.6,create objective function
v2.0.6,load training data
v2.0.6,initialize the objective function
v2.0.6,initialize the boosting
v2.0.6,add validation data into boosting
v2.0.6,convert model to if-else statement code
v2.0.6,create predictor
v2.0.6,counts for all labels
v2.0.6,"start from top label, and accumulate DCG"
v2.0.6,counts for all labels
v2.0.6,calculate k Max DCG by one pass
v2.0.6,get sorted indices by score
v2.0.6,calculate dcg
v2.0.6,get sorted indices by score
v2.0.6,calculate multi dcg by one pass
v2.0.6,wait for all client start up
v2.0.6,default set to -1
v2.0.6,"distance at k-th communication, distance[k] = 2^k"
v2.0.6,set incoming rank at k-th commuication
v2.0.6,set outgoing rank at k-th commuication
v2.0.6,defalut set as -1
v2.0.6,construct all recursive halving map for all machines
v2.0.6,let 1 << k <= num_machines
v2.0.6,distance of each communication
v2.0.6,"if num_machines = 2^k, don't need to group machines"
v2.0.6,"communication direction, %2 == 0 is positive"
v2.0.6,neighbor at k-th communication
v2.0.6,receive data block at k-th communication
v2.0.6,send data block at k-th communication
v2.0.6,static member definition
v2.0.6,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.0.6,assign the blocks to every rank.
v2.0.6,do reduce scatter
v2.0.6,do all gather
v2.0.6,assign blocks
v2.0.6,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.0.6,copy back
v2.0.6,assign blocks
v2.0.6,start all gather
v2.0.6,use output as receive buffer
v2.0.6,get current local block size
v2.0.6,get out rank
v2.0.6,get in rank
v2.0.6,get send information
v2.0.6,get recv information
v2.0.6,send and recv at same time
v2.0.6,rotate in-place
v2.0.6,get target
v2.0.6,get send information
v2.0.6,get recv information
v2.0.6,send and recv at same time
v2.0.6,reduce
v2.0.6,copy result
v2.0.6,start up socket
v2.0.6,parse clients from file
v2.0.6,get ip list of local machine
v2.0.6,get local rank
v2.0.6,construct listener
v2.0.6,construct communication topo
v2.0.6,construct linkers
v2.0.6,free listener
v2.0.6,set timeout
v2.0.6,accept incoming socket
v2.0.6,receive rank
v2.0.6,add new socket
v2.0.6,save ranks that need to connect with
v2.0.6,start listener
v2.0.6,start connect
v2.0.6,let smaller rank connect to larger rank
v2.0.6,send local rank
v2.0.6,wait for listener
v2.0.6,print connected linkers
v2.0.6,Get some statistic from 2 line
v2.0.6,if only have one line on file
v2.0.6,get column names
v2.0.6,load label idx first
v2.0.6,erase label column name
v2.0.6,load ignore columns
v2.0.6,load weight idx
v2.0.6,load group idx
v2.0.6,don't support query id in data file when training in parallel
v2.0.6,read data to memory
v2.0.6,sample data
v2.0.6,construct feature bin mappers
v2.0.6,initialize label
v2.0.6,extract features
v2.0.6,sample data from file
v2.0.6,construct feature bin mappers
v2.0.6,initialize label
v2.0.6,extract features
v2.0.6,load data from binary file
v2.0.6,check meta data
v2.0.6,need to check training data
v2.0.6,read data in memory
v2.0.6,initialize label
v2.0.6,extract features
v2.0.6,Get number of lines of data file
v2.0.6,initialize label
v2.0.6,extract features
v2.0.6,load data from binary file
v2.0.6,not need to check validation data
v2.0.6,check meta data
v2.0.6,buffer to read binary file
v2.0.6,check token
v2.0.6,read size of header
v2.0.6,re-allocmate space if not enough
v2.0.6,read header
v2.0.6,get header
v2.0.6,num_groups
v2.0.6,real_feature_idx_
v2.0.6,feature2group
v2.0.6,feature2subfeature
v2.0.6,group_bin_boundaries
v2.0.6,group_feature_start_
v2.0.6,group_feature_cnt_
v2.0.6,get feature names
v2.0.6,write feature names
v2.0.6,read size of meta data
v2.0.6,re-allocate space if not enough
v2.0.6,read meta data
v2.0.6,load meta data
v2.0.6,sample local used data if need to partition
v2.0.6,"if not contain query file, minimal sample unit is one record"
v2.0.6,"if contain query file, minimal sample unit is one query"
v2.0.6,if is new query
v2.0.6,read feature data
v2.0.6,read feature size
v2.0.6,re-allocate space if not enough
v2.0.6,fill feature_names_ if not header
v2.0.6,---- private functions ----
v2.0.6,"if features are ordered, not need to use hist_buf"
v2.0.6,read all lines
v2.0.6,get query data
v2.0.6,"if not contain query data, minimal sample unit is one record"
v2.0.6,"if contain query data, minimal sample unit is one query"
v2.0.6,if is new query
v2.0.6,get query data
v2.0.6,"if not contain query file, minimal sample unit is one record"
v2.0.6,"if contain query file, minimal sample unit is one query"
v2.0.6,if is new query
v2.0.6,parse features
v2.0.6,-1 means doesn't use this feature
v2.0.6,"check the range of label_idx, weight_idx and group_idx"
v2.0.6,fill feature_names_ if not header
v2.0.6,start find bins
v2.0.6,"if only one machine, find bin locally"
v2.0.6,"if have multi-machines, need to find bin distributed"
v2.0.6,different machines will find bin for different features
v2.0.6,start and len will store the process feature indices for different machines
v2.0.6,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.0.6,get size of bin mapper with max_bin size
v2.0.6,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.0.6,find local feature bins and copy to buffer
v2.0.6,free
v2.0.6,convert to binary size
v2.0.6,gather global feature bin mappers
v2.0.6,restore features bins from buffer
v2.0.6,if doesn't need to prediction with initial model
v2.0.6,parser
v2.0.6,set label
v2.0.6,free processed line:
v2.0.6,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.6,text_reader_->Lines()[i].shrink_to_fit();
v2.0.6,push data
v2.0.6,if is used feature
v2.0.6,if need to prediction with initial model
v2.0.6,parser
v2.0.6,set initial score
v2.0.6,set label
v2.0.6,free processed line:
v2.0.6,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.6,text_reader_->Lines()[i].shrink_to_fit();
v2.0.6,push data
v2.0.6,if is used feature
v2.0.6,metadata_ will manage space of init_score
v2.0.6,text data can be free after loaded feature values
v2.0.6,parser
v2.0.6,set initial score
v2.0.6,set label
v2.0.6,push data
v2.0.6,if is used feature
v2.0.6,only need part of data
v2.0.6,need full data
v2.0.6,metadata_ will manage space of init_score
v2.0.6,read size of token
v2.0.6,deep copy function for BinMapper
v2.0.6,mean size for one bin
v2.0.6,need a new bin
v2.0.6,update bin upper bound
v2.0.6,last bin upper bound
v2.0.6,find distinct_values first
v2.0.6,push zero in the front
v2.0.6,push zero in the back
v2.0.6,convert to int type first
v2.0.6,sort by counts
v2.0.6,avoid first bin is zero
v2.0.6,will ignore the categorical of small counts
v2.0.6,need an additional bin for NaN
v2.0.6,use -1 to represent NaN
v2.0.6,Use MissingType::None to represent this bin contains all categoricals
v2.0.6,check trival(num_bin_ == 1) feature
v2.0.6,check useless bin
v2.0.6,calculate sparse rate
v2.0.6,sparse threshold
v2.0.6,"for lambdarank, it needs query data for partition data in parallel learning"
v2.0.6,need convert query_id to boundaries
v2.0.6,check weights
v2.0.6,check query boundries
v2.0.6,contain initial score file
v2.0.6,check weights
v2.0.6,get local weights
v2.0.6,check query boundries
v2.0.6,get local query boundaries
v2.0.6,contain initial score file
v2.0.6,get local initial scores
v2.0.6,re-load query weight
v2.0.6,save to nullptr
v2.0.6,save to nullptr
v2.0.6,save to nullptr
v2.0.6,default weight file name
v2.0.6,default weight file name
v2.0.6,use first line to count number class
v2.0.6,default query file name
v2.0.6,root is in the depth 0
v2.0.6,non-leaf
v2.0.6,leaf
v2.0.6,use this for the missing value conversion
v2.0.6,non-leaf
v2.0.6,left subtree
v2.0.6,right subtree
v2.0.6,leaf
v2.0.6,clear old metrics
v2.0.6,to lower
v2.0.6,split
v2.0.6,remove duplicate
v2.0.6,load main config types
v2.0.6,generate seeds by seed.
v2.0.6,sub-config setup
v2.0.6,check for conflicts
v2.0.6,"check if objective_type, metric_type, and num_class match"
v2.0.6,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.0.6,Check max_depth and num_leaves
v2.0.6,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.6,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.6,default eval ndcg @[1-5]
v2.0.6,"filter is based on sampling data, so decrease its range"
v2.0.6,put dense feature first
v2.0.6,sort by non zero cnt
v2.0.6,"sort by non zero cnt, bigger first"
v2.0.6,"take apart small sparse group, due it will not gain on speed"
v2.0.6,shuffle groups
v2.0.6,get num_features
v2.0.6,get bin_mappers
v2.0.6,copy feature bin mapper data
v2.0.6,copy feature bin mapper data
v2.0.6,"if not pass a filename, just append "".bin"" of original file"
v2.0.6,get size of header
v2.0.6,size of feature names
v2.0.6,write header
v2.0.6,write feature names
v2.0.6,get size of meta data
v2.0.6,write meta data
v2.0.6,write feature data
v2.0.6,get size of feature
v2.0.6,write feature
v2.0.6,feature is not used
v2.0.6,construct histograms for smaller leaf
v2.0.6,if not use ordered bin
v2.0.6,used ordered bin
v2.0.6,feature is not used
v2.0.6,construct histograms for smaller leaf
v2.0.6,if not use ordered bin
v2.0.6,used ordered bin
v2.0.6,fixed hessian.
v2.0.6,feature is not used
v2.0.6,construct histograms for smaller leaf
v2.0.6,if not use ordered bin
v2.0.6,used ordered bin
v2.0.6,feature is not used
v2.0.6,construct histograms for smaller leaf
v2.0.6,if not use ordered bin
v2.0.6,used ordered bin
v2.0.6,fixed hessian.
v2.0.6,set zero
v2.0.6,predict all the trees for one iteration
v2.0.6,check early stopping
v2.0.6,margin_threshold will be captured by value
v2.0.6,copy and sort
v2.0.6,margin_threshold will be captured by value
v2.0.6,init tree learner
v2.0.6,push training metrics
v2.0.6,create buffer for gradients and hessians
v2.0.6,get max feature index
v2.0.6,get label index
v2.0.6,get feature names
v2.0.6,"if need bagging, create buffer"
v2.0.6,reset config for tree learner
v2.0.6,+ 1 here for the binary classification
v2.0.6,multi-class
v2.0.6,binary class
v2.0.6,push training metrics
v2.0.6,"not same training data, need reset score and others"
v2.0.6,create score tracker
v2.0.6,update score
v2.0.6,create buffer for gradients and hessians
v2.0.6,get max feature index
v2.0.6,get label index
v2.0.6,get feature names
v2.0.6,"if need bagging, create buffer"
v2.0.6,"for a validation dataset, we need its score and metric"
v2.0.6,update score
v2.0.6,"random bagging, minimal unit is one record"
v2.0.6,if need bagging
v2.0.6,set bagging data to tree learner
v2.0.6,get subset
v2.0.6,we need to predict out-of-bag scores of data for boosting
v2.0.6,output used time per iteration
v2.0.6,"boosting from average label; or customized ""average"" if implemented for the current objective"
v2.0.6,boosting first
v2.0.6,bagging logic
v2.0.6,need to use subset gradient and hessian
v2.0.6,get sub gradients
v2.0.6,cannot multi-threading here.
v2.0.6,shrinkage by learning rate
v2.0.6,update score
v2.0.6,only add default score one-time
v2.0.6,add model
v2.0.6,reset score
v2.0.6,remove model
v2.0.6,print message for metric
v2.0.6,pop last early_stopping_round_ models
v2.0.6,update training score
v2.0.6,update validation score
v2.0.6,print training metric
v2.0.6,print validation metric
v2.0.6,set zero
v2.0.6,predict all the trees for one iteration
v2.0.6,check early stopping
v2.0.6,objective function will calculate gradients and hessians
v2.0.6,PredictRaw
v2.0.6,Predict
v2.0.6,PredictLeafIndex
v2.0.6,output model type
v2.0.6,output number of class
v2.0.6,output label index
v2.0.6,output max_feature_idx
v2.0.6,output objective
v2.0.6,output tree models
v2.0.6,use serialized string to restore this object
v2.0.6,get number of classes
v2.0.6,get index of label
v2.0.6,get max_feature_idx first
v2.0.6,get boost_from_average_
v2.0.6,get average_output
v2.0.6,get feature names
v2.0.6,get tree models
v2.0.6,store the importance first
v2.0.6,sort the importance
v2.0.6,Get the max size of pool
v2.0.6,at least need 2 leaves
v2.0.6,push split information for all leaves
v2.0.6,get ordered bin
v2.0.6,check existing for ordered bin
v2.0.6,initialize splits for leaf
v2.0.6,initialize data partition
v2.0.6,initialize ordered gradients and hessians
v2.0.6,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.6,get ordered bin
v2.0.6,initialize splits for leaf
v2.0.6,initialize data partition
v2.0.6,initialize ordered gradients and hessians
v2.0.6,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.6,Get the max size of pool
v2.0.6,at least need 2 leaves
v2.0.6,push split information for all leaves
v2.0.6,some initial works before training
v2.0.6,root leaf
v2.0.6,only root leaf can be splitted on first time
v2.0.6,some initial works before finding best split
v2.0.6,find best threshold for every feature
v2.0.6,Get a leaf with max split gain
v2.0.6,Get split information for best leaf
v2.0.6,"cannot split, quit"
v2.0.6,split tree with best leaf
v2.0.6,avoid zero hessians.
v2.0.6,reset histogram pool
v2.0.6,initialize used features
v2.0.6,Get used feature at current tree
v2.0.6,initialize data partition
v2.0.6,reset the splits for leaves
v2.0.6,Sumup for root
v2.0.6,use all data
v2.0.6,"use bagging, only use part of data"
v2.0.6,"if has ordered bin, need to initialize the ordered bin"
v2.0.6,"use all data, pass nullptr"
v2.0.6,"bagging, only use part of data"
v2.0.6,mark used data
v2.0.6,initialize ordered bin
v2.0.6,check depth of current leaf
v2.0.6,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.0.6,no enough data to continue
v2.0.6,only have root
v2.0.6,put parent(left) leaf's histograms into larger leaf's histograms
v2.0.6,put parent(left) leaf's histograms to larger leaf's histograms
v2.0.6,split for the ordered bin
v2.0.6,mark data that at left-leaf
v2.0.6,split the ordered bin
v2.0.6,construct smaller leaf
v2.0.6,construct larger leaf
v2.0.6,find splits
v2.0.6,only has root leaf
v2.0.6,find best threshold for larger child
v2.0.6,left = parent
v2.0.6,"split tree, will return right leaf"
v2.0.6,init the leaves that used on next iteration
v2.0.6,get feature partition
v2.0.6,get local used features
v2.0.6,get best split at smaller leaf
v2.0.6,find local best split for larger leaf
v2.0.6,sync global best info
v2.0.6,update best split
v2.0.6,"instantiate template classes, otherwise linker cannot find the code"
v2.0.6,initialize SerialTreeLearner
v2.0.6,Get local rank and global machine size
v2.0.6,allocate buffer for communication
v2.0.6,generate feature partition for current tree
v2.0.6,get local used feature
v2.0.6,get block start and block len for reduce scatter
v2.0.6,get buffer_write_start_pos_
v2.0.6,get buffer_read_start_pos_
v2.0.6,sync global data sumup info
v2.0.6,global sumup reduce
v2.0.6,copy back
v2.0.6,set global sumup info
v2.0.6,init global data count in leaf
v2.0.6,construct local histograms
v2.0.6,copy to buffer
v2.0.6,Reduce scatter for histogram
v2.0.6,restore global histograms from buffer
v2.0.6,find best threshold for smaller child
v2.0.6,only root leaf
v2.0.6,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.0.6,find best threshold for larger child
v2.0.6,find local best split for larger leaf
v2.0.6,sync global best info
v2.0.6,set best split
v2.0.6,need update global number of data in leaf
v2.0.6,"instantiate template classes, otherwise linker cannot find the code"
v2.0.6,initialize SerialTreeLearner
v2.0.6,some additional variables needed for GPU trainer
v2.0.6,Initialize GPU buffers and kernels
v2.0.6,some functions used for debugging the GPU histogram construction
v2.0.6,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.0.6,goto err;
v2.0.6,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.0.6,goto err;
v2.0.6,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.0.6,also guarantee that there are at least 2K examples per workgroup
v2.0.6,return 0;
v2.0.6,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.0.6,decide the best number of workgroups working on one feature4 tuple
v2.0.6,set work group size based on feature size
v2.0.6,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.0.6,we need to refresh the kernel arguments after reallocating
v2.0.6,The only argument that needs to be changed later is num_data_
v2.0.6,"the GPU kernel will process all features in one call, and each"
v2.0.6,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.0.6,process one feature4 tuple
v2.0.6,"for the root node, indices are not copied"
v2.0.6,"for constant hessian, hessians are not copied except for the root node"
v2.0.6,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.0.6,and we will launch num_feature workgroups for this kernel
v2.0.6,will launch threads for all features
v2.0.6,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.0.6,copy the results asynchronously. Size depends on if double precision is used
v2.0.6,we will wait for this object in WaitAndGetHistograms
v2.0.6,"when the output is ready, the computation is done"
v2.0.6,values of this feature has been redistributed to multiple bins; need a reduction here
v2.0.6,how many feature-group tuples we have
v2.0.6,leave some safe margin for prefetching
v2.0.6,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.0.6,clear sparse/dense maps
v2.0.6,do nothing if no features can be processed on GPU
v2.0.6,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.0.6,unpin old buffer if necessary before destructing them
v2.0.6,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.0.6,allocate space for gradients and hessians on device
v2.0.6,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.0.6,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.0.6,copy indices to the device
v2.0.6,histogram bin entry size depends on the precision (single/double)
v2.0.6,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.0.6,each work group generates a sub-histogram of dword_features_ features.
v2.0.6,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.0.6,create atomic counters for inter-group coordination
v2.0.6,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.0.6,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.0.6,looking for dword_features_ non-sparse feature-groups
v2.0.6,decide if we need to redistribute the bin
v2.0.6,multiplier must be a power of 2
v2.0.6,device_bin_mults_.push_back(1);
v2.0.6,found
v2.0.6,for data transfer time
v2.0.6,"Now generate new data structure feature4, and copy data to the device"
v2.0.6,"preallocate arrays for all threads, and pin them"
v2.0.6,building Feature4 bundles; each thread handles dword_features_ features
v2.0.6,one feature datapoint is 4 bits
v2.0.6,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.6,one feature datapoint is one byte
v2.0.6,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.6,Dense bin
v2.0.6,Dense 4-bit bin
v2.0.6,working on the remaining (less than dword_features_) feature groups
v2.0.6,fill the leftover features
v2.0.6,"fill this empty feature with some ""random"" value"
v2.0.6,"fill this empty feature with some ""random"" value"
v2.0.6,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.0.6,deallocate pinned space for feature copying
v2.0.6,data transfer time
v2.0.6,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.0.6,"Something bad happened. Just return ""No log available."""
v2.0.6,"build is okay, log may contain warnings"
v2.0.6,destroy any old kernels
v2.0.6,create OpenCL kernels for different number of workgroups per feature
v2.0.6,currently we don't use constant memory
v2.0.6,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.0.6,kernel with indices in an array
v2.0.6,"kernel with all features enabled, with elimited branches"
v2.0.6,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.0.6,do nothing if no features can be processed on GPU
v2.0.6,The only argument that needs to be changed later is num_data_
v2.0.6,"hessian is passed as a parameter, but it is not available now."
v2.0.6,hessian will be set in BeforeTrain()
v2.0.6,"Get the max bin size, used for selecting best GPU kernel"
v2.0.6,initialize GPU
v2.0.6,determine which kernel to use based on the max number of bins
v2.0.6,setup GPU kernel arguments after we allocating all the buffers
v2.0.6,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.0.6,this should rarely occur
v2.0.6,GPU memory has to been reallocated because data may have been changed
v2.0.6,setup GPU kernel arguments after we allocating all the buffers
v2.0.6,Copy initial full hessians and gradients to GPU.
v2.0.6,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.0.6,setup hessian parameters only
v2.0.6,hessian is passed as a parameter
v2.0.6,use bagging
v2.0.6,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.0.6,copy used gradients and hessians to ordered buffer
v2.0.6,transfer the indices to GPU
v2.0.6,transfer hessian to GPU
v2.0.6,setup hessian parameters only
v2.0.6,hessian is passed as a parameter
v2.0.6,transfer gradients to GPU
v2.0.6,only have root
v2.0.6,"Copy indices, gradients and hessians as early as possible"
v2.0.6,only need to initialize for smaller leaf
v2.0.6,Get leaf boundary
v2.0.6,copy indices to the GPU:
v2.0.6,copy ordered hessians to the GPU:
v2.0.6,copy ordered gradients to the GPU:
v2.0.6,do nothing if no features can be processed on GPU
v2.0.6,copy data indices if it is not null
v2.0.6,generate and copy ordered_gradients if gradients is not null
v2.0.6,generate and copy ordered_hessians if hessians is not null
v2.0.6,converted indices in is_feature_used to feature-group indices
v2.0.6,construct the feature masks for dense feature-groups
v2.0.6,"if no feature group is used, just return and do not use GPU"
v2.0.6,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.0.6,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.0.6,"All data have been prepared, now run the GPU kernel"
v2.0.6,construct smaller leaf
v2.0.6,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.0.6,then construct sparse features on CPU
v2.0.6,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.6,"wait for GPU to finish, only if GPU is actually used"
v2.0.6,use double precision
v2.0.6,use single precision
v2.0.6,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.0.6,#define GPU_DEBUG_COMPARE
v2.0.6,construct larger leaf
v2.0.6,then construct sparse features on CPU
v2.0.6,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.6,"wait for GPU to finish, only if GPU is actually used"
v2.0.6,use double precision
v2.0.6,use single precision
v2.0.6,do some sanity check for the GPU algorithm
v2.0.6,limit top k
v2.0.6,get max bin
v2.0.6,calculate buffer size
v2.0.6,"left and right on same time, so need double size"
v2.0.6,initialize histograms for global
v2.0.6,sync global data sumup info
v2.0.6,set global sumup info
v2.0.6,init global data count in leaf
v2.0.6,get local sumup
v2.0.6,get local sumup
v2.0.6,get mean number on machines
v2.0.6,weighted gain
v2.0.6,get top k
v2.0.6,"Copy histogram to buffer, and Get local aggregate features"
v2.0.6,copy histograms.
v2.0.6,copy smaller leaf histograms first
v2.0.6,mark local aggregated feature
v2.0.6,copy
v2.0.6,then copy larger leaf histograms
v2.0.6,mark local aggregated feature
v2.0.6,copy
v2.0.6,use local data to find local best splits
v2.0.6,find splits
v2.0.6,only has root leaf
v2.0.6,find best threshold for larger child
v2.0.6,local voting
v2.0.6,gather
v2.0.6,get all top-k from all machines
v2.0.6,global voting
v2.0.6,copy local histgrams to buffer
v2.0.6,Reduce scatter for histogram
v2.0.6,find best split from local aggregated histograms
v2.0.6,restore from buffer
v2.0.6,find best threshold
v2.0.6,restore from buffer
v2.0.6,find best threshold
v2.0.6,find local best
v2.0.6,find local best split for larger leaf
v2.0.6,sync global best info
v2.0.6,copy back
v2.0.6,set the global number of data for leaves
v2.0.6,init the global sumup info
v2.0.6,"instantiate template classes, otherwise linker cannot find the code"
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable=invalid-name, exec-used"
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, W0105"
v2.0.3,Most of legacy advanced options becomes callbacks
v2.0.3,check evaluation result.
v2.0.3,"lambdarank task, split according to groups"
v2.0.3,run preprocessing on the data set if needed
v2.0.3,setup callbacks
v2.0.3,coding: utf-8
v2.0.3,pylint: disable = C0103
v2.0.3,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.0.3,because of u'...' Unicode literals.
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.0.3,minor change to support `**kwargs`
v2.0.3,"user can set verbose with kwargs, it has higher priority"
v2.0.3,free dataset
v2.0.3,Switch to using a multiclass objective in the underlying LGBM instance
v2.0.3,coding: utf-8
v2.0.3,we don't need lib_lightgbm while building docs
v2.0.3,coding: utf-8
v2.0.3,pylint: disable = C0103
v2.0.3,coding: utf-8
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, C0111, C0301"
v2.0.3,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.0.3,TypeError: obj is not a string or a number
v2.0.3,ValueError: invalid literal
v2.0.3,load init score
v2.0.3,need re group init score
v2.0.3,set feature names
v2.0.3,"group data from LightGBM is boundaries data, need to convert to group size"
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, W0105, C0301"
v2.0.3,Callback environment used by callbacks
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, C0111"
v2.0.3,load or create your dataset
v2.0.3,create dataset for lightgbm
v2.0.3,"if you want to re-use data, remember to set free_raw_data=False"
v2.0.3,specify your configurations as a dict
v2.0.3,generate a feature name
v2.0.3,feature_name and categorical_feature
v2.0.3,check feature name
v2.0.3,save model to file
v2.0.3,load model to predict
v2.0.3,can only predict with the best iteration (or the saving iteration)
v2.0.3,eval with loaded model
v2.0.3,dump model with pickle
v2.0.3,load model with pickle to predict
v2.0.3,can predict with any iteration when loaded in pickle way
v2.0.3,eval with loaded model
v2.0.3,continue training
v2.0.3,init_model accepts:
v2.0.3,1. model file name
v2.0.3,2. Booster()
v2.0.3,decay learning rates
v2.0.3,learning_rates accepts:
v2.0.3,1. list/tuple with length = num_boost_round
v2.0.3,2. function(curr_iter)
v2.0.3,change other parameters during training
v2.0.3,self-defined objective function
v2.0.3,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.0.3,log likelihood loss
v2.0.3,self-defined eval metric
v2.0.3,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.0.3,binary error
v2.0.3,callback
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, C0111"
v2.0.3,load or create your dataset
v2.0.3,train
v2.0.3,predict
v2.0.3,eval
v2.0.3,feature importances
v2.0.3,other scikit-learn modules
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, C0111"
v2.0.3,load or create your dataset
v2.0.3,create dataset for lightgbm
v2.0.3,specify your configurations as a dict
v2.0.3,train
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = invalid-name, C0111"
v2.0.3,load or create your dataset
v2.0.3,create dataset for lightgbm
v2.0.3,specify your configurations as a dict
v2.0.3,train
v2.0.3,save model to file
v2.0.3,predict
v2.0.3,eval
v2.0.3,dump model to json (and save to file)
v2.0.3,feature importances
v2.0.3,coding: utf-8
v2.0.3,"pylint: disable = C0111, C0103"
v2.0.3,print out the pmml for a decision tree
v2.0.3,specify the objective as function name and binarySplit for
v2.0.3,splitCharacteristic because each node has 2 children
v2.0.3,"list each feature name as a mining field, and treat all outliers as is,"
v2.0.3,unless specified
v2.0.3,begin printing out the decision tree
v2.0.3,open the model file and then process it
v2.0.3,ignore first 6 and empty lines
v2.0.3,print out data dictionary entries for each column
v2.0.3,"not adding any interval definition, all values are currently"
v2.0.3,valid
v2.0.3,"list each feature name as a mining field, and treat all outliers"
v2.0.3,"as is, unless specified"
v2.0.3,read each array that contains pertinent information for the pmml
v2.0.3,these arrays will be used to recreate the traverse the decision tree
v2.0.3,!/usr/bin/env python3
v2.0.3,-*- coding: utf-8 -*-
v2.0.3,
v2.0.3,"LightGBM documentation build configuration file, created by"
v2.0.3,sphinx-quickstart on Thu May  4 14:30:58 2017.
v2.0.3,
v2.0.3,This file is execfile()d with the current directory set to its
v2.0.3,containing dir.
v2.0.3,
v2.0.3,Note that not all possible configuration values are present in this
v2.0.3,autogenerated file.
v2.0.3,
v2.0.3,All configuration values have a default; values that are commented out
v2.0.3,serve to show the default.
v2.0.3,"If extensions (or modules to document with autodoc) are in another directory,"
v2.0.3,add these directories to sys.path here. If the directory is relative to the
v2.0.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
v2.0.3,
v2.0.3,-- mock out modules
v2.0.3,-- General configuration ------------------------------------------------
v2.0.3,"If your documentation needs a minimal Sphinx version, state it here."
v2.0.3,
v2.0.3,needs_sphinx = '1.0'
v2.0.3,"Add any Sphinx extension module names here, as strings. They can be"
v2.0.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
v2.0.3,ones.
v2.0.3,"Add any paths that contain templates here, relative to this directory."
v2.0.3,The suffix(es) of source filenames.
v2.0.3,You can specify multiple suffix as a list of string:
v2.0.3,The master toctree document.
v2.0.3,General information about the project.
v2.0.3,"The version info for the project you're documenting, acts as replacement for"
v2.0.3,"|version| and |release|, also used in various other places throughout the"
v2.0.3,built documents.
v2.0.3,
v2.0.3,The short X.Y version.
v2.0.3,"The full version, including alpha/beta/rc tags."
v2.0.3,The language for content autogenerated by Sphinx. Refer to documentation
v2.0.3,for a list of supported languages.
v2.0.3,
v2.0.3,This is also used if you do content translation via gettext catalogs.
v2.0.3,"Usually you set ""language"" from the command line for these cases."
v2.0.3,"List of patterns, relative to source directory, that match files and"
v2.0.3,directories to ignore when looking for source files.
v2.0.3,This patterns also effect to html_static_path and html_extra_path
v2.0.3,The name of the Pygments (syntax highlighting) style to use.
v2.0.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
v2.0.3,Both the class' and the __init__ method's docstring are concatenated and inserted.
v2.0.3,-- Options for HTML output ----------------------------------------------
v2.0.3,The theme to use for HTML and HTML Help pages.  See the documentation for
v2.0.3,a list of builtin themes.
v2.0.3,
v2.0.3,Theme options are theme-specific and customize the look and feel of a theme
v2.0.3,"further.  For a list of options available for each theme, see the"
v2.0.3,documentation.
v2.0.3,
v2.0.3,html_theme_options = {}
v2.0.3,"Add any paths that contain custom static files (such as style sheets) here,"
v2.0.3,"relative to this directory. They are copied after the builtin static files,"
v2.0.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
v2.0.3,-- Options for HTMLHelp output ------------------------------------------
v2.0.3,Output file base name for HTML help builder.
v2.0.3,-- Options for LaTeX output ---------------------------------------------
v2.0.3,The paper size ('letterpaper' or 'a4paper').
v2.0.3,
v2.0.3,"'papersize': 'letterpaper',"
v2.0.3,"The font size ('10pt', '11pt' or '12pt')."
v2.0.3,
v2.0.3,"'pointsize': '10pt',"
v2.0.3,Additional stuff for the LaTeX preamble.
v2.0.3,
v2.0.3,"'preamble': '',"
v2.0.3,Latex figure (float) alignment
v2.0.3,
v2.0.3,"'figure_align': 'htbp',"
v2.0.3,Grouping the document tree into LaTeX files. List of tuples
v2.0.3,"(source start file, target name, title,"
v2.0.3,"author, documentclass [howto, manual, or own class])."
v2.0.3,-- Options for manual page output ---------------------------------------
v2.0.3,One entry per manual page. List of tuples
v2.0.3,"(source start file, name, description, authors, manual section)."
v2.0.3,-- Options for Texinfo output -------------------------------------------
v2.0.3,Grouping the document tree into Texinfo files. List of tuples
v2.0.3,"(source start file, target name, title, author,"
v2.0.3,"dir menu entry, description, category)"
v2.0.3,https://recommonmark.readthedocs.io/en/latest/
v2.0.3,coding: utf-8
v2.0.3,coding: utf-8
v2.0.3,pylint: skip-file
v2.0.3,we don't need lib_lightgbm while building docs
v2.0.3,coding: utf-8
v2.0.3,pylint: skip-file
v2.0.3,check saved model persistence
v2.0.3,"we need to check the consistency of model file here, so test for exact equal"
v2.0.3,"check early stopping is working. Make it stop very early, so the scores should be very close to zero"
v2.0.3,"scores likely to be different, but prediction should still be the same"
v2.0.3,check pmml
v2.0.3,coding: utf-8
v2.0.3,pylint: skip-file
v2.0.3,coding: utf-8
v2.0.3,pylint: skip-file
v2.0.3,coding: utf-8
v2.0.3,pylint: skip-file
v2.0.3,no early stopping
v2.0.3,early stopping occurs
v2.0.3,test custom eval metrics
v2.0.3,test dump model
v2.0.3,"shuffle = False, override metric in params"
v2.0.3,"shuffle = True, callbacks"
v2.0.3,self defined folds
v2.0.3,lambdarank
v2.0.3,test feature_names with whitespaces
v2.0.3,convert from one-based to  zero-based index
v2.0.3,convert from boundaries to size
v2.0.3,--- start Booster interfaces
v2.0.3,create boosting
v2.0.3,initialize the boosting
v2.0.3,create objective function
v2.0.3,initialize the objective function
v2.0.3,create training metric
v2.0.3,reset the boosting
v2.0.3,create objective function
v2.0.3,initialize the objective function
v2.0.3,some help functions used to convert data
v2.0.3,Row iterator of on column for CSC matrix
v2.0.3,"return value at idx, only can access by ascent order"
v2.0.3,"return next non-zero pair, if index < 0, means no more data"
v2.0.3,start of c_api functions
v2.0.3,sample data first
v2.0.3,sample data first
v2.0.3,sample data first
v2.0.3,no more data
v2.0.3,---- start of booster
v2.0.3,---- start of some help functions
v2.0.3,set number of threads for openmp
v2.0.3,check for alias
v2.0.3,read parameters from config file
v2.0.3,"remove str after ""#"""
v2.0.3,Command-line has higher priority
v2.0.3,check for alias again
v2.0.3,load configs
v2.0.3,prediction is needed if using input initial model(continued train)
v2.0.3,need to continue training
v2.0.3,sync up random seed for data partition
v2.0.3,load Training data
v2.0.3,load data for parallel training
v2.0.3,load data for single machine
v2.0.3,need save binary file
v2.0.3,create training metric
v2.0.3,only when have metrics then need to construct validation data
v2.0.3,"Add validation data, if it exists"
v2.0.3,add
v2.0.3,need save binary file
v2.0.3,add metric for validation data
v2.0.3,output used time on each iteration
v2.0.3,need init network
v2.0.3,sync global random seed for feature patition
v2.0.3,create boosting
v2.0.3,create objective function
v2.0.3,load training data
v2.0.3,initialize the objective function
v2.0.3,initialize the boosting
v2.0.3,add validation data into boosting
v2.0.3,output used time per iteration
v2.0.3,save model to file
v2.0.3,convert model to if-else statement code
v2.0.3,create predictor
v2.0.3,no need to sync if not parallel learning
v2.0.3,counts for all labels
v2.0.3,"start from top label, and accumulate DCG"
v2.0.3,counts for all labels
v2.0.3,calculate k Max DCG by one pass
v2.0.3,get sorted indices by score
v2.0.3,calculate dcg
v2.0.3,get sorted indices by score
v2.0.3,calculate multi dcg by one pass
v2.0.3,wait for all client start up
v2.0.3,default set to -1
v2.0.3,"distance at k-th communication, distance[k] = 2^k"
v2.0.3,set incoming rank at k-th commuication
v2.0.3,set outgoing rank at k-th commuication
v2.0.3,defalut set as -1
v2.0.3,construct all recursive halving map for all machines
v2.0.3,let 1 << k <= num_machines
v2.0.3,distance of each communication
v2.0.3,"if num_machines = 2^k, don't need to group machines"
v2.0.3,"communication direction, %2 == 0 is positive"
v2.0.3,neighbor at k-th communication
v2.0.3,receive data block at k-th communication
v2.0.3,send data block at k-th communication
v2.0.3,"if num_machines != 2^k, need to group machines"
v2.0.3,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.0.3,let left machine as group leader
v2.0.3,"cache block information for groups, group with 2 machines will have double block size"
v2.0.3,convert from group to node leader
v2.0.3,convert from node to group
v2.0.3,meet new group
v2.0.3,add block len for this group
v2.0.3,calculate the group block start
v2.0.3,not need to construct
v2.0.3,get receive block informations
v2.0.3,accumulate block len
v2.0.3,get send block informations
v2.0.3,accumulate block len
v2.0.3,static member definition
v2.0.3,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.0.3,assign the blocks to every rank.
v2.0.3,do reduce scatter
v2.0.3,do all gather
v2.0.3,assign blocks
v2.0.3,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.0.3,copy back
v2.0.3,assign blocks
v2.0.3,start all gather
v2.0.3,use output as receive buffer
v2.0.3,get current local block size
v2.0.3,get out rank
v2.0.3,get send information
v2.0.3,get in rank
v2.0.3,get recv information
v2.0.3,send and recv at same time
v2.0.3,rotate in-place
v2.0.3,send local data to neighbor first
v2.0.3,receive neighbor data first
v2.0.3,reduce
v2.0.3,start recursive halfing
v2.0.3,get target
v2.0.3,get send information
v2.0.3,get recv information
v2.0.3,send and recv at same time
v2.0.3,reduce
v2.0.3,send result to neighbor
v2.0.3,receive result from neighbor
v2.0.3,copy result
v2.0.3,start up socket
v2.0.3,parse clients from file
v2.0.3,get ip list of local machine
v2.0.3,get local rank
v2.0.3,construct listener
v2.0.3,construct communication topo
v2.0.3,construct linkers
v2.0.3,free listener
v2.0.3,set timeout
v2.0.3,accept incoming socket
v2.0.3,receive rank
v2.0.3,add new socket
v2.0.3,save ranks that need to connect with
v2.0.3,start listener
v2.0.3,start connect
v2.0.3,let smaller rank connect to larger rank
v2.0.3,send local rank
v2.0.3,wait for listener
v2.0.3,print connected linkers
v2.0.3,Get some statistic from 2 line
v2.0.3,if only have one line on file
v2.0.3,get column names
v2.0.3,load label idx first
v2.0.3,erase label column name
v2.0.3,load ignore columns
v2.0.3,load weight idx
v2.0.3,load group idx
v2.0.3,don't support query id in data file when training in parallel
v2.0.3,read data to memory
v2.0.3,sample data
v2.0.3,construct feature bin mappers
v2.0.3,initialize label
v2.0.3,extract features
v2.0.3,sample data from file
v2.0.3,construct feature bin mappers
v2.0.3,initialize label
v2.0.3,extract features
v2.0.3,load data from binary file
v2.0.3,check meta data
v2.0.3,need to check training data
v2.0.3,read data in memory
v2.0.3,initialize label
v2.0.3,extract features
v2.0.3,Get number of lines of data file
v2.0.3,initialize label
v2.0.3,extract features
v2.0.3,load data from binary file
v2.0.3,not need to check validation data
v2.0.3,check meta data
v2.0.3,buffer to read binary file
v2.0.3,check token
v2.0.3,read size of header
v2.0.3,re-allocmate space if not enough
v2.0.3,read header
v2.0.3,get header
v2.0.3,num_groups
v2.0.3,real_feature_idx_
v2.0.3,feature2group
v2.0.3,feature2subfeature
v2.0.3,group_bin_boundaries
v2.0.3,group_feature_start_
v2.0.3,group_feature_cnt_
v2.0.3,get feature names
v2.0.3,write feature names
v2.0.3,read size of meta data
v2.0.3,re-allocate space if not enough
v2.0.3,read meta data
v2.0.3,load meta data
v2.0.3,sample local used data if need to partition
v2.0.3,"if not contain query file, minimal sample unit is one record"
v2.0.3,"if contain query file, minimal sample unit is one query"
v2.0.3,if is new query
v2.0.3,read feature data
v2.0.3,read feature size
v2.0.3,re-allocate space if not enough
v2.0.3,fill feature_names_ if not header
v2.0.3,---- private functions ----
v2.0.3,"if features are ordered, not need to use hist_buf"
v2.0.3,read all lines
v2.0.3,get query data
v2.0.3,"if not contain query data, minimal sample unit is one record"
v2.0.3,"if contain query data, minimal sample unit is one query"
v2.0.3,if is new query
v2.0.3,get query data
v2.0.3,"if not contain query file, minimal sample unit is one record"
v2.0.3,"if contain query file, minimal sample unit is one query"
v2.0.3,if is new query
v2.0.3,parse features
v2.0.3,-1 means doesn't use this feature
v2.0.3,"check the range of label_idx, weight_idx and group_idx"
v2.0.3,fill feature_names_ if not header
v2.0.3,start find bins
v2.0.3,"if only one machine, find bin locally"
v2.0.3,"if have multi-machines, need to find bin distributed"
v2.0.3,different machines will find bin for different features
v2.0.3,start and len will store the process feature indices for different machines
v2.0.3,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.0.3,get max_bin
v2.0.3,sync global max_bin
v2.0.3,get size of bin mapper with max_bin size
v2.0.3,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.0.3,find local feature bins and copy to buffer
v2.0.3,free
v2.0.3,convert to binary size
v2.0.3,gather global feature bin mappers
v2.0.3,restore features bins from buffer
v2.0.3,if doesn't need to prediction with initial model
v2.0.3,parser
v2.0.3,set label
v2.0.3,free processed line:
v2.0.3,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.3,text_reader_->Lines()[i].shrink_to_fit();
v2.0.3,push data
v2.0.3,if is used feature
v2.0.3,if need to prediction with initial model
v2.0.3,parser
v2.0.3,set initial score
v2.0.3,set label
v2.0.3,free processed line:
v2.0.3,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0.3,text_reader_->Lines()[i].shrink_to_fit();
v2.0.3,push data
v2.0.3,if is used feature
v2.0.3,metadata_ will manage space of init_score
v2.0.3,text data can be free after loaded feature values
v2.0.3,parser
v2.0.3,set initial score
v2.0.3,set label
v2.0.3,push data
v2.0.3,if is used feature
v2.0.3,only need part of data
v2.0.3,need full data
v2.0.3,metadata_ will manage space of init_score
v2.0.3,read size of token
v2.0.3,deep copy function for BinMapper
v2.0.3,mean size for one bin
v2.0.3,need a new bin
v2.0.3,update bin upper bound
v2.0.3,last bin upper bound
v2.0.3,find distinct_values first
v2.0.3,push zero in the front
v2.0.3,push zero in the back
v2.0.3,convert to int type first
v2.0.3,sort by counts
v2.0.3,will ignore the categorical of small counts
v2.0.3,check trival(num_bin_ == 1) feature
v2.0.3,check useless bin
v2.0.3,calculate sparse rate
v2.0.3,sparse threshold
v2.0.3,"for lambdarank, it needs query data for partition data in parallel learning"
v2.0.3,need convert query_id to boundaries
v2.0.3,check weights
v2.0.3,check query boundries
v2.0.3,contain initial score file
v2.0.3,check weights
v2.0.3,get local weights
v2.0.3,check query boundries
v2.0.3,get local query boundaries
v2.0.3,contain initial score file
v2.0.3,get local initial scores
v2.0.3,re-load query weight
v2.0.3,save to nullptr
v2.0.3,save to nullptr
v2.0.3,save to nullptr
v2.0.3,default weight file name
v2.0.3,default weight file name
v2.0.3,use first line to count number class
v2.0.3,default query file name
v2.0.3,root is in the depth 0
v2.0.3,update parent info
v2.0.3,if cur node is left child
v2.0.3,add new node
v2.0.3,add two new leaves
v2.0.3,update new leaves
v2.0.3,save current leaf value to internal node before change
v2.0.3,update leaf depth
v2.0.3,non-leaf
v2.0.3,leaf
v2.0.3,non-leaf
v2.0.3,left subtree
v2.0.3,right subtree
v2.0.3,leaf
v2.0.3,load main config types
v2.0.3,generate seeds by seed.
v2.0.3,sub-config setup
v2.0.3,check for conflicts
v2.0.3,clear old metrics
v2.0.3,to lower
v2.0.3,split
v2.0.3,remove duplicate
v2.0.3,"check if objective_type, metric_type, and num_class match"
v2.0.3,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.0.3,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.3,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0.3,default eval ndcg @[1-5]
v2.0.3,"filter is based on sampling data, so decrease its range"
v2.0.3,put dense feature first
v2.0.3,sort by non zero cnt
v2.0.3,"sort by non zero cnt, bigger first"
v2.0.3,"take apart small sparse group, due it will not gain on speed"
v2.0.3,shuffle groups
v2.0.3,get num_features
v2.0.3,get bin_mappers
v2.0.3,copy feature bin mapper data
v2.0.3,copy feature bin mapper data
v2.0.3,"if not pass a filename, just append "".bin"" of original file"
v2.0.3,get size of header
v2.0.3,size of feature names
v2.0.3,write header
v2.0.3,write feature names
v2.0.3,get size of meta data
v2.0.3,write meta data
v2.0.3,write feature data
v2.0.3,get size of feature
v2.0.3,write feature
v2.0.3,feature is not used
v2.0.3,construct histograms for smaller leaf
v2.0.3,if not use ordered bin
v2.0.3,used ordered bin
v2.0.3,feature is not used
v2.0.3,construct histograms for smaller leaf
v2.0.3,if not use ordered bin
v2.0.3,used ordered bin
v2.0.3,fixed hessian.
v2.0.3,feature is not used
v2.0.3,construct histograms for smaller leaf
v2.0.3,if not use ordered bin
v2.0.3,used ordered bin
v2.0.3,feature is not used
v2.0.3,construct histograms for smaller leaf
v2.0.3,if not use ordered bin
v2.0.3,used ordered bin
v2.0.3,fixed hessian.
v2.0.3,set zero
v2.0.3,predict all the trees for one iteration
v2.0.3,check early stopping
v2.0.3,margin_threshold will be captured by value
v2.0.3,copy and sort
v2.0.3,margin_threshold will be captured by value
v2.0.3,init tree learner
v2.0.3,push training metrics
v2.0.3,"not same training data, need reset score and others"
v2.0.3,create score tracker
v2.0.3,update score
v2.0.3,create buffer for gradients and hessians
v2.0.3,get max feature index
v2.0.3,get label index
v2.0.3,get feature names
v2.0.3,"if need bagging, create buffer"
v2.0.3,reset config for tree learner
v2.0.3,+ 1 here for the binary classification
v2.0.3,multi-class
v2.0.3,binary class
v2.0.3,"for a validation dataset, we need its score and metric"
v2.0.3,update score
v2.0.3,"random bagging, minimal unit is one record"
v2.0.3,if need bagging
v2.0.3,set bagging data to tree learner
v2.0.3,get subset
v2.0.3,we need to predict out-of-bag scores of data for boosting
v2.0.3,"boosting from average prediction. It doesn't work well for classification, remove it for now."
v2.0.3,boosting first
v2.0.3,bagging logic
v2.0.3,get sub gradients
v2.0.3,cannot multi-threading here.
v2.0.3,shrinkage by learning rate
v2.0.3,update score
v2.0.3,only add default score one-time
v2.0.3,add model
v2.0.3,reset score
v2.0.3,remove model
v2.0.3,print message for metric
v2.0.3,pop last early_stopping_round_ models
v2.0.3,update training score
v2.0.3,update validation score
v2.0.3,print training metric
v2.0.3,print validation metric
v2.0.3,objective function will calculate gradients and hessians
v2.0.3,PredictRaw
v2.0.3,Predict
v2.0.3,PredictLeafIndex
v2.0.3,output model type
v2.0.3,output number of class
v2.0.3,output label index
v2.0.3,output max_feature_idx
v2.0.3,output objective
v2.0.3,output tree models
v2.0.3,use serialized string to restore this object
v2.0.3,get number of classes
v2.0.3,get index of label
v2.0.3,get max_feature_idx first
v2.0.3,get boost_from_average_
v2.0.3,get feature names
v2.0.3,get tree models
v2.0.3,store the importance first
v2.0.3,sort the importance
v2.0.3,Get the max size of pool
v2.0.3,at least need 2 leaves
v2.0.3,push split information for all leaves
v2.0.3,get ordered bin
v2.0.3,check existing for ordered bin
v2.0.3,initialize splits for leaf
v2.0.3,initialize data partition
v2.0.3,initialize ordered gradients and hessians
v2.0.3,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.3,get ordered bin
v2.0.3,initialize splits for leaf
v2.0.3,initialize data partition
v2.0.3,initialize ordered gradients and hessians
v2.0.3,"if has ordered bin, need to allocate a buffer to fast split"
v2.0.3,Get the max size of pool
v2.0.3,at least need 2 leaves
v2.0.3,push split information for all leaves
v2.0.3,some initial works before training
v2.0.3,root leaf
v2.0.3,only root leaf can be splitted on first time
v2.0.3,some initial works before finding best split
v2.0.3,find best threshold for every feature
v2.0.3,find best split from all features
v2.0.3,Get a leaf with max split gain
v2.0.3,Get split information for best leaf
v2.0.3,"cannot split, quit"
v2.0.3,split tree with best leaf
v2.0.3,avoid zero hessians.
v2.0.3,reset histogram pool
v2.0.3,initialize used features
v2.0.3,Get used feature at current tree
v2.0.3,initialize data partition
v2.0.3,reset the splits for leaves
v2.0.3,Sumup for root
v2.0.3,use all data
v2.0.3,"use bagging, only use part of data"
v2.0.3,"if has ordered bin, need to initialize the ordered bin"
v2.0.3,"use all data, pass nullptr"
v2.0.3,"bagging, only use part of data"
v2.0.3,mark used data
v2.0.3,initialize ordered bin
v2.0.3,check depth of current leaf
v2.0.3,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.0.3,no enough data to continue
v2.0.3,only have root
v2.0.3,put parent(left) leaf's histograms into larger leaf's histograms
v2.0.3,put parent(left) leaf's histograms to larger leaf's histograms
v2.0.3,split for the ordered bin
v2.0.3,mark data that at left-leaf
v2.0.3,split the ordered bin
v2.0.3,construct smaller leaf
v2.0.3,construct larger leaf
v2.0.3,find splits
v2.0.3,only has root leaf
v2.0.3,find best threshold for larger child
v2.0.3,left = parent
v2.0.3,"split tree, will return right leaf"
v2.0.3,split data partition
v2.0.3,init the leaves that used on next iteration
v2.0.3,get feature partition
v2.0.3,get local used features
v2.0.3,get best split at smaller leaf
v2.0.3,find local best split for larger leaf
v2.0.3,sync global best info
v2.0.3,copy back
v2.0.3,update best split
v2.0.3,"instantiate template classes, otherwise linker cannot find the code"
v2.0.3,initialize SerialTreeLearner
v2.0.3,Get local rank and global machine size
v2.0.3,allocate buffer for communication
v2.0.3,generate feature partition for current tree
v2.0.3,get local used feature
v2.0.3,get block start and block len for reduce scatter
v2.0.3,get buffer_write_start_pos_
v2.0.3,get buffer_read_start_pos_
v2.0.3,sync global data sumup info
v2.0.3,global sumup reduce
v2.0.3,copy back
v2.0.3,set global sumup info
v2.0.3,init global data count in leaf
v2.0.3,construct local histograms
v2.0.3,copy to buffer
v2.0.3,Reduce scatter for histogram
v2.0.3,restore global histograms from buffer
v2.0.3,find best threshold for smaller child
v2.0.3,only root leaf
v2.0.3,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.0.3,find best threshold for larger child
v2.0.3,find local best split for larger leaf
v2.0.3,sync global best info
v2.0.3,set best split
v2.0.3,need update global number of data in leaf
v2.0.3,"instantiate template classes, otherwise linker cannot find the code"
v2.0.3,initialize SerialTreeLearner
v2.0.3,some additional variables needed for GPU trainer
v2.0.3,Initialize GPU buffers and kernels
v2.0.3,some functions used for debugging the GPU histogram construction
v2.0.3,"printf(""grad %g != %g (%d ULPs)\n"", h1[i].sum_gradients, h2[i].sum_gradients, ulps);"
v2.0.3,goto err;
v2.0.3,"printf(""hessian %g != %g (%d ULPs)\n"", h1[i].sum_hessians, h2[i].sum_hessians, ulps);"
v2.0.3,goto err;
v2.0.3,"we roughly want 256 workgroups per device, and we have num_dense_feature4_ feature tuples."
v2.0.3,also guarantee that there are at least 2K examples per workgroup
v2.0.3,return 0;
v2.0.3,"we have already copied ordered gradients, ordered hessians and indices to GPU"
v2.0.3,decide the best number of workgroups working on one feature4 tuple
v2.0.3,set work group size based on feature size
v2.0.3,each 2^exp_workgroups_per_feature workgroups work on a feature4 tuple
v2.0.3,we need to refresh the kernel arguments after reallocating
v2.0.3,The only argument that needs to be changed later is num_data_
v2.0.3,"the GPU kernel will process all features in one call, and each"
v2.0.3,2^exp_workgroups_per_feature (compile time constant) workgroup will
v2.0.3,process one feature4 tuple
v2.0.3,"for the root node, indices are not copied"
v2.0.3,"for constant hessian, hessians are not copied except for the root node"
v2.0.3,there will be 2^exp_workgroups_per_feature = num_workgroups / num_dense_feature4 sub-histogram per feature4
v2.0.3,and we will launch num_feature workgroups for this kernel
v2.0.3,will launch threads for all features
v2.0.3,"the queue should be asynchrounous, and we will can WaitAndGetHistograms() before we start processing dense feature groups"
v2.0.3,copy the results asynchronously. Size depends on if double precision is used
v2.0.3,we will wait for this object in WaitAndGetHistograms
v2.0.3,"when the output is ready, the computation is done"
v2.0.3,values of this feature has been redistributed to multiple bins; need a reduction here
v2.0.3,how many feature-group tuples we have
v2.0.3,leave some safe margin for prefetching
v2.0.3,256 work-items per workgroup. Each work-item prefetches one tuple for that feature
v2.0.3,clear sparse/dense maps
v2.0.3,do nothing if no features can be processed on GPU
v2.0.3,"allocate memory for all features (FIXME: 4 GB barrier on some devices, need to split to multiple buffers)"
v2.0.3,unpin old buffer if necessary before destructing them
v2.0.3,"make ordered_gradients and hessians larger (including extra room for prefetching), and pin them"
v2.0.3,allocate space for gradients and hessians on device
v2.0.3,we will copy gradients and hessians in after ordered_gradients_ and ordered_hessians_ are constructed
v2.0.3,"allocate feature mask, for disabling some feature-groups' histogram calculation"
v2.0.3,copy indices to the device
v2.0.3,histogram bin entry size depends on the precision (single/double)
v2.0.3,"create output buffer, each feature has a histogram with device_bin_size_ bins,"
v2.0.3,each work group generates a sub-histogram of dword_features_ features.
v2.0.3,"only initialize once here, as this will not need to change when ResetTrainingData() is called"
v2.0.3,create atomic counters for inter-group coordination
v2.0.3,"The output buffer is allocated to host directly, to overlap compute and data transfer"
v2.0.3,find the dense feature-groups and group then into Feature4 data structure (several feature-groups packed into 4 bytes)
v2.0.3,looking for dword_features_ non-sparse feature-groups
v2.0.3,decide if we need to redistribute the bin
v2.0.3,multiplier must be a power of 2
v2.0.3,device_bin_mults_.push_back(1);
v2.0.3,found
v2.0.3,for data transfer time
v2.0.3,"Now generate new data structure feature4, and copy data to the device"
v2.0.3,"preallocate arrays for all threads, and pin them"
v2.0.3,building Feature4 bundles; each thread handles dword_features_ features
v2.0.3,one feature datapoint is 4 bits
v2.0.3,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.3,one feature datapoint is one byte
v2.0.3,"this guarantees that the RawGet() function is inlined, rather than using virtual function dispatching"
v2.0.3,Dense bin
v2.0.3,Dense 4-bit bin
v2.0.3,working on the remaining (less than dword_features_) feature groups
v2.0.3,fill the leftover features
v2.0.3,"fill this empty feature with some ""random"" value"
v2.0.3,"fill this empty feature with some ""random"" value"
v2.0.3,copying the last 1 to (dword_features - 1) feature-groups in the last tuple
v2.0.3,deallocate pinned space for feature copying
v2.0.3,data transfer time
v2.0.3,"for other types of failure, build log might not be available; program.build_log() can crash"
v2.0.3,"Something bad happened. Just return ""No log available."""
v2.0.3,"build is okay, log may contain warnings"
v2.0.3,destroy any old kernels
v2.0.3,create OpenCL kernels for different number of workgroups per feature
v2.0.3,currently we don't use constant memory
v2.0.3,"compile the GPU kernel depending if double precision is used, constant hessian is used, etc"
v2.0.3,kernel with indices in an array
v2.0.3,"kernel with all features enabled, with elimited branches"
v2.0.3,"kernel with all data indices (for root node, and assumes that root node always uses all features)"
v2.0.3,do nothing if no features can be processed on GPU
v2.0.3,The only argument that needs to be changed later is num_data_
v2.0.3,"hessian is passed as a parameter, but it is not available now."
v2.0.3,hessian will be set in BeforeTrain()
v2.0.3,"Get the max bin size, used for selecting best GPU kernel"
v2.0.3,initialize GPU
v2.0.3,determine which kernel to use based on the max number of bins
v2.0.3,setup GPU kernel arguments after we allocating all the buffers
v2.0.3,check if we need to recompile the GPU kernel (is_constant_hessian changed)
v2.0.3,this should rarely occur
v2.0.3,GPU memory has to been reallocated because data may have been changed
v2.0.3,setup GPU kernel arguments after we allocating all the buffers
v2.0.3,Copy initial full hessians and gradients to GPU.
v2.0.3,"We start copying as early as possible, instead of at ConstructHistogram()."
v2.0.3,setup hessian parameters only
v2.0.3,hessian is passed as a parameter
v2.0.3,use bagging
v2.0.3,"On GPU, we start copying indices, gradients and hessians now, instead at ConstructHistogram()"
v2.0.3,copy used gradients and hessians to ordered buffer
v2.0.3,transfer the indices to GPU
v2.0.3,transfer hessian to GPU
v2.0.3,setup hessian parameters only
v2.0.3,hessian is passed as a parameter
v2.0.3,transfer gradients to GPU
v2.0.3,only have root
v2.0.3,"Copy indices, gradients and hessians as early as possible"
v2.0.3,only need to initialize for smaller leaf
v2.0.3,Get leaf boundary
v2.0.3,copy indices to the GPU:
v2.0.3,copy ordered hessians to the GPU:
v2.0.3,copy ordered gradients to the GPU:
v2.0.3,do nothing if no features can be processed on GPU
v2.0.3,copy data indices if it is not null
v2.0.3,generate and copy ordered_gradients if gradients is not null
v2.0.3,generate and copy ordered_hessians if hessians is not null
v2.0.3,converted indices in is_feature_used to feature-group indices
v2.0.3,construct the feature masks for dense feature-groups
v2.0.3,"if no feature group is used, just return and do not use GPU"
v2.0.3,"if not all feature groups are used, we need to transfer the feature mask to GPU"
v2.0.3,"otherwise, we will use a specialized GPU kernel with all feature groups enabled"
v2.0.3,"All data have been prepared, now run the GPU kernel"
v2.0.3,construct smaller leaf
v2.0.3,ConstructGPUHistogramsAsync will return true if there are availabe feature gourps dispatched to GPU
v2.0.3,then construct sparse features on CPU
v2.0.3,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.3,"wait for GPU to finish, only if GPU is actually used"
v2.0.3,use double precision
v2.0.3,use single precision
v2.0.3,"Compare GPU histogram with CPU histogram, useful for debuggin GPU code problem"
v2.0.3,#define GPU_DEBUG_COMPARE
v2.0.3,construct larger leaf
v2.0.3,then construct sparse features on CPU
v2.0.3,We set data_indices to null to avoid rebuilding ordered gradients/hessians
v2.0.3,"wait for GPU to finish, only if GPU is actually used"
v2.0.3,use double precision
v2.0.3,use single precision
v2.0.3,do some sanity check for the GPU algorithm
v2.0.3,limit top k
v2.0.3,get max bin
v2.0.3,calculate buffer size
v2.0.3,"left and right on same time, so need double size"
v2.0.3,initialize histograms for global
v2.0.3,sync global data sumup info
v2.0.3,set global sumup info
v2.0.3,init global data count in leaf
v2.0.3,get local sumup
v2.0.3,get local sumup
v2.0.3,get mean number on machines
v2.0.3,weighted gain
v2.0.3,get top k
v2.0.3,"Copy histogram to buffer, and Get local aggregate features"
v2.0.3,copy histograms.
v2.0.3,copy smaller leaf histograms first
v2.0.3,mark local aggregated feature
v2.0.3,copy
v2.0.3,then copy larger leaf histograms
v2.0.3,mark local aggregated feature
v2.0.3,copy
v2.0.3,use local data to find local best splits
v2.0.3,find splits
v2.0.3,only has root leaf
v2.0.3,find best threshold for larger child
v2.0.3,local voting
v2.0.3,gather
v2.0.3,get all top-k from all machines
v2.0.3,global voting
v2.0.3,copy local histgrams to buffer
v2.0.3,Reduce scatter for histogram
v2.0.3,find best split from local aggregated histograms
v2.0.3,restore from buffer
v2.0.3,find best threshold
v2.0.3,restore from buffer
v2.0.3,find best threshold
v2.0.3,find local best
v2.0.3,find local best split for larger leaf
v2.0.3,sync global best info
v2.0.3,copy back
v2.0.3,set the global number of data for leaves
v2.0.3,init the global sumup info
v2.0.3,"instantiate template classes, otherwise linker cannot find the code"
v2.0,coding: utf-8
v2.0,"pylint: disable=invalid-name, exec-used"
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, W0105"
v2.0,Most of legacy advanced options becomes callbacks
v2.0,check evaluation result.
v2.0,run preprocessing on the data set if needed
v2.0,setup callbacks
v2.0,coding: utf-8
v2.0,pylint: disable = C0103
v2.0,"simplejson does not support Python 3.2, it throws a SyntaxError"
v2.0,because of u'...' Unicode literals.
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, W0105, C0111, C0301"
v2.0,Switch to using a multiclass objective in the underlying LGBM instance
v2.0,coding: utf-8
v2.0,coding: utf-8
v2.0,pylint: disable = C0103
v2.0,coding: utf-8
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, C0111, C0301"
v2.0,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v2.0,TypeError: obj is not a string or a number
v2.0,ValueError: invalid literal
v2.0,load init score
v2.0,need re group init score
v2.0,set feature names
v2.0,"group data from LightGBM is boundaries data, need to convert to group size"
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, W0105, C0301"
v2.0,Callback environment used by callbacks
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, C0111"
v2.0,load or create your dataset
v2.0,create dataset for lightgbm
v2.0,"if you want to re-use data, remember to set free_raw_data=False"
v2.0,specify your configurations as a dict
v2.0,generate a feature name
v2.0,feature_name and categorical_feature
v2.0,check feature name
v2.0,save model to file
v2.0,load model to predict
v2.0,can only predict with the best iteration (or the saving iteration)
v2.0,eval with loaded model
v2.0,dump model with pickle
v2.0,load model with pickle to predict
v2.0,can predict with any iteration when loaded in pickle way
v2.0,eval with loaded model
v2.0,continue training
v2.0,init_model accepts:
v2.0,1. model file name
v2.0,2. Booster()
v2.0,decay learning rates
v2.0,learning_rates accepts:
v2.0,1. list/tuple with length = num_boost_round
v2.0,2. function(curr_iter)
v2.0,change other parameters during training
v2.0,self-defined objective function
v2.0,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v2.0,log likelihood loss
v2.0,self-defined eval metric
v2.0,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v2.0,binary error
v2.0,callback
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, C0111"
v2.0,load or create your dataset
v2.0,train
v2.0,predict
v2.0,eval
v2.0,feature importances
v2.0,other scikit-learn modules
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, C0111"
v2.0,load or create your dataset
v2.0,create dataset for lightgbm
v2.0,specify your configurations as a dict
v2.0,train
v2.0,coding: utf-8
v2.0,"pylint: disable = invalid-name, C0111"
v2.0,load or create your dataset
v2.0,create dataset for lightgbm
v2.0,specify your configurations as a dict
v2.0,train
v2.0,save model to file
v2.0,predict
v2.0,eval
v2.0,dump model to json (and save to file)
v2.0,feature importances
v2.0,coding: utf-8
v2.0,"pylint: disable = C0111, C0103"
v2.0,print out the pmml for a decision tree
v2.0,specify the objective as function name and binarySplit for
v2.0,splitCharacteristic because each node has 2 children
v2.0,"list each feature name as a mining field, and treat all outliers as is,"
v2.0,unless specified
v2.0,begin printing out the decision tree
v2.0,open the model file and then process it
v2.0,ignore first 6 and empty lines
v2.0,print out data dictionary entries for each column
v2.0,"not adding any interval definition, all values are currently"
v2.0,valid
v2.0,"list each feature name as a mining field, and treat all outliers"
v2.0,"as is, unless specified"
v2.0,read each array that contains pertinent information for the pmml
v2.0,these arrays will be used to recreate the traverse the decision tree
v2.0,coding: utf-8
v2.0,pylint: skip-file
v2.0,coding: utf-8
v2.0,pylint: skip-file
v2.0,check saved model persistence
v2.0,check pmml
v2.0,coding: utf-8
v2.0,pylint: skip-file
v2.0,coding: utf-8
v2.0,pylint: skip-file
v2.0,coding: utf-8
v2.0,pylint: skip-file
v2.0,no early stopping
v2.0,early stopping occurs
v2.0,application
v2.0,boosting
v2.0,io
v2.0,metric
v2.0,network
v2.0,objective
v2.0,treelearner
v2.0,c_api
v2.0,convert from one-based to  zero-based index
v2.0,convert from boundaries to size
v2.0,--- start Booster interfaces
v2.0,application
v2.0,boosting
v2.0,io
v2.0,metric
v2.0,network
v2.0,objective
v2.0,treelearner
v2.0,c_api
v2.0,create boosting
v2.0,initialize the boosting
v2.0,create objective function
v2.0,initialize the objective function
v2.0,create training metric
v2.0,reset the boosting
v2.0,create objective function
v2.0,initialize the objective function
v2.0,not threading safe now
v2.0,boosting_->SetNumIterationForPred may be set by other thread during prediction.
v2.0,some help functions used to convert data
v2.0,Row iterator of on column for CSC matrix
v2.0,"return value at idx, only can access by ascent order"
v2.0,"return next non-zero pair, if index < 0, means no more data"
v2.0,start of c_api functions
v2.0,sample data first
v2.0,sample data first
v2.0,sample data first
v2.0,no more data
v2.0,---- start of booster
v2.0,---- start of some help functions
v2.0,set number of threads for openmp
v2.0,check for alias
v2.0,read parameters from config file
v2.0,"remove str after ""#"""
v2.0,Command-line has higher priority
v2.0,check for alias again
v2.0,load configs
v2.0,prediction is needed if using input initial model(continued train)
v2.0,need to continue training
v2.0,sync up random seed for data partition
v2.0,load Training data
v2.0,load data for parallel training
v2.0,load data for single machine
v2.0,need save binary file
v2.0,create training metric
v2.0,only when have metrics then need to construct validation data
v2.0,"Add validation data, if it exists"
v2.0,add
v2.0,need save binary file
v2.0,add metric for validation data
v2.0,output used time on each iteration
v2.0,need init network
v2.0,sync global random seed for feature patition
v2.0,create boosting
v2.0,create objective function
v2.0,load training data
v2.0,initialize the objective function
v2.0,initialize the boosting
v2.0,add validation data into boosting
v2.0,output used time per iteration
v2.0,save model to file
v2.0,create predictor
v2.0,no need to sync if not parallel learning
v2.0,counts for all labels
v2.0,"start from top label, and accumulate DCG"
v2.0,counts for all labels
v2.0,calculate k Max DCG by one pass
v2.0,get sorted indices by score
v2.0,calculate dcg
v2.0,get sorted indices by score
v2.0,calculate multi dcg by one pass
v2.0,wait for all client start up
v2.0,default set to -1
v2.0,"distance at k-th communication, distance[k] = 2^k"
v2.0,set incoming rank at k-th commuication
v2.0,set outgoing rank at k-th commuication
v2.0,defalut set as -1
v2.0,construct all recursive halving map for all machines
v2.0,let 1 << k <= num_machines
v2.0,distance of each communication
v2.0,"if num_machines = 2^k, don't need to group machines"
v2.0,"communication direction, %2 == 0 is positive"
v2.0,neighbor at k-th communication
v2.0,receive data block at k-th communication
v2.0,send data block at k-th communication
v2.0,"if num_machines != 2^k, need to group machines"
v2.0,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v2.0,let left machine as group leader
v2.0,"cache block information for groups, group with 2 machines will have double block size"
v2.0,convert from group to node leader
v2.0,convert from node to group
v2.0,meet new group
v2.0,add block len for this group
v2.0,calculate the group block start
v2.0,not need to construct
v2.0,get receive block informations
v2.0,accumulate block len
v2.0,get send block informations
v2.0,accumulate block len
v2.0,static member definition
v2.0,"if small package or small count , do it by all gather.(reduce the communication times.)"
v2.0,assign the blocks to every rank.
v2.0,do reduce scatter
v2.0,do all gather
v2.0,assign blocks
v2.0,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v2.0,copy back
v2.0,assign blocks
v2.0,start all gather
v2.0,use output as receive buffer
v2.0,get current local block size
v2.0,get out rank
v2.0,get send information
v2.0,get in rank
v2.0,get recv information
v2.0,send and recv at same time
v2.0,rotate in-place
v2.0,send local data to neighbor first
v2.0,receive neighbor data first
v2.0,reduce
v2.0,start recursive halfing
v2.0,get target
v2.0,get send information
v2.0,get recv information
v2.0,send and recv at same time
v2.0,reduce
v2.0,send result to neighbor
v2.0,receive result from neighbor
v2.0,copy result
v2.0,start up socket
v2.0,parse clients from file
v2.0,get ip list of local machine
v2.0,get local rank
v2.0,construct listener
v2.0,construct communication topo
v2.0,construct linkers
v2.0,free listener
v2.0,set timeout
v2.0,accept incoming socket
v2.0,receive rank
v2.0,add new socket
v2.0,save ranks that need to connect with
v2.0,start listener
v2.0,start connect
v2.0,let smaller rank connect to larger rank
v2.0,send local rank
v2.0,wait for listener
v2.0,print connected linkers
v2.0,Get some statistic from 2 line
v2.0,if only have one line on file
v2.0,get column names
v2.0,load label idx first
v2.0,erase label column name
v2.0,load ignore columns
v2.0,load weight idx
v2.0,load group idx
v2.0,don't support query id in data file when training in parallel
v2.0,read data to memory
v2.0,sample data
v2.0,construct feature bin mappers
v2.0,initialize label
v2.0,extract features
v2.0,sample data from file
v2.0,construct feature bin mappers
v2.0,initialize label
v2.0,extract features
v2.0,load data from binary file
v2.0,check meta data
v2.0,need to check training data
v2.0,read data in memory
v2.0,initialize label
v2.0,extract features
v2.0,Get number of lines of data file
v2.0,initialize label
v2.0,extract features
v2.0,load data from binary file
v2.0,not need to check validation data
v2.0,check meta data
v2.0,buffer to read binary file
v2.0,check token
v2.0,read size of header
v2.0,re-allocmate space if not enough
v2.0,read header
v2.0,get header
v2.0,num_groups
v2.0,real_feature_idx_
v2.0,feature2group
v2.0,feature2subfeature
v2.0,group_bin_boundaries
v2.0,group_feature_start_
v2.0,group_feature_cnt_
v2.0,get feature names
v2.0,write feature names
v2.0,read size of meta data
v2.0,re-allocate space if not enough
v2.0,read meta data
v2.0,load meta data
v2.0,sample local used data if need to partition
v2.0,"if not contain query file, minimal sample unit is one record"
v2.0,"if contain query file, minimal sample unit is one query"
v2.0,if is new query
v2.0,read feature data
v2.0,read feature size
v2.0,re-allocate space if not enough
v2.0,fill feature_names_ if not header
v2.0,---- private functions ----
v2.0,"if features are ordered, not need to use hist_buf"
v2.0,read all lines
v2.0,get query data
v2.0,"if not contain query data, minimal sample unit is one record"
v2.0,"if contain query data, minimal sample unit is one query"
v2.0,if is new query
v2.0,get query data
v2.0,"if not contain query file, minimal sample unit is one record"
v2.0,"if contain query file, minimal sample unit is one query"
v2.0,if is new query
v2.0,parse features
v2.0,-1 means doesn't use this feature
v2.0,"check the range of label_idx, weight_idx and group_idx"
v2.0,fill feature_names_ if not header
v2.0,start find bins
v2.0,"if only one machine, find bin locally"
v2.0,"if have multi-machines, need to find bin distributed"
v2.0,different machines will find bin for different features
v2.0,start and len will store the process feature indices for different machines
v2.0,"machine i will find bins for features in [ start[i], start[i] + len[i] )"
v2.0,get max_bin
v2.0,sync global max_bin
v2.0,get size of bin mapper with max_bin size
v2.0,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v2.0,find local feature bins and copy to buffer
v2.0,free
v2.0,convert to binary size
v2.0,gather global feature bin mappers
v2.0,restore features bins from buffer
v2.0,if doesn't need to prediction with initial model
v2.0,parser
v2.0,set label
v2.0,free processed line:
v2.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0,text_reader_->Lines()[i].shrink_to_fit();
v2.0,push data
v2.0,if is used feature
v2.0,if need to prediction with initial model
v2.0,parser
v2.0,set initial score
v2.0,set label
v2.0,free processed line:
v2.0,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v2.0,text_reader_->Lines()[i].shrink_to_fit();
v2.0,push data
v2.0,if is used feature
v2.0,metadata_ will manage space of init_score
v2.0,text data can be free after loaded feature values
v2.0,parser
v2.0,set initial score
v2.0,set label
v2.0,push data
v2.0,if is used feature
v2.0,only need part of data
v2.0,need full data
v2.0,metadata_ will manage space of init_score
v2.0,read size of token
v2.0,deep copy function for BinMapper
v2.0,find distinct_values first
v2.0,push zero in the front
v2.0,push zero in the back
v2.0,use distinct value is enough
v2.0,mean size for one bin
v2.0,need a new bin
v2.0,update bin upper bound
v2.0,last bin upper bound
v2.0,convert to int type first
v2.0,sort by counts
v2.0,will ignore the categorical of small counts
v2.0,check trival(num_bin_ == 1) feature
v2.0,check useless bin
v2.0,calculate sparse rate
v2.0,sparse threshold
v2.0,"for lambdarank, it needs query data for partition data in parallel learning"
v2.0,need convert query_id to boundaries
v2.0,check weights
v2.0,check query boundries
v2.0,contain initial score file
v2.0,check weights
v2.0,get local weights
v2.0,check query boundries
v2.0,get local query boundaries
v2.0,contain initial score file
v2.0,get local initial scores
v2.0,re-load query weight
v2.0,save to nullptr
v2.0,save to nullptr
v2.0,save to nullptr
v2.0,default weight file name
v2.0,default weight file name
v2.0,use first line to count number class
v2.0,default query file name
v2.0,root is in the depth 0
v2.0,update parent info
v2.0,if cur node is left child
v2.0,add new node
v2.0,add two new leaves
v2.0,update new leaves
v2.0,save current leaf value to internal node before change
v2.0,update leaf depth
v2.0,non-leaf
v2.0,leaf
v2.0,load main config types
v2.0,generate seeds by seed.
v2.0,sub-config setup
v2.0,check for conflicts
v2.0,clear old metrics
v2.0,to lower
v2.0,split
v2.0,remove duplicate
v2.0,"check if objective_type, metric_type, and num_class match"
v2.0,Change pool size to -1 (no limit) when using data parallel to reduce communication costs
v2.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v2.0,default eval ndcg @[1-5]
v2.0,get num_features
v2.0,get bin_mappers
v2.0,copy feature bin mapper data
v2.0,copy feature bin mapper data
v2.0,"if not pass a filename, just append "".bin"" of original file"
v2.0,get size of header
v2.0,size of feature names
v2.0,write header
v2.0,write feature names
v2.0,get size of meta data
v2.0,write meta data
v2.0,write feature data
v2.0,get size of feature
v2.0,write feature
v2.0,feature is not used
v2.0,construct histograms for smaller leaf
v2.0,if not use ordered bin
v2.0,used ordered bin
v2.0,only binary classification need sigmoid transform
v2.0,init tree learner
v2.0,push training metrics
v2.0,"not same training data, need reset score and others"
v2.0,create score tracker
v2.0,update score
v2.0,create buffer for gradients and hessians
v2.0,get max feature index
v2.0,get label index
v2.0,get feature names
v2.0,"if need bagging, create buffer"
v2.0,reset config for tree learner
v2.0,+ 1 here for the binary classification
v2.0,multi-class
v2.0,binary class
v2.0,"for a validation dataset, we need its score and metric"
v2.0,update score
v2.0,"random bagging, minimal unit is one record"
v2.0,if need bagging
v2.0,set bagging data to tree learner
v2.0,get subset
v2.0,we need to predict out-of-bag scores of data for boosting
v2.0,"boosting from average prediction. It doesn't work well for classification, remove it for now."
v2.0,boosting first
v2.0,bagging logic
v2.0,get sub gradients
v2.0,cannot multi-threading here.
v2.0,shrinkage by learning rate
v2.0,update score
v2.0,only add default score one-time
v2.0,add model
v2.0,reset score
v2.0,remove model
v2.0,print message for metric
v2.0,pop last early_stopping_round_ models
v2.0,update training score
v2.0,update validation score
v2.0,print training metric
v2.0,print validation metric
v2.0,objective function will calculate gradients and hessians
v2.0,output model type
v2.0,output number of class
v2.0,output label index
v2.0,output max_feature_idx
v2.0,output objective name
v2.0,output sigmoid parameter
v2.0,output tree models
v2.0,use serialized string to restore this object
v2.0,get number of classes
v2.0,get index of label
v2.0,get max_feature_idx first
v2.0,get sigmoid parameter
v2.0,get boost_from_average_
v2.0,get feature names
v2.0,get tree models
v2.0,store the importance first
v2.0,sort the importance
v2.0,if need sigmoid transform
v2.0,Get the max size of pool
v2.0,at least need 2 leaves
v2.0,push split information for all leaves
v2.0,get ordered bin
v2.0,check existing for ordered bin
v2.0,initialize splits for leaf
v2.0,initialize data partition
v2.0,initialize ordered gradients and hessians
v2.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.0,get ordered bin
v2.0,check existing for ordered bin
v2.0,initialize splits for leaf
v2.0,initialize data partition
v2.0,initialize ordered gradients and hessians
v2.0,"if has ordered bin, need to allocate a buffer to fast split"
v2.0,Get the max size of pool
v2.0,at least need 2 leaves
v2.0,push split information for all leaves
v2.0,some initial works before training
v2.0,root leaf
v2.0,only root leaf can be splitted on first time
v2.0,some initial works before finding best split
v2.0,find best threshold for every feature
v2.0,find best split from all features
v2.0,Get a leaf with max split gain
v2.0,Get split information for best leaf
v2.0,"cannot split, quit"
v2.0,split tree with best leaf
v2.0,avoid zero hessians.
v2.0,reset histogram pool
v2.0,initialize used features
v2.0,Get used feature at current tree
v2.0,initialize data partition
v2.0,reset the splits for leaves
v2.0,Sumup for root
v2.0,use all data
v2.0,"use bagging, only use part of data"
v2.0,"if has ordered bin, need to initialize the ordered bin"
v2.0,"use all data, pass nullptr"
v2.0,"bagging, only use part of data"
v2.0,mark used data
v2.0,initialize ordered bin
v2.0,check depth of current leaf
v2.0,"only need to check left leaf, since right leaf is in same level of left leaf"
v2.0,no enough data to continue
v2.0,only have root
v2.0,put parent(left) leaf's histograms into larger leaf's histograms
v2.0,put parent(left) leaf's histograms to larger leaf's histograms
v2.0,split for the ordered bin
v2.0,mark data that at left-leaf
v2.0,split the ordered bin
v2.0,construct smaller leaf
v2.0,construct larger leaf
v2.0,find splits
v2.0,only has root leaf
v2.0,find best threshold for larger child
v2.0,left = parent
v2.0,"split tree, will return right leaf"
v2.0,split data partition
v2.0,init the leaves that used on next iteration
v2.0,get feature partition
v2.0,get local used features
v2.0,get best split at smaller leaf
v2.0,find local best split for larger leaf
v2.0,sync global best info
v2.0,copy back
v2.0,update best split
v2.0,initialize SerialTreeLearner
v2.0,Get local rank and global machine size
v2.0,allocate buffer for communication
v2.0,generate feature partition for current tree
v2.0,get local used feature
v2.0,get block start and block len for reduce scatter
v2.0,get buffer_write_start_pos_
v2.0,get buffer_read_start_pos_
v2.0,sync global data sumup info
v2.0,global sumup reduce
v2.0,copy back
v2.0,set global sumup info
v2.0,init global data count in leaf
v2.0,construct local histograms
v2.0,copy to buffer
v2.0,Reduce scatter for histogram
v2.0,restore global histograms from buffer
v2.0,find best threshold for smaller child
v2.0,only root leaf
v2.0,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v2.0,find best threshold for larger child
v2.0,find local best split for larger leaf
v2.0,sync global best info
v2.0,set best split
v2.0,need update global number of data in leaf
v2.0,limit top k
v2.0,get max bin
v2.0,calculate buffer size
v2.0,"left and right on same time, so need double size"
v2.0,initialize histograms for global
v2.0,sync global data sumup info
v2.0,set global sumup info
v2.0,init global data count in leaf
v2.0,get local sumup
v2.0,get local sumup
v2.0,get mean number on machines
v2.0,weighted gain
v2.0,get top k
v2.0,"Copy histogram to buffer, and Get local aggregate features"
v2.0,copy histograms.
v2.0,copy smaller leaf histograms first
v2.0,mark local aggregated feature
v2.0,copy
v2.0,then copy larger leaf histograms
v2.0,mark local aggregated feature
v2.0,copy
v2.0,use local data to find local best splits
v2.0,construct smaller leaf
v2.0,construct larger leaf
v2.0,find splits
v2.0,only has root leaf
v2.0,find best threshold for larger child
v2.0,local voting
v2.0,gather
v2.0,get all top-k from all machines
v2.0,global voting
v2.0,copy local histgrams to buffer
v2.0,Reduce scatter for histogram
v2.0,find best split from local aggregated histograms
v2.0,restore from buffer
v2.0,find best threshold
v2.0,restore from buffer
v2.0,find best threshold
v2.0,find local best
v2.0,find local best split for larger leaf
v2.0,sync global best info
v2.0,copy back
v2.0,set the global number of data for leaves
v2.0,init the global sumup info
v1,coding: utf-8
v1,"pylint: disable=invalid-name, exec-used"
v1,coding: utf-8
v1,"pylint: disable = invalid-name, W0105"
v1,Most of legacy advanced options becomes callbacks
v1,check evaluation result.
v1,run preprocessing on the data set if needed
v1,setup callbacks
v1,coding: utf-8
v1,pylint: disable = C0103
v1,"simplejson does not support Python 3.2, it throws a SyntaxError"
v1,because of u'...' Unicode literals.
v1,coding: utf-8
v1,"pylint: disable = invalid-name, W0105, C0111, C0301"
v1,Switch to using a multiclass objective in the underlying LGBM instance
v1,coding: utf-8
v1,coding: utf-8
v1,pylint: disable = C0103
v1,coding: utf-8
v1,coding: utf-8
v1,"pylint: disable = invalid-name, C0111, C0301"
v1,"pylint: disable = R0912, R0913, R0914, W0105, W0201, W0212"
v1,TypeError: obj is not a string or a number
v1,ValueError: invalid literal
v1,load init score
v1,need re group init score
v1,set feature names
v1,"group data from LightGBM is boundaries data, need to convert to group size"
v1,coding: utf-8
v1,"pylint: disable = invalid-name, W0105, C0301"
v1,Callback environment used by callbacks
v1,coding: utf-8
v1,"pylint: disable = invalid-name, C0111"
v1,load or create your dataset
v1,create dataset for lightgbm
v1,"if you want to re-use data, remember to set free_raw_data=False"
v1,specify your configurations as a dict
v1,generate a feature name
v1,feature_name and categorical_feature
v1,check feature name
v1,save model to file
v1,continue training
v1,init_model accepts:
v1,1. model file name
v1,2. Booster()
v1,decay learning rates
v1,learning_rates accepts:
v1,1. list/tuple with length = num_boost_round
v1,2. function(curr_iter)
v1,change other parameters during training
v1,self-defined objective function
v1,"f(preds: array, train_data: Dataset) -> grad: array, hess: array"
v1,log likelihood loss
v1,self-defined eval metric
v1,"f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool"
v1,binary error
v1,callback
v1,coding: utf-8
v1,"pylint: disable = invalid-name, C0111"
v1,load or create your dataset
v1,train
v1,predict
v1,eval
v1,feature importances
v1,other scikit-learn modules
v1,coding: utf-8
v1,"pylint: disable = invalid-name, C0111"
v1,load or create your dataset
v1,create dataset for lightgbm
v1,specify your configurations as a dict
v1,train
v1,coding: utf-8
v1,"pylint: disable = invalid-name, C0111"
v1,load or create your dataset
v1,create dataset for lightgbm
v1,specify your configurations as a dict
v1,train
v1,save model to file
v1,predict
v1,eval
v1,dump model to json (and save to file)
v1,feature importances
v1,coding: utf-8
v1,"pylint: disable = C0111, C0103"
v1,print out the pmml for a decision tree
v1,specify the objective as function name and binarySplit for
v1,splitCharacteristic because each node has 2 children
v1,"list each feature name as a mining field, and treat all outliers as is,"
v1,unless specified
v1,begin printing out the decision tree
v1,open the model file and then process it
v1,ignore first 6 and empty lines
v1,print out data dictionary entries for each column
v1,"not adding any interval definition, all values are currently"
v1,valid
v1,"list each feature name as a mining field, and treat all outliers"
v1,"as is, unless specified"
v1,read each array that contains pertinent information for the pmml
v1,these arrays will be used to recreate the traverse the decision tree
v1,coding: utf-8
v1,pylint: skip-file
v1,coding: utf-8
v1,pylint: skip-file
v1,check saved model persistence
v1,check pmml
v1,coding: utf-8
v1,pylint: skip-file
v1,coding: utf-8
v1,pylint: skip-file
v1,coding: utf-8
v1,pylint: skip-file
v1,application
v1,boosting
v1,io
v1,metric
v1,network
v1,objective
v1,treelearner
v1,c_api
v1,convert from one-based to  zero-based index
v1,convert from boundaries to size
v1,--- start Booster interfaces
v1,create boosting
v1,initialize the boosting
v1,create objective function
v1,initialize the objective function
v1,create training metric
v1,reset the boosting
v1,create objective function
v1,initialize the objective function
v1,not threading safe now
v1,boosting_->SetNumIterationForPred may be set by other thread during prediction.
v1,some help functions used to convert data
v1,Row iterator of on column for CSC matrix
v1,"return value at idx, only can access by ascent order"
v1,"return next non-zero pair, if index < 0, means no more data"
v1,start of c_api functions
v1,sample data first
v1,sample data first
v1,if need expand feature set
v1,edit the feature value
v1,sample data first
v1,no more data
v1,---- start of booster
v1,---- start of some help functions
v1,set number of threads for openmp
v1,check for alias
v1,read parameters from config file
v1,"remove str after ""#"""
v1,Command-line has higher priority
v1,check for alias again
v1,load configs
v1,prediction is needed if using input initial model(continued train)
v1,need to continue training
v1,sync up random seed for data partition
v1,load Training data
v1,load data for parallel training
v1,load data for single machine
v1,need save binary file
v1,create training metric
v1,only when have metrics then need to construct validation data
v1,"Add validation data, if it exists"
v1,add
v1,need save binary file
v1,add metric for validation data
v1,output used time on each iteration
v1,need init network
v1,sync global random seed for feature patition
v1,create boosting
v1,create objective function
v1,load training data
v1,initialize the objective function
v1,initialize the boosting
v1,add validation data into boosting
v1,output used time per iteration
v1,save model to file
v1,create predictor
v1,no need to sync if not parallel learning
v1,only inited one time
v1,counts for all labels
v1,"start from top label, and accumulate DCG"
v1,counts for all labels
v1,calculate k Max DCG by one pass
v1,get sorted indices by score
v1,calculate dcg
v1,get sorted indices by score
v1,calculate multi dcg by one pass
v1,wait for all client start up
v1,default set to -1
v1,"distance at k-th communication, distance[k] = 2^k"
v1,set incoming rank at k-th commuication
v1,set outgoing rank at k-th commuication
v1,defalut set as -1
v1,construct all recursive halving map for all machines
v1,let 1 << k <= num_machines
v1,distance of each communication
v1,"if num_machines = 2^k, don't need to group machines"
v1,"communication direction, %2 == 0 is positive"
v1,neighbor at k-th communication
v1,receive data block at k-th communication
v1,send data block at k-th communication
v1,"if num_machines != 2^k, need to group machines"
v1,"group, two machine in one group, total ""rest"" groups will have 2 machines."
v1,let left machine as group leader
v1,"cache block information for groups, group with 2 machines will have double block size"
v1,convert from group to node leader
v1,convert from node to group
v1,meet new group
v1,add block len for this group
v1,calculate the group block start
v1,not need to construct
v1,get receive block informations
v1,accumulate block len
v1,get send block informations
v1,accumulate block len
v1,static member definition
v1,"if small package or small count , do it by all gather.(reduce the communication times.)"
v1,assign the blocks to every rank.
v1,do reduce scatter
v1,do all gather
v1,assign blocks
v1,"need use buffer here, since size of ""output"" is smaller than size after all gather"
v1,copy back
v1,assign blocks
v1,start all gather
v1,use output as receive buffer
v1,get current local block size
v1,get out rank
v1,get send information
v1,get in rank
v1,get recv information
v1,send and recv at same time
v1,rotate in-place
v1,send local data to neighbor first
v1,receive neighbor data first
v1,reduce
v1,start recursive halfing
v1,get target
v1,get send information
v1,get recv information
v1,send and recv at same time
v1,reduce
v1,send result to neighbor
v1,receive result from neighbor
v1,copy result
v1,start up socket
v1,parser clients from file
v1,get ip list of local machine
v1,get local rank
v1,construct listener
v1,construct communication topo
v1,construct linkers
v1,free listener
v1,set timeout
v1,accept incoming socket
v1,receive rank
v1,add new socket
v1,save ranks that need to connect with
v1,start listener
v1,start connect
v1,let smaller rank connect to larger rank
v1,send local rank
v1,wait for listener
v1,print connected linkers
v1,Get some statistic from 2 line
v1,if only have one line on file
v1,get column names
v1,load label idx first
v1,erase label column name
v1,load ignore columns
v1,load weight idx
v1,load group idx
v1,load categorical features
v1,don't support query id in data file when training in parallel
v1,read data to memory
v1,sample data
v1,construct feature bin mappers
v1,initialize label
v1,extract features
v1,sample data from file
v1,construct feature bin mappers
v1,initialize label
v1,extract features
v1,load data from binary file
v1,check meta data
v1,need to check training data
v1,read data in memory
v1,initialize label
v1,extract features
v1,Get number of lines of data file
v1,initialize label
v1,extract features
v1,load data from binary file
v1,not need to check validation data
v1,check meta data
v1,buffer to read binary file
v1,check token
v1,read size of header
v1,re-allocmate space if not enough
v1,read header
v1,get header
v1,get feature names
v1,write feature names
v1,read size of meta data
v1,re-allocate space if not enough
v1,read meta data
v1,load meta data
v1,sample local used data if need to partition
v1,"if not contain query file, minimal sample unit is one record"
v1,"if contain query file, minimal sample unit is one query"
v1,if is new query
v1,read feature data
v1,read feature size
v1,re-allocate space if not enough
v1,fill feature_names_ if not header
v1,-1 means doesn't use this feature
v1,map real feature index to used feature index
v1,push new feature
v1,"if feature is trival(only 1 bin), free spaces"
v1,---- private functions ----
v1,read all lines
v1,get query data
v1,"if not contain query data, minimal sample unit is one record"
v1,"if contain query data, minimal sample unit is one query"
v1,if is new query
v1,get query data
v1,"if not contain query file, minimal sample unit is one record"
v1,"if contain query file, minimal sample unit is one query"
v1,if is new query
v1,"sample_values[i][j], means the value of j-th sample on i-th feature"
v1,temp buffer for one line features and label
v1,parse features
v1,if need expand feature set
v1,-1 means doesn't use this feature
v1,"check the range of label_idx, weight_idx and group_idx"
v1,fill feature_names_ if not header
v1,start find bins
v1,"if only one machine, find bin locally"
v1,map real feature index to used feature index
v1,push new feature
v1,"if feature is trival(only 1 bin), free spaces"
v1,"if have multi-machines, need find bin distributed"
v1,different machines will find bin for different features
v1,start and len will store the process feature indices for different machines
v1,"machine i will find bins for features in [ strat[i], start[i] + len[i] )"
v1,get size of bin mapper with max_bin_ size
v1,"since sizes of different feature may not be same, we expand all bin mapper to type_size"
v1,find local feature bins and copy to buffer
v1,convert to binary size
v1,gather global feature bin mappers
v1,restore features bins from buffer
v1,if doesn't need to prediction with initial model
v1,parser
v1,set label
v1,free processed line:
v1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v1,text_reader_->Lines()[i].shrink_to_fit();
v1,push data
v1,if is used feature
v1,if need to prediction with initial model
v1,parser
v1,set initial score
v1,set label
v1,free processed line:
v1,"shrink_to_fit will be very slow in linux, and seems not free memory, disable for now"
v1,text_reader_->Lines()[i].shrink_to_fit();
v1,push data
v1,if is used feature
v1,metadata_ will manage space of init_score
v1,text data can be free after loaded feature values
v1,parser
v1,set initial score
v1,set label
v1,push data
v1,if is used feature
v1,only need part of data
v1,need full data
v1,metadata_ will manage space of init_score
v1,read size of token
v1,deep copy function for BinMapper
v1,find distinct_values first
v1,push zero in the front
v1,push zero in the back
v1,use distinct value is enough
v1,mean size for one bin
v1,need a new bin
v1,update bin upper bound
v1,last bin upper bound
v1,convert to int type first
v1,sort by counts
v1,will ingore the categorical of small counts
v1,check trival(num_bin_ == 1) feature
v1,calculate sparse rate
v1,sparse threshold
v1,"for lambdarank, it needs query data for partition data in parallel learning"
v1,need convert query_id to boundaries
v1,check weights
v1,check query boundries
v1,contain initial score file
v1,check weights
v1,get local weights
v1,check query boundries
v1,get local query boundaries
v1,contain initial score file
v1,get local initial scores
v1,re-load query weight
v1,save to nullptr
v1,save to nullptr
v1,save to nullptr
v1,default weight file name
v1,default weight file name
v1,use first line to count number class
v1,default query file name
v1,root is in the depth 0
v1,update parent info
v1,if cur node is left child
v1,add new node
v1,add two new leaves
v1,update new leaves
v1,save current leaf value to internal node before change
v1,update leaf depth
v1,non-leaf
v1,leaf
v1,load main config types
v1,generate seeds by seed.
v1,sub-config setup
v1,check for conflicts
v1,clear old metrics
v1,to lower
v1,split
v1,remove dumplicate
v1,"check if objective_type, metric_type, and num_class match"
v1,Change pool size to -1 (not limit) when using data parallel to reduce communication costs
v1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v1,"label_gain = 2^i - 1, may overflow, so we use 31 here"
v1,default eval ndcg @[1-5]
v1,copy feature bin mapper data
v1,"if not pass a filename, just append "".bin"" of original file"
v1,get size of header
v1,size of feature names
v1,write header
v1,write feature names
v1,get size of meta data
v1,write meta data
v1,write feature data
v1,get size of feature
v1,write feature
v1,only binary classification need sigmoid transform
v1,init tree learner
v1,push training metrics
v1,"not same training data, need reset score and others"
v1,create score tracker
v1,update score
v1,create buffer for gradients and hessians
v1,get max feature index
v1,get label index
v1,get feature names
v1,get feature infos
v1,"if need bagging, create buffer"
v1,reset config for tree learner
v1,"for a validation dataset, we need its score and metric"
v1,update score
v1,"random bagging, minimal unit is one record"
v1,if need bagging
v1,set bagging data to tree learner
v1,get subset
v1,we need to predict out-of-bag socres of data for boosting
v1,boosting first
v1,bagging logic
v1,get sub gradients
v1,train a new tree
v1,"if cannot learn a new tree, then stop"
v1,shrinkage by learning rate
v1,update score
v1,add model
v1,reset score
v1,remove model
v1,print message for metric
v1,pop last early_stopping_round_ models
v1,update training score
v1,update validation score
v1,print training metric
v1,print validation metric
v1,objective function will calculate gradients and hessians
v1,output model type
v1,output number of class
v1,output label index
v1,output max_feature_idx
v1,output objective name
v1,output sigmoid parameter
v1,output tree models
v1,use serialized string to restore this object
v1,get number of classes
v1,get index of label
v1,get max_feature_idx first
v1,get sigmoid parameter
v1,get feature names
v1,"returns offset, or lines.size() if not found."
v1,load feature information
v1,search for each feature name
v1,get tree models
v1,store the importance first
v1,sort the importance
v1,if need sigmoid transform
v1,Get the max size of pool
v1,at least need 2 leaves
v1,push split information for all leaves
v1,initialize ordered_bins_ with nullptr
v1,get ordered bin
v1,check existing for ordered bin
v1,initialize splits for leaf
v1,initialize data partition
v1,initialize ordered gradients and hessians
v1,"if has ordered bin, need to allocate a buffer to fast split"
v1,initialize ordered_bins_ with nullptr
v1,get ordered bin
v1,check existing for ordered bin
v1,initialize splits for leaf
v1,initialize data partition
v1,initialize ordered gradients and hessians
v1,"if has ordered bin, need to allocate a buffer to fast split"
v1,Get the max size of pool
v1,at least need 2 leaves
v1,push split information for all leaves
v1,some initial works before training
v1,save pointer to last trained tree
v1,root leaf
v1,only root leaf can be splitted on first time
v1,some initial works before finding best split
v1,find best threshold for every feature
v1,find best split from all features
v1,Get a leaf with max split gain
v1,Get split information for best leaf
v1,"cannot split, quit"
v1,split tree with best leaf
v1,reset histogram pool
v1,initialize used features
v1,Get used feature at current tree
v1,initialize data partition
v1,reset the splits for leaves
v1,Sumup for root
v1,use all data
v1,"point to gradients, avoid copy"
v1,"use bagging, only use part of data"
v1,copy used gradients and hessians to ordered buffer
v1,point to ordered_gradients_ and ordered_hessians_
v1,"if has ordered bin, need to initialize the ordered bin"
v1,"use all data, pass nullptr"
v1,"bagging, only use part of data"
v1,mark used data
v1,initialize ordered bin
v1,check depth of current leaf
v1,"only need to check left leaf, since right leaf is in same level of left leaf"
v1,no enough data to continue
v1,-1 if only has one leaf. else equal the index of smaller leaf
v1,only have root
v1,put parent(left) leaf's histograms into larger leaf's histograms
v1,put parent(left) leaf's histograms to larger leaf's histograms
v1,"init for the ordered gradients, only initialize when have 2 leaves"
v1,only need to initialize for smaller leaf
v1,Get leaf boundary
v1,copy
v1,assign pointer
v1,need order gradient for larger leaf
v1,copy
v1,split for the ordered bin
v1,mark data that at left-leaf
v1,split the ordered bin
v1,feature is not used
v1,if parent(larger) leaf cannot split at current feature
v1,construct histograms for smaller leaf
v1,if not use ordered bin
v1,used ordered bin
v1,find best threshold for smaller child
v1,only has root leaf
v1,"construct histgroms for large leaf, we initialize larger leaf as the parent,"
v1,so we can just subtract the smaller leaf's histograms
v1,if not use ordered bin
v1,used ordered bin
v1,find best threshold for larger child
v1,left = parent
v1,"split tree, will return right leaf"
v1,split data partition
v1,init the leaves that used on next iteration
v1,get feature partition
v1,get local used features
v1,get best split at smaller leaf
v1,get best split at larger leaf
v1,sync global best info
v1,copy back
v1,update best split
v1,initialize SerialTreeLearner
v1,Get local rank and global machine size
v1,allocate buffer for communication
v1,generate feature partition for current tree
v1,get local used feature
v1,get block start and block len for reduce scatter
v1,get buffer_write_start_pos_
v1,get buffer_read_start_pos_
v1,sync global data sumup info
v1,global sumup reduce
v1,copy back
v1,set global sumup info
v1,init global data count in leaf
v1,construct local histograms
v1,construct histograms for smaller leaf
v1,if not use ordered bin
v1,used ordered bin
v1,copy to buffer
v1,Reduce scatter for histogram
v1,restore global histograms from buffer
v1,find best threshold for smaller child
v1,only root leaf
v1,"construct histgroms for large leaf, we init larger leaf as the parent, so we can just subtract the smaller leaf's histograms"
v1,find best threshold for larger child
v1,find local best split for smaller leaf
v1,find local best split for larger leaf
v1,sync global best info
v1,set best split
v1,need update global number of data in leaf
v1,limit top k
v1,get max bin
v1,calculate buffer size
v1,"left and right on same time, so need double size"
v1,initialize histograms for global
v1,sync global data sumup info
v1,set global sumup info
v1,init global data count in leaf
v1,get local sumup
v1,get local sumup
v1,get mean number on machines
v1,weighted gain
v1,get top k
v1,"Copy histogram to buffer, and Get local aggregate features"
v1,copy histograms.
v1,copy smaller leaf histograms first
v1,mark local aggregated feature
v1,copy
v1,then copy larger leaf histograms
v1,mark local aggregated feature
v1,copy
v1,use local data to find local best splits
v1,local voting
v1,gather
v1,get all top-k from all machines
v1,global voting
v1,copy local histgrams to buffer
v1,Reduce scatter for histogram
v1,find best split from local aggregated histograms
v1,restore from buffer
v1,find best threshold
v1,restore from buffer
v1,find best threshold
v1,find local best
v1,sync global best info
v1,copy back
v1,set the global number of data for leaves
v1,init the global sumup info
