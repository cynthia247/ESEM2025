Version,Commit Message
2.8.0,Build a nightly package by default.
2.8.0,Environment-specific dependencies.
2.8.0,get the version from deepchem/__init__.py
2.8.0,nightly version : .devYearMonthDayHourMinute
2.8.0,Force to add `.dev` if `--release` option isn't passed when building
2.8.0,!/usr/bin/env python3
2.8.0,-*- coding: utf-8 -*-
2.8.0,Datasets and models used in the benchmark test
2.8.0,"irv, rf, rf_regression should be assigned manually"
2.8.0,Evaluate performances with different training set fraction
2.8.0,Datasets and models used in the benchmark test
2.8.0,Uncomment the two lines below if hyper_parameters are provided
2.8.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.8.0,hyper_parameters = pickle.load(f)
2.8.0,!/usr/bin/env python3
2.8.0,-*- coding: utf-8 -*-
2.8.0,Datasets and models used in the benchmark test
2.8.0,Load Delaney dataset
2.8.0,Get Metric
2.8.0,Fit trained model
2.8.0,Fit trained model
2.8.0,Set numpy seed
2.8.0,##Load data###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Featurize Kinase dataset
2.8.0,##Load data###
2.8.0,num_trials = 5
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,Force matplotlib to not use any Xwindows backend.
2.8.0,##Load data###
2.8.0,the histogram of the data
2.8.0,Set numpy seed
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,num_trials = 5
2.8.0,Set some global variables up top
2.8.0,Fit trained model
2.8.0,Featurize PCBA dataset
2.8.0,Initialize transformers
2.8.0,Fit trained model
2.8.0,Load sider models now
2.8.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.8.0,transformers to predict functions
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,##Load data###
2.8.0,"n_estimators=100, max_features=int(num_features/3),"
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,Fit trained model
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Load tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,This example shows how to use Pandas to load data directly
2.8.0,without using a CSVLoader object. This may be useful if you
2.8.0,want the flexibility of processing your data with Pandas
2.8.0,directly.
2.8.0,Now let's convert from a dataset back to a pandas dataframe
2.8.0,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.8.0,Featurize FACTORS dataset
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,Force matplotlib to not use any Xwindows backend.
2.8.0,##Load data###
2.8.0,the histogram of the data
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Load QM7 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Fit trained model
2.8.0,Fit trained model
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Fit trained model
2.8.0,Load QM8 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Fit trained model
2.8.0,Set numpy seed
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,Load ChEMBL dataset
2.8.0,Fit models
2.8.0,Do setup required for tf/keras models
2.8.0,Number of features on conv-mols
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,DeepCrystal Technologies 2017 - Patrick Hop
2.8.0,MIT License - have fun!!
2.8.0,Set to higher values to get better numbers
2.8.0,======================================================================
2.8.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,Only for debug!
2.8.0,Load Delaney dataset
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Do setup required for tf/keras models
2.8.0,Number of features on conv-mols
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load Delaney dataset
2.8.0,Get Metric
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load Delaney dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load MUV dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Evaluate train/test scores
2.8.0,Load MUV data
2.8.0,Build model
2.8.0,Fit trained model
2.8.0,Evaluate train/test scores
2.8.0,Extract active site
2.8.0,Featurize ligand
2.8.0,Default for CircularFingerprint
2.8.0,Featurize pocket
2.8.0,Note broadcast operation
2.8.0,Compute labels for pockets
2.8.0,Some complexes have labels but no PDB files. Filter these manually
2.8.0,Some of the ligand-names are of form (FMN ox). Use regex
2.8.0,to merge into form (FMN-ox)
2.8.0,Filter if missing PDB files
2.8.0,Load PDBBind dataset
2.8.0,Define featurizers
2.8.0,Featurize Dataset
2.8.0,########################################################## DEBUG
2.8.0,########################################################## DEBUG
2.8.0,For stable runs
2.8.0,Fit trained model
2.8.0,4-fold splits
2.8.0,num positive/negative ligands
2.8.0,10 trials on test-set
2.8.0,Define metric
2.8.0,Get supports on test-set
2.8.0,Compute accuracies
2.8.0,Number of features on conv-mols
2.8.0,Batch size of models
2.8.0,graph_model = dc.nn.SequentialGraph(n_feat)
2.8.0,Fit trained model
2.8.0,Test model
2.8.0,Join information for all tasks.
2.8.0,4-fold splits
2.8.0,num positive/negative ligands
2.8.0,Define metric
2.8.0,Get supports on test-set
2.8.0,Compute accuracies
2.8.0,Number of features on conv-mols
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Test model
2.8.0,Join information for all tasks.
2.8.0,4-fold splits
2.8.0,num positive/negative ligands
2.8.0,10 trials on test-set
2.8.0,Define metric
2.8.0,Get supports on test-set
2.8.0,Compute accuracies
2.8.0,Number of features on conv-mols
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Test model
2.8.0,Join information for all tasks.
2.8.0,4-fold splits
2.8.0,num positive/negative ligands
2.8.0,10 trials on test-set
2.8.0,Define metric
2.8.0,Get supports on test-set
2.8.0,Compute accuracies
2.8.0,Train model on support
2.8.0,Test model
2.8.0,Join information for all tasks.
2.8.0,4-fold splits
2.8.0,num positive/negative ligands
2.8.0,10 trials on test-set
2.8.0,Define metric
2.8.0,Get supports on test-set
2.8.0,Compute accuracies
2.8.0,Train model on support
2.8.0,Test model
2.8.0,Join information for all tasks.
2.8.0,Set some global variables up top
2.8.0,Featurize Tox21 dataset
2.8.0,Initialize transformers
2.8.0,Set some global variables up top
2.8.0,Featurize Tox21 dataset
2.8.0,Initialize transformers
2.8.0,Load MUV dataset
2.8.0,Featurize MUV dataset
2.8.0,Initialize transformers
2.8.0,Load MUV dataset
2.8.0,Featurize MUV dataset
2.8.0,Initialize transformers
2.8.0,Featurize SIDER dataset
2.8.0,Initialize transformers
2.8.0,Featurize SIDER dataset
2.8.0,Initialize transformers
2.8.0,Load the data.
2.8.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.8.0,sparse: most tasks do not include data for most molecules.  It also is very
2.8.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.8.0,create a list of alternating positives and negatives so each batch will have
2.8.0,equal numbers of both.
2.8.0,Define a MetaLearner describing the learning problem.
2.8.0,Run meta-learning on 80% of the tasks.
2.8.0,Validate on the remaining tasks.
2.8.0,4-fold splits
2.8.0,10 positive/negative ligands
2.8.0,10 trials on test-set
2.8.0,Sample supports without replacement (all pos/neg should be different)
2.8.0,Define metric
2.8.0,Get supports on test-set
2.8.0,Compute accuracies
2.8.0,Train model on support
2.8.0,Test model
2.8.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.8.0,Join information for all tasks.
2.8.0,4-fold splits
2.8.0,num positive/negative ligands
2.8.0,Define metric
2.8.0,Get supports on test-set
2.8.0,Compute accuracies
2.8.0,Train model on support
2.8.0,Test model
2.8.0,Join information for all tasks.
2.8.0,replace with your own scratch directory
2.8.0,Number of conformations in each file increases exponentially.
2.8.0,Start with a smaller dataset before continuing. Use all of them
2.8.0,for production
2.8.0,"'ani_gdb_s03.h5',"
2.8.0,"'ani_gdb_s04.h5',"
2.8.0,"'ani_gdb_s05.h5',"
2.8.0,"'ani_gdb_s06.h5',"
2.8.0,"'ani_gdb_s07.h5',"
2.8.0,'ani_gdb_s08.h5'
2.8.0,Extract the data
2.8.0,Print the data
2.8.0,self-interaction energies taken from
2.8.0,https://github.com/isayev/ANI1_dataset README
2.8.0,flush once more at the end
2.8.0,"# For production, set nb_epoch to 100+"
2.8.0,"print(""Train scores"")"
2.8.0,print(train_scores)
2.8.0,"print(""Minimization of a single test set structure:"")"
2.8.0,"print(model.minimize_structure(coords, atomic_nums))"
2.8.0,Written by Roman Zubatyuk and Justin S. Smith
2.8.0,Modified by Yutong Zhao to make python2 compatible
2.8.0,opening file
2.8.0,print(store_loc)
2.8.0,print(type(v[0]))
2.8.0,print(k)
2.8.0,print(path)
2.8.0,Number of conformations in each file increases exponentially.
2.8.0,Start with a smaller dataset before continuing. Use all of them
2.8.0,for production
2.8.0,Extract the data
2.8.0,NOTE THE RENAMING:
2.8.0,Note sensitivity = recall
2.8.0,Load nci dataset
2.8.0,Featurize nci dataset
2.8.0,Initialize transformers
2.8.0,Set some global variables up top
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load hiv dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load hiv dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load delaney dataset
2.8.0,Fit models
2.8.0,Load delaney dataset
2.8.0,Fit models
2.8.0,Fit models
2.8.0,Load delaney dataset
2.8.0,Fit models
2.8.0,TODO: Once improved splitting API is merged in swap to simpler API
2.8.0,The return values are dc.data.Dataset objects so we need to extract
2.8.0,the ids
2.8.0,TODO once improved splitting API is merged in swap out for simpler
2.8.0,API
2.8.0,The return values are dc.data.Dataset objects so we need to extract
2.8.0,the ids
2.8.0,Fit trained model
2.8.0,Load SIDER dataset
2.8.0,Featurize SIDER dataset
2.8.0,Initialize transformers
2.8.0,Featurize permeability dataset
2.8.0,Load Tox21 dataset
2.8.0,Fit trained model
2.8.0,TODO: This should be swapped for simpler splitter API once that's merged in.
2.8.0,The return values are dc.data.Dataset objects so we need to extract
2.8.0,the ids
2.8.0,Only for debug!
2.8.0,Load clintox dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load clintox dataset
2.8.0,Fit models
2.8.0,Do setup required for tf/keras models
2.8.0,Number of features on conv-mols
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,-*- coding: utf-8 -*-
2.8.0,#############################################################################
2.8.0,## save dataset
2.8.0,#############################################################################
2.8.0,## load datasets
2.8.0,load sweetfda
2.8.0,load aact
2.8.0,## fixup smiles for matching
2.8.0,return smiles
2.8.0,map original smiles to converted smiles
2.8.0,"## join dataframes, index on smiles"
2.8.0,map original smiles back
2.8.0,## fill all nan with 0
2.8.0,## construct datasets
2.8.0,store in new datasets
2.8.0,## save datasets
2.8.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.8.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.8.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.8.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.8.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.8.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.8.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.8.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.8.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.8.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.8.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.8.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.8.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.8.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.8.0,For stable runs
2.8.0,Fit trained model
2.8.0,For stable runs
2.8.0,Fit trained model
2.8.0,For stable runs
2.8.0,Fit trained model
2.8.0,TODO The below line should be fixes
2.8.0,See: https://github.com/deepchem/deepchem/issues/2373
2.8.0,model.save()
2.8.0,transformers = [
2.8.0,"dc.trans.LogTransformer(transform_X=True),"
2.8.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.8.0,dataset=train_dataset)]
2.8.0,Featurize UV dataset
2.8.0,##Load data###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Set numpy seed
2.8.0,##Load data###
2.8.0,##Create model###
2.8.0,Use R2 classification metric
2.8.0,Only use for final evaluation
2.8.0,Force matplotlib to not use any Xwindows backend.
2.8.0,##Load data###
2.8.0,the histogram of the data
2.8.0,##Load data###
2.8.0,###################################################### DEBUG
2.8.0,###################################################### DEBUG
2.8.0,Load HOPV dataset
2.8.0,Fit models
2.8.0,Number of features on conv-mols
2.8.0,Batch size of models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load HOPV dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load HOPV dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load HOPV dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Only for debug!
2.8.0,Load HOPV dataset
2.8.0,Fit models
2.8.0,Fit trained model
2.8.0,Load TOXCAST dataset
2.8.0,Featurize TOXCAST dataset
2.8.0,Initialize transformers
2.8.0,Fit trained model
2.8.0,Processing of ToxCast data
2.8.0,Author - Aneesh Pappu
2.8.0,Loading dataframes and editing indices
2.8.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.8.0,get corresponding casn
2.8.0,get corresponding smiles
2.8.0,write to cell
2.8.0,Tidy up and write to csv
2.8.0,TODO(rbharath): Check that this operation is differentiable.
2.8.0,The number of cells which we should theoretically have
2.8.0,The number of cells which we should theoretically have
2.8.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.8.0,The number of cells which we should theoretically have
2.8.0,TODO(rbharath): The test below only checks that shapes work out.
2.8.0,Need to do a correctness implementation vs. a simple CPU impl.
2.8.0,The number of cells which we should theoretically have
2.8.0,TODO(rbharath): The test below only checks that shapes work out.
2.8.0,Need to do a correctness implementation vs. a simple CPU impl.
2.8.0,The number of cells which we should theoretically have
2.8.0,TODO(rbharath): The test below only checks that shapes work out.
2.8.0,Need to do a correctness implementation vs. a simple CPU impl.
2.8.0,TODO(rbharath): Commenting this out due to weird segfaults
2.8.0,def test_vina_generate_conformers(self):
2.8.0,"""""""Test that Vina Model can generate conformers"""""""
2.8.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.8.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.8.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.8.0,max_protein_atoms = 3500
2.8.0,max_ligand_atoms = 100
2.8.0,"print(""Loading protein file"")"
2.8.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.8.0,protein_Z = pad_array(
2.8.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.8.0,max_protein_atoms)
2.8.0,"print(""Loading ligand file"")"
2.8.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.8.0,ligand_Z = pad_array(
2.8.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.8.0,max_ligand_atoms)
2.8.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.8.0,"Shape (n_cells, k)"
2.8.0,"Shape (N, 1)"
2.8.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0,"conditions, so does wrapround. O(constant)"
2.8.0,"Shape (n_cells, 26)"
2.8.0,"Shape (N, 26)"
2.8.0,"coords of shape (N, ndim)"
2.8.0,"Shape (N, 26, k, ndim)"
2.8.0,"Shape (N, 26, k)"
2.8.0,"Shape (N, 26, k)"
2.8.0,"Shape (N, 26, k, ndim)"
2.8.0,"For smaller systems especially, the periodic boundary conditions can"
2.8.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.8.0,make sure duplicate neighbors are ignored?
2.8.0,TODO(rbharath): How does distance need to be modified here to
2.8.0,account for periodic boundary conditions?
2.8.0,"Shape (N, 26, k)"
2.8.0,"Shape (N, 26*k)"
2.8.0,TODO(rbharath): This will cause an issue with duplicates!
2.8.0,"Shape (N, M)"
2.8.0,"N elts of size (M,) each"
2.8.0,"Shape (N, 26*k)"
2.8.0,"N elts of size (26*k,) each"
2.8.0,"N elts of size (M,) each"
2.8.0,"Shape (N, M)"
2.8.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.8.0,"N tensors of shape (n_cells, 1)"
2.8.0,"Shape (N*n_cells, 1) after tile"
2.8.0,"List of N tensors of shape (n_cells, 1)"
2.8.0,Lists of length N
2.8.0,Lists of length n_cells
2.8.0,Get indices of k atoms closest to each cell point
2.8.0,TODO(rbharath): tf.stack for tf 1.0
2.8.0,"Tensor of shape (n_cells, k, ndim)"
2.8.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.8.0,"Tensor of shape (26, k, ndim)"
2.8.0,"Reshape to (26*k, ndim)"
2.8.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.8.0,"Dists of shape (26*k, 1)"
2.8.0,"Of shape (k, ndim)"
2.8.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.8.0,TODO(rbharath): Change this for tf 1.0
2.8.0,"n_cells tensors of shape (N, 1)"
2.8.0,"Shape (N*n_cells, 1) after tile"
2.8.0,"List of n_cells tensors of shape (N, 1)"
2.8.0,Lists of length n_cells
2.8.0,Lists of length n_cells
2.8.0,Get indices of k atoms closest to each cell point
2.8.0,"n_cells tensors of shape (k, ndim)"
2.8.0,"Tensor of shape (n_cells, k)"
2.8.0,TODO(rbharath):
2.8.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.8.0,- Need to group closest atoms amongst cell neighbors
2.8.0,- Need to do another top_k to find indices of closest neighbors.
2.8.0,- Return N lists corresponding to neighbors for every atom.
2.8.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.8.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.8.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.8.0,the cube.
2.8.0,Number of neighbors of central cube in 3-space is
2.8.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.8.0,TODO(rbharath)
2.8.0,n_cells = int(cells.get_shape()[0])
2.8.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.8.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.8.0,"Tile (a, a, a, b, b, b, etc.)"
2.8.0,"Tile (a, b, c, a, b, c, ...)"
2.8.0,"Lists of n_cells tensors of shape (N, 1)"
2.8.0,Lists of length n_cells
2.8.0,Lists of length n_cells
2.8.0,Get indices of k atoms closest to each cell point
2.8.0,"n_cells tensors of shape (26,)"
2.8.0,TODO(rbharath): Make this handle minibatches
2.8.0,"Shape (N_protein+N_ligand, 3)"
2.8.0,"Shape (N_protein+N_ligand,)"
2.8.0,"Shape (N_protein+N_ligand, M)"
2.8.0,"Shape (N_protein+N_ligand,)"
2.8.0,"Shape (N_protein+N_ligand, 3)"
2.8.0,"Shape (N_protein+N_ligand,)"
2.8.0,"Shape (N_protein+N_ligand, M)"
2.8.0,"Shape (N_protein+N_ligand, M, 3)"
2.8.0,"Shape (N_protein+N_ligand, M)"
2.8.0,"Shape (N_protein+N_ligand, M, 3)"
2.8.0,"Shape (N_protein+N_ligand, M)"
2.8.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.8.0,"Shape (N_protein+N_ligand, M)"
2.8.0,"Shape (N_protein+N_ligand, M)"
2.8.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.8.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.8.0,computing free-energy. This implementation currently uses all interaction
2.8.0,terms. Not sure if this makes a difference.
2.8.0,"Shape (N_protein+N_ligand, M)"
2.8.0,Shape () -- scalar
2.8.0,Keep track of the layers
2.8.0,"For graphical layers, add connectivity placeholders"
2.8.0,Add layer to the layer list
2.8.0,Keep track of the layers
2.8.0,Create graph topology and x
2.8.0,Keep track of the layers
2.8.0,Whether or not we have used the GraphGather layer yet
2.8.0,Update new value of x
2.8.0,Update new value of x
2.8.0,Update new value of x
2.8.0,Get train function
2.8.0,Initialize
2.8.0,################################################################### DEBUG
2.8.0,self.test_label_placeholder = Input(
2.8.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.8.0,"name=""label_placeholder""))"
2.8.0,self.test_weight_placeholder = Input(
2.8.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.8.0,"name=""weight_placeholder""))"
2.8.0,TODO(rbharath): Should weights for the support be used?
2.8.0,Support labels
2.8.0,self.support_label_placeholder = Input(
2.8.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.8.0,"name=""support_label_placeholder""))"
2.8.0,################################################################### DEBUG
2.8.0,Generate dictionary elements for support
2.8.0,Get graph information for test
2.8.0,Generate dictionary elements for test
2.8.0,Perform the optimization
2.8.0,Create different support sets
2.8.0,Get batch to try it out on
2.8.0,"Train on support set, batch pair"
2.8.0,Get featurization for test
2.8.0,"Shape (n_test, n_feat)"
2.8.0,Get featurization for support
2.8.0,"Shape (n_support, n_feat)"
2.8.0,Computes the inner part c() of the kernel
2.8.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.8.0,Normalize
2.8.0,TODO(rbharath): euclidean kernel is broken!
2.8.0,elif self.similarity == 'euclidean':
2.8.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.8.0,"Note that gram matrix g has shape (n_test, n_support)"
2.8.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.8.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.8.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.8.0,each test entry) to get attention vector
2.8.0,"Shape (n_test, n_support)"
2.8.0,Weighted sum of support labels
2.8.0,"Shape (n_support, 1)"
2.8.0,pred is yhat in eqn (1) of Matching Networks.
2.8.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.8.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.8.0,"Shape (n_test,)"
2.8.0,Convert to logit space using inverse sigmoid (logit) function
2.8.0,logit function: log(pred) - log(1-pred)
2.8.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.8.0,in Cross Entropy calculation.
2.8.0,"Shape (n_test,)"
2.8.0,Get scores
2.8.0,Remove padded elements
2.8.0,Get scores
2.8.0,pred corresponds to prob(example == 1)
2.8.0,Remove padded elements
2.8.0,Get batches
2.8.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.8.0,multitask case with missing data...
2.8.0,Join information for all tasks.
2.8.0,TODO(rbharath): Find a way to get rid of this import?
2.8.0,Extract model info
2.8.0,Get graph topology for x
2.8.0,Building outputs
2.8.0,Set epsilon
2.8.0,Initialize
2.8.0,"Path to save checkpoint files, which matches the"
2.8.0,replicated supervisor's default path.
2.8.0,Create target inputs
2.8.0,Get train function
2.8.0,TODO(rbharath): I believe this is total amount of data
2.8.0,Get graph information
2.8.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.8.0,the number of labeled data points in target_i. This is to normalize each task
2.8.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.8.0,Get other optimizer information
2.8.0,TODO(rbharath): Figure out how to handle phase appropriately
2.8.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.8.0,"tensors of shape (batch_size,)"
2.8.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.8.0,examples (effect averages out)
2.8.0,Perform the optimization
2.8.0,TODO(rbharath): Disabling saving for now to try to debug.
2.8.0,run eval data through the model
2.8.0,"Shape (n_samples, n_tasks)"
2.8.0,Create target inputs
2.8.0,TODO(rbharath): Find a way to get rid of this import?
2.8.0,Obtain appropriate loss function
2.8.0,Extract model info
2.8.0,Get graph topology for x
2.8.0,Raw logit outputs
2.8.0,Set epsilon
2.8.0,Initialize
2.8.0,"Path to save checkpoint files, which matches the"
2.8.0,replicated supervisor's default path.
2.8.0,Create target inputs
2.8.0,############################################################### DEBUG
2.8.0,"print(""multitask classifier"")"
2.8.0,"print(""feat"")"
2.8.0,print(feat)
2.8.0,############################################################### DEBUG
2.8.0,Get train function
2.8.0,TODO(rbharath): I believe this is total amount of data
2.8.0,Get graph information
2.8.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.8.0,the number of labeled data points in target_i. This is to normalize each task
2.8.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.8.0,Get other optimizer information
2.8.0,TODO(rbharath): Figure out how to handle phase appropriately
2.8.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.8.0,"tensors of shape (batch_size,)"
2.8.0,Convert the labels into one-hot vector encodings.
2.8.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.8.0,un-softmaxed logits rather than softmax outputs.
2.8.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.8.0,examples (effect averages out)
2.8.0,Perform the optimization
2.8.0,TODO(rbharath): Disabling saving for now to try to debug.
2.8.0,run eval data through the model
2.8.0,"Shape (n_samples, n_tasks)"
2.8.0,run eval data through the model
2.8.0,self.n_atoms = n_atoms
2.8.0,Define the list of tensors to be used as topology
2.8.0,Merge mol conv objects
2.8.0,Generate dicts
2.8.0,Define the list of tensors to be used as topology
2.8.0,Extract atom numbers
2.8.0,Generate dicts
2.8.0,molecule * atom(graph) => step => features
2.8.0,molecule * atom(graph) => step
2.8.0,molecule * atom(graph) => step
2.8.0,Define the list of tensors to be used as topology
2.8.0,calculation orders for a batch of molecules
2.8.0,padding atom features vector of each molecule with 0
2.8.0,self.n_atoms = n_atoms
2.8.0,Define the list of tensors to be used as topology
2.8.0,Extract atom numbers
2.8.0,Generate dicts
2.8.0,self.n_atoms = n_atoms
2.8.0,Define the list of tensors to be used as topology
2.8.0,Extract atom numbers
2.8.0,number of atoms in each molecule
2.8.0,index of pair features
2.8.0,number of pairs for each atom
2.8.0,atom features
2.8.0,pair features
2.8.0,Generate dicts
2.8.0,Load Tox21 dataset
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.8.0,Fit trained model
2.8.0,Fit models
2.8.0,Batch size of models
2.8.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.8.0,Fit trained model
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,number positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply an attention lstm layer
2.8.0,Number of folds for split
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,number positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply an attention lstm layer
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,number positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply an attention lstm layer
2.8.0,Number of folds for split
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Number of folds for split
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply a residual lstm layer
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply a residual lstm layer
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply a residual lstm layer
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply a residual lstm layer
2.8.0,Number of folds for split
2.8.0,Depth of attention module
2.8.0,number positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,Apply an attention lstm layer
2.8.0,Number of folds for split
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Number of features on conv-mols
2.8.0,Define metric
2.8.0,Train support model on train
2.8.0,Add layers
2.8.0,# Gather Projection
2.8.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.8.0,There should be 8 layers in graph_model
2.8.0,assert len(graph_model.layers) == 6
2.8.0,Add layers
2.8.0,Need to add batch-norm separately to test/support due to differing
2.8.0,shapes.
2.8.0,Apply an attention lstm layer
2.8.0,Gather Projection
2.8.0,Add layers
2.8.0,Need to add batch-norm separately to test/support due to differing
2.8.0,shapes.
2.8.0,Apply an attention lstm layer
2.8.0,Gather Projection
2.8.0,Degrees from 1 to max_deg inclusive
2.8.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.8.0,"Should have shape (?, deg)"
2.8.0,"Shape of atom_features should be (?, n_feat)"
2.8.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.8.0,-*- coding: utf-8 -*-
2.8.0,Save hyperparameters
2.8.0,-*- coding: utf-8 -*-
2.8.0,Save hyperparameters
2.8.0,setup optimizer
2.8.0,setup optimizer
2.8.0,"print(""tasK: %d"" %task)"
2.8.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.8.0,"print(""scores"")"
2.8.0,print(scores.size())
2.8.0,"print(""task_label"")"
2.8.0,print(task_label.size())
2.8.0,"task_loss =  self.criterion(scores, task_label)"
2.8.0,"print(""task_loss"")"
2.8.0,print(task_loss.size())
2.8.0,-*- coding: utf-8 -*-
2.8.0,Save hyperparameters
2.8.0,weight decay
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,Turns out there are valid cases where we don't want pad-batches
2.8.0,on by default.
2.8.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0,Run training op.
2.8.0,############################################################# TIMING
2.8.0,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0,Remove padded examples.
2.8.0,Special case to handle singletasks.
2.8.0,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0,Remove padded examples.
2.8.0,References
2.8.0,Arguments
2.8.0,Aliases.
2.8.0,Aliases.
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,TODO(rbharath): This class does not yet have a
2.8.0,"TensorGraph equivalent, but one may not be required."
2.8.0,"Commented out for now, remove if OK."
2.8.0,class AlternateWeaveLayer(WeaveLayer):
2.8.0,""""""" Alternate implementation of weave module"
2.8.0,"same variables, different graph structures"
2.8.0,""""""""
2.8.0,
2.8.0,"def call(self, x, mask=None):"
2.8.0,"""""""Execute this layer on input tensors."
2.8.0,
2.8.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.8.0,
2.8.0,Parameters
2.8.0,----------
2.8.0,x: list
2.8.0,list of Tensors of form described above.
2.8.0,"mask: bool, optional"
2.8.0,Ignored. Present only to shadow superclass call() method.
2.8.0,
2.8.0,Returns
2.8.0,-------
2.8.0,A: Tensor
2.8.0,Tensor of atom_features
2.8.0,P: Tensor
2.8.0,Tensor of pair_features
2.8.0,""""""""
2.8.0,# Add trainable weights
2.8.0,self.build()
2.8.0,
2.8.0,atom_features = x[0]
2.8.0,pair_features = x[1]
2.8.0,
2.8.0,pair_split = x[2]
2.8.0,atom_to_pair = x[4]
2.8.0,
2.8.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.8.0,AA = self.activation(AA)
2.8.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.8.0,PA = self.activation(PA)
2.8.0,"PA = tf.segment_sum(PA, pair_split)"
2.8.0,
2.8.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.8.0,A = self.activation(A)
2.8.0,
2.8.0,if self.update_pair:
2.8.0,AP_ij = tf.matmul(
2.8.0,tf.reshape(
2.8.0,"tf.gather(atom_features, atom_to_pair),"
2.8.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.8.0,AP_ij = self.activation(AP_ij)
2.8.0,AP_ji = tf.matmul(
2.8.0,tf.reshape(
2.8.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.8.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.8.0,AP_ji = self.activation(AP_ji)
2.8.0,
2.8.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.8.0,PP = self.activation(PP)
2.8.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.8.0,P = self.activation(P)
2.8.0,else:
2.8.0,P = pair_features
2.8.0,
2.8.0,"return A, P"
2.8.0,TODO(rbharath): This class does not yet have a
2.8.0,"TensorGraph equivalent, but one may not be required."
2.8.0,"Commented out for now, remove if OK."
2.8.0,class WeaveConcat(Layer):
2.8.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.8.0,""""""""
2.8.0,
2.8.0,"def __init__(self,"
2.8.0,"batch_size,"
2.8.0,"n_atom_input_feat=50,"
2.8.0,"n_output=128,"
2.8.0,"init='glorot_uniform',"
2.8.0,"activation='tanh',"
2.8.0,**kwargs):
2.8.0,""""""""
2.8.0,Parameters
2.8.0,----------
2.8.0,batch_size: int
2.8.0,number of molecules in a batch
2.8.0,"n_atom_input_feat: int, optional"
2.8.0,Number of features for each atom in input.
2.8.0,"n_output: int, optional"
2.8.0,Number of output features for each atom(concatenated)
2.8.0,"init: str, optional"
2.8.0,Weight initialization for filters.
2.8.0,"activation: str, optional"
2.8.0,Activation function applied
2.8.0,
2.8.0,""""""""
2.8.0,self.batch_size = batch_size
2.8.0,self.n_atom_input_feat = n_atom_input_feat
2.8.0,self.n_output = n_output
2.8.0,self.init = initializations.get(init)  # Set weight initialization
2.8.0,self.activation = activations.get(activation)  # Get activations
2.8.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.8.0,
2.8.0,def build(self):
2.8.0,"""""""""Construct internal trainable weights."
2.8.0,""""""""
2.8.0,
2.8.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.8.0,self.b = model_ops.zeros(shape=[
2.8.0,"self.n_output,"
2.8.0,])
2.8.0,
2.8.0,self.trainable_weights = self.W + self.b
2.8.0,
2.8.0,"def call(self, x, mask=None):"
2.8.0,"""""""Execute this layer on input tensors."
2.8.0,
2.8.0,"x = [atom_features, atom_mask]"
2.8.0,
2.8.0,Parameters
2.8.0,----------
2.8.0,x: list
2.8.0,Tensors as listed above
2.8.0,"mask: bool, optional"
2.8.0,Ignored. Present only to shadow superclass call() method.
2.8.0,
2.8.0,Returns
2.8.0,-------
2.8.0,outputs: Tensor
2.8.0,Tensor of concatenated atom features
2.8.0,""""""""
2.8.0,self.build()
2.8.0,atom_features = x[0]
2.8.0,atom_masks = x[1]
2.8.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.8.0,A_mask = tf.split(
2.8.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.8.0,outputs = tf.concat(
2.8.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.8.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.8.0,outputs = self.activation(outputs)
2.8.0,return outputs
2.8.0,TODO(rbharath): This class does not yet have a
2.8.0,"TensorGraph equivalent, but one may not be required."
2.8.0,"Commented out for now, remove if OK."
2.8.0,class AlternateWeaveGather(WeaveGather):
2.8.0,"""""""Alternate implementation of weave gather layer"
2.8.0,corresponding to AlternateWeaveLayer
2.8.0,""""""""
2.8.0,
2.8.0,"def call(self, x, mask=None):"
2.8.0,"""""""Execute this layer on input tensors."
2.8.0,
2.8.0,"x = [atom_features, atom_split]"
2.8.0,
2.8.0,Parameters
2.8.0,----------
2.8.0,x: list
2.8.0,Tensors as listed above
2.8.0,"mask: bool, optional"
2.8.0,Ignored. Present only to shadow superclass call() method.
2.8.0,
2.8.0,Returns
2.8.0,-------
2.8.0,outputs: Tensor
2.8.0,Tensor of molecular features
2.8.0,""""""""
2.8.0,# Add trainable weights
2.8.0,self.build()
2.8.0,outputs = x[0]
2.8.0,atom_split = x[1]
2.8.0,
2.8.0,if self.gaussian_expand:
2.8.0,outputs = self.gaussian_histogram(outputs)
2.8.0,
2.8.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.8.0,
2.8.0,if self.gaussian_expand:
2.8.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.8.0,output_molecules = self.activation(output_molecules)
2.8.0,return output_molecules
2.8.0,Each directory holds a range of assay results
2.8.0,Just write NA
2.8.0,"Now, write out the results csv, going line by line through all molecule results"
2.8.0,printing the mol_id
2.8.0,printing the SMILES
2.8.0,Now gzip it
2.8.0,Now remove the intermediate csv
2.8.0,First download all SDF files. We need these to get smiles
2.8.0,Next download all Bioassays
2.8.0,RDKit consistently hangs when trying to read this file
2.8.0,TODO (LESWING) Lazy Load
2.8.0,TODO (LESWING) Lazy Load
2.8.0,from simdna import simulations
2.8.0,define layer out functions
2.8.0,get layer outputs for a positive simulation example
2.8.0,plot layer outputs
2.8.0,highlight motif sites
2.8.0,get a positive and a negative example from the simulation data
2.8.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.8.0,get motif site locations
2.8.0,organize legends
2.8.0,plot scores and highlight motif site locations
2.8.0,initialize fwd and reverse scores to -infinity
2.8.0,"cross-correlate separately for each base,"
2.8.0,for both the PSSM and its reverse complement
2.8.0,sum over the bases
2.8.0,take max of fwd and reverse scores at each position
2.8.0,return 1D view of sequence characters
2.8.0,class SequenceDNN(Model):
2.8.0,""""""""
2.8.0,Sequence DNN models.
2.8.0,
2.8.0,Parameters
2.8.0,----------
2.8.0,"seq_length : int, optional"
2.8.0,length of input sequence.
2.8.0,"keras_model : instance of keras.models.Sequential, optional"
2.8.0,seq_length or keras_model must be specified.
2.8.0,"num_tasks : int, optional"
2.8.0,number of tasks. Default: 1.
2.8.0,num_filters : list[int] | tuple[int]
2.8.0,"number of convolutional filters in each layer. Default: (15,)."
2.8.0,conv_width : list[int] | tuple[int]
2.8.0,"width of each layer's convolutional filters. Default: (15,)."
2.8.0,pool_width : int
2.8.0,width of max pooling after the last layer. Default: 35.
2.8.0,L1 : float
2.8.0,strength of L1 penalty.
2.8.0,dropout : float
2.8.0,dropout probability in every convolutional layer. Default: 0.
2.8.0,verbose: int
2.8.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.8.0,
2.8.0,Returns
2.8.0,-------
2.8.0,Compiled DNN model.
2.8.0,""""""""
2.8.0,
2.8.0,"def __init__(self,"
2.8.0,"seq_length=None,"
2.8.0,"keras_model=None,"
2.8.0,"use_RNN=False,"
2.8.0,"num_tasks=1,"
2.8.0,"num_filters=(15, 15, 15),"
2.8.0,"conv_width=(15, 15, 15),"
2.8.0,"pool_width=35,"
2.8.0,"GRU_size=35,"
2.8.0,"TDD_size=15,"
2.8.0,"L1=0,"
2.8.0,"dropout=0.0,"
2.8.0,"num_epochs=100,"
2.8.0,verbose=1):
2.8.0,self.num_tasks = num_tasks
2.8.0,self.num_epochs = num_epochs
2.8.0,self.verbose = verbose
2.8.0,self.train_metrics = []
2.8.0,self.valid_metrics = []
2.8.0,if keras_model is not None and seq_length is None:
2.8.0,self.model = keras_model
2.8.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.8.0,elif seq_length is not None and keras_model is None:
2.8.0,self.model = Sequential()
2.8.0,assert len(num_filters) == len(conv_width)
2.8.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.8.0,conv_height = 4 if i == 0 else 1
2.8.0,self.model.add(
2.8.0,Convolution2D(
2.8.0,"nb_filter=nb_filter,"
2.8.0,"nb_row=conv_height,"
2.8.0,"nb_col=nb_col,"
2.8.0,"activation='linear',"
2.8.0,"init='he_normal',"
2.8.0,"input_shape=(1, 4, seq_length),"
2.8.0,"W_regularizer=l1(L1),"
2.8.0,b_regularizer=l1(L1)))
2.8.0,self.model.add(Activation('relu'))
2.8.0,self.model.add(Dropout(dropout))
2.8.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.8.0,if use_RNN:
2.8.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.8.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.8.0,"self.model.add(Permute((2, 1)))"
2.8.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.8.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.8.0,self.model.add(Flatten())
2.8.0,self.model.add(Dense(output_dim=self.num_tasks))
2.8.0,self.model.add(Activation('sigmoid'))
2.8.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.8.0,else:
2.8.0,raise ValueError(
2.8.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.8.0,
2.8.0,"def train(self,"
2.8.0,"X,"
2.8.0,"y,"
2.8.0,"validation_data,"
2.8.0,"early_stopping_metric='Loss',"
2.8.0,"early_stopping_patience=5,"
2.8.0,save_best_model_to_prefix=None):
2.8.0,if y.dtype != bool:
2.8.0,"assert set(np.unique(y)) == {0, 1}"
2.8.0,y = y.astype(bool)
2.8.0,multitask = y.shape[1] > 1
2.8.0,if not multitask:
2.8.0,num_positives = y.sum()
2.8.0,num_sequences = len(y)
2.8.0,num_negatives = num_sequences - num_positives
2.8.0,if self.verbose >= 1:
2.8.0,print('Training model (* indicates new best result)...')
2.8.0,"X_valid, y_valid = validation_data"
2.8.0,early_stopping_wait = 0
2.8.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.8.0,"for epoch in range(1, self.num_epochs + 1):"
2.8.0,self.model.fit(
2.8.0,"X,"
2.8.0,"y,"
2.8.0,"batch_size=128,"
2.8.0,"nb_epoch=1,"
2.8.0,class_weight={
2.8.0,"True: num_sequences / num_positives,"
2.8.0,False: num_sequences / num_negatives
2.8.0,"} if not multitask else None,"
2.8.0,verbose=self.verbose >= 2)
2.8.0,"epoch_train_metrics = self.test(X, y)"
2.8.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.8.0,self.train_metrics.append(epoch_train_metrics)
2.8.0,self.valid_metrics.append(epoch_valid_metrics)
2.8.0,if self.verbose >= 1:
2.8.0,print('Epoch {}:'.format(epoch))
2.8.0,print('Train {}'.format(epoch_train_metrics))
2.8.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.8.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.8.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.8.0,if self.verbose >= 1:
2.8.0,print(' *')
2.8.0,best_metric = current_metric
2.8.0,best_epoch = epoch
2.8.0,early_stopping_wait = 0
2.8.0,if save_best_model_to_prefix is not None:
2.8.0,self.save(save_best_model_to_prefix)
2.8.0,else:
2.8.0,if self.verbose >= 1:
2.8.0,print()
2.8.0,if early_stopping_wait >= early_stopping_patience:
2.8.0,break
2.8.0,early_stopping_wait += 1
2.8.0,if self.verbose >= 1:
2.8.0,print('Finished training after {} epochs.'.format(epoch))
2.8.0,if save_best_model_to_prefix is not None:
2.8.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.8.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.8.0,"best_epoch, save_best_model_to_prefix))"
2.8.0,
2.8.0,"def predict(self, X):"
2.8.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.8.0,
2.8.0,def get_sequence_filters(self):
2.8.0,""""""""
2.8.0,Returns 3D array of 2D sequence filters.
2.8.0,""""""""
2.8.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.8.0,
2.8.0,"def deeplift(self, X, batch_size=200):"
2.8.0,""""""""
2.8.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.8.0,""""""""
2.8.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.8.0,from deeplift.conversion import keras_conversion as kc
2.8.0,
2.8.0,# convert to deeplift model and get scoring function
2.8.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.8.0,score_func = deeplift_model.get_target_contribs_func(
2.8.0,find_scores_layer_idx=0)
2.8.0,# use a 40% GC reference
2.8.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.8.0,# get deeplift scores
2.8.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.8.0,for i in range(self.num_tasks):
2.8.0,deeplift_scores[i] = score_func(
2.8.0,"task_idx=i,"
2.8.0,"input_data_list=[X],"
2.8.0,"batch_size=batch_size,"
2.8.0,"progress_update=None,"
2.8.0,input_references_list=input_references)
2.8.0,return deeplift_scores
2.8.0,
2.8.0,"def in_silico_mutagenesis(self, X):"
2.8.0,""""""""
2.8.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.8.0,""""""""
2.8.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.8.0,wild_type_predictions = self.predict(X)
2.8.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.8.0,np.newaxis]
2.8.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.8.0,"zip(X, wild_type_predictions)):"
2.8.0,mutated_sequences = np.repeat(
2.8.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.8.0,# remove wild-type
2.8.0,arange = np.arange(len(mutated_sequences))
2.8.0,horizontal_cycle = np.tile(
2.8.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.8.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.8.0,# add mutant
2.8.0,vertical_repeat = np.repeat(
2.8.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.8.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.8.0,# make mutant predictions
2.8.0,mutated_predictions = self.predict(mutated_sequences)
2.8.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.8.0,"(self.num_tasks,))"
2.8.0,mutagenesis_scores[
2.8.0,sequence_index] = wild_type_prediction - mutated_predictions
2.8.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.8.0,
2.8.0,@staticmethod
2.8.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.8.0,from dragonn.plot import plot_bases_on_ax
2.8.0,scores = score_func(X).squeeze(
2.8.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.8.0,try:
2.8.0,os.makedirs(output_directory)
2.8.0,except OSError:
2.8.0,pass
2.8.0,num_tasks = len(scores)
2.8.0,"for task_index, task_scores in enumerate(scores):"
2.8.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.8.0,# sequence_scores is num_bases x sequence_length
2.8.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.8.0,plt.clf()
2.8.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.8.0,top_axis.plot(
2.8.0,"range(1,"
2.8.0,"len(basewise_max_sequence_scores) + 1),"
2.8.0,basewise_max_sequence_scores)
2.8.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.8.0,peak_position = basewise_max_sequence_scores.argmax()
2.8.0,top_axis.axvspan(
2.8.0,"peak_position - peak_width,"
2.8.0,"peak_position + peak_width,"
2.8.0,"color='grey',"
2.8.0,alpha=0.1)
2.8.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.8.0,peak_position + peak_width].T
2.8.0,# Set non-max letter_heights to zero
2.8.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.8.0,"letter_heights[np.arange(len(letter_heights)),"
2.8.0,peak_sequence_scores.argmax(axis=1)] = \
2.8.0,basewise_max_sequence_scores[peak_position - peak_width :
2.8.0,peak_position + peak_width]
2.8.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.8.0,bottom_axis.set_xticklabels(
2.8.0,tuple(
2.8.0,"map(str,"
2.8.0,"np.arange(peak_position - peak_width,"
2.8.0,peak_position + peak_width + 1))))
2.8.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.8.0,plt.xlabel('Position')
2.8.0,plt.ylabel('Score')
2.8.0,plt.savefig(
2.8.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.8.0,"sequence_index, '_task_{}'.format(task_index)"
2.8.0,if num_tasks > 1 else '')))
2.8.0,plt.close()
2.8.0,
2.8.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.8.0,self._plot_scores(
2.8.0,"X,"
2.8.0,"output_directory,"
2.8.0,"peak_width,"
2.8.0,"score_func=self.deeplift,"
2.8.0,score_name='DeepLift')
2.8.0,
2.8.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.8.0,self._plot_scores(
2.8.0,"X,"
2.8.0,"output_directory,"
2.8.0,"peak_width,"
2.8.0,"score_func=self.in_silico_mutagenesis,"
2.8.0,score_name='ISM')
2.8.0,
2.8.0,"def plot_architecture(self, output_file):"
2.8.0,from dragonn.visualize_util import plot as plot_keras_model
2.8.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.8.0,
2.8.0,"def save(self, save_best_model_to_prefix):"
2.8.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.8.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.8.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.8.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.8.0,
2.8.0,@staticmethod
2.8.0,"def load(arch_fname, weights_fname=None):"
2.8.0,model_json_string = open(arch_fname).read()
2.8.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.8.0,if weights_fname is not None:
2.8.0,sequence_dnn.model.load_weights(weights_fname)
2.8.0,return sequence_dnn
2.8.0,create temporary fasta files
2.8.0,run command
2.8.0,remove fasta files
2.8.0,write test fasta file
2.8.0,test gkmsvm
2.8.0,get classification results
2.8.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.8.0,"Using canonical smiles for glycine, as in original research paper"
2.8.0,Atom features with padding
2.8.0,A_tilda_k computation
2.8.0,Final feed_dict setup
2.8.0,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.8.0,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.8.0,self.num_node_features)
2.8.0,Fit models
2.8.0,Args
2.8.0,2017 DeepCrystal Technologies - Patrick Hop
2.8.0,
2.8.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.8.0,
2.8.0,MIT License - have fun!!
2.8.0,===========================================================
2.8.0,x = F.selu( fc(x) )
2.8.0,x = F.selu( fc(x) )
2.8.0,2017 DeepCrystal Technologies - Patrick Hop
2.8.0,
2.8.0,Data loading a splitting file
2.8.0,
2.8.0,MIT License - have fun!!
2.8.0,===========================================================
2.8.0,Args
2.8.0,TODO (VIGS25): Account for the reload option
2.8.0,Downloading train files
2.8.0,Parsing training data
2.8.0,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.8.0,Test Files loading
2.8.0,One Hot Featurization
2.8.0,Consistency check
2.8.0,Handle output layer
2.8.0,Iterate over all previous tasks.
2.8.0,prev_layers is a list with elements of size
2.8.0,"(batch_size, layer_sizes[i-1])"
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,Save an initial checkpoint.
2.8.0,Turns out there are valid cases where we don't want pad-batches
2.8.0,on by default.
2.8.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0,Run training op.
2.8.0,Always save a final checkpoint when complete.
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,Note that we divide by the batch size and not the number of
2.8.0,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0,calculate with div/sum so it stays on the GPU.
2.8.0,aggregated costs
2.8.0,weight decay
2.8.0,Dummy placeholders
2.8.0,Dummy placeholders
2.8.0,run eval data through the model
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,Handle edge case when batch-size is 1.
2.8.0,Prune away any padding that was added
2.8.0,allow_soft_placement=True allows ops without a GPU implementation
2.8.0,to run on the CPU instead.
2.8.0,!/usr/bin/python
2.8.0,
2.8.0,Copyright 2015 Google Inc.
2.8.0,
2.8.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.8.0,you may not use this file except in compliance with the License.
2.8.0,You may obtain a copy of the License at
2.8.0,
2.8.0,http://www.apache.org/licenses/LICENSE-2.0
2.8.0,
2.8.0,"Unless required by applicable law or agreed to in writing, software"
2.8.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.8.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.8.0,See the License for the specific language governing permissions and
2.8.0,limitations under the License.
2.8.0,parse CheckpointState proto
2.8.0,parse path to actual checkpoint
2.8.0,the provided mask has to be the same shape as features
2.8.0,test k = 1..4
2.8.0,central moments
2.8.0,standardized moments
2.8.0,central across one axis
2.8.0,standardized across one axis
2.8.0,Fit just on task zero
2.8.0,Notice that we keep the session open
2.8.0,Fit on task one
2.8.0,The predictions for task zero should not change after training
2.8.0,on task one.
2.8.0,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.8.0,!/usr/bin/python
2.8.0,
2.8.0,Copyright 2015 Google Inc.
2.8.0,
2.8.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.8.0,you may not use this file except in compliance with the License.
2.8.0,You may obtain a copy of the License at
2.8.0,
2.8.0,http://www.apache.org/licenses/LICENSE-2.0
2.8.0,
2.8.0,"Unless required by applicable law or agreed to in writing, software"
2.8.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.8.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.8.0,See the License for the specific language governing permissions and
2.8.0,limitations under the License.
2.8.0,get the divisor
2.8.0,compute the requested central moment
2.8.0,"note that mean is a raw moment, not a central moment"
2.8.0,TODO(user): median is not implemented yet in TensorFlow
2.8.0,Add the input features.
2.8.0,"layer has shape [None, layer_sizes[i]]"
2.8.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.8.0,TODO(rbharath): Might want to make it feasible to have multiple
2.8.0,bypass layers.
2.8.0,Construct task bypass layer
2.8.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.8.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.8.0,"layer has shape [None, layer_sizes[i]]"
2.8.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.8.0,TODO(rbharath): Might want to make it feasible to have multiple
2.8.0,bypass layers.
2.8.0,Construct task bypass layer
2.8.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.8.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.8.0,Consistency check
2.8.0,Lazily created by _get_shared_session().
2.8.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.8.0,when subclass-overridden methods use the same scopes.
2.8.0,Setup graph
2.8.0,Create placeholders
2.8.0,Handle output layer
2.8.0,Iterate over all previous tasks.
2.8.0,prev_layers is a list with elements of size
2.8.0,"(batch_size, layer_sizes[i-1])"
2.8.0,Note that we divide by the batch size and not the number of
2.8.0,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0,calculate with div/sum so it stays on the GPU.
2.8.0,aggregated costs
2.8.0,weight decay
2.8.0,Dummy placeholders
2.8.0,Dummy placeholders
2.8.0,run eval data through the model
2.8.0,"Shape (n_tasks, n__samples)"
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,Handle edge case when batch-size is 1.
2.8.0,with self._get_shared_session(train=True) as sess:
2.8.0,Save an initial checkpoint.
2.8.0,Always save a final checkpoint when complete.
2.8.0,Note that we divide by the batch size and not the number of
2.8.0,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0,calculate with div/sum so it stays on the GPU.
2.8.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.8.0,Dummy placeholders
2.8.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.8.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.8.0,Dummy placeholders
2.8.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.8.0,allow_soft_placement=True allows ops without a GPU implementation
2.8.0,to run on the CPU instead.
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,Turns out there are valid cases where we don't want pad-batches
2.8.0,on by default.
2.8.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.8.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,"(n_samples, n_classes)"
2.8.0,"(n_samples, n_tasks, n_classes)"
2.8.0,Save hyperparameters
2.8.0,Guard variable to make sure we don't Restore() this model
2.8.0,from a disk checkpoint more than once.
2.8.0,"Path to save checkpoint files, which matches the"
2.8.0,replicated supervisor's default path.
2.8.0,Lazily created by _get_shared_session().
2.8.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.8.0,when subclass-overridden methods use the same scopes.
2.8.0,Setup graph
2.8.0,Note that we divide by the batch size and not the number of
2.8.0,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0,calculate with div/sum so it stays on the GPU.
2.8.0,aggregated costs
2.8.0,weight decay
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,Save an initial checkpoint.
2.8.0,Define the code that runs on a separate thread to feed data into the queue.
2.8.0,Main training loop.
2.8.0,Run training op.
2.8.0,We have reached the end of an epoch.
2.8.0,We have reached the end of the data.
2.8.0,Always save a final checkpoint when complete.
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,allow_soft_placement=True allows ops without a GPU implementation
2.8.0,to run on the CPU instead.
2.8.0,gpu memory growth option
2.8.0,gpu memory growth option
2.8.0,TODO(rbharath): Is setting train=False right here?
2.8.0,Discard any padded predictions
2.8.0,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0,Remove padded examples.
2.8.0,Special case to handle singletasks.
2.8.0,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0,Remove padded examples.
2.8.0,TODO(rbharath): Verify this can be safely removed.
2.8.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.8.0,""""""""
2.8.0,Evaluates the performance of this model on specified dataset.
2.8.0,
2.8.0,Parameters
2.8.0,----------
2.8.0,dataset: dc.data.Dataset
2.8.0,Dataset object.
2.8.0,metric: deepchem.metrics.Metric
2.8.0,Evaluation metric
2.8.0,transformers: list
2.8.0,List of deepchem.transformers.Transformer
2.8.0,Returns
2.8.0,-------
2.8.0,dict
2.8.0,Maps tasks to scores under metric.
2.8.0,""""""""
2.8.0,"evaluator = Evaluator(self, dataset, transformers)"
2.8.0,scores = evaluator.compute_model_performance(metrics)
2.8.0,return scores
2.8.0,checkpoints look like model_dir/model.ckpt-N
2.8.0,"self._save_path is ""model_dir/model.ckpt"""
2.8.0,run eval data through the model
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,run eval data through the model
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,Note that softmax is already applied in construct_grpah
2.8.0,run eval data through the model
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,Handle edge case when batch-size is 1.
2.8.0,Prune away any padding that was added
2.8.0,Handle case of 0-dimensional scalar output
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,inputs placeholder
2.8.0,data preprocessing and augmentation
2.8.0,first conv layer
2.8.0,downsample by max pooling
2.8.0,each module is a residual convolutional block
2.8.0,followed by a convolutional downsample layer
2.8.0,max pooling over the final outcome
2.8.0,fully connected layers
2.8.0,dropout for dense layers
2.8.0,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.8.0,weight decay regularizer
2.8.0,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.8.0,sample cut ratio from a clipped gaussian
2.8.0,train/valid differences
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,Define and build model
2.8.0,model.restore()
2.8.0,Set random seeds
2.8.0,Setup directories
2.8.0,Model constants
2.8.0,Load and transform datasets
2.8.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0,Atomic convolution variables
2.8.0,at = atomic numbers (atom types)
2.8.0,"radial basis function parameters [cutoff, mean, width]"
2.8.0,Model hyperparameters
2.8.0,Initialize model
2.8.0,Fit model
2.8.0,Evaluate model
2.8.0,Set random seeds
2.8.0,Setup directories
2.8.0,Model constants
2.8.0,Load and transform datasets
2.8.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0,Atomic convolution variables
2.8.0,at = atomic numbers (atom types)
2.8.0,"radial basis function parameters [cutoff, mean, width]"
2.8.0,Model hyperparameters
2.8.0,Initialize model
2.8.0,Fit model
2.8.0,Evaluate model
2.8.0,Set random seeds
2.8.0,Setup directories
2.8.0,Model constants
2.8.0,Load and transform datasets
2.8.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0,Atomic convolution variables
2.8.0,at = atomic numbers (atom types)
2.8.0,"radial basis function parameters [cutoff, mean, width]"
2.8.0,Model hyperparameters
2.8.0,Initialize model
2.8.0,Fit model
2.8.0,Evaluate model
2.8.0,Set random seeds
2.8.0,Setup directories
2.8.0,Model constants
2.8.0,Load and transform datasets
2.8.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0,Atomic convolution variables
2.8.0,at = atomic numbers (atom types)
2.8.0,"radial basis function parameters [cutoff, mean, width]"
2.8.0,Model hyperparameters
2.8.0,Initialize model
2.8.0,Fit model
2.8.0,Evaluate model
2.8.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.8.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.8.0,test_scores = test_evaluator.compute_model_performance(metric)
2.8.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.8.0,param.update(test_scores)
2.8.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0,for transformer in transformers:
2.8.0,train_dataset = transformer.transform(train_dataset)
2.8.0,test_dataset = transformer.transform(test_dataset)
2.8.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0,for transformer in transformers:
2.8.0,train_dataset = transformer.transform(train_dataset)
2.8.0,test_dataset = transformer.transform(test_dataset)
2.8.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0,for transformer in transformers:
2.8.0,train_dataset = transformer.transform(train_dataset)
2.8.0,test_dataset = transformer.transform(test_dataset)
2.8.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0,for transformer in transformers:
2.8.0,train_dataset = transformer.transform(train_dataset)
2.8.0,test_dataset = transformer.transform(test_dataset)
2.8.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0,Create some directories for analysis
2.8.0,The base_dir holds the results of all analysis
2.8.0,Make directories to store the raw and featurized datasets.
2.8.0,Load PDBBind dataset
2.8.0,Define featurizers
2.8.0,Currently featurizes with shard_size=1
2.8.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.8.0,This could be done with openbabel in python
2.8.0,Compute cells for this molecule. O(constant)
2.8.0,min == max if molecule is planar in some direction
2.8.0,we should still create a bin
2.8.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.8.0,Note neighbors contains self!
2.8.0,Associate each atom with cell it belongs to. O(N)
2.8.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0,"conditions, so does wrapround. O(constant)"
2.8.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.8.0,Accept as neighbors only those within threshold. This computation should be
2.8.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.8.0,Sort neighbors by distance
2.8.0,Pick up to max_num_neighbors
2.8.0,Type of data created by this featurizer
2.8.0,assumes that every array is of the same dimension
2.8.0,rem_dataset is remaining portion of dataset
2.8.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.8.0,to k-1.
2.8.0,returns list of per column sum of non zero elements
2.8.0,Compute number of actives needed per task.
2.8.0,loop through each column and obtain index required to splice out for
2.8.0,required fraction of hits
2.8.0,Find the first index where the cumulative number of actives equals
2.8.0,the actives_count
2.8.0,Note that np.where tells us last index required to exceed
2.8.0,"actives_count, so we actually want the following location"
2.8.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.8.0,potentially refactor those to match this.
2.8.0,Handle edge case where frac_split is 1
2.8.0,Create weight matrices fpor two haves.
2.8.0,copy over up to required index for weight first_split
2.8.0,check out if any rows in either w_1 or w_2 are just zeros
2.8.0,"Obtain original x, y, and w arrays and shuffle"
2.8.0,calculate percent split for valid (out of test and valid)
2.8.0,"split test data into valid and test, treating sub test set also as sparse"
2.8.0,rem_dataset is remaining portion of dataset
2.8.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.8.0,to k-1.
2.8.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.8.0,This can be generalized in the future with some common demoninator determination.
2.8.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.8.0,Append remaining examples to train
2.8.0,Sort by increasing MW
2.8.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.8.0,for m_idx in cluster:
2.8.0,"continue until we find an active in all the tasks, otherwise we can't"
2.8.0,compute a meaningful AUC
2.8.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.8.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.8.0,Sort from largest to smallest scaffold sets
2.8.0,Sort from largest to smallest scaffold sets
2.8.0,"(n_samples, n_classes)"
2.8.0,"(n_samples, n_tasks, n_classes)"
2.8.0,Save hyperparameters
2.8.0,Guard variable to make sure we don't Restore() this model
2.8.0,from a disk checkpoint more than once.
2.8.0,"Path to save checkpoint files, which matches the"
2.8.0,replicated supervisor's default path.
2.8.0,Lazily created by _get_shared_session().
2.8.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.8.0,when subclass-overridden methods use the same scopes.
2.8.0,Setup graph
2.8.0,Note that we divide by the batch size and not the number of
2.8.0,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0,calculate with div/sum so it stays on the GPU.
2.8.0,aggregated costs
2.8.0,weight decay
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,Save an initial checkpoint.
2.8.0,Turns out there are valid cases where we don't want pad-batches
2.8.0,on by default.
2.8.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0,Run training op.
2.8.0,Always save a final checkpoint when complete.
2.8.0,############################################################# TIMING
2.8.0,############################################################# TIMING
2.8.0,allow_soft_placement=True allows ops without a GPU implementation
2.8.0,to run on the CPU instead.
2.8.0,TODO(rbharath): Is setting train=False right here?
2.8.0,Discard any padded predictions
2.8.0,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0,Remove padded examples.
2.8.0,Special case to handle singletasks.
2.8.0,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0,Remove padded examples.
2.8.0,TODO(rbharath): Verify this can be safely removed.
2.8.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.8.0,""""""""
2.8.0,Evaluates the performance of this model on specified dataset.
2.8.0,
2.8.0,Parameters
2.8.0,----------
2.8.0,dataset: dc.data.Dataset
2.8.0,Dataset object.
2.8.0,metric: deepchem.metrics.Metric
2.8.0,Evaluation metric
2.8.0,transformers: list
2.8.0,List of deepchem.transformers.Transformer
2.8.0,Returns
2.8.0,-------
2.8.0,dict
2.8.0,Maps tasks to scores under metric.
2.8.0,""""""""
2.8.0,"evaluator = Evaluator(self, dataset, transformers)"
2.8.0,scores = evaluator.compute_model_performance(metrics)
2.8.0,return scores
2.8.0,checkpoints look like logdir/model.ckpt-N
2.8.0,"self._save_path is ""logdir/model.ckpt"""
2.8.0,run eval data through the model
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,run eval data through the model
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,Note that softmax is already applied in construct_grpah
2.8.0,run eval data through the model
2.8.0,reshape to batch_size x n_tasks x ...
2.8.0,Handle edge case when batch-size is 1.
2.8.0,Prune away any padding that was added
2.8.0,Handle case of 0-dimensional scalar output
2.8.0,Dummy placeholders
2.8.0,Dummy placeholders
2.8.0,## AtomicNet fully-connected layer ops ###
2.8.0,## Atomicnet coordinate transform ops ###
2.8.0,## Atomicnet symmetry function kernel ops ###
2.8.0,## Atomicnet symmetry function ops ###
2.8.0,## Atomcnet symmetry function layer ops ###
2.8.0,We apply the radial pooling filter before atom type conv
2.8.0,to reduce computation
2.8.0,## Misc convenience ops ###
2.8.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.8.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.8.0,"game).  The average reward for any bet is slightly negative, so the best"
2.8.0,strategy is to walk away.
2.8.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0,Optimize it.
2.8.0,"It should have learned that the expected value is very close to zero, and that the best"
2.8.0,action is to walk away.
2.8.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.8.0,get the same result.
2.8.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0,Run the algorithm.
2.8.0,Save a file checkpoint.
2.8.0,Build the tree.
2.8.0,Compute the final probabilities and expected reward.
2.8.0,Mark this node as terminal
2.8.0,Expand this node.
2.8.0,Select the next action to perform.
2.8.0,Recursively build the tree.
2.8.0,Update statistics for this node.
2.8.0,Configuration file for the Sphinx documentation builder.
2.8.0,
2.8.0,This file only contains a selection of the most common options. For a full
2.8.0,list see the documentation:
2.8.0,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.8.0,-- Path setup --------------------------------------------------------------
2.8.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.8.0,add these directories to sys.path here. If the directory is relative to the
2.8.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.8.0,
2.8.0,-- Project information -----------------------------------------------------
2.8.0,"The full version, including alpha/beta/rc tags"
2.8.0,-- General configuration ---------------------------------------------------
2.8.0,"Add any Sphinx extension module names here, as strings. They can be"
2.8.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.8.0,ones.
2.8.0,Options for autodoc directives
2.8.0,How to represents typehints
2.8.0,"Add any paths that contain templates here, relative to this directory."
2.8.0,The suffix of source filenames.
2.8.0,The master toctree document.
2.8.0,autosectionlabel setting
2.8.0,"List of patterns, relative to source directory, that match files and"
2.8.0,directories to ignore when looking for source files.
2.8.0,This pattern also affects html_static_path and html_extra_path.
2.8.0,"If true, the current module name will be prepended to all description"
2.8.0,unit titles (such as .. function::).
2.8.0,-- Options for HTML output -------------------------------------------------
2.8.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.8.0,a list of builtin themes.
2.8.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.8.0,"relative to this directory. They are copied after the builtin static files,"
2.8.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.8.0,The name of an image file (relative to this directory) to place at the top
2.8.0,of the sidebar.
2.8.0,Customize the sphinx theme
2.8.0,-- Source code links ---------------------------------------------------
2.8.0,Resolve function for the linkcode extension.
2.8.0,"try to find the file and line number, based on code from numpy:"
2.8.0,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.8.0,lines in the label file have format
2.8.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.8.0,"print line[0], line[3]"
2.8.0,"If you push the tag, please remove `.dev`"
2.8.0,Record inputs.
2.8.0,Create the output directory if necessary.
2.8.0,Select a device.
2.8.0,Create the optimizers for meta-optimization and task optimization.
2.8.0,Main optimization loop.
2.8.0,Do checkpointing.
2.8.0,Save the checkpoint to a file.
2.8.0,Rename and delete older files.
2.8.0,Record inputs.
2.8.0,Create the output directory if necessary.
2.8.0,Create the optimizers for meta-optimization and task optimization.
2.8.0,Create a Checkpoint for saving.
2.8.0,Main optimization loop.
2.8.0,Do checkpointing.
2.8.0,flake8: noqa
2.8.0,This is a MetaLearner that learns to generate sine functions with variable
2.8.0,amplitude and phase.
2.8.0,Optimize it.
2.8.0,Test it out on some new tasks and see how it works.
2.8.0,Initially the model should do a bad job of fitting the sine function.
2.8.0,After one step of optimization it should do much better.
2.8.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.8.0,get the same result.
2.8.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0,Optimize it.
2.8.0,Test it out on some new tasks and see how it works.
2.8.0,Initially the model should do a bad job of fitting the sine function.
2.8.0,After one step of optimization it should do much better.
2.8.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.8.0,get the same result.
2.8.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0,We know use_pose_generator_scores == False in this case
2.8.0,check whether self.featurizer is instance of ComplexFeaturizer or not
2.8.0,TODO: How to handle the failure here?
2.8.0,TODO(rbharath): The autodock vina source computes surface distances
2.8.0,which take into account the van der Waals radius of each atom type.
2.8.0,"Shape (N, M)"
2.8.0,"Shape (N, M)"
2.8.0,Parse complex
2.8.0,check filetypes
2.8.0,Define locations of log and output files
2.8.0,Write GNINA conf file
2.8.0,Run GNINA
2.8.0,read output and log
2.8.0,Parse complex
2.8.0,Prepare protein
2.8.0,Get protein centroid and range
2.8.0,TODO(rbharath: Does vina divide box dimensions by 2?
2.8.0,Prepare ligand
2.8.0,Write Vina conf file
2.8.0,Define locations of output files
2.8.0,flake8: noqa
2.8.0,We provide no scoring model so the docker won't score
2.8.0,Check only one output since num_modes==1
2.8.0,We provide no scoring model so the docker won't score
2.8.0,Check only one output since num_modes==1
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Check returned files exist
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Check returned files exist
2.8.0,"Where d is greater than zero, the repulsion is just zeros"
2.8.0,"When d is 0, this should just be 1"
2.8.0,"When d == 0, the hbond interaction is 0"
2.8.0,The exponential returns 1 when input 0.
2.8.0,This exponential returns 1 when input 3
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Note this may download autodock Vina...
2.8.0,Let's turn on logging since this test will run for a while
2.8.0,Note this may download autodock Vina...
2.8.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.8.0,Test that every atom in pocket maps exists
2.8.0,scalar case
2.8.0,per-example case
2.8.0,This is a little arcane but it repeats w across tasks.
2.8.0,"If w.shape == (n_samples, 1) handle it as 1D"
2.8.0,"w.shape == (n_samples, n_tasks)"
2.8.0,scalar case
2.8.0,Handle n_classes/n_task shape ambiguity
2.8.0,Add in task dimension
2.8.0,Insert a task dimension (we know n_tasks=1 from above0
2.8.0,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.8.0,Handle classification. We need to convert labels into one-hot representation.
2.8.0,check whether n_classes is int or not
2.8.0,Handle n_classes/n_task shape ambiguity
2.8.0,Add in task dimension
2.8.0,Make everything 2D so easy to handle
2.8.0,Handle each task separately.
2.8.0,Handle continuous class probabilites of positive class for binary
2.8.0,Fill in class 0 probabilities
2.8.0,Add a task dimension to concatenate on
2.8.0,Handle binary labels
2.8.0,"make y_hot of shape (N, n_classes)"
2.8.0,Add a task dimension to concatenate on
2.8.0,Insert a task dimension
2.8.0,"Now of shape (N,)"
2.8.0,"Now of shape (N, 1)"
2.8.0,"Returns shape (N, n_tasks)"
2.8.0,"Now of shape (N,)"
2.8.0,"Now of shape (N, n_classes)"
2.8.0,"Now of shape (N, 1, n_classes)"
2.8.0,"Returns shape (N, n_tasks, n_classes)"
2.8.0,These are some smart defaults
2.8.0,These are some smart defaults corresponding to sklearn's required
2.8.0,behavior
2.8.0,Attempt some limited shape imputation to find n_tasks
2.8.0,check whether n_tasks is int or not
2.8.0,This is because `normalize_weight_shape` require int value.
2.8.0,FIXME: Incompatible types in assignment
2.8.0,Attempt to convert both into the same type
2.8.0,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.8.0,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.8.0,initialize fwd and reverse scores to -infinity
2.8.0,"cross-correlate separately for each base,"
2.8.0,for both the PSSM and its reverse complement
2.8.0,sum over the bases
2.8.0,take max of fwd and reverse scores at each position
2.8.0,"Shape (N_sequences, num_tasks)"
2.8.0,check whether wild_type_predictions is np.ndarray or not
2.8.0,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.8.0,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.8.0,Mutates every position of the sequence to every letter
2.8.0,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.8.0,Breakdown:
2.8.0,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.8.0,remove wild-type
2.8.0,len(arange) = N_letters * sequence_length
2.8.0,len(horizontal cycle) = N_letters * sequence_length
2.8.0,add mutant
2.8.0,make mutant predictions
2.8.0,check whether wild_type_predictions is np.ndarray or not
2.8.0,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.8.0,validation
2.8.0,flake8: noqa
2.8.0,metric class
2.8.0,metrics utils
2.8.0,sklearn & scipy score function
2.8.0,original score function
2.8.0,Get a random prediction matrix
2.8.0,"Of shape (N, n_classes)"
2.8.0,"Of shape (N, 1, n_classes)"
2.8.0,This has w for each task.
2.8.0,Best score case
2.8.0,Worst score case
2.8.0,best case
2.8.0,duplicate prediction value
2.8.0,Encode motif
2.8.0,"sequences now has shape (3, 4, 5, 1)"
2.8.0,"sequences now has shape (3, 4, 5, 1)"
2.8.0,Construct and train SequenceDNN model
2.8.0,Call in-silico mutagenesis
2.8.0,Construct and train SequenceDNN model
2.8.0,Call in-silico mutagenesis
2.8.0,Check nonzero elements exist
2.8.0,Special case handling of single input
2.8.0,Featurize task results iff they exist.
2.8.0,Filter out examples where featurization failed.
2.8.0,"For prospective data where results are unknown, it"
2.8.0,makes no sense to have y values or weights.
2.8.0,Featurize task results if they exist.
2.8.0,Filter out examples where featurization failed.
2.8.0,"For prospective data where results are unknown, it"
2.8.0,makes no sense to have y values or weights.
2.8.0,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.8.0,The field in which load_sdf_files return value stores smiles
2.8.0,Special case handling of single input
2.8.0,Featurize task results iff they exist.
2.8.0,Filter out examples where featurization failed.
2.8.0,"For prospective data where results are unknown, it"
2.8.0,makes no sense to have y values or weights.
2.8.0,Process legacy toggle
2.8.0,Set attributes
2.8.0,Handle special featurizer cases
2.8.0,Set self.featurizer
2.8.0,"(X, y, w, ids)"
2.8.0,TODO don't convert all sequences into np array (allow shards)
2.8.0,Check if line is a header
2.8.0,Handle empty sequence
2.8.0,Annotate start/stop of sequence
2.8.0,Open index file
2.8.0,create an empty list to store lines in files.
2.8.0,iterate through each line in the input file
2.8.0,If the number of lines iterated through is equal or less than the shard size:
2.8.0,append to list
2.8.0,else yield the list
2.8.0,set the line_number variable to the last line number (num) before 'yield' was called
2.8.0,yield list (shard/batch)
2.8.0,Re-initialize list with the index line to begin a new shard.
2.8.0,Set attributes
2.8.0,Handle special featurizer cases
2.8.0,Set self.featurizer
2.8.0,Set self.return_quality_scores
2.8.0,Featurize sequences
2.8.0,"(X, y , w, ids)"
2.8.0,Featurize sequences
2.8.0,"(X, y , w, ids)"
2.8.0,Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
2.8.0,First line : header description
2.8.0,second line : sequence
2.8.0,third line : more description usually the same as the first line
2.8.0,fourth line: quality scores of the sequence
2.8.0,Second line : add sequence to the sequence array
2.8.0,Fourth line
2.8.0,Handle empty sequence
2.8.0,Annotate start/stop of sequence
2.8.0,Sometimes zip files contain directories within. Traverse directories
2.8.0,Sort image files
2.8.0,Sometimes zip files contain directories within. Traverse directories
2.8.0,Sort label image files
2.8.0,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.8.0,Set attributes
2.8.0,Handle special featurizer cases
2.8.0,Set self.featurizer
2.8.0,"(X, y, w, ids)"
2.8.0,Set attributes
2.8.0,Handle special featurizer cases
2.8.0,Set self.featurizer
2.8.0,"(X, y, w, ids)"
2.8.0,Set attributes
2.8.0,Handle special featurizer cases
2.8.0,Set self.featurizer
2.8.0,"(X, y, w, ids)"
2.8.0,Remove support indices
2.8.0,Remove support indices
2.8.0,Remove support indices
2.8.0,Get task specific entries
2.8.0,Now just get weights for this task
2.8.0,Get task specific entries
2.8.0,Now just get weights for this task
2.8.0,Now just get weights for this task
2.8.0,Now just get weights for this task
2.8.0,Split data into pos and neg lists.
2.8.0,No replacement allowed for supports
2.8.0,Handle one-d vs. non one-d feature matrices
2.8.0,Init the iterator
2.8.0,Set initial iterator state
2.8.0,support = self.supports[task][self.trial_num]
2.8.0,Increment and update logic
2.8.0,Init the iterator
2.8.0,Set initial iterator state
2.8.0,support = self.supports[task][self.trial_num]
2.8.0,Increment and update logic
2.8.0,Ensure that every worker will pick the same random order for each epoch.
2.8.0,Ensure that every worker will pick the same random order for each epoch.
2.8.0,"By invariant of when this is called, can assume num_samples > 0"
2.8.0,and num_samples < batch_size
2.8.0,Fill in batch arrays
2.8.0,"By invariant of when this is called, can assume num_samples > 0"
2.8.0,and num_samples < batch_size
2.8.0,Fill in batch arrays
2.8.0,Only the first set of copy will be counted in training loss
2.8.0,Retrieve the first sample so we can determine the dtypes.
2.8.0,Create a Tensorflow Dataset.
2.8.0,Find the X values.
2.8.0,Find the y values.
2.8.0,Find the w values.
2.8.0,Find the ids.
2.8.0,"Set labels to be zero, with zero weights"
2.8.0,The line here assumes that y generated by shard_generator is a numpy array
2.8.0,Load obsolete format -> save in new format
2.8.0,note that this corresponds to the _construct_metadata column order
2.8.0,Create temp directory to store resharded version
2.8.0,Get correct shapes for y/w
2.8.0,Write data in new shards
2.8.0,Handle shapes
2.8.0,Note that this means that DiskDataset resharding currently doesn't
2.8.0,work for datasets that aren't regression/classification.
2.8.0,Handle spillover from last shard
2.8.0,Should have updated to non-legacy metadata
2.8.0,Note that this resets the cache internally
2.8.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.8.0,"than process based pools, since process based pools need to pickle/serialize"
2.8.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.8.0,we're actually protected by the GIL.
2.8.0,mp.dummy aliases ThreadPool to Pool
2.8.0,(ytz): this skips everything except possibly the last shard
2.8.0,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.8.0,make a NumpyDataset under the hood
2.8.0,"raw_data = (X, y, w, ids)"
2.8.0,Protect against generator exhaustion
2.8.0,This ensures tasks are consistent for all datasets
2.8.0,determine the shard sizes of the datasets to merge
2.8.0,"otherwise the entire dataset is the ""shard size"""
2.8.0,we must reshard the dataset to have a uniform size
2.8.0,choose the smallest shard size
2.8.0,Get full dataset in memory
2.8.0,Shuffle in memory
2.8.0,Write shuffled shards out to disk
2.8.0,Shuffle the arrays corresponding to each row in metadata_df
2.8.0,Reset cache
2.8.0,See if we have a cached copy of this shard.
2.8.0,"We don't, so load it from disk."
2.8.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.8.0,Try to cache this shard for later use.  Since the normal usage pattern is
2.8.0,"a series of passes through the whole dataset, there's no point doing"
2.8.0,anything fancy.  It never makes sense to evict another shard from the
2.8.0,"cache to make room for this one, because we'll probably want that other"
2.8.0,shard again before the next time we want this one.  So just cache as many
2.8.0,as we can and then stop.
2.8.0,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.8.0,Handle edge case with empty indices
2.8.0,We use two loops here. The outer while loop walks over selection shards
2.8.0,(the chunks of the indices to select that should go into separate
2.8.0,"output shards), while the inner for loop walks over the shards in the"
2.8.0,source datasets to select out the shard indices from that  source shard
2.8.0,Find indices which rest in this shard
2.8.0,Need to offset indices to fit within shard_size
2.8.0,Handle empty case where no data from this shard needed
2.8.0,Handle the case of datasets with y/w missing
2.8.0,Break if all indices have been used up already
2.8.0,Note these will be in the sorted order
2.8.0,We need to recover the original ordering. We can do this by using
2.8.0,np.where to find the locatios of the original indices in the sorted
2.8.0,indices.
2.8.0,We know there's only one match for np.where since this is a
2.8.0,"permutation, so the [0][0] pulls out the exact match location."
2.8.0,If shape metadata is available use it to directly compute shape from
2.8.0,metadata
2.8.0,"In absense of shape metadata, fall back to loading data from disk to"
2.8.0,find shape.
2.8.0,Case n_samples should be 1
2.8.0,flake8: noqa
2.8.0,TODO(rbharath): Get rid of * import
2.8.0,Test merging of numpy datasets
2.8.0,Load MUV dataset
2.8.0,Do an approximate comparison since splits are sometimes slightly off from
2.8.0,the exact fraction.
2.8.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.8.0,reloading will cause the transform to be reapplied. This is undesirable in
2.8.0,almost all cases. Need to understand a method to fix this.
2.8.0,The shuffling should have switched up the ordering
2.8.0,But all the same entries should still be present
2.8.0,All the data should have same shape
2.8.0,The shuffling should have switched up the ordering
2.8.0,But all the same entries should still be present
2.8.0,All the data should have same shape
2.8.0,The ids should now store the performed permutation. Check that the
2.8.0,original dataset is recoverable.
2.8.0,The ids should now store the performed permutation. Check that the
2.8.0,original dataset is recoverable.
2.8.0,Generate data
2.8.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.8.0,around for testing resharding.
2.8.0,Set cache to 0 size to avoid cache hiding errors
2.8.0,Generate data
2.8.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.8.0,around for testing resharding.
2.8.0,Set cache to 0 size to avoid cache hiding errors
2.8.0,Featurize emols dataset
2.8.0,example.fasta contains 3 sequences each of length 58.
2.8.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.8.0,"There is one ""image channel""."
2.8.0,"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
2.8.0,TODO: test with full uniprot file once sharding support is added.
2.8.0,Generate dummy dataset
2.8.0,Generate dummy dataset
2.8.0,Generate dummy dataset
2.8.0,Set last n_samples/2 weights to 0
2.8.0,Check that no support elements are sample from zero-weight samples
2.8.0,Generate dummy dataset
2.8.0,Generate dummy dataset
2.8.0,Create support generator
2.8.0,Generate dummy dataset
2.8.0,Create support generator
2.8.0,Generate dummy dataset
2.8.0,Assert all support elements have been removed
2.8.0,Generate dummy dataset
2.8.0,Assert all remove elements have been removed
2.8.0,Generate dummy dataset
2.8.0,Assert all support elements have been removed
2.8.0,Generate dummy dataset
2.8.0,Assert all remove elements have been removed
2.8.0,Generate dummy dataset
2.8.0,Set last n_samples/2 weights to 0
2.8.0,Sample from first n_samples/2 elements for support
2.8.0,Should lie within first n_samples/2 samples only
2.8.0,Generate dummy dataset
2.8.0,Create support generator
2.8.0,Generate dummy dataset
2.8.0,This is necessary since from_numpy adds in shape information
2.8.0,This is necessary since from_numpy adds in shape information
2.8.0,This is necessary since from_numpy adds in shape information
2.8.0,Generate data
2.8.0,Generate data
2.8.0,Generate data
2.8.0,Should now have 10 shards
2.8.0,This is the shape of legacy_data
2.8.0,legacy_dataset is a dataset in the legacy format kept around for testing
2.8.0,purposes.
2.8.0,This is the shape of legacy_data_reshard
2.8.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.8.0,around for testing
2.8.0,Should now have 10 shards
2.8.0,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.8.0,Test constructor reload works for legacy format
2.8.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.8.0,around for testing resharding.
2.8.0,Reshard copy
2.8.0,Check metadata has been updated
2.8.0,First try using images for X.
2.8.0,Now try using images for y.
2.8.0,Transform it
2.8.0,Test on identity matrix
2.8.0,Generate random sparse features dataset
2.8.0,Test edge case with array of all zeros
2.8.0,Test cases where n_samples < 2*n_samples < batch_size
2.8.0,Test cases where n_samples < batch_size
2.8.0,Test case where n_samples == batch_size
2.8.0,Test case for object featurization.
2.8.0,Test case for more complicated object featurization
2.8.0,Test case with multidimensional data
2.8.0,Test cases where n_samples < 2*n_samples < batch_size
2.8.0,Test cases where n_samples < batch_size
2.8.0,Test case where n_samples == batch_size
2.8.0,Test case for object featurization.
2.8.0,Test case for more complicated object featurization
2.8.0,Test case with multidimensional data
2.8.0,Test first resharding worked
2.8.0,Test second resharding worked
2.8.0,approx 1/15! chance of equality
2.8.0,Generate data
2.8.0,Generate data
2.8.0,Transform it
2.8.0,special case to test
2.8.0,deterministic
2.8.0,non-deterministic
2.8.0,we don't know the order in which the shards are iterated in.
2.8.0,Check that we have all the data in
2.8.0,Test iterating in order.
2.8.0,Test iterating out of order.
2.8.0,Test iterating in batches.
2.8.0,Test iterating with multiple workers.
2.8.0,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.8.0,Try specifying particular columns.
2.8.0,Try specifying particular columns
2.8.0,Test id shrinkage
2.8.0,Test task shrinkage
2.8.0,Test max print size
2.8.0,Create image file
2.8.0,Create directory of multiple image files
2.8.0,Zip directory of multiple image files
2.8.0,Create zip of image file
2.8.0,Create zip of multiple image files
2.8.0,"Create zip of multiple image files, multiple_types"
2.8.0,Create image directory
2.8.0,These are the known dimensions of face.png
2.8.0,These are the known dimensions of face.png
2.8.0,These are the known dimensions of face.png
2.8.0,TODO(rbharath): Where are the color channels?
2.8.0,These are the known dimensions of a_image.tif
2.8.0,These are the known dimensions of a_image.tif
2.8.0,"Since the different files have different shapes, makes an object array"
2.8.0,Test that the order of the contents of an unzipped file is preserved.
2.8.0,Load the zip file
2.8.0,Load multi_path directly
2.8.0,Check that the order of the files is the same
2.8.0,Splits featurized samples into train/test
2.8.0,Splits featurized samples into train/test
2.8.0,Splits featurized samples into train/test
2.8.0,Splits featurized samples into train/test
2.8.0,Now perform move
2.8.0,Only for debug!
2.8.0,Make directories to store the raw and featurized datasets.
2.8.0,Load dataset
2.8.0,Featurize tox21 dataset
2.8.0,featurization
2.8.0,train/valid split.
2.8.0,singletask load
2.8.0,comparison
2.8.0,Only for debug!
2.8.0,Make directories to store the raw and featurized datasets.
2.8.0,Load dataset
2.8.0,Featurize tox21 dataset
2.8.0,For debugging purposes
2.8.0,multitask load
2.8.0,Do train/valid split.
2.8.0,singletask load
2.8.0,comparison
2.8.0,Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
2.8.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.8.0,"Expected shape is now (4, 192, 5)"
2.8.0,Get the labels/weights
2.8.0,Normalize shapes
2.8.0,Remove labels with zero weights
2.8.0,Note that we may have 0 elements of a given class since we remove those
2.8.0,labels with zero weight.
2.8.0,this works because y is 1D
2.8.0,This is the right ratio since int(N/num_c) * num_c \approx N
2.8.0,for all classes
2.8.0,Flattening is safe because of shape check above
2.8.0,Hack to allow for easy unpickling:
2.8.0,http://stefaanlippens.net/pickleproblem
2.8.0,Some transformation must happen
2.8.0,Add this case in to handle non-DiskDataset that should be written to disk
2.8.0,Note that transformers have to be undone in reversed order
2.8.0,Handle division by zero
2.8.0,Handle division by zero
2.8.0,Control for pathological case with no variance.
2.8.0,Handle case with 1 task correctly
2.8.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.8.0,Find the task dimension of z
2.8.0,Prevent broadcasting on wrong dimension
2.8.0,BalancingTransformer can only transform weights.
2.8.0,Compute weighting factors from dataset.
2.8.0,Handle 1-D case
2.8.0,Remove labels with zero weights
2.8.0,Note that we may have 0 elements of a given class since we remove those
2.8.0,labels with zero weight. This typically happens in multitask datasets
2.8.0,where some datapoints only have labels for some tasks.
2.8.0,this works because task_y is 1D
2.8.0,This is the right ratio since N_task/num_c * num_c = N_task
2.8.0,for all classes
2.8.0,Set to the class weight computed previously
2.8.0,Need this for transform_y
2.8.0,Handle 1D case
2.8.0,THis reshape is safe because of guard above.
2.8.0,map the indices to labels
2.8.0,generating batch of data by slicing similarity matrix
2.8.0,into 100*reference_dataset_length
2.8.0,concatenate batches of data together
2.8.0,highest similarity is 1: target is in the reference
2.8.0,use the following K points
2.8.0,"highest less than 1: target not in the reference, use top K points"
2.8.0,calculate matrix multiplicatin on slices
2.8.0,concatenate the slices together
2.8.0,list of calculation orders for DAGs
2.8.0,stemming from one specific atom in the molecule
2.8.0,starting from the adjacency list derived by graphconv featurizer
2.8.0,"number of atoms, also number of DAGs"
2.8.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.8.0,each step calculating graph features for one atom.
2.8.0,`max_atoms` is the maximum number of steps
2.8.0,each iteration generates the DAG starting from atom with index `count`
2.8.0,"list of lists, elements represent the calculation orders"
2.8.0,for atoms in the current graph
2.8.0,starting from the target atom with index `count`
2.8.0,flags of whether the atom is already included in the DAG
2.8.0,atom `count` is in the DAG
2.8.0,recording number of radial propagation steps
2.8.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.8.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.8.0,will be added in the second loop(radial=1).
2.8.0,atoms i-bond away will be added in i-th loop
2.8.0,"when molecules have separate parts, starting from one part,"
2.8.0,it is not possible to include all atoms.
2.8.0,this break quit the loop when going into such condition
2.8.0,reinitialize targets for next iteration
2.8.0,atoms connected to current_atom
2.8.0,generate the dependency map of current DAG
2.8.0,atoms connected to `current_atoms`(and not included in the DAG)
2.8.0,"are added, and will be the `current_atoms` for next iteration."
2.8.0,"DAG starts from the target atom, calculation should go in reverse"
2.8.0,`edge[1]` is the parent of `edge[0]`
2.8.0,"after this loop, `parents[i]` includes all parents of atom i"
2.8.0,manually adding the atom index into its parents list
2.8.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.8.0,atoms with less parents(farther from the target atom) come first.
2.8.0,"graph features of atoms without parents will be first calculated,"
2.8.0,then atoms with more parents can be calculated in order
2.8.0,based on previously calculated graph features.
2.8.0,target atom of this DAG will be calculated in the last step
2.8.0,padding with `max_atoms`
2.8.0,padding
2.8.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.8.0,which is a max_atoms * max_atoms numpy array after padding
2.8.0,class ANITransformer(Transformer):
2.8.0,"""""""Performs transform from 3D coordinates to ANI symmetry functions"
2.8.0,Note
2.8.0,----
2.8.0,This class requires TensorFlow to be installed.
2.8.0,""""""""
2.8.0,"def __init__(self,"
2.8.0,"max_atoms=23,"
2.8.0,"radial_cutoff=4.6,"
2.8.0,"angular_cutoff=3.1,"
2.8.0,"radial_length=32,"
2.8.0,"angular_length=8,"
2.8.0,"atom_cases=[1, 6, 7, 8, 16],"
2.8.0,"atomic_number_differentiated=True,"
2.8.0,coordinates_in_bohr=True):
2.8.0,""""""""
2.8.0,Only X can be transformed
2.8.0,""""""""
2.8.0,import tensorflow as tf
2.8.0,self.max_atoms = max_atoms
2.8.0,self.radial_cutoff = radial_cutoff
2.8.0,self.angular_cutoff = angular_cutoff
2.8.0,self.radial_length = radial_length
2.8.0,self.angular_length = angular_length
2.8.0,self.atom_cases = atom_cases
2.8.0,self.atomic_number_differentiated = atomic_number_differentiated
2.8.0,self.coordinates_in_bohr = coordinates_in_bohr
2.8.0,self.compute_graph = self.build()
2.8.0,self.sess = tf.Session(graph=self.compute_graph)
2.8.0,self.transform_batch_size = 32
2.8.0,"super(ANITransformer, self).__init__(transform_X=True)"
2.8.0,"def transform_array(self, X, y, w):"
2.8.0,if self.transform_X:
2.8.0,X_out = []
2.8.0,num_transformed = 0
2.8.0,start = 0
2.8.0,batch_size = self.transform_batch_size
2.8.0,while True:
2.8.0,"end = min((start + 1) * batch_size, X.shape[0])"
2.8.0,X_batch = X[(start * batch_size):end]
2.8.0,output = self.sess.run(
2.8.0,"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
2.8.0,X_out.append(output)
2.8.0,num_transformed = num_transformed + X_batch.shape[0]
2.8.0,logger.info('%i samples transformed' % num_transformed)
2.8.0,start += 1
2.8.0,if end >= len(X):
2.8.0,break
2.8.0,"X_new = np.concatenate(X_out, axis=0)"
2.8.0,assert X_new.shape[0] == X.shape[0]
2.8.0,"return (X_new, y, w)"
2.8.0,"def untransform(self, z):"
2.8.0,raise NotImplementedError(
2.8.0,"""Cannot untransform datasets with ANITransformer."")"
2.8.0,def build(self):
2.8.0,""""""" tensorflow computation graph for transform """""""
2.8.0,import tensorflow as tf
2.8.0,graph = tf.Graph()
2.8.0,with graph.as_default():
2.8.0,self.inputs = tf.keras.Input(
2.8.0,"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
2.8.0,"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
2.8.0,flags = tf.sign(atom_numbers)
2.8.0,flags = tf.cast(
2.8.0,"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
2.8.0,"coordinates = self.inputs[:, :, 1:]"
2.8.0,if self.coordinates_in_bohr:
2.8.0,coordinates = coordinates * 0.52917721092
2.8.0,"d = self.distance_matrix(coordinates, flags)"
2.8.0,"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
2.8.0,"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
2.8.0,"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
2.8.0,"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
2.8.0,coordinates)
2.8.0,self.outputs = tf.concat(
2.8.0,[
2.8.0,"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
2.8.0,angular_sym
2.8.0,"],"
2.8.0,axis=2)
2.8.0,return graph
2.8.0,"def distance_matrix(self, coordinates, flags):"
2.8.0,""""""" Generate distance matrix """""""
2.8.0,import tensorflow as tf
2.8.0,max_atoms = self.max_atoms
2.8.0,"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
2.8.0,"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
2.8.0,# Calculate pairwise distance
2.8.0,"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
2.8.0,# Masking for valid atom index
2.8.0,d = d * flags
2.8.0,return d
2.8.0,"def distance_cutoff(self, d, cutoff, flags):"
2.8.0,""""""" Generate distance matrix with trainable cutoff """""""
2.8.0,import tensorflow as tf
2.8.0,# Cutoff with threshold Rc
2.8.0,d_flag = flags * tf.sign(cutoff - d)
2.8.0,d_flag = tf.nn.relu(d_flag)
2.8.0,d_flag = d_flag * tf.expand_dims(
2.8.0,"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
2.8.0,d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
2.8.0,return d * d_flag
2.8.0,"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
2.8.0,""""""" Radial Symmetry Function """""""
2.8.0,import tensorflow as tf
2.8.0,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.8.0,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.8.0,"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
2.8.0,ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
2.8.0,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
2.8.0,"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
2.8.0,length = ita.get_shape().as_list()[-1]
2.8.0,"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
2.8.0,"d = tf.stack([d] * length, axis=3)"
2.8.0,out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
2.8.0,if self.atomic_number_differentiated:
2.8.0,out_tensors = []
2.8.0,for atom_type in self.atom_cases:
2.8.0,selected_atoms = tf.expand_dims(
2.8.0,"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
2.8.0,axis=3)
2.8.0,"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
2.8.0,"return tf.concat(out_tensors, axis=2)"
2.8.0,else:
2.8.0,"return tf.reduce_sum(out, axis=2)"
2.8.0,"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
2.8.0,""""""" Angular Symmetry Function """""""
2.8.0,import tensorflow as tf
2.8.0,max_atoms = self.max_atoms
2.8.0,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.8.0,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.8.0,"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
2.8.0,ita = 3 / (Rs[1] - Rs[0])**2
2.8.0,"thetas = np.linspace(0., np.pi, self.angular_length)"
2.8.0,zeta = float(self.angular_length**2)
2.8.0,"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
2.8.0,"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0,"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0,"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0,length = zeta.get_shape().as_list()[-1]
2.8.0,"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
2.8.0,"[coordinates] * max_atoms, 2)"
2.8.0,"R_ij = tf.stack([d] * max_atoms, axis=3)"
2.8.0,"R_ik = tf.stack([d] * max_atoms, axis=2)"
2.8.0,"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
2.8.0,"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
2.8.0,# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.8.0,"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
2.8.0,"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
2.8.0,vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
2.8.0,"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
2.8.0,"R_ij = tf.stack([R_ij] * length, axis=4)"
2.8.0,"R_ik = tf.stack([R_ik] * length, axis=4)"
2.8.0,"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
2.8.0,"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
2.8.0,"theta = tf.stack([theta] * length, axis=4)"
2.8.0,"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
2.8.0,tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
2.8.0,if self.atomic_number_differentiated:
2.8.0,out_tensors = []
2.8.0,"for id_j, atom_type_j in enumerate(self.atom_cases):"
2.8.0,for atom_type_k in self.atom_cases[id_j:]:
2.8.0,"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
2.8.0,"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
2.8.0,selected_atoms = tf.expand_dims(
2.8.0,"tf.expand_dims(selected_atoms, axis=1), axis=4)"
2.8.0,out_tensors.append(
2.8.0,"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
2.8.0,"return tf.concat(out_tensors, axis=2)"
2.8.0,else:
2.8.0,"return tf.reduce_sum(out_tensor, axis=(2, 3))"
2.8.0,def get_num_feats(self):
2.8.0,n_feat = self.outputs.get_shape().as_list()[-1]
2.8.0,return n_feat
2.8.0,flake8: noqa
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a y transformer
2.8.0,Check w is unchanged since this is a y transformer
2.8.0,Check y is now a logarithmic version of itself
2.8.0,Check that untransform does the right thing.
2.8.0,Check ids are unchanged.
2.8.0,Check y is unchanged since this is a X transformer
2.8.0,Check w is unchanged since this is a y transformer
2.8.0,Check y is now a logarithmic version of itself
2.8.0,Check that untransform does the right thing.
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a y transformer
2.8.0,Check w is unchanged since this is a y transformer
2.8.0,Check y is now a logarithmic version of itself
2.8.0,Check that untransform does the right thing.
2.8.0,Tests logarithmic data transformer with selection.
2.8.0,Check ids are unchanged.
2.8.0,Check y is unchanged since this is a X transformer
2.8.0,Check w is unchanged since this is a y transformer
2.8.0,Check y is now a logarithmic version of itself
2.8.0,Check that untransform does the right thing.
2.8.0,Check ids are unchanged.
2.8.0,Check y is unchanged since this is an X transformer
2.8.0,Check w is unchanged since this is an X transformer
2.8.0,Check X is now holding the proper values when sorted.
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is an y transformer
2.8.0,Check w is unchanged since this is an y transformer
2.8.0,Check y is now holding the proper values when sorted.
2.8.0,Check that untransform does the right thing.
2.8.0,Check ids are unchanged.
2.8.0,Check y is unchanged since this is an X transformer
2.8.0,Check w is unchanged since this is an X transformer
2.8.0,Check X is now holding the proper values when sorted.
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a y transformer
2.8.0,Check w is unchanged since this is a y transformer
2.8.0,Check y is now holding the proper values when sorted.
2.8.0,Check ids are unchanged before and after transformation
2.8.0,Check X is unchanged since transform_y is true
2.8.0,Check w is unchanged since transform_y is true
2.8.0,Check minimum and maximum values of transformed y are 0 and 1
2.8.0,Check untransform works correctly
2.8.0,Check ids are unchanged before and after transformation
2.8.0,Check X is unchanged since transform_y is true
2.8.0,Check w is unchanged since transform_y is true
2.8.0,Check minimum and maximum values of transformed y are 0 and 1
2.8.0,Test if dimensionality expansion is handled correctly by untransform
2.8.0,Check ids are unchanged before and after transformation
2.8.0,Check X is unchanged since transform_y is true
2.8.0,Check w is unchanged since transform_y is true
2.8.0,Check minimum and maximum values of transformed y are 0 and 1
2.8.0,Check untransform works correctly
2.8.0,Load mini log-solubility dataset.
2.8.0,The transformer generates n DAGs for a molecule with n
2.8.0,"atoms. These are denoted the ""parents"""
2.8.0,extract only the images (no need of the labels)
2.8.0,reshaping the vector to image
2.8.0,Check Blurring
2.8.0,Check center crop
2.8.0,Check crop
2.8.0,Check convert2gray
2.8.0,Check rotation
2.8.0,Some more test cases for flip
2.8.0,Check flip
2.8.0,Check Scales
2.8.0,Check shift
2.8.0,check gaussian noise
2.8.0,check salt and pepper noise
2.8.0,Check median filter
2.8.0,transforming y should raise an exception
2.8.0,transforming w should raise an exception
2.8.0,transforming X should be okay
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a y transformer
2.8.0,Check w is unchanged since this is a y transformer
2.8.0,"Check that y_t has zero mean, unit std."
2.8.0,Check that untransform does the right thing.
2.8.0,Check ids are unchanged.
2.8.0,Check y is unchanged since this is a X transformer
2.8.0,Check w is unchanged since this is a y transformer
2.8.0,"Check that X_t has zero mean, unit std."
2.8.0,np.set_printoptions(threshold='nan')
2.8.0,Entries with zero std are not normalized
2.8.0,Check that untransform does the right thing.
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a w transformer
2.8.0,Check y is unchanged since this is a w transformer
2.8.0,Assert that entries with zero weight retain zero weight
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a w transformer
2.8.0,Check y is unchanged since this is a w transformer
2.8.0,Assert that entries with zero weight retain zero weight
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a w transformer
2.8.0,Check y is unchanged since this is a w transformer
2.8.0,Assert that entries with zero weight retain zero weight
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a w transformer
2.8.0,Check y is unchanged since this is a w transformer
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is a w transformer
2.8.0,Check y is unchanged since this is a w transformer
2.8.0,Assert that entries with zero weight retain zero weight
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,Check ids are unchanged.
2.8.0,Check y is unchanged since this is an X transformer
2.8.0,Check w is unchanged since this is an X transformer
2.8.0,Check X is now holding the proper values in each column.
2.8.0,Check ids are unchanged.
2.8.0,Check X is unchanged since this is an X transformer
2.8.0,Check w is unchanged since this is an X transformer
2.8.0,Check y is now holding the proper values in each column.
2.8.0,Check that untransform does the right thing.
2.8.0,Check that we have length 8 now with duplication
2.8.0,Check shapes
2.8.0,Check that we have 4 positives and 4 negatives
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,Note that nothing should change in this dataset since weights balance!
2.8.0,Check that still we have length 6
2.8.0,Check shapes
2.8.0,Check that we have 2 positives and 4 negatives
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,Check that we have length 8 now with duplication
2.8.0,Check shapes
2.8.0,Check that we have 4 positives and 4 negatives
2.8.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0,6-1 imbalance in favor of class 0
2.8.0,Check that we have length 30 now with duplication
2.8.0,Check shapes
2.8.0,Check that we have 6 of each class
2.8.0,Check that sum of all class weights is equal by comparing to 0 weight
2.8.0,Note class imbalance. This will round to 2x duplication for 1
2.8.0,Check that we have length 13 now with duplication
2.8.0,Check shapes
2.8.0,Check that we have 6 positives and 7 negatives
2.8.0,safe operations
2.8.0,occupation number gradients
2.8.0,get the length of the tensor output
2.8.0,other tensor ops
2.8.0,add the diagonal with a small eps to safeguard from nan
2.8.0,replace the diagonal with infinite (usually used for coulomb matrix)
2.8.0,################################################################
2.8.0,save.py is out of date. You should not import any functions from here.
2.8.0,################################################################
2.8.0,flake8: noqa
2.8.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.8.0,Walk through the original file and extract ATOM/HETATM lines and
2.8.0,add PDBQT charge annotations.
2.8.0,Remove rotatable bonds from this molecule
2.8.0,Get the connected components now that the rotatable bonds have
2.8.0,been removed.
2.8.0,The root is the largest connected component.
2.8.0,Write the root component
2.8.0,"We've looked at the root, so take note of that"
2.8.0,Compute partial charges on molecule if RDKit Mol
2.8.0,indices to atoms to keep
2.8.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.8.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.8.0,nonzero contact.
2.8.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.8.0,TODO: This is duplicated! Clean up
2.8.0,Updates charges in place
2.8.0,data.shape and segment_ids.shape should be equal
2.8.0,"return V.set_(V.storage(), V.storage_offset(), V.size(), tuple(reversed(V.stride())))"
2.8.0,initial embedding
2.8.0,minimization and pruning
2.8.0,always keep lowest-energy conformer
2.8.0,discard conformers after max_conformers is reached
2.8.0,get RMSD to selected conformers
2.8.0,discard conformers within the RMSD threshold
2.8.0,create a new molecule to hold the chosen conformers
2.8.0,this ensures proper conformer IDs and energy-based ordering
2.8.0,isotope-averaged atom masses in a.m.u.
2.8.0,from https://www.angelo.edu/faculty/kboudrea/periodic/structure_mass.htm
2.8.0,"JCP 41, 3199 (1964); DOI:10.1063/1.1725697"
2.8.0,taken from PySCF:
2.8.0,https://github.com/pyscf/pyscf/blob/45582e915e91890722fcae2bc30fb04867d5c95f/pyscf/data/radii.py#L23
2.8.0,I don't know why H has 0.35 while in the reference it is 0.
2.8.0,"They are in angstrom, so we need to convert it to Bohr"
2.8.0,False here specifies that water is to be removed
2.8.0,Updates charges in place
2.8.0,TODO: This is wrong. Should return all molecules
2.8.0,TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.8.0,This updates in place
2.8.0,indices of atoms to keep
2.8.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.8.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.8.0,nonzero contact.
2.8.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.8.0,####################################################
2.8.0,Compute partial charges on molecule if rdkit
2.8.0,####################################################
2.8.0,Number of voxels per one edge of box to voxelize.
2.8.0,NOTE We have lot of type ignores here since grover mol-graph which is of type
2.8.0,"GraphData have kwargs which are not attributes of GraphData. Hence, in these"
2.8.0,cases mypy raises GraphData does not have attributes `..`.
2.8.0,max with 1 to fix a crash in rare case of all single-heavy-atom mols
2.8.0,graph_index indicates which atom belongs to which molecule
2.8.0,padding
2.8.0,computing a2b
2.8.0,only needed if using atom messages
2.8.0,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.8.0,If interval1 < interval2 entirely
2.8.0,If interval2 < interval1 entirely
2.8.0,Each triangle in the simplices is a set of 3 atoms from
2.8.0,coordinates which forms the vertices of an exterior triangle on
2.8.0,the convex hull of the macromolecule.
2.8.0,Points is the set of atom coordinates that make up this
2.8.0,triangular face on the convex hull
2.8.0,Let's extract x/y/z coords for this face
2.8.0,Let's compute min/max points
2.8.0,"Nitrogen has atomic number 7, and oxygen 8."
2.8.0,If atom is a hydrogen
2.8.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.8.0,If atom is a hydrogen
2.8.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.8.0,if ring from mol1 is aromatic
2.8.0,...and atom from mol2 is a cation
2.8.0,if angle and distance are correct
2.8.0,count atoms forming a contact
2.8.0,if ring is aromatic
2.8.0,"save its indices, center, and normal"
2.8.0,remember mol1-mol2 pairs we already counted
2.8.0,"if this pair is new, count atoms forming a contact"
2.8.0,"if this pair is new, count atoms forming a contact"
2.8.0,find interacting rings from mol1 and cations from mol2
2.8.0,find interacting cations from mol1 and rings from mol2
2.8.0,merge counters
2.8.0,the line has format
2.8.0,REMARK VINA RESULT: score ...
2.8.0,There is only 1 such line per model so we can append it
2.8.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.8.0,Apply common fixes to PDB files
2.8.0,Optimize ligand
2.8.0,"For a node-prediction task, label is not added to edge features and other global features"
2.8.0,because label here is a node-level attribute and not a graph-level attribute
2.8.0,"In this case, the 'y' attribute of GraphData will contain the"
2.8.0,node-level labels.
2.8.0,not a self-loop
2.8.0,Make sure input is a list
2.8.0,FIXME: Incompatible types in assignment
2.8.0,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.8.0,Ensure that metric is wrapped in a list.
2.8.0,This case checks if input is a function then wraps a
2.8.0,dc.metrics.Metric object around it
2.8.0,Process input metrics
2.8.0,Compute multitask metrics
2.8.0,We use y/w to aggregate labels/weights across generator.
2.8.0,This is a KerasModel.
2.8.0,Some datasets have weights
2.8.0,Process predictions and populate y/w lists
2.8.0,Combine labels/weights
2.8.0,Undo data transformations.
2.8.0,Compute multitask metrics
2.8.0,for each node (E[(X-E[X])^n])^{1/n}
2.8.0,EPS is added to the absolute value of expectation before taking the nth root for stability
2.8.0,"each scaler is a function that takes as input X (B x N x Din), adj (B x N x N) and"
2.8.0,avg_d (dictionary containing averages over training set) and returns X_scaled (B x N x Din) as output
2.8.0,Generate the raising operator matrix
2.8.0,Generate the lowering operator matrix
2.8.0,Generate the z-generator matrix
2.8.0,"Combine the matrices to form the x, z, and y generators"
2.8.0,Stack the generators along the first dimension to create a tensor
2.8.0,The transformation matrix generated is used to change the basis of a vector of
2.8.0,real spherical harmonics with representation index 1 to complex spherical harmonics.
2.8.0,"Construct the transformation matrix Q for m in range(-k, 0)"
2.8.0,Set the diagonal elements for m = 0
2.8.0,"Construct the transformation matrix Q for m in range(1, k + 1)"
2.8.0,Apply the factor of (-1j)**k to make the Clebsch-Gordan coefficients real
2.8.0,Handle dtype and device options
2.8.0,Ensure the tensor is contiguous and on the specified device
2.8.0,Get the SU(2) generators for the given quantum angular momentum (spin) value.
2.8.0,Get the transformation matrix to change the basis from real to complex spherical harmonics.
2.8.0,Convert the SU(2) generators to the SO(3) basis using the transformation matrix Q.
2.8.0,"X represents the SU(2) generators, and Q is the transformation matrix from real to complex spherical harmonics."
2.8.0,The resulting X matrix will be the SO(3) generators in the complex basis.
2.8.0,Return the real part of the SO(3) generators to ensure they are purely real.
2.8.0,"Ensure that alpha, beta, and gamma have the same shape for broadcasting."
2.8.0,"Ensure the angles are within the range [0, 2*pi) using modulo."
2.8.0,Get the SO(3) generators for the given quantum angular momentum (spin) value 'k'.
2.8.0,Calculate the Wigner D matrix using the matrix exponential of the generators
2.8.0,"and the rotation angles alpha, beta, and gamma in the appropriate order."
2.8.0,These functions have moved to deepchem.utils_docking_utils
2.8.0,flake8: noqa
2.8.0,The number of elements to print for dataset ids/tasks
2.8.0,"If a dataset contains more than this number of elements, it won't"
2.8.0,print any dataset ids
2.8.0,Warnings
2.8.0,An activation function for a layer: either a function or the name of a standard activation
2.8.0,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.8.0,"A single value of some type, or multiple values of that type"
2.8.0,The shape of a NumPy array
2.8.0,"A NumPy array, or an object that can be converted to one.  Once we move to"
2.8.0,"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
2.8.0,type of RDKit object
2.8.0,type of Pymatgen object
2.8.0,Calculation of Step Size and steps
2.8.0,Number of atoms per molecule is calculated by counting all the non zero elements(numbers) of every molecule.
2.8.0,"It loops over the molecules in the Coulomb matrix and takes the ""2.4"" root of the diagonal of ""2X"" of each molecule's representation."
2.8.0,Calculates the Gaussian Distance by passing distance by a gaussian function.
2.8.0,Generate a random temporary file name
2.8.0,Ensure the file is created
2.8.0,Open the file in the given mode
2.8.0,dtype=object allows for arrays(images here) of arbitrary size
2.8.0,Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
2.8.0,Structures are stored in .sdf file
2.8.0,"Note: Here, the order of columns is based on the order in which the values"
2.8.0,"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
2.8.0,"tasks above, they occur after `tasks` here."
2.8.0,"FIXME Ideally, we should use something like a dictionary here to keep it independent"
2.8.0,of column ordering.
2.8.0,Reset aggregator
2.8.0,Handle final leftovers for this file
2.8.0,First line of user-specified CSV *must* be header.
2.8.0,"If gzipped, need to compute extension again"
2.8.0,First line of user-specified CSV *must* be header.
2.8.0,The label encoder is given characters for ACGTN
2.8.0,Peak at the first sequence to get the length of the sequence.
2.8.0,"pattern to split the names, e.g. ""model.params[1]"" into [""model"", ""params"", ""[1]""]"
2.8.0,"return: (nao, nao)"
2.8.0,init an one-hot vector
2.8.0,"If include_unknown_set is True, set the last index is 1."
2.8.0,################################################################
2.8.0,atom (node) featurization
2.8.0,################################################################
2.8.0,################################################################
2.8.0,bond (edge) featurization
2.8.0,################################################################
2.8.0,get the unique parameters of A
2.8.0,separate the parameters for A and for M
2.8.0,"grad_x: (*BABEM, nr, ncols)"
2.8.0,"x: (*BABEM, nr, ncols)"
2.8.0,solve (A-biases*M)^T v = grad_x
2.8.0,this is the grad of B
2.8.0,calculate the grad of matrices parameters
2.8.0,calculate the biases gradient
2.8.0,calculate the gradient to the biases matrices
2.8.0,Hidden
2.8.0,"NOTE: currently only works for batched B (1 batch dim), but unbatched A"
2.8.0,check the parameters
2.8.0,convert the numpy/scipy
2.8.0,NOTE: The line below is very inefficient for large na and ncols
2.8.0,"if B is all zeros, then return zeros"
2.8.0,setup the preconditioning and the matrix problem
2.8.0,get the stopping matrix
2.8.0,prepare the initial guess (it's just all zeros)
2.8.0,correct the residual calculation
2.8.0,check for the stopping condition
2.8.0,move to the next index
2.8.0,"x: (ncols, *, nr, 1)"
2.8.0,"if B is all zeros, then return zeros"
2.8.0,setup the preconditioning and the matrix problem
2.8.0,get the stopping matrix
2.8.0,prepare the initial guess (it's just all zeros)
2.8.0,correct the residual calculation regularly
2.8.0,calculate the residual
2.8.0,save the best results
2.8.0,check for the stopping conditions
2.8.0,"x: (ncols, *, nr, 1)"
2.8.0,"if B is all zeros, then return zeros"
2.8.0,setup the preconditioning and the matrix problem
2.8.0,get the stopping matrix
2.8.0,prepare the initial guess (it's just all zeros)
2.8.0,res = res * B_norm
2.8.0,save the best results
2.8.0,general helpers
2.8.0,get the linear operator (including the MXE part)
2.8.0,"A: (*BA, nr, nr) linop"
2.8.0,"B: (*BB, nr, ncols)"
2.8.0,"E: (*BE, ncols)"
2.8.0,"M: (*BM, nr, nr) linop"
2.8.0,"x: (ncols, *BX, nr, 1)"
2.8.0,"x: (ncols, *BX, nr, 1)"
2.8.0,estimate if it's posdef with power iteration
2.8.0,set posdef to False to make the operator becomes AT * A so it is
2.8.0,hermitian
2.8.0,TODO: the posdef check by largest eival only works for Hermitian/symmetric
2.8.0,"matrix, but it doesn't always work for non-symmetric matrix."
2.8.0,"In non-symmetric case, one need to do Cholesky LDL decomposition"
2.8.0,"if the largest eigenvalue is negative, then it's not posdef"
2.8.0,"otherwise, calculate the lowest eigenvalue to check if it's positive"
2.8.0,get the linear operation if it is not a posdef (A -> AT.A)
2.8.0,cg and bicgstab helpers
2.8.0,rootfinder-based
2.8.0,using rootfinder algorithm
2.8.0,set up the function for the rootfinding
2.8.0,"xi: (*BX, nr*ncols)"
2.8.0,setup the initial guess (the batch dimension must be the largest)
2.8.0,check if xnorm is converging
2.8.0,Broadcast Utilities
2.8.0,check the hermitian
2.8.0,check which methods are implemented
2.8.0,@abstractmethod
2.8.0,@abstractmethod # (optional)
2.8.0,@abstractmethod
2.8.0,@abstractmethod
2.8.0,implemented functions
2.8.0,use batched mv as mm
2.8.0,move the last dimension to the very first dimension to be broadcasted
2.8.0,apply batched mv and restore the initial shape
2.8.0,use batched mv as mm
2.8.0,move the last dimension to the very first dimension to be broadcasted
2.8.0,apply batched mv and restore the initial shape
2.8.0,special functions
2.8.0,properties
2.8.0,implementation
2.8.0,private functions
2.8.0,"xt: (*BY, p)"
2.8.0,"xdummy: (*BY, q)"
2.8.0,calculate y = Ax
2.8.0,calculate (dL/dx)^T = A^T (dL/dy)^T with (dL/dy)^T = xt
2.8.0,Helper Classes
2.8.0,"if it is a method from an object, unroll the parameters and add"
2.8.0,the object's parameters as well
2.8.0,get the unique ids
2.8.0,search the id if it has been added to the list
2.8.0,debugging
2.8.0,check the method input
2.8.0,assert if the method preserve the float tensors of the object
2.8.0,now assert if all_params0 == all_params1
2.8.0,get all tensor parameters in the object
2.8.0,get the parameter tensors used in the operation and the tensors specified by the developer
2.8.0,check if the userparams contains non-tensor
2.8.0,"check if there are missing parameters (present in operating params, but not in the user params)"
2.8.0,if oper_names[i] not in user_names:
2.8.0,"if there are missing parameters, give a warning (because the program"
2.8.0,"can still run correctly, e.g. missing parameters are parameters that"
2.8.0,are never set to require grad)
2.8.0,"check if there are excessive parameters (present in the user params, but not in the operating params)"
2.8.0,if user_names[i] not in oper_names:
2.8.0,"if there are excess parameters, give warnings"
2.8.0,get all the tensors recursively
2.8.0,copy the tensors and require them to be differentiable
2.8.0,run the method and see which one has the gradients
2.8.0,return the original tensor
2.8.0,traversing functions
2.8.0,None is set as default arg to avoid expanding list for multiple
2.8.0,invokes of _get_tensors without exception_ids argument
2.8.0,add exception to avoid infinite loop if there is a mutual dependant on objects
2.8.0,get the tensors recursively towards torch.nn.Module
2.8.0,traverse down the object to collect the tensors
2.8.0,traverse down the object to collect the tensors
2.8.0,flake8: noqa
2.8.0,TODO: implement robust LOBPCG and put it here
2.8.0,get the unique parameters of A & M
2.8.0,adapted from scipy.sparse.linalg.svds
2.8.0,clamp the eigenvalues to a small positive values to avoid numerical
2.8.0,instability
2.8.0,separate the sets of parameters
2.8.0,options for calculating the backward (not for `solve`)
2.8.0,save for the backward
2.8.0,"evals: (*BAM, neig)"
2.8.0,"evecs: (*BAM, na, neig)"
2.8.0,get the variables from ctx
2.8.0,set the default values of degen_*tol
2.8.0,check the degeneracy
2.8.0,"idx_degen: (*BAM, neig, neig)"
2.8.0,the loss function where the gradient will be retrieved
2.8.0,"warnings: if not all params have the connection to the output of A,"
2.8.0,it could cause an infinite loop because pytorch will keep looking
2.8.0,for the *params node and propagate further backward via the `evecs`
2.8.0,path. So make sure all the *params are all connected in the graph.
2.8.0,"if degenerate, check the conditions for finite derivative"
2.8.0,"if the requirements are not satisfied, raises a warning"
2.8.0,calculate the contributions from the eigenvalues
2.8.0,calculate the contributions from the eigenvectors
2.8.0,orthogonalize the grad_evecs with evecs
2.8.0,"Based on test cases, complex datatype is more likely to suffer from"
2.8.0,"singularity error when doing the inverse. Therefore, I add a small"
2.8.0,offset here to prevent that from happening
2.8.0,orthogonalize gevecs w.r.t. evecs
2.8.0,accummulate the gradient contributions
2.8.0,the contribution from the parallel elements
2.8.0,"evals: (*BAM, neig)"
2.8.0,get the index of degeneracies
2.8.0,contracted using opt_einsum
2.8.0,"evals, evecs = torch.linalg.eigh(Amatrix, eigenvectors=True)  # (*BA, q), (*BA, q, q)"
2.8.0,M decomposition to make A symmetric
2.8.0,it is done this way to make it numerically stable in avoiding
2.8.0,complex eigenvalues for (near-)degenerate case
2.8.0,calculate the eigenvalues and eigenvectors
2.8.0,(the eigvecs are normalized in M-space)
2.8.0,"evals, evecs = torch.linalg.eigh(A2, eigenvectors=True)  # (*BAM, q, q)"
2.8.0,temporary solution to https://github.com/pytorch/pytorch/issues/47599
2.8.0,remove the degenerate part
2.8.0,see https://arxiv.org/pdf/2011.04366.pdf
2.8.0,take the contribution from the eivec
2.8.0,calculate the contribution from the eival
2.8.0,symmetrize to reduce numerical instability
2.8.0,TODO: optimize for large linear operator and strict min_eps
2.8.0,Ideas:
2.8.0,(1) use better strategy to get the estimate on eigenvalues
2.8.0,(2) use restart strategy
2.8.0,get the shape of the transformation
2.8.0,set up the initial guess
2.8.0,Can be optimized by saving AV from the previous iteration and only
2.8.0,operate AV for the new V. This works because the old V has already
2.8.0,"been orthogonalized, so it will stay the same"
2.8.0,"AV = A.mm(V) # (*BAM,na,nguess)"
2.8.0,eigvals are sorted from the lowest
2.8.0,"eval: (*BAM, nguess), evec: (*BAM, nguess, nguess)"
2.8.0,calculate the eigenvectors of A
2.8.0,calculate the residual
2.8.0,print information and check convergence
2.8.0,apply the preconditioner
2.8.0,orthogonalize t with the rest of the V
2.8.0,orthogonalize V
2.8.0,check idxs
2.8.0,make the function a functional (depends on all parameters in the object)
2.8.0,params tensor is the LinearOperator's parameters
2.8.0,"if the object parameter is still the same, then use the pre-calculated values"
2.8.0,"otherwise, reevaluate by replacing the parameters with the new tensor params"
2.8.0,self.yfcn: (*nin)
2.8.0,file mostly from SciPy:
2.8.0,https://github.com/scipy/scipy/blob/914523af3bc03fe7bf61f621363fca27e97ca1d6/scipy/optimize/nonlin.py#L221
2.8.0,and converted to PyTorch for GPU efficiency
2.8.0,jacobian parameters
2.8.0,stopping criteria
2.8.0,algorithm parameters
2.8.0,misc parameters
2.8.0,"solving complex rootfinder by concatenating real and imaginary part,"
2.8.0,making the variable twice as long
2.8.0,represents x as a long real vector
2.8.0,pack a long real vector into the shape accepted by fcn
2.8.0,shorthand for the function
2.8.0,set up the jacobian
2.8.0,solver tolerance
2.8.0,save the best results
2.8.0,print out dx and df
2.8.0,adjust forcing parameters for inexact solve
2.8.0,jacobian parameters
2.8.0,stopping criteria
2.8.0,algorithm parameters
2.8.0,misc parameters
2.8.0,"No suitable step length found. Take the full Newton step,"
2.8.0,and hope for the best.
2.8.0,"Otherwise, compute the minimizer of a quadratic interpolant:"
2.8.0,"Otherwise, loop with cubic interpolation until we find an alpha which"
2.8.0,"satisfies the first Wolfe condition (since we are backtracking, we will"
2.8.0,assume that the value of alpha is not too small and satisfies the second
2.8.0,condition.
2.8.0,Failed to find a suitable step length
2.8.0,gd parameters
2.8.0,stopping conditions
2.8.0,misc parameters
2.8.0,update the step
2.8.0,check the stopping conditions
2.8.0,gd parameters
2.8.0,stopping conditions
2.8.0,misc parameters
2.8.0,update the step
2.8.0,check the stopping conditions
2.8.0,get the best values
2.8.0,usually user set maxiter == 0 just to wrap the minimizer backprop
2.8.0,taking most of the part from SciPy
2.8.0,setup the approximate inverse Jacobian
2.8.0,update Gm
2.8.0,keep the rank small
2.8.0,keep the rank small
2.8.0,raise RuntimeError
2.8.0,"u: (n, 1), s: (1,), vh: (1, n)"
2.8.0,"equilibrium can use all rootfinder methods, but there are several methods developed specifically for"
2.8.0,equilibrium (or fixed-point iterations). This dictionary gives the list of those special methods.
2.8.0,"minimization can use rootfinder algorithm, so check if it is actually"
2.8.0,"using the optimization algorithm, not the rootfinder algorithm"
2.8.0,the rootfinder algorithms are designed to move to the opposite direction
2.8.0,"of the output of the function, so the output of this function is just"
2.8.0,the grad of z w.r.t. y
2.8.0,"if it is going to optimization method, then also returns the value"
2.8.0,"if using the optimization algorithm, then the forward function is the one"
2.8.0,that returns f and grad
2.8.0,"if it is just using the rootfinder algorithm, then the forward function"
2.8.0,is the one that returns only the grad
2.8.0,set default options
2.8.0,split tensors and non-tensors params
2.8.0,merge the tensor and nontensor parameters
2.8.0,dL/df
2.8.0,get the grad for the params
2.8.0,anderson_acc parameters
2.8.0,stopping criteria
2.8.0,misc options
2.8.0,"x0: (..., *nfeats)"
2.8.0,"reshape x to have a shape of (batch_size, feats_dim)"
2.8.0,"x: (..., *nfeats)"
2.8.0,"xn: (..., feats_tot)"
2.8.0,"x: (..., feats_dim)"
2.8.0,"torch.bmm(g, g.transpose(-2, -1))"
2.8.0,"alpha: (batch_size, nsize)"
2.8.0,check the stopping condition
2.8.0,update the xn
2.8.0,One sequence has length longer than others. This should throw a
2.8.0,ValueError.
2.8.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.8.0,Loosening atol to see if tests stop failing sporadically
2.8.0,string set
2.8.0,integer set
2.8.0,include_unknown_set is False
2.8.0,include_unknown_set is True
2.8.0,check unknown atoms
2.8.0,check original set
2.8.0,"Generally, =O behaves as an electron acceptor"
2.8.0,we must compute partial charges before using `get_atom_partial_charge`
2.8.0,The C-N bond is a single bond
2.8.0,"6 atoms: CC -> 2, CCC -> 3"
2.8.0,"7 bonds: CC -> 2, CCC -> 4 (bonds are considered as undirected"
2.8.0,and a single bond contributes to 2 bonds)
2.8.0,graph-level labels
2.8.0,node-level labels
2.8.0,graph.y contains node-labels and graph.node_features.shape[0]
2.8.0,holds number of nodes in that graph
2.8.0,Get Data
2.8.0,Checks that all atoms exits in array
2.8.0,Checks shape of gaussian distance
2.8.0,Checks all molecule membership exist
2.8.0,Check Distance Membership shape
2.8.0,Prepare Data
2.8.0,Run
2.8.0,Prepare Data
2.8.0,Inputs property
2.8.0,Without reverse input
2.8.0,With revercse input
2.8.0,Prepare Data
2.8.0,Inputs property
2.8.0,TODO test more formats for ligand
2.8.0,Test if the output has the correct shape.
2.8.0,Test for the case of zero momentum (j=0).
2.8.0,Test for the case of momentum j=1 (spin-1).
2.8.0,"Expected J_x, J_z, J_y matrices for j=1"
2.8.0,"Test for j = 0, which means we have a 1x1 transformation matrix"
2.8.0,"Test for j = 2, which means we have a 5x5 transformation matrix"
2.8.0,Test for device placement (CPU to CUDA)
2.8.0,Test for dtype conversion (complex128 to complex64)
2.8.0,TODO test more formats for ligand
2.8.0,adding hydrogens and charges is tested in dc.utils
2.8.0,self.ligand_file is for 3ws9_ligand.sdf
2.8.0,dummy function which can be passed as the parameter f to simultaneous_move and single_move
2.8.0,test for gauss_initialize_position
2.8.0,testing symmetric simultaneous_move
2.8.0,testing asymmetric simultaneous_move
2.8.0,testing symmetric single_move
2.8.0,testing asymmetric single_move
2.8.0,simple flat ring
2.8.0,self.cycle4.Compute2DCoords()
2.8.0,load and sanitize two real molecules
2.8.0,parallel normals
2.8.0,perpendicular normals
2.8.0,too far away
2.8.0,perpendicular normals
2.8.0,parallel normals
2.8.0,too far away
2.8.0,order of the molecules shouldn't matter
2.8.0,with this criteria we should find both types of stacking
2.8.0,parallel normals
2.8.0,perpendicular normals
2.8.0,too far away
2.8.0,def test_compute_cation_pi(self):
2.8.0,"# TODO(rbharath): find better example, currently dicts are empty"
2.8.0,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.8.0,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.8.0,"TODO find better example, currently dicts are empty"
2.8.0,TODO test more formats for ligand
2.8.0,Test on RDKit
2.8.0,3D vector with unit length
2.8.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.8.0,"random coords between 0 and 1, so the max possible distance in sqrt(3)"
2.8.0,check if correct distance metric was used
2.8.0,Construct a random class probability matrix
2.8.0,Construct a random class probability matrix
2.8.0,"Note that since no name as provided, metrics are index by order"
2.8.0,given.
2.8.0,"Note that since no name as provided, metrics are index by order"
2.8.0,given.
2.8.0,"Note that since no name as provided, metrics are index by order"
2.8.0,given.
2.8.0,"Note that since no name as provided, metrics are index by order"
2.8.0,given.
2.8.0,TODO: Fix this case with correct thresholding
2.8.0,TODO: Fix this case with correct thresholding
2.8.0,There are 4 faces to the shape created by coords
2.8.0,flake8: noqa
2.8.0,get the location and weights of the integration in its original
2.8.0,coordinate
2.8.0,get the coordinate in Cartesian
2.8.0,integration element
2.8.0,Grid Transformations
2.8.0,"xnew is from [xmin, xmax]"
2.8.0,"r is approximately from [rmin, rmax]"
2.8.0,type of the atom Z
2.8.0,input types
2.8.0,"if the basis has been normalized before, then do nothing"
2.8.0,normalize to have individual gaussian integral to be 1 (if coeff is 1)
2.8.0,normalize the coefficients in the basis (because some basis such as
2.8.0,def2-svp-jkfit is not normalized to have 1 in overlap)
2.8.0,input basis type
2.8.0,QR decomposition's solution is not unique in a way that every column
2.8.0,can be multiplied by -1 and it still a solution
2.8.0,"So, to remove the non-uniqueness, we will make the sign of the sum"
2.8.0,positive.
2.8.0,construct the rotation parameters
2.8.0,calculate the orthogonal orbital
2.8.0,"orb: (*, nao, norb)"
2.8.0,the orbital becomes the coefficients while params is all zeros (no rotation)
2.8.0,properties
2.8.0,setups
2.8.0,fock matrix components
2.8.0,interface to dm
2.8.0,energy of the Hamiltonian
2.8.0,free parameters for variational method
2.8.0,Editable module
2.8.0,1 / cell.vol == det(b) / (2 pi)^3
2.8.0,drop ls that has norm > rcut * 1.05
2.8.0,System Properties
2.8.0,all-time calculations
2.8.0,(i.e. meaning it does not have to be executed to run the functions below)
2.8.0,"densinfo.value & lapl: (*BD, nr)"
2.8.0,"densinfo.grad: (*BD, ndim, nr)"
2.8.0,"return: (*BD, nr)"
2.8.0,"densinfo.value & lapl: (*BD, nr)"
2.8.0,"densinfo.grad: (*BD, ndim, nr)"
2.8.0,return:
2.8.0,"potentialinfo.value & lapl: (*BD, nr)"
2.8.0,"potentialinfo.grad: (*BD, ndim, nr)"
2.8.0,mark the densinfo components as requiring grads
2.8.0,"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
2.8.0,"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
2.8.0,set the vars to require grad and returns the previous state of the vars
2.8.0,restore the state of requiring grad based on reqgrads list
2.8.0,all vars before this function requires grad
2.8.0,getting which parameters should require grad
2.8.0,set the params to require grad
2.8.0,special operations
2.8.0,properties
2.8.0,TODO: use regex!
2.8.0,convert the atomz to tensor
2.8.0,convert the atompos to tensor
2.8.0,"convert to dtype if atomzs is a floating point tensor, not an integer tensor"
2.8.0,flake8: noqa
2.8.0,Get the degree id list (which corrects for min_deg)
2.8.0,Get the size of each degree block
2.8.0,Get the the start indices for items in each block
2.8.0,Get the node indices when they are reset when the degree changes
2.8.0,Convert to numpy array
2.8.0,Reorder old atom_features
2.8.0,Reorder old deg lists
2.8.0,Sort membership
2.8.0,Create old to new dictionary. not exactly intuitive
2.8.0,Reorder adjacency lists
2.8.0,Get numpy version of degree list for indexing
2.8.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.8.0,Parse as deg separated
2.8.0,Get indices corresponding to the current degree
2.8.0,Extract and save adjacency list for the current degree
2.8.0,Construct the slice information
2.8.0,Get the cumulative indices after the first index
2.8.0,Set indices with zero sized slices to zero to avoid indexing errors
2.8.0,TODO(rbharath): Can this be removed?
2.8.0,Use random insted of zeros to prevent weird issues with summing to zero
2.8.0,"Combine the features, then sort them by (atom_degree, mol_index)"
2.8.0,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.8.0,Create a map from the original atom indices within each molecule to the
2.8.0,indices in the combined object.
2.8.0,Sort all atoms by degree.
2.8.0,"Get the size of each atom list separated by molecule id, then by degree"
2.8.0,Get the final size of each degree block
2.8.0,"Get the index at which each degree starts, not resetting after each degree"
2.8.0,And not stopping at any specific molecule
2.8.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.8.0,first column telling the start indices of each degree block and the
2.8.0,second colum telling the size of each degree block
2.8.0,Determine the membership (atom i belongs to molecule membership[i])
2.8.0,Initialize the new degree separated adjacency lists
2.8.0,Update the old adjacency lists with the new atom indices and then combine
2.8.0,all together
2.8.0,Iterate through all the molecules
2.8.0,Get the adjacency lists for this molecule and current degree id
2.8.0,"Correct all atom indices to the final indices, and then save the"
2.8.0,results into the new adjacency lists
2.8.0,Increment once row is done
2.8.0,Get the final aggregated molecule
2.8.0,Break the loop if max_records is set
2.8.0,Break the loop if max_records is set
2.8.0,Break the loop if max_records is set
2.8.0,"Requriments - transformers, tokenizers"
2.8.0,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.8.0,The vocab may be expanded in the near future
2.8.0,add vocab_file dict
2.8.0,"unk_token=""[UNK]"","
2.8.0,"sep_token=""[SEP]"","
2.8.0,"pad_token=""[PAD]"","
2.8.0,"cls_token=""[CLS]"","
2.8.0,"mask_token=""[MASK]"","
2.8.0,flake8: noqa
2.8.0,Initalize with 1
2.8.0,Replace the hybridization
2.8.0,global possible_hybridization_list
2.8.0,Allow 0 index to correspond to null molecule 1
2.8.0,Correct for null
2.8.0,"print(6-k-1, id)"
2.8.0,Correct for last one
2.8.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.8.0,"Handle edge case of self-pairs (i, i)"
2.8.0,Increment by 1 since we don't want 0-indexing
2.8.0,"This creates a matrix of shape (2, num_pairs)"
2.8.0,Get mapping
2.8.0,first `bt_len` features are bond features(if applicable)
2.8.0,For ring pairs outside max pairs distance continue
2.8.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.8.0,graph distance between two atoms
2.8.0,distance is a matrix of 1-hot encoded distances for all atoms
2.8.0,For ring pairs outside max pairs distance continue
2.8.0,Euclidean distance between atoms
2.8.0,atoms `radial` bonds away from `a1`
2.8.0,atoms less than `radial` bonds away
2.8.0,find atoms `radial`+1 bonds away
2.8.0,create temporary valid ids serving to filter out failed featurizations from every sublist
2.8.0,"of features (i.e. every molecules' frags list), and also totally failed sublists."
2.8.0,This makes output digestable by Loaders
2.8.0,Get the node features
2.8.0,Stack nodes into an array
2.8.0,Get bond lists with reverse edges included
2.8.0,Get canonical adjacency list
2.8.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.8.0,only support datasets providing Cartesian coordinates)
2.8.0,Set dtype
2.8.0,If includes explicit hydrogens
2.8.0,If uses use_chirality
2.8.0,Atom features
2.8.0,Stack nodes into an array
2.8.0,Get bond lists
2.8.0,Get canonical adjacency list
2.8.0,Calculate pair features
2.8.0,the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
2.8.0,"SMILES is unique, so set a canonical order of atoms"
2.8.0,Add hydrogens and generate a conformation.
2.8.0,Record properties of the molecules.
2.8.0,Create the output object.
2.8.0,"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
2.8.0,flake8: noqa
2.8.0,base classes for featurizers
2.8.0,molecule featurizers
2.8.0,complex featurizers
2.8.0,material featurizers
2.8.0,biological sequence featurizers
2.8.0,tokenizers
2.8.0,support classes
2.8.0,dqc dependencies
2.8.0,get the density matrix from PySCF's CCSD calculation
2.8.0,for str
2.8.0,for list
2.8.0,validation
2.8.0,skip list
2.8.0,skip path string
2.8.0,main logic
2.8.0,Find a successful featurization
2.8.0,Replace failed featurizations with appropriate array
2.8.0,Special case handling of single molecule
2.8.0,Convert iterables to list
2.8.0,condition if the original atom order is required
2.8.0,"mol must be a RDKit Mol object, so parse a SMILES"
2.8.0,"mol must be a RDKit Mol object, so parse a SMILES"
2.8.0,"SMILES is unique, so set a canonical order of atoms"
2.8.0,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.8.0,atom_name is of format RESX-ATOMTYPE
2.8.0,where X is a 1 to 4 digit number
2.8.0,validate params
2.8.0,"np.max() method works only for a non-empty array, so size of the array should be non-zero"
2.8.0,Adding shapes of kwargs
2.8.0,This assumes that the edge features for self loops are full-zero tensors
2.8.0,In the future we may want to support featurization for self loops
2.8.0,Create a mapping from the original node indices to the new node indices
2.8.0,Filter and reindex node features
2.8.0,Filter and reindex edge indices and edge features
2.8.0,stack features
2.8.0,"before stacking edge_features or node_pos_features,"
2.8.0,we should check whether these are None or not
2.8.0,create new edge index
2.8.0,number of nodes in each graph
2.8.0,cumulative number of nodes for each graph. This is necessary because the values in edge_index are node indices of all of the graphs in graph_list and so we need to offset the indices by the number of nodes in the previous graphs.
2.8.0,"columns are the edge index, values are the node index"
2.8.0,graph_index indicates which nodes belong to which graph
2.8.0,Batch user defined attributes
2.8.0,Convert edge_index to adjacency list
2.8.0,Breadth-first search
2.8.0,Setup image
2.8.0,Compute bond properties
2.8.0,Compute atom properties
2.8.0,Setup image
2.8.0,Compute bond properties
2.8.0,Compute atom properties
2.8.0,Reshape done for proper broadcast
2.8.0,"Reshapes, and axes manipulations to facilitate vector processing."
2.8.0,Draw a line between the two atoms.
2.8.0,"The coordinates of this line, are indicated in line_coords"
2.8.0,Turn the line coordinates into image positions
2.8.0,Turn atomic coordinates into image positions
2.8.0,Set the bond line coordinates to the bond property used.
2.8.0,Set the atom positions in image to different atomic properties in channels
2.8.0,With fixed res and img_size some molecules (e.g. long chains) may not fit.
2.8.0,Check whether num_confs >=1 or not
2.8.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.8.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.8.0,consistent with most QM software packages.
2.8.0,Dimension of atom feature vector
2.8.0,len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
2.8.0,+ 2 at end for is_in_aromatic and mass
2.8.0,dictionary of available feature generators
2.8.0,for H2
2.8.0,"not all features are equally long, so used methane as dummy molecule to determine length"
2.8.0,Fix nans in features
2.8.0,add edge list considering a directed graph
2.8.0,get atom features
2.8.0,get edge(bond) features
2.8.0,get edge index
2.8.0,get global features
2.8.0,0 represents a masked bond
2.8.0,atoms
2.8.0,bonds
2.8.0,"Graph connectivity in COO format with shape [2, num_edges]"
2.8.0,"Edge feature matrix with shape [num_edges, num_edge_features]"
2.8.0,Always treat the bond as directed.
2.8.0,add mapping between bond b1 and atom a2 (destination atom)
2.8.0,add mapping between bond id and atom id (a1)
2.8.0,add mapping between bond id and atom a1 (source atom)
2.8.0,update index on bond and reverse bond mappings
2.8.0,generate SMILES for fragments
2.8.0,Featurize data using featurize() in parent class
2.8.0,Featurize str data
2.8.0,Extend shorter strings with padding
2.8.0,Padding before and after
2.8.0,Featurize data using featurize() in parent class
2.8.0,Featurize str data
2.8.0,Featurize mol data
2.8.0,Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
2.8.0,"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
2.8.0,load pretrained models
2.8.0,convert errors to zero
2.8.0,flake8: noqa
2.8.0,If partial charges were not computed
2.8.0,construct atom (node) feature
2.8.0,construct edge (bond) index
2.8.0,add edge list considering a directed graph
2.8.0,construct edge (bond) feature
2.8.0,load_sdf_files returns pos as strings but user can also specify
2.8.0,numpy arrays for atom coordinates
2.8.0,The 1.0 float value represents True Boolean
2.8.0,This will return a boolean vector with all entries False
2.8.0,To get the shortest paths between two nodes.
2.8.0,To get info if two nodes belong to the same ring.
2.8.0,Featurizer
2.8.0,"row, col = edge_index"
2.8.0,load_sdf_files returns pos as strings but user can also specify
2.8.0,numpy arrays for atom coordinates
2.8.0,get atom features
2.8.0,get edge index
2.8.0,user has not specified a descriptor list
2.8.0,creates normalized functions dictionary if normalized features are required
2.8.0,get cdf(feature) for that descriptor
2.8.0,get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
2.8.0,get required distribution_ from `scipy.stats` module.
2.8.0,cdf => cumulative density functions
2.8.0,make the cdf with the parameters.
2.8.0,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.8.0,Check whether num_confs >=1 or not
2.8.0,Convert AtomPositions from Angstrom to bohr (atomic units)
2.8.0,"`(1, max_atoms)` -> `(max_atoms,)`"
2.8.0,similar to SNAP featurizer. both taken from Open Graph Benchmark (OGB) github.com/snap-stanford/ogb
2.8.0,"The difference between this and the SNAP features is the lack of masking tokens, possible_implicit_valence_list, possible_bond_dirs"
2.8.0,"and the prescence of possible_bond_stereo_list,  possible_is_conjugated_list, possible_is_in_ring_list,"
2.8.0,FIXME Add support for multiple conformers (wip)
2.8.0,"def __init__(self, num_conformers: int = 1, rmsd_cutoff: float = 2):"
2.8.0,""""""""
2.8.0,Initialize the RDKitConformerFeaturizer with the given parameters.
2.8.0,Parameters
2.8.0,----------
2.8.0,"num_conformers : int, optional, default=1"
2.8.0,The number of conformers to generate for each molecule.
2.8.0,"rmsd_cutoff : float, optional, default=2"
2.8.0,The root-mean-square deviation (RMSD) cutoff value. Conformers with an RMSD
2.8.0,greater than this value will be discarded.
2.8.0,""""""""
2.8.0,self.num_conformers = num_conformers
2.8.0,self.rmsd_cutoff = rmsd_cutoff
2.8.0,Derived from https://github.com/HannesStark/3DInfomax/blob/5cd32629c690e119bcae8726acedefdb0aa037fc/datasets/qm9_dataset_rdkit_conformers.py#L377
2.8.0,add hydrogen bonds to molecule because they are not in the smiles representation
2.8.0,FIXME Add support for multiple conformers (wip)
2.8.0,"AllChem.EmbedMultipleConfs(mol, self.num_conformers)"
2.8.0,AllChem.MMFFOptimizeMolecule(mol)
2.8.0,rmsd_list = []
2.8.0,"rdMolAlign.AlignMolConformers(mol, RMSlist=rmsd_list)"
2.8.0,# insert 0 RMSD for first conformer
2.8.0,"rmsd_list.insert(0, 0)"
2.8.0,conformers = [
2.8.0,mol.GetConformer(i)
2.8.0,for i in range(self.num_conformers)
2.8.0,if rmsd_list[i] < self.rmsd_cutoff
2.8.0,]
2.8.0,"# if conformer list is less than num_conformers, pad by repeating conformers"
2.8.0,conf_idx = 0
2.8.0,while len(conformers) < self.num_conformers:
2.8.0,conformers.append(conformers[conf_idx])
2.8.0,conf_idx += 1
2.8.0,coordinates = [conf.GetPositions() for conf in conformers]
2.8.0,add edges in both directions
2.8.0,"Graph connectivity in COO format with shape [2, num_edges]"
2.8.0,FIXME Add support for multiple conformers (wip)
2.8.0,graph_list = []
2.8.0,for i in range(self.num_conformers):
2.8.0,graph_list.append(
2.8.0,"GraphData(node_pos_features=np.array(coordinates[i]),"
2.8.0,"node_features=np.array(atom_features_list),"
2.8.0,"edge_features=np.array(edge_features_list),"
2.8.0,edge_index=np.array(edges_list).T))
2.8.0,return graph_list
2.8.0,bond labels
2.8.0,atom labels
2.8.0,create bond encoders and decoders
2.8.0,create atom encoders and decoders
2.8.0,Special case handling of single molecule
2.8.0,Convert iterables to list
2.8.0,"sort first by frequency, then alphabetically"
2.8.0,"sort first by frequency, then alphabetically"
2.8.0,sorting the atoms neighbors
2.8.0,concatenating the sorted neighbors
2.8.0,"sort first by frequency, then alphabetically"
2.8.0,"sort first by frequency, then alphabetically"
2.8.0,flake8: noqa
2.8.0,superclass accepts a DeepChem dataset while huggingface vocabulary builders
2.8.0,reads data from file
2.8.0,test with max size
2.8.0,test build from csv
2.8.0,test with max size
2.8.0,load the vocabulary by passing filename
2.8.0,test tokenization of a single point
2.8.0,load the vocabulary by passing the filename
2.8.0,test tokenization of a single point
2.8.0,Build vocabulary by wrapping in huggingface vocabulary builder
2.8.0,Load vocabulary and do a basic sanity check on the vocabulary
2.8.0,Set up site environment matcher
2.8.0,Graphical option
2.8.0,tolerance for grouping nodes
2.8.0,determine minimum distance between sitetypes.
2.8.0,This is used to determine the existence of an edge
2.8.0,Sort by bond
2.8.0,You want to maximize this in order to make sure every node gets an edge
2.8.0,construct graph
2.8.0,matcher options
2.8.0,construct graph
2.8.0,Add nodes
2.8.0,Add edge. distance is edge attribute
2.8.0,construct graph
2.8.0,Gets the isomorphic mapping. Also the most time consuming part of the code
2.8.0,reconstruct graph after alinging point order
2.8.0,RMSD
2.8.0,Construct one hot encoding
2.8.0,get mapping between all site index to active site index
2.8.0,Get Neighbors
2.8.0,Read Data
2.8.0,get map between two environment
2.8.0,align input to the primitive cell (reference)
2.8.0,apply permutations
2.8.0,remove spectators
2.8.0,map it to active sites
2.8.0,Extract the right number of sites by distance
2.8.0,if PBC condition is fulfilled..
2.8.0,Get full N x N SCM
2.8.0,flake8: noqa
2.8.0,load atom_init.json
2.8.0,check whether the atom feature exists or not
2.8.0,construct bi-directed graph
2.8.0,Increase dimension of distance tensor and apply filter
2.8.0,We compute pairwise contact fingerprints
2.8.0,We compute pairwise contact fingerprints
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0,We compute pairwise contact fingerprints
2.8.0,"rdks = [frag1[1], frag2[1]]"
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0,We compute pairwise contact fingerprints
2.8.0,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.8.0,"rdks = [frag1[1], frag2[1]]"
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0,We compute pairwise contact fingerprints
2.8.0,"rdks = [frag1[1], frag2[1]]"
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.8.0,We compute pairwise contact fingerprints
2.8.0,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.8.0,We compute pairwise contact fingerprints
2.8.0,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.8.0,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.8.0,"xyzs = [frag1_xyz, frag2_xyz]"
2.8.0,"rdks = [frag1[1], frag2[1]]"
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0,We compute pairwise contact fingerprints
2.8.0,"rdks = [frag1[1], frag2[1]]"
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0,check if user tries to set removed arguments
2.8.0,list of features that require sanitized molecules
2.8.0,not implemented featurization types
2.8.0,default values
2.8.0,update with cutoffs specified by the user
2.8.0,"each entry is a tuple (is_flat, feature_name)"
2.8.0,list of features that cannot be calculated with specified parameters
2.8.0,this list is used to define <flat/voxel/all>_combined subset
2.8.0,parse provided feature types
2.8.0,flake8: noqa
2.8.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.8.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.8.0,nonzero contact.
2.8.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.8.0,We compute pairwise contact fingerprints
2.8.0,Get coordinates
2.8.0,We compute pairwise contact fingerprints
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.8.0,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.8.0,axis.
2.8.0,Type of data created by this featurizer
2.8.0,TODO(rbharath): Should this return a list?
2.8.0,Type of data created by this featurizer
2.8.0,Currently handles loading failures by returning None
2.8.0,TODO: Is there a better handling procedure?
2.8.0,pad outputs
2.8.0,Deprecation warnings for old atomic conv featurizer name #
2.8.0,We compute pairwise contact fingerprints
2.8.0,Get coordinates
2.8.0,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.8.0,We compute pairwise contact fingerprints
2.8.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0,decode the source in the mixed and separated cases
2.8.0,number of indices where feature count is more than 1
2.8.0,no normalized feature value should be greater than 1.0
2.8.0,"151 = 133 + 18 (133 -> one hot encoding from _ATOM_FEATURES, 18 other features)"
2.8.0,TODO test more formats for ligand
2.8.0,TODO test more formats for ligand
2.8.0,with one conformer
2.8.0,with multiple conformers
2.8.0,include explicit hydrogens
2.8.0,with one conformer
2.8.0,with multiple conformers
2.8.0,include explicit hydrogens
2.8.0,NOTE: The test depends on the the pretrained vocabulary
2.8.0,(seyonec/PubChem10M_SMILES_BPE_60k). If the pretrained vocabulary is modified
2.8.0,"(which can be since it is an external resource), the test might fail."
2.8.0,construct edge (bond) index
2.8.0,add edge list considering a directed graph
2.8.0,test for 'MolGraphConvFeaturizer' class
2.8.0,"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
2.8.0,"Requirements - transformers, tokenizers"
2.8.0,no normalized feature value should be greater than 1.0
2.8.0,these are the properties used in grover
2.8.0,"assert ""C1=CC=CN=C1"""
2.8.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0,"assert ""C1=CC=CN=C1"""
2.8.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0,"assert ""C1=CC=CN=C1"""
2.8.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0,"assert ""C1=CC=CN=C1"""
2.8.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0,Test featurizer with atom 3-D coordinates as kwargs
2.8.0,load sample dataset
2.8.0,"assert ""C1=CC=CN=C1"""
2.8.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0,"number of indices, where feature count is more than 1, should be 0"
2.8.0,number of indices where feature count is more than 1
2.8.0,check for separate count and SMILES entries for each fragment
2.8.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.8.0,max num atoms instead of exact.
2.8.0,Cutoff in angstroms
2.8.0,"Coords are padded, neighbor list and Z are not"
2.8.0,both reactant and product are null
2.8.0,reactant is null
2.8.0,product is null
2.8.0,valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
2.8.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.8.0,def test_hydrogen_bond_counter():
2.8.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.8.0,"protein_file = os.path.join(current_dir, 'data',"
2.8.0,'3ws9_protein_fixer_rdkit.pdb')
2.8.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.8.0,
2.8.0,cutoff = 4.5
2.8.0,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.8.0,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.8.0,# TODO: Add shape test
2.8.0,
2.8.0,
2.8.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.8.0,def test_hydrogen_bond_voxelizer():
2.8.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.8.0,"protein_file = os.path.join(current_dir, 'data',"
2.8.0,'3ws9_protein_fixer_rdkit.pdb')
2.8.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.8.0,
2.8.0,cutoff = 4.5
2.8.0,box_width = 16
2.8.0,voxel_width = 1.0
2.8.0,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.8.0,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.8.0,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.8.0,# TODO: Add shape test
2.8.0,test if default parameters work
2.8.0,check if use-case from examples works
2.8.0,test if input is flattened when flat features are used
2.8.0,test voxel features
2.8.0,test flat features
2.8.0,check if aromatic features are ignored if sanitize=False
2.8.0,test flattened voxel features
2.8.0,test voxel features
2.8.0,test flat features
2.8.0,test rotations
2.8.0,not support array style inputs
2.8.0,z is kwargs
2.8.0,check convert function
2.8.0,z is kwargs
2.8.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.8.0,of degree 1 (connected only to central nitrogen).
2.8.0,5 atoms in compound
2.8.0,Get the adjacency lists grouped by degree
2.8.0,The 4 outer atoms connected to central nitrogen
2.8.0,Central nitrogen connected to everything else.
2.8.0,Only one carbon
2.8.0,"No bonds, so degree adjacency lists are empty"
2.8.0,3 carbonds in alkane
2.8.0,Outer two carbonds are connected to central carbon
2.8.0,Central carbon connected to outer two
2.8.0,test featurization
2.8.0,test defeaturization
2.8.0,sanity check; see if something weird does not happen with rdkit
2.8.0,check if original smiles match defeaturized smiles
2.8.0,sanity check; see if something weird does not happen with rdkit
2.8.0,test featurization
2.8.0,test defeaturization
2.8.0,check if original smiles match defeaturized smiles
2.8.0,untransform
2.8.0,untranform
2.8.0,untranform
2.8.0,untranform
2.8.0,untranform
2.8.0,Check the SDF file.
2.8.0,Check the PDB file.
2.8.0,Check the SMILES string.
2.8.0,Set up tests.
2.8.0,Set up testing parameters.
2.8.0,the atom order for 'C' is same in case of canonical and original ordering
2.8.0,Do a manual distance computation and make
2.8.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.8.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.8.0,Do a manual distance computation and ensure that selected neighbor is
2.8.0,closest since we set max_num_neighbors = 1
2.8.0,Carbon
2.8.0,Test distance 1
2.8.0,Test distance 2
2.8.0,Test alkane
2.8.0,Test distance 1
2.8.0,3 self connections and 2 bonds which are both counted twice because of
2.8.0,symmetry for 7 total
2.8.0,Test distance 2
2.8.0,Everything is connected at this distance
2.8.0,Test alkane
2.8.0,Test distance infinity
2.8.0,Everything is connected at this distance
2.8.0,Test pentane
2.8.0,Test distance infinity
2.8.0,Everything is connected at this distance
2.8.0,Only one carbon
2.8.0,Test feature sizes
2.8.0,"No bonds, so only 1 pair feature (for the self interaction)"
2.8.0,Only 4 atoms
2.8.0,Test feature sizes for chirality
2.8.0,3 carbonds in alkane
2.8.0,Test feature sizes
2.8.0,Should be a 3x3 interaction grid
2.8.0,mol_list = featurizer.featurize(mols)
2.8.0,mol = mol_list[0]
2.8.0,3 carbonds in alkane
2.8.0,Test feature sizes
2.8.0,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.8.0,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.8.0,symmetry)
2.8.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.8.0,of degree 1 (connected only to central nitrogen).
2.8.0,import rdkit.Chem
2.8.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.8.0,5 atoms in compound
2.8.0,Test feature sizes
2.8.0,Should be a 3x3 interaction grid
2.8.0,"bond and its reverse bond, therefore num_bonds * 2 edges"
2.8.0,9 atom features
2.8.0,"Graph connectivity in COO format with shape [2, num_edges]"
2.8.0,3 bond features
2.8.0,3 xyz coordinates for each atom in the conformer
2.8.0,Artificial feature array.
2.8.0,0 atoms of degree 0
2.8.0,0 atoms of degree 1
2.8.0,4 atoms of degree 2
2.8.0,0 atoms of degree 3
2.8.0,0 atoms of degree 4
2.8.0,0 atoms of degree 5
2.8.0,0 atoms of degree 6
2.8.0,0 atoms of degree 7
2.8.0,0 atoms of degree 8
2.8.0,0 atoms of degree 9
2.8.0,0 atoms of degree 10
2.8.0,atom 4 has 0 neighbors
2.8.0,atom 0 has 2 neighbors
2.8.0,atom 1 has 2 neighbors
2.8.0,atom 2 has 2 neighbors
2.8.0,atom 3 has 3 neighbors.
2.8.0,Verify that atom features have been sorted by atom degree.
2.8.0,Sorting is done by atom degree as before. So the ordering goes
2.8.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.8.0,from new position to old position is
2.8.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.8.0,list respects this reordering and returns correct adjacency list.
2.8.0,First example molecule
2.8.0,Artificial feature array.
2.8.0,Second example molecule
2.8.0,Third example molecule
2.8.0,Test agglomerate molecule method
2.8.0,No atoms of degree 0
2.8.0,3 atoms of degree 1
2.8.0,8 atoms of degree 2
2.8.0,1 atom of degree 3
2.8.0,0 atoms of degree 4
2.8.0,0 atoms of degree 5
2.8.0,Check that atoms are only connected to themselves.
2.8.0,Check that there's one atom of each degree.
2.8.0,calculate coordinates
2.8.0,not zero values
2.8.0,Calculate frequency
2.8.0,flake8: noqa
2.8.0,assumes that every array is of the same dimension
2.8.0,rem_dataset is remaining portion of dataset
2.8.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.8.0,to k-1.
2.8.0,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.8.0,validation
2.8.0,skip list
2.8.0,skip path string
2.8.0,main logic
2.8.0,for str
2.8.0,for list
2.8.0,dict is needed in case groups aren't strictly flattened or
2.8.0,hashed by something non-integer like
2.8.0,Figure out how many positive samples we want for each task in each dataset.
2.8.0,Assign the positive samples to datasets.  Since a sample may be positive
2.8.0,"on more than one task, we need to keep track of the effect of each added"
2.8.0,"sample on each task.  To try to keep everything balanced, we cycle through"
2.8.0,"tasks, assigning one positive sample for each one."
2.8.0,We have a sample that hasn't been assigned yet.  Assign it to
2.8.0,whichever set currently has the lowest fraction of its target for
2.8.0,this task.
2.8.0,The remaining samples are negative for all tasks.  Add them to fill out
2.8.0,each set to the correct total number.
2.8.0,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.8.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.8.0,This can be generalized in the future with some common demoninator determination.
2.8.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.8.0,Append remaining examples to train
2.8.0,################################################################
2.8.0,Splitter for molecule datasets
2.8.0,################################################################
2.8.0,Sort by increasing MW
2.8.0,calcaulate scaffold sets
2.8.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.8.0,Compute fingerprints for all molecules.
2.8.0,Split into two groups: training set and everything else.
2.8.0,Split the second group into validation and test sets.
2.8.0,Begin by assigning the first molecule to the first group.
2.8.0,Return identity if no tuple to split to
2.8.0,Decide which group to assign a molecule to.
2.8.0,Identify the unassigned molecule that is least similar to everything in
2.8.0,the other group.
2.8.0,Add it to the group.
2.8.0,Update the data on unassigned molecules.
2.8.0,Sort from largest to smallest scaffold sets
2.8.0,################################################################
2.8.0,Not well supported splitters
2.8.0,################################################################
2.8.0,All datasets share features and identifiers by assumption.
2.8.0,flake8: noqa
2.8.0,basic splitter
2.8.0,molecule splitter
2.8.0,other splitter
2.8.0,################################################################
2.8.0,Removed API
2.8.0,################################################################
2.8.0,Note that the extra task goes to test
2.8.0,Number tasks per fold
2.8.0,Find the tasks that correspond to this test fold
2.8.0,Assert that all arrays look like they should
2.8.0,"task_type = ""regression"""
2.8.0,0 1 2 3 4 5 6 7 8 9
2.8.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.8.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.8.0,reshard() to handle this?
2.8.0,Verify lengths is 10/k == 2
2.8.0,Verify that compounds in this fold are subset of original compounds
2.8.0,Verify that no two folds have overlapping compounds.
2.8.0,Verify lengths is 10/k == 2
2.8.0,Verify that compounds in this fold are subset of original compounds
2.8.0,Verify that no two folds have overlapping compounds.
2.8.0,Verify lengths is 10/k == 2
2.8.0,Verify that compounds in this fold are subset of original compounds
2.8.0,Verify that no two folds have overlapping compounds.
2.8.0,Test singletask case.
2.8.0,The split index should partition dataset in half.
2.8.0,Test singletask case.
2.8.0,Test case where some weights are zero (i.e. masked)
2.8.0,Set half the positives to have zero weight
2.8.0,There are 10 nonzero actives.
2.8.0,"The split index should partition this into half, so expect 5"
2.8.0,The split index should partition the positives for each task roughly in half.
2.8.0,Mask half the examples
2.8.0,The split index should partition dataset in half.
2.8.0,Test singletask case.
2.8.0,Should have split cleanly in half (picked random seed to ensure this)
2.8.0,Check positives are correctly distributed
2.8.0,Test singletask case.
2.8.0,Should have made an 80/10/10 train/valid/test split of actives.
2.8.0,Verify lengths is 100/k == 20
2.8.0,Note: This wouldn't work for multitask str
2.8.0,assert len(fold_dataset) == n_samples/K
2.8.0,Verify that each fold has n_positives/K = 4 positive examples.
2.8.0,Verify that compounds in this fold are subset of original compounds
2.8.0,Verify that no two folds have overlapping compounds.
2.8.0,The amount of datapoints has to be the same
2.8.0,The number of scaffolds generated by the splitter
2.8.0,has to be smaller or equal than number of total molecules
2.8.0,Invalid because valence for atom 5 N is greater than permitted (4)
2.8.0,edges logits used during training
2.8.0,nodes logits used during training
2.8.0,edges logits
2.8.0,nodes logits
2.8.0,training of the model
2.8.0,generating compounds
2.8.0,nodes logits used during compound generation
2.8.0,Create the inputs.
2.8.0,Create the generators.
2.8.0,Create the discriminators.
2.8.0,Compute the loss functions.
2.8.0,Create learnable weights for the generators and discriminators.
2.8.0,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.8.0,Compute the weighted errors
2.8.0,Add an entropy term to the loss.
2.8.0,Create the Keras model.
2.8.0,"Every call to fit_generator() will increment global_step, but we only"
2.8.0,"want it to get incremented once for the entire batch, so record the"
2.8.0,value and keep resetting it.
2.8.0,Train the discriminator.
2.8.0,Train the generator.
2.8.0,Write checkpoints and report progress.
2.8.0,Write out final results.
2.8.0,Chain of flows is also a normalizing flow
2.8.0,An instance of tfd.TransformedDistribution
2.8.0,TODO: Incompability between TF and TFP means that TF doesn't track
2.8.0,trainable variables in the flow; must override `_create_gradient_fn`
2.8.0,self._variables = self.flow.trainable_variables
2.8.0,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.8.0,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.8.0,This is for API consistency
2.8.0,extended one of probabilites to binary distribution
2.8.0,extended one of probabilites to binary distribution
2.8.0,NOTE The below comment is from original source code
2.8.0,dist_loss = av_dist_loss + bv_dist_loss + fg_dist_loss
2.8.0,return av_loss + fg_loss + dist_coff * dist_loss
2.8.0,"return overall_loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss"
2.8.0,We just return overall_loss since TorchModel can handle only a single loss
2.8.0,loss for nodes
2.8.0,converting the binary classification to multiclass classification
2.8.0,positive context prediction is the dot product of substructure representation and true context representation
2.8.0,negative context prediction is the dot product of substructure representation and negative (random) context representation.
2.8.0,positive substructure prediction is the dot product of expanded substructure representation and true overlapped node representation.
2.8.0,shift indices of substructures to create negative examples
2.8.0,negative substructure prediction is the dot product of shifted expanded substructure representation and true overlapped node representation.
2.8.0,Compute the loss for positive and negative context representations
2.8.0,The final loss is the sum of positive and negative context losses
2.8.0,-*- coding: utf-8 -*-
2.8.0,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0,"Shape (N_atoms, M_nbrs)"
2.8.0,Generate the nb_affine weights and biases
2.8.0,Extract atom_features
2.8.0,Extract graph topology
2.8.0,Sum all neighbors using adjacency matrix
2.8.0,Get collection of modified atom features
2.8.0,Obtain relevant atoms for this degree
2.8.0,Get self atoms
2.8.0,Apply hidden affine to relevant atoms and append
2.8.0,Determine the min_deg=0 case
2.8.0,Only use the self layer
2.8.0,Combine all atoms back into the list
2.8.0,Tensorflow correctly processes empty lists when using concat
2.8.0,"Sum along neighbors as well as self, and store"
2.8.0,Perform the mol gather
2.8.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.8.0,"self.max_degree, self.min_degree)"
2.8.0,Tensorflow correctly processes empty lists when using concat
2.8.0,Get self atoms
2.8.0,"There are no neighbors of this degree, so just create an empty tensor directly."
2.8.0,Expand dims
2.8.0,always deg-1 for deg_adj_lists
2.8.0,Extract graph topology
2.8.0,means that this is second loop of convolution
2.8.0,No other forget biases supported right now.
2.8.0,Taken from Keras code [citation needed]
2.8.0,"x is test set, xp is support set."
2.8.0,Get initializations
2.8.0,Process using attention
2.8.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.8.0,Generate new attention states
2.8.0,Support set lstm
2.8.0,Test lstm
2.8.0,Get initializations
2.8.0,Rename support
2.8.0,Process support xp using attention
2.8.0,Get linear combination of support set
2.8.0,Process test x using attention
2.8.0,Generate new support attention states
2.8.0,Generate new test attention states
2.8.0,Redefine
2.8.0,Number of rotatable bonds
2.8.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.8.0,a difference.
2.8.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.8.0,neighbors lists an argument instead of a part of this layer.
2.8.0,"Shape (N, M)"
2.8.0,"Shape (N, M)"
2.8.0,"Shape (N, M)"
2.8.0,Number of grid cells
2.8.0,TODO(rbharath): Support batching
2.8.0,"Shape (n_cells, ndim)"
2.8.0,"List of length N_atoms, each element of different length uniques_i"
2.8.0,"List of length N_atoms, each element of different length uniques_i"
2.8.0,"List of length N_atoms, each a tensor of shape"
2.8.0,"(uniques_i, ndim)"
2.8.0,Add phantom atoms that exist far outside the box
2.8.0,"List of length N_atoms, each of shape (1, ndim)"
2.8.0,TODO(rbharath): How does distance need to be modified here to
2.8.0,account for periodic boundary conditions?
2.8.0,List of length N_atoms each of shape (M_nbrs)
2.8.0,"N_atoms elts of size (M_nbrs,) each"
2.8.0,"Shape (N_atoms, 1)"
2.8.0,Find M_nbrs atoms closest to each cell
2.8.0,"Shape (n_cells, M_nbrs)"
2.8.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0,"conditions, so does wrapround. O(constant)"
2.8.0,"Shape (n_cells, n_nbr_cells)"
2.8.0,"Shape (N_atoms, n_nbr_cells)"
2.8.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.8.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.8.0,"List of length N_atoms, each element length uniques_i"
2.8.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.8.0,element removed to remove self from list of neighbors. Need to verify
2.8.0,this holds more broadly or come up with robust alternative.
2.8.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.8.0,Shape (N_atoms*n_cells)
2.8.0,"Shape (n_cells, N_atoms)"
2.8.0,Find k atoms closest to this cell. Notice negative sign since
2.8.0,tf.nn.top_k returns *largest* not smallest.
2.8.0,"Tensor of shape (n_cells, M_nbrs)"
2.8.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0,"Shape (N_atoms*n_cells, 1) after tile"
2.8.0,9 neighbors in 2-space
2.8.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.8.0,Number of cells for cube in 3-space is
2.8.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.8.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.8.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.8.0,the cube.
2.8.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.8.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.8.0,"Tile (a, a, a, b, b, b, etc.)"
2.8.0,"Tile (a, b, c, a, b, c, ...)"
2.8.0,N: Maximum number of atoms
2.8.0,M: Maximum number of neighbors
2.8.0,d: Number of coordinates/features/filters
2.8.0,B: Batch Size
2.8.0,Compute the distances and radial symmetry functions.
2.8.0,check that there isnt just one or zero inputs
2.8.0,create subspaces
2.8.0,"concatenate subspaces, reshape to size of original input, then stack"
2.8.0,"such that out_tensor has shape (2,?,original_cols)"
2.8.0,creates subspaces the same way it was done in AlphaShare
2.8.0,calculate squared Frobenius norm
2.8.0,"(TODO YTZ:) faster, less memory intensive way"
2.8.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.8.0,"r = tf.expand_dims(r, -1)"
2.8.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.8.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.8.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.8.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.8.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.8.0,Calculate pairwise distance
2.8.0,Cutoff with threshold Rc
2.8.0,return d
2.8.0,tf.stack issues again...
2.8.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.8.0,So the Tensor has known dimensions
2.8.0,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.8.0,normalization
2.8.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.8.0,and embeddings of atom j(both gone through a hidden layer)
2.8.0,"for atom i, sum the influence from all other atom j in the molecule"
2.8.0,number of inputs each step
2.8.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.8.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.8.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.8.0,`count`-th step
2.8.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.8.0,generating index for graph features used in the inputs
2.8.0,"extracting graph features for parents of the target atoms, then flatten"
2.8.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.8.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.8.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.8.0,of shape: (batch_size*max_atoms) * n_graph_features
2.8.0,representing the graph features of target atoms in each graph
2.8.0,index for targe atoms
2.8.0,Extract atom_features
2.8.0,sum all graph outputs
2.8.0,"Default message function: edge network, update function: GRU"
2.8.0,more options to be implemented
2.8.0,Add another value(~-Inf) to prevent error in softmax
2.8.0,Model using this layer must set pad_batches=True
2.8.0,Perform one step of LSTM
2.8.0,task_metadata_rows = {task: [] for task in tasks}
2.8.0,Extract those datapoints which are present for this task
2.8.0,Loading is done on-the-fly
2.8.0,Build the model.
2.8.0,Final atom-layer convolution. Note this differs slightly from the paper
2.8.0,since we use a tanh activation as default. This seems necessary for numerical
2.8.0,stability.
2.8.0,Now fully connected layers
2.8.0,Should this allow for training?
2.8.0,"pair_edges is of shape (2, N)"
2.8.0,number of atoms in each molecule
2.8.0,index of pair features
2.8.0,Get starting pair atoms
2.8.0,number of pairs for each atom
2.8.0,atom features
2.8.0,pair features
2.8.0,Build the model.
2.8.0,Build the model.
2.8.0,calculation orders for a batch of molecules
2.8.0,padding atom features vector of each molecule with 0
2.8.0,Build the model.
2.8.0,number of atoms in each molecule
2.8.0,index of pair features
2.8.0,number of pairs for each atom
2.8.0,atom features
2.8.0,pair features
2.8.0,################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
2.8.0,Add the input features.
2.8.0,Add the shared dense layers
2.8.0,Add task-specific bypass layers
2.8.0,Add the input features.
2.8.0,Add the shared dense layers
2.8.0,Add task-specific bypass layers
2.8.0,W&B flag support (DEPRECATED)
2.8.0,"If `wandb=True` and no logger is provided, initialize default logger"
2.8.0,Setup and initialize W&B logging
2.8.0,Update config with KerasModel params
2.8.0,Backwards compatibility
2.8.0,The optimizer creates internal variables the first time apply_gradients()
2.8.0,is called for a new set of variables.  If that happens inside a function
2.8.0,"annotated with tf.function it throws an exception, so call it once here."
2.8.0,Main training loop.
2.8.0,"Execute the loss function, accumulating the gradients."
2.8.0,Report progress and write checkpoints.
2.8.0,Capture the last avg_loss in case of return since we're resetting to
2.8.0,0 now
2.8.0,Report final results.
2.8.0,Invoke the model.
2.8.0,Apply tranformers and record results.
2.8.0,Concatenate arrays to create the final results.
2.8.0,Use a GradientTape to compute gradients.
2.8.0,Ensure weights for both models are built.
2.8.0,Define the PyTorch Module that implements the model.
2.8.0,Define the PyTorch Module that implements the model.
2.8.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.8.0,set wandb init arguments
2.8.0,Dataset ids are used to differentiate datasets seen by the logger
2.8.0,log data
2.8.0,Similarity values
2.8.0,Labels for all top K similar samples
2.8.0,Discard any padded predictions
2.8.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.8.0,Build the model.
2.8.0,Character embedding
2.8.0,Multiple convolutional layers with different filter widths
2.8.0,Max-over-time pooling
2.8.0,Concat features from all filters(one feature per filter)
2.8.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.8.0,SMILES strings
2.8.0,Maximum length is expanded to allow length variation during train and inference
2.8.0,'_' served as delimiter and padding
2.8.0,Initialize common characters as keys
2.8.0,Include space to avoid extra keys
2.8.0,"For 'Cl', 'Br', etc."
2.8.0,"Character not recognized, add to extra_keys"
2.8.0,Add all extra_keys to char_dict
2.8.0,Transform SMILES sequence to integers
2.8.0,Skip all spaces
2.8.0,"For 'Cl', 'Br', etc."
2.8.0,Padding with '_'
2.8.0,################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
2.8.0,"layer_sizes=[32, 32, 16],"
2.8.0,Add the dense layers
2.8.0,Do a simple greedy search.
2.8.0,Do a beam search with length normalization.
2.8.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.8.0,This candidate sequence has already been terminated
2.8.0,Consider all possible tokens we could add to this candidate sequence.
2.8.0,Add the input features.
2.8.0,Handle output layer
2.8.0,Iterate over all previous tasks.
2.8.0,prev_layers is a list with elements of size
2.8.0,"(batch_size, layer_sizes[i-1])"
2.8.0,Log data to Wandb
2.8.0,flake8: noqa
2.8.0,Tensorflow Dependency Models
2.8.0,scikit-learn model
2.8.0,PyTorch models
2.8.0,Pytorch models with torch-geometric dependency
2.8.0,TODO We should clean up DMPNN and remove torch_geometric dependency during import
2.8.0,Pytorch-lightning modules import
2.8.0,Jax models
2.8.0,####################################################################################
2.8.0,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.8.0,####################################################################################
2.8.0,#######################################################################################
2.8.0,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.8.0,#######################################################################################
2.8.0,Last layer sequences not returned.
2.8.0,This is needed because ImageDataGenerator does infinite looping
2.8.0,"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
2.8.0,but turns out to be slightly faster
2.8.0,JAX depend
2.8.0,Main training loop
2.8.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0,Report final results.
2.8.0,Apply tranformers and record results.
2.8.0,Concatenate arrays to create the final results.
2.8.0,"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
2.8.0,") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
2.8.0,""""""""
2.8.0,"Predict the model's outputs, along with the uncertainty in each one."
2.8.0,The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
2.8.0,It involves repeating the prediction many times with different dropout masks.
2.8.0,The prediction is computed as the average over all the predictions.  The
2.8.0,uncertainty includes both the variation among the predicted values (epistemic
2.8.0,uncertainty) and the model's own estimates for how well it fits the data
2.8.0,(aleatoric uncertainty).  Not all models support uncertainty prediction.
2.8.0,Parameters
2.8.0,----------
2.8.0,dataset: dc.data.Dataset
2.8.0,Dataset to make prediction on
2.8.0,masks: int
2.8.0,the number of dropout masks to average over
2.8.0,Returns
2.8.0,-------
2.8.0,"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
2.8.0,"value of the output, and each element of y_std estimates the standard"
2.8.0,deviation of the corresponding element of y_pred
2.8.0,""""""""
2.8.0,sum_pred: List[np.ndarray] = []
2.8.0,sum_sq_pred: List[np.ndarray] = []
2.8.0,sum_var: List[np.ndarray] = []
2.8.0,for i in range(masks):
2.8.0,generator = self.default_generator(
2.8.0,"dataset, mode='uncertainty', pad_batches=False)"
2.8.0,"results = self._predict(generator, [], True, None)"
2.8.0,if len(sum_pred) == 0:
2.8.0,"for p, v in results:"
2.8.0,sum_pred.append(p)
2.8.0,sum_sq_pred.append(p * p)
2.8.0,sum_var.append(v)
2.8.0,else:
2.8.0,"for j, (p, v) in enumerate(results):"
2.8.0,sum_pred[j] += p
2.8.0,sum_sq_pred[j] += p * p
2.8.0,sum_var[j] += v
2.8.0,output = []
2.8.0,std = []
2.8.0,for i in range(len(sum_pred)):
2.8.0,p = sum_pred[i] / masks
2.8.0,output.append(p)
2.8.0,std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
2.8.0,if len(output) == 1:
2.8.0,"return (output[0], std[0])"
2.8.0,else:
2.8.0,"return list(zip(output, std))"
2.8.0,JAX dependencies
2.8.0,Main training loop
2.8.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0,Report final results.
2.8.0,Apply tranformers and record results.
2.8.0,Concatenate arrays to create the final results.
2.8.0,flake8:noqa
2.8.0,The PINNModel requires you to create two functions
2.8.0,`create_eval`_fn for letting the model know how to compute the model in inference and
2.8.0,`gradient_fn` for letting model know how to compute the gradient and different regulariser
2.8.0,equation loss depending on the differential equation
2.8.0,defining the Haiku model
2.8.0,"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
2.8.0,"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
2.8.0,which will be used as the differential loss(regulariser)
2.8.0,The expected solution must be as close to cos(x)
2.8.0,Initialize the weights with random values
2.8.0,Forward function which takes the params
2.8.0,Loss Function
2.8.0,JaxModel Working
2.8.0,sample network
2.8.0,Model Initialization
2.8.0,Loss Function
2.8.0,JaxModel Working
2.8.0,sample network
2.8.0,Model Initilisation
2.8.0,Loss Function
2.8.0,JaxModel Working
2.8.0,Model Initilisation
2.8.0,Loss Function
2.8.0,JaxModel Working
2.8.0,Model Initilisation
2.8.0,Loss Function
2.8.0,JaxModel Working
2.8.0,Model Initilisation
2.8.0,Loss Function
2.8.0,JaxModel Working
2.8.0,Each epoch is a single step for this model
2.8.0,@pytest.mark.jax
2.8.0,@pytest.mark.slow
2.8.0,def test_uncertainty():
2.8.0,"""""""Test estimating uncertainty a TorchModel."""""""
2.8.0,n_samples = 30
2.8.0,n_features = 1
2.8.0,noise = 0.1
2.8.0,"X = np.random.rand(n_samples, n_features)"
2.8.0,"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
2.8.0,"dataset = dc.data.NumpyDataset(X, y)"
2.8.0,class Net(hk.Module):
2.8.0,"def __init__(self, output_size: int = 1):"
2.8.0,super().__init__()
2.8.0,"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.8.0,"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.8.0,self.output = hk.Linear(output_size)
2.8.0,self.log_var = hk.Linear(output_size)
2.8.0,"def __call__(self, x):"
2.8.0,x = self._network1(x)
2.8.0,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.8.0,x = self._network2(x)
2.8.0,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.8.0,output = self.output(x)
2.8.0,log_var = self.log_var(x)
2.8.0,var = jnp.exp(log_var)
2.8.0,"return output, var, output, log_var"
2.8.0,def f(x):
2.8.0,net = Net(1)
2.8.0,return net(x)
2.8.0,"def loss(outputs, labels, weights):"
2.8.0,diff = labels[0] - outputs[0]
2.8.0,log_var = outputs[1]
2.8.0,var = jnp.exp(log_var)
2.8.0,return jnp.mean(diff * diff / var + log_var)
2.8.0,class UncertaintyModel(JaxModel):
2.8.0,"def default_generator(self,"
2.8.0,"dataset,"
2.8.0,"epochs=1,"
2.8.0,"mode='fit',"
2.8.0,"deterministic=True,"
2.8.0,pad_batches=True):
2.8.0,for epoch in range(epochs):
2.8.0,"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
2.8.0,"batch_size=self.batch_size,"
2.8.0,"deterministic=deterministic,"
2.8.0,pad_batches=pad_batches):
2.8.0,"yield ([X_b], [y_b], [w_b])"
2.8.0,jm_model = hk.transform(f)
2.8.0,rng = jax.random.PRNGKey(500)
2.8.0,"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
2.8.0,modified_inputs = jnp.array(
2.8.0,[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
2.8.0,"params = jm_model.init(rng, modified_inputs)"
2.8.0,model = UncertaintyModel(
2.8.0,"jm_model.apply,"
2.8.0,"params,"
2.8.0,"loss,"
2.8.0,"output_types=['prediction', 'variance', 'loss', 'loss'],"
2.8.0,learning_rate=0.003)
2.8.0,"model.fit(dataset, nb_epochs=2500)"
2.8.0,"pred, std = model.predict_uncertainty(dataset)"
2.8.0,assert np.mean(np.abs(y - pred)) < 2.0
2.8.0,assert noise < np.mean(std) < 1.0
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,"Create an identical model, do a single step of fitting with restore=True and make sure it got restored correctly."
2.8.0,check that the first layer is still the same between the two models
2.8.0,check that the predictions are different because of the fine tuning
2.8.0,check that the first layer is different between the two models
2.8.0,Conv2d and Linear layers test(CNN classification)
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,test if adjacency matrix input is correctly set
2.8.0,test if nodes features matrix input is correctly set
2.8.0,check discriminator shape
2.8.0,check training edges logits shape
2.8.0,check training nodes logits shapes
2.8.0,True will be assigned up successful training attempt
2.8.0,force clear tensor flow backend
2.8.0,create new model
2.8.0,to avoid flake8 E125/yapf incompatibility
2.8.0,generate input
2.8.0,train model
2.8.0,generate sample
2.8.0,check how many valid molecules were created and add to list
2.8.0,finally test if there was at least one valid training session
2.8.0,as the model structure improves this should become more and more strict
2.8.0,Predict the output and uncertainty.
2.8.0,predict datset with no y (ensured by tasks = [])
2.8.0,Predict the output and uncertainty.
2.8.0,The DAG models have high error with dropout
2.8.0,"Despite a lot of effort tweaking it , there appears to be"
2.8.0,a limit to how low the error can go with dropout.
2.8.0,assert mean_error < 0.5 * mean_value
2.8.0,Predict the output and uncertainty.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,testing batch size > 1
2.8.0,testing true values
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,load datasets
2.8.0,disable transformer
2.8.0,check train
2.8.0,check predict shape
2.8.0,check overfit
2.8.0,load datasets
2.8.0,disable transformer
2.8.0,check train
2.8.0,check predict shape
2.8.0,check overfit
2.8.0,load datasets
2.8.0,disable transformer
2.8.0,check train
2.8.0,check predict shape
2.8.0,check overfit
2.8.0,reload
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Check same predictions are made.
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Load trained model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload trained model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Reload trained model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload trained model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reloaded Trained Model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Check predictions match on random sample
2.8.0,3D Multivariate Gaussian base distribution
2.8.0,Check that reloaded model can sample from the distribution
2.8.0,Check that density estimation is same for reloaded model
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload trained model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload Trained Model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload Trained Model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload trained model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Check predictions match on random sample
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload trained model
2.8.0,Eval model on train
2.8.0,Check predictions match on random sample
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload trained model
2.8.0,Eval model on train
2.8.0,Check predictions match on random sample
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Reload trained model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Reload trained Model
2.8.0,Check predictions match on random sample
2.8.0,Eval model on train
2.8.0,Reload Trained Model
2.8.0,Check predictions match on random sample
2.8.0,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.8.0,Reload Trained Model
2.8.0,Check predictions match on original dataset
2.8.0,TODO: We need a cleaner usage example for this
2.8.0,Fit trained model
2.8.0,Check predictions match on random sample
2.8.0,Train the model on random sequences.  We aren't training long enough to
2.8.0,"really make it reliable, but I want to keep this test fast, and it should"
2.8.0,still be able to reproduce a reasonable fraction of input sequences.
2.8.0,Test it out.
2.8.0,https://github.com/diffqc/dqc/blob/742eb2576418464609f942def4fb7c3bbdc0cd82/dqc/test/test_xc.py#L15
2.8.0,check predict shape
2.8.0,check overfit
2.8.0,needs change
2.8.0,check predict shape
2.8.0,check overfit
2.8.0,reload
2.8.0,.8 to save resources for a difficult task
2.8.0,.2 to save resources for a difficult task
2.8.0,first iteration loss is around 50
2.8.0,The first pass of the transformation should be 0
2.8.0,Test sampling method
2.8.0,Test log_prob method (this method is used when inverse pass)
2.8.0,Output must be a Nth zero array since nothing is being learned yet
2.8.0,Featurize to assert for tests
2.8.0,Assert errors for sample method
2.8.0,Assert errors for log_prob method
2.8.0,atom features
2.8.0,Try without compression
2.8.0,"Outputs should be [mol1_vec, mol2_vec]"
2.8.0,atom features
2.8.0,Try with compression
2.8.0,"Outputs should be [mol1_vec, mol2_vec]"
2.8.0,get data
2.8.0,prepare batch (size 1)
2.8.0,initialize the model
2.8.0,get output
2.8.0,get data
2.8.0,prepare batch (size 1)
2.8.0,initialize the model
2.8.0,get output
2.8.0,get data
2.8.0,prepare batch (size 1)
2.8.0,initialize the model
2.8.0,get output
2.8.0,load sample dataset
2.8.0,initialize the model
2.8.0,overfit test
2.8.0,load sample dataset
2.8.0,initialize the model
2.8.0,overfit test
2.8.0,load sample dataset
2.8.0,initialize the model
2.8.0,fit the model
2.8.0,reload the model
2.8.0,index of pair features
2.8.0,number of pairs for each atom
2.8.0,atom features
2.8.0,pair features
2.8.0,Assigning tensorflow equivalent weights to torch layer
2.8.0,"Outputs should be [A, P]"
2.8.0,There are 4 atoms each of which have 75 atom features
2.8.0,There are 10 pairs with infinity distance and 14 pair features
2.8.0,4 atoms in total
2.8.0,10 pairs in total
2.8.0,10 pairs in total each with start/finish
2.8.0,There are 4 atoms each of which have 75 atom features
2.8.0,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.8.0,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.8.0,connections and accounting for symmetry.)
2.8.0,4 atoms in total
2.8.0,10 pairs in total
2.8.0,The center atom is self connected and to both neighbors so it appears
2.8.0,thrice. The canonical ranking used in MolecularFeaturizer means this
2.8.0,central atom is ranked last in ordering.
2.8.0,10 pairs in total each with start/finish
2.8.0,def test_weave_fit_simple_infinity_distance():
2.8.0,featurizer = WeaveFeaturizer(max_pair_distance=None)
2.8.0,"X = featurizer([""C"", ""CCC""])"
2.8.0,"y = np.array([0, 1.])"
2.8.0,"dataset = NumpyDataset(X, y)"
2.8.0,batch_size = 20
2.8.0,model = WeaveModel(
2.8.0,"1,"
2.8.0,"batch_size=batch_size,"
2.8.0,"mode='classification',"
2.8.0,"fully_connected_layer_sizes=[2000, 1000],"
2.8.0,"batch_normalize=True,"
2.8.0,batch_normalize_kwargs={
2.8.0,"""fused"": False,"
2.8.0,"""trainable"": True,"
2.8.0,"""renorm"": True"
2.8.0,"},"
2.8.0,learning_rate=0.0005)
2.8.0,"model.fit(dataset, nb_epoch=200)"
2.8.0,transformers = []
2.8.0,metric = Metric(
2.8.0,"roc_auc_score, np.mean, mode=""classification"")"
2.8.0,"scores = model.evaluate(dataset, [metric], transformers)"
2.8.0,assert scores['mean-roc_auc_score'] >= 0.9
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,load datasets
2.8.0,initialize model
2.8.0,overfit test
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Fit trained model
2.8.0,Predict the output and uncertainty.
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,xgboost test
2.8.0,fit trained model
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,lightgbm test
2.8.0,fit trained model
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,xgboost test
2.8.0,fit trained model
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,lightgbm test
2.8.0,fit trained model
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,xgboost test
2.8.0,fit trained model
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,lightgbm test
2.8.0,fit trained model
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,xgboost test
2.8.0,fit trained model
2.8.0,reload
2.8.0,check predictions match on test dataset
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,global setting
2.8.0,lightgbm test
2.8.0,fit trained model
2.8.0,reload
2.8.0,check predictions match on test dataset
2.8.0,eval model on test
2.8.0,prepare dataset
2.8.0,xgboost test
2.8.0,fit trained model
2.8.0,"If ES rounds are more than total epochs, it will never trigger"
2.8.0,Find the number of boosting rounds in the model
2.8.0,"If rounds boosted are less than total estimators, it means ES was triggered"
2.8.0,prepare dataset
2.8.0,lightgbm test
2.8.0,fit trained model
2.8.0,"If ES rounds are more than total epochs, it will never trigger"
2.8.0,Find the number of boosting rounds in the model
2.8.0,"If rounds ran are less than estimators, it means ES was triggered"
2.8.0,"For simplicity, let's assume both molecules have same number of"
2.8.0,atoms.
2.8.0,Creates a set of dummy features that contain the coordinate and
2.8.0,neighbor-list features required by the AtomicConvModel.
2.8.0,Creates a set of dummy features that contain the coordinate and
2.8.0,neighbor-list features required by the AtomicConvModel.
2.8.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.8.0,max num atoms instead of exact.
2.8.0,Cutoff in angstroms
2.8.0,arbitrary label
2.8.0,Run a fitting operation
2.8.0,Testing graphnet for a single graph
2.8.0,Testing for consistency
2.8.0,Testing with a batch of Graphs
2.8.0,"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
2.8.0,the above import raises a ModuleNotFoundError. It is safe to ignore it
2.8.0,since the below tests only run in an environment with pytorch installed.
2.8.0,TODO The test is skipped as FakeGraphGenerator has to be updated
2.8.0,to generate regression labels
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Fit trained model
2.8.0,Eval model on train/test
2.8.0,Fit trained model
2.8.0,Eval model on train/test
2.8.0,Fit trained model
2.8.0,Eval model on train/test
2.8.0,"Linear layers for making query, key, value"
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,No training has been done after reload
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,We have to set the gradient penalty very small because the generator's
2.8.0,"output is only a single number, so the default penalty would constrain"
2.8.0,it far too much.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,We have to set the gradient penalty very small because the generator's
2.8.0,"output is only a single number, so the default penalty would constrain"
2.8.0,it far too much.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,n_samples = 100
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Most weights should be close to zero.
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Most weights should be close to zero.
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Generate dummy dataset
2.8.0,Fit trained model
2.8.0,Predict the output and uncertainty.
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Check that predicting internal layers works.
2.8.0,Each epoch is a single step for this model
2.8.0,Create two models using the same model directory.
2.8.0,Check that they produce different results.
2.8.0,"Save a checkpoint from the first model and load it into the second one,"
2.8.0,and make sure they now match.
2.8.0,Train a model to overfit the dataset.
2.8.0,"Create an identical model, do a single step of fitting with restore=True,"
2.8.0,and make sure it got restored correctly.
2.8.0,Build a model that predicts uncertainty.
2.8.0,Fit the model and see if its predictions are correct.
2.8.0,Take a tiny step in the direction of s and see if the output changes by
2.8.0,the expected amount.
2.8.0,Load dataset and Models
2.8.0,call model.fit again to test multiple fit() calls
2.8.0,Set up tests.
2.8.0,def test_singletask_to_multitask_classification(self):
2.8.0,n_features = 10
2.8.0,n_tasks = 17
2.8.0,tasks = range(n_tasks)
2.8.0,# Define train dataset
2.8.0,n_train = 100
2.8.0,"X_train = np.random.rand(n_train, n_features)"
2.8.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.8.0,w_train = np.ones_like(y_train)
2.8.0,"ids_train = [""C""] * n_train"
2.8.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.8.0,"X_train, y_train, w_train, ids_train)"
2.8.0,# Define test dataset
2.8.0,n_test = 10
2.8.0,"X_test = np.random.rand(n_test, n_features)"
2.8.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.8.0,w_test = np.ones_like(y_test)
2.8.0,"ids_test = [""C""] * n_test"
2.8.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.8.0,"X_test, y_test, w_test, ids_test)"
2.8.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.8.0,def model_builder(model_dir):
2.8.0,sklearn_model = LogisticRegression()
2.8.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.8.0,multitask_model = dc.models.SingletaskToMultitask(
2.8.0,"tasks, model_builder)"
2.8.0,# Fit trained model
2.8.0,multitask_model.fit(train_dataset)
2.8.0,multitask_model.save()
2.8.0,# Eval multitask_model on train/test
2.8.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.8.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.8.0,Generate data
2.8.0,Cleanup
2.8.0,Finetune model weights should not match before loading from pretrained model
2.8.0,Finetune model weights should match after loading from pretrained model
2.8.0,"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
2.8.0,the above import raises a ModuleNotFoundError. It is safe to ignore it
2.8.0,since the below tests only run in an environment with pytorch installed.
2.8.0,Test for the init function of FerminetModel class
2.8.0,Testing ionic initialization
2.8.0,Testing whether error throws up when spin is wrong
2.8.0,Testing the spin values
2.8.0,Testing ionic initialization
2.8.0,Test for the evaluate_hf_solution function of FerminetModel class
2.8.0,"The solution should be of the shape (number of electrons, number of electrons)"
2.8.0,Test for the init function of FerminetModel class
2.8.0,Testing ionic initialization
2.8.0,Test for the init function of FerminetModel class
2.8.0,Testing ionic initialization
2.8.0,Test for the init function of FerminetModel class
2.8.0,Testing ionic initialization
2.8.0,Test for the init function of FerminetModel class
2.8.0,Testing ionic initialization
2.8.0,Train the model while logging the validation ROC AUC.
2.8.0,Parse the log to pull out the AUC scores.
2.8.0,The last reported score should match the current performance of the model.
2.8.0,The highest recorded score should match get_best_score().
2.8.0,Reload the save model and confirm that it matches the best logged score.
2.8.0,Make sure get_best_score() still works when save_dir is not specified
2.8.0,3D Multivariate Gaussian base distribution
2.8.0,Must be float32 for RealNVP
2.8.0,Tests a simple flow of one RealNVP layer.
2.8.0,log likelihoods should be negative
2.8.0,# Fit model
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,x and y are the same tensor (equivalent at every element)
2.8.0,the pairwise inner product of the rows in x and y will always be 1
2.8.0,"the output tensor will be of shape (5,5)"
2.8.0,each row in x1 is orthogonal to each row in x2
2.8.0,the pairwise inner product of the rows in x and y will always be 0
2.8.0,"the output tensor will be of shape (256,256)"
2.8.0,"Creating a second layer should produce different results, since it has"
2.8.0,different random weights.
2.8.0,But evaluating the first layer again should produce the same result as before.
2.8.0,index of pair features
2.8.0,number of pairs for each atom
2.8.0,atom features
2.8.0,pair features
2.8.0,"Outputs should be [A, P]"
2.8.0,atom features
2.8.0,Try without compression
2.8.0,"Outputs should be [mol1_vec, mol2_vec)"
2.8.0,Try with compression
2.8.0,"Outputs should be [mol1_vec, mol2_vec)"
2.8.0,atom features
2.8.0,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.8.0,Gaussian histograms expands into 11 Gaussian buckets.
2.8.0,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.8.0,TODO What should shape[1] be?  It's not documented.
2.8.0,"n_atoms = 4  # In CCC and C, there are 4 atoms"
2.8.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.8.0,"Creating a second layer should produce different results, since it has"
2.8.0,different random weights.
2.8.0,But evaluating the first layer again should produce the same result as before.
2.8.0,"Creating a second layer should produce different results, since it has"
2.8.0,different random weights.
2.8.0,But evaluating the first layer again should produce the same result as before.
2.8.0,"Creating a second layer should produce different results, since it has"
2.8.0,different random weights.
2.8.0,But evaluating the first layer again should produce the same result as before.
2.8.0,"TODO What should the output shape be?  It's not documented, and there"
2.8.0,are no other test cases for it.
2.8.0,"Creating a second layer should produce different results, since it has"
2.8.0,different random weights.
2.8.0,But evaluating the first layer again should produce the same result as before.
2.8.0,"Creating a second layer should produce different results, since it has"
2.8.0,different random weights.
2.8.0,But evaluating the first layer again should produce the same result as before.
2.8.0,"Recall that the DAG layer expects a MultiConvMol as input,"
2.8.0,"so the ""batch"" is a pooled set of atoms from all the"
2.8.0,"molecules in the batch, just as it is for the graph conv."
2.8.0,This means that n_atoms is the batch-size
2.8.0,dropout_switch = False
2.8.0,dropout_switch
2.8.0,TODO(rbharath): What is the shape of outputs supposed to be?
2.8.0,"I'm getting (7, 30) here. Where does 7 come from??"
2.8.0,TODO(rbharath): We need more documentation about why
2.8.0,these numbers work.
2.8.0,"By setting the `box_size` to effectively zero, the result should only contain `nan`."
2.8.0,Check that layer has three trainable parameters.
2.8.0,Check when `box_size` is of wrong dimensionality.
2.8.0,Check when `inputs` is of wrong length.
2.8.0,Create random local node representations and global graph representations
2.8.0,Compute similarity scores using the discriminator
2.8.0,Check if the output has the correct shape and dtype
2.8.0,total_n_atoms = 4
2.8.0,n_atom_feat = 4
2.8.0,"atom_feat = np.random.rand(total_n_atoms, n_atom_feat)"
2.8.0,Embeddings and results from Tensorflow implementation
2.8.0,Weights and Embeddings from Tensorflow implementation
2.8.0,init parameters
2.8.0,generate features for testing
2.8.0,index of pair features
2.8.0,atom features
2.8.0,pair features
2.8.0,tensors for torch layer
2.8.0,assigning tensorflow layer weights to torch layer
2.8.0,Create a dataset and an input function for processing it.
2.8.0,Create a dataset and an input function for processing it.
2.8.0,Generate dummy dataset
2.8.0,Dataset of SMILES strings for testing SeqToSeq models.
2.8.0,Train the model on random sequences. We aren't training long enough to
2.8.0,"really make it reliable, but I want to keep this test fast, and it should"
2.8.0,still be able to reproduce a reasonable fraction of input sequences.
2.8.0,Test it out.
2.8.0,Check that it got at least a quarter of them correct.
2.8.0,Initialize the model
2.8.0,Fit the Model
2.8.0,Fit trained model
2.8.0,Eval model on test
2.8.0,Eval model on train
2.8.0,Fit trained model
2.8.0,Eval model on test
2.8.0,Fit trained model
2.8.0,Eval model on test
2.8.0,Fit trained model
2.8.0,Eval model on test
2.8.0,Fit trained model
2.8.0,Eval model on test
2.8.0,Each epoch is a single step for this model
2.8.0,Create two models using the same model directory.
2.8.0,Check that they produce different results.
2.8.0,"Save a checkpoint from the first model and load it into the second one,"
2.8.0,and make sure they now match.
2.8.0,Train a model to overfit the dataset.
2.8.0,"Create an identical model, do a single step of fitting with restore=True,"
2.8.0,and make sure it got restored correctly.
2.8.0,Build a model that predicts uncertainty.
2.8.0,Fit the model and see if its predictions are correct.
2.8.0,Take a tiny step in the direction of s and see if the output changes by
2.8.0,the expected amount.
2.8.0,Load dataset and Models
2.8.0,call model.fit again to test multiple fit() calls
2.8.0,There are 4 atoms each of which have 75 atom features
2.8.0,There are 10 pairs with infinity distance and 14 pair features
2.8.0,4 atoms in total
2.8.0,10 pairs in total
2.8.0,10 pairs in total each with start/finish
2.8.0,There are 4 atoms each of which have 75 atom features
2.8.0,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.8.0,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.8.0,connections and accounting for symmetry.)
2.8.0,4 atoms in total
2.8.0,10 pairs in total
2.8.0,The center atom is self connected and to both neighbors so it appears
2.8.0,thrice. The canonical ranking used in MolecularFeaturizer means this
2.8.0,central atom is ranked last in ordering.
2.8.0,10 pairs in total each with start/finish
2.8.0,Note: Following are some changes
2.8.0,compared to the TensorFlow unit test:
2.8.0,1. Changed nb_epoch to 300.
2.8.0,2. Increased the learning_rate to 0.0003.
2.8.0,Note: This needs to be inspected in future to understand low score as compared to a score of 0.9 in tensorflow unit test.
2.8.0,Note: Following are some changes
2.8.0,compared to the TensorFlow unit test:
2.8.0,1. Changed nb_epoch to 400.
2.8.0,2. Reduced the number of data points to 2.
2.8.0,3. Increased batch_size to 20.
2.8.0,4. Increased the learning_rate to 0.0003.
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Note: This needs to be inspected in future to understand low score.
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Note: Compared to the tensorflow unit test increased the nb_epoch to 600.
2.8.0,Eval model on train
2.8.0,Note: This needs to be inspected in future to understand low score as compared to a score of 0.8 in tensorflow unit test.
2.8.0,Create input tensor with values within full_atom_feature_dims
2.8.0,Create input tensor with values within full_bond_feature_dims
2.8.0,Compute the global graph representation
2.8.0,Compute positive and negative scores
2.8.0,Check if the loss is a scalar and has the correct dtype
2.8.0,Check if the loss is a scalar and non-negative
2.8.0,Train the model on random sequences.  We aren't training long enough to
2.8.0,"really make it reliable, but I want to keep this test fast, and it should"
2.8.0,still be able to reproduce a reasonable fraction of input sequences.
2.8.0,Test it out.
2.8.0,Check that it got at least a quarter of them correct.
2.8.0,Test it out.
2.8.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.8.0,"few steps of training to make sure nothing crashes, then check that the"
2.8.0,results are at least internally consistent.
2.8.0,Get Data
2.8.0,Check Shape
2.8.0,Check number of parameters
2.8.0,Eval model on train
2.8.0,load sample dataset
2.8.0,initialize the model
2.8.0,fit the model
2.8.0,reload the model
2.8.0,atom features
2.8.0,Gaussian histograms expands into 11 Gaussian buckets.
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,overfit test
2.8.0,test on a small MoleculeNet dataset
2.8.0,load datasets
2.8.0,initialize models
2.8.0,edges logits used during training
2.8.0,nodes logits used during training
2.8.0,edges logits
2.8.0,nodes logits
2.8.0,For training
2.8.0,For sample generation
2.8.0,Define the dense layers
2.8.0,"Ignoring type. For TorchModel, loss is a required argument but HuggingFace computes"
2.8.0,"loss during the forward iteration, removing the need for a loss function."
2.8.0,FIXME Transformers library has an api like AutoModel.from_pretrained. It allows to
2.8.0,initialise and create a model instance directly without requiring a class instance initialisation step.
2.8.0,"To use `load_from_pretrained` in DeepChem, we need to follow a two step process"
2.8.0,of initialising class instance and then loading weights via `load_from_pretrained`.
2.8.0,y is None during predict
2.8.0,Main training loop.
2.8.0,Report progress and write checkpoints.
2.8.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0,Report final results.
2.8.0,Invoke the model.
2.8.0,Apply tranformers and record results.
2.8.0,Concatenate arrays to create the final results.
2.8.0,copy the input graph to avoid in-place operations
2.8.0,"FIXME For pretraining task, both model2d and model3d but the super class"
2.8.0,"can't handle two models for contrastive learning, hence we pass only model2d"
2.8.0,torch's one-hot encoding works with integer data types.
2.8.0,"We convert labels to integer, one-hot encode and convert it back to float"
2.8.0,for making it suitable to loss function
2.8.0,graphs = [[
2.8.0,graph_data.to_dgl_graph().to(self.device) for graph_data in row
2.8.0,] for row in inputs]
2.8.0,convert the GraphData objects to DGL graphs
2.8.0,"TODO Ideally, we should use a lr schedule but we need to update lr_scheduler.step() method"
2.8.0,in ModularTorchModel.fit_generator to accept a metric.
2.8.0,"self._lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(self._pytorch_optimizer,"
2.8.0,mode='min')
2.8.0,Initialize buffers
2.8.0,Accumulate statistics for Fisher matrices
2.8.0,Initialize buffers
2.8.0,p_grad_mat is of output_dim * input_dim
2.8.0,inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
2.8.0,we always put gradient w.r.t weight in [0]
2.8.0,and w.r.t bias in [1]
2.8.0,do kl clip
2.8.0,TODO Explain in detail what the four outcompes are
2.8.0,The bond and rev bond have even and odd ids respectively.
2.8.0,FIXME This layer is similar to DMPNNEncoderLayer and they
2.8.0,must be unified.
2.8.0,Shared weight matrix across depths (default)
2.8.0,Except reverse bond its-self(w) ! \sum_{k\in N(u) \ w}
2.8.0,"FIXME When input_layer is none, for the first iteration of message passing, we should ideally"
2.8.0,be using different weight matrix since message will be of shape num_bonds x f_bonds_dim
2.8.0,"in the first iteration and in the subsequent iterations, it will be num_bonds x hidden_size"
2.8.0,FIXME We assume that we are using a hidden layer to transform the initial atom message
2.8.0,and bond messages to hidden dimension size.
2.8.0,self.atom_messages is False
2.8.0,Note: Elementwise affine has to be consistent with the pre-training phase
2.8.0,multi-headed attention
2.8.0,support no residual connection in MTBlock.
2.8.0,atom input to atom output
2.8.0,bond to atom
2.8.0,atom input to bond output
2.8.0,bond input to bond output
2.8.0,Inputs
2.8.0,Noise Input
2.8.0,Data Input
2.8.0,Data Inputs
2.8.0,Conditional Input
2.8.0,Conditional Inputs
2.8.0,Generators
2.8.0,Discriminators
2.8.0,Forward pass through generators
2.8.0,Forward pass through discriminators
2.8.0,Compute loss functions
2.8.0,Compute the weighted errors
2.8.0,Create learnable weights for the generators and discriminators.
2.8.0,Compute the weighted errors
2.8.0,Add an entropy term to the loss.
2.8.0,"Every call to fit_generator() will increment global_step, but we only"
2.8.0,"want it to get incremented once for the entire batch, so record the"
2.8.0,value and keep resetting it.
2.8.0,Train the discriminator.
2.8.0,Train the generator.
2.8.0,Write checkpoints and report progress.
2.8.0,Write out final results.
2.8.0,PyTorch layers require input and output channels as parameter
2.8.0,"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
2.8.0,"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
2.8.0,initializing layer bias with nn.init gives mypy typecheck error
2.8.0,using the following workaround
2.8.0,residual blocks can only be used when successive layers have the same output shape
2.8.0,Used for converting edges back to their original shape
2.8.0,Compute mean edge features for each node by dst_index (each node
2.8.0,"receives information from edges which have that node as its destination,"
2.8.0,hence the computation uses dst_index to aggregate information)
2.8.0,holding bi-directional edges in case of undirected graphs
2.8.0,coonverting edge features to its original shape
2.8.0,Input
2.8.0,Shared weight matrix across depths (default):
2.8.0,For messages hidden states
2.8.0,For atom hidden states
2.8.0,num_atoms x hidden_size
2.8.0,num_molecules x hidden_size
2.8.0,concat global features
2.8.0,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0,"Shape (N_atoms, M_nbrs)"
2.8.0,Number of grid cells
2.8.0,TODO(rbharath): Support batching
2.8.0,"Shape (n_cells, ndim)"
2.8.0,"List of length N_atoms, each element of different length uniques_i"
2.8.0,"List of length N_atoms, each element of different length uniques_i"
2.8.0,"List of length N_atoms, each a tensor of shape"
2.8.0,"(uniques_i, ndim)"
2.8.0,Add phantom atoms that exist far outside the box
2.8.0,"List of length N_atoms, each of shape (1, ndim)"
2.8.0,TODO(rbharath): How does distance need to be modified here to
2.8.0,account for periodic boundary conditions?
2.8.0,List of length N_atoms each of shape (M_nbrs)
2.8.0,"N_atoms elts of size (M_nbrs,) each"
2.8.0,"Shape (N_atoms, 1)"
2.8.0,Find M_nbrs atoms closest to each cell
2.8.0,"Shape (n_cells, M_nbrs)"
2.8.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0,"conditions, so does wrapround. O(constant)"
2.8.0,"Shape (n_cells, n_nbr_cells)"
2.8.0,"Shape (N_atoms, n_nbr_cells)"
2.8.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.8.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.8.0,"List of length N_atoms, each element length uniques_i"
2.8.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.8.0,element removed to remove self from list of neighbors. Need to verify
2.8.0,this holds more broadly or come up with robust alternative.
2.8.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.8.0,Shape (N_atoms*n_cells)
2.8.0,"Shape (n_cells, N_atoms)"
2.8.0,Find k atoms closest to this cell.
2.8.0,"Tensor of shape (n_cells, M_nbrs)"
2.8.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0,"Shape (N_atoms*n_cells, 1) after tile"
2.8.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.8.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.8.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.8.0,the cube.
2.8.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.8.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.8.0,"Tile (a, a, a, b, b, b, etc.)"
2.8.0,"Tile (a, b, c, a, b, c, ...)"
2.8.0,No other forget biases supported right now.
2.8.0,Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
2.8.0,Define the layers
2.8.0,Create the final layers
2.8.0,Add another value(~-Inf) to prevent error in softmax
2.8.0,Model using this layer must set `pad_batches=True`
2.8.0,"z = torch.mm(h, self.U) + self.b"
2.8.0,create a boolean mask for each partition
2.8.0,partition the input tensor using the masks
2.8.0,Case when >2 inputs are passed
2.8.0,Loop over the remaining convolution layers
2.8.0,Apply the current layer to the outputs from the previous layer
2.8.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.8.0,and embeddings of atom j(both gone through a hidden layer)
2.8.0,"for atom i, sum the influence from all other atom j in the molecule"
2.8.0,Construct internal trainable weights
2.8.0,Weight matrix and bias matrix required to compute new atom layer from the previous atom layer
2.8.0,Weight matrix and bias matrix required to compute new atom layer from the previous pair layer
2.8.0,Weight matrix and bias matrix required to compute new pair layer from the previous atom layer
2.8.0,Weight matrix and bias matrix required to compute new pair layer from the previous pair layer
2.8.0,flake8: noqa
2.8.0,Converting the input to torch tensors
2.8.0,"AA is a tensor with shape[total_num_atoms,n_hidden_AA]"
2.8.0,"PA is a tensor with shape[total number of pairs,n_hidden_PA]"
2.8.0,Split the PA tensor according to the 'pair_split' tensor
2.8.0,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.8.0,normalization
2.8.0,"PP is a tensor with shape [total number of pairs,n_hidden_PP]"
2.8.0,Integrate the Cross Layer Mapping inside the Global Message Passing
2.8.0,Message Passing operation
2.8.0,Update function f_u
2.8.0,Message Passing operation
2.8.0,Teacher forcing: Feed the target as the next input
2.8.0,Without teacher forcing: use its own predictions as the next input
2.8.0,Initializing the first layer (first layer has different dims than others)
2.8.0,filling the weights with xavier uniform method for the linear weights and random assignment for the bias
2.8.0,Calculating one-electron feature's average
2.8.0,temporary lists containing each electron's embeddings which will be torch.stack on the end
2.8.0,Calculating two-electron feature's average
2.8.0,"initialized weights with torch.zeros, torch.eye and using xavier init."
2.8.0,temporary list to stack upon electrons axis at the end
2.8.0,Integrate the Cross Layer Mapping inside the Local Message Passing
2.8.0,Message Passing 1
2.8.0,Message Passing 2
2.8.0,Aggregation
2.8.0,Update function f_u
2.8.0,Output Module
2.8.0,import torch.nn as nn
2.8.0,creating one and two electron features
2.8.0,setting the fermient layer and fermient envelope layer batch size to be that of the current batch size of the model. This enables for vectorized calculations of hessians and jacobians.
2.8.0,using functorch to calcualte hessian and jacobian in one go
2.8.0,using index tensors to index out the hessian elemennts corresponding to the same variable (cross-variable derivatives are ignored)
2.8.0,"doing all the calculation and detaching from graph to save memory, which allows larger batch size"
2.8.0,cloning self.input which will serve as the new input for the vectorized functions.
2.8.0,lambda function for calculating the log of absolute value of the wave function.
2.8.0,using jacrev for the jacobian and jacrev twice for to calculate the hessian. The functorch's hessian function if directly used does not give stable results.
2.8.0,making the batch size temporarily as 1 for the vectorization of hessian and jacobian.
2.8.0,Initialization for ionic molecules
2.8.0,hook function below is an efficient way modifying the gradients on the go rather than looping
2.8.0,using non-local variables as a means of parameter passing
2.8.0,the move function calculates the energy of sampled electrons and samples new set of electrons (does not calculate loss)
2.8.0,clipping local energies which are away 5 times the variance from the median
2.8.0,using the sampled electrons from the electron sampler for bacward pass and modifying gradients
2.8.0,going through each step of random walk and calculating the modified gradients with local energies
2.8.0,embedding node features
2.8.0,convolutional layer
2.8.0,pooling
2.8.0,for n_tasks == 1 case
2.8.0,mypy check is ignored for global_features as it is not a default attribute
2.8.0,of GraphData. It is created during runtime using **kwargs.
2.8.0,mapping from bond index to the index of the atom (where the bond is coming from)
2.8.0,"mapping from bond index to concat(in_atom, bond) features"
2.8.0,mapping from atom index to list of indicies of incoming bonds
2.8.0,mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
2.8.0,zero padded at the end
2.8.0,get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
2.8.0,padded with -1 at the end
2.8.0,mapping from bond index to the index of the atom (where the bond if going to)
2.8.0,mapping from atom index to list of indicies of incoming bonds
2.8.0,get maximum number of incoming bonds
2.8.0,Make number of incoming bonds equal to maximum number of bonds.
2.8.0,This is done by appending -1 to fill remaining space at each atom indices.
2.8.0,mapping from bond index to the index of the reverse bond
2.8.0,get encoder
2.8.0,get input size for ffn
2.8.0,get output size for ffn
2.8.0,get ffn
2.8.0,Steps to get `molecules_unbatch_key`:
2.8.0,1. Get the tensor containing the indices of first atoms of each molecule
2.8.0,2. Get the tensor containing number of atoms of each molecule
2.8.0,by taking the difference between consecutive indices.
2.8.0,3. Convert the tensor to a list.
2.8.0,num_molecules x (enc_hidden + global_features_size)
2.8.0,ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
2.8.0,"atom feature matrix with shape [number of atoms, number of features]"
2.8.0,concatenated feature vector which contains concatenation of initial atom and bond features
2.8.0,mapping from atom index to list of indicies of incoming bonds
2.8.0,mapping that maps bond index to 'array of indices of the bonds'
2.8.0,incoming at the initial atom of the bond (excluding the reverse bonds)
2.8.0,array of global molecular features
2.8.0,maximum number of incoming bonds in the batch
2.8.0,generate concatenated feature vector and mappings
2.8.0,pad all mappings to maximum number of incoming bonds in the batch
2.8.0,the hidden size here is the output size of last layer in mol_atom_from_atom_ffn and mol_atom_from_bond_ffn components.
2.8.0,it is necessary that aforementioned components produces output tensor of same size.
2.8.0,"In training mode, we return atom level aggregated output and bond level aggregated output."
2.8.0,The training has an additional objective which ensures that atom and bond level aggregated outputs
2.8.0,are similar to each other apart from the objective of making the aggregated output closer to each
2.8.0,other.
2.8.0,"FIXME In the above step, we initialize modular torch model but"
2.8.0,something is missing here. The attribute loss from TorchModel gets assigned `loss_func`
2.8.0,by super class initialization in ModularTorchModel but here we reinitialize it.
2.8.0,in eval mode.
2.8.0,"Also adding features, this is optional"
2.8.0,FIXME I am rewriting restore because the restore method in parent class
2.8.0,does not restore layers which are not components. This restore method
2.8.0,can restore an full model.
2.8.0,may mess with loading pretrained weights
2.8.0,remove relu for the last layer
2.8.0,"reshapes node_representation to (num_nodes, num_layers * emb_dim)"
2.8.0,"for supervised tasks, add prediction head"
2.8.0,unsupervised tasks do not need a pred head
2.8.0,negative contexts are obtained by shifting the indicies of context embeddings
2.8.0,"sample x distinct nodes to be masked, based on mask rate. But"
2.8.0,will sample at least 1 node
2.8.0,create mask node label by copying node feature of mask node
2.8.0,modify the original node feature of the masked node
2.8.0,zeros are meant to represent the masked features. This is distinct from the
2.8.0,"original implementation, where the masked features are represented by the"
2.8.0,the last feature token 119.
2.8.0,link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L241
2.8.0,create mask edge labels by copying edge features of edges that are connected to
2.8.0,mask nodes
2.8.0,create mask edge labels by copying edge features of the edges connected to
2.8.0,the mask nodes
2.8.0,edge ordering is such that two directions of a single
2.8.0,"edge occur in pairs, so to get the unique undirected"
2.8.0,"edge indices, we take every 2nd edge index from list"
2.8.0,modify the original edge features of the edges connected to the mask nodes
2.8.0,zeros are meant to represent the masked features. This is distinct from the
2.8.0,"original implementation, where the masked features are represented by the"
2.8.0,the last feature token 4.
2.8.0,link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L268
2.8.0,"sample x distinct edges to be masked, based on mask rate. But"
2.8.0,will sample at least 1 edge
2.8.0,"during sampling, we only pick the 1st direction of a particular"
2.8.0,edge pair
2.8.0,create ground truth edge features for the edges that correspond to
2.8.0,the masked indices
2.8.0,"created new masked edge_attr, where both directions of the masked"
2.8.0,edges have masked edge type. For message passing in gcn
2.8.0,append the 2nd direction of the masked edges
2.8.0,zeros are meant to represent the masked features. This is distinct from the
2.8.0,"original implementation, where the masked features are represented by 0s and"
2.8.0,an additional mask feature
2.8.0,link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/bio/util.py#L101
2.8.0,"Take the entire graph, but can be modified to take a subgraph of k-hops from the root node"
2.8.0,Get node idx between root and the inner diameter l1
2.8.0,Get node idx between root and the outer diameter l2
2.8.0,takes a ring around the root node outside of l1 and inside of l2
2.8.0,"Get indices of overlapping nodes between substruct and context, WRT context ordering"
2.8.0,Decide first number of GAT layers
2.8.0,set2set doubles the dimensionality of the embedding
2.8.0,n_tasks is Optional[int] while argument 2 of nn.Linear has to be of type int
2.8.0,original implementation also includes an option if not using a separate encoder:
2.8.0,loss = sup_loss + local_unsup_loss * self.learning_rate
2.8.0,Below functions were taken from DeepChem TextCNN tensorflow implementation
2.8.0,Transform SMILES sequence to integers
2.8.0,Maximum length is expanded to allow length variation during train and inference
2.8.0,'_' served as delimiter and padding
2.8.0,Initialize common characters as keys
2.8.0,Include space to avoid extra keys
2.8.0,"For 'Cl', 'Br', etc."
2.8.0,"Character not recognized, add to extra_keys"
2.8.0,Add all extra_keys to char_dict
2.8.0,Skip all spaces
2.8.0,"For 'Cl', 'Br', etc."
2.8.0,Padding with '_'
2.8.0,We convert deepchem.feat.GraphData to a PyG graph and then
2.8.0,batch it.
2.8.0,The default_generator method returns an array of dc.feat.GraphData objects
2.8.0,"nested inside a list. To access the nested array of graphs, we are"
2.8.0,indexing by 0 here.
2.8.0,Do a simple greedy search.
2.8.0,Do a beam search with length normalization.
2.8.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.8.0,This candidate sequence has already been terminated
2.8.0,Consider all possible tokens we could add to this candidate sequence.
2.8.0,flake8:noqa
2.8.0,get DTNNEmbedding
2.8.0,get DTNNSteps
2.8.0,get DTNNGather
2.8.0,get Final Linear Layer
2.8.0,"pair_edges is of shape (2, N)"
2.8.0,number of atoms in each molecule
2.8.0,index of pair features
2.8.0,Get starting pair atoms
2.8.0,number of pairs for each atom
2.8.0,atom features
2.8.0,pair features
2.8.0,pretransformation
2.8.0,aggregation
2.8.0,post-transformation
2.8.0,The model predicts unnormalized probabilities for each class and task
2.8.0,"print (logits, proba)"
2.8.0,FIXME self.loss_func is an incorrect argument for TorchModel.loss because
2.8.0,it performs more than computing loss
2.8.0,FIXME This line is not needed as loss is computed inside the call to loss_func
2.8.0,Main training loop.
2.8.0,"Execute the loss function, accumulating the gradients."
2.8.0,Report progress and write checkpoints.
2.8.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0,Report final results.
2.8.0,Ensure weights for both models are built.
2.8.0,Rename and delete older files.
2.8.0,Select a device.
2.8.0,W&B logging
2.8.0,"If `wandb=True` and no logger is provided, initialize default logger"
2.8.0,Setup and initialize W&B logging
2.8.0,Update config with KerasModel params
2.8.0,Main training loop.
2.8.0,"Execute the loss function, accumulating the gradients."
2.8.0,Report progress and write checkpoints.
2.8.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0,Report final results.
2.8.0,Invoke the model.
2.8.0,Apply tranformers and record results.
2.8.0,Concatenate arrays to create the final results.
2.8.0,Compute the gradients.
2.8.0,Save the checkpoint to a file.
2.8.0,Rename and delete older files.
2.8.0,Ensure weights for both models are built.
2.8.0,True will be assigned up successful training attempt
2.8.0,create new model
2.8.0,to avoid flake8 E125/yapf incompatibility
2.8.0,generate input
2.8.0,train model
2.8.0,generate sample
2.8.0,check how many valid molecules were created and add to list
2.8.0,finally test if there was at least one valid training session
2.8.0,as the model structure improves this should become more and more strict
2.8.0,import torch.nn as nn
2.8.0,import torch.nn.functional as F
2.8.0,Testing Shapes
2.8.0,Testing values
2.8.0,Dense1 is a list of dense layers
2.8.0,Testing Values
2.8.0,Testing Shapes with TF Model Output
2.8.0,Testing Shapes
2.8.0,Testing Values
2.8.0,testing first convolution layer
2.8.0,dense1 layer - list of dense layers
2.8.0,dense2 layer - single dense layer
2.8.0,testing rest of the convolution layer
2.8.0,dense1 layer - list of dense layers
2.8.0,dense2 layer - single dense layer
2.8.0,Loading input tensors
2.8.0,Testing output
2.8.0,Testing MultiConvolution Layer
2.8.0,Testing First Convolution Layer
2.8.0,dense1 layer - list of dense layers
2.8.0,dense2 layer - single dense layer
2.8.0,Testing rest of the Multi convolution layer
2.8.0,dense1 layer - list of dense layers
2.8.0,dense2 layer - single dense layer
2.8.0,Testing Aggregation Layer
2.8.0,Loading input tensors
2.8.0,Testing output
2.8.0,9: number of atoms
2.8.0,6: number of bonds
2.8.0,3: number of molecules
2.8.0,logits for class 1
2.8.0,logits for class 2
2.8.0,since pretraining is a self-supervision task where labels are generated during
2.8.0,"preparing batch, we mock _prepare_batch_for_pretraining to set all labels to 0."
2.8.0,The test here is checking whether the model predict 0's after overfitting.
2.8.0,preparing for test by setting 0 labels
2.8.0,arranging test - preparing dataset
2.8.0,acting - tests
2.8.0,asserting
2.8.0,arranging for tests
2.8.0,checking weights don't match before restore
2.8.0,norm layers and cached zero vectors have constant weights
2.8.0,restoring model
2.8.0,checking matching of weights after restore
2.8.0,asserting that weights are not same before reloading
2.8.0,"notm and bias layers have constant weights, hence they are not checked"
2.8.0,acting - loading pretrained weights
2.8.0,asserting that weight matches after loading
2.8.0,"For simplicity, let's assume both molecules have same number of"
2.8.0,atoms.
2.8.0,Creates a set of dummy features that contain the coordinate and
2.8.0,neighbor-list features required by the AtomicConvModel.
2.8.0,Creates a set of dummy features that contain the coordinate and
2.8.0,neighbor-list features required by the AtomicConvModel.
2.8.0,"helper classes that depend on torch, they need to be in the try/catch block"
2.8.0,Define the dense layers
2.8.0,Define the dense layers
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,No training has been done after reload
2.8.0,We have to set the gradient penalty very small because the generator's
2.8.0,"output is only a single number, so the default penalty would constrain"
2.8.0,it far too much.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,We have to set the gradient penalty very small because the generator's
2.8.0,"output is only a single number, so the default penalty would constrain"
2.8.0,it far too much.
2.8.0,See if it has done a plausible job of learning the distribution.
2.8.0,Create dummy data
2.8.0,Call forward
2.8.0,Asserts
2.8.0,Create dummy data
2.8.0,Call forward
2.8.0,"Since the penalty is a squared norm of the gradients minus 1, multiplied by a constant,"
2.8.0,it should be non-negative
2.8.0,Create pretrained model
2.8.0,Create finetuning model
2.8.0,Load pretrained model
2.8.0,check weights match
2.8.0,all keys should match
2.8.0,keys should not match
2.8.0,"In a batched graph, atoms and bonds belonging to different graphs are differentiated"
2.8.0,"via scopes. In the below scenario, we assume a batched mol graph of three molecules"
2.8.0,"with 10 atoms, 20 bonds. On the 10 atoms, we consider the first 3 belonging to mol1,"
2.8.0,next 3 belonging to mol2 and remaining 4 belonging to mol4.
2.8.0,"Hence, the atom scope is [(0, 3), (3, 3), (6, 4)]. Similarly, for bonds, we have first 5 bonds belonging to mol1, next 4 to mol2 and remaining 11 to bond3."
2.8.0,"TODO Write tests for undirected = True case, currently fails. for this case, we have"
2.8.0,"to generate inputs (a2b, b2a, b2revb) for undirected graph."
2.8.0,The shapes should match the earlier shapes because message passing only updates node features.
2.8.0,The following variables are utility variables used during message passing to compute neighbors. Here we are asserting that MTBlock layer is not modifying these variables.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Load mini log-solubility dataset.
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Fit trained model
2.8.0,Eval model on train
2.8.0,Load dataset
2.8.0,Intiliaze torch TextCNN
2.8.0,Copy other layer weights
2.8.0,Run prediction
2.8.0,Pretraining in MLM mode
2.8.0,Pretraining in Multitask Regression Mode
2.8.0,test regression
2.8.0,test multitask regression
2.8.0,test classification
2.8.0,logit scores
2.8.0,check weights match
2.8.0,all keys values should match
2.8.0,new model's model attribute is an entirely new model initiated by AutoModel.load_from_pretrained
2.8.0,and hence it should have a different identifier
2.8.0,testing a simple scenario where each embedding corresponds to an unique graph
2.8.0,"here embeddings 0, 1 belong to a scope, 2, 3 to another scope and 4, 5 to another scope"
2.8.0,"thus, we sill have 3 graphs"
2.8.0,Porting the weights from TF to PyTorch
2.8.0,task 0 layer 0
2.8.0,task 0 layer 1
2.8.0,task 0 output layer
2.8.0,task 1 layer 0
2.8.0,task 1 layer 1
2.8.0,task 1 output layer
2.8.0,task 1 adapter 0
2.8.0,task 1 adapter 1
2.8.0,Generate dummy dataset
2.8.0,Generate dummy dataset
2.8.0,model is None when reloading a model
2.8.0,Some scikit-learn models don't use weights.
2.8.0,flake8: ignore
2.8.0,This will not work for ModularTorchModel as it is directly uses `loss_func` to compute loss.
2.8.0,flake8:noqa
2.8.0,flake8: noqa
2.8.0,"torch.nn.module prefix has no ending dot, while xt prefix has"
2.8.0,neural network xc functional of GGA (receives the density and grad as inputs)
2.8.0,"densinfo.value: (*BD, nr)"
2.8.0,"densinfo.grad : (*BD, nr, 3)"
2.8.0,"collect the total density (n), spin density (xi), and normalized gradients (s)"
2.8.0,normalize the gradient
2.8.0,decide how to transform the density to be the input of nn
2.8.0,get the neural network output
2.8.0,QR decomposition's solution is not unique in a way that every column
2.8.0,can be multiplied by -1 and it still a solution
2.8.0,"So, to remove the non-uniqueness, we will make the sign of the sum"
2.8.0,positive.
2.8.0,construct the rotation parameters
2.8.0,calculate the orthogonal orbital
2.8.0,"orb: (*, nao, norb)"
2.8.0,the orbital becomes the coefficients while params is all zeros (no rotation)
2.8.0,GDBT doesn't support multi-output(task)
2.8.0,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.8.0,retrain model to whole data using best n_estimators * 1.25
2.8.0,GDBT doesn't support multi-output(task)
2.8.0,########################################
2.8.0,Deprecation warnings for XGBoostModel
2.8.0,########################################
2.8.0,flake8: noqa
2.8.0,-*- coding: utf-8 -*-
2.8.0,Assigning featurizer if not user defined
2.8.0,loading datasets
2.8.0,Assembling train and valid datasets
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,Building tensorflow MultitaskDNN model
2.8.0,Building tensorflow robust MultitaskDNN model
2.8.0,Building scikit logistic regression model
2.8.0,Transform fingerprints to IRV features
2.8.0,Building tensorflow IRV model
2.8.0,Building scikit random forest model
2.8.0,Building scikit learn Kernel SVM model
2.8.0,Building xgboost classification model
2.8.0,Remove token for paddings
2.8.0,Building scikit random forest model
2.8.0,Building scikit learn Kernel Ridge Regression model
2.8.0,Building scikit learn Kernel Ridge Regression model
2.8.0,Building xgboost regression model
2.8.0,Loading hyperparameters
2.8.0,num positive/negative ligands
2.8.0,Set batch sizes for network
2.8.0,Model structure
2.8.0,Traning settings
2.8.0,Fit trained model
2.8.0,Evaluating low data model
2.8.0,-*- coding: utf-8 -*-
2.8.0,Assigning featurizer if not user defined
2.8.0,loading datasets
2.8.0,
2.8.0,Note by @XericZephyr. Reason why I spun off this function:
2.8.0,1. Some model needs dataset information.
2.8.0,2. It offers us possibility to **cache** the dataset
2.8.0,"if the featurizer runs very slow, e.g., GraphConv."
2.8.0,2+. The cache can even happen at Travis CI to accelerate
2.8.0,CI testing.
2.8.0,
2.8.0,loading datasets
2.8.0,!/usr/bin/env python2
2.8.0,-*- coding: utf-8 -*-
2.8.0,"TODO For this dataset and model, the R2-scores are less than 0.3."
2.8.0,This has to be improved.
2.8.0,See: https://github.com/deepchem/deepchem/issues/2776
2.8.0,TODO: Check for this
2.8.0,Download files if they don't exist
2.8.0,Featurize the KINASE dataset
2.8.0,Shuffle the training data
2.8.0,Apply transformations
2.8.0,TIMING
2.8.0,transformers = [
2.8.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.8.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.8.0,dataset=train_dataset)]
2.8.0,Set shard size low to avoid memory problems.
2.8.0,TIMING
2.8.0,TIMING
2.8.0,Set some global variables up top
2.8.0,Featurize KAGGLE dataset
2.8.0,TIMING
2.8.0,TIMING
2.8.0,Build the path to the dataset on disk.
2.8.0,Try to reload cached datasets.
2.8.0,Create the dataset
2.8.0,Split and transform the dataset.
2.8.0,. clinical trial toxicity (or absence of toxicity)
2.8.0,. FDA approval status.
2.8.0,Read the zip file
2.8.0,Get the labels from filenames
2.8.0,Download files if they don't exist
2.8.0,Featurizing datasets
2.8.0,Missing entry removal
2.8.0,Shuffle the training data
2.8.0,Apply transformations
2.8.0,TIMING
2.8.0,TODO: Check if anything needs to be added
2.8.0,Featurize the FACTORS dataset
2.8.0,Shuffle the training data
2.8.0,Apply transformations
2.8.0,TIMING
2.8.0,dict of accepted featurizers for this dataset
2.8.0,modify the returned dicts for your dataset
2.8.0,Names of supported featurizers
2.8.0,dict of accepted transformers
2.8.0,dict of accepted splitters
2.8.0,names of supported splitters
2.8.0,Warning message about this template
2.8.0,Featurize mydataset
2.8.0,Get DeepChem data directory if needed
2.8.0,Check for str args to featurizer and splitter
2.8.0,Reload from disk
2.8.0,First type of supported featurizers
2.8.0,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.8.0,Changer loader to match featurizer and data file type
2.8.0,Featurize dataset
2.8.0,Initialize transformers
2.8.0,"get pdb and sdf filenames, labels and pdbids"
2.8.0,load and featurize each complex
2.8.0,Extract locations of data
2.8.0,Extract labels
2.8.0,Lines have format
2.8.0,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.8.0,"The base-10 logarithm, -log kd/pk"
2.8.0,"def load_pcba_146(featurizer='ECFP',"
2.8.0,"split='random',"
2.8.0,"reload=True,"
2.8.0,"data_dir=None,"
2.8.0,"save_dir=None,"
2.8.0,**kwargs):
2.8.0,return load_pcba_dataset(
2.8.0,"featurizer=featurizer,"
2.8.0,"split=split,"
2.8.0,"reload=reload,"
2.8.0,"assay_file_name=""pcba_146.csv.gz"","
2.8.0,"data_dir=data_dir,"
2.8.0,"save_dir=save_dir,"
2.8.0,**kwargs)
2.8.0,"def load_pcba_2475(featurizer='ECFP',"
2.8.0,"split='random',"
2.8.0,"reload=True,"
2.8.0,"data_dir=None,"
2.8.0,"save_dir=None,"
2.8.0,**kwargs):
2.8.0,return load_pcba_dataset(
2.8.0,"featurizer=featurizer,"
2.8.0,"split=split,"
2.8.0,"reload=reload,"
2.8.0,"assay_file_name=""pcba_2475.csv.gz"","
2.8.0,"data_dir=data_dir,"
2.8.0,"save_dir=save_dir,"
2.8.0,**kwargs)
2.8.0,Range of optimization
2.8.0,We know from guard above that this is an int/float
2.8.0,Specify logfile
2.8.0,Make logdir if it doesn't exist.
2.8.0,setup range
2.8.0,Stores all results
2.8.0,Store all model references so we don't have to reload
2.8.0,Stores all model locations
2.8.0,"param values are always float in BO, so this line converts float to int"
2.8.0,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.8.0,Record hyperparameters
2.8.0,We have already evaluated the model for these hyperparameters.
2.8.0,Add it on to the information needed for the constructor
2.8.0,Not all models have nb_epoch
2.8.0,Some models autosave
2.8.0,Record performances
2.8.0,Store all results
2.8.0,Store reference to model
2.8.0,GPGO maximize performance by default
2.8.0,set performance to its negative value for minimization
2.8.0,Demarcating internal function for readability
2.8.0,execute GPGO
2.8.0,FIXME: Incompatible types in assignment
2.8.0,Let's fetch the model with the best parameters
2.8.0,Compare best model to default hyperparameters
2.8.0,Record hyperparameters
2.8.0,Return default hyperparameters
2.8.0,Construction dictionary mapping hyperparameter names to values
2.8.0,"mypy test throws error, so ignoring it in try"
2.8.0,Not all models have nb_epoch
2.8.0,Some models autosave
2.8.0,arbitrarily return last model
2.8.0,hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
2.8.0,"mypy test throws error, so ignoring it in try"
2.8.0,Not all models have nb_epoch
2.8.0,Some models autosave
2.8.0,Update best validation score so far
2.8.0,"if `hyp_str` not in `all_scores`, store it in `all_scores`"
2.8.0,arbitrarily return last model trained
2.8.0,"If callable, sample it for a maximum n times"
2.8.0,flake8: noqa
2.8.0,"2 model variants, 1 results.txt file"
2.8.0,Generate dummy dataset
2.8.0,Generate dummy dataset
2.8.0,These are per-example multiplier
2.8.0,Test that 2 parameters were optimized
2.8.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.8.0,Generate dummy dataset
2.8.0,Define nb_epoch in hyperparam_search function call
2.8.0,"max_iter model variants, 1 results.txt file"
2.8.0,Generate dummy dataset
2.8.0,Generate dummy dataset
2.8.0,These are per-example multiplier
2.8.0,Test that 2 parameters were optimized
2.8.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.8.0,Generate dummy dataset
2.8.0,Define nb_epoch in hyperparam_search function call
2.8.0,Generate dummy dataset
2.8.0,Generate dummy dataset
2.8.0,These are per-example multiplier
2.8.0,Test that 2 parameters were optimized
2.8.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.8.0,Generate dummy dataset
2.8.0,Have the worker threads generate the rollouts for this iteration.
2.8.0,Perform optimization.
2.8.0,Build the inputs and run the optimizer.
2.8.0,Update the number of steps taken so far and perform checkpointing.
2.8.0,Merge all the rollouts into a single set of arrays.
2.8.0,Iterate slices.
2.8.0,Generate the rollout.
2.8.0,Compute an estimate of the reward for the rest of the episode.
2.8.0,Compute the discounted rewards and advantages.
2.8.0,Convert the actions to one-hot.
2.8.0,Rearrange the states into the proper set of arrays.
2.8.0,Return the processed arrays.
2.8.0,Training loop.
2.8.0,Do checkpointing.
2.8.0,Generate the rollout.
2.8.0,Compute an estimate of the reward for the rest of the episode.
2.8.0,Compute the discounted rewards and advantages.
2.8.0,"Record the actions, converting to one-hot if necessary."
2.8.0,Rearrange the states into the proper set of arrays.
2.8.0,Build the inputs and apply gradients.
2.8.0,Assume all arrays are float32.
2.8.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.8.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.8.0,"game).  The average reward for any bet is slightly negative, so the best"
2.8.0,strategy is to walk away.
2.8.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0,Optimize it.
2.8.0,"It should have learned that the expected value is very close to zero, and that the best"
2.8.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.8.0,top actions).
2.8.0,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.8.0,get the same result.
2.8.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0,The environment just has a constant state.
2.8.0,The policy includes a single recurrent layer.
2.8.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.8.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.8.0,"On the first call, the initial state should be all zeros."
2.8.0,It should still be zeros since we didn't save it last time.
2.8.0,It should be different now.
2.8.0,This should be the same as the previous one.
2.8.0,"Now we reset it, so we should get the same result as initially."
2.8.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.8.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.8.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.8.0,at all.  Using hindsight makes it much easier.
2.8.0,A simple policy with two hidden layers.
2.8.0,Optimize it.
2.8.0,Try running it a few times and see if it succeeds.
2.8.0,The state consists of two numbers: a current value and a target value.
2.8.0,The policy just needs to learn to output the target value (or at least
2.8.0,move toward it).
2.8.0,A simple policy with no hidden layers.
2.8.0,Optimize it.
2.8.0,Try running it and see if it reaches the target
2.8.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.8.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.8.0,"game).  The average reward for any bet is slightly negative, so the best"
2.8.0,strategy is to walk away.
2.8.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0,Optimize it.
2.8.0,"It should have learned that the expected value is very close to zero, and that the best"
2.8.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.8.0,top actions).
2.8.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.8.0,get the same result.
2.8.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0,The environment just has a constant state.
2.8.0,The policy includes a single recurrent layer.
2.8.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.8.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.8.0,"On the first call, the initial state should be all zeros."
2.8.0,It should still be zeros since we didn't save it last time.
2.8.0,It should be different now.
2.8.0,This should be the same as the previous one.
2.8.0,"Now we reset it, so we should get the same result as initially."
2.8.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.8.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.8.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.8.0,at all.  Using hindsight makes it much easier.
2.8.0,A simple policy with two hidden layers.
2.8.0,Optimize it.
2.8.0,Try running it a few times and see if it succeeds.
2.8.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0,Randomize who goes first
2.8.0,Illegal move -- the square is not empty
2.8.0,Move X
2.8.0,Did X Win
2.8.0,Did O Win
2.8.0,"default channels are ""conda-forge"" and ""omnia"""
2.8.0,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.8.0.pre,Build a nightly package by default.
2.8.0.pre,Environment-specific dependencies.
2.8.0.pre,get the version from deepchem/__init__.py
2.8.0.pre,nightly version : .devYearMonthDayHourMinute
2.8.0.pre,Force to add `.dev` if `--release` option isn't passed when building
2.8.0.pre,!/usr/bin/env python3
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Datasets and models used in the benchmark test
2.8.0.pre,"irv, rf, rf_regression should be assigned manually"
2.8.0.pre,Evaluate performances with different training set fraction
2.8.0.pre,Datasets and models used in the benchmark test
2.8.0.pre,Uncomment the two lines below if hyper_parameters are provided
2.8.0.pre,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.8.0.pre,hyper_parameters = pickle.load(f)
2.8.0.pre,!/usr/bin/env python3
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Datasets and models used in the benchmark test
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Get Metric
2.8.0.pre,Fit trained model
2.8.0.pre,Fit trained model
2.8.0.pre,Set numpy seed
2.8.0.pre,##Load data###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Featurize Kinase dataset
2.8.0.pre,##Load data###
2.8.0.pre,num_trials = 5
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,Force matplotlib to not use any Xwindows backend.
2.8.0.pre,##Load data###
2.8.0.pre,the histogram of the data
2.8.0.pre,Set numpy seed
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,num_trials = 5
2.8.0.pre,Set some global variables up top
2.8.0.pre,Fit trained model
2.8.0.pre,Featurize PCBA dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Fit trained model
2.8.0.pre,Load sider models now
2.8.0.pre,Load sweetlead dataset now. Pass in dataset object and appropriate
2.8.0.pre,transformers to predict functions
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,##Load data###
2.8.0.pre,"n_estimators=100, max_features=int(num_features/3),"
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,Fit trained model
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Load tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,This example shows how to use Pandas to load data directly
2.8.0.pre,without using a CSVLoader object. This may be useful if you
2.8.0.pre,want the flexibility of processing your data with Pandas
2.8.0.pre,directly.
2.8.0.pre,Now let's convert from a dataset back to a pandas dataframe
2.8.0.pre,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.8.0.pre,Featurize FACTORS dataset
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,Force matplotlib to not use any Xwindows backend.
2.8.0.pre,##Load data###
2.8.0.pre,the histogram of the data
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Load QM7 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Fit trained model
2.8.0.pre,Fit trained model
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Fit trained model
2.8.0.pre,Load QM8 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Fit trained model
2.8.0.pre,Set numpy seed
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,Load ChEMBL dataset
2.8.0.pre,Fit models
2.8.0.pre,Do setup required for tf/keras models
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,DeepCrystal Technologies 2017 - Patrick Hop
2.8.0.pre,MIT License - have fun!!
2.8.0.pre,Set to higher values to get better numbers
2.8.0.pre,======================================================================
2.8.0.pre,"Run Benchmarks {GC-DNN, SVR, RF}"
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Only for debug!
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Do setup required for tf/keras models
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Get Metric
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load Delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load MUV dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Evaluate train/test scores
2.8.0.pre,Load MUV data
2.8.0.pre,Build model
2.8.0.pre,Fit trained model
2.8.0.pre,Evaluate train/test scores
2.8.0.pre,Extract active site
2.8.0.pre,Featurize ligand
2.8.0.pre,Default for CircularFingerprint
2.8.0.pre,Featurize pocket
2.8.0.pre,Note broadcast operation
2.8.0.pre,Compute labels for pockets
2.8.0.pre,Some complexes have labels but no PDB files. Filter these manually
2.8.0.pre,Some of the ligand-names are of form (FMN ox). Use regex
2.8.0.pre,to merge into form (FMN-ox)
2.8.0.pre,Filter if missing PDB files
2.8.0.pre,Load PDBBind dataset
2.8.0.pre,Define featurizers
2.8.0.pre,Featurize Dataset
2.8.0.pre,########################################################## DEBUG
2.8.0.pre,########################################################## DEBUG
2.8.0.pre,For stable runs
2.8.0.pre,Fit trained model
2.8.0.pre,4-fold splits
2.8.0.pre,num positive/negative ligands
2.8.0.pre,10 trials on test-set
2.8.0.pre,Define metric
2.8.0.pre,Get supports on test-set
2.8.0.pre,Compute accuracies
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Batch size of models
2.8.0.pre,graph_model = dc.nn.SequentialGraph(n_feat)
2.8.0.pre,Fit trained model
2.8.0.pre,Test model
2.8.0.pre,Join information for all tasks.
2.8.0.pre,4-fold splits
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Define metric
2.8.0.pre,Get supports on test-set
2.8.0.pre,Compute accuracies
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Test model
2.8.0.pre,Join information for all tasks.
2.8.0.pre,4-fold splits
2.8.0.pre,num positive/negative ligands
2.8.0.pre,10 trials on test-set
2.8.0.pre,Define metric
2.8.0.pre,Get supports on test-set
2.8.0.pre,Compute accuracies
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Test model
2.8.0.pre,Join information for all tasks.
2.8.0.pre,4-fold splits
2.8.0.pre,num positive/negative ligands
2.8.0.pre,10 trials on test-set
2.8.0.pre,Define metric
2.8.0.pre,Get supports on test-set
2.8.0.pre,Compute accuracies
2.8.0.pre,Train model on support
2.8.0.pre,Test model
2.8.0.pre,Join information for all tasks.
2.8.0.pre,4-fold splits
2.8.0.pre,num positive/negative ligands
2.8.0.pre,10 trials on test-set
2.8.0.pre,Define metric
2.8.0.pre,Get supports on test-set
2.8.0.pre,Compute accuracies
2.8.0.pre,Train model on support
2.8.0.pre,Test model
2.8.0.pre,Join information for all tasks.
2.8.0.pre,Set some global variables up top
2.8.0.pre,Featurize Tox21 dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Set some global variables up top
2.8.0.pre,Featurize Tox21 dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Load MUV dataset
2.8.0.pre,Featurize MUV dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Load MUV dataset
2.8.0.pre,Featurize MUV dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Featurize SIDER dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Featurize SIDER dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Load the data.
2.8.0.pre,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.8.0.pre,sparse: most tasks do not include data for most molecules.  It also is very
2.8.0.pre,"unbalanced: there are many more negatives than positives.  For each task,"
2.8.0.pre,create a list of alternating positives and negatives so each batch will have
2.8.0.pre,equal numbers of both.
2.8.0.pre,Define a MetaLearner describing the learning problem.
2.8.0.pre,Run meta-learning on 80% of the tasks.
2.8.0.pre,Validate on the remaining tasks.
2.8.0.pre,4-fold splits
2.8.0.pre,10 positive/negative ligands
2.8.0.pre,10 trials on test-set
2.8.0.pre,Sample supports without replacement (all pos/neg should be different)
2.8.0.pre,Define metric
2.8.0.pre,Get supports on test-set
2.8.0.pre,Compute accuracies
2.8.0.pre,Train model on support
2.8.0.pre,Test model
2.8.0.pre,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.8.0.pre,Join information for all tasks.
2.8.0.pre,4-fold splits
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Define metric
2.8.0.pre,Get supports on test-set
2.8.0.pre,Compute accuracies
2.8.0.pre,Train model on support
2.8.0.pre,Test model
2.8.0.pre,Join information for all tasks.
2.8.0.pre,replace with your own scratch directory
2.8.0.pre,Number of conformations in each file increases exponentially.
2.8.0.pre,Start with a smaller dataset before continuing. Use all of them
2.8.0.pre,for production
2.8.0.pre,"'ani_gdb_s03.h5',"
2.8.0.pre,"'ani_gdb_s04.h5',"
2.8.0.pre,"'ani_gdb_s05.h5',"
2.8.0.pre,"'ani_gdb_s06.h5',"
2.8.0.pre,"'ani_gdb_s07.h5',"
2.8.0.pre,'ani_gdb_s08.h5'
2.8.0.pre,Extract the data
2.8.0.pre,Print the data
2.8.0.pre,self-interaction energies taken from
2.8.0.pre,https://github.com/isayev/ANI1_dataset README
2.8.0.pre,flush once more at the end
2.8.0.pre,"# For production, set nb_epoch to 100+"
2.8.0.pre,"print(""Train scores"")"
2.8.0.pre,print(train_scores)
2.8.0.pre,"print(""Minimization of a single test set structure:"")"
2.8.0.pre,"print(model.minimize_structure(coords, atomic_nums))"
2.8.0.pre,Written by Roman Zubatyuk and Justin S. Smith
2.8.0.pre,Modified by Yutong Zhao to make python2 compatible
2.8.0.pre,opening file
2.8.0.pre,print(store_loc)
2.8.0.pre,print(type(v[0]))
2.8.0.pre,print(k)
2.8.0.pre,print(path)
2.8.0.pre,Number of conformations in each file increases exponentially.
2.8.0.pre,Start with a smaller dataset before continuing. Use all of them
2.8.0.pre,for production
2.8.0.pre,Extract the data
2.8.0.pre,NOTE THE RENAMING:
2.8.0.pre,Note sensitivity = recall
2.8.0.pre,Load nci dataset
2.8.0.pre,Featurize nci dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Set some global variables up top
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load hiv dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load hiv dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Load delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit models
2.8.0.pre,Load delaney dataset
2.8.0.pre,Fit models
2.8.0.pre,TODO: Once improved splitting API is merged in swap to simpler API
2.8.0.pre,The return values are dc.data.Dataset objects so we need to extract
2.8.0.pre,the ids
2.8.0.pre,TODO once improved splitting API is merged in swap out for simpler
2.8.0.pre,API
2.8.0.pre,The return values are dc.data.Dataset objects so we need to extract
2.8.0.pre,the ids
2.8.0.pre,Fit trained model
2.8.0.pre,Load SIDER dataset
2.8.0.pre,Featurize SIDER dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Featurize permeability dataset
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit trained model
2.8.0.pre,TODO: This should be swapped for simpler splitter API once that's merged in.
2.8.0.pre,The return values are dc.data.Dataset objects so we need to extract
2.8.0.pre,the ids
2.8.0.pre,Only for debug!
2.8.0.pre,Load clintox dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load clintox dataset
2.8.0.pre,Fit models
2.8.0.pre,Do setup required for tf/keras models
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,#############################################################################
2.8.0.pre,## save dataset
2.8.0.pre,#############################################################################
2.8.0.pre,## load datasets
2.8.0.pre,load sweetfda
2.8.0.pre,load aact
2.8.0.pre,## fixup smiles for matching
2.8.0.pre,return smiles
2.8.0.pre,map original smiles to converted smiles
2.8.0.pre,"## join dataframes, index on smiles"
2.8.0.pre,map original smiles back
2.8.0.pre,## fill all nan with 0
2.8.0.pre,## construct datasets
2.8.0.pre,store in new datasets
2.8.0.pre,## save datasets
2.8.0.pre,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.8.0.pre,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.8.0.pre,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.8.0.pre,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.8.0.pre,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.8.0.pre,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.8.0.pre,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.8.0.pre,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.8.0.pre,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.8.0.pre,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.8.0.pre,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.8.0.pre,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.8.0.pre,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.8.0.pre,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.8.0.pre,For stable runs
2.8.0.pre,Fit trained model
2.8.0.pre,For stable runs
2.8.0.pre,Fit trained model
2.8.0.pre,For stable runs
2.8.0.pre,Fit trained model
2.8.0.pre,TODO The below line should be fixes
2.8.0.pre,See: https://github.com/deepchem/deepchem/issues/2373
2.8.0.pre,model.save()
2.8.0.pre,transformers = [
2.8.0.pre,"dc.trans.LogTransformer(transform_X=True),"
2.8.0.pre,"dc.trans.NormalizationTransformer(transform_y=True,"
2.8.0.pre,dataset=train_dataset)]
2.8.0.pre,Featurize UV dataset
2.8.0.pre,##Load data###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Set numpy seed
2.8.0.pre,##Load data###
2.8.0.pre,##Create model###
2.8.0.pre,Use R2 classification metric
2.8.0.pre,Only use for final evaluation
2.8.0.pre,Force matplotlib to not use any Xwindows backend.
2.8.0.pre,##Load data###
2.8.0.pre,the histogram of the data
2.8.0.pre,##Load data###
2.8.0.pre,###################################################### DEBUG
2.8.0.pre,###################################################### DEBUG
2.8.0.pre,Load HOPV dataset
2.8.0.pre,Fit models
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Batch size of models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load HOPV dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load HOPV dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load HOPV dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Only for debug!
2.8.0.pre,Load HOPV dataset
2.8.0.pre,Fit models
2.8.0.pre,Fit trained model
2.8.0.pre,Load TOXCAST dataset
2.8.0.pre,Featurize TOXCAST dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,Fit trained model
2.8.0.pre,Processing of ToxCast data
2.8.0.pre,Author - Aneesh Pappu
2.8.0.pre,Loading dataframes and editing indices
2.8.0.pre,Loop through rows of hitc matrix and replace codes with smiles strings
2.8.0.pre,get corresponding casn
2.8.0.pre,get corresponding smiles
2.8.0.pre,write to cell
2.8.0.pre,Tidy up and write to csv
2.8.0.pre,TODO(rbharath): Check that this operation is differentiable.
2.8.0.pre,The number of cells which we should theoretically have
2.8.0.pre,The number of cells which we should theoretically have
2.8.0.pre,"Each atom neighbors tensor should be (k, ndim) shaped."
2.8.0.pre,The number of cells which we should theoretically have
2.8.0.pre,TODO(rbharath): The test below only checks that shapes work out.
2.8.0.pre,Need to do a correctness implementation vs. a simple CPU impl.
2.8.0.pre,The number of cells which we should theoretically have
2.8.0.pre,TODO(rbharath): The test below only checks that shapes work out.
2.8.0.pre,Need to do a correctness implementation vs. a simple CPU impl.
2.8.0.pre,The number of cells which we should theoretically have
2.8.0.pre,TODO(rbharath): The test below only checks that shapes work out.
2.8.0.pre,Need to do a correctness implementation vs. a simple CPU impl.
2.8.0.pre,TODO(rbharath): Commenting this out due to weird segfaults
2.8.0.pre,def test_vina_generate_conformers(self):
2.8.0.pre,"""""""Test that Vina Model can generate conformers"""""""
2.8.0.pre,data_dir = os.path.dirname(os.path.realpath(__file__))
2.8.0.pre,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.8.0.pre,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.8.0.pre,max_protein_atoms = 3500
2.8.0.pre,max_ligand_atoms = 100
2.8.0.pre,"print(""Loading protein file"")"
2.8.0.pre,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.8.0.pre,protein_Z = pad_array(
2.8.0.pre,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.8.0.pre,max_protein_atoms)
2.8.0.pre,"print(""Loading ligand file"")"
2.8.0.pre,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.8.0.pre,ligand_Z = pad_array(
2.8.0.pre,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.8.0.pre,max_ligand_atoms)
2.8.0.pre,Associate each atom with cell it belongs to. O(N*n_cells)
2.8.0.pre,"Shape (n_cells, k)"
2.8.0.pre,"Shape (N, 1)"
2.8.0.pre,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0.pre,"conditions, so does wrapround. O(constant)"
2.8.0.pre,"Shape (n_cells, 26)"
2.8.0.pre,"Shape (N, 26)"
2.8.0.pre,"coords of shape (N, ndim)"
2.8.0.pre,"Shape (N, 26, k, ndim)"
2.8.0.pre,"Shape (N, 26, k)"
2.8.0.pre,"Shape (N, 26, k)"
2.8.0.pre,"Shape (N, 26, k, ndim)"
2.8.0.pre,"For smaller systems especially, the periodic boundary conditions can"
2.8.0.pre,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.8.0.pre,make sure duplicate neighbors are ignored?
2.8.0.pre,TODO(rbharath): How does distance need to be modified here to
2.8.0.pre,account for periodic boundary conditions?
2.8.0.pre,"Shape (N, 26, k)"
2.8.0.pre,"Shape (N, 26*k)"
2.8.0.pre,TODO(rbharath): This will cause an issue with duplicates!
2.8.0.pre,"Shape (N, M)"
2.8.0.pre,"N elts of size (M,) each"
2.8.0.pre,"Shape (N, 26*k)"
2.8.0.pre,"N elts of size (26*k,) each"
2.8.0.pre,"N elts of size (M,) each"
2.8.0.pre,"Shape (N, M)"
2.8.0.pre,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.8.0.pre,"N tensors of shape (n_cells, 1)"
2.8.0.pre,"Shape (N*n_cells, 1) after tile"
2.8.0.pre,"List of N tensors of shape (n_cells, 1)"
2.8.0.pre,Lists of length N
2.8.0.pre,Lists of length n_cells
2.8.0.pre,Get indices of k atoms closest to each cell point
2.8.0.pre,TODO(rbharath): tf.stack for tf 1.0
2.8.0.pre,"Tensor of shape (n_cells, k, ndim)"
2.8.0.pre,atoms_in_cells = tf.stack(atoms_in_cells)
2.8.0.pre,"Tensor of shape (26, k, ndim)"
2.8.0.pre,"Reshape to (26*k, ndim)"
2.8.0.pre,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.8.0.pre,"Dists of shape (26*k, 1)"
2.8.0.pre,"Of shape (k, ndim)"
2.8.0.pre,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.8.0.pre,TODO(rbharath): Change this for tf 1.0
2.8.0.pre,"n_cells tensors of shape (N, 1)"
2.8.0.pre,"Shape (N*n_cells, 1) after tile"
2.8.0.pre,"List of n_cells tensors of shape (N, 1)"
2.8.0.pre,Lists of length n_cells
2.8.0.pre,Lists of length n_cells
2.8.0.pre,Get indices of k atoms closest to each cell point
2.8.0.pre,"n_cells tensors of shape (k, ndim)"
2.8.0.pre,"Tensor of shape (n_cells, k)"
2.8.0.pre,TODO(rbharath):
2.8.0.pre,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.8.0.pre,- Need to group closest atoms amongst cell neighbors
2.8.0.pre,- Need to do another top_k to find indices of closest neighbors.
2.8.0.pre,- Return N lists corresponding to neighbors for every atom.
2.8.0.pre,TODO(rbharath): Do we need to handle periodic boundary conditions
2.8.0.pre,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.8.0.pre,"looking for 26 neighbors, which isn't right for boundary cells in"
2.8.0.pre,the cube.
2.8.0.pre,Number of neighbors of central cube in 3-space is
2.8.0.pre,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.8.0.pre,TODO(rbharath)
2.8.0.pre,n_cells = int(cells.get_shape()[0])
2.8.0.pre,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.8.0.pre,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.8.0.pre,"Tile (a, a, a, b, b, b, etc.)"
2.8.0.pre,"Tile (a, b, c, a, b, c, ...)"
2.8.0.pre,"Lists of n_cells tensors of shape (N, 1)"
2.8.0.pre,Lists of length n_cells
2.8.0.pre,Lists of length n_cells
2.8.0.pre,Get indices of k atoms closest to each cell point
2.8.0.pre,"n_cells tensors of shape (26,)"
2.8.0.pre,TODO(rbharath): Make this handle minibatches
2.8.0.pre,"Shape (N_protein+N_ligand, 3)"
2.8.0.pre,"Shape (N_protein+N_ligand,)"
2.8.0.pre,"Shape (N_protein+N_ligand, M)"
2.8.0.pre,"Shape (N_protein+N_ligand,)"
2.8.0.pre,"Shape (N_protein+N_ligand, 3)"
2.8.0.pre,"Shape (N_protein+N_ligand,)"
2.8.0.pre,"Shape (N_protein+N_ligand, M)"
2.8.0.pre,"Shape (N_protein+N_ligand, M, 3)"
2.8.0.pre,"Shape (N_protein+N_ligand, M)"
2.8.0.pre,"Shape (N_protein+N_ligand, M, 3)"
2.8.0.pre,"Shape (N_protein+N_ligand, M)"
2.8.0.pre,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.8.0.pre,"Shape (N_protein+N_ligand, M)"
2.8.0.pre,"Shape (N_protein+N_ligand, M)"
2.8.0.pre,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.8.0.pre,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.8.0.pre,computing free-energy. This implementation currently uses all interaction
2.8.0.pre,terms. Not sure if this makes a difference.
2.8.0.pre,"Shape (N_protein+N_ligand, M)"
2.8.0.pre,Shape () -- scalar
2.8.0.pre,Keep track of the layers
2.8.0.pre,"For graphical layers, add connectivity placeholders"
2.8.0.pre,Add layer to the layer list
2.8.0.pre,Keep track of the layers
2.8.0.pre,Create graph topology and x
2.8.0.pre,Keep track of the layers
2.8.0.pre,Whether or not we have used the GraphGather layer yet
2.8.0.pre,Update new value of x
2.8.0.pre,Update new value of x
2.8.0.pre,Update new value of x
2.8.0.pre,Get train function
2.8.0.pre,Initialize
2.8.0.pre,################################################################### DEBUG
2.8.0.pre,self.test_label_placeholder = Input(
2.8.0.pre,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.8.0.pre,"name=""label_placeholder""))"
2.8.0.pre,self.test_weight_placeholder = Input(
2.8.0.pre,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.8.0.pre,"name=""weight_placeholder""))"
2.8.0.pre,TODO(rbharath): Should weights for the support be used?
2.8.0.pre,Support labels
2.8.0.pre,self.support_label_placeholder = Input(
2.8.0.pre,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.8.0.pre,"name=""support_label_placeholder""))"
2.8.0.pre,################################################################### DEBUG
2.8.0.pre,Generate dictionary elements for support
2.8.0.pre,Get graph information for test
2.8.0.pre,Generate dictionary elements for test
2.8.0.pre,Perform the optimization
2.8.0.pre,Create different support sets
2.8.0.pre,Get batch to try it out on
2.8.0.pre,"Train on support set, batch pair"
2.8.0.pre,Get featurization for test
2.8.0.pre,"Shape (n_test, n_feat)"
2.8.0.pre,Get featurization for support
2.8.0.pre,"Shape (n_support, n_feat)"
2.8.0.pre,Computes the inner part c() of the kernel
2.8.0.pre,(the inset equation in section 2.1.1 of Matching networks paper).
2.8.0.pre,Normalize
2.8.0.pre,TODO(rbharath): euclidean kernel is broken!
2.8.0.pre,elif self.similarity == 'euclidean':
2.8.0.pre,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.8.0.pre,"Note that gram matrix g has shape (n_test, n_support)"
2.8.0.pre,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.8.0.pre,https://arxiv.org/pdf/1606.04080v1.pdf
2.8.0.pre,"Computes softmax across axis 1, (so sums distances to support set for"
2.8.0.pre,each test entry) to get attention vector
2.8.0.pre,"Shape (n_test, n_support)"
2.8.0.pre,Weighted sum of support labels
2.8.0.pre,"Shape (n_support, 1)"
2.8.0.pre,pred is yhat in eqn (1) of Matching Networks.
2.8.0.pre,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.8.0.pre,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.8.0.pre,"Shape (n_test,)"
2.8.0.pre,Convert to logit space using inverse sigmoid (logit) function
2.8.0.pre,logit function: log(pred) - log(1-pred)
2.8.0.pre,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.8.0.pre,in Cross Entropy calculation.
2.8.0.pre,"Shape (n_test,)"
2.8.0.pre,Get scores
2.8.0.pre,Remove padded elements
2.8.0.pre,Get scores
2.8.0.pre,pred corresponds to prob(example == 1)
2.8.0.pre,Remove padded elements
2.8.0.pre,Get batches
2.8.0.pre,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.8.0.pre,multitask case with missing data...
2.8.0.pre,Join information for all tasks.
2.8.0.pre,TODO(rbharath): Find a way to get rid of this import?
2.8.0.pre,Extract model info
2.8.0.pre,Get graph topology for x
2.8.0.pre,Building outputs
2.8.0.pre,Set epsilon
2.8.0.pre,Initialize
2.8.0.pre,"Path to save checkpoint files, which matches the"
2.8.0.pre,replicated supervisor's default path.
2.8.0.pre,Create target inputs
2.8.0.pre,Get train function
2.8.0.pre,TODO(rbharath): I believe this is total amount of data
2.8.0.pre,Get graph information
2.8.0.pre,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.8.0.pre,the number of labeled data points in target_i. This is to normalize each task
2.8.0.pre,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.8.0.pre,Get other optimizer information
2.8.0.pre,TODO(rbharath): Figure out how to handle phase appropriately
2.8.0.pre,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.8.0.pre,"tensors of shape (batch_size,)"
2.8.0.pre,It's ok to divide by just the batch_size rather than the number of nonzero
2.8.0.pre,examples (effect averages out)
2.8.0.pre,Perform the optimization
2.8.0.pre,TODO(rbharath): Disabling saving for now to try to debug.
2.8.0.pre,run eval data through the model
2.8.0.pre,"Shape (n_samples, n_tasks)"
2.8.0.pre,Create target inputs
2.8.0.pre,TODO(rbharath): Find a way to get rid of this import?
2.8.0.pre,Obtain appropriate loss function
2.8.0.pre,Extract model info
2.8.0.pre,Get graph topology for x
2.8.0.pre,Raw logit outputs
2.8.0.pre,Set epsilon
2.8.0.pre,Initialize
2.8.0.pre,"Path to save checkpoint files, which matches the"
2.8.0.pre,replicated supervisor's default path.
2.8.0.pre,Create target inputs
2.8.0.pre,############################################################### DEBUG
2.8.0.pre,"print(""multitask classifier"")"
2.8.0.pre,"print(""feat"")"
2.8.0.pre,print(feat)
2.8.0.pre,############################################################### DEBUG
2.8.0.pre,Get train function
2.8.0.pre,TODO(rbharath): I believe this is total amount of data
2.8.0.pre,Get graph information
2.8.0.pre,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.8.0.pre,the number of labeled data points in target_i. This is to normalize each task
2.8.0.pre,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.8.0.pre,Get other optimizer information
2.8.0.pre,TODO(rbharath): Figure out how to handle phase appropriately
2.8.0.pre,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.8.0.pre,"tensors of shape (batch_size,)"
2.8.0.pre,Convert the labels into one-hot vector encodings.
2.8.0.pre,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.8.0.pre,un-softmaxed logits rather than softmax outputs.
2.8.0.pre,It's ok to divide by just the batch_size rather than the number of nonzero
2.8.0.pre,examples (effect averages out)
2.8.0.pre,Perform the optimization
2.8.0.pre,TODO(rbharath): Disabling saving for now to try to debug.
2.8.0.pre,run eval data through the model
2.8.0.pre,"Shape (n_samples, n_tasks)"
2.8.0.pre,run eval data through the model
2.8.0.pre,self.n_atoms = n_atoms
2.8.0.pre,Define the list of tensors to be used as topology
2.8.0.pre,Merge mol conv objects
2.8.0.pre,Generate dicts
2.8.0.pre,Define the list of tensors to be used as topology
2.8.0.pre,Extract atom numbers
2.8.0.pre,Generate dicts
2.8.0.pre,molecule * atom(graph) => step => features
2.8.0.pre,molecule * atom(graph) => step
2.8.0.pre,molecule * atom(graph) => step
2.8.0.pre,Define the list of tensors to be used as topology
2.8.0.pre,calculation orders for a batch of molecules
2.8.0.pre,padding atom features vector of each molecule with 0
2.8.0.pre,self.n_atoms = n_atoms
2.8.0.pre,Define the list of tensors to be used as topology
2.8.0.pre,Extract atom numbers
2.8.0.pre,Generate dicts
2.8.0.pre,self.n_atoms = n_atoms
2.8.0.pre,Define the list of tensors to be used as topology
2.8.0.pre,Extract atom numbers
2.8.0.pre,number of atoms in each molecule
2.8.0.pre,index of pair features
2.8.0.pre,number of pairs for each atom
2.8.0.pre,atom features
2.8.0.pre,pair features
2.8.0.pre,Generate dicts
2.8.0.pre,Load Tox21 dataset
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.8.0.pre,Fit trained model
2.8.0.pre,Fit models
2.8.0.pre,Batch size of models
2.8.0.pre,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.8.0.pre,Fit trained model
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,number positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply an attention lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,number positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply an attention lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,number positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply an attention lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Number of folds for split
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply a residual lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply a residual lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply a residual lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply a residual lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,Depth of attention module
2.8.0.pre,number positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,Apply an attention lstm layer
2.8.0.pre,Number of folds for split
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Number of features on conv-mols
2.8.0.pre,Define metric
2.8.0.pre,Train support model on train
2.8.0.pre,Add layers
2.8.0.pre,# Gather Projection
2.8.0.pre,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.8.0.pre,There should be 8 layers in graph_model
2.8.0.pre,assert len(graph_model.layers) == 6
2.8.0.pre,Add layers
2.8.0.pre,Need to add batch-norm separately to test/support due to differing
2.8.0.pre,shapes.
2.8.0.pre,Apply an attention lstm layer
2.8.0.pre,Gather Projection
2.8.0.pre,Add layers
2.8.0.pre,Need to add batch-norm separately to test/support due to differing
2.8.0.pre,shapes.
2.8.0.pre,Apply an attention lstm layer
2.8.0.pre,Gather Projection
2.8.0.pre,Degrees from 1 to max_deg inclusive
2.8.0.pre,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.8.0.pre,"Should have shape (?, deg)"
2.8.0.pre,"Shape of atom_features should be (?, n_feat)"
2.8.0.pre,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Save hyperparameters
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Save hyperparameters
2.8.0.pre,setup optimizer
2.8.0.pre,setup optimizer
2.8.0.pre,"print(""tasK: %d"" %task)"
2.8.0.pre,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.8.0.pre,"print(""scores"")"
2.8.0.pre,print(scores.size())
2.8.0.pre,"print(""task_label"")"
2.8.0.pre,print(task_label.size())
2.8.0.pre,"task_loss =  self.criterion(scores, task_label)"
2.8.0.pre,"print(""task_loss"")"
2.8.0.pre,print(task_loss.size())
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Save hyperparameters
2.8.0.pre,weight decay
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,Turns out there are valid cases where we don't want pad-batches
2.8.0.pre,on by default.
2.8.0.pre,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0.pre,Run training op.
2.8.0.pre,############################################################# TIMING
2.8.0.pre,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0.pre,Remove padded examples.
2.8.0.pre,Special case to handle singletasks.
2.8.0.pre,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0.pre,Remove padded examples.
2.8.0.pre,References
2.8.0.pre,Arguments
2.8.0.pre,Aliases.
2.8.0.pre,Aliases.
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,TODO(rbharath): This class does not yet have a
2.8.0.pre,"TensorGraph equivalent, but one may not be required."
2.8.0.pre,"Commented out for now, remove if OK."
2.8.0.pre,class AlternateWeaveLayer(WeaveLayer):
2.8.0.pre,""""""" Alternate implementation of weave module"
2.8.0.pre,"same variables, different graph structures"
2.8.0.pre,""""""""
2.8.0.pre,
2.8.0.pre,"def call(self, x, mask=None):"
2.8.0.pre,"""""""Execute this layer on input tensors."
2.8.0.pre,
2.8.0.pre,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.8.0.pre,
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,x: list
2.8.0.pre,list of Tensors of form described above.
2.8.0.pre,"mask: bool, optional"
2.8.0.pre,Ignored. Present only to shadow superclass call() method.
2.8.0.pre,
2.8.0.pre,Returns
2.8.0.pre,-------
2.8.0.pre,A: Tensor
2.8.0.pre,Tensor of atom_features
2.8.0.pre,P: Tensor
2.8.0.pre,Tensor of pair_features
2.8.0.pre,""""""""
2.8.0.pre,# Add trainable weights
2.8.0.pre,self.build()
2.8.0.pre,
2.8.0.pre,atom_features = x[0]
2.8.0.pre,pair_features = x[1]
2.8.0.pre,
2.8.0.pre,pair_split = x[2]
2.8.0.pre,atom_to_pair = x[4]
2.8.0.pre,
2.8.0.pre,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.8.0.pre,AA = self.activation(AA)
2.8.0.pre,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.8.0.pre,PA = self.activation(PA)
2.8.0.pre,"PA = tf.segment_sum(PA, pair_split)"
2.8.0.pre,
2.8.0.pre,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.8.0.pre,A = self.activation(A)
2.8.0.pre,
2.8.0.pre,if self.update_pair:
2.8.0.pre,AP_ij = tf.matmul(
2.8.0.pre,tf.reshape(
2.8.0.pre,"tf.gather(atom_features, atom_to_pair),"
2.8.0.pre,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.8.0.pre,AP_ij = self.activation(AP_ij)
2.8.0.pre,AP_ji = tf.matmul(
2.8.0.pre,tf.reshape(
2.8.0.pre,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.8.0.pre,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.8.0.pre,AP_ji = self.activation(AP_ji)
2.8.0.pre,
2.8.0.pre,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.8.0.pre,PP = self.activation(PP)
2.8.0.pre,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.8.0.pre,P = self.activation(P)
2.8.0.pre,else:
2.8.0.pre,P = pair_features
2.8.0.pre,
2.8.0.pre,"return A, P"
2.8.0.pre,TODO(rbharath): This class does not yet have a
2.8.0.pre,"TensorGraph equivalent, but one may not be required."
2.8.0.pre,"Commented out for now, remove if OK."
2.8.0.pre,class WeaveConcat(Layer):
2.8.0.pre,""""""""" Concat a batch of molecules into a batch of atoms"
2.8.0.pre,""""""""
2.8.0.pre,
2.8.0.pre,"def __init__(self,"
2.8.0.pre,"batch_size,"
2.8.0.pre,"n_atom_input_feat=50,"
2.8.0.pre,"n_output=128,"
2.8.0.pre,"init='glorot_uniform',"
2.8.0.pre,"activation='tanh',"
2.8.0.pre,**kwargs):
2.8.0.pre,""""""""
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,batch_size: int
2.8.0.pre,number of molecules in a batch
2.8.0.pre,"n_atom_input_feat: int, optional"
2.8.0.pre,Number of features for each atom in input.
2.8.0.pre,"n_output: int, optional"
2.8.0.pre,Number of output features for each atom(concatenated)
2.8.0.pre,"init: str, optional"
2.8.0.pre,Weight initialization for filters.
2.8.0.pre,"activation: str, optional"
2.8.0.pre,Activation function applied
2.8.0.pre,
2.8.0.pre,""""""""
2.8.0.pre,self.batch_size = batch_size
2.8.0.pre,self.n_atom_input_feat = n_atom_input_feat
2.8.0.pre,self.n_output = n_output
2.8.0.pre,self.init = initializations.get(init)  # Set weight initialization
2.8.0.pre,self.activation = activations.get(activation)  # Get activations
2.8.0.pre,"super(WeaveConcat, self).__init__(**kwargs)"
2.8.0.pre,
2.8.0.pre,def build(self):
2.8.0.pre,"""""""""Construct internal trainable weights."
2.8.0.pre,""""""""
2.8.0.pre,
2.8.0.pre,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.8.0.pre,self.b = model_ops.zeros(shape=[
2.8.0.pre,"self.n_output,"
2.8.0.pre,])
2.8.0.pre,
2.8.0.pre,self.trainable_weights = self.W + self.b
2.8.0.pre,
2.8.0.pre,"def call(self, x, mask=None):"
2.8.0.pre,"""""""Execute this layer on input tensors."
2.8.0.pre,
2.8.0.pre,"x = [atom_features, atom_mask]"
2.8.0.pre,
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,x: list
2.8.0.pre,Tensors as listed above
2.8.0.pre,"mask: bool, optional"
2.8.0.pre,Ignored. Present only to shadow superclass call() method.
2.8.0.pre,
2.8.0.pre,Returns
2.8.0.pre,-------
2.8.0.pre,outputs: Tensor
2.8.0.pre,Tensor of concatenated atom features
2.8.0.pre,""""""""
2.8.0.pre,self.build()
2.8.0.pre,atom_features = x[0]
2.8.0.pre,atom_masks = x[1]
2.8.0.pre,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.8.0.pre,A_mask = tf.split(
2.8.0.pre,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.8.0.pre,outputs = tf.concat(
2.8.0.pre,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.8.0.pre,"outputs = tf.matmul(outputs, self.W) + self.b"
2.8.0.pre,outputs = self.activation(outputs)
2.8.0.pre,return outputs
2.8.0.pre,TODO(rbharath): This class does not yet have a
2.8.0.pre,"TensorGraph equivalent, but one may not be required."
2.8.0.pre,"Commented out for now, remove if OK."
2.8.0.pre,class AlternateWeaveGather(WeaveGather):
2.8.0.pre,"""""""Alternate implementation of weave gather layer"
2.8.0.pre,corresponding to AlternateWeaveLayer
2.8.0.pre,""""""""
2.8.0.pre,
2.8.0.pre,"def call(self, x, mask=None):"
2.8.0.pre,"""""""Execute this layer on input tensors."
2.8.0.pre,
2.8.0.pre,"x = [atom_features, atom_split]"
2.8.0.pre,
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,x: list
2.8.0.pre,Tensors as listed above
2.8.0.pre,"mask: bool, optional"
2.8.0.pre,Ignored. Present only to shadow superclass call() method.
2.8.0.pre,
2.8.0.pre,Returns
2.8.0.pre,-------
2.8.0.pre,outputs: Tensor
2.8.0.pre,Tensor of molecular features
2.8.0.pre,""""""""
2.8.0.pre,# Add trainable weights
2.8.0.pre,self.build()
2.8.0.pre,outputs = x[0]
2.8.0.pre,atom_split = x[1]
2.8.0.pre,
2.8.0.pre,if self.gaussian_expand:
2.8.0.pre,outputs = self.gaussian_histogram(outputs)
2.8.0.pre,
2.8.0.pre,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.8.0.pre,
2.8.0.pre,if self.gaussian_expand:
2.8.0.pre,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.8.0.pre,output_molecules = self.activation(output_molecules)
2.8.0.pre,return output_molecules
2.8.0.pre,Each directory holds a range of assay results
2.8.0.pre,Just write NA
2.8.0.pre,"Now, write out the results csv, going line by line through all molecule results"
2.8.0.pre,printing the mol_id
2.8.0.pre,printing the SMILES
2.8.0.pre,Now gzip it
2.8.0.pre,Now remove the intermediate csv
2.8.0.pre,First download all SDF files. We need these to get smiles
2.8.0.pre,Next download all Bioassays
2.8.0.pre,RDKit consistently hangs when trying to read this file
2.8.0.pre,TODO (LESWING) Lazy Load
2.8.0.pre,TODO (LESWING) Lazy Load
2.8.0.pre,from simdna import simulations
2.8.0.pre,define layer out functions
2.8.0.pre,get layer outputs for a positive simulation example
2.8.0.pre,plot layer outputs
2.8.0.pre,highlight motif sites
2.8.0.pre,get a positive and a negative example from the simulation data
2.8.0.pre,"get motif scores, ISM scores, and DeepLIFT scores"
2.8.0.pre,get motif site locations
2.8.0.pre,organize legends
2.8.0.pre,plot scores and highlight motif site locations
2.8.0.pre,initialize fwd and reverse scores to -infinity
2.8.0.pre,"cross-correlate separately for each base,"
2.8.0.pre,for both the PSSM and its reverse complement
2.8.0.pre,sum over the bases
2.8.0.pre,take max of fwd and reverse scores at each position
2.8.0.pre,return 1D view of sequence characters
2.8.0.pre,class SequenceDNN(Model):
2.8.0.pre,""""""""
2.8.0.pre,Sequence DNN models.
2.8.0.pre,
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,"seq_length : int, optional"
2.8.0.pre,length of input sequence.
2.8.0.pre,"keras_model : instance of keras.models.Sequential, optional"
2.8.0.pre,seq_length or keras_model must be specified.
2.8.0.pre,"num_tasks : int, optional"
2.8.0.pre,number of tasks. Default: 1.
2.8.0.pre,num_filters : list[int] | tuple[int]
2.8.0.pre,"number of convolutional filters in each layer. Default: (15,)."
2.8.0.pre,conv_width : list[int] | tuple[int]
2.8.0.pre,"width of each layer's convolutional filters. Default: (15,)."
2.8.0.pre,pool_width : int
2.8.0.pre,width of max pooling after the last layer. Default: 35.
2.8.0.pre,L1 : float
2.8.0.pre,strength of L1 penalty.
2.8.0.pre,dropout : float
2.8.0.pre,dropout probability in every convolutional layer. Default: 0.
2.8.0.pre,verbose: int
2.8.0.pre,"Verbosity level during training. Valida values: 0, 1, 2."
2.8.0.pre,
2.8.0.pre,Returns
2.8.0.pre,-------
2.8.0.pre,Compiled DNN model.
2.8.0.pre,""""""""
2.8.0.pre,
2.8.0.pre,"def __init__(self,"
2.8.0.pre,"seq_length=None,"
2.8.0.pre,"keras_model=None,"
2.8.0.pre,"use_RNN=False,"
2.8.0.pre,"num_tasks=1,"
2.8.0.pre,"num_filters=(15, 15, 15),"
2.8.0.pre,"conv_width=(15, 15, 15),"
2.8.0.pre,"pool_width=35,"
2.8.0.pre,"GRU_size=35,"
2.8.0.pre,"TDD_size=15,"
2.8.0.pre,"L1=0,"
2.8.0.pre,"dropout=0.0,"
2.8.0.pre,"num_epochs=100,"
2.8.0.pre,verbose=1):
2.8.0.pre,self.num_tasks = num_tasks
2.8.0.pre,self.num_epochs = num_epochs
2.8.0.pre,self.verbose = verbose
2.8.0.pre,self.train_metrics = []
2.8.0.pre,self.valid_metrics = []
2.8.0.pre,if keras_model is not None and seq_length is None:
2.8.0.pre,self.model = keras_model
2.8.0.pre,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.8.0.pre,elif seq_length is not None and keras_model is None:
2.8.0.pre,self.model = Sequential()
2.8.0.pre,assert len(num_filters) == len(conv_width)
2.8.0.pre,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.8.0.pre,conv_height = 4 if i == 0 else 1
2.8.0.pre,self.model.add(
2.8.0.pre,Convolution2D(
2.8.0.pre,"nb_filter=nb_filter,"
2.8.0.pre,"nb_row=conv_height,"
2.8.0.pre,"nb_col=nb_col,"
2.8.0.pre,"activation='linear',"
2.8.0.pre,"init='he_normal',"
2.8.0.pre,"input_shape=(1, 4, seq_length),"
2.8.0.pre,"W_regularizer=l1(L1),"
2.8.0.pre,b_regularizer=l1(L1)))
2.8.0.pre,self.model.add(Activation('relu'))
2.8.0.pre,self.model.add(Dropout(dropout))
2.8.0.pre,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.8.0.pre,if use_RNN:
2.8.0.pre,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.8.0.pre,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.8.0.pre,"self.model.add(Permute((2, 1)))"
2.8.0.pre,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.8.0.pre,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.8.0.pre,self.model.add(Flatten())
2.8.0.pre,self.model.add(Dense(output_dim=self.num_tasks))
2.8.0.pre,self.model.add(Activation('sigmoid'))
2.8.0.pre,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.8.0.pre,else:
2.8.0.pre,raise ValueError(
2.8.0.pre,"""Exactly one of seq_length or keras_model must be specified!"")"
2.8.0.pre,
2.8.0.pre,"def train(self,"
2.8.0.pre,"X,"
2.8.0.pre,"y,"
2.8.0.pre,"validation_data,"
2.8.0.pre,"early_stopping_metric='Loss',"
2.8.0.pre,"early_stopping_patience=5,"
2.8.0.pre,save_best_model_to_prefix=None):
2.8.0.pre,if y.dtype != bool:
2.8.0.pre,"assert set(np.unique(y)) == {0, 1}"
2.8.0.pre,y = y.astype(bool)
2.8.0.pre,multitask = y.shape[1] > 1
2.8.0.pre,if not multitask:
2.8.0.pre,num_positives = y.sum()
2.8.0.pre,num_sequences = len(y)
2.8.0.pre,num_negatives = num_sequences - num_positives
2.8.0.pre,if self.verbose >= 1:
2.8.0.pre,print('Training model (* indicates new best result)...')
2.8.0.pre,"X_valid, y_valid = validation_data"
2.8.0.pre,early_stopping_wait = 0
2.8.0.pre,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.8.0.pre,"for epoch in range(1, self.num_epochs + 1):"
2.8.0.pre,self.model.fit(
2.8.0.pre,"X,"
2.8.0.pre,"y,"
2.8.0.pre,"batch_size=128,"
2.8.0.pre,"nb_epoch=1,"
2.8.0.pre,class_weight={
2.8.0.pre,"True: num_sequences / num_positives,"
2.8.0.pre,False: num_sequences / num_negatives
2.8.0.pre,"} if not multitask else None,"
2.8.0.pre,verbose=self.verbose >= 2)
2.8.0.pre,"epoch_train_metrics = self.test(X, y)"
2.8.0.pre,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.8.0.pre,self.train_metrics.append(epoch_train_metrics)
2.8.0.pre,self.valid_metrics.append(epoch_valid_metrics)
2.8.0.pre,if self.verbose >= 1:
2.8.0.pre,print('Epoch {}:'.format(epoch))
2.8.0.pre,print('Train {}'.format(epoch_train_metrics))
2.8.0.pre,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.8.0.pre,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.8.0.pre,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.8.0.pre,if self.verbose >= 1:
2.8.0.pre,print(' *')
2.8.0.pre,best_metric = current_metric
2.8.0.pre,best_epoch = epoch
2.8.0.pre,early_stopping_wait = 0
2.8.0.pre,if save_best_model_to_prefix is not None:
2.8.0.pre,self.save(save_best_model_to_prefix)
2.8.0.pre,else:
2.8.0.pre,if self.verbose >= 1:
2.8.0.pre,print()
2.8.0.pre,if early_stopping_wait >= early_stopping_patience:
2.8.0.pre,break
2.8.0.pre,early_stopping_wait += 1
2.8.0.pre,if self.verbose >= 1:
2.8.0.pre,print('Finished training after {} epochs.'.format(epoch))
2.8.0.pre,if save_best_model_to_prefix is not None:
2.8.0.pre,"print(""The best model's architecture and weights (from epoch {0}) """
2.8.0.pre,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.8.0.pre,"best_epoch, save_best_model_to_prefix))"
2.8.0.pre,
2.8.0.pre,"def predict(self, X):"
2.8.0.pre,"return self.model.predict(X, batch_size=128, verbose=False)"
2.8.0.pre,
2.8.0.pre,def get_sequence_filters(self):
2.8.0.pre,""""""""
2.8.0.pre,Returns 3D array of 2D sequence filters.
2.8.0.pre,""""""""
2.8.0.pre,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.8.0.pre,
2.8.0.pre,"def deeplift(self, X, batch_size=200):"
2.8.0.pre,""""""""
2.8.0.pre,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.8.0.pre,""""""""
2.8.0.pre,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.8.0.pre,from deeplift.conversion import keras_conversion as kc
2.8.0.pre,
2.8.0.pre,# convert to deeplift model and get scoring function
2.8.0.pre,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.8.0.pre,score_func = deeplift_model.get_target_contribs_func(
2.8.0.pre,find_scores_layer_idx=0)
2.8.0.pre,# use a 40% GC reference
2.8.0.pre,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.8.0.pre,# get deeplift scores
2.8.0.pre,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.8.0.pre,for i in range(self.num_tasks):
2.8.0.pre,deeplift_scores[i] = score_func(
2.8.0.pre,"task_idx=i,"
2.8.0.pre,"input_data_list=[X],"
2.8.0.pre,"batch_size=batch_size,"
2.8.0.pre,"progress_update=None,"
2.8.0.pre,input_references_list=input_references)
2.8.0.pre,return deeplift_scores
2.8.0.pre,
2.8.0.pre,"def in_silico_mutagenesis(self, X):"
2.8.0.pre,""""""""
2.8.0.pre,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.8.0.pre,""""""""
2.8.0.pre,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.8.0.pre,wild_type_predictions = self.predict(X)
2.8.0.pre,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.8.0.pre,np.newaxis]
2.8.0.pre,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.8.0.pre,"zip(X, wild_type_predictions)):"
2.8.0.pre,mutated_sequences = np.repeat(
2.8.0.pre,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.8.0.pre,# remove wild-type
2.8.0.pre,arange = np.arange(len(mutated_sequences))
2.8.0.pre,horizontal_cycle = np.tile(
2.8.0.pre,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.8.0.pre,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.8.0.pre,# add mutant
2.8.0.pre,vertical_repeat = np.repeat(
2.8.0.pre,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.8.0.pre,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.8.0.pre,# make mutant predictions
2.8.0.pre,mutated_predictions = self.predict(mutated_sequences)
2.8.0.pre,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.8.0.pre,"(self.num_tasks,))"
2.8.0.pre,mutagenesis_scores[
2.8.0.pre,sequence_index] = wild_type_prediction - mutated_predictions
2.8.0.pre,"return np.rollaxis(mutagenesis_scores, -1)"
2.8.0.pre,
2.8.0.pre,@staticmethod
2.8.0.pre,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.8.0.pre,from dragonn.plot import plot_bases_on_ax
2.8.0.pre,scores = score_func(X).squeeze(
2.8.0.pre,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.8.0.pre,try:
2.8.0.pre,os.makedirs(output_directory)
2.8.0.pre,except OSError:
2.8.0.pre,pass
2.8.0.pre,num_tasks = len(scores)
2.8.0.pre,"for task_index, task_scores in enumerate(scores):"
2.8.0.pre,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.8.0.pre,# sequence_scores is num_bases x sequence_length
2.8.0.pre,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.8.0.pre,plt.clf()
2.8.0.pre,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.8.0.pre,top_axis.plot(
2.8.0.pre,"range(1,"
2.8.0.pre,"len(basewise_max_sequence_scores) + 1),"
2.8.0.pre,basewise_max_sequence_scores)
2.8.0.pre,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.8.0.pre,peak_position = basewise_max_sequence_scores.argmax()
2.8.0.pre,top_axis.axvspan(
2.8.0.pre,"peak_position - peak_width,"
2.8.0.pre,"peak_position + peak_width,"
2.8.0.pre,"color='grey',"
2.8.0.pre,alpha=0.1)
2.8.0.pre,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.8.0.pre,peak_position + peak_width].T
2.8.0.pre,# Set non-max letter_heights to zero
2.8.0.pre,letter_heights = np.zeros_like(peak_sequence_scores)
2.8.0.pre,"letter_heights[np.arange(len(letter_heights)),"
2.8.0.pre,peak_sequence_scores.argmax(axis=1)] = \
2.8.0.pre,basewise_max_sequence_scores[peak_position - peak_width :
2.8.0.pre,peak_position + peak_width]
2.8.0.pre,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.8.0.pre,bottom_axis.set_xticklabels(
2.8.0.pre,tuple(
2.8.0.pre,"map(str,"
2.8.0.pre,"np.arange(peak_position - peak_width,"
2.8.0.pre,peak_position + peak_width + 1))))
2.8.0.pre,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.8.0.pre,plt.xlabel('Position')
2.8.0.pre,plt.ylabel('Score')
2.8.0.pre,plt.savefig(
2.8.0.pre,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.8.0.pre,"sequence_index, '_task_{}'.format(task_index)"
2.8.0.pre,if num_tasks > 1 else '')))
2.8.0.pre,plt.close()
2.8.0.pre,
2.8.0.pre,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.8.0.pre,self._plot_scores(
2.8.0.pre,"X,"
2.8.0.pre,"output_directory,"
2.8.0.pre,"peak_width,"
2.8.0.pre,"score_func=self.deeplift,"
2.8.0.pre,score_name='DeepLift')
2.8.0.pre,
2.8.0.pre,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.8.0.pre,self._plot_scores(
2.8.0.pre,"X,"
2.8.0.pre,"output_directory,"
2.8.0.pre,"peak_width,"
2.8.0.pre,"score_func=self.in_silico_mutagenesis,"
2.8.0.pre,score_name='ISM')
2.8.0.pre,
2.8.0.pre,"def plot_architecture(self, output_file):"
2.8.0.pre,from dragonn.visualize_util import plot as plot_keras_model
2.8.0.pre,"plot_keras_model(self.model, output_file, show_shape=True)"
2.8.0.pre,
2.8.0.pre,"def save(self, save_best_model_to_prefix):"
2.8.0.pre,arch_fname = save_best_model_to_prefix + '.arch.json'
2.8.0.pre,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.8.0.pre,"open(arch_fname, 'w').write(self.model.to_json())"
2.8.0.pre,"self.model.save_weights(weights_fname, overwrite=True)"
2.8.0.pre,
2.8.0.pre,@staticmethod
2.8.0.pre,"def load(arch_fname, weights_fname=None):"
2.8.0.pre,model_json_string = open(arch_fname).read()
2.8.0.pre,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.8.0.pre,if weights_fname is not None:
2.8.0.pre,sequence_dnn.model.load_weights(weights_fname)
2.8.0.pre,return sequence_dnn
2.8.0.pre,create temporary fasta files
2.8.0.pre,run command
2.8.0.pre,remove fasta files
2.8.0.pre,write test fasta file
2.8.0.pre,test gkmsvm
2.8.0.pre,get classification results
2.8.0.pre,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.8.0.pre,"Using canonical smiles for glycine, as in original research paper"
2.8.0.pre,Atom features with padding
2.8.0.pre,A_tilda_k computation
2.8.0.pre,Final feed_dict setup
2.8.0.pre,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.8.0.pre,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.8.0.pre,self.num_node_features)
2.8.0.pre,Fit models
2.8.0.pre,Args
2.8.0.pre,2017 DeepCrystal Technologies - Patrick Hop
2.8.0.pre,
2.8.0.pre,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.8.0.pre,
2.8.0.pre,MIT License - have fun!!
2.8.0.pre,===========================================================
2.8.0.pre,x = F.selu( fc(x) )
2.8.0.pre,x = F.selu( fc(x) )
2.8.0.pre,2017 DeepCrystal Technologies - Patrick Hop
2.8.0.pre,
2.8.0.pre,Data loading a splitting file
2.8.0.pre,
2.8.0.pre,MIT License - have fun!!
2.8.0.pre,===========================================================
2.8.0.pre,Args
2.8.0.pre,TODO (VIGS25): Account for the reload option
2.8.0.pre,Downloading train files
2.8.0.pre,Parsing training data
2.8.0.pre,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.8.0.pre,Test Files loading
2.8.0.pre,One Hot Featurization
2.8.0.pre,Consistency check
2.8.0.pre,Handle output layer
2.8.0.pre,Iterate over all previous tasks.
2.8.0.pre,prev_layers is a list with elements of size
2.8.0.pre,"(batch_size, layer_sizes[i-1])"
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,Save an initial checkpoint.
2.8.0.pre,Turns out there are valid cases where we don't want pad-batches
2.8.0.pre,on by default.
2.8.0.pre,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0.pre,Run training op.
2.8.0.pre,Always save a final checkpoint when complete.
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,Note that we divide by the batch size and not the number of
2.8.0.pre,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0.pre,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0.pre,calculate with div/sum so it stays on the GPU.
2.8.0.pre,aggregated costs
2.8.0.pre,weight decay
2.8.0.pre,Dummy placeholders
2.8.0.pre,Dummy placeholders
2.8.0.pre,run eval data through the model
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,Handle edge case when batch-size is 1.
2.8.0.pre,Prune away any padding that was added
2.8.0.pre,allow_soft_placement=True allows ops without a GPU implementation
2.8.0.pre,to run on the CPU instead.
2.8.0.pre,!/usr/bin/python
2.8.0.pre,
2.8.0.pre,Copyright 2015 Google Inc.
2.8.0.pre,
2.8.0.pre,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.8.0.pre,you may not use this file except in compliance with the License.
2.8.0.pre,You may obtain a copy of the License at
2.8.0.pre,
2.8.0.pre,http://www.apache.org/licenses/LICENSE-2.0
2.8.0.pre,
2.8.0.pre,"Unless required by applicable law or agreed to in writing, software"
2.8.0.pre,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.8.0.pre,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.8.0.pre,See the License for the specific language governing permissions and
2.8.0.pre,limitations under the License.
2.8.0.pre,parse CheckpointState proto
2.8.0.pre,parse path to actual checkpoint
2.8.0.pre,the provided mask has to be the same shape as features
2.8.0.pre,test k = 1..4
2.8.0.pre,central moments
2.8.0.pre,standardized moments
2.8.0.pre,central across one axis
2.8.0.pre,standardized across one axis
2.8.0.pre,Fit just on task zero
2.8.0.pre,Notice that we keep the session open
2.8.0.pre,Fit on task one
2.8.0.pre,The predictions for task zero should not change after training
2.8.0.pre,on task one.
2.8.0.pre,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.8.0.pre,!/usr/bin/python
2.8.0.pre,
2.8.0.pre,Copyright 2015 Google Inc.
2.8.0.pre,
2.8.0.pre,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.8.0.pre,you may not use this file except in compliance with the License.
2.8.0.pre,You may obtain a copy of the License at
2.8.0.pre,
2.8.0.pre,http://www.apache.org/licenses/LICENSE-2.0
2.8.0.pre,
2.8.0.pre,"Unless required by applicable law or agreed to in writing, software"
2.8.0.pre,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.8.0.pre,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.8.0.pre,See the License for the specific language governing permissions and
2.8.0.pre,limitations under the License.
2.8.0.pre,get the divisor
2.8.0.pre,compute the requested central moment
2.8.0.pre,"note that mean is a raw moment, not a central moment"
2.8.0.pre,TODO(user): median is not implemented yet in TensorFlow
2.8.0.pre,Add the input features.
2.8.0.pre,"layer has shape [None, layer_sizes[i]]"
2.8.0.pre,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.8.0.pre,TODO(rbharath): Might want to make it feasible to have multiple
2.8.0.pre,bypass layers.
2.8.0.pre,Construct task bypass layer
2.8.0.pre,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.8.0.pre,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.8.0.pre,"layer has shape [None, layer_sizes[i]]"
2.8.0.pre,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.8.0.pre,TODO(rbharath): Might want to make it feasible to have multiple
2.8.0.pre,bypass layers.
2.8.0.pre,Construct task bypass layer
2.8.0.pre,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.8.0.pre,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.8.0.pre,Consistency check
2.8.0.pre,Lazily created by _get_shared_session().
2.8.0.pre,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.8.0.pre,when subclass-overridden methods use the same scopes.
2.8.0.pre,Setup graph
2.8.0.pre,Create placeholders
2.8.0.pre,Handle output layer
2.8.0.pre,Iterate over all previous tasks.
2.8.0.pre,prev_layers is a list with elements of size
2.8.0.pre,"(batch_size, layer_sizes[i-1])"
2.8.0.pre,Note that we divide by the batch size and not the number of
2.8.0.pre,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0.pre,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0.pre,calculate with div/sum so it stays on the GPU.
2.8.0.pre,aggregated costs
2.8.0.pre,weight decay
2.8.0.pre,Dummy placeholders
2.8.0.pre,Dummy placeholders
2.8.0.pre,run eval data through the model
2.8.0.pre,"Shape (n_tasks, n__samples)"
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,Handle edge case when batch-size is 1.
2.8.0.pre,with self._get_shared_session(train=True) as sess:
2.8.0.pre,Save an initial checkpoint.
2.8.0.pre,Always save a final checkpoint when complete.
2.8.0.pre,Note that we divide by the batch size and not the number of
2.8.0.pre,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0.pre,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0.pre,calculate with div/sum so it stays on the GPU.
2.8.0.pre,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.8.0.pre,Dummy placeholders
2.8.0.pre,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.8.0.pre,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.8.0.pre,Dummy placeholders
2.8.0.pre,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.8.0.pre,allow_soft_placement=True allows ops without a GPU implementation
2.8.0.pre,to run on the CPU instead.
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,Turns out there are valid cases where we don't want pad-batches
2.8.0.pre,on by default.
2.8.0.pre,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0.pre,if epoch%checkpoint_interval == checkpoint_interval-1:
2.8.0.pre,"saver.save(sess, self._save_path, global_step=epoch)"
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,"(n_samples, n_classes)"
2.8.0.pre,"(n_samples, n_tasks, n_classes)"
2.8.0.pre,Save hyperparameters
2.8.0.pre,Guard variable to make sure we don't Restore() this model
2.8.0.pre,from a disk checkpoint more than once.
2.8.0.pre,"Path to save checkpoint files, which matches the"
2.8.0.pre,replicated supervisor's default path.
2.8.0.pre,Lazily created by _get_shared_session().
2.8.0.pre,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.8.0.pre,when subclass-overridden methods use the same scopes.
2.8.0.pre,Setup graph
2.8.0.pre,Note that we divide by the batch size and not the number of
2.8.0.pre,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0.pre,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0.pre,calculate with div/sum so it stays on the GPU.
2.8.0.pre,aggregated costs
2.8.0.pre,weight decay
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,Save an initial checkpoint.
2.8.0.pre,Define the code that runs on a separate thread to feed data into the queue.
2.8.0.pre,Main training loop.
2.8.0.pre,Run training op.
2.8.0.pre,We have reached the end of an epoch.
2.8.0.pre,We have reached the end of the data.
2.8.0.pre,Always save a final checkpoint when complete.
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,allow_soft_placement=True allows ops without a GPU implementation
2.8.0.pre,to run on the CPU instead.
2.8.0.pre,gpu memory growth option
2.8.0.pre,gpu memory growth option
2.8.0.pre,TODO(rbharath): Is setting train=False right here?
2.8.0.pre,Discard any padded predictions
2.8.0.pre,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0.pre,Remove padded examples.
2.8.0.pre,Special case to handle singletasks.
2.8.0.pre,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0.pre,Remove padded examples.
2.8.0.pre,TODO(rbharath): Verify this can be safely removed.
2.8.0.pre,"def evaluate(self, dataset, metrics, transformers=[]):"
2.8.0.pre,""""""""
2.8.0.pre,Evaluates the performance of this model on specified dataset.
2.8.0.pre,
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,dataset: dc.data.Dataset
2.8.0.pre,Dataset object.
2.8.0.pre,metric: deepchem.metrics.Metric
2.8.0.pre,Evaluation metric
2.8.0.pre,transformers: list
2.8.0.pre,List of deepchem.transformers.Transformer
2.8.0.pre,Returns
2.8.0.pre,-------
2.8.0.pre,dict
2.8.0.pre,Maps tasks to scores under metric.
2.8.0.pre,""""""""
2.8.0.pre,"evaluator = Evaluator(self, dataset, transformers)"
2.8.0.pre,scores = evaluator.compute_model_performance(metrics)
2.8.0.pre,return scores
2.8.0.pre,checkpoints look like model_dir/model.ckpt-N
2.8.0.pre,"self._save_path is ""model_dir/model.ckpt"""
2.8.0.pre,run eval data through the model
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,run eval data through the model
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,Note that softmax is already applied in construct_grpah
2.8.0.pre,run eval data through the model
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,Handle edge case when batch-size is 1.
2.8.0.pre,Prune away any padding that was added
2.8.0.pre,Handle case of 0-dimensional scalar output
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,inputs placeholder
2.8.0.pre,data preprocessing and augmentation
2.8.0.pre,first conv layer
2.8.0.pre,downsample by max pooling
2.8.0.pre,each module is a residual convolutional block
2.8.0.pre,followed by a convolutional downsample layer
2.8.0.pre,max pooling over the final outcome
2.8.0.pre,fully connected layers
2.8.0.pre,dropout for dense layers
2.8.0.pre,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.8.0.pre,weight decay regularizer
2.8.0.pre,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.8.0.pre,sample cut ratio from a clipped gaussian
2.8.0.pre,train/valid differences
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Define and build model
2.8.0.pre,model.restore()
2.8.0.pre,Set random seeds
2.8.0.pre,Setup directories
2.8.0.pre,Model constants
2.8.0.pre,Load and transform datasets
2.8.0.pre,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0.pre,Atomic convolution variables
2.8.0.pre,at = atomic numbers (atom types)
2.8.0.pre,"radial basis function parameters [cutoff, mean, width]"
2.8.0.pre,Model hyperparameters
2.8.0.pre,Initialize model
2.8.0.pre,Fit model
2.8.0.pre,Evaluate model
2.8.0.pre,Set random seeds
2.8.0.pre,Setup directories
2.8.0.pre,Model constants
2.8.0.pre,Load and transform datasets
2.8.0.pre,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0.pre,Atomic convolution variables
2.8.0.pre,at = atomic numbers (atom types)
2.8.0.pre,"radial basis function parameters [cutoff, mean, width]"
2.8.0.pre,Model hyperparameters
2.8.0.pre,Initialize model
2.8.0.pre,Fit model
2.8.0.pre,Evaluate model
2.8.0.pre,Set random seeds
2.8.0.pre,Setup directories
2.8.0.pre,Model constants
2.8.0.pre,Load and transform datasets
2.8.0.pre,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0.pre,Atomic convolution variables
2.8.0.pre,at = atomic numbers (atom types)
2.8.0.pre,"radial basis function parameters [cutoff, mean, width]"
2.8.0.pre,Model hyperparameters
2.8.0.pre,Initialize model
2.8.0.pre,Fit model
2.8.0.pre,Evaluate model
2.8.0.pre,Set random seeds
2.8.0.pre,Setup directories
2.8.0.pre,Model constants
2.8.0.pre,Load and transform datasets
2.8.0.pre,convert -logKi to dG = +RTlogKi [kJ/mol]
2.8.0.pre,Atomic convolution variables
2.8.0.pre,at = atomic numbers (atom types)
2.8.0.pre,"radial basis function parameters [cutoff, mean, width]"
2.8.0.pre,Model hyperparameters
2.8.0.pre,Initialize model
2.8.0.pre,Fit model
2.8.0.pre,Evaluate model
2.8.0.pre,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.8.0.pre,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.8.0.pre,test_scores = test_evaluator.compute_model_performance(metric)
2.8.0.pre,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.8.0.pre,param.update(test_scores)
2.8.0.pre,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0.pre,for transformer in transformers:
2.8.0.pre,train_dataset = transformer.transform(train_dataset)
2.8.0.pre,test_dataset = transformer.transform(test_dataset)
2.8.0.pre,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0.pre,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0.pre,for transformer in transformers:
2.8.0.pre,train_dataset = transformer.transform(train_dataset)
2.8.0.pre,test_dataset = transformer.transform(test_dataset)
2.8.0.pre,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0.pre,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0.pre,for transformer in transformers:
2.8.0.pre,train_dataset = transformer.transform(train_dataset)
2.8.0.pre,test_dataset = transformer.transform(test_dataset)
2.8.0.pre,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0.pre,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.8.0.pre,for transformer in transformers:
2.8.0.pre,train_dataset = transformer.transform(train_dataset)
2.8.0.pre,test_dataset = transformer.transform(test_dataset)
2.8.0.pre,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.8.0.pre,Create some directories for analysis
2.8.0.pre,The base_dir holds the results of all analysis
2.8.0.pre,Make directories to store the raw and featurized datasets.
2.8.0.pre,Load PDBBind dataset
2.8.0.pre,Define featurizers
2.8.0.pre,Currently featurizes with shard_size=1
2.8.0.pre,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.8.0.pre,This could be done with openbabel in python
2.8.0.pre,Compute cells for this molecule. O(constant)
2.8.0.pre,min == max if molecule is planar in some direction
2.8.0.pre,we should still create a bin
2.8.0.pre,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.8.0.pre,Note neighbors contains self!
2.8.0.pre,Associate each atom with cell it belongs to. O(N)
2.8.0.pre,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0.pre,"conditions, so does wrapround. O(constant)"
2.8.0.pre,"For each atom, loop through all atoms in its cell and neighboring cells."
2.8.0.pre,Accept as neighbors only those within threshold. This computation should be
2.8.0.pre,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.8.0.pre,Sort neighbors by distance
2.8.0.pre,Pick up to max_num_neighbors
2.8.0.pre,Type of data created by this featurizer
2.8.0.pre,assumes that every array is of the same dimension
2.8.0.pre,rem_dataset is remaining portion of dataset
2.8.0.pre,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.8.0.pre,to k-1.
2.8.0.pre,returns list of per column sum of non zero elements
2.8.0.pre,Compute number of actives needed per task.
2.8.0.pre,loop through each column and obtain index required to splice out for
2.8.0.pre,required fraction of hits
2.8.0.pre,Find the first index where the cumulative number of actives equals
2.8.0.pre,the actives_count
2.8.0.pre,Note that np.where tells us last index required to exceed
2.8.0.pre,"actives_count, so we actually want the following location"
2.8.0.pre,TODO(rbharath): Refactor this split method to match API of other splits (or
2.8.0.pre,potentially refactor those to match this.
2.8.0.pre,Handle edge case where frac_split is 1
2.8.0.pre,Create weight matrices fpor two haves.
2.8.0.pre,copy over up to required index for weight first_split
2.8.0.pre,check out if any rows in either w_1 or w_2 are just zeros
2.8.0.pre,"Obtain original x, y, and w arrays and shuffle"
2.8.0.pre,calculate percent split for valid (out of test and valid)
2.8.0.pre,"split test data into valid and test, treating sub test set also as sparse"
2.8.0.pre,rem_dataset is remaining portion of dataset
2.8.0.pre,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.8.0.pre,to k-1.
2.8.0.pre,JSG Assert that split fractions can be written as proper fractions over 10.
2.8.0.pre,This can be generalized in the future with some common demoninator determination.
2.8.0.pre,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.8.0.pre,Append remaining examples to train
2.8.0.pre,Sort by increasing MW
2.8.0.pre,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.8.0.pre,for m_idx in cluster:
2.8.0.pre,"continue until we find an active in all the tasks, otherwise we can't"
2.8.0.pre,compute a meaningful AUC
2.8.0.pre,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.8.0.pre,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.8.0.pre,Sort from largest to smallest scaffold sets
2.8.0.pre,Sort from largest to smallest scaffold sets
2.8.0.pre,"(n_samples, n_classes)"
2.8.0.pre,"(n_samples, n_tasks, n_classes)"
2.8.0.pre,Save hyperparameters
2.8.0.pre,Guard variable to make sure we don't Restore() this model
2.8.0.pre,from a disk checkpoint more than once.
2.8.0.pre,"Path to save checkpoint files, which matches the"
2.8.0.pre,replicated supervisor's default path.
2.8.0.pre,Lazily created by _get_shared_session().
2.8.0.pre,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.8.0.pre,when subclass-overridden methods use the same scopes.
2.8.0.pre,Setup graph
2.8.0.pre,Note that we divide by the batch size and not the number of
2.8.0.pre,"non-zero weight examples in the batch.  Also, instead of using"
2.8.0.pre,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.8.0.pre,calculate with div/sum so it stays on the GPU.
2.8.0.pre,aggregated costs
2.8.0.pre,weight decay
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,Save an initial checkpoint.
2.8.0.pre,Turns out there are valid cases where we don't want pad-batches
2.8.0.pre,on by default.
2.8.0.pre,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.8.0.pre,Run training op.
2.8.0.pre,Always save a final checkpoint when complete.
2.8.0.pre,############################################################# TIMING
2.8.0.pre,############################################################# TIMING
2.8.0.pre,allow_soft_placement=True allows ops without a GPU implementation
2.8.0.pre,to run on the CPU instead.
2.8.0.pre,TODO(rbharath): Is setting train=False right here?
2.8.0.pre,Discard any padded predictions
2.8.0.pre,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0.pre,Remove padded examples.
2.8.0.pre,Special case to handle singletasks.
2.8.0.pre,The iterbatches does padding with zero-weight examples on the last batch.
2.8.0.pre,Remove padded examples.
2.8.0.pre,TODO(rbharath): Verify this can be safely removed.
2.8.0.pre,"def evaluate(self, dataset, metrics, transformers=[]):"
2.8.0.pre,""""""""
2.8.0.pre,Evaluates the performance of this model on specified dataset.
2.8.0.pre,
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,dataset: dc.data.Dataset
2.8.0.pre,Dataset object.
2.8.0.pre,metric: deepchem.metrics.Metric
2.8.0.pre,Evaluation metric
2.8.0.pre,transformers: list
2.8.0.pre,List of deepchem.transformers.Transformer
2.8.0.pre,Returns
2.8.0.pre,-------
2.8.0.pre,dict
2.8.0.pre,Maps tasks to scores under metric.
2.8.0.pre,""""""""
2.8.0.pre,"evaluator = Evaluator(self, dataset, transformers)"
2.8.0.pre,scores = evaluator.compute_model_performance(metrics)
2.8.0.pre,return scores
2.8.0.pre,checkpoints look like logdir/model.ckpt-N
2.8.0.pre,"self._save_path is ""logdir/model.ckpt"""
2.8.0.pre,run eval data through the model
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,run eval data through the model
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,Note that softmax is already applied in construct_grpah
2.8.0.pre,run eval data through the model
2.8.0.pre,reshape to batch_size x n_tasks x ...
2.8.0.pre,Handle edge case when batch-size is 1.
2.8.0.pre,Prune away any padding that was added
2.8.0.pre,Handle case of 0-dimensional scalar output
2.8.0.pre,Dummy placeholders
2.8.0.pre,Dummy placeholders
2.8.0.pre,## AtomicNet fully-connected layer ops ###
2.8.0.pre,## Atomicnet coordinate transform ops ###
2.8.0.pre,## Atomicnet symmetry function kernel ops ###
2.8.0.pre,## Atomicnet symmetry function ops ###
2.8.0.pre,## Atomcnet symmetry function layer ops ###
2.8.0.pre,We apply the radial pooling filter before atom type conv
2.8.0.pre,to reduce computation
2.8.0.pre,## Misc convenience ops ###
2.8.0.pre,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.8.0.pre,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.8.0.pre,"game).  The average reward for any bet is slightly negative, so the best"
2.8.0.pre,strategy is to walk away.
2.8.0.pre,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0.pre,Optimize it.
2.8.0.pre,"It should have learned that the expected value is very close to zero, and that the best"
2.8.0.pre,action is to walk away.
2.8.0.pre,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.8.0.pre,get the same result.
2.8.0.pre,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0.pre,Run the algorithm.
2.8.0.pre,Save a file checkpoint.
2.8.0.pre,Build the tree.
2.8.0.pre,Compute the final probabilities and expected reward.
2.8.0.pre,Mark this node as terminal
2.8.0.pre,Expand this node.
2.8.0.pre,Select the next action to perform.
2.8.0.pre,Recursively build the tree.
2.8.0.pre,Update statistics for this node.
2.8.0.pre,Configuration file for the Sphinx documentation builder.
2.8.0.pre,
2.8.0.pre,This file only contains a selection of the most common options. For a full
2.8.0.pre,list see the documentation:
2.8.0.pre,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.8.0.pre,-- Path setup --------------------------------------------------------------
2.8.0.pre,"If extensions (or modules to document with autodoc) are in another directory,"
2.8.0.pre,add these directories to sys.path here. If the directory is relative to the
2.8.0.pre,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.8.0.pre,
2.8.0.pre,-- Project information -----------------------------------------------------
2.8.0.pre,"The full version, including alpha/beta/rc tags"
2.8.0.pre,-- General configuration ---------------------------------------------------
2.8.0.pre,"Add any Sphinx extension module names here, as strings. They can be"
2.8.0.pre,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.8.0.pre,ones.
2.8.0.pre,Options for autodoc directives
2.8.0.pre,How to represents typehints
2.8.0.pre,"Add any paths that contain templates here, relative to this directory."
2.8.0.pre,The suffix of source filenames.
2.8.0.pre,The master toctree document.
2.8.0.pre,autosectionlabel setting
2.8.0.pre,"List of patterns, relative to source directory, that match files and"
2.8.0.pre,directories to ignore when looking for source files.
2.8.0.pre,This pattern also affects html_static_path and html_extra_path.
2.8.0.pre,"If true, the current module name will be prepended to all description"
2.8.0.pre,unit titles (such as .. function::).
2.8.0.pre,-- Options for HTML output -------------------------------------------------
2.8.0.pre,The theme to use for HTML and HTML Help pages.  See the documentation for
2.8.0.pre,a list of builtin themes.
2.8.0.pre,"Add any paths that contain custom static files (such as style sheets) here,"
2.8.0.pre,"relative to this directory. They are copied after the builtin static files,"
2.8.0.pre,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.8.0.pre,The name of an image file (relative to this directory) to place at the top
2.8.0.pre,of the sidebar.
2.8.0.pre,Customize the sphinx theme
2.8.0.pre,-- Source code links ---------------------------------------------------
2.8.0.pre,Resolve function for the linkcode extension.
2.8.0.pre,"try to find the file and line number, based on code from numpy:"
2.8.0.pre,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.8.0.pre,lines in the label file have format
2.8.0.pre,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.8.0.pre,"print line[0], line[3]"
2.8.0.pre,"If you push the tag, please remove `.dev`"
2.8.0.pre,Record inputs.
2.8.0.pre,Create the output directory if necessary.
2.8.0.pre,Select a device.
2.8.0.pre,Create the optimizers for meta-optimization and task optimization.
2.8.0.pre,Main optimization loop.
2.8.0.pre,Do checkpointing.
2.8.0.pre,Save the checkpoint to a file.
2.8.0.pre,Rename and delete older files.
2.8.0.pre,Record inputs.
2.8.0.pre,Create the output directory if necessary.
2.8.0.pre,Create the optimizers for meta-optimization and task optimization.
2.8.0.pre,Create a Checkpoint for saving.
2.8.0.pre,Main optimization loop.
2.8.0.pre,Do checkpointing.
2.8.0.pre,flake8: noqa
2.8.0.pre,This is a MetaLearner that learns to generate sine functions with variable
2.8.0.pre,amplitude and phase.
2.8.0.pre,Optimize it.
2.8.0.pre,Test it out on some new tasks and see how it works.
2.8.0.pre,Initially the model should do a bad job of fitting the sine function.
2.8.0.pre,After one step of optimization it should do much better.
2.8.0.pre,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.8.0.pre,get the same result.
2.8.0.pre,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0.pre,Optimize it.
2.8.0.pre,Test it out on some new tasks and see how it works.
2.8.0.pre,Initially the model should do a bad job of fitting the sine function.
2.8.0.pre,After one step of optimization it should do much better.
2.8.0.pre,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.8.0.pre,get the same result.
2.8.0.pre,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0.pre,We know use_pose_generator_scores == False in this case
2.8.0.pre,check whether self.featurizer is instance of ComplexFeaturizer or not
2.8.0.pre,TODO: How to handle the failure here?
2.8.0.pre,TODO(rbharath): The autodock vina source computes surface distances
2.8.0.pre,which take into account the van der Waals radius of each atom type.
2.8.0.pre,"Shape (N, M)"
2.8.0.pre,"Shape (N, M)"
2.8.0.pre,Parse complex
2.8.0.pre,check filetypes
2.8.0.pre,Define locations of log and output files
2.8.0.pre,Write GNINA conf file
2.8.0.pre,Run GNINA
2.8.0.pre,read output and log
2.8.0.pre,Parse complex
2.8.0.pre,Prepare protein
2.8.0.pre,Get protein centroid and range
2.8.0.pre,TODO(rbharath: Does vina divide box dimensions by 2?
2.8.0.pre,Prepare ligand
2.8.0.pre,Write Vina conf file
2.8.0.pre,Define locations of output files
2.8.0.pre,flake8: noqa
2.8.0.pre,We provide no scoring model so the docker won't score
2.8.0.pre,Check only one output since num_modes==1
2.8.0.pre,We provide no scoring model so the docker won't score
2.8.0.pre,Check only one output since num_modes==1
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Check returned files exist
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Check returned files exist
2.8.0.pre,"Where d is greater than zero, the repulsion is just zeros"
2.8.0.pre,"When d is 0, this should just be 1"
2.8.0.pre,"When d == 0, the hbond interaction is 0"
2.8.0.pre,The exponential returns 1 when input 0.
2.8.0.pre,This exponential returns 1 when input 3
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Note this may download autodock Vina...
2.8.0.pre,Let's turn on logging since this test will run for a while
2.8.0.pre,Note this may download autodock Vina...
2.8.0.pre,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.8.0.pre,Test that every atom in pocket maps exists
2.8.0.pre,scalar case
2.8.0.pre,per-example case
2.8.0.pre,This is a little arcane but it repeats w across tasks.
2.8.0.pre,"If w.shape == (n_samples, 1) handle it as 1D"
2.8.0.pre,"w.shape == (n_samples, n_tasks)"
2.8.0.pre,scalar case
2.8.0.pre,Handle n_classes/n_task shape ambiguity
2.8.0.pre,Add in task dimension
2.8.0.pre,Insert a task dimension (we know n_tasks=1 from above0
2.8.0.pre,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.8.0.pre,Handle classification. We need to convert labels into one-hot representation.
2.8.0.pre,check whether n_classes is int or not
2.8.0.pre,Handle n_classes/n_task shape ambiguity
2.8.0.pre,Add in task dimension
2.8.0.pre,Make everything 2D so easy to handle
2.8.0.pre,Handle each task separately.
2.8.0.pre,Handle continuous class probabilites of positive class for binary
2.8.0.pre,Fill in class 0 probabilities
2.8.0.pre,Add a task dimension to concatenate on
2.8.0.pre,Handle binary labels
2.8.0.pre,"make y_hot of shape (N, n_classes)"
2.8.0.pre,Add a task dimension to concatenate on
2.8.0.pre,Insert a task dimension
2.8.0.pre,"Now of shape (N,)"
2.8.0.pre,"Now of shape (N, 1)"
2.8.0.pre,"Returns shape (N, n_tasks)"
2.8.0.pre,"Now of shape (N,)"
2.8.0.pre,"Now of shape (N, n_classes)"
2.8.0.pre,"Now of shape (N, 1, n_classes)"
2.8.0.pre,"Returns shape (N, n_tasks, n_classes)"
2.8.0.pre,These are some smart defaults
2.8.0.pre,These are some smart defaults corresponding to sklearn's required
2.8.0.pre,behavior
2.8.0.pre,Attempt some limited shape imputation to find n_tasks
2.8.0.pre,check whether n_tasks is int or not
2.8.0.pre,This is because `normalize_weight_shape` require int value.
2.8.0.pre,FIXME: Incompatible types in assignment
2.8.0.pre,Attempt to convert both into the same type
2.8.0.pre,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.8.0.pre,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.8.0.pre,initialize fwd and reverse scores to -infinity
2.8.0.pre,"cross-correlate separately for each base,"
2.8.0.pre,for both the PSSM and its reverse complement
2.8.0.pre,sum over the bases
2.8.0.pre,take max of fwd and reverse scores at each position
2.8.0.pre,"Shape (N_sequences, num_tasks)"
2.8.0.pre,check whether wild_type_predictions is np.ndarray or not
2.8.0.pre,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.8.0.pre,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.8.0.pre,Mutates every position of the sequence to every letter
2.8.0.pre,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.8.0.pre,Breakdown:
2.8.0.pre,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.8.0.pre,remove wild-type
2.8.0.pre,len(arange) = N_letters * sequence_length
2.8.0.pre,len(horizontal cycle) = N_letters * sequence_length
2.8.0.pre,add mutant
2.8.0.pre,make mutant predictions
2.8.0.pre,check whether wild_type_predictions is np.ndarray or not
2.8.0.pre,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.8.0.pre,validation
2.8.0.pre,flake8: noqa
2.8.0.pre,metric class
2.8.0.pre,metrics utils
2.8.0.pre,sklearn & scipy score function
2.8.0.pre,original score function
2.8.0.pre,Get a random prediction matrix
2.8.0.pre,"Of shape (N, n_classes)"
2.8.0.pre,"Of shape (N, 1, n_classes)"
2.8.0.pre,This has w for each task.
2.8.0.pre,Best score case
2.8.0.pre,Worst score case
2.8.0.pre,best case
2.8.0.pre,duplicate prediction value
2.8.0.pre,Encode motif
2.8.0.pre,"sequences now has shape (3, 4, 5, 1)"
2.8.0.pre,"sequences now has shape (3, 4, 5, 1)"
2.8.0.pre,Construct and train SequenceDNN model
2.8.0.pre,Call in-silico mutagenesis
2.8.0.pre,Construct and train SequenceDNN model
2.8.0.pre,Call in-silico mutagenesis
2.8.0.pre,Check nonzero elements exist
2.8.0.pre,Special case handling of single input
2.8.0.pre,Featurize task results iff they exist.
2.8.0.pre,Filter out examples where featurization failed.
2.8.0.pre,"For prospective data where results are unknown, it"
2.8.0.pre,makes no sense to have y values or weights.
2.8.0.pre,Featurize task results if they exist.
2.8.0.pre,Filter out examples where featurization failed.
2.8.0.pre,"For prospective data where results are unknown, it"
2.8.0.pre,makes no sense to have y values or weights.
2.8.0.pre,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.8.0.pre,The field in which load_sdf_files return value stores smiles
2.8.0.pre,Special case handling of single input
2.8.0.pre,Featurize task results iff they exist.
2.8.0.pre,Filter out examples where featurization failed.
2.8.0.pre,"For prospective data where results are unknown, it"
2.8.0.pre,makes no sense to have y values or weights.
2.8.0.pre,Process legacy toggle
2.8.0.pre,Set attributes
2.8.0.pre,Handle special featurizer cases
2.8.0.pre,Set self.featurizer
2.8.0.pre,"(X, y, w, ids)"
2.8.0.pre,TODO don't convert all sequences into np array (allow shards)
2.8.0.pre,Check if line is a header
2.8.0.pre,Handle empty sequence
2.8.0.pre,Annotate start/stop of sequence
2.8.0.pre,Open index file
2.8.0.pre,create an empty list to store lines in files.
2.8.0.pre,iterate through each line in the input file
2.8.0.pre,If the number of lines iterated through is equal or less than the shard size:
2.8.0.pre,append to list
2.8.0.pre,else yield the list
2.8.0.pre,set the line_number variable to the last line number (num) before 'yield' was called
2.8.0.pre,yield list (shard/batch)
2.8.0.pre,Re-initialize list with the index line to begin a new shard.
2.8.0.pre,Set attributes
2.8.0.pre,Handle special featurizer cases
2.8.0.pre,Set self.featurizer
2.8.0.pre,Set self.return_quality_scores
2.8.0.pre,Featurize sequences
2.8.0.pre,"(X, y , w, ids)"
2.8.0.pre,Featurize sequences
2.8.0.pre,"(X, y , w, ids)"
2.8.0.pre,Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
2.8.0.pre,First line : header description
2.8.0.pre,second line : sequence
2.8.0.pre,third line : more description usually the same as the first line
2.8.0.pre,fourth line: quality scores of the sequence
2.8.0.pre,Second line : add sequence to the sequence array
2.8.0.pre,Fourth line
2.8.0.pre,Handle empty sequence
2.8.0.pre,Annotate start/stop of sequence
2.8.0.pre,Sometimes zip files contain directories within. Traverse directories
2.8.0.pre,Sort image files
2.8.0.pre,Sometimes zip files contain directories within. Traverse directories
2.8.0.pre,Sort label image files
2.8.0.pre,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.8.0.pre,Set attributes
2.8.0.pre,Handle special featurizer cases
2.8.0.pre,Set self.featurizer
2.8.0.pre,"(X, y, w, ids)"
2.8.0.pre,Set attributes
2.8.0.pre,Handle special featurizer cases
2.8.0.pre,Set self.featurizer
2.8.0.pre,"(X, y, w, ids)"
2.8.0.pre,Set attributes
2.8.0.pre,Handle special featurizer cases
2.8.0.pre,Set self.featurizer
2.8.0.pre,"(X, y, w, ids)"
2.8.0.pre,Remove support indices
2.8.0.pre,Remove support indices
2.8.0.pre,Remove support indices
2.8.0.pre,Get task specific entries
2.8.0.pre,Now just get weights for this task
2.8.0.pre,Get task specific entries
2.8.0.pre,Now just get weights for this task
2.8.0.pre,Now just get weights for this task
2.8.0.pre,Now just get weights for this task
2.8.0.pre,Split data into pos and neg lists.
2.8.0.pre,No replacement allowed for supports
2.8.0.pre,Handle one-d vs. non one-d feature matrices
2.8.0.pre,Init the iterator
2.8.0.pre,Set initial iterator state
2.8.0.pre,support = self.supports[task][self.trial_num]
2.8.0.pre,Increment and update logic
2.8.0.pre,Init the iterator
2.8.0.pre,Set initial iterator state
2.8.0.pre,support = self.supports[task][self.trial_num]
2.8.0.pre,Increment and update logic
2.8.0.pre,Ensure that every worker will pick the same random order for each epoch.
2.8.0.pre,Ensure that every worker will pick the same random order for each epoch.
2.8.0.pre,"By invariant of when this is called, can assume num_samples > 0"
2.8.0.pre,and num_samples < batch_size
2.8.0.pre,Fill in batch arrays
2.8.0.pre,"By invariant of when this is called, can assume num_samples > 0"
2.8.0.pre,and num_samples < batch_size
2.8.0.pre,Fill in batch arrays
2.8.0.pre,Only the first set of copy will be counted in training loss
2.8.0.pre,Retrieve the first sample so we can determine the dtypes.
2.8.0.pre,Create a Tensorflow Dataset.
2.8.0.pre,Find the X values.
2.8.0.pre,Find the y values.
2.8.0.pre,Find the w values.
2.8.0.pre,Find the ids.
2.8.0.pre,"Set labels to be zero, with zero weights"
2.8.0.pre,The line here assumes that y generated by shard_generator is a numpy array
2.8.0.pre,Load obsolete format -> save in new format
2.8.0.pre,note that this corresponds to the _construct_metadata column order
2.8.0.pre,Create temp directory to store resharded version
2.8.0.pre,Get correct shapes for y/w
2.8.0.pre,Write data in new shards
2.8.0.pre,Handle shapes
2.8.0.pre,Note that this means that DiskDataset resharding currently doesn't
2.8.0.pre,work for datasets that aren't regression/classification.
2.8.0.pre,Handle spillover from last shard
2.8.0.pre,Should have updated to non-legacy metadata
2.8.0.pre,Note that this resets the cache internally
2.8.0.pre,"(ytz): Depending on the application, thread-based pools may be faster"
2.8.0.pre,"than process based pools, since process based pools need to pickle/serialize"
2.8.0.pre,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.8.0.pre,we're actually protected by the GIL.
2.8.0.pre,mp.dummy aliases ThreadPool to Pool
2.8.0.pre,(ytz): this skips everything except possibly the last shard
2.8.0.pre,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.8.0.pre,make a NumpyDataset under the hood
2.8.0.pre,"raw_data = (X, y, w, ids)"
2.8.0.pre,Protect against generator exhaustion
2.8.0.pre,This ensures tasks are consistent for all datasets
2.8.0.pre,determine the shard sizes of the datasets to merge
2.8.0.pre,"otherwise the entire dataset is the ""shard size"""
2.8.0.pre,we must reshard the dataset to have a uniform size
2.8.0.pre,choose the smallest shard size
2.8.0.pre,Get full dataset in memory
2.8.0.pre,Shuffle in memory
2.8.0.pre,Write shuffled shards out to disk
2.8.0.pre,Shuffle the arrays corresponding to each row in metadata_df
2.8.0.pre,Reset cache
2.8.0.pre,See if we have a cached copy of this shard.
2.8.0.pre,"We don't, so load it from disk."
2.8.0.pre,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.8.0.pre,Try to cache this shard for later use.  Since the normal usage pattern is
2.8.0.pre,"a series of passes through the whole dataset, there's no point doing"
2.8.0.pre,anything fancy.  It never makes sense to evict another shard from the
2.8.0.pre,"cache to make room for this one, because we'll probably want that other"
2.8.0.pre,shard again before the next time we want this one.  So just cache as many
2.8.0.pre,as we can and then stop.
2.8.0.pre,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.8.0.pre,Handle edge case with empty indices
2.8.0.pre,We use two loops here. The outer while loop walks over selection shards
2.8.0.pre,(the chunks of the indices to select that should go into separate
2.8.0.pre,"output shards), while the inner for loop walks over the shards in the"
2.8.0.pre,source datasets to select out the shard indices from that  source shard
2.8.0.pre,Find indices which rest in this shard
2.8.0.pre,Need to offset indices to fit within shard_size
2.8.0.pre,Handle empty case where no data from this shard needed
2.8.0.pre,Handle the case of datasets with y/w missing
2.8.0.pre,Break if all indices have been used up already
2.8.0.pre,Note these will be in the sorted order
2.8.0.pre,We need to recover the original ordering. We can do this by using
2.8.0.pre,np.where to find the locatios of the original indices in the sorted
2.8.0.pre,indices.
2.8.0.pre,We know there's only one match for np.where since this is a
2.8.0.pre,"permutation, so the [0][0] pulls out the exact match location."
2.8.0.pre,If shape metadata is available use it to directly compute shape from
2.8.0.pre,metadata
2.8.0.pre,"In absense of shape metadata, fall back to loading data from disk to"
2.8.0.pre,find shape.
2.8.0.pre,Case n_samples should be 1
2.8.0.pre,flake8: noqa
2.8.0.pre,TODO(rbharath): Get rid of * import
2.8.0.pre,Test merging of numpy datasets
2.8.0.pre,Load MUV dataset
2.8.0.pre,Do an approximate comparison since splits are sometimes slightly off from
2.8.0.pre,the exact fraction.
2.8.0.pre,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.8.0.pre,reloading will cause the transform to be reapplied. This is undesirable in
2.8.0.pre,almost all cases. Need to understand a method to fix this.
2.8.0.pre,The shuffling should have switched up the ordering
2.8.0.pre,But all the same entries should still be present
2.8.0.pre,All the data should have same shape
2.8.0.pre,The shuffling should have switched up the ordering
2.8.0.pre,But all the same entries should still be present
2.8.0.pre,All the data should have same shape
2.8.0.pre,The ids should now store the performed permutation. Check that the
2.8.0.pre,original dataset is recoverable.
2.8.0.pre,The ids should now store the performed permutation. Check that the
2.8.0.pre,original dataset is recoverable.
2.8.0.pre,Generate data
2.8.0.pre,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.8.0.pre,around for testing resharding.
2.8.0.pre,Set cache to 0 size to avoid cache hiding errors
2.8.0.pre,Generate data
2.8.0.pre,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.8.0.pre,around for testing resharding.
2.8.0.pre,Set cache to 0 size to avoid cache hiding errors
2.8.0.pre,Featurize emols dataset
2.8.0.pre,example.fasta contains 3 sequences each of length 58.
2.8.0.pre,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.8.0.pre,"There is one ""image channel""."
2.8.0.pre,"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
2.8.0.pre,TODO: test with full uniprot file once sharding support is added.
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Set last n_samples/2 weights to 0
2.8.0.pre,Check that no support elements are sample from zero-weight samples
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Create support generator
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Create support generator
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Assert all support elements have been removed
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Assert all remove elements have been removed
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Assert all support elements have been removed
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Assert all remove elements have been removed
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Set last n_samples/2 weights to 0
2.8.0.pre,Sample from first n_samples/2 elements for support
2.8.0.pre,Should lie within first n_samples/2 samples only
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Create support generator
2.8.0.pre,Generate dummy dataset
2.8.0.pre,This is necessary since from_numpy adds in shape information
2.8.0.pre,This is necessary since from_numpy adds in shape information
2.8.0.pre,This is necessary since from_numpy adds in shape information
2.8.0.pre,Generate data
2.8.0.pre,Generate data
2.8.0.pre,Generate data
2.8.0.pre,Should now have 10 shards
2.8.0.pre,This is the shape of legacy_data
2.8.0.pre,legacy_dataset is a dataset in the legacy format kept around for testing
2.8.0.pre,purposes.
2.8.0.pre,This is the shape of legacy_data_reshard
2.8.0.pre,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.8.0.pre,around for testing
2.8.0.pre,Should now have 10 shards
2.8.0.pre,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.8.0.pre,Test constructor reload works for legacy format
2.8.0.pre,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.8.0.pre,around for testing resharding.
2.8.0.pre,Reshard copy
2.8.0.pre,Check metadata has been updated
2.8.0.pre,First try using images for X.
2.8.0.pre,Now try using images for y.
2.8.0.pre,Transform it
2.8.0.pre,Test on identity matrix
2.8.0.pre,Generate random sparse features dataset
2.8.0.pre,Test edge case with array of all zeros
2.8.0.pre,Test cases where n_samples < 2*n_samples < batch_size
2.8.0.pre,Test cases where n_samples < batch_size
2.8.0.pre,Test case where n_samples == batch_size
2.8.0.pre,Test case for object featurization.
2.8.0.pre,Test case for more complicated object featurization
2.8.0.pre,Test case with multidimensional data
2.8.0.pre,Test cases where n_samples < 2*n_samples < batch_size
2.8.0.pre,Test cases where n_samples < batch_size
2.8.0.pre,Test case where n_samples == batch_size
2.8.0.pre,Test case for object featurization.
2.8.0.pre,Test case for more complicated object featurization
2.8.0.pre,Test case with multidimensional data
2.8.0.pre,Test first resharding worked
2.8.0.pre,Test second resharding worked
2.8.0.pre,approx 1/15! chance of equality
2.8.0.pre,Generate data
2.8.0.pre,Generate data
2.8.0.pre,Transform it
2.8.0.pre,special case to test
2.8.0.pre,deterministic
2.8.0.pre,non-deterministic
2.8.0.pre,we don't know the order in which the shards are iterated in.
2.8.0.pre,Check that we have all the data in
2.8.0.pre,Test iterating in order.
2.8.0.pre,Test iterating out of order.
2.8.0.pre,Test iterating in batches.
2.8.0.pre,Test iterating with multiple workers.
2.8.0.pre,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.8.0.pre,Try specifying particular columns.
2.8.0.pre,Try specifying particular columns
2.8.0.pre,Test id shrinkage
2.8.0.pre,Test task shrinkage
2.8.0.pre,Test max print size
2.8.0.pre,Create image file
2.8.0.pre,Create directory of multiple image files
2.8.0.pre,Zip directory of multiple image files
2.8.0.pre,Create zip of image file
2.8.0.pre,Create zip of multiple image files
2.8.0.pre,"Create zip of multiple image files, multiple_types"
2.8.0.pre,Create image directory
2.8.0.pre,These are the known dimensions of face.png
2.8.0.pre,These are the known dimensions of face.png
2.8.0.pre,These are the known dimensions of face.png
2.8.0.pre,TODO(rbharath): Where are the color channels?
2.8.0.pre,These are the known dimensions of a_image.tif
2.8.0.pre,These are the known dimensions of a_image.tif
2.8.0.pre,"Since the different files have different shapes, makes an object array"
2.8.0.pre,Test that the order of the contents of an unzipped file is preserved.
2.8.0.pre,Load the zip file
2.8.0.pre,Load multi_path directly
2.8.0.pre,Check that the order of the files is the same
2.8.0.pre,Splits featurized samples into train/test
2.8.0.pre,Splits featurized samples into train/test
2.8.0.pre,Splits featurized samples into train/test
2.8.0.pre,Splits featurized samples into train/test
2.8.0.pre,Now perform move
2.8.0.pre,Only for debug!
2.8.0.pre,Make directories to store the raw and featurized datasets.
2.8.0.pre,Load dataset
2.8.0.pre,Featurize tox21 dataset
2.8.0.pre,featurization
2.8.0.pre,train/valid split.
2.8.0.pre,singletask load
2.8.0.pre,comparison
2.8.0.pre,Only for debug!
2.8.0.pre,Make directories to store the raw and featurized datasets.
2.8.0.pre,Load dataset
2.8.0.pre,Featurize tox21 dataset
2.8.0.pre,For debugging purposes
2.8.0.pre,multitask load
2.8.0.pre,Do train/valid split.
2.8.0.pre,singletask load
2.8.0.pre,comparison
2.8.0.pre,Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
2.8.0.pre,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.8.0.pre,"Expected shape is now (4, 192, 5)"
2.8.0.pre,Get the labels/weights
2.8.0.pre,Normalize shapes
2.8.0.pre,Remove labels with zero weights
2.8.0.pre,Note that we may have 0 elements of a given class since we remove those
2.8.0.pre,labels with zero weight.
2.8.0.pre,this works because y is 1D
2.8.0.pre,This is the right ratio since int(N/num_c) * num_c \approx N
2.8.0.pre,for all classes
2.8.0.pre,Flattening is safe because of shape check above
2.8.0.pre,Hack to allow for easy unpickling:
2.8.0.pre,http://stefaanlippens.net/pickleproblem
2.8.0.pre,Some transformation must happen
2.8.0.pre,Add this case in to handle non-DiskDataset that should be written to disk
2.8.0.pre,Note that transformers have to be undone in reversed order
2.8.0.pre,Handle division by zero
2.8.0.pre,Handle division by zero
2.8.0.pre,Control for pathological case with no variance.
2.8.0.pre,Handle case with 1 task correctly
2.8.0.pre,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.8.0.pre,Find the task dimension of z
2.8.0.pre,Prevent broadcasting on wrong dimension
2.8.0.pre,BalancingTransformer can only transform weights.
2.8.0.pre,Compute weighting factors from dataset.
2.8.0.pre,Handle 1-D case
2.8.0.pre,Remove labels with zero weights
2.8.0.pre,Note that we may have 0 elements of a given class since we remove those
2.8.0.pre,labels with zero weight. This typically happens in multitask datasets
2.8.0.pre,where some datapoints only have labels for some tasks.
2.8.0.pre,this works because task_y is 1D
2.8.0.pre,This is the right ratio since N_task/num_c * num_c = N_task
2.8.0.pre,for all classes
2.8.0.pre,Set to the class weight computed previously
2.8.0.pre,Need this for transform_y
2.8.0.pre,Handle 1D case
2.8.0.pre,THis reshape is safe because of guard above.
2.8.0.pre,map the indices to labels
2.8.0.pre,generating batch of data by slicing similarity matrix
2.8.0.pre,into 100*reference_dataset_length
2.8.0.pre,concatenate batches of data together
2.8.0.pre,highest similarity is 1: target is in the reference
2.8.0.pre,use the following K points
2.8.0.pre,"highest less than 1: target not in the reference, use top K points"
2.8.0.pre,calculate matrix multiplicatin on slices
2.8.0.pre,concatenate the slices together
2.8.0.pre,list of calculation orders for DAGs
2.8.0.pre,stemming from one specific atom in the molecule
2.8.0.pre,starting from the adjacency list derived by graphconv featurizer
2.8.0.pre,"number of atoms, also number of DAGs"
2.8.0.pre,"DAG on a molecule with k atoms includes k steps of calculation,"
2.8.0.pre,each step calculating graph features for one atom.
2.8.0.pre,`max_atoms` is the maximum number of steps
2.8.0.pre,each iteration generates the DAG starting from atom with index `count`
2.8.0.pre,"list of lists, elements represent the calculation orders"
2.8.0.pre,for atoms in the current graph
2.8.0.pre,starting from the target atom with index `count`
2.8.0.pre,flags of whether the atom is already included in the DAG
2.8.0.pre,atom `count` is in the DAG
2.8.0.pre,recording number of radial propagation steps
2.8.0.pre,"in the fisrt loop, atoms directly connected to `count` will be added"
2.8.0.pre,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.8.0.pre,will be added in the second loop(radial=1).
2.8.0.pre,atoms i-bond away will be added in i-th loop
2.8.0.pre,"when molecules have separate parts, starting from one part,"
2.8.0.pre,it is not possible to include all atoms.
2.8.0.pre,this break quit the loop when going into such condition
2.8.0.pre,reinitialize targets for next iteration
2.8.0.pre,atoms connected to current_atom
2.8.0.pre,generate the dependency map of current DAG
2.8.0.pre,atoms connected to `current_atoms`(and not included in the DAG)
2.8.0.pre,"are added, and will be the `current_atoms` for next iteration."
2.8.0.pre,"DAG starts from the target atom, calculation should go in reverse"
2.8.0.pre,`edge[1]` is the parent of `edge[0]`
2.8.0.pre,"after this loop, `parents[i]` includes all parents of atom i"
2.8.0.pre,manually adding the atom index into its parents list
2.8.0.pre,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.8.0.pre,atoms with less parents(farther from the target atom) come first.
2.8.0.pre,"graph features of atoms without parents will be first calculated,"
2.8.0.pre,then atoms with more parents can be calculated in order
2.8.0.pre,based on previously calculated graph features.
2.8.0.pre,target atom of this DAG will be calculated in the last step
2.8.0.pre,padding with `max_atoms`
2.8.0.pre,padding
2.8.0.pre,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.8.0.pre,which is a max_atoms * max_atoms numpy array after padding
2.8.0.pre,class ANITransformer(Transformer):
2.8.0.pre,"""""""Performs transform from 3D coordinates to ANI symmetry functions"
2.8.0.pre,Note
2.8.0.pre,----
2.8.0.pre,This class requires TensorFlow to be installed.
2.8.0.pre,""""""""
2.8.0.pre,"def __init__(self,"
2.8.0.pre,"max_atoms=23,"
2.8.0.pre,"radial_cutoff=4.6,"
2.8.0.pre,"angular_cutoff=3.1,"
2.8.0.pre,"radial_length=32,"
2.8.0.pre,"angular_length=8,"
2.8.0.pre,"atom_cases=[1, 6, 7, 8, 16],"
2.8.0.pre,"atomic_number_differentiated=True,"
2.8.0.pre,coordinates_in_bohr=True):
2.8.0.pre,""""""""
2.8.0.pre,Only X can be transformed
2.8.0.pre,""""""""
2.8.0.pre,import tensorflow as tf
2.8.0.pre,self.max_atoms = max_atoms
2.8.0.pre,self.radial_cutoff = radial_cutoff
2.8.0.pre,self.angular_cutoff = angular_cutoff
2.8.0.pre,self.radial_length = radial_length
2.8.0.pre,self.angular_length = angular_length
2.8.0.pre,self.atom_cases = atom_cases
2.8.0.pre,self.atomic_number_differentiated = atomic_number_differentiated
2.8.0.pre,self.coordinates_in_bohr = coordinates_in_bohr
2.8.0.pre,self.compute_graph = self.build()
2.8.0.pre,self.sess = tf.Session(graph=self.compute_graph)
2.8.0.pre,self.transform_batch_size = 32
2.8.0.pre,"super(ANITransformer, self).__init__(transform_X=True)"
2.8.0.pre,"def transform_array(self, X, y, w):"
2.8.0.pre,if self.transform_X:
2.8.0.pre,X_out = []
2.8.0.pre,num_transformed = 0
2.8.0.pre,start = 0
2.8.0.pre,batch_size = self.transform_batch_size
2.8.0.pre,while True:
2.8.0.pre,"end = min((start + 1) * batch_size, X.shape[0])"
2.8.0.pre,X_batch = X[(start * batch_size):end]
2.8.0.pre,output = self.sess.run(
2.8.0.pre,"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
2.8.0.pre,X_out.append(output)
2.8.0.pre,num_transformed = num_transformed + X_batch.shape[0]
2.8.0.pre,logger.info('%i samples transformed' % num_transformed)
2.8.0.pre,start += 1
2.8.0.pre,if end >= len(X):
2.8.0.pre,break
2.8.0.pre,"X_new = np.concatenate(X_out, axis=0)"
2.8.0.pre,assert X_new.shape[0] == X.shape[0]
2.8.0.pre,"return (X_new, y, w)"
2.8.0.pre,"def untransform(self, z):"
2.8.0.pre,raise NotImplementedError(
2.8.0.pre,"""Cannot untransform datasets with ANITransformer."")"
2.8.0.pre,def build(self):
2.8.0.pre,""""""" tensorflow computation graph for transform """""""
2.8.0.pre,import tensorflow as tf
2.8.0.pre,graph = tf.Graph()
2.8.0.pre,with graph.as_default():
2.8.0.pre,self.inputs = tf.keras.Input(
2.8.0.pre,"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
2.8.0.pre,"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
2.8.0.pre,flags = tf.sign(atom_numbers)
2.8.0.pre,flags = tf.cast(
2.8.0.pre,"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
2.8.0.pre,"coordinates = self.inputs[:, :, 1:]"
2.8.0.pre,if self.coordinates_in_bohr:
2.8.0.pre,coordinates = coordinates * 0.52917721092
2.8.0.pre,"d = self.distance_matrix(coordinates, flags)"
2.8.0.pre,"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
2.8.0.pre,"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
2.8.0.pre,"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
2.8.0.pre,"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
2.8.0.pre,coordinates)
2.8.0.pre,self.outputs = tf.concat(
2.8.0.pre,[
2.8.0.pre,"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
2.8.0.pre,angular_sym
2.8.0.pre,"],"
2.8.0.pre,axis=2)
2.8.0.pre,return graph
2.8.0.pre,"def distance_matrix(self, coordinates, flags):"
2.8.0.pre,""""""" Generate distance matrix """""""
2.8.0.pre,import tensorflow as tf
2.8.0.pre,max_atoms = self.max_atoms
2.8.0.pre,"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
2.8.0.pre,"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
2.8.0.pre,# Calculate pairwise distance
2.8.0.pre,"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
2.8.0.pre,# Masking for valid atom index
2.8.0.pre,d = d * flags
2.8.0.pre,return d
2.8.0.pre,"def distance_cutoff(self, d, cutoff, flags):"
2.8.0.pre,""""""" Generate distance matrix with trainable cutoff """""""
2.8.0.pre,import tensorflow as tf
2.8.0.pre,# Cutoff with threshold Rc
2.8.0.pre,d_flag = flags * tf.sign(cutoff - d)
2.8.0.pre,d_flag = tf.nn.relu(d_flag)
2.8.0.pre,d_flag = d_flag * tf.expand_dims(
2.8.0.pre,"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
2.8.0.pre,d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
2.8.0.pre,return d * d_flag
2.8.0.pre,"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
2.8.0.pre,""""""" Radial Symmetry Function """""""
2.8.0.pre,import tensorflow as tf
2.8.0.pre,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.8.0.pre,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.8.0.pre,"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
2.8.0.pre,ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
2.8.0.pre,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
2.8.0.pre,"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
2.8.0.pre,length = ita.get_shape().as_list()[-1]
2.8.0.pre,"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
2.8.0.pre,"d = tf.stack([d] * length, axis=3)"
2.8.0.pre,out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
2.8.0.pre,if self.atomic_number_differentiated:
2.8.0.pre,out_tensors = []
2.8.0.pre,for atom_type in self.atom_cases:
2.8.0.pre,selected_atoms = tf.expand_dims(
2.8.0.pre,"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
2.8.0.pre,axis=3)
2.8.0.pre,"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
2.8.0.pre,"return tf.concat(out_tensors, axis=2)"
2.8.0.pre,else:
2.8.0.pre,"return tf.reduce_sum(out, axis=2)"
2.8.0.pre,"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
2.8.0.pre,""""""" Angular Symmetry Function """""""
2.8.0.pre,import tensorflow as tf
2.8.0.pre,max_atoms = self.max_atoms
2.8.0.pre,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.8.0.pre,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.8.0.pre,"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
2.8.0.pre,ita = 3 / (Rs[1] - Rs[0])**2
2.8.0.pre,"thetas = np.linspace(0., np.pi, self.angular_length)"
2.8.0.pre,zeta = float(self.angular_length**2)
2.8.0.pre,"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
2.8.0.pre,"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0.pre,"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0.pre,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0.pre,"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
2.8.0.pre,length = zeta.get_shape().as_list()[-1]
2.8.0.pre,"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
2.8.0.pre,"[coordinates] * max_atoms, 2)"
2.8.0.pre,"R_ij = tf.stack([d] * max_atoms, axis=3)"
2.8.0.pre,"R_ik = tf.stack([d] * max_atoms, axis=2)"
2.8.0.pre,"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
2.8.0.pre,"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
2.8.0.pre,# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.8.0.pre,"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
2.8.0.pre,"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
2.8.0.pre,vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
2.8.0.pre,"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
2.8.0.pre,"R_ij = tf.stack([R_ij] * length, axis=4)"
2.8.0.pre,"R_ik = tf.stack([R_ik] * length, axis=4)"
2.8.0.pre,"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
2.8.0.pre,"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
2.8.0.pre,"theta = tf.stack([theta] * length, axis=4)"
2.8.0.pre,"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
2.8.0.pre,tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
2.8.0.pre,if self.atomic_number_differentiated:
2.8.0.pre,out_tensors = []
2.8.0.pre,"for id_j, atom_type_j in enumerate(self.atom_cases):"
2.8.0.pre,for atom_type_k in self.atom_cases[id_j:]:
2.8.0.pre,"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
2.8.0.pre,"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
2.8.0.pre,selected_atoms = tf.expand_dims(
2.8.0.pre,"tf.expand_dims(selected_atoms, axis=1), axis=4)"
2.8.0.pre,out_tensors.append(
2.8.0.pre,"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
2.8.0.pre,"return tf.concat(out_tensors, axis=2)"
2.8.0.pre,else:
2.8.0.pre,"return tf.reduce_sum(out_tensor, axis=(2, 3))"
2.8.0.pre,def get_num_feats(self):
2.8.0.pre,n_feat = self.outputs.get_shape().as_list()[-1]
2.8.0.pre,return n_feat
2.8.0.pre,flake8: noqa
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a y transformer
2.8.0.pre,Check w is unchanged since this is a y transformer
2.8.0.pre,Check y is now a logarithmic version of itself
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check y is unchanged since this is a X transformer
2.8.0.pre,Check w is unchanged since this is a y transformer
2.8.0.pre,Check y is now a logarithmic version of itself
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a y transformer
2.8.0.pre,Check w is unchanged since this is a y transformer
2.8.0.pre,Check y is now a logarithmic version of itself
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Tests logarithmic data transformer with selection.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check y is unchanged since this is a X transformer
2.8.0.pre,Check w is unchanged since this is a y transformer
2.8.0.pre,Check y is now a logarithmic version of itself
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check y is unchanged since this is an X transformer
2.8.0.pre,Check w is unchanged since this is an X transformer
2.8.0.pre,Check X is now holding the proper values when sorted.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is an y transformer
2.8.0.pre,Check w is unchanged since this is an y transformer
2.8.0.pre,Check y is now holding the proper values when sorted.
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check y is unchanged since this is an X transformer
2.8.0.pre,Check w is unchanged since this is an X transformer
2.8.0.pre,Check X is now holding the proper values when sorted.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a y transformer
2.8.0.pre,Check w is unchanged since this is a y transformer
2.8.0.pre,Check y is now holding the proper values when sorted.
2.8.0.pre,Check ids are unchanged before and after transformation
2.8.0.pre,Check X is unchanged since transform_y is true
2.8.0.pre,Check w is unchanged since transform_y is true
2.8.0.pre,Check minimum and maximum values of transformed y are 0 and 1
2.8.0.pre,Check untransform works correctly
2.8.0.pre,Check ids are unchanged before and after transformation
2.8.0.pre,Check X is unchanged since transform_y is true
2.8.0.pre,Check w is unchanged since transform_y is true
2.8.0.pre,Check minimum and maximum values of transformed y are 0 and 1
2.8.0.pre,Test if dimensionality expansion is handled correctly by untransform
2.8.0.pre,Check ids are unchanged before and after transformation
2.8.0.pre,Check X is unchanged since transform_y is true
2.8.0.pre,Check w is unchanged since transform_y is true
2.8.0.pre,Check minimum and maximum values of transformed y are 0 and 1
2.8.0.pre,Check untransform works correctly
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,The transformer generates n DAGs for a molecule with n
2.8.0.pre,"atoms. These are denoted the ""parents"""
2.8.0.pre,extract only the images (no need of the labels)
2.8.0.pre,reshaping the vector to image
2.8.0.pre,Check Blurring
2.8.0.pre,Check center crop
2.8.0.pre,Check crop
2.8.0.pre,Check convert2gray
2.8.0.pre,Check rotation
2.8.0.pre,Some more test cases for flip
2.8.0.pre,Check flip
2.8.0.pre,Check Scales
2.8.0.pre,Check shift
2.8.0.pre,check gaussian noise
2.8.0.pre,check salt and pepper noise
2.8.0.pre,Check median filter
2.8.0.pre,transforming y should raise an exception
2.8.0.pre,transforming w should raise an exception
2.8.0.pre,transforming X should be okay
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a y transformer
2.8.0.pre,Check w is unchanged since this is a y transformer
2.8.0.pre,"Check that y_t has zero mean, unit std."
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check y is unchanged since this is a X transformer
2.8.0.pre,Check w is unchanged since this is a y transformer
2.8.0.pre,"Check that X_t has zero mean, unit std."
2.8.0.pre,np.set_printoptions(threshold='nan')
2.8.0.pre,Entries with zero std are not normalized
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a w transformer
2.8.0.pre,Check y is unchanged since this is a w transformer
2.8.0.pre,Assert that entries with zero weight retain zero weight
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a w transformer
2.8.0.pre,Check y is unchanged since this is a w transformer
2.8.0.pre,Assert that entries with zero weight retain zero weight
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a w transformer
2.8.0.pre,Check y is unchanged since this is a w transformer
2.8.0.pre,Assert that entries with zero weight retain zero weight
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a w transformer
2.8.0.pre,Check y is unchanged since this is a w transformer
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is a w transformer
2.8.0.pre,Check y is unchanged since this is a w transformer
2.8.0.pre,Assert that entries with zero weight retain zero weight
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check y is unchanged since this is an X transformer
2.8.0.pre,Check w is unchanged since this is an X transformer
2.8.0.pre,Check X is now holding the proper values in each column.
2.8.0.pre,Check ids are unchanged.
2.8.0.pre,Check X is unchanged since this is an X transformer
2.8.0.pre,Check w is unchanged since this is an X transformer
2.8.0.pre,Check y is now holding the proper values in each column.
2.8.0.pre,Check that untransform does the right thing.
2.8.0.pre,Check that we have length 8 now with duplication
2.8.0.pre,Check shapes
2.8.0.pre,Check that we have 4 positives and 4 negatives
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,Note that nothing should change in this dataset since weights balance!
2.8.0.pre,Check that still we have length 6
2.8.0.pre,Check shapes
2.8.0.pre,Check that we have 2 positives and 4 negatives
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,Check that we have length 8 now with duplication
2.8.0.pre,Check shapes
2.8.0.pre,Check that we have 4 positives and 4 negatives
2.8.0.pre,Check that sum of 0s equals sum of 1s in transformed for each task
2.8.0.pre,6-1 imbalance in favor of class 0
2.8.0.pre,Check that we have length 30 now with duplication
2.8.0.pre,Check shapes
2.8.0.pre,Check that we have 6 of each class
2.8.0.pre,Check that sum of all class weights is equal by comparing to 0 weight
2.8.0.pre,Note class imbalance. This will round to 2x duplication for 1
2.8.0.pre,Check that we have length 13 now with duplication
2.8.0.pre,Check shapes
2.8.0.pre,Check that we have 6 positives and 7 negatives
2.8.0.pre,safe operations
2.8.0.pre,occupation number gradients
2.8.0.pre,get the length of the tensor output
2.8.0.pre,other tensor ops
2.8.0.pre,add the diagonal with a small eps to safeguard from nan
2.8.0.pre,replace the diagonal with infinite (usually used for coulomb matrix)
2.8.0.pre,################################################################
2.8.0.pre,save.py is out of date. You should not import any functions from here.
2.8.0.pre,################################################################
2.8.0.pre,flake8: noqa
2.8.0.pre,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.8.0.pre,Walk through the original file and extract ATOM/HETATM lines and
2.8.0.pre,add PDBQT charge annotations.
2.8.0.pre,Remove rotatable bonds from this molecule
2.8.0.pre,Get the connected components now that the rotatable bonds have
2.8.0.pre,been removed.
2.8.0.pre,The root is the largest connected component.
2.8.0.pre,Write the root component
2.8.0.pre,"We've looked at the root, so take note of that"
2.8.0.pre,Compute partial charges on molecule if RDKit Mol
2.8.0.pre,indices to atoms to keep
2.8.0.pre,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.8.0.pre,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.8.0.pre,nonzero contact.
2.8.0.pre,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.8.0.pre,TODO: This is duplicated! Clean up
2.8.0.pre,Updates charges in place
2.8.0.pre,data.shape and segment_ids.shape should be equal
2.8.0.pre,"return V.set_(V.storage(), V.storage_offset(), V.size(), tuple(reversed(V.stride())))"
2.8.0.pre,initial embedding
2.8.0.pre,minimization and pruning
2.8.0.pre,always keep lowest-energy conformer
2.8.0.pre,discard conformers after max_conformers is reached
2.8.0.pre,get RMSD to selected conformers
2.8.0.pre,discard conformers within the RMSD threshold
2.8.0.pre,create a new molecule to hold the chosen conformers
2.8.0.pre,this ensures proper conformer IDs and energy-based ordering
2.8.0.pre,isotope-averaged atom masses in a.m.u.
2.8.0.pre,from https://www.angelo.edu/faculty/kboudrea/periodic/structure_mass.htm
2.8.0.pre,"JCP 41, 3199 (1964); DOI:10.1063/1.1725697"
2.8.0.pre,taken from PySCF:
2.8.0.pre,https://github.com/pyscf/pyscf/blob/45582e915e91890722fcae2bc30fb04867d5c95f/pyscf/data/radii.py#L23
2.8.0.pre,I don't know why H has 0.35 while in the reference it is 0.
2.8.0.pre,"They are in angstrom, so we need to convert it to Bohr"
2.8.0.pre,False here specifies that water is to be removed
2.8.0.pre,Updates charges in place
2.8.0.pre,TODO: This is wrong. Should return all molecules
2.8.0.pre,TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.8.0.pre,This updates in place
2.8.0.pre,indices of atoms to keep
2.8.0.pre,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.8.0.pre,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.8.0.pre,nonzero contact.
2.8.0.pre,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.8.0.pre,####################################################
2.8.0.pre,Compute partial charges on molecule if rdkit
2.8.0.pre,####################################################
2.8.0.pre,Number of voxels per one edge of box to voxelize.
2.8.0.pre,NOTE We have lot of type ignores here since grover mol-graph which is of type
2.8.0.pre,"GraphData have kwargs which are not attributes of GraphData. Hence, in these"
2.8.0.pre,cases mypy raises GraphData does not have attributes `..`.
2.8.0.pre,max with 1 to fix a crash in rare case of all single-heavy-atom mols
2.8.0.pre,graph_index indicates which atom belongs to which molecule
2.8.0.pre,padding
2.8.0.pre,computing a2b
2.8.0.pre,only needed if using atom messages
2.8.0.pre,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.8.0.pre,If interval1 < interval2 entirely
2.8.0.pre,If interval2 < interval1 entirely
2.8.0.pre,Each triangle in the simplices is a set of 3 atoms from
2.8.0.pre,coordinates which forms the vertices of an exterior triangle on
2.8.0.pre,the convex hull of the macromolecule.
2.8.0.pre,Points is the set of atom coordinates that make up this
2.8.0.pre,triangular face on the convex hull
2.8.0.pre,Let's extract x/y/z coords for this face
2.8.0.pre,Let's compute min/max points
2.8.0.pre,"Nitrogen has atomic number 7, and oxygen 8."
2.8.0.pre,If atom is a hydrogen
2.8.0.pre,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.8.0.pre,If atom is a hydrogen
2.8.0.pre,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.8.0.pre,if ring from mol1 is aromatic
2.8.0.pre,...and atom from mol2 is a cation
2.8.0.pre,if angle and distance are correct
2.8.0.pre,count atoms forming a contact
2.8.0.pre,if ring is aromatic
2.8.0.pre,"save its indices, center, and normal"
2.8.0.pre,remember mol1-mol2 pairs we already counted
2.8.0.pre,"if this pair is new, count atoms forming a contact"
2.8.0.pre,"if this pair is new, count atoms forming a contact"
2.8.0.pre,find interacting rings from mol1 and cations from mol2
2.8.0.pre,find interacting cations from mol1 and rings from mol2
2.8.0.pre,merge counters
2.8.0.pre,the line has format
2.8.0.pre,REMARK VINA RESULT: score ...
2.8.0.pre,There is only 1 such line per model so we can append it
2.8.0.pre,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.8.0.pre,Apply common fixes to PDB files
2.8.0.pre,Optimize ligand
2.8.0.pre,"For a node-prediction task, label is not added to edge features and other global features"
2.8.0.pre,because label here is a node-level attribute and not a graph-level attribute
2.8.0.pre,"In this case, the 'y' attribute of GraphData will contain the"
2.8.0.pre,node-level labels.
2.8.0.pre,not a self-loop
2.8.0.pre,Make sure input is a list
2.8.0.pre,FIXME: Incompatible types in assignment
2.8.0.pre,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.8.0.pre,Ensure that metric is wrapped in a list.
2.8.0.pre,This case checks if input is a function then wraps a
2.8.0.pre,dc.metrics.Metric object around it
2.8.0.pre,Process input metrics
2.8.0.pre,Compute multitask metrics
2.8.0.pre,We use y/w to aggregate labels/weights across generator.
2.8.0.pre,This is a KerasModel.
2.8.0.pre,Some datasets have weights
2.8.0.pre,Process predictions and populate y/w lists
2.8.0.pre,Combine labels/weights
2.8.0.pre,Undo data transformations.
2.8.0.pre,Compute multitask metrics
2.8.0.pre,for each node (E[(X-E[X])^n])^{1/n}
2.8.0.pre,EPS is added to the absolute value of expectation before taking the nth root for stability
2.8.0.pre,"each scaler is a function that takes as input X (B x N x Din), adj (B x N x N) and"
2.8.0.pre,avg_d (dictionary containing averages over training set) and returns X_scaled (B x N x Din) as output
2.8.0.pre,Generate the raising operator matrix
2.8.0.pre,Generate the lowering operator matrix
2.8.0.pre,Generate the z-generator matrix
2.8.0.pre,"Combine the matrices to form the x, z, and y generators"
2.8.0.pre,Stack the generators along the first dimension to create a tensor
2.8.0.pre,The transformation matrix generated is used to change the basis of a vector of
2.8.0.pre,real spherical harmonics with representation index 1 to complex spherical harmonics.
2.8.0.pre,"Construct the transformation matrix Q for m in range(-k, 0)"
2.8.0.pre,Set the diagonal elements for m = 0
2.8.0.pre,"Construct the transformation matrix Q for m in range(1, k + 1)"
2.8.0.pre,Apply the factor of (-1j)**k to make the Clebsch-Gordan coefficients real
2.8.0.pre,Handle dtype and device options
2.8.0.pre,Ensure the tensor is contiguous and on the specified device
2.8.0.pre,Get the SU(2) generators for the given quantum angular momentum (spin) value.
2.8.0.pre,Get the transformation matrix to change the basis from real to complex spherical harmonics.
2.8.0.pre,Convert the SU(2) generators to the SO(3) basis using the transformation matrix Q.
2.8.0.pre,"X represents the SU(2) generators, and Q is the transformation matrix from real to complex spherical harmonics."
2.8.0.pre,The resulting X matrix will be the SO(3) generators in the complex basis.
2.8.0.pre,Return the real part of the SO(3) generators to ensure they are purely real.
2.8.0.pre,"Ensure that alpha, beta, and gamma have the same shape for broadcasting."
2.8.0.pre,"Ensure the angles are within the range [0, 2*pi) using modulo."
2.8.0.pre,Get the SO(3) generators for the given quantum angular momentum (spin) value 'k'.
2.8.0.pre,Calculate the Wigner D matrix using the matrix exponential of the generators
2.8.0.pre,"and the rotation angles alpha, beta, and gamma in the appropriate order."
2.8.0.pre,These functions have moved to deepchem.utils_docking_utils
2.8.0.pre,flake8: noqa
2.8.0.pre,The number of elements to print for dataset ids/tasks
2.8.0.pre,"If a dataset contains more than this number of elements, it won't"
2.8.0.pre,print any dataset ids
2.8.0.pre,Warnings
2.8.0.pre,An activation function for a layer: either a function or the name of a standard activation
2.8.0.pre,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.8.0.pre,"A single value of some type, or multiple values of that type"
2.8.0.pre,The shape of a NumPy array
2.8.0.pre,"A NumPy array, or an object that can be converted to one.  Once we move to"
2.8.0.pre,"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
2.8.0.pre,type of RDKit object
2.8.0.pre,type of Pymatgen object
2.8.0.pre,Calculation of Step Size and steps
2.8.0.pre,Number of atoms per molecule is calculated by counting all the non zero elements(numbers) of every molecule.
2.8.0.pre,"It loops over the molecules in the Coulomb matrix and takes the ""2.4"" root of the diagonal of ""2X"" of each molecule's representation."
2.8.0.pre,Calculates the Gaussian Distance by passing distance by a gaussian function.
2.8.0.pre,Generate a random temporary file name
2.8.0.pre,Ensure the file is created
2.8.0.pre,Open the file in the given mode
2.8.0.pre,dtype=object allows for arrays(images here) of arbitrary size
2.8.0.pre,Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
2.8.0.pre,Structures are stored in .sdf file
2.8.0.pre,"Note: Here, the order of columns is based on the order in which the values"
2.8.0.pre,"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
2.8.0.pre,"tasks above, they occur after `tasks` here."
2.8.0.pre,"FIXME Ideally, we should use something like a dictionary here to keep it independent"
2.8.0.pre,of column ordering.
2.8.0.pre,Reset aggregator
2.8.0.pre,Handle final leftovers for this file
2.8.0.pre,First line of user-specified CSV *must* be header.
2.8.0.pre,"If gzipped, need to compute extension again"
2.8.0.pre,First line of user-specified CSV *must* be header.
2.8.0.pre,The label encoder is given characters for ACGTN
2.8.0.pre,Peak at the first sequence to get the length of the sequence.
2.8.0.pre,"pattern to split the names, e.g. ""model.params[1]"" into [""model"", ""params"", ""[1]""]"
2.8.0.pre,"return: (nao, nao)"
2.8.0.pre,init an one-hot vector
2.8.0.pre,"If include_unknown_set is True, set the last index is 1."
2.8.0.pre,################################################################
2.8.0.pre,atom (node) featurization
2.8.0.pre,################################################################
2.8.0.pre,################################################################
2.8.0.pre,bond (edge) featurization
2.8.0.pre,################################################################
2.8.0.pre,get the unique parameters of A
2.8.0.pre,separate the parameters for A and for M
2.8.0.pre,"grad_x: (*BABEM, nr, ncols)"
2.8.0.pre,"x: (*BABEM, nr, ncols)"
2.8.0.pre,solve (A-biases*M)^T v = grad_x
2.8.0.pre,this is the grad of B
2.8.0.pre,calculate the grad of matrices parameters
2.8.0.pre,calculate the biases gradient
2.8.0.pre,calculate the gradient to the biases matrices
2.8.0.pre,Hidden
2.8.0.pre,"NOTE: currently only works for batched B (1 batch dim), but unbatched A"
2.8.0.pre,check the parameters
2.8.0.pre,convert the numpy/scipy
2.8.0.pre,NOTE: The line below is very inefficient for large na and ncols
2.8.0.pre,"if B is all zeros, then return zeros"
2.8.0.pre,setup the preconditioning and the matrix problem
2.8.0.pre,get the stopping matrix
2.8.0.pre,prepare the initial guess (it's just all zeros)
2.8.0.pre,correct the residual calculation
2.8.0.pre,check for the stopping condition
2.8.0.pre,move to the next index
2.8.0.pre,"x: (ncols, *, nr, 1)"
2.8.0.pre,"if B is all zeros, then return zeros"
2.8.0.pre,setup the preconditioning and the matrix problem
2.8.0.pre,get the stopping matrix
2.8.0.pre,prepare the initial guess (it's just all zeros)
2.8.0.pre,correct the residual calculation regularly
2.8.0.pre,calculate the residual
2.8.0.pre,save the best results
2.8.0.pre,check for the stopping conditions
2.8.0.pre,"x: (ncols, *, nr, 1)"
2.8.0.pre,"if B is all zeros, then return zeros"
2.8.0.pre,setup the preconditioning and the matrix problem
2.8.0.pre,get the stopping matrix
2.8.0.pre,prepare the initial guess (it's just all zeros)
2.8.0.pre,res = res * B_norm
2.8.0.pre,save the best results
2.8.0.pre,general helpers
2.8.0.pre,get the linear operator (including the MXE part)
2.8.0.pre,"A: (*BA, nr, nr) linop"
2.8.0.pre,"B: (*BB, nr, ncols)"
2.8.0.pre,"E: (*BE, ncols)"
2.8.0.pre,"M: (*BM, nr, nr) linop"
2.8.0.pre,"x: (ncols, *BX, nr, 1)"
2.8.0.pre,"x: (ncols, *BX, nr, 1)"
2.8.0.pre,estimate if it's posdef with power iteration
2.8.0.pre,set posdef to False to make the operator becomes AT * A so it is
2.8.0.pre,hermitian
2.8.0.pre,TODO: the posdef check by largest eival only works for Hermitian/symmetric
2.8.0.pre,"matrix, but it doesn't always work for non-symmetric matrix."
2.8.0.pre,"In non-symmetric case, one need to do Cholesky LDL decomposition"
2.8.0.pre,"if the largest eigenvalue is negative, then it's not posdef"
2.8.0.pre,"otherwise, calculate the lowest eigenvalue to check if it's positive"
2.8.0.pre,get the linear operation if it is not a posdef (A -> AT.A)
2.8.0.pre,cg and bicgstab helpers
2.8.0.pre,rootfinder-based
2.8.0.pre,using rootfinder algorithm
2.8.0.pre,set up the function for the rootfinding
2.8.0.pre,"xi: (*BX, nr*ncols)"
2.8.0.pre,setup the initial guess (the batch dimension must be the largest)
2.8.0.pre,check if xnorm is converging
2.8.0.pre,Broadcast Utilities
2.8.0.pre,check the hermitian
2.8.0.pre,check which methods are implemented
2.8.0.pre,@abstractmethod
2.8.0.pre,@abstractmethod # (optional)
2.8.0.pre,@abstractmethod
2.8.0.pre,@abstractmethod
2.8.0.pre,implemented functions
2.8.0.pre,use batched mv as mm
2.8.0.pre,move the last dimension to the very first dimension to be broadcasted
2.8.0.pre,apply batched mv and restore the initial shape
2.8.0.pre,use batched mv as mm
2.8.0.pre,move the last dimension to the very first dimension to be broadcasted
2.8.0.pre,apply batched mv and restore the initial shape
2.8.0.pre,special functions
2.8.0.pre,properties
2.8.0.pre,implementation
2.8.0.pre,private functions
2.8.0.pre,"xt: (*BY, p)"
2.8.0.pre,"xdummy: (*BY, q)"
2.8.0.pre,calculate y = Ax
2.8.0.pre,calculate (dL/dx)^T = A^T (dL/dy)^T with (dL/dy)^T = xt
2.8.0.pre,Helper Classes
2.8.0.pre,"if it is a method from an object, unroll the parameters and add"
2.8.0.pre,the object's parameters as well
2.8.0.pre,get the unique ids
2.8.0.pre,search the id if it has been added to the list
2.8.0.pre,debugging
2.8.0.pre,check the method input
2.8.0.pre,assert if the method preserve the float tensors of the object
2.8.0.pre,now assert if all_params0 == all_params1
2.8.0.pre,get all tensor parameters in the object
2.8.0.pre,get the parameter tensors used in the operation and the tensors specified by the developer
2.8.0.pre,check if the userparams contains non-tensor
2.8.0.pre,"check if there are missing parameters (present in operating params, but not in the user params)"
2.8.0.pre,if oper_names[i] not in user_names:
2.8.0.pre,"if there are missing parameters, give a warning (because the program"
2.8.0.pre,"can still run correctly, e.g. missing parameters are parameters that"
2.8.0.pre,are never set to require grad)
2.8.0.pre,"check if there are excessive parameters (present in the user params, but not in the operating params)"
2.8.0.pre,if user_names[i] not in oper_names:
2.8.0.pre,"if there are excess parameters, give warnings"
2.8.0.pre,get all the tensors recursively
2.8.0.pre,copy the tensors and require them to be differentiable
2.8.0.pre,run the method and see which one has the gradients
2.8.0.pre,return the original tensor
2.8.0.pre,traversing functions
2.8.0.pre,None is set as default arg to avoid expanding list for multiple
2.8.0.pre,invokes of _get_tensors without exception_ids argument
2.8.0.pre,add exception to avoid infinite loop if there is a mutual dependant on objects
2.8.0.pre,get the tensors recursively towards torch.nn.Module
2.8.0.pre,traverse down the object to collect the tensors
2.8.0.pre,traverse down the object to collect the tensors
2.8.0.pre,flake8: noqa
2.8.0.pre,TODO: implement robust LOBPCG and put it here
2.8.0.pre,get the unique parameters of A & M
2.8.0.pre,adapted from scipy.sparse.linalg.svds
2.8.0.pre,clamp the eigenvalues to a small positive values to avoid numerical
2.8.0.pre,instability
2.8.0.pre,separate the sets of parameters
2.8.0.pre,options for calculating the backward (not for `solve`)
2.8.0.pre,save for the backward
2.8.0.pre,"evals: (*BAM, neig)"
2.8.0.pre,"evecs: (*BAM, na, neig)"
2.8.0.pre,get the variables from ctx
2.8.0.pre,set the default values of degen_*tol
2.8.0.pre,check the degeneracy
2.8.0.pre,"idx_degen: (*BAM, neig, neig)"
2.8.0.pre,the loss function where the gradient will be retrieved
2.8.0.pre,"warnings: if not all params have the connection to the output of A,"
2.8.0.pre,it could cause an infinite loop because pytorch will keep looking
2.8.0.pre,for the *params node and propagate further backward via the `evecs`
2.8.0.pre,path. So make sure all the *params are all connected in the graph.
2.8.0.pre,"if degenerate, check the conditions for finite derivative"
2.8.0.pre,"if the requirements are not satisfied, raises a warning"
2.8.0.pre,calculate the contributions from the eigenvalues
2.8.0.pre,calculate the contributions from the eigenvectors
2.8.0.pre,orthogonalize the grad_evecs with evecs
2.8.0.pre,"Based on test cases, complex datatype is more likely to suffer from"
2.8.0.pre,"singularity error when doing the inverse. Therefore, I add a small"
2.8.0.pre,offset here to prevent that from happening
2.8.0.pre,orthogonalize gevecs w.r.t. evecs
2.8.0.pre,accummulate the gradient contributions
2.8.0.pre,the contribution from the parallel elements
2.8.0.pre,"evals: (*BAM, neig)"
2.8.0.pre,get the index of degeneracies
2.8.0.pre,contracted using opt_einsum
2.8.0.pre,"evals, evecs = torch.linalg.eigh(Amatrix, eigenvectors=True)  # (*BA, q), (*BA, q, q)"
2.8.0.pre,M decomposition to make A symmetric
2.8.0.pre,it is done this way to make it numerically stable in avoiding
2.8.0.pre,complex eigenvalues for (near-)degenerate case
2.8.0.pre,calculate the eigenvalues and eigenvectors
2.8.0.pre,(the eigvecs are normalized in M-space)
2.8.0.pre,"evals, evecs = torch.linalg.eigh(A2, eigenvectors=True)  # (*BAM, q, q)"
2.8.0.pre,temporary solution to https://github.com/pytorch/pytorch/issues/47599
2.8.0.pre,remove the degenerate part
2.8.0.pre,see https://arxiv.org/pdf/2011.04366.pdf
2.8.0.pre,take the contribution from the eivec
2.8.0.pre,calculate the contribution from the eival
2.8.0.pre,symmetrize to reduce numerical instability
2.8.0.pre,TODO: optimize for large linear operator and strict min_eps
2.8.0.pre,Ideas:
2.8.0.pre,(1) use better strategy to get the estimate on eigenvalues
2.8.0.pre,(2) use restart strategy
2.8.0.pre,get the shape of the transformation
2.8.0.pre,set up the initial guess
2.8.0.pre,Can be optimized by saving AV from the previous iteration and only
2.8.0.pre,operate AV for the new V. This works because the old V has already
2.8.0.pre,"been orthogonalized, so it will stay the same"
2.8.0.pre,"AV = A.mm(V) # (*BAM,na,nguess)"
2.8.0.pre,eigvals are sorted from the lowest
2.8.0.pre,"eval: (*BAM, nguess), evec: (*BAM, nguess, nguess)"
2.8.0.pre,calculate the eigenvectors of A
2.8.0.pre,calculate the residual
2.8.0.pre,print information and check convergence
2.8.0.pre,apply the preconditioner
2.8.0.pre,orthogonalize t with the rest of the V
2.8.0.pre,orthogonalize V
2.8.0.pre,check idxs
2.8.0.pre,make the function a functional (depends on all parameters in the object)
2.8.0.pre,params tensor is the LinearOperator's parameters
2.8.0.pre,"if the object parameter is still the same, then use the pre-calculated values"
2.8.0.pre,"otherwise, reevaluate by replacing the parameters with the new tensor params"
2.8.0.pre,self.yfcn: (*nin)
2.8.0.pre,file mostly from SciPy:
2.8.0.pre,https://github.com/scipy/scipy/blob/914523af3bc03fe7bf61f621363fca27e97ca1d6/scipy/optimize/nonlin.py#L221
2.8.0.pre,and converted to PyTorch for GPU efficiency
2.8.0.pre,jacobian parameters
2.8.0.pre,stopping criteria
2.8.0.pre,algorithm parameters
2.8.0.pre,misc parameters
2.8.0.pre,"solving complex rootfinder by concatenating real and imaginary part,"
2.8.0.pre,making the variable twice as long
2.8.0.pre,represents x as a long real vector
2.8.0.pre,pack a long real vector into the shape accepted by fcn
2.8.0.pre,shorthand for the function
2.8.0.pre,set up the jacobian
2.8.0.pre,solver tolerance
2.8.0.pre,save the best results
2.8.0.pre,print out dx and df
2.8.0.pre,adjust forcing parameters for inexact solve
2.8.0.pre,jacobian parameters
2.8.0.pre,stopping criteria
2.8.0.pre,algorithm parameters
2.8.0.pre,misc parameters
2.8.0.pre,"No suitable step length found. Take the full Newton step,"
2.8.0.pre,and hope for the best.
2.8.0.pre,"Otherwise, compute the minimizer of a quadratic interpolant:"
2.8.0.pre,"Otherwise, loop with cubic interpolation until we find an alpha which"
2.8.0.pre,"satisfies the first Wolfe condition (since we are backtracking, we will"
2.8.0.pre,assume that the value of alpha is not too small and satisfies the second
2.8.0.pre,condition.
2.8.0.pre,Failed to find a suitable step length
2.8.0.pre,gd parameters
2.8.0.pre,stopping conditions
2.8.0.pre,misc parameters
2.8.0.pre,update the step
2.8.0.pre,check the stopping conditions
2.8.0.pre,gd parameters
2.8.0.pre,stopping conditions
2.8.0.pre,misc parameters
2.8.0.pre,update the step
2.8.0.pre,check the stopping conditions
2.8.0.pre,get the best values
2.8.0.pre,usually user set maxiter == 0 just to wrap the minimizer backprop
2.8.0.pre,taking most of the part from SciPy
2.8.0.pre,setup the approximate inverse Jacobian
2.8.0.pre,update Gm
2.8.0.pre,keep the rank small
2.8.0.pre,keep the rank small
2.8.0.pre,raise RuntimeError
2.8.0.pre,"u: (n, 1), s: (1,), vh: (1, n)"
2.8.0.pre,"equilibrium can use all rootfinder methods, but there are several methods developed specifically for"
2.8.0.pre,equilibrium (or fixed-point iterations). This dictionary gives the list of those special methods.
2.8.0.pre,"minimization can use rootfinder algorithm, so check if it is actually"
2.8.0.pre,"using the optimization algorithm, not the rootfinder algorithm"
2.8.0.pre,the rootfinder algorithms are designed to move to the opposite direction
2.8.0.pre,"of the output of the function, so the output of this function is just"
2.8.0.pre,the grad of z w.r.t. y
2.8.0.pre,"if it is going to optimization method, then also returns the value"
2.8.0.pre,"if using the optimization algorithm, then the forward function is the one"
2.8.0.pre,that returns f and grad
2.8.0.pre,"if it is just using the rootfinder algorithm, then the forward function"
2.8.0.pre,is the one that returns only the grad
2.8.0.pre,set default options
2.8.0.pre,split tensors and non-tensors params
2.8.0.pre,merge the tensor and nontensor parameters
2.8.0.pre,dL/df
2.8.0.pre,get the grad for the params
2.8.0.pre,anderson_acc parameters
2.8.0.pre,stopping criteria
2.8.0.pre,misc options
2.8.0.pre,"x0: (..., *nfeats)"
2.8.0.pre,"reshape x to have a shape of (batch_size, feats_dim)"
2.8.0.pre,"x: (..., *nfeats)"
2.8.0.pre,"xn: (..., feats_tot)"
2.8.0.pre,"x: (..., feats_dim)"
2.8.0.pre,"torch.bmm(g, g.transpose(-2, -1))"
2.8.0.pre,"alpha: (batch_size, nsize)"
2.8.0.pre,check the stopping condition
2.8.0.pre,update the xn
2.8.0.pre,One sequence has length longer than others. This should throw a
2.8.0.pre,ValueError.
2.8.0.pre,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.8.0.pre,Loosening atol to see if tests stop failing sporadically
2.8.0.pre,string set
2.8.0.pre,integer set
2.8.0.pre,include_unknown_set is False
2.8.0.pre,include_unknown_set is True
2.8.0.pre,check unknown atoms
2.8.0.pre,check original set
2.8.0.pre,"Generally, =O behaves as an electron acceptor"
2.8.0.pre,we must compute partial charges before using `get_atom_partial_charge`
2.8.0.pre,The C-N bond is a single bond
2.8.0.pre,"6 atoms: CC -> 2, CCC -> 3"
2.8.0.pre,"7 bonds: CC -> 2, CCC -> 4 (bonds are considered as undirected"
2.8.0.pre,and a single bond contributes to 2 bonds)
2.8.0.pre,graph-level labels
2.8.0.pre,node-level labels
2.8.0.pre,graph.y contains node-labels and graph.node_features.shape[0]
2.8.0.pre,holds number of nodes in that graph
2.8.0.pre,Get Data
2.8.0.pre,Checks that all atoms exits in array
2.8.0.pre,Checks shape of gaussian distance
2.8.0.pre,Checks all molecule membership exist
2.8.0.pre,Check Distance Membership shape
2.8.0.pre,Prepare Data
2.8.0.pre,Run
2.8.0.pre,Prepare Data
2.8.0.pre,Inputs property
2.8.0.pre,Without reverse input
2.8.0.pre,With revercse input
2.8.0.pre,Prepare Data
2.8.0.pre,Inputs property
2.8.0.pre,TODO test more formats for ligand
2.8.0.pre,Test if the output has the correct shape.
2.8.0.pre,Test for the case of zero momentum (j=0).
2.8.0.pre,Test for the case of momentum j=1 (spin-1).
2.8.0.pre,"Expected J_x, J_z, J_y matrices for j=1"
2.8.0.pre,"Test for j = 0, which means we have a 1x1 transformation matrix"
2.8.0.pre,"Test for j = 2, which means we have a 5x5 transformation matrix"
2.8.0.pre,Test for device placement (CPU to CUDA)
2.8.0.pre,Test for dtype conversion (complex128 to complex64)
2.8.0.pre,TODO test more formats for ligand
2.8.0.pre,adding hydrogens and charges is tested in dc.utils
2.8.0.pre,self.ligand_file is for 3ws9_ligand.sdf
2.8.0.pre,dummy function which can be passed as the parameter f to simultaneous_move and single_move
2.8.0.pre,test for gauss_initialize_position
2.8.0.pre,testing symmetric simultaneous_move
2.8.0.pre,testing asymmetric simultaneous_move
2.8.0.pre,testing symmetric single_move
2.8.0.pre,testing asymmetric single_move
2.8.0.pre,simple flat ring
2.8.0.pre,self.cycle4.Compute2DCoords()
2.8.0.pre,load and sanitize two real molecules
2.8.0.pre,parallel normals
2.8.0.pre,perpendicular normals
2.8.0.pre,too far away
2.8.0.pre,perpendicular normals
2.8.0.pre,parallel normals
2.8.0.pre,too far away
2.8.0.pre,order of the molecules shouldn't matter
2.8.0.pre,with this criteria we should find both types of stacking
2.8.0.pre,parallel normals
2.8.0.pre,perpendicular normals
2.8.0.pre,too far away
2.8.0.pre,def test_compute_cation_pi(self):
2.8.0.pre,"# TODO(rbharath): find better example, currently dicts are empty"
2.8.0.pre,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.8.0.pre,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.8.0.pre,"TODO find better example, currently dicts are empty"
2.8.0.pre,TODO test more formats for ligand
2.8.0.pre,Test on RDKit
2.8.0.pre,3D vector with unit length
2.8.0.pre,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.8.0.pre,"random coords between 0 and 1, so the max possible distance in sqrt(3)"
2.8.0.pre,check if correct distance metric was used
2.8.0.pre,Construct a random class probability matrix
2.8.0.pre,Construct a random class probability matrix
2.8.0.pre,"Note that since no name as provided, metrics are index by order"
2.8.0.pre,given.
2.8.0.pre,"Note that since no name as provided, metrics are index by order"
2.8.0.pre,given.
2.8.0.pre,"Note that since no name as provided, metrics are index by order"
2.8.0.pre,given.
2.8.0.pre,"Note that since no name as provided, metrics are index by order"
2.8.0.pre,given.
2.8.0.pre,TODO: Fix this case with correct thresholding
2.8.0.pre,TODO: Fix this case with correct thresholding
2.8.0.pre,There are 4 faces to the shape created by coords
2.8.0.pre,flake8: noqa
2.8.0.pre,get the location and weights of the integration in its original
2.8.0.pre,coordinate
2.8.0.pre,get the coordinate in Cartesian
2.8.0.pre,integration element
2.8.0.pre,Grid Transformations
2.8.0.pre,"xnew is from [xmin, xmax]"
2.8.0.pre,"r is approximately from [rmin, rmax]"
2.8.0.pre,type of the atom Z
2.8.0.pre,input types
2.8.0.pre,"if the basis has been normalized before, then do nothing"
2.8.0.pre,normalize to have individual gaussian integral to be 1 (if coeff is 1)
2.8.0.pre,normalize the coefficients in the basis (because some basis such as
2.8.0.pre,def2-svp-jkfit is not normalized to have 1 in overlap)
2.8.0.pre,input basis type
2.8.0.pre,QR decomposition's solution is not unique in a way that every column
2.8.0.pre,can be multiplied by -1 and it still a solution
2.8.0.pre,"So, to remove the non-uniqueness, we will make the sign of the sum"
2.8.0.pre,positive.
2.8.0.pre,construct the rotation parameters
2.8.0.pre,calculate the orthogonal orbital
2.8.0.pre,"orb: (*, nao, norb)"
2.8.0.pre,the orbital becomes the coefficients while params is all zeros (no rotation)
2.8.0.pre,properties
2.8.0.pre,setups
2.8.0.pre,fock matrix components
2.8.0.pre,interface to dm
2.8.0.pre,energy of the Hamiltonian
2.8.0.pre,free parameters for variational method
2.8.0.pre,Editable module
2.8.0.pre,1 / cell.vol == det(b) / (2 pi)^3
2.8.0.pre,drop ls that has norm > rcut * 1.05
2.8.0.pre,System Properties
2.8.0.pre,all-time calculations
2.8.0.pre,(i.e. meaning it does not have to be executed to run the functions below)
2.8.0.pre,"densinfo.value & lapl: (*BD, nr)"
2.8.0.pre,"densinfo.grad: (*BD, ndim, nr)"
2.8.0.pre,"return: (*BD, nr)"
2.8.0.pre,"densinfo.value & lapl: (*BD, nr)"
2.8.0.pre,"densinfo.grad: (*BD, ndim, nr)"
2.8.0.pre,return:
2.8.0.pre,"potentialinfo.value & lapl: (*BD, nr)"
2.8.0.pre,"potentialinfo.grad: (*BD, ndim, nr)"
2.8.0.pre,mark the densinfo components as requiring grads
2.8.0.pre,"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
2.8.0.pre,"mgga might only use one of either lapl or kin, so we need to change the deriv manually to 0s"
2.8.0.pre,set the vars to require grad and returns the previous state of the vars
2.8.0.pre,restore the state of requiring grad based on reqgrads list
2.8.0.pre,all vars before this function requires grad
2.8.0.pre,getting which parameters should require grad
2.8.0.pre,set the params to require grad
2.8.0.pre,special operations
2.8.0.pre,properties
2.8.0.pre,TODO: use regex!
2.8.0.pre,convert the atomz to tensor
2.8.0.pre,convert the atompos to tensor
2.8.0.pre,"convert to dtype if atomzs is a floating point tensor, not an integer tensor"
2.8.0.pre,flake8: noqa
2.8.0.pre,Get the degree id list (which corrects for min_deg)
2.8.0.pre,Get the size of each degree block
2.8.0.pre,Get the the start indices for items in each block
2.8.0.pre,Get the node indices when they are reset when the degree changes
2.8.0.pre,Convert to numpy array
2.8.0.pre,Reorder old atom_features
2.8.0.pre,Reorder old deg lists
2.8.0.pre,Sort membership
2.8.0.pre,Create old to new dictionary. not exactly intuitive
2.8.0.pre,Reorder adjacency lists
2.8.0.pre,Get numpy version of degree list for indexing
2.8.0.pre,"Initialize adj_lists, which supports min_deg = 1 only"
2.8.0.pre,Parse as deg separated
2.8.0.pre,Get indices corresponding to the current degree
2.8.0.pre,Extract and save adjacency list for the current degree
2.8.0.pre,Construct the slice information
2.8.0.pre,Get the cumulative indices after the first index
2.8.0.pre,Set indices with zero sized slices to zero to avoid indexing errors
2.8.0.pre,TODO(rbharath): Can this be removed?
2.8.0.pre,Use random insted of zeros to prevent weird issues with summing to zero
2.8.0.pre,"Combine the features, then sort them by (atom_degree, mol_index)"
2.8.0.pre,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.8.0.pre,Create a map from the original atom indices within each molecule to the
2.8.0.pre,indices in the combined object.
2.8.0.pre,Sort all atoms by degree.
2.8.0.pre,"Get the size of each atom list separated by molecule id, then by degree"
2.8.0.pre,Get the final size of each degree block
2.8.0.pre,"Get the index at which each degree starts, not resetting after each degree"
2.8.0.pre,And not stopping at any specific molecule
2.8.0.pre,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.8.0.pre,first column telling the start indices of each degree block and the
2.8.0.pre,second colum telling the size of each degree block
2.8.0.pre,Determine the membership (atom i belongs to molecule membership[i])
2.8.0.pre,Initialize the new degree separated adjacency lists
2.8.0.pre,Update the old adjacency lists with the new atom indices and then combine
2.8.0.pre,all together
2.8.0.pre,Iterate through all the molecules
2.8.0.pre,Get the adjacency lists for this molecule and current degree id
2.8.0.pre,"Correct all atom indices to the final indices, and then save the"
2.8.0.pre,results into the new adjacency lists
2.8.0.pre,Increment once row is done
2.8.0.pre,Get the final aggregated molecule
2.8.0.pre,Break the loop if max_records is set
2.8.0.pre,Break the loop if max_records is set
2.8.0.pre,Break the loop if max_records is set
2.8.0.pre,"Requriments - transformers, tokenizers"
2.8.0.pre,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.8.0.pre,The vocab may be expanded in the near future
2.8.0.pre,add vocab_file dict
2.8.0.pre,"unk_token=""[UNK]"","
2.8.0.pre,"sep_token=""[SEP]"","
2.8.0.pre,"pad_token=""[PAD]"","
2.8.0.pre,"cls_token=""[CLS]"","
2.8.0.pre,"mask_token=""[MASK]"","
2.8.0.pre,flake8: noqa
2.8.0.pre,Initalize with 1
2.8.0.pre,Replace the hybridization
2.8.0.pre,global possible_hybridization_list
2.8.0.pre,Allow 0 index to correspond to null molecule 1
2.8.0.pre,Correct for null
2.8.0.pre,"print(6-k-1, id)"
2.8.0.pre,Correct for last one
2.8.0.pre,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.8.0.pre,"Handle edge case of self-pairs (i, i)"
2.8.0.pre,Increment by 1 since we don't want 0-indexing
2.8.0.pre,"This creates a matrix of shape (2, num_pairs)"
2.8.0.pre,Get mapping
2.8.0.pre,first `bt_len` features are bond features(if applicable)
2.8.0.pre,For ring pairs outside max pairs distance continue
2.8.0.pre,`bt_len`-th feature is if the pair of atoms are in the same ring
2.8.0.pre,graph distance between two atoms
2.8.0.pre,distance is a matrix of 1-hot encoded distances for all atoms
2.8.0.pre,For ring pairs outside max pairs distance continue
2.8.0.pre,Euclidean distance between atoms
2.8.0.pre,atoms `radial` bonds away from `a1`
2.8.0.pre,atoms less than `radial` bonds away
2.8.0.pre,find atoms `radial`+1 bonds away
2.8.0.pre,create temporary valid ids serving to filter out failed featurizations from every sublist
2.8.0.pre,"of features (i.e. every molecules' frags list), and also totally failed sublists."
2.8.0.pre,This makes output digestable by Loaders
2.8.0.pre,Get the node features
2.8.0.pre,Stack nodes into an array
2.8.0.pre,Get bond lists with reverse edges included
2.8.0.pre,Get canonical adjacency list
2.8.0.pre,"Distance is either graph distance(True) or Euclidean distance(False,"
2.8.0.pre,only support datasets providing Cartesian coordinates)
2.8.0.pre,Set dtype
2.8.0.pre,If includes explicit hydrogens
2.8.0.pre,If uses use_chirality
2.8.0.pre,Atom features
2.8.0.pre,Stack nodes into an array
2.8.0.pre,Get bond lists
2.8.0.pre,Get canonical adjacency list
2.8.0.pre,Calculate pair features
2.8.0.pre,the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
2.8.0.pre,"SMILES is unique, so set a canonical order of atoms"
2.8.0.pre,Add hydrogens and generate a conformation.
2.8.0.pre,Record properties of the molecules.
2.8.0.pre,Create the output object.
2.8.0.pre,"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
2.8.0.pre,flake8: noqa
2.8.0.pre,base classes for featurizers
2.8.0.pre,molecule featurizers
2.8.0.pre,complex featurizers
2.8.0.pre,material featurizers
2.8.0.pre,biological sequence featurizers
2.8.0.pre,tokenizers
2.8.0.pre,support classes
2.8.0.pre,dqc dependencies
2.8.0.pre,get the density matrix from PySCF's CCSD calculation
2.8.0.pre,for str
2.8.0.pre,for list
2.8.0.pre,validation
2.8.0.pre,skip list
2.8.0.pre,skip path string
2.8.0.pre,main logic
2.8.0.pre,Find a successful featurization
2.8.0.pre,Replace failed featurizations with appropriate array
2.8.0.pre,Special case handling of single molecule
2.8.0.pre,Convert iterables to list
2.8.0.pre,condition if the original atom order is required
2.8.0.pre,"mol must be a RDKit Mol object, so parse a SMILES"
2.8.0.pre,"mol must be a RDKit Mol object, so parse a SMILES"
2.8.0.pre,"SMILES is unique, so set a canonical order of atoms"
2.8.0.pre,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.8.0.pre,atom_name is of format RESX-ATOMTYPE
2.8.0.pre,where X is a 1 to 4 digit number
2.8.0.pre,validate params
2.8.0.pre,"np.max() method works only for a non-empty array, so size of the array should be non-zero"
2.8.0.pre,Adding shapes of kwargs
2.8.0.pre,This assumes that the edge features for self loops are full-zero tensors
2.8.0.pre,In the future we may want to support featurization for self loops
2.8.0.pre,Create a mapping from the original node indices to the new node indices
2.8.0.pre,Filter and reindex node features
2.8.0.pre,Filter and reindex edge indices and edge features
2.8.0.pre,stack features
2.8.0.pre,"before stacking edge_features or node_pos_features,"
2.8.0.pre,we should check whether these are None or not
2.8.0.pre,create new edge index
2.8.0.pre,number of nodes in each graph
2.8.0.pre,cumulative number of nodes for each graph. This is necessary because the values in edge_index are node indices of all of the graphs in graph_list and so we need to offset the indices by the number of nodes in the previous graphs.
2.8.0.pre,"columns are the edge index, values are the node index"
2.8.0.pre,graph_index indicates which nodes belong to which graph
2.8.0.pre,Batch user defined attributes
2.8.0.pre,Convert edge_index to adjacency list
2.8.0.pre,Breadth-first search
2.8.0.pre,Setup image
2.8.0.pre,Compute bond properties
2.8.0.pre,Compute atom properties
2.8.0.pre,Setup image
2.8.0.pre,Compute bond properties
2.8.0.pre,Compute atom properties
2.8.0.pre,Reshape done for proper broadcast
2.8.0.pre,"Reshapes, and axes manipulations to facilitate vector processing."
2.8.0.pre,Draw a line between the two atoms.
2.8.0.pre,"The coordinates of this line, are indicated in line_coords"
2.8.0.pre,Turn the line coordinates into image positions
2.8.0.pre,Turn atomic coordinates into image positions
2.8.0.pre,Set the bond line coordinates to the bond property used.
2.8.0.pre,Set the atom positions in image to different atomic properties in channels
2.8.0.pre,With fixed res and img_size some molecules (e.g. long chains) may not fit.
2.8.0.pre,Check whether num_confs >=1 or not
2.8.0.pre,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.8.0.pre,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.8.0.pre,consistent with most QM software packages.
2.8.0.pre,Dimension of atom feature vector
2.8.0.pre,len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
2.8.0.pre,+ 2 at end for is_in_aromatic and mass
2.8.0.pre,dictionary of available feature generators
2.8.0.pre,for H2
2.8.0.pre,"not all features are equally long, so used methane as dummy molecule to determine length"
2.8.0.pre,Fix nans in features
2.8.0.pre,add edge list considering a directed graph
2.8.0.pre,get atom features
2.8.0.pre,get edge(bond) features
2.8.0.pre,get edge index
2.8.0.pre,get global features
2.8.0.pre,0 represents a masked bond
2.8.0.pre,atoms
2.8.0.pre,bonds
2.8.0.pre,"Graph connectivity in COO format with shape [2, num_edges]"
2.8.0.pre,"Edge feature matrix with shape [num_edges, num_edge_features]"
2.8.0.pre,Always treat the bond as directed.
2.8.0.pre,add mapping between bond b1 and atom a2 (destination atom)
2.8.0.pre,add mapping between bond id and atom id (a1)
2.8.0.pre,add mapping between bond id and atom a1 (source atom)
2.8.0.pre,update index on bond and reverse bond mappings
2.8.0.pre,generate SMILES for fragments
2.8.0.pre,Featurize data using featurize() in parent class
2.8.0.pre,Featurize str data
2.8.0.pre,Extend shorter strings with padding
2.8.0.pre,Padding before and after
2.8.0.pre,Featurize data using featurize() in parent class
2.8.0.pre,Featurize str data
2.8.0.pre,Featurize mol data
2.8.0.pre,Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
2.8.0.pre,"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
2.8.0.pre,load pretrained models
2.8.0.pre,convert errors to zero
2.8.0.pre,flake8: noqa
2.8.0.pre,If partial charges were not computed
2.8.0.pre,construct atom (node) feature
2.8.0.pre,construct edge (bond) index
2.8.0.pre,add edge list considering a directed graph
2.8.0.pre,construct edge (bond) feature
2.8.0.pre,load_sdf_files returns pos as strings but user can also specify
2.8.0.pre,numpy arrays for atom coordinates
2.8.0.pre,The 1.0 float value represents True Boolean
2.8.0.pre,This will return a boolean vector with all entries False
2.8.0.pre,To get the shortest paths between two nodes.
2.8.0.pre,To get info if two nodes belong to the same ring.
2.8.0.pre,Featurizer
2.8.0.pre,"row, col = edge_index"
2.8.0.pre,load_sdf_files returns pos as strings but user can also specify
2.8.0.pre,numpy arrays for atom coordinates
2.8.0.pre,get atom features
2.8.0.pre,get edge index
2.8.0.pre,user has not specified a descriptor list
2.8.0.pre,creates normalized functions dictionary if normalized features are required
2.8.0.pre,get cdf(feature) for that descriptor
2.8.0.pre,get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
2.8.0.pre,get required distribution_ from `scipy.stats` module.
2.8.0.pre,cdf => cumulative density functions
2.8.0.pre,make the cdf with the parameters.
2.8.0.pre,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.8.0.pre,Check whether num_confs >=1 or not
2.8.0.pre,Convert AtomPositions from Angstrom to bohr (atomic units)
2.8.0.pre,"`(1, max_atoms)` -> `(max_atoms,)`"
2.8.0.pre,similar to SNAP featurizer. both taken from Open Graph Benchmark (OGB) github.com/snap-stanford/ogb
2.8.0.pre,"The difference between this and the SNAP features is the lack of masking tokens, possible_implicit_valence_list, possible_bond_dirs"
2.8.0.pre,"and the prescence of possible_bond_stereo_list,  possible_is_conjugated_list, possible_is_in_ring_list,"
2.8.0.pre,FIXME Add support for multiple conformers (wip)
2.8.0.pre,"def __init__(self, num_conformers: int = 1, rmsd_cutoff: float = 2):"
2.8.0.pre,""""""""
2.8.0.pre,Initialize the RDKitConformerFeaturizer with the given parameters.
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,"num_conformers : int, optional, default=1"
2.8.0.pre,The number of conformers to generate for each molecule.
2.8.0.pre,"rmsd_cutoff : float, optional, default=2"
2.8.0.pre,The root-mean-square deviation (RMSD) cutoff value. Conformers with an RMSD
2.8.0.pre,greater than this value will be discarded.
2.8.0.pre,""""""""
2.8.0.pre,self.num_conformers = num_conformers
2.8.0.pre,self.rmsd_cutoff = rmsd_cutoff
2.8.0.pre,Derived from https://github.com/HannesStark/3DInfomax/blob/5cd32629c690e119bcae8726acedefdb0aa037fc/datasets/qm9_dataset_rdkit_conformers.py#L377
2.8.0.pre,add hydrogen bonds to molecule because they are not in the smiles representation
2.8.0.pre,FIXME Add support for multiple conformers (wip)
2.8.0.pre,"AllChem.EmbedMultipleConfs(mol, self.num_conformers)"
2.8.0.pre,AllChem.MMFFOptimizeMolecule(mol)
2.8.0.pre,rmsd_list = []
2.8.0.pre,"rdMolAlign.AlignMolConformers(mol, RMSlist=rmsd_list)"
2.8.0.pre,# insert 0 RMSD for first conformer
2.8.0.pre,"rmsd_list.insert(0, 0)"
2.8.0.pre,conformers = [
2.8.0.pre,mol.GetConformer(i)
2.8.0.pre,for i in range(self.num_conformers)
2.8.0.pre,if rmsd_list[i] < self.rmsd_cutoff
2.8.0.pre,]
2.8.0.pre,"# if conformer list is less than num_conformers, pad by repeating conformers"
2.8.0.pre,conf_idx = 0
2.8.0.pre,while len(conformers) < self.num_conformers:
2.8.0.pre,conformers.append(conformers[conf_idx])
2.8.0.pre,conf_idx += 1
2.8.0.pre,coordinates = [conf.GetPositions() for conf in conformers]
2.8.0.pre,add edges in both directions
2.8.0.pre,"Graph connectivity in COO format with shape [2, num_edges]"
2.8.0.pre,FIXME Add support for multiple conformers (wip)
2.8.0.pre,graph_list = []
2.8.0.pre,for i in range(self.num_conformers):
2.8.0.pre,graph_list.append(
2.8.0.pre,"GraphData(node_pos_features=np.array(coordinates[i]),"
2.8.0.pre,"node_features=np.array(atom_features_list),"
2.8.0.pre,"edge_features=np.array(edge_features_list),"
2.8.0.pre,edge_index=np.array(edges_list).T))
2.8.0.pre,return graph_list
2.8.0.pre,bond labels
2.8.0.pre,atom labels
2.8.0.pre,create bond encoders and decoders
2.8.0.pre,create atom encoders and decoders
2.8.0.pre,Special case handling of single molecule
2.8.0.pre,Convert iterables to list
2.8.0.pre,"sort first by frequency, then alphabetically"
2.8.0.pre,"sort first by frequency, then alphabetically"
2.8.0.pre,sorting the atoms neighbors
2.8.0.pre,concatenating the sorted neighbors
2.8.0.pre,"sort first by frequency, then alphabetically"
2.8.0.pre,"sort first by frequency, then alphabetically"
2.8.0.pre,flake8: noqa
2.8.0.pre,superclass accepts a DeepChem dataset while huggingface vocabulary builders
2.8.0.pre,reads data from file
2.8.0.pre,test with max size
2.8.0.pre,test build from csv
2.8.0.pre,test with max size
2.8.0.pre,load the vocabulary by passing filename
2.8.0.pre,test tokenization of a single point
2.8.0.pre,load the vocabulary by passing the filename
2.8.0.pre,test tokenization of a single point
2.8.0.pre,Build vocabulary by wrapping in huggingface vocabulary builder
2.8.0.pre,Load vocabulary and do a basic sanity check on the vocabulary
2.8.0.pre,Set up site environment matcher
2.8.0.pre,Graphical option
2.8.0.pre,tolerance for grouping nodes
2.8.0.pre,determine minimum distance between sitetypes.
2.8.0.pre,This is used to determine the existence of an edge
2.8.0.pre,Sort by bond
2.8.0.pre,You want to maximize this in order to make sure every node gets an edge
2.8.0.pre,construct graph
2.8.0.pre,matcher options
2.8.0.pre,construct graph
2.8.0.pre,Add nodes
2.8.0.pre,Add edge. distance is edge attribute
2.8.0.pre,construct graph
2.8.0.pre,Gets the isomorphic mapping. Also the most time consuming part of the code
2.8.0.pre,reconstruct graph after alinging point order
2.8.0.pre,RMSD
2.8.0.pre,Construct one hot encoding
2.8.0.pre,get mapping between all site index to active site index
2.8.0.pre,Get Neighbors
2.8.0.pre,Read Data
2.8.0.pre,get map between two environment
2.8.0.pre,align input to the primitive cell (reference)
2.8.0.pre,apply permutations
2.8.0.pre,remove spectators
2.8.0.pre,map it to active sites
2.8.0.pre,Extract the right number of sites by distance
2.8.0.pre,if PBC condition is fulfilled..
2.8.0.pre,Get full N x N SCM
2.8.0.pre,flake8: noqa
2.8.0.pre,load atom_init.json
2.8.0.pre,check whether the atom feature exists or not
2.8.0.pre,construct bi-directed graph
2.8.0.pre,Increase dimension of distance tensor and apply filter
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"rdks = [frag1[1], frag2[1]]"
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.8.0.pre,"rdks = [frag1[1], frag2[1]]"
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"rdks = [frag1[1], frag2[1]]"
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.8.0.pre,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.8.0.pre,"xyzs = [frag1_xyz, frag2_xyz]"
2.8.0.pre,"rdks = [frag1[1], frag2[1]]"
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"rdks = [frag1[1], frag2[1]]"
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0.pre,check if user tries to set removed arguments
2.8.0.pre,list of features that require sanitized molecules
2.8.0.pre,not implemented featurization types
2.8.0.pre,default values
2.8.0.pre,update with cutoffs specified by the user
2.8.0.pre,"each entry is a tuple (is_flat, feature_name)"
2.8.0.pre,list of features that cannot be calculated with specified parameters
2.8.0.pre,this list is used to define <flat/voxel/all>_combined subset
2.8.0.pre,parse provided feature types
2.8.0.pre,flake8: noqa
2.8.0.pre,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.8.0.pre,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.8.0.pre,nonzero contact.
2.8.0.pre,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,Get coordinates
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.8.0.pre,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.8.0.pre,axis.
2.8.0.pre,Type of data created by this featurizer
2.8.0.pre,TODO(rbharath): Should this return a list?
2.8.0.pre,Type of data created by this featurizer
2.8.0.pre,Currently handles loading failures by returning None
2.8.0.pre,TODO: Is there a better handling procedure?
2.8.0.pre,pad outputs
2.8.0.pre,Deprecation warnings for old atomic conv featurizer name #
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,Get coordinates
2.8.0.pre,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.8.0.pre,We compute pairwise contact fingerprints
2.8.0.pre,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.8.0.pre,decode the source in the mixed and separated cases
2.8.0.pre,number of indices where feature count is more than 1
2.8.0.pre,no normalized feature value should be greater than 1.0
2.8.0.pre,"151 = 133 + 18 (133 -> one hot encoding from _ATOM_FEATURES, 18 other features)"
2.8.0.pre,TODO test more formats for ligand
2.8.0.pre,TODO test more formats for ligand
2.8.0.pre,with one conformer
2.8.0.pre,with multiple conformers
2.8.0.pre,include explicit hydrogens
2.8.0.pre,with one conformer
2.8.0.pre,with multiple conformers
2.8.0.pre,include explicit hydrogens
2.8.0.pre,NOTE: The test depends on the the pretrained vocabulary
2.8.0.pre,(seyonec/PubChem10M_SMILES_BPE_60k). If the pretrained vocabulary is modified
2.8.0.pre,"(which can be since it is an external resource), the test might fail."
2.8.0.pre,construct edge (bond) index
2.8.0.pre,add edge list considering a directed graph
2.8.0.pre,test for 'MolGraphConvFeaturizer' class
2.8.0.pre,"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
2.8.0.pre,"Requirements - transformers, tokenizers"
2.8.0.pre,no normalized feature value should be greater than 1.0
2.8.0.pre,these are the properties used in grover
2.8.0.pre,"assert ""C1=CC=CN=C1"""
2.8.0.pre,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0.pre,"assert ""C1=CC=CN=C1"""
2.8.0.pre,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0.pre,"assert ""C1=CC=CN=C1"""
2.8.0.pre,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0.pre,"assert ""C1=CC=CN=C1"""
2.8.0.pre,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0.pre,Test featurizer with atom 3-D coordinates as kwargs
2.8.0.pre,load sample dataset
2.8.0.pre,"assert ""C1=CC=CN=C1"""
2.8.0.pre,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.8.0.pre,"number of indices, where feature count is more than 1, should be 0"
2.8.0.pre,number of indices where feature count is more than 1
2.8.0.pre,check for separate count and SMILES entries for each fragment
2.8.0.pre,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.8.0.pre,max num atoms instead of exact.
2.8.0.pre,Cutoff in angstroms
2.8.0.pre,"Coords are padded, neighbor list and Z are not"
2.8.0.pre,both reactant and product are null
2.8.0.pre,reactant is null
2.8.0.pre,product is null
2.8.0.pre,valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
2.8.0.pre,"# TODO: This is failing, something about the hydrogen bond counting?"
2.8.0.pre,def test_hydrogen_bond_counter():
2.8.0.pre,current_dir = os.path.dirname(os.path.realpath(__file__))
2.8.0.pre,"protein_file = os.path.join(current_dir, 'data',"
2.8.0.pre,'3ws9_protein_fixer_rdkit.pdb')
2.8.0.pre,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.8.0.pre,
2.8.0.pre,cutoff = 4.5
2.8.0.pre,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.8.0.pre,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.8.0.pre,# TODO: Add shape test
2.8.0.pre,
2.8.0.pre,
2.8.0.pre,"# TODO: This is failing, something about the hydrogen bond counting?"
2.8.0.pre,def test_hydrogen_bond_voxelizer():
2.8.0.pre,current_dir = os.path.dirname(os.path.realpath(__file__))
2.8.0.pre,"protein_file = os.path.join(current_dir, 'data',"
2.8.0.pre,'3ws9_protein_fixer_rdkit.pdb')
2.8.0.pre,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.8.0.pre,
2.8.0.pre,cutoff = 4.5
2.8.0.pre,box_width = 16
2.8.0.pre,voxel_width = 1.0
2.8.0.pre,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.8.0.pre,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.8.0.pre,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.8.0.pre,# TODO: Add shape test
2.8.0.pre,test if default parameters work
2.8.0.pre,check if use-case from examples works
2.8.0.pre,test if input is flattened when flat features are used
2.8.0.pre,test voxel features
2.8.0.pre,test flat features
2.8.0.pre,check if aromatic features are ignored if sanitize=False
2.8.0.pre,test flattened voxel features
2.8.0.pre,test voxel features
2.8.0.pre,test flat features
2.8.0.pre,test rotations
2.8.0.pre,not support array style inputs
2.8.0.pre,z is kwargs
2.8.0.pre,check convert function
2.8.0.pre,z is kwargs
2.8.0.pre,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.8.0.pre,of degree 1 (connected only to central nitrogen).
2.8.0.pre,5 atoms in compound
2.8.0.pre,Get the adjacency lists grouped by degree
2.8.0.pre,The 4 outer atoms connected to central nitrogen
2.8.0.pre,Central nitrogen connected to everything else.
2.8.0.pre,Only one carbon
2.8.0.pre,"No bonds, so degree adjacency lists are empty"
2.8.0.pre,3 carbonds in alkane
2.8.0.pre,Outer two carbonds are connected to central carbon
2.8.0.pre,Central carbon connected to outer two
2.8.0.pre,test featurization
2.8.0.pre,test defeaturization
2.8.0.pre,sanity check; see if something weird does not happen with rdkit
2.8.0.pre,check if original smiles match defeaturized smiles
2.8.0.pre,sanity check; see if something weird does not happen with rdkit
2.8.0.pre,test featurization
2.8.0.pre,test defeaturization
2.8.0.pre,check if original smiles match defeaturized smiles
2.8.0.pre,untransform
2.8.0.pre,untranform
2.8.0.pre,untranform
2.8.0.pre,untranform
2.8.0.pre,untranform
2.8.0.pre,Check the SDF file.
2.8.0.pre,Check the PDB file.
2.8.0.pre,Check the SMILES string.
2.8.0.pre,Set up tests.
2.8.0.pre,Set up testing parameters.
2.8.0.pre,the atom order for 'C' is same in case of canonical and original ordering
2.8.0.pre,Do a manual distance computation and make
2.8.0.pre,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.8.0.pre,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.8.0.pre,Do a manual distance computation and ensure that selected neighbor is
2.8.0.pre,closest since we set max_num_neighbors = 1
2.8.0.pre,Carbon
2.8.0.pre,Test distance 1
2.8.0.pre,Test distance 2
2.8.0.pre,Test alkane
2.8.0.pre,Test distance 1
2.8.0.pre,3 self connections and 2 bonds which are both counted twice because of
2.8.0.pre,symmetry for 7 total
2.8.0.pre,Test distance 2
2.8.0.pre,Everything is connected at this distance
2.8.0.pre,Test alkane
2.8.0.pre,Test distance infinity
2.8.0.pre,Everything is connected at this distance
2.8.0.pre,Test pentane
2.8.0.pre,Test distance infinity
2.8.0.pre,Everything is connected at this distance
2.8.0.pre,Only one carbon
2.8.0.pre,Test feature sizes
2.8.0.pre,"No bonds, so only 1 pair feature (for the self interaction)"
2.8.0.pre,Only 4 atoms
2.8.0.pre,Test feature sizes for chirality
2.8.0.pre,3 carbonds in alkane
2.8.0.pre,Test feature sizes
2.8.0.pre,Should be a 3x3 interaction grid
2.8.0.pre,mol_list = featurizer.featurize(mols)
2.8.0.pre,mol = mol_list[0]
2.8.0.pre,3 carbonds in alkane
2.8.0.pre,Test feature sizes
2.8.0.pre,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.8.0.pre,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.8.0.pre,symmetry)
2.8.0.pre,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.8.0.pre,of degree 1 (connected only to central nitrogen).
2.8.0.pre,import rdkit.Chem
2.8.0.pre,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.8.0.pre,5 atoms in compound
2.8.0.pre,Test feature sizes
2.8.0.pre,Should be a 3x3 interaction grid
2.8.0.pre,"bond and its reverse bond, therefore num_bonds * 2 edges"
2.8.0.pre,9 atom features
2.8.0.pre,"Graph connectivity in COO format with shape [2, num_edges]"
2.8.0.pre,3 bond features
2.8.0.pre,3 xyz coordinates for each atom in the conformer
2.8.0.pre,Artificial feature array.
2.8.0.pre,0 atoms of degree 0
2.8.0.pre,0 atoms of degree 1
2.8.0.pre,4 atoms of degree 2
2.8.0.pre,0 atoms of degree 3
2.8.0.pre,0 atoms of degree 4
2.8.0.pre,0 atoms of degree 5
2.8.0.pre,0 atoms of degree 6
2.8.0.pre,0 atoms of degree 7
2.8.0.pre,0 atoms of degree 8
2.8.0.pre,0 atoms of degree 9
2.8.0.pre,0 atoms of degree 10
2.8.0.pre,atom 4 has 0 neighbors
2.8.0.pre,atom 0 has 2 neighbors
2.8.0.pre,atom 1 has 2 neighbors
2.8.0.pre,atom 2 has 2 neighbors
2.8.0.pre,atom 3 has 3 neighbors.
2.8.0.pre,Verify that atom features have been sorted by atom degree.
2.8.0.pre,Sorting is done by atom degree as before. So the ordering goes
2.8.0.pre,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.8.0.pre,from new position to old position is
2.8.0.pre,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.8.0.pre,list respects this reordering and returns correct adjacency list.
2.8.0.pre,First example molecule
2.8.0.pre,Artificial feature array.
2.8.0.pre,Second example molecule
2.8.0.pre,Third example molecule
2.8.0.pre,Test agglomerate molecule method
2.8.0.pre,No atoms of degree 0
2.8.0.pre,3 atoms of degree 1
2.8.0.pre,8 atoms of degree 2
2.8.0.pre,1 atom of degree 3
2.8.0.pre,0 atoms of degree 4
2.8.0.pre,0 atoms of degree 5
2.8.0.pre,Check that atoms are only connected to themselves.
2.8.0.pre,Check that there's one atom of each degree.
2.8.0.pre,calculate coordinates
2.8.0.pre,not zero values
2.8.0.pre,Calculate frequency
2.8.0.pre,flake8: noqa
2.8.0.pre,assumes that every array is of the same dimension
2.8.0.pre,rem_dataset is remaining portion of dataset
2.8.0.pre,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.8.0.pre,to k-1.
2.8.0.pre,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.8.0.pre,validation
2.8.0.pre,skip list
2.8.0.pre,skip path string
2.8.0.pre,main logic
2.8.0.pre,for str
2.8.0.pre,for list
2.8.0.pre,dict is needed in case groups aren't strictly flattened or
2.8.0.pre,hashed by something non-integer like
2.8.0.pre,Figure out how many positive samples we want for each task in each dataset.
2.8.0.pre,Assign the positive samples to datasets.  Since a sample may be positive
2.8.0.pre,"on more than one task, we need to keep track of the effect of each added"
2.8.0.pre,"sample on each task.  To try to keep everything balanced, we cycle through"
2.8.0.pre,"tasks, assigning one positive sample for each one."
2.8.0.pre,We have a sample that hasn't been assigned yet.  Assign it to
2.8.0.pre,whichever set currently has the lowest fraction of its target for
2.8.0.pre,this task.
2.8.0.pre,The remaining samples are negative for all tasks.  Add them to fill out
2.8.0.pre,each set to the correct total number.
2.8.0.pre,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.8.0.pre,JSG Assert that split fractions can be written as proper fractions over 10.
2.8.0.pre,This can be generalized in the future with some common demoninator determination.
2.8.0.pre,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.8.0.pre,Append remaining examples to train
2.8.0.pre,################################################################
2.8.0.pre,Splitter for molecule datasets
2.8.0.pre,################################################################
2.8.0.pre,Sort by increasing MW
2.8.0.pre,calcaulate scaffold sets
2.8.0.pre,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.8.0.pre,Compute fingerprints for all molecules.
2.8.0.pre,Split into two groups: training set and everything else.
2.8.0.pre,Split the second group into validation and test sets.
2.8.0.pre,Begin by assigning the first molecule to the first group.
2.8.0.pre,Return identity if no tuple to split to
2.8.0.pre,Decide which group to assign a molecule to.
2.8.0.pre,Identify the unassigned molecule that is least similar to everything in
2.8.0.pre,the other group.
2.8.0.pre,Add it to the group.
2.8.0.pre,Update the data on unassigned molecules.
2.8.0.pre,Sort from largest to smallest scaffold sets
2.8.0.pre,################################################################
2.8.0.pre,Not well supported splitters
2.8.0.pre,################################################################
2.8.0.pre,All datasets share features and identifiers by assumption.
2.8.0.pre,flake8: noqa
2.8.0.pre,basic splitter
2.8.0.pre,molecule splitter
2.8.0.pre,other splitter
2.8.0.pre,################################################################
2.8.0.pre,Removed API
2.8.0.pre,################################################################
2.8.0.pre,Note that the extra task goes to test
2.8.0.pre,Number tasks per fold
2.8.0.pre,Find the tasks that correspond to this test fold
2.8.0.pre,Assert that all arrays look like they should
2.8.0.pre,"task_type = ""regression"""
2.8.0.pre,0 1 2 3 4 5 6 7 8 9
2.8.0.pre,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.8.0.pre,data. Make a test for properly splitting of sharded data. Perhaps using
2.8.0.pre,reshard() to handle this?
2.8.0.pre,Verify lengths is 10/k == 2
2.8.0.pre,Verify that compounds in this fold are subset of original compounds
2.8.0.pre,Verify that no two folds have overlapping compounds.
2.8.0.pre,Verify lengths is 10/k == 2
2.8.0.pre,Verify that compounds in this fold are subset of original compounds
2.8.0.pre,Verify that no two folds have overlapping compounds.
2.8.0.pre,Verify lengths is 10/k == 2
2.8.0.pre,Verify that compounds in this fold are subset of original compounds
2.8.0.pre,Verify that no two folds have overlapping compounds.
2.8.0.pre,Test singletask case.
2.8.0.pre,The split index should partition dataset in half.
2.8.0.pre,Test singletask case.
2.8.0.pre,Test case where some weights are zero (i.e. masked)
2.8.0.pre,Set half the positives to have zero weight
2.8.0.pre,There are 10 nonzero actives.
2.8.0.pre,"The split index should partition this into half, so expect 5"
2.8.0.pre,The split index should partition the positives for each task roughly in half.
2.8.0.pre,Mask half the examples
2.8.0.pre,The split index should partition dataset in half.
2.8.0.pre,Test singletask case.
2.8.0.pre,Should have split cleanly in half (picked random seed to ensure this)
2.8.0.pre,Check positives are correctly distributed
2.8.0.pre,Test singletask case.
2.8.0.pre,Should have made an 80/10/10 train/valid/test split of actives.
2.8.0.pre,Verify lengths is 100/k == 20
2.8.0.pre,Note: This wouldn't work for multitask str
2.8.0.pre,assert len(fold_dataset) == n_samples/K
2.8.0.pre,Verify that each fold has n_positives/K = 4 positive examples.
2.8.0.pre,Verify that compounds in this fold are subset of original compounds
2.8.0.pre,Verify that no two folds have overlapping compounds.
2.8.0.pre,The amount of datapoints has to be the same
2.8.0.pre,The number of scaffolds generated by the splitter
2.8.0.pre,has to be smaller or equal than number of total molecules
2.8.0.pre,Invalid because valence for atom 5 N is greater than permitted (4)
2.8.0.pre,edges logits used during training
2.8.0.pre,nodes logits used during training
2.8.0.pre,edges logits
2.8.0.pre,nodes logits
2.8.0.pre,training of the model
2.8.0.pre,generating compounds
2.8.0.pre,nodes logits used during compound generation
2.8.0.pre,Create the inputs.
2.8.0.pre,Create the generators.
2.8.0.pre,Create the discriminators.
2.8.0.pre,Compute the loss functions.
2.8.0.pre,Create learnable weights for the generators and discriminators.
2.8.0.pre,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.8.0.pre,Compute the weighted errors
2.8.0.pre,Add an entropy term to the loss.
2.8.0.pre,Create the Keras model.
2.8.0.pre,"Every call to fit_generator() will increment global_step, but we only"
2.8.0.pre,"want it to get incremented once for the entire batch, so record the"
2.8.0.pre,value and keep resetting it.
2.8.0.pre,Train the discriminator.
2.8.0.pre,Train the generator.
2.8.0.pre,Write checkpoints and report progress.
2.8.0.pre,Write out final results.
2.8.0.pre,Chain of flows is also a normalizing flow
2.8.0.pre,An instance of tfd.TransformedDistribution
2.8.0.pre,TODO: Incompability between TF and TFP means that TF doesn't track
2.8.0.pre,trainable variables in the flow; must override `_create_gradient_fn`
2.8.0.pre,self._variables = self.flow.trainable_variables
2.8.0.pre,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.8.0.pre,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.8.0.pre,This is for API consistency
2.8.0.pre,extended one of probabilites to binary distribution
2.8.0.pre,extended one of probabilites to binary distribution
2.8.0.pre,NOTE The below comment is from original source code
2.8.0.pre,dist_loss = av_dist_loss + bv_dist_loss + fg_dist_loss
2.8.0.pre,return av_loss + fg_loss + dist_coff * dist_loss
2.8.0.pre,"return overall_loss, av_loss, bv_loss, fg_loss, av_dist_loss, bv_dist_loss, fg_dist_loss"
2.8.0.pre,We just return overall_loss since TorchModel can handle only a single loss
2.8.0.pre,loss for nodes
2.8.0.pre,converting the binary classification to multiclass classification
2.8.0.pre,positive context prediction is the dot product of substructure representation and true context representation
2.8.0.pre,negative context prediction is the dot product of substructure representation and negative (random) context representation.
2.8.0.pre,positive substructure prediction is the dot product of expanded substructure representation and true overlapped node representation.
2.8.0.pre,shift indices of substructures to create negative examples
2.8.0.pre,negative substructure prediction is the dot product of shifted expanded substructure representation and true overlapped node representation.
2.8.0.pre,Compute the loss for positive and negative context representations
2.8.0.pre,The final loss is the sum of positive and negative context losses
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0.pre,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0.pre,"Shape (N_atoms, M_nbrs)"
2.8.0.pre,Generate the nb_affine weights and biases
2.8.0.pre,Extract atom_features
2.8.0.pre,Extract graph topology
2.8.0.pre,Sum all neighbors using adjacency matrix
2.8.0.pre,Get collection of modified atom features
2.8.0.pre,Obtain relevant atoms for this degree
2.8.0.pre,Get self atoms
2.8.0.pre,Apply hidden affine to relevant atoms and append
2.8.0.pre,Determine the min_deg=0 case
2.8.0.pre,Only use the self layer
2.8.0.pre,Combine all atoms back into the list
2.8.0.pre,Tensorflow correctly processes empty lists when using concat
2.8.0.pre,"Sum along neighbors as well as self, and store"
2.8.0.pre,Perform the mol gather
2.8.0.pre,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.8.0.pre,"self.max_degree, self.min_degree)"
2.8.0.pre,Tensorflow correctly processes empty lists when using concat
2.8.0.pre,Get self atoms
2.8.0.pre,"There are no neighbors of this degree, so just create an empty tensor directly."
2.8.0.pre,Expand dims
2.8.0.pre,always deg-1 for deg_adj_lists
2.8.0.pre,Extract graph topology
2.8.0.pre,means that this is second loop of convolution
2.8.0.pre,No other forget biases supported right now.
2.8.0.pre,Taken from Keras code [citation needed]
2.8.0.pre,"x is test set, xp is support set."
2.8.0.pre,Get initializations
2.8.0.pre,Process using attention
2.8.0.pre,"Eqn (4), appendix A.1 of Matching Networks paper"
2.8.0.pre,Generate new attention states
2.8.0.pre,Support set lstm
2.8.0.pre,Test lstm
2.8.0.pre,Get initializations
2.8.0.pre,Rename support
2.8.0.pre,Process support xp using attention
2.8.0.pre,Get linear combination of support set
2.8.0.pre,Process test x using attention
2.8.0.pre,Generate new support attention states
2.8.0.pre,Generate new test attention states
2.8.0.pre,Redefine
2.8.0.pre,Number of rotatable bonds
2.8.0.pre,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.8.0.pre,a difference.
2.8.0.pre,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.8.0.pre,neighbors lists an argument instead of a part of this layer.
2.8.0.pre,"Shape (N, M)"
2.8.0.pre,"Shape (N, M)"
2.8.0.pre,"Shape (N, M)"
2.8.0.pre,Number of grid cells
2.8.0.pre,TODO(rbharath): Support batching
2.8.0.pre,"Shape (n_cells, ndim)"
2.8.0.pre,"List of length N_atoms, each element of different length uniques_i"
2.8.0.pre,"List of length N_atoms, each element of different length uniques_i"
2.8.0.pre,"List of length N_atoms, each a tensor of shape"
2.8.0.pre,"(uniques_i, ndim)"
2.8.0.pre,Add phantom atoms that exist far outside the box
2.8.0.pre,"List of length N_atoms, each of shape (1, ndim)"
2.8.0.pre,TODO(rbharath): How does distance need to be modified here to
2.8.0.pre,account for periodic boundary conditions?
2.8.0.pre,List of length N_atoms each of shape (M_nbrs)
2.8.0.pre,"N_atoms elts of size (M_nbrs,) each"
2.8.0.pre,"Shape (N_atoms, 1)"
2.8.0.pre,Find M_nbrs atoms closest to each cell
2.8.0.pre,"Shape (n_cells, M_nbrs)"
2.8.0.pre,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0.pre,"conditions, so does wrapround. O(constant)"
2.8.0.pre,"Shape (n_cells, n_nbr_cells)"
2.8.0.pre,"Shape (N_atoms, n_nbr_cells)"
2.8.0.pre,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.8.0.pre,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.8.0.pre,"List of length N_atoms, each element length uniques_i"
2.8.0.pre,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.8.0.pre,element removed to remove self from list of neighbors. Need to verify
2.8.0.pre,this holds more broadly or come up with robust alternative.
2.8.0.pre,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0.pre,"Shape (N_atoms*n_cells, ndim) after tile"
2.8.0.pre,Shape (N_atoms*n_cells)
2.8.0.pre,"Shape (n_cells, N_atoms)"
2.8.0.pre,Find k atoms closest to this cell. Notice negative sign since
2.8.0.pre,tf.nn.top_k returns *largest* not smallest.
2.8.0.pre,"Tensor of shape (n_cells, M_nbrs)"
2.8.0.pre,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0.pre,"Shape (N_atoms*n_cells, 1) after tile"
2.8.0.pre,9 neighbors in 2-space
2.8.0.pre,TODO(rbharath): Shoddy handling of higher dimensions...
2.8.0.pre,Number of cells for cube in 3-space is
2.8.0.pre,TODO(rbharath): Do we need to handle periodic boundary conditions
2.8.0.pre,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.8.0.pre,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.8.0.pre,the cube.
2.8.0.pre,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.8.0.pre,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.8.0.pre,"Tile (a, a, a, b, b, b, etc.)"
2.8.0.pre,"Tile (a, b, c, a, b, c, ...)"
2.8.0.pre,N: Maximum number of atoms
2.8.0.pre,M: Maximum number of neighbors
2.8.0.pre,d: Number of coordinates/features/filters
2.8.0.pre,B: Batch Size
2.8.0.pre,Compute the distances and radial symmetry functions.
2.8.0.pre,check that there isnt just one or zero inputs
2.8.0.pre,create subspaces
2.8.0.pre,"concatenate subspaces, reshape to size of original input, then stack"
2.8.0.pre,"such that out_tensor has shape (2,?,original_cols)"
2.8.0.pre,creates subspaces the same way it was done in AlphaShare
2.8.0.pre,calculate squared Frobenius norm
2.8.0.pre,"(TODO YTZ:) faster, less memory intensive way"
2.8.0.pre,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.8.0.pre,"r = tf.expand_dims(r, -1)"
2.8.0.pre,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.8.0.pre,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.8.0.pre,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.8.0.pre,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.8.0.pre,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.8.0.pre,Calculate pairwise distance
2.8.0.pre,Cutoff with threshold Rc
2.8.0.pre,return d
2.8.0.pre,tf.stack issues again...
2.8.0.pre,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.8.0.pre,So the Tensor has known dimensions
2.8.0.pre,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.8.0.pre,normalization
2.8.0.pre,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.8.0.pre,and embeddings of atom j(both gone through a hidden layer)
2.8.0.pre,"for atom i, sum the influence from all other atom j in the molecule"
2.8.0.pre,number of inputs each step
2.8.0.pre,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.8.0.pre,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.8.0.pre,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.8.0.pre,`count`-th step
2.8.0.pre,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.8.0.pre,generating index for graph features used in the inputs
2.8.0.pre,"extracting graph features for parents of the target atoms, then flatten"
2.8.0.pre,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.8.0.pre,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.8.0.pre,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.8.0.pre,of shape: (batch_size*max_atoms) * n_graph_features
2.8.0.pre,representing the graph features of target atoms in each graph
2.8.0.pre,index for targe atoms
2.8.0.pre,Extract atom_features
2.8.0.pre,sum all graph outputs
2.8.0.pre,"Default message function: edge network, update function: GRU"
2.8.0.pre,more options to be implemented
2.8.0.pre,Add another value(~-Inf) to prevent error in softmax
2.8.0.pre,Model using this layer must set pad_batches=True
2.8.0.pre,Perform one step of LSTM
2.8.0.pre,task_metadata_rows = {task: [] for task in tasks}
2.8.0.pre,Extract those datapoints which are present for this task
2.8.0.pre,Loading is done on-the-fly
2.8.0.pre,Build the model.
2.8.0.pre,Final atom-layer convolution. Note this differs slightly from the paper
2.8.0.pre,since we use a tanh activation as default. This seems necessary for numerical
2.8.0.pre,stability.
2.8.0.pre,Now fully connected layers
2.8.0.pre,Should this allow for training?
2.8.0.pre,"pair_edges is of shape (2, N)"
2.8.0.pre,number of atoms in each molecule
2.8.0.pre,index of pair features
2.8.0.pre,Get starting pair atoms
2.8.0.pre,number of pairs for each atom
2.8.0.pre,atom features
2.8.0.pre,pair features
2.8.0.pre,Build the model.
2.8.0.pre,Build the model.
2.8.0.pre,calculation orders for a batch of molecules
2.8.0.pre,padding atom features vector of each molecule with 0
2.8.0.pre,Build the model.
2.8.0.pre,number of atoms in each molecule
2.8.0.pre,index of pair features
2.8.0.pre,number of pairs for each atom
2.8.0.pre,atom features
2.8.0.pre,pair features
2.8.0.pre,################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
2.8.0.pre,Add the input features.
2.8.0.pre,Add the shared dense layers
2.8.0.pre,Add task-specific bypass layers
2.8.0.pre,Add the input features.
2.8.0.pre,Add the shared dense layers
2.8.0.pre,Add task-specific bypass layers
2.8.0.pre,W&B flag support (DEPRECATED)
2.8.0.pre,"If `wandb=True` and no logger is provided, initialize default logger"
2.8.0.pre,Setup and initialize W&B logging
2.8.0.pre,Update config with KerasModel params
2.8.0.pre,Backwards compatibility
2.8.0.pre,The optimizer creates internal variables the first time apply_gradients()
2.8.0.pre,is called for a new set of variables.  If that happens inside a function
2.8.0.pre,"annotated with tf.function it throws an exception, so call it once here."
2.8.0.pre,Main training loop.
2.8.0.pre,"Execute the loss function, accumulating the gradients."
2.8.0.pre,Report progress and write checkpoints.
2.8.0.pre,Capture the last avg_loss in case of return since we're resetting to
2.8.0.pre,0 now
2.8.0.pre,Report final results.
2.8.0.pre,Invoke the model.
2.8.0.pre,Apply tranformers and record results.
2.8.0.pre,Concatenate arrays to create the final results.
2.8.0.pre,Use a GradientTape to compute gradients.
2.8.0.pre,Ensure weights for both models are built.
2.8.0.pre,Define the PyTorch Module that implements the model.
2.8.0.pre,Define the PyTorch Module that implements the model.
2.8.0.pre,Run fit transformers on dummy dataset to determine n_features after transformation
2.8.0.pre,set wandb init arguments
2.8.0.pre,Dataset ids are used to differentiate datasets seen by the logger
2.8.0.pre,log data
2.8.0.pre,Similarity values
2.8.0.pre,Labels for all top K similar samples
2.8.0.pre,Discard any padded predictions
2.8.0.pre,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.8.0.pre,Build the model.
2.8.0.pre,Character embedding
2.8.0.pre,Multiple convolutional layers with different filter widths
2.8.0.pre,Max-over-time pooling
2.8.0.pre,Concat features from all filters(one feature per filter)
2.8.0.pre,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.8.0.pre,SMILES strings
2.8.0.pre,Maximum length is expanded to allow length variation during train and inference
2.8.0.pre,'_' served as delimiter and padding
2.8.0.pre,Initialize common characters as keys
2.8.0.pre,Include space to avoid extra keys
2.8.0.pre,"For 'Cl', 'Br', etc."
2.8.0.pre,"Character not recognized, add to extra_keys"
2.8.0.pre,Add all extra_keys to char_dict
2.8.0.pre,Transform SMILES sequence to integers
2.8.0.pre,Skip all spaces
2.8.0.pre,"For 'Cl', 'Br', etc."
2.8.0.pre,Padding with '_'
2.8.0.pre,################### Deprecation warnings for renamed TensorGraph models ####################  # noqa: E266
2.8.0.pre,"layer_sizes=[32, 32, 16],"
2.8.0.pre,Add the dense layers
2.8.0.pre,Do a simple greedy search.
2.8.0.pre,Do a beam search with length normalization.
2.8.0.pre,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.8.0.pre,This candidate sequence has already been terminated
2.8.0.pre,Consider all possible tokens we could add to this candidate sequence.
2.8.0.pre,Add the input features.
2.8.0.pre,Handle output layer
2.8.0.pre,Iterate over all previous tasks.
2.8.0.pre,prev_layers is a list with elements of size
2.8.0.pre,"(batch_size, layer_sizes[i-1])"
2.8.0.pre,Log data to Wandb
2.8.0.pre,flake8: noqa
2.8.0.pre,Tensorflow Dependency Models
2.8.0.pre,scikit-learn model
2.8.0.pre,PyTorch models
2.8.0.pre,Pytorch models with torch-geometric dependency
2.8.0.pre,TODO We should clean up DMPNN and remove torch_geometric dependency during import
2.8.0.pre,Pytorch-lightning modules import
2.8.0.pre,Jax models
2.8.0.pre,####################################################################################
2.8.0.pre,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.8.0.pre,####################################################################################
2.8.0.pre,#######################################################################################
2.8.0.pre,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.8.0.pre,#######################################################################################
2.8.0.pre,Last layer sequences not returned.
2.8.0.pre,This is needed because ImageDataGenerator does infinite looping
2.8.0.pre,"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
2.8.0.pre,but turns out to be slightly faster
2.8.0.pre,JAX depend
2.8.0.pre,Main training loop
2.8.0.pre,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0.pre,Report final results.
2.8.0.pre,Apply tranformers and record results.
2.8.0.pre,Concatenate arrays to create the final results.
2.8.0.pre,"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
2.8.0.pre,") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
2.8.0.pre,""""""""
2.8.0.pre,"Predict the model's outputs, along with the uncertainty in each one."
2.8.0.pre,The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
2.8.0.pre,It involves repeating the prediction many times with different dropout masks.
2.8.0.pre,The prediction is computed as the average over all the predictions.  The
2.8.0.pre,uncertainty includes both the variation among the predicted values (epistemic
2.8.0.pre,uncertainty) and the model's own estimates for how well it fits the data
2.8.0.pre,(aleatoric uncertainty).  Not all models support uncertainty prediction.
2.8.0.pre,Parameters
2.8.0.pre,----------
2.8.0.pre,dataset: dc.data.Dataset
2.8.0.pre,Dataset to make prediction on
2.8.0.pre,masks: int
2.8.0.pre,the number of dropout masks to average over
2.8.0.pre,Returns
2.8.0.pre,-------
2.8.0.pre,"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
2.8.0.pre,"value of the output, and each element of y_std estimates the standard"
2.8.0.pre,deviation of the corresponding element of y_pred
2.8.0.pre,""""""""
2.8.0.pre,sum_pred: List[np.ndarray] = []
2.8.0.pre,sum_sq_pred: List[np.ndarray] = []
2.8.0.pre,sum_var: List[np.ndarray] = []
2.8.0.pre,for i in range(masks):
2.8.0.pre,generator = self.default_generator(
2.8.0.pre,"dataset, mode='uncertainty', pad_batches=False)"
2.8.0.pre,"results = self._predict(generator, [], True, None)"
2.8.0.pre,if len(sum_pred) == 0:
2.8.0.pre,"for p, v in results:"
2.8.0.pre,sum_pred.append(p)
2.8.0.pre,sum_sq_pred.append(p * p)
2.8.0.pre,sum_var.append(v)
2.8.0.pre,else:
2.8.0.pre,"for j, (p, v) in enumerate(results):"
2.8.0.pre,sum_pred[j] += p
2.8.0.pre,sum_sq_pred[j] += p * p
2.8.0.pre,sum_var[j] += v
2.8.0.pre,output = []
2.8.0.pre,std = []
2.8.0.pre,for i in range(len(sum_pred)):
2.8.0.pre,p = sum_pred[i] / masks
2.8.0.pre,output.append(p)
2.8.0.pre,std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
2.8.0.pre,if len(output) == 1:
2.8.0.pre,"return (output[0], std[0])"
2.8.0.pre,else:
2.8.0.pre,"return list(zip(output, std))"
2.8.0.pre,JAX dependencies
2.8.0.pre,Main training loop
2.8.0.pre,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0.pre,Report final results.
2.8.0.pre,Apply tranformers and record results.
2.8.0.pre,Concatenate arrays to create the final results.
2.8.0.pre,flake8:noqa
2.8.0.pre,The PINNModel requires you to create two functions
2.8.0.pre,`create_eval`_fn for letting the model know how to compute the model in inference and
2.8.0.pre,`gradient_fn` for letting model know how to compute the gradient and different regulariser
2.8.0.pre,equation loss depending on the differential equation
2.8.0.pre,defining the Haiku model
2.8.0.pre,"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
2.8.0.pre,"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
2.8.0.pre,which will be used as the differential loss(regulariser)
2.8.0.pre,The expected solution must be as close to cos(x)
2.8.0.pre,Initialize the weights with random values
2.8.0.pre,Forward function which takes the params
2.8.0.pre,Loss Function
2.8.0.pre,JaxModel Working
2.8.0.pre,sample network
2.8.0.pre,Model Initialization
2.8.0.pre,Loss Function
2.8.0.pre,JaxModel Working
2.8.0.pre,sample network
2.8.0.pre,Model Initilisation
2.8.0.pre,Loss Function
2.8.0.pre,JaxModel Working
2.8.0.pre,Model Initilisation
2.8.0.pre,Loss Function
2.8.0.pre,JaxModel Working
2.8.0.pre,Model Initilisation
2.8.0.pre,Loss Function
2.8.0.pre,JaxModel Working
2.8.0.pre,Model Initilisation
2.8.0.pre,Loss Function
2.8.0.pre,JaxModel Working
2.8.0.pre,Each epoch is a single step for this model
2.8.0.pre,@pytest.mark.jax
2.8.0.pre,@pytest.mark.slow
2.8.0.pre,def test_uncertainty():
2.8.0.pre,"""""""Test estimating uncertainty a TorchModel."""""""
2.8.0.pre,n_samples = 30
2.8.0.pre,n_features = 1
2.8.0.pre,noise = 0.1
2.8.0.pre,"X = np.random.rand(n_samples, n_features)"
2.8.0.pre,"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
2.8.0.pre,"dataset = dc.data.NumpyDataset(X, y)"
2.8.0.pre,class Net(hk.Module):
2.8.0.pre,"def __init__(self, output_size: int = 1):"
2.8.0.pre,super().__init__()
2.8.0.pre,"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.8.0.pre,"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.8.0.pre,self.output = hk.Linear(output_size)
2.8.0.pre,self.log_var = hk.Linear(output_size)
2.8.0.pre,"def __call__(self, x):"
2.8.0.pre,x = self._network1(x)
2.8.0.pre,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.8.0.pre,x = self._network2(x)
2.8.0.pre,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.8.0.pre,output = self.output(x)
2.8.0.pre,log_var = self.log_var(x)
2.8.0.pre,var = jnp.exp(log_var)
2.8.0.pre,"return output, var, output, log_var"
2.8.0.pre,def f(x):
2.8.0.pre,net = Net(1)
2.8.0.pre,return net(x)
2.8.0.pre,"def loss(outputs, labels, weights):"
2.8.0.pre,diff = labels[0] - outputs[0]
2.8.0.pre,log_var = outputs[1]
2.8.0.pre,var = jnp.exp(log_var)
2.8.0.pre,return jnp.mean(diff * diff / var + log_var)
2.8.0.pre,class UncertaintyModel(JaxModel):
2.8.0.pre,"def default_generator(self,"
2.8.0.pre,"dataset,"
2.8.0.pre,"epochs=1,"
2.8.0.pre,"mode='fit',"
2.8.0.pre,"deterministic=True,"
2.8.0.pre,pad_batches=True):
2.8.0.pre,for epoch in range(epochs):
2.8.0.pre,"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
2.8.0.pre,"batch_size=self.batch_size,"
2.8.0.pre,"deterministic=deterministic,"
2.8.0.pre,pad_batches=pad_batches):
2.8.0.pre,"yield ([X_b], [y_b], [w_b])"
2.8.0.pre,jm_model = hk.transform(f)
2.8.0.pre,rng = jax.random.PRNGKey(500)
2.8.0.pre,"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
2.8.0.pre,modified_inputs = jnp.array(
2.8.0.pre,[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
2.8.0.pre,"params = jm_model.init(rng, modified_inputs)"
2.8.0.pre,model = UncertaintyModel(
2.8.0.pre,"jm_model.apply,"
2.8.0.pre,"params,"
2.8.0.pre,"loss,"
2.8.0.pre,"output_types=['prediction', 'variance', 'loss', 'loss'],"
2.8.0.pre,learning_rate=0.003)
2.8.0.pre,"model.fit(dataset, nb_epochs=2500)"
2.8.0.pre,"pred, std = model.predict_uncertainty(dataset)"
2.8.0.pre,assert np.mean(np.abs(y - pred)) < 2.0
2.8.0.pre,assert noise < np.mean(std) < 1.0
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,"Create an identical model, do a single step of fitting with restore=True and make sure it got restored correctly."
2.8.0.pre,check that the first layer is still the same between the two models
2.8.0.pre,check that the predictions are different because of the fine tuning
2.8.0.pre,check that the first layer is different between the two models
2.8.0.pre,Conv2d and Linear layers test(CNN classification)
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,test if adjacency matrix input is correctly set
2.8.0.pre,test if nodes features matrix input is correctly set
2.8.0.pre,check discriminator shape
2.8.0.pre,check training edges logits shape
2.8.0.pre,check training nodes logits shapes
2.8.0.pre,True will be assigned up successful training attempt
2.8.0.pre,force clear tensor flow backend
2.8.0.pre,create new model
2.8.0.pre,to avoid flake8 E125/yapf incompatibility
2.8.0.pre,generate input
2.8.0.pre,train model
2.8.0.pre,generate sample
2.8.0.pre,check how many valid molecules were created and add to list
2.8.0.pre,finally test if there was at least one valid training session
2.8.0.pre,as the model structure improves this should become more and more strict
2.8.0.pre,Predict the output and uncertainty.
2.8.0.pre,predict datset with no y (ensured by tasks = [])
2.8.0.pre,Predict the output and uncertainty.
2.8.0.pre,The DAG models have high error with dropout
2.8.0.pre,"Despite a lot of effort tweaking it , there appears to be"
2.8.0.pre,a limit to how low the error can go with dropout.
2.8.0.pre,assert mean_error < 0.5 * mean_value
2.8.0.pre,Predict the output and uncertainty.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,testing batch size > 1
2.8.0.pre,testing true values
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,load datasets
2.8.0.pre,disable transformer
2.8.0.pre,check train
2.8.0.pre,check predict shape
2.8.0.pre,check overfit
2.8.0.pre,load datasets
2.8.0.pre,disable transformer
2.8.0.pre,check train
2.8.0.pre,check predict shape
2.8.0.pre,check overfit
2.8.0.pre,load datasets
2.8.0.pre,disable transformer
2.8.0.pre,check train
2.8.0.pre,check predict shape
2.8.0.pre,check overfit
2.8.0.pre,reload
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Check same predictions are made.
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Load trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Reload trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reloaded Trained Model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,3D Multivariate Gaussian base distribution
2.8.0.pre,Check that reloaded model can sample from the distribution
2.8.0.pre,Check that density estimation is same for reloaded model
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload Trained Model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload Trained Model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Reload trained Model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Eval model on train
2.8.0.pre,Reload Trained Model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.8.0.pre,Reload Trained Model
2.8.0.pre,Check predictions match on original dataset
2.8.0.pre,TODO: We need a cleaner usage example for this
2.8.0.pre,Fit trained model
2.8.0.pre,Check predictions match on random sample
2.8.0.pre,Train the model on random sequences.  We aren't training long enough to
2.8.0.pre,"really make it reliable, but I want to keep this test fast, and it should"
2.8.0.pre,still be able to reproduce a reasonable fraction of input sequences.
2.8.0.pre,Test it out.
2.8.0.pre,https://github.com/diffqc/dqc/blob/742eb2576418464609f942def4fb7c3bbdc0cd82/dqc/test/test_xc.py#L15
2.8.0.pre,check predict shape
2.8.0.pre,check overfit
2.8.0.pre,needs change
2.8.0.pre,check predict shape
2.8.0.pre,check overfit
2.8.0.pre,reload
2.8.0.pre,.8 to save resources for a difficult task
2.8.0.pre,.2 to save resources for a difficult task
2.8.0.pre,first iteration loss is around 50
2.8.0.pre,The first pass of the transformation should be 0
2.8.0.pre,Test sampling method
2.8.0.pre,Test log_prob method (this method is used when inverse pass)
2.8.0.pre,Output must be a Nth zero array since nothing is being learned yet
2.8.0.pre,Featurize to assert for tests
2.8.0.pre,Assert errors for sample method
2.8.0.pre,Assert errors for log_prob method
2.8.0.pre,atom features
2.8.0.pre,Try without compression
2.8.0.pre,"Outputs should be [mol1_vec, mol2_vec]"
2.8.0.pre,atom features
2.8.0.pre,Try with compression
2.8.0.pre,"Outputs should be [mol1_vec, mol2_vec]"
2.8.0.pre,get data
2.8.0.pre,prepare batch (size 1)
2.8.0.pre,initialize the model
2.8.0.pre,get output
2.8.0.pre,get data
2.8.0.pre,prepare batch (size 1)
2.8.0.pre,initialize the model
2.8.0.pre,get output
2.8.0.pre,get data
2.8.0.pre,prepare batch (size 1)
2.8.0.pre,initialize the model
2.8.0.pre,get output
2.8.0.pre,load sample dataset
2.8.0.pre,initialize the model
2.8.0.pre,overfit test
2.8.0.pre,load sample dataset
2.8.0.pre,initialize the model
2.8.0.pre,overfit test
2.8.0.pre,load sample dataset
2.8.0.pre,initialize the model
2.8.0.pre,fit the model
2.8.0.pre,reload the model
2.8.0.pre,index of pair features
2.8.0.pre,number of pairs for each atom
2.8.0.pre,atom features
2.8.0.pre,pair features
2.8.0.pre,Assigning tensorflow equivalent weights to torch layer
2.8.0.pre,"Outputs should be [A, P]"
2.8.0.pre,There are 4 atoms each of which have 75 atom features
2.8.0.pre,There are 10 pairs with infinity distance and 14 pair features
2.8.0.pre,4 atoms in total
2.8.0.pre,10 pairs in total
2.8.0.pre,10 pairs in total each with start/finish
2.8.0.pre,There are 4 atoms each of which have 75 atom features
2.8.0.pre,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.8.0.pre,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.8.0.pre,connections and accounting for symmetry.)
2.8.0.pre,4 atoms in total
2.8.0.pre,10 pairs in total
2.8.0.pre,The center atom is self connected and to both neighbors so it appears
2.8.0.pre,thrice. The canonical ranking used in MolecularFeaturizer means this
2.8.0.pre,central atom is ranked last in ordering.
2.8.0.pre,10 pairs in total each with start/finish
2.8.0.pre,def test_weave_fit_simple_infinity_distance():
2.8.0.pre,featurizer = WeaveFeaturizer(max_pair_distance=None)
2.8.0.pre,"X = featurizer([""C"", ""CCC""])"
2.8.0.pre,"y = np.array([0, 1.])"
2.8.0.pre,"dataset = NumpyDataset(X, y)"
2.8.0.pre,batch_size = 20
2.8.0.pre,model = WeaveModel(
2.8.0.pre,"1,"
2.8.0.pre,"batch_size=batch_size,"
2.8.0.pre,"mode='classification',"
2.8.0.pre,"fully_connected_layer_sizes=[2000, 1000],"
2.8.0.pre,"batch_normalize=True,"
2.8.0.pre,batch_normalize_kwargs={
2.8.0.pre,"""fused"": False,"
2.8.0.pre,"""trainable"": True,"
2.8.0.pre,"""renorm"": True"
2.8.0.pre,"},"
2.8.0.pre,learning_rate=0.0005)
2.8.0.pre,"model.fit(dataset, nb_epoch=200)"
2.8.0.pre,transformers = []
2.8.0.pre,metric = Metric(
2.8.0.pre,"roc_auc_score, np.mean, mode=""classification"")"
2.8.0.pre,"scores = model.evaluate(dataset, [metric], transformers)"
2.8.0.pre,assert scores['mean-roc_auc_score'] >= 0.9
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,load datasets
2.8.0.pre,initialize model
2.8.0.pre,overfit test
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Fit trained model
2.8.0.pre,Predict the output and uncertainty.
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,xgboost test
2.8.0.pre,fit trained model
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,lightgbm test
2.8.0.pre,fit trained model
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,xgboost test
2.8.0.pre,fit trained model
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,lightgbm test
2.8.0.pre,fit trained model
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,xgboost test
2.8.0.pre,fit trained model
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,lightgbm test
2.8.0.pre,fit trained model
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,xgboost test
2.8.0.pre,fit trained model
2.8.0.pre,reload
2.8.0.pre,check predictions match on test dataset
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,global setting
2.8.0.pre,lightgbm test
2.8.0.pre,fit trained model
2.8.0.pre,reload
2.8.0.pre,check predictions match on test dataset
2.8.0.pre,eval model on test
2.8.0.pre,prepare dataset
2.8.0.pre,xgboost test
2.8.0.pre,fit trained model
2.8.0.pre,"If ES rounds are more than total epochs, it will never trigger"
2.8.0.pre,Find the number of boosting rounds in the model
2.8.0.pre,"If rounds boosted are less than total estimators, it means ES was triggered"
2.8.0.pre,prepare dataset
2.8.0.pre,lightgbm test
2.8.0.pre,fit trained model
2.8.0.pre,"If ES rounds are more than total epochs, it will never trigger"
2.8.0.pre,Find the number of boosting rounds in the model
2.8.0.pre,"If rounds ran are less than estimators, it means ES was triggered"
2.8.0.pre,"For simplicity, let's assume both molecules have same number of"
2.8.0.pre,atoms.
2.8.0.pre,Creates a set of dummy features that contain the coordinate and
2.8.0.pre,neighbor-list features required by the AtomicConvModel.
2.8.0.pre,Creates a set of dummy features that contain the coordinate and
2.8.0.pre,neighbor-list features required by the AtomicConvModel.
2.8.0.pre,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.8.0.pre,max num atoms instead of exact.
2.8.0.pre,Cutoff in angstroms
2.8.0.pre,arbitrary label
2.8.0.pre,Run a fitting operation
2.8.0.pre,Testing graphnet for a single graph
2.8.0.pre,Testing for consistency
2.8.0.pre,Testing with a batch of Graphs
2.8.0.pre,"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
2.8.0.pre,the above import raises a ModuleNotFoundError. It is safe to ignore it
2.8.0.pre,since the below tests only run in an environment with pytorch installed.
2.8.0.pre,TODO The test is skipped as FakeGraphGenerator has to be updated
2.8.0.pre,to generate regression labels
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train/test
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train/test
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train/test
2.8.0.pre,"Linear layers for making query, key, value"
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,No training has been done after reload
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,We have to set the gradient penalty very small because the generator's
2.8.0.pre,"output is only a single number, so the default penalty would constrain"
2.8.0.pre,it far too much.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,We have to set the gradient penalty very small because the generator's
2.8.0.pre,"output is only a single number, so the default penalty would constrain"
2.8.0.pre,it far too much.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,n_samples = 100
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Most weights should be close to zero.
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Most weights should be close to zero.
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Fit trained model
2.8.0.pre,Predict the output and uncertainty.
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Check that predicting internal layers works.
2.8.0.pre,Each epoch is a single step for this model
2.8.0.pre,Create two models using the same model directory.
2.8.0.pre,Check that they produce different results.
2.8.0.pre,"Save a checkpoint from the first model and load it into the second one,"
2.8.0.pre,and make sure they now match.
2.8.0.pre,Train a model to overfit the dataset.
2.8.0.pre,"Create an identical model, do a single step of fitting with restore=True,"
2.8.0.pre,and make sure it got restored correctly.
2.8.0.pre,Build a model that predicts uncertainty.
2.8.0.pre,Fit the model and see if its predictions are correct.
2.8.0.pre,Take a tiny step in the direction of s and see if the output changes by
2.8.0.pre,the expected amount.
2.8.0.pre,Load dataset and Models
2.8.0.pre,call model.fit again to test multiple fit() calls
2.8.0.pre,Set up tests.
2.8.0.pre,def test_singletask_to_multitask_classification(self):
2.8.0.pre,n_features = 10
2.8.0.pre,n_tasks = 17
2.8.0.pre,tasks = range(n_tasks)
2.8.0.pre,# Define train dataset
2.8.0.pre,n_train = 100
2.8.0.pre,"X_train = np.random.rand(n_train, n_features)"
2.8.0.pre,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.8.0.pre,w_train = np.ones_like(y_train)
2.8.0.pre,"ids_train = [""C""] * n_train"
2.8.0.pre,train_dataset = dc.data.DiskDataset.from_numpy(
2.8.0.pre,"X_train, y_train, w_train, ids_train)"
2.8.0.pre,# Define test dataset
2.8.0.pre,n_test = 10
2.8.0.pre,"X_test = np.random.rand(n_test, n_features)"
2.8.0.pre,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.8.0.pre,w_test = np.ones_like(y_test)
2.8.0.pre,"ids_test = [""C""] * n_test"
2.8.0.pre,test_dataset = dc.data.DiskDataset.from_numpy(
2.8.0.pre,"X_test, y_test, w_test, ids_test)"
2.8.0.pre,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.8.0.pre,def model_builder(model_dir):
2.8.0.pre,sklearn_model = LogisticRegression()
2.8.0.pre,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.8.0.pre,multitask_model = dc.models.SingletaskToMultitask(
2.8.0.pre,"tasks, model_builder)"
2.8.0.pre,# Fit trained model
2.8.0.pre,multitask_model.fit(train_dataset)
2.8.0.pre,multitask_model.save()
2.8.0.pre,# Eval multitask_model on train/test
2.8.0.pre,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.8.0.pre,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.8.0.pre,Generate data
2.8.0.pre,Cleanup
2.8.0.pre,Finetune model weights should not match before loading from pretrained model
2.8.0.pre,Finetune model weights should match after loading from pretrained model
2.8.0.pre,"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
2.8.0.pre,the above import raises a ModuleNotFoundError. It is safe to ignore it
2.8.0.pre,since the below tests only run in an environment with pytorch installed.
2.8.0.pre,Test for the init function of FerminetModel class
2.8.0.pre,Testing ionic initialization
2.8.0.pre,Testing whether error throws up when spin is wrong
2.8.0.pre,Testing the spin values
2.8.0.pre,Testing ionic initialization
2.8.0.pre,Test for the evaluate_hf_solution function of FerminetModel class
2.8.0.pre,"The solution should be of the shape (number of electrons, number of electrons)"
2.8.0.pre,Test for the init function of FerminetModel class
2.8.0.pre,Testing ionic initialization
2.8.0.pre,Test for the init function of FerminetModel class
2.8.0.pre,Testing ionic initialization
2.8.0.pre,Test for the init function of FerminetModel class
2.8.0.pre,Testing ionic initialization
2.8.0.pre,Test for the init function of FerminetModel class
2.8.0.pre,Testing ionic initialization
2.8.0.pre,Train the model while logging the validation ROC AUC.
2.8.0.pre,Parse the log to pull out the AUC scores.
2.8.0.pre,The last reported score should match the current performance of the model.
2.8.0.pre,The highest recorded score should match get_best_score().
2.8.0.pre,Reload the save model and confirm that it matches the best logged score.
2.8.0.pre,Make sure get_best_score() still works when save_dir is not specified
2.8.0.pre,3D Multivariate Gaussian base distribution
2.8.0.pre,Must be float32 for RealNVP
2.8.0.pre,Tests a simple flow of one RealNVP layer.
2.8.0.pre,log likelihoods should be negative
2.8.0.pre,# Fit model
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,x and y are the same tensor (equivalent at every element)
2.8.0.pre,the pairwise inner product of the rows in x and y will always be 1
2.8.0.pre,"the output tensor will be of shape (5,5)"
2.8.0.pre,each row in x1 is orthogonal to each row in x2
2.8.0.pre,the pairwise inner product of the rows in x and y will always be 0
2.8.0.pre,"the output tensor will be of shape (256,256)"
2.8.0.pre,"Creating a second layer should produce different results, since it has"
2.8.0.pre,different random weights.
2.8.0.pre,But evaluating the first layer again should produce the same result as before.
2.8.0.pre,index of pair features
2.8.0.pre,number of pairs for each atom
2.8.0.pre,atom features
2.8.0.pre,pair features
2.8.0.pre,"Outputs should be [A, P]"
2.8.0.pre,atom features
2.8.0.pre,Try without compression
2.8.0.pre,"Outputs should be [mol1_vec, mol2_vec)"
2.8.0.pre,Try with compression
2.8.0.pre,"Outputs should be [mol1_vec, mol2_vec)"
2.8.0.pre,atom features
2.8.0.pre,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.8.0.pre,Gaussian histograms expands into 11 Gaussian buckets.
2.8.0.pre,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.8.0.pre,TODO What should shape[1] be?  It's not documented.
2.8.0.pre,"n_atoms = 4  # In CCC and C, there are 4 atoms"
2.8.0.pre,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.8.0.pre,"Creating a second layer should produce different results, since it has"
2.8.0.pre,different random weights.
2.8.0.pre,But evaluating the first layer again should produce the same result as before.
2.8.0.pre,"Creating a second layer should produce different results, since it has"
2.8.0.pre,different random weights.
2.8.0.pre,But evaluating the first layer again should produce the same result as before.
2.8.0.pre,"Creating a second layer should produce different results, since it has"
2.8.0.pre,different random weights.
2.8.0.pre,But evaluating the first layer again should produce the same result as before.
2.8.0.pre,"TODO What should the output shape be?  It's not documented, and there"
2.8.0.pre,are no other test cases for it.
2.8.0.pre,"Creating a second layer should produce different results, since it has"
2.8.0.pre,different random weights.
2.8.0.pre,But evaluating the first layer again should produce the same result as before.
2.8.0.pre,"Creating a second layer should produce different results, since it has"
2.8.0.pre,different random weights.
2.8.0.pre,But evaluating the first layer again should produce the same result as before.
2.8.0.pre,"Recall that the DAG layer expects a MultiConvMol as input,"
2.8.0.pre,"so the ""batch"" is a pooled set of atoms from all the"
2.8.0.pre,"molecules in the batch, just as it is for the graph conv."
2.8.0.pre,This means that n_atoms is the batch-size
2.8.0.pre,dropout_switch = False
2.8.0.pre,dropout_switch
2.8.0.pre,TODO(rbharath): What is the shape of outputs supposed to be?
2.8.0.pre,"I'm getting (7, 30) here. Where does 7 come from??"
2.8.0.pre,TODO(rbharath): We need more documentation about why
2.8.0.pre,these numbers work.
2.8.0.pre,"By setting the `box_size` to effectively zero, the result should only contain `nan`."
2.8.0.pre,Check that layer has three trainable parameters.
2.8.0.pre,Check when `box_size` is of wrong dimensionality.
2.8.0.pre,Check when `inputs` is of wrong length.
2.8.0.pre,Create random local node representations and global graph representations
2.8.0.pre,Compute similarity scores using the discriminator
2.8.0.pre,Check if the output has the correct shape and dtype
2.8.0.pre,total_n_atoms = 4
2.8.0.pre,n_atom_feat = 4
2.8.0.pre,"atom_feat = np.random.rand(total_n_atoms, n_atom_feat)"
2.8.0.pre,Embeddings and results from Tensorflow implementation
2.8.0.pre,Weights and Embeddings from Tensorflow implementation
2.8.0.pre,init parameters
2.8.0.pre,generate features for testing
2.8.0.pre,index of pair features
2.8.0.pre,atom features
2.8.0.pre,pair features
2.8.0.pre,tensors for torch layer
2.8.0.pre,assigning tensorflow layer weights to torch layer
2.8.0.pre,Create a dataset and an input function for processing it.
2.8.0.pre,Create a dataset and an input function for processing it.
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Dataset of SMILES strings for testing SeqToSeq models.
2.8.0.pre,Train the model on random sequences. We aren't training long enough to
2.8.0.pre,"really make it reliable, but I want to keep this test fast, and it should"
2.8.0.pre,still be able to reproduce a reasonable fraction of input sequences.
2.8.0.pre,Test it out.
2.8.0.pre,Check that it got at least a quarter of them correct.
2.8.0.pre,Initialize the model
2.8.0.pre,Fit the Model
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on test
2.8.0.pre,Eval model on train
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on test
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on test
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on test
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on test
2.8.0.pre,Each epoch is a single step for this model
2.8.0.pre,Create two models using the same model directory.
2.8.0.pre,Check that they produce different results.
2.8.0.pre,"Save a checkpoint from the first model and load it into the second one,"
2.8.0.pre,and make sure they now match.
2.8.0.pre,Train a model to overfit the dataset.
2.8.0.pre,"Create an identical model, do a single step of fitting with restore=True,"
2.8.0.pre,and make sure it got restored correctly.
2.8.0.pre,Build a model that predicts uncertainty.
2.8.0.pre,Fit the model and see if its predictions are correct.
2.8.0.pre,Take a tiny step in the direction of s and see if the output changes by
2.8.0.pre,the expected amount.
2.8.0.pre,Load dataset and Models
2.8.0.pre,call model.fit again to test multiple fit() calls
2.8.0.pre,There are 4 atoms each of which have 75 atom features
2.8.0.pre,There are 10 pairs with infinity distance and 14 pair features
2.8.0.pre,4 atoms in total
2.8.0.pre,10 pairs in total
2.8.0.pre,10 pairs in total each with start/finish
2.8.0.pre,There are 4 atoms each of which have 75 atom features
2.8.0.pre,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.8.0.pre,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.8.0.pre,connections and accounting for symmetry.)
2.8.0.pre,4 atoms in total
2.8.0.pre,10 pairs in total
2.8.0.pre,The center atom is self connected and to both neighbors so it appears
2.8.0.pre,thrice. The canonical ranking used in MolecularFeaturizer means this
2.8.0.pre,central atom is ranked last in ordering.
2.8.0.pre,10 pairs in total each with start/finish
2.8.0.pre,Note: Following are some changes
2.8.0.pre,compared to the TensorFlow unit test:
2.8.0.pre,1. Changed nb_epoch to 300.
2.8.0.pre,2. Increased the learning_rate to 0.0003.
2.8.0.pre,Note: This needs to be inspected in future to understand low score as compared to a score of 0.9 in tensorflow unit test.
2.8.0.pre,Note: Following are some changes
2.8.0.pre,compared to the TensorFlow unit test:
2.8.0.pre,1. Changed nb_epoch to 400.
2.8.0.pre,2. Reduced the number of data points to 2.
2.8.0.pre,3. Increased batch_size to 20.
2.8.0.pre,4. Increased the learning_rate to 0.0003.
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Note: This needs to be inspected in future to understand low score.
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Note: Compared to the tensorflow unit test increased the nb_epoch to 600.
2.8.0.pre,Eval model on train
2.8.0.pre,Note: This needs to be inspected in future to understand low score as compared to a score of 0.8 in tensorflow unit test.
2.8.0.pre,Create input tensor with values within full_atom_feature_dims
2.8.0.pre,Create input tensor with values within full_bond_feature_dims
2.8.0.pre,Compute the global graph representation
2.8.0.pre,Compute positive and negative scores
2.8.0.pre,Check if the loss is a scalar and has the correct dtype
2.8.0.pre,Check if the loss is a scalar and non-negative
2.8.0.pre,Train the model on random sequences.  We aren't training long enough to
2.8.0.pre,"really make it reliable, but I want to keep this test fast, and it should"
2.8.0.pre,still be able to reproduce a reasonable fraction of input sequences.
2.8.0.pre,Test it out.
2.8.0.pre,Check that it got at least a quarter of them correct.
2.8.0.pre,Test it out.
2.8.0.pre,Actually training a VAE takes far too long for a unit test.  Just run a
2.8.0.pre,"few steps of training to make sure nothing crashes, then check that the"
2.8.0.pre,results are at least internally consistent.
2.8.0.pre,Get Data
2.8.0.pre,Check Shape
2.8.0.pre,Check number of parameters
2.8.0.pre,Eval model on train
2.8.0.pre,load sample dataset
2.8.0.pre,initialize the model
2.8.0.pre,fit the model
2.8.0.pre,reload the model
2.8.0.pre,atom features
2.8.0.pre,Gaussian histograms expands into 11 Gaussian buckets.
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,overfit test
2.8.0.pre,test on a small MoleculeNet dataset
2.8.0.pre,load datasets
2.8.0.pre,initialize models
2.8.0.pre,edges logits used during training
2.8.0.pre,nodes logits used during training
2.8.0.pre,edges logits
2.8.0.pre,nodes logits
2.8.0.pre,For training
2.8.0.pre,For sample generation
2.8.0.pre,Define the dense layers
2.8.0.pre,"Ignoring type. For TorchModel, loss is a required argument but HuggingFace computes"
2.8.0.pre,"loss during the forward iteration, removing the need for a loss function."
2.8.0.pre,FIXME Transformers library has an api like AutoModel.from_pretrained. It allows to
2.8.0.pre,initialise and create a model instance directly without requiring a class instance initialisation step.
2.8.0.pre,"To use `load_from_pretrained` in DeepChem, we need to follow a two step process"
2.8.0.pre,of initialising class instance and then loading weights via `load_from_pretrained`.
2.8.0.pre,y is None during predict
2.8.0.pre,Main training loop.
2.8.0.pre,Report progress and write checkpoints.
2.8.0.pre,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0.pre,Report final results.
2.8.0.pre,Invoke the model.
2.8.0.pre,Apply tranformers and record results.
2.8.0.pre,Concatenate arrays to create the final results.
2.8.0.pre,copy the input graph to avoid in-place operations
2.8.0.pre,"FIXME For pretraining task, both model2d and model3d but the super class"
2.8.0.pre,"can't handle two models for contrastive learning, hence we pass only model2d"
2.8.0.pre,torch's one-hot encoding works with integer data types.
2.8.0.pre,"We convert labels to integer, one-hot encode and convert it back to float"
2.8.0.pre,for making it suitable to loss function
2.8.0.pre,graphs = [[
2.8.0.pre,graph_data.to_dgl_graph().to(self.device) for graph_data in row
2.8.0.pre,] for row in inputs]
2.8.0.pre,convert the GraphData objects to DGL graphs
2.8.0.pre,"TODO Ideally, we should use a lr schedule but we need to update lr_scheduler.step() method"
2.8.0.pre,in ModularTorchModel.fit_generator to accept a metric.
2.8.0.pre,"self._lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(self._pytorch_optimizer,"
2.8.0.pre,mode='min')
2.8.0.pre,Initialize buffers
2.8.0.pre,Accumulate statistics for Fisher matrices
2.8.0.pre,Initialize buffers
2.8.0.pre,p_grad_mat is of output_dim * input_dim
2.8.0.pre,inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
2.8.0.pre,we always put gradient w.r.t weight in [0]
2.8.0.pre,and w.r.t bias in [1]
2.8.0.pre,do kl clip
2.8.0.pre,TODO Explain in detail what the four outcompes are
2.8.0.pre,The bond and rev bond have even and odd ids respectively.
2.8.0.pre,FIXME This layer is similar to DMPNNEncoderLayer and they
2.8.0.pre,must be unified.
2.8.0.pre,Shared weight matrix across depths (default)
2.8.0.pre,Except reverse bond its-self(w) ! \sum_{k\in N(u) \ w}
2.8.0.pre,"FIXME When input_layer is none, for the first iteration of message passing, we should ideally"
2.8.0.pre,be using different weight matrix since message will be of shape num_bonds x f_bonds_dim
2.8.0.pre,"in the first iteration and in the subsequent iterations, it will be num_bonds x hidden_size"
2.8.0.pre,FIXME We assume that we are using a hidden layer to transform the initial atom message
2.8.0.pre,and bond messages to hidden dimension size.
2.8.0.pre,self.atom_messages is False
2.8.0.pre,Note: Elementwise affine has to be consistent with the pre-training phase
2.8.0.pre,multi-headed attention
2.8.0.pre,support no residual connection in MTBlock.
2.8.0.pre,atom input to atom output
2.8.0.pre,bond to atom
2.8.0.pre,atom input to bond output
2.8.0.pre,bond input to bond output
2.8.0.pre,Inputs
2.8.0.pre,Noise Input
2.8.0.pre,Data Input
2.8.0.pre,Data Inputs
2.8.0.pre,Conditional Input
2.8.0.pre,Conditional Inputs
2.8.0.pre,Generators
2.8.0.pre,Discriminators
2.8.0.pre,Forward pass through generators
2.8.0.pre,Forward pass through discriminators
2.8.0.pre,Compute loss functions
2.8.0.pre,Compute the weighted errors
2.8.0.pre,Create learnable weights for the generators and discriminators.
2.8.0.pre,Compute the weighted errors
2.8.0.pre,Add an entropy term to the loss.
2.8.0.pre,"Every call to fit_generator() will increment global_step, but we only"
2.8.0.pre,"want it to get incremented once for the entire batch, so record the"
2.8.0.pre,value and keep resetting it.
2.8.0.pre,Train the discriminator.
2.8.0.pre,Train the generator.
2.8.0.pre,Write checkpoints and report progress.
2.8.0.pre,Write out final results.
2.8.0.pre,PyTorch layers require input and output channels as parameter
2.8.0.pre,"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
2.8.0.pre,"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
2.8.0.pre,initializing layer bias with nn.init gives mypy typecheck error
2.8.0.pre,using the following workaround
2.8.0.pre,residual blocks can only be used when successive layers have the same output shape
2.8.0.pre,Used for converting edges back to their original shape
2.8.0.pre,Compute mean edge features for each node by dst_index (each node
2.8.0.pre,"receives information from edges which have that node as its destination,"
2.8.0.pre,hence the computation uses dst_index to aggregate information)
2.8.0.pre,holding bi-directional edges in case of undirected graphs
2.8.0.pre,coonverting edge features to its original shape
2.8.0.pre,Input
2.8.0.pre,Shared weight matrix across depths (default):
2.8.0.pre,For messages hidden states
2.8.0.pre,For atom hidden states
2.8.0.pre,num_atoms x hidden_size
2.8.0.pre,num_molecules x hidden_size
2.8.0.pre,concat global features
2.8.0.pre,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0.pre,"Shape (N_atoms, M_nbrs, ndim)"
2.8.0.pre,"Shape (N_atoms, M_nbrs)"
2.8.0.pre,Number of grid cells
2.8.0.pre,TODO(rbharath): Support batching
2.8.0.pre,"Shape (n_cells, ndim)"
2.8.0.pre,"List of length N_atoms, each element of different length uniques_i"
2.8.0.pre,"List of length N_atoms, each element of different length uniques_i"
2.8.0.pre,"List of length N_atoms, each a tensor of shape"
2.8.0.pre,"(uniques_i, ndim)"
2.8.0.pre,Add phantom atoms that exist far outside the box
2.8.0.pre,"List of length N_atoms, each of shape (1, ndim)"
2.8.0.pre,TODO(rbharath): How does distance need to be modified here to
2.8.0.pre,account for periodic boundary conditions?
2.8.0.pre,List of length N_atoms each of shape (M_nbrs)
2.8.0.pre,"N_atoms elts of size (M_nbrs,) each"
2.8.0.pre,"Shape (N_atoms, 1)"
2.8.0.pre,Find M_nbrs atoms closest to each cell
2.8.0.pre,"Shape (n_cells, M_nbrs)"
2.8.0.pre,Associate each cell with its neighbor cells. Assumes periodic boundary
2.8.0.pre,"conditions, so does wrapround. O(constant)"
2.8.0.pre,"Shape (n_cells, n_nbr_cells)"
2.8.0.pre,"Shape (N_atoms, n_nbr_cells)"
2.8.0.pre,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.8.0.pre,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.8.0.pre,"List of length N_atoms, each element length uniques_i"
2.8.0.pre,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.8.0.pre,element removed to remove self from list of neighbors. Need to verify
2.8.0.pre,this holds more broadly or come up with robust alternative.
2.8.0.pre,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0.pre,"Shape (N_atoms*n_cells, ndim) after tile"
2.8.0.pre,Shape (N_atoms*n_cells)
2.8.0.pre,"Shape (n_cells, N_atoms)"
2.8.0.pre,Find k atoms closest to this cell.
2.8.0.pre,"Tensor of shape (n_cells, M_nbrs)"
2.8.0.pre,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.8.0.pre,"Shape (N_atoms*n_cells, 1) after tile"
2.8.0.pre,TODO(rbharath): Do we need to handle periodic boundary conditions
2.8.0.pre,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.8.0.pre,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.8.0.pre,the cube.
2.8.0.pre,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.8.0.pre,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.8.0.pre,"Tile (a, a, a, b, b, b, etc.)"
2.8.0.pre,"Tile (a, b, c, a, b, c, ...)"
2.8.0.pre,No other forget biases supported right now.
2.8.0.pre,Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
2.8.0.pre,Define the layers
2.8.0.pre,Create the final layers
2.8.0.pre,Add another value(~-Inf) to prevent error in softmax
2.8.0.pre,Model using this layer must set `pad_batches=True`
2.8.0.pre,"z = torch.mm(h, self.U) + self.b"
2.8.0.pre,create a boolean mask for each partition
2.8.0.pre,partition the input tensor using the masks
2.8.0.pre,Case when >2 inputs are passed
2.8.0.pre,Loop over the remaining convolution layers
2.8.0.pre,Apply the current layer to the outputs from the previous layer
2.8.0.pre,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.8.0.pre,and embeddings of atom j(both gone through a hidden layer)
2.8.0.pre,"for atom i, sum the influence from all other atom j in the molecule"
2.8.0.pre,Construct internal trainable weights
2.8.0.pre,Weight matrix and bias matrix required to compute new atom layer from the previous atom layer
2.8.0.pre,Weight matrix and bias matrix required to compute new atom layer from the previous pair layer
2.8.0.pre,Weight matrix and bias matrix required to compute new pair layer from the previous atom layer
2.8.0.pre,Weight matrix and bias matrix required to compute new pair layer from the previous pair layer
2.8.0.pre,flake8: noqa
2.8.0.pre,Converting the input to torch tensors
2.8.0.pre,"AA is a tensor with shape[total_num_atoms,n_hidden_AA]"
2.8.0.pre,"PA is a tensor with shape[total number of pairs,n_hidden_PA]"
2.8.0.pre,Split the PA tensor according to the 'pair_split' tensor
2.8.0.pre,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.8.0.pre,normalization
2.8.0.pre,"PP is a tensor with shape [total number of pairs,n_hidden_PP]"
2.8.0.pre,Integrate the Cross Layer Mapping inside the Global Message Passing
2.8.0.pre,Message Passing operation
2.8.0.pre,Update function f_u
2.8.0.pre,Message Passing operation
2.8.0.pre,Teacher forcing: Feed the target as the next input
2.8.0.pre,Without teacher forcing: use its own predictions as the next input
2.8.0.pre,Initializing the first layer (first layer has different dims than others)
2.8.0.pre,filling the weights with xavier uniform method for the linear weights and random assignment for the bias
2.8.0.pre,Calculating one-electron feature's average
2.8.0.pre,temporary lists containing each electron's embeddings which will be torch.stack on the end
2.8.0.pre,Calculating two-electron feature's average
2.8.0.pre,"initialized weights with torch.zeros, torch.eye and using xavier init."
2.8.0.pre,temporary list to stack upon electrons axis at the end
2.8.0.pre,Integrate the Cross Layer Mapping inside the Local Message Passing
2.8.0.pre,Message Passing 1
2.8.0.pre,Message Passing 2
2.8.0.pre,Aggregation
2.8.0.pre,Update function f_u
2.8.0.pre,Output Module
2.8.0.pre,import torch.nn as nn
2.8.0.pre,creating one and two electron features
2.8.0.pre,setting the fermient layer and fermient envelope layer batch size to be that of the current batch size of the model. This enables for vectorized calculations of hessians and jacobians.
2.8.0.pre,using functorch to calcualte hessian and jacobian in one go
2.8.0.pre,using index tensors to index out the hessian elemennts corresponding to the same variable (cross-variable derivatives are ignored)
2.8.0.pre,"doing all the calculation and detaching from graph to save memory, which allows larger batch size"
2.8.0.pre,cloning self.input which will serve as the new input for the vectorized functions.
2.8.0.pre,lambda function for calculating the log of absolute value of the wave function.
2.8.0.pre,using jacrev for the jacobian and jacrev twice for to calculate the hessian. The functorch's hessian function if directly used does not give stable results.
2.8.0.pre,making the batch size temporarily as 1 for the vectorization of hessian and jacobian.
2.8.0.pre,Initialization for ionic molecules
2.8.0.pre,hook function below is an efficient way modifying the gradients on the go rather than looping
2.8.0.pre,using non-local variables as a means of parameter passing
2.8.0.pre,the move function calculates the energy of sampled electrons and samples new set of electrons (does not calculate loss)
2.8.0.pre,clipping local energies which are away 5 times the variance from the median
2.8.0.pre,using the sampled electrons from the electron sampler for bacward pass and modifying gradients
2.8.0.pre,going through each step of random walk and calculating the modified gradients with local energies
2.8.0.pre,embedding node features
2.8.0.pre,convolutional layer
2.8.0.pre,pooling
2.8.0.pre,for n_tasks == 1 case
2.8.0.pre,mypy check is ignored for global_features as it is not a default attribute
2.8.0.pre,of GraphData. It is created during runtime using **kwargs.
2.8.0.pre,mapping from bond index to the index of the atom (where the bond is coming from)
2.8.0.pre,"mapping from bond index to concat(in_atom, bond) features"
2.8.0.pre,mapping from atom index to list of indicies of incoming bonds
2.8.0.pre,mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
2.8.0.pre,zero padded at the end
2.8.0.pre,get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
2.8.0.pre,padded with -1 at the end
2.8.0.pre,mapping from bond index to the index of the atom (where the bond if going to)
2.8.0.pre,mapping from atom index to list of indicies of incoming bonds
2.8.0.pre,get maximum number of incoming bonds
2.8.0.pre,Make number of incoming bonds equal to maximum number of bonds.
2.8.0.pre,This is done by appending -1 to fill remaining space at each atom indices.
2.8.0.pre,mapping from bond index to the index of the reverse bond
2.8.0.pre,get encoder
2.8.0.pre,get input size for ffn
2.8.0.pre,get output size for ffn
2.8.0.pre,get ffn
2.8.0.pre,Steps to get `molecules_unbatch_key`:
2.8.0.pre,1. Get the tensor containing the indices of first atoms of each molecule
2.8.0.pre,2. Get the tensor containing number of atoms of each molecule
2.8.0.pre,by taking the difference between consecutive indices.
2.8.0.pre,3. Convert the tensor to a list.
2.8.0.pre,num_molecules x (enc_hidden + global_features_size)
2.8.0.pre,ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
2.8.0.pre,"atom feature matrix with shape [number of atoms, number of features]"
2.8.0.pre,concatenated feature vector which contains concatenation of initial atom and bond features
2.8.0.pre,mapping from atom index to list of indicies of incoming bonds
2.8.0.pre,mapping that maps bond index to 'array of indices of the bonds'
2.8.0.pre,incoming at the initial atom of the bond (excluding the reverse bonds)
2.8.0.pre,array of global molecular features
2.8.0.pre,maximum number of incoming bonds in the batch
2.8.0.pre,generate concatenated feature vector and mappings
2.8.0.pre,pad all mappings to maximum number of incoming bonds in the batch
2.8.0.pre,the hidden size here is the output size of last layer in mol_atom_from_atom_ffn and mol_atom_from_bond_ffn components.
2.8.0.pre,it is necessary that aforementioned components produces output tensor of same size.
2.8.0.pre,"In training mode, we return atom level aggregated output and bond level aggregated output."
2.8.0.pre,The training has an additional objective which ensures that atom and bond level aggregated outputs
2.8.0.pre,are similar to each other apart from the objective of making the aggregated output closer to each
2.8.0.pre,other.
2.8.0.pre,"FIXME In the above step, we initialize modular torch model but"
2.8.0.pre,something is missing here. The attribute loss from TorchModel gets assigned `loss_func`
2.8.0.pre,by super class initialization in ModularTorchModel but here we reinitialize it.
2.8.0.pre,in eval mode.
2.8.0.pre,"Also adding features, this is optional"
2.8.0.pre,FIXME I am rewriting restore because the restore method in parent class
2.8.0.pre,does not restore layers which are not components. This restore method
2.8.0.pre,can restore an full model.
2.8.0.pre,may mess with loading pretrained weights
2.8.0.pre,remove relu for the last layer
2.8.0.pre,"reshapes node_representation to (num_nodes, num_layers * emb_dim)"
2.8.0.pre,"for supervised tasks, add prediction head"
2.8.0.pre,unsupervised tasks do not need a pred head
2.8.0.pre,negative contexts are obtained by shifting the indicies of context embeddings
2.8.0.pre,"sample x distinct nodes to be masked, based on mask rate. But"
2.8.0.pre,will sample at least 1 node
2.8.0.pre,create mask node label by copying node feature of mask node
2.8.0.pre,modify the original node feature of the masked node
2.8.0.pre,zeros are meant to represent the masked features. This is distinct from the
2.8.0.pre,"original implementation, where the masked features are represented by the"
2.8.0.pre,the last feature token 119.
2.8.0.pre,link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L241
2.8.0.pre,create mask edge labels by copying edge features of edges that are connected to
2.8.0.pre,mask nodes
2.8.0.pre,create mask edge labels by copying edge features of the edges connected to
2.8.0.pre,the mask nodes
2.8.0.pre,edge ordering is such that two directions of a single
2.8.0.pre,"edge occur in pairs, so to get the unique undirected"
2.8.0.pre,"edge indices, we take every 2nd edge index from list"
2.8.0.pre,modify the original edge features of the edges connected to the mask nodes
2.8.0.pre,zeros are meant to represent the masked features. This is distinct from the
2.8.0.pre,"original implementation, where the masked features are represented by the"
2.8.0.pre,the last feature token 4.
2.8.0.pre,link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/chem/util.py#L268
2.8.0.pre,"sample x distinct edges to be masked, based on mask rate. But"
2.8.0.pre,will sample at least 1 edge
2.8.0.pre,"during sampling, we only pick the 1st direction of a particular"
2.8.0.pre,edge pair
2.8.0.pre,create ground truth edge features for the edges that correspond to
2.8.0.pre,the masked indices
2.8.0.pre,"created new masked edge_attr, where both directions of the masked"
2.8.0.pre,edges have masked edge type. For message passing in gcn
2.8.0.pre,append the 2nd direction of the masked edges
2.8.0.pre,zeros are meant to represent the masked features. This is distinct from the
2.8.0.pre,"original implementation, where the masked features are represented by 0s and"
2.8.0.pre,an additional mask feature
2.8.0.pre,link to source: https://github.com/snap-stanford/pretrain-gnns/blob/08f126ac13623e551a396dd5e511d766f9d4f8ff/bio/util.py#L101
2.8.0.pre,"Take the entire graph, but can be modified to take a subgraph of k-hops from the root node"
2.8.0.pre,Get node idx between root and the inner diameter l1
2.8.0.pre,Get node idx between root and the outer diameter l2
2.8.0.pre,takes a ring around the root node outside of l1 and inside of l2
2.8.0.pre,"Get indices of overlapping nodes between substruct and context, WRT context ordering"
2.8.0.pre,Decide first number of GAT layers
2.8.0.pre,set2set doubles the dimensionality of the embedding
2.8.0.pre,n_tasks is Optional[int] while argument 2 of nn.Linear has to be of type int
2.8.0.pre,original implementation also includes an option if not using a separate encoder:
2.8.0.pre,loss = sup_loss + local_unsup_loss * self.learning_rate
2.8.0.pre,Below functions were taken from DeepChem TextCNN tensorflow implementation
2.8.0.pre,Transform SMILES sequence to integers
2.8.0.pre,Maximum length is expanded to allow length variation during train and inference
2.8.0.pre,'_' served as delimiter and padding
2.8.0.pre,Initialize common characters as keys
2.8.0.pre,Include space to avoid extra keys
2.8.0.pre,"For 'Cl', 'Br', etc."
2.8.0.pre,"Character not recognized, add to extra_keys"
2.8.0.pre,Add all extra_keys to char_dict
2.8.0.pre,Skip all spaces
2.8.0.pre,"For 'Cl', 'Br', etc."
2.8.0.pre,Padding with '_'
2.8.0.pre,We convert deepchem.feat.GraphData to a PyG graph and then
2.8.0.pre,batch it.
2.8.0.pre,The default_generator method returns an array of dc.feat.GraphData objects
2.8.0.pre,"nested inside a list. To access the nested array of graphs, we are"
2.8.0.pre,indexing by 0 here.
2.8.0.pre,Do a simple greedy search.
2.8.0.pre,Do a beam search with length normalization.
2.8.0.pre,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.8.0.pre,This candidate sequence has already been terminated
2.8.0.pre,Consider all possible tokens we could add to this candidate sequence.
2.8.0.pre,flake8:noqa
2.8.0.pre,get DTNNEmbedding
2.8.0.pre,get DTNNSteps
2.8.0.pre,get DTNNGather
2.8.0.pre,get Final Linear Layer
2.8.0.pre,"pair_edges is of shape (2, N)"
2.8.0.pre,number of atoms in each molecule
2.8.0.pre,index of pair features
2.8.0.pre,Get starting pair atoms
2.8.0.pre,number of pairs for each atom
2.8.0.pre,atom features
2.8.0.pre,pair features
2.8.0.pre,pretransformation
2.8.0.pre,aggregation
2.8.0.pre,post-transformation
2.8.0.pre,The model predicts unnormalized probabilities for each class and task
2.8.0.pre,"print (logits, proba)"
2.8.0.pre,FIXME self.loss_func is an incorrect argument for TorchModel.loss because
2.8.0.pre,it performs more than computing loss
2.8.0.pre,FIXME This line is not needed as loss is computed inside the call to loss_func
2.8.0.pre,Main training loop.
2.8.0.pre,"Execute the loss function, accumulating the gradients."
2.8.0.pre,Report progress and write checkpoints.
2.8.0.pre,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0.pre,Report final results.
2.8.0.pre,Ensure weights for both models are built.
2.8.0.pre,Rename and delete older files.
2.8.0.pre,Select a device.
2.8.0.pre,W&B logging
2.8.0.pre,"If `wandb=True` and no logger is provided, initialize default logger"
2.8.0.pre,Setup and initialize W&B logging
2.8.0.pre,Update config with KerasModel params
2.8.0.pre,Main training loop.
2.8.0.pre,"Execute the loss function, accumulating the gradients."
2.8.0.pre,Report progress and write checkpoints.
2.8.0.pre,Capture the last avg_loss in case of return since we're resetting to 0 now
2.8.0.pre,Report final results.
2.8.0.pre,Invoke the model.
2.8.0.pre,Apply tranformers and record results.
2.8.0.pre,Concatenate arrays to create the final results.
2.8.0.pre,Compute the gradients.
2.8.0.pre,Save the checkpoint to a file.
2.8.0.pre,Rename and delete older files.
2.8.0.pre,Ensure weights for both models are built.
2.8.0.pre,True will be assigned up successful training attempt
2.8.0.pre,create new model
2.8.0.pre,to avoid flake8 E125/yapf incompatibility
2.8.0.pre,generate input
2.8.0.pre,train model
2.8.0.pre,generate sample
2.8.0.pre,check how many valid molecules were created and add to list
2.8.0.pre,finally test if there was at least one valid training session
2.8.0.pre,as the model structure improves this should become more and more strict
2.8.0.pre,import torch.nn as nn
2.8.0.pre,import torch.nn.functional as F
2.8.0.pre,Testing Shapes
2.8.0.pre,Testing values
2.8.0.pre,Dense1 is a list of dense layers
2.8.0.pre,Testing Values
2.8.0.pre,Testing Shapes with TF Model Output
2.8.0.pre,Testing Shapes
2.8.0.pre,Testing Values
2.8.0.pre,testing first convolution layer
2.8.0.pre,dense1 layer - list of dense layers
2.8.0.pre,dense2 layer - single dense layer
2.8.0.pre,testing rest of the convolution layer
2.8.0.pre,dense1 layer - list of dense layers
2.8.0.pre,dense2 layer - single dense layer
2.8.0.pre,Loading input tensors
2.8.0.pre,Testing output
2.8.0.pre,Testing MultiConvolution Layer
2.8.0.pre,Testing First Convolution Layer
2.8.0.pre,dense1 layer - list of dense layers
2.8.0.pre,dense2 layer - single dense layer
2.8.0.pre,Testing rest of the Multi convolution layer
2.8.0.pre,dense1 layer - list of dense layers
2.8.0.pre,dense2 layer - single dense layer
2.8.0.pre,Testing Aggregation Layer
2.8.0.pre,Loading input tensors
2.8.0.pre,Testing output
2.8.0.pre,9: number of atoms
2.8.0.pre,6: number of bonds
2.8.0.pre,3: number of molecules
2.8.0.pre,logits for class 1
2.8.0.pre,logits for class 2
2.8.0.pre,since pretraining is a self-supervision task where labels are generated during
2.8.0.pre,"preparing batch, we mock _prepare_batch_for_pretraining to set all labels to 0."
2.8.0.pre,The test here is checking whether the model predict 0's after overfitting.
2.8.0.pre,preparing for test by setting 0 labels
2.8.0.pre,arranging test - preparing dataset
2.8.0.pre,acting - tests
2.8.0.pre,asserting
2.8.0.pre,arranging for tests
2.8.0.pre,checking weights don't match before restore
2.8.0.pre,norm layers and cached zero vectors have constant weights
2.8.0.pre,restoring model
2.8.0.pre,checking matching of weights after restore
2.8.0.pre,asserting that weights are not same before reloading
2.8.0.pre,"notm and bias layers have constant weights, hence they are not checked"
2.8.0.pre,acting - loading pretrained weights
2.8.0.pre,asserting that weight matches after loading
2.8.0.pre,"For simplicity, let's assume both molecules have same number of"
2.8.0.pre,atoms.
2.8.0.pre,Creates a set of dummy features that contain the coordinate and
2.8.0.pre,neighbor-list features required by the AtomicConvModel.
2.8.0.pre,Creates a set of dummy features that contain the coordinate and
2.8.0.pre,neighbor-list features required by the AtomicConvModel.
2.8.0.pre,"helper classes that depend on torch, they need to be in the try/catch block"
2.8.0.pre,Define the dense layers
2.8.0.pre,Define the dense layers
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,No training has been done after reload
2.8.0.pre,We have to set the gradient penalty very small because the generator's
2.8.0.pre,"output is only a single number, so the default penalty would constrain"
2.8.0.pre,it far too much.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,We have to set the gradient penalty very small because the generator's
2.8.0.pre,"output is only a single number, so the default penalty would constrain"
2.8.0.pre,it far too much.
2.8.0.pre,See if it has done a plausible job of learning the distribution.
2.8.0.pre,Create dummy data
2.8.0.pre,Call forward
2.8.0.pre,Asserts
2.8.0.pre,Create dummy data
2.8.0.pre,Call forward
2.8.0.pre,"Since the penalty is a squared norm of the gradients minus 1, multiplied by a constant,"
2.8.0.pre,it should be non-negative
2.8.0.pre,Create pretrained model
2.8.0.pre,Create finetuning model
2.8.0.pre,Load pretrained model
2.8.0.pre,check weights match
2.8.0.pre,all keys should match
2.8.0.pre,keys should not match
2.8.0.pre,"In a batched graph, atoms and bonds belonging to different graphs are differentiated"
2.8.0.pre,"via scopes. In the below scenario, we assume a batched mol graph of three molecules"
2.8.0.pre,"with 10 atoms, 20 bonds. On the 10 atoms, we consider the first 3 belonging to mol1,"
2.8.0.pre,next 3 belonging to mol2 and remaining 4 belonging to mol4.
2.8.0.pre,"Hence, the atom scope is [(0, 3), (3, 3), (6, 4)]. Similarly, for bonds, we have first 5 bonds belonging to mol1, next 4 to mol2 and remaining 11 to bond3."
2.8.0.pre,"TODO Write tests for undirected = True case, currently fails. for this case, we have"
2.8.0.pre,"to generate inputs (a2b, b2a, b2revb) for undirected graph."
2.8.0.pre,The shapes should match the earlier shapes because message passing only updates node features.
2.8.0.pre,The following variables are utility variables used during message passing to compute neighbors. Here we are asserting that MTBlock layer is not modifying these variables.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Load mini log-solubility dataset.
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Fit trained model
2.8.0.pre,Eval model on train
2.8.0.pre,Load dataset
2.8.0.pre,Intiliaze torch TextCNN
2.8.0.pre,Copy other layer weights
2.8.0.pre,Run prediction
2.8.0.pre,Pretraining in MLM mode
2.8.0.pre,Pretraining in Multitask Regression Mode
2.8.0.pre,test regression
2.8.0.pre,test multitask regression
2.8.0.pre,test classification
2.8.0.pre,logit scores
2.8.0.pre,check weights match
2.8.0.pre,all keys values should match
2.8.0.pre,new model's model attribute is an entirely new model initiated by AutoModel.load_from_pretrained
2.8.0.pre,and hence it should have a different identifier
2.8.0.pre,testing a simple scenario where each embedding corresponds to an unique graph
2.8.0.pre,"here embeddings 0, 1 belong to a scope, 2, 3 to another scope and 4, 5 to another scope"
2.8.0.pre,"thus, we sill have 3 graphs"
2.8.0.pre,Porting the weights from TF to PyTorch
2.8.0.pre,task 0 layer 0
2.8.0.pre,task 0 layer 1
2.8.0.pre,task 0 output layer
2.8.0.pre,task 1 layer 0
2.8.0.pre,task 1 layer 1
2.8.0.pre,task 1 output layer
2.8.0.pre,task 1 adapter 0
2.8.0.pre,task 1 adapter 1
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Generate dummy dataset
2.8.0.pre,model is None when reloading a model
2.8.0.pre,Some scikit-learn models don't use weights.
2.8.0.pre,flake8: ignore
2.8.0.pre,This will not work for ModularTorchModel as it is directly uses `loss_func` to compute loss.
2.8.0.pre,flake8:noqa
2.8.0.pre,flake8: noqa
2.8.0.pre,"torch.nn.module prefix has no ending dot, while xt prefix has"
2.8.0.pre,neural network xc functional of GGA (receives the density and grad as inputs)
2.8.0.pre,"densinfo.value: (*BD, nr)"
2.8.0.pre,"densinfo.grad : (*BD, nr, 3)"
2.8.0.pre,"collect the total density (n), spin density (xi), and normalized gradients (s)"
2.8.0.pre,normalize the gradient
2.8.0.pre,decide how to transform the density to be the input of nn
2.8.0.pre,get the neural network output
2.8.0.pre,QR decomposition's solution is not unique in a way that every column
2.8.0.pre,can be multiplied by -1 and it still a solution
2.8.0.pre,"So, to remove the non-uniqueness, we will make the sign of the sum"
2.8.0.pre,positive.
2.8.0.pre,construct the rotation parameters
2.8.0.pre,calculate the orthogonal orbital
2.8.0.pre,"orb: (*, nao, norb)"
2.8.0.pre,the orbital becomes the coefficients while params is all zeros (no rotation)
2.8.0.pre,GDBT doesn't support multi-output(task)
2.8.0.pre,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.8.0.pre,retrain model to whole data using best n_estimators * 1.25
2.8.0.pre,GDBT doesn't support multi-output(task)
2.8.0.pre,########################################
2.8.0.pre,Deprecation warnings for XGBoostModel
2.8.0.pre,########################################
2.8.0.pre,flake8: noqa
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Assigning featurizer if not user defined
2.8.0.pre,loading datasets
2.8.0.pre,Assembling train and valid datasets
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Building tensorflow MultitaskDNN model
2.8.0.pre,Building tensorflow robust MultitaskDNN model
2.8.0.pre,Building scikit logistic regression model
2.8.0.pre,Transform fingerprints to IRV features
2.8.0.pre,Building tensorflow IRV model
2.8.0.pre,Building scikit random forest model
2.8.0.pre,Building scikit learn Kernel SVM model
2.8.0.pre,Building xgboost classification model
2.8.0.pre,Remove token for paddings
2.8.0.pre,Building scikit random forest model
2.8.0.pre,Building scikit learn Kernel Ridge Regression model
2.8.0.pre,Building scikit learn Kernel Ridge Regression model
2.8.0.pre,Building xgboost regression model
2.8.0.pre,Loading hyperparameters
2.8.0.pre,num positive/negative ligands
2.8.0.pre,Set batch sizes for network
2.8.0.pre,Model structure
2.8.0.pre,Traning settings
2.8.0.pre,Fit trained model
2.8.0.pre,Evaluating low data model
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,Assigning featurizer if not user defined
2.8.0.pre,loading datasets
2.8.0.pre,
2.8.0.pre,Note by @XericZephyr. Reason why I spun off this function:
2.8.0.pre,1. Some model needs dataset information.
2.8.0.pre,2. It offers us possibility to **cache** the dataset
2.8.0.pre,"if the featurizer runs very slow, e.g., GraphConv."
2.8.0.pre,2+. The cache can even happen at Travis CI to accelerate
2.8.0.pre,CI testing.
2.8.0.pre,
2.8.0.pre,loading datasets
2.8.0.pre,!/usr/bin/env python2
2.8.0.pre,-*- coding: utf-8 -*-
2.8.0.pre,"TODO For this dataset and model, the R2-scores are less than 0.3."
2.8.0.pre,This has to be improved.
2.8.0.pre,See: https://github.com/deepchem/deepchem/issues/2776
2.8.0.pre,TODO: Check for this
2.8.0.pre,Download files if they don't exist
2.8.0.pre,Featurize the KINASE dataset
2.8.0.pre,Shuffle the training data
2.8.0.pre,Apply transformations
2.8.0.pre,TIMING
2.8.0.pre,transformers = [
2.8.0.pre,"deepchem.trans.LogTransformer(transform_X=True),"
2.8.0.pre,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.8.0.pre,dataset=train_dataset)]
2.8.0.pre,Set shard size low to avoid memory problems.
2.8.0.pre,TIMING
2.8.0.pre,TIMING
2.8.0.pre,Set some global variables up top
2.8.0.pre,Featurize KAGGLE dataset
2.8.0.pre,TIMING
2.8.0.pre,TIMING
2.8.0.pre,Build the path to the dataset on disk.
2.8.0.pre,Try to reload cached datasets.
2.8.0.pre,Create the dataset
2.8.0.pre,Split and transform the dataset.
2.8.0.pre,. clinical trial toxicity (or absence of toxicity)
2.8.0.pre,. FDA approval status.
2.8.0.pre,Read the zip file
2.8.0.pre,Get the labels from filenames
2.8.0.pre,Download files if they don't exist
2.8.0.pre,Featurizing datasets
2.8.0.pre,Missing entry removal
2.8.0.pre,Shuffle the training data
2.8.0.pre,Apply transformations
2.8.0.pre,TIMING
2.8.0.pre,TODO: Check if anything needs to be added
2.8.0.pre,Featurize the FACTORS dataset
2.8.0.pre,Shuffle the training data
2.8.0.pre,Apply transformations
2.8.0.pre,TIMING
2.8.0.pre,dict of accepted featurizers for this dataset
2.8.0.pre,modify the returned dicts for your dataset
2.8.0.pre,Names of supported featurizers
2.8.0.pre,dict of accepted transformers
2.8.0.pre,dict of accepted splitters
2.8.0.pre,names of supported splitters
2.8.0.pre,Warning message about this template
2.8.0.pre,Featurize mydataset
2.8.0.pre,Get DeepChem data directory if needed
2.8.0.pre,Check for str args to featurizer and splitter
2.8.0.pre,Reload from disk
2.8.0.pre,First type of supported featurizers
2.8.0.pre,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.8.0.pre,Changer loader to match featurizer and data file type
2.8.0.pre,Featurize dataset
2.8.0.pre,Initialize transformers
2.8.0.pre,"get pdb and sdf filenames, labels and pdbids"
2.8.0.pre,load and featurize each complex
2.8.0.pre,Extract locations of data
2.8.0.pre,Extract labels
2.8.0.pre,Lines have format
2.8.0.pre,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.8.0.pre,"The base-10 logarithm, -log kd/pk"
2.8.0.pre,"def load_pcba_146(featurizer='ECFP',"
2.8.0.pre,"split='random',"
2.8.0.pre,"reload=True,"
2.8.0.pre,"data_dir=None,"
2.8.0.pre,"save_dir=None,"
2.8.0.pre,**kwargs):
2.8.0.pre,return load_pcba_dataset(
2.8.0.pre,"featurizer=featurizer,"
2.8.0.pre,"split=split,"
2.8.0.pre,"reload=reload,"
2.8.0.pre,"assay_file_name=""pcba_146.csv.gz"","
2.8.0.pre,"data_dir=data_dir,"
2.8.0.pre,"save_dir=save_dir,"
2.8.0.pre,**kwargs)
2.8.0.pre,"def load_pcba_2475(featurizer='ECFP',"
2.8.0.pre,"split='random',"
2.8.0.pre,"reload=True,"
2.8.0.pre,"data_dir=None,"
2.8.0.pre,"save_dir=None,"
2.8.0.pre,**kwargs):
2.8.0.pre,return load_pcba_dataset(
2.8.0.pre,"featurizer=featurizer,"
2.8.0.pre,"split=split,"
2.8.0.pre,"reload=reload,"
2.8.0.pre,"assay_file_name=""pcba_2475.csv.gz"","
2.8.0.pre,"data_dir=data_dir,"
2.8.0.pre,"save_dir=save_dir,"
2.8.0.pre,**kwargs)
2.8.0.pre,Range of optimization
2.8.0.pre,We know from guard above that this is an int/float
2.8.0.pre,Specify logfile
2.8.0.pre,Make logdir if it doesn't exist.
2.8.0.pre,setup range
2.8.0.pre,Stores all results
2.8.0.pre,Store all model references so we don't have to reload
2.8.0.pre,Stores all model locations
2.8.0.pre,"param values are always float in BO, so this line converts float to int"
2.8.0.pre,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.8.0.pre,Record hyperparameters
2.8.0.pre,We have already evaluated the model for these hyperparameters.
2.8.0.pre,Add it on to the information needed for the constructor
2.8.0.pre,Not all models have nb_epoch
2.8.0.pre,Some models autosave
2.8.0.pre,Record performances
2.8.0.pre,Store all results
2.8.0.pre,Store reference to model
2.8.0.pre,GPGO maximize performance by default
2.8.0.pre,set performance to its negative value for minimization
2.8.0.pre,Demarcating internal function for readability
2.8.0.pre,execute GPGO
2.8.0.pre,FIXME: Incompatible types in assignment
2.8.0.pre,Let's fetch the model with the best parameters
2.8.0.pre,Compare best model to default hyperparameters
2.8.0.pre,Record hyperparameters
2.8.0.pre,Return default hyperparameters
2.8.0.pre,Construction dictionary mapping hyperparameter names to values
2.8.0.pre,"mypy test throws error, so ignoring it in try"
2.8.0.pre,Not all models have nb_epoch
2.8.0.pre,Some models autosave
2.8.0.pre,arbitrarily return last model
2.8.0.pre,hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
2.8.0.pre,"mypy test throws error, so ignoring it in try"
2.8.0.pre,Not all models have nb_epoch
2.8.0.pre,Some models autosave
2.8.0.pre,Update best validation score so far
2.8.0.pre,"if `hyp_str` not in `all_scores`, store it in `all_scores`"
2.8.0.pre,arbitrarily return last model trained
2.8.0.pre,"If callable, sample it for a maximum n times"
2.8.0.pre,flake8: noqa
2.8.0.pre,"2 model variants, 1 results.txt file"
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Generate dummy dataset
2.8.0.pre,These are per-example multiplier
2.8.0.pre,Test that 2 parameters were optimized
2.8.0.pre,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Define nb_epoch in hyperparam_search function call
2.8.0.pre,"max_iter model variants, 1 results.txt file"
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Generate dummy dataset
2.8.0.pre,These are per-example multiplier
2.8.0.pre,Test that 2 parameters were optimized
2.8.0.pre,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Define nb_epoch in hyperparam_search function call
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Generate dummy dataset
2.8.0.pre,These are per-example multiplier
2.8.0.pre,Test that 2 parameters were optimized
2.8.0.pre,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.8.0.pre,Generate dummy dataset
2.8.0.pre,Have the worker threads generate the rollouts for this iteration.
2.8.0.pre,Perform optimization.
2.8.0.pre,Build the inputs and run the optimizer.
2.8.0.pre,Update the number of steps taken so far and perform checkpointing.
2.8.0.pre,Merge all the rollouts into a single set of arrays.
2.8.0.pre,Iterate slices.
2.8.0.pre,Generate the rollout.
2.8.0.pre,Compute an estimate of the reward for the rest of the episode.
2.8.0.pre,Compute the discounted rewards and advantages.
2.8.0.pre,Convert the actions to one-hot.
2.8.0.pre,Rearrange the states into the proper set of arrays.
2.8.0.pre,Return the processed arrays.
2.8.0.pre,Training loop.
2.8.0.pre,Do checkpointing.
2.8.0.pre,Generate the rollout.
2.8.0.pre,Compute an estimate of the reward for the rest of the episode.
2.8.0.pre,Compute the discounted rewards and advantages.
2.8.0.pre,"Record the actions, converting to one-hot if necessary."
2.8.0.pre,Rearrange the states into the proper set of arrays.
2.8.0.pre,Build the inputs and apply gradients.
2.8.0.pre,Assume all arrays are float32.
2.8.0.pre,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.8.0.pre,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.8.0.pre,"game).  The average reward for any bet is slightly negative, so the best"
2.8.0.pre,strategy is to walk away.
2.8.0.pre,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0.pre,Optimize it.
2.8.0.pre,"It should have learned that the expected value is very close to zero, and that the best"
2.8.0.pre,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.8.0.pre,top actions).
2.8.0.pre,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.8.0.pre,get the same result.
2.8.0.pre,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0.pre,The environment just has a constant state.
2.8.0.pre,The policy includes a single recurrent layer.
2.8.0.pre,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.8.0.pre,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.8.0.pre,"On the first call, the initial state should be all zeros."
2.8.0.pre,It should still be zeros since we didn't save it last time.
2.8.0.pre,It should be different now.
2.8.0.pre,This should be the same as the previous one.
2.8.0.pre,"Now we reset it, so we should get the same result as initially."
2.8.0.pre,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.8.0.pre,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.8.0.pre,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.8.0.pre,at all.  Using hindsight makes it much easier.
2.8.0.pre,A simple policy with two hidden layers.
2.8.0.pre,Optimize it.
2.8.0.pre,Try running it a few times and see if it succeeds.
2.8.0.pre,The state consists of two numbers: a current value and a target value.
2.8.0.pre,The policy just needs to learn to output the target value (or at least
2.8.0.pre,move toward it).
2.8.0.pre,A simple policy with no hidden layers.
2.8.0.pre,Optimize it.
2.8.0.pre,Try running it and see if it reaches the target
2.8.0.pre,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.8.0.pre,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.8.0.pre,"game).  The average reward for any bet is slightly negative, so the best"
2.8.0.pre,strategy is to walk away.
2.8.0.pre,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0.pre,Optimize it.
2.8.0.pre,"It should have learned that the expected value is very close to zero, and that the best"
2.8.0.pre,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.8.0.pre,top actions).
2.8.0.pre,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.8.0.pre,get the same result.
2.8.0.pre,"Do the same thing, only using the ""restore"" argument to fit()."
2.8.0.pre,The environment just has a constant state.
2.8.0.pre,The policy includes a single recurrent layer.
2.8.0.pre,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.8.0.pre,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.8.0.pre,"On the first call, the initial state should be all zeros."
2.8.0.pre,It should still be zeros since we didn't save it last time.
2.8.0.pre,It should be different now.
2.8.0.pre,This should be the same as the previous one.
2.8.0.pre,"Now we reset it, so we should get the same result as initially."
2.8.0.pre,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.8.0.pre,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.8.0.pre,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.8.0.pre,at all.  Using hindsight makes it much easier.
2.8.0.pre,A simple policy with two hidden layers.
2.8.0.pre,Optimize it.
2.8.0.pre,Try running it a few times and see if it succeeds.
2.8.0.pre,"This policy just learns a constant probability for each action, and a constant for the value."
2.8.0.pre,Randomize who goes first
2.8.0.pre,Illegal move -- the square is not empty
2.8.0.pre,Move X
2.8.0.pre,Did X Win
2.8.0.pre,Did O Win
2.8.0.pre,"default channels are ""conda-forge"" and ""omnia"""
2.8.0.pre,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.7.1,Build a nightly package by default.
2.7.1,Environment-specific dependencies.
2.7.1,get the version from deepchem/__init__.py
2.7.1,nightly version : .devYearMonthDayHourMinute
2.7.1,Force to add `.dev` if `--release` option isn't passed when building
2.7.1,!/usr/bin/env python3
2.7.1,-*- coding: utf-8 -*-
2.7.1,Datasets and models used in the benchmark test
2.7.1,"irv, rf, rf_regression should be assigned manually"
2.7.1,Evaluate performances with different training set fraction
2.7.1,Datasets and models used in the benchmark test
2.7.1,Uncomment the two lines below if hyper_parameters are provided
2.7.1,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.7.1,hyper_parameters = pickle.load(f)
2.7.1,!/usr/bin/env python3
2.7.1,-*- coding: utf-8 -*-
2.7.1,Datasets and models used in the benchmark test
2.7.1,Load Delaney dataset
2.7.1,Get Metric
2.7.1,Fit trained model
2.7.1,Fit trained model
2.7.1,Set numpy seed
2.7.1,##Load data###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Featurize Kinase dataset
2.7.1,##Load data###
2.7.1,num_trials = 5
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,Force matplotlib to not use any Xwindows backend.
2.7.1,##Load data###
2.7.1,the histogram of the data
2.7.1,Set numpy seed
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,num_trials = 5
2.7.1,Set some global variables up top
2.7.1,Fit trained model
2.7.1,Featurize PCBA dataset
2.7.1,Initialize transformers
2.7.1,Fit trained model
2.7.1,Load sider models now
2.7.1,Load sweetlead dataset now. Pass in dataset object and appropriate
2.7.1,transformers to predict functions
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,##Load data###
2.7.1,"n_estimators=100, max_features=int(num_features/3),"
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,Fit trained model
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Load tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,This example shows how to use Pandas to load data directly
2.7.1,without using a CSVLoader object. This may be useful if you
2.7.1,want the flexibility of processing your data with Pandas
2.7.1,directly.
2.7.1,Now let's convert from a dataset back to a pandas dataframe
2.7.1,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.7.1,Featurize FACTORS dataset
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,Force matplotlib to not use any Xwindows backend.
2.7.1,##Load data###
2.7.1,the histogram of the data
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Load QM7 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Fit trained model
2.7.1,Fit trained model
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Fit trained model
2.7.1,Load QM8 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Fit trained model
2.7.1,Set numpy seed
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,Load ChEMBL dataset
2.7.1,Fit models
2.7.1,Do setup required for tf/keras models
2.7.1,Number of features on conv-mols
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,DeepCrystal Technologies 2017 - Patrick Hop
2.7.1,MIT License - have fun!!
2.7.1,Set to higher values to get better numbers
2.7.1,======================================================================
2.7.1,"Run Benchmarks {GC-DNN, SVR, RF}"
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,Only for debug!
2.7.1,Load Delaney dataset
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Do setup required for tf/keras models
2.7.1,Number of features on conv-mols
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load Delaney dataset
2.7.1,Get Metric
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load Delaney dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load MUV dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Evaluate train/test scores
2.7.1,Load MUV data
2.7.1,Build model
2.7.1,Fit trained model
2.7.1,Evaluate train/test scores
2.7.1,Extract active site
2.7.1,Featurize ligand
2.7.1,Default for CircularFingerprint
2.7.1,Featurize pocket
2.7.1,Note broadcast operation
2.7.1,Compute labels for pockets
2.7.1,Some complexes have labels but no PDB files. Filter these manually
2.7.1,Some of the ligand-names are of form (FMN ox). Use regex
2.7.1,to merge into form (FMN-ox)
2.7.1,Filter if missing PDB files
2.7.1,Load PDBBind dataset
2.7.1,Define featurizers
2.7.1,Featurize Dataset
2.7.1,########################################################## DEBUG
2.7.1,########################################################## DEBUG
2.7.1,For stable runs
2.7.1,Fit trained model
2.7.1,4-fold splits
2.7.1,num positive/negative ligands
2.7.1,10 trials on test-set
2.7.1,Define metric
2.7.1,Get supports on test-set
2.7.1,Compute accuracies
2.7.1,Number of features on conv-mols
2.7.1,Batch size of models
2.7.1,graph_model = dc.nn.SequentialGraph(n_feat)
2.7.1,Fit trained model
2.7.1,Test model
2.7.1,Join information for all tasks.
2.7.1,4-fold splits
2.7.1,num positive/negative ligands
2.7.1,Define metric
2.7.1,Get supports on test-set
2.7.1,Compute accuracies
2.7.1,Number of features on conv-mols
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Test model
2.7.1,Join information for all tasks.
2.7.1,4-fold splits
2.7.1,num positive/negative ligands
2.7.1,10 trials on test-set
2.7.1,Define metric
2.7.1,Get supports on test-set
2.7.1,Compute accuracies
2.7.1,Number of features on conv-mols
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Test model
2.7.1,Join information for all tasks.
2.7.1,4-fold splits
2.7.1,num positive/negative ligands
2.7.1,10 trials on test-set
2.7.1,Define metric
2.7.1,Get supports on test-set
2.7.1,Compute accuracies
2.7.1,Train model on support
2.7.1,Test model
2.7.1,Join information for all tasks.
2.7.1,4-fold splits
2.7.1,num positive/negative ligands
2.7.1,10 trials on test-set
2.7.1,Define metric
2.7.1,Get supports on test-set
2.7.1,Compute accuracies
2.7.1,Train model on support
2.7.1,Test model
2.7.1,Join information for all tasks.
2.7.1,Set some global variables up top
2.7.1,Featurize Tox21 dataset
2.7.1,Initialize transformers
2.7.1,Set some global variables up top
2.7.1,Featurize Tox21 dataset
2.7.1,Initialize transformers
2.7.1,Load MUV dataset
2.7.1,Featurize MUV dataset
2.7.1,Initialize transformers
2.7.1,Load MUV dataset
2.7.1,Featurize MUV dataset
2.7.1,Initialize transformers
2.7.1,Featurize SIDER dataset
2.7.1,Initialize transformers
2.7.1,Featurize SIDER dataset
2.7.1,Initialize transformers
2.7.1,Load the data.
2.7.1,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.7.1,sparse: most tasks do not include data for most molecules.  It also is very
2.7.1,"unbalanced: there are many more negatives than positives.  For each task,"
2.7.1,create a list of alternating positives and negatives so each batch will have
2.7.1,equal numbers of both.
2.7.1,Define a MetaLearner describing the learning problem.
2.7.1,Run meta-learning on 80% of the tasks.
2.7.1,Validate on the remaining tasks.
2.7.1,4-fold splits
2.7.1,10 positive/negative ligands
2.7.1,10 trials on test-set
2.7.1,Sample supports without replacement (all pos/neg should be different)
2.7.1,Define metric
2.7.1,Get supports on test-set
2.7.1,Compute accuracies
2.7.1,Train model on support
2.7.1,Test model
2.7.1,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.7.1,Join information for all tasks.
2.7.1,4-fold splits
2.7.1,num positive/negative ligands
2.7.1,Define metric
2.7.1,Get supports on test-set
2.7.1,Compute accuracies
2.7.1,Train model on support
2.7.1,Test model
2.7.1,Join information for all tasks.
2.7.1,replace with your own scratch directory
2.7.1,Number of conformations in each file increases exponentially.
2.7.1,Start with a smaller dataset before continuing. Use all of them
2.7.1,for production
2.7.1,"'ani_gdb_s03.h5',"
2.7.1,"'ani_gdb_s04.h5',"
2.7.1,"'ani_gdb_s05.h5',"
2.7.1,"'ani_gdb_s06.h5',"
2.7.1,"'ani_gdb_s07.h5',"
2.7.1,'ani_gdb_s08.h5'
2.7.1,Extract the data
2.7.1,Print the data
2.7.1,self-interaction energies taken from
2.7.1,https://github.com/isayev/ANI1_dataset README
2.7.1,flush once more at the end
2.7.1,"# For production, set nb_epoch to 100+"
2.7.1,"print(""Train scores"")"
2.7.1,print(train_scores)
2.7.1,"print(""Minimization of a single test set structure:"")"
2.7.1,"print(model.minimize_structure(coords, atomic_nums))"
2.7.1,Written by Roman Zubatyuk and Justin S. Smith
2.7.1,Modified by Yutong Zhao to make python2 compatible
2.7.1,opening file
2.7.1,print(store_loc)
2.7.1,print(type(v[0]))
2.7.1,print(k)
2.7.1,print(path)
2.7.1,Number of conformations in each file increases exponentially.
2.7.1,Start with a smaller dataset before continuing. Use all of them
2.7.1,for production
2.7.1,Extract the data
2.7.1,NOTE THE RENAMING:
2.7.1,Note sensitivity = recall
2.7.1,Load nci dataset
2.7.1,Featurize nci dataset
2.7.1,Initialize transformers
2.7.1,Set some global variables up top
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load hiv dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load hiv dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load delaney dataset
2.7.1,Fit models
2.7.1,Load delaney dataset
2.7.1,Fit models
2.7.1,Fit models
2.7.1,Load delaney dataset
2.7.1,Fit models
2.7.1,TODO: Once improved splitting API is merged in swap to simpler API
2.7.1,The return values are dc.data.Dataset objects so we need to extract
2.7.1,the ids
2.7.1,TODO once improved splitting API is merged in swap out for simpler
2.7.1,API
2.7.1,The return values are dc.data.Dataset objects so we need to extract
2.7.1,the ids
2.7.1,Fit trained model
2.7.1,Load SIDER dataset
2.7.1,Featurize SIDER dataset
2.7.1,Initialize transformers
2.7.1,Featurize permeability dataset
2.7.1,Load Tox21 dataset
2.7.1,Fit trained model
2.7.1,TODO: This should be swapped for simpler splitter API once that's merged in.
2.7.1,The return values are dc.data.Dataset objects so we need to extract
2.7.1,the ids
2.7.1,Only for debug!
2.7.1,Load clintox dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load clintox dataset
2.7.1,Fit models
2.7.1,Do setup required for tf/keras models
2.7.1,Number of features on conv-mols
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,-*- coding: utf-8 -*-
2.7.1,#############################################################################
2.7.1,## save dataset
2.7.1,#############################################################################
2.7.1,## load datasets
2.7.1,load sweetfda
2.7.1,load aact
2.7.1,## fixup smiles for matching
2.7.1,return smiles
2.7.1,map original smiles to converted smiles
2.7.1,"## join dataframes, index on smiles"
2.7.1,map original smiles back
2.7.1,## fill all nan with 0
2.7.1,## construct datasets
2.7.1,store in new datasets
2.7.1,## save datasets
2.7.1,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.7.1,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.7.1,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.7.1,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.7.1,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.7.1,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.7.1,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.7.1,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.7.1,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.7.1,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.7.1,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.7.1,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.7.1,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.7.1,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.7.1,For stable runs
2.7.1,Fit trained model
2.7.1,For stable runs
2.7.1,Fit trained model
2.7.1,For stable runs
2.7.1,Fit trained model
2.7.1,TODO The below line should be fixes
2.7.1,See: https://github.com/deepchem/deepchem/issues/2373
2.7.1,model.save()
2.7.1,transformers = [
2.7.1,"dc.trans.LogTransformer(transform_X=True),"
2.7.1,"dc.trans.NormalizationTransformer(transform_y=True,"
2.7.1,dataset=train_dataset)]
2.7.1,Featurize UV dataset
2.7.1,##Load data###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Set numpy seed
2.7.1,##Load data###
2.7.1,##Create model###
2.7.1,Use R2 classification metric
2.7.1,Only use for final evaluation
2.7.1,Force matplotlib to not use any Xwindows backend.
2.7.1,##Load data###
2.7.1,the histogram of the data
2.7.1,##Load data###
2.7.1,###################################################### DEBUG
2.7.1,###################################################### DEBUG
2.7.1,Load HOPV dataset
2.7.1,Fit models
2.7.1,Number of features on conv-mols
2.7.1,Batch size of models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load HOPV dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load HOPV dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load HOPV dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Only for debug!
2.7.1,Load HOPV dataset
2.7.1,Fit models
2.7.1,Fit trained model
2.7.1,Load TOXCAST dataset
2.7.1,Featurize TOXCAST dataset
2.7.1,Initialize transformers
2.7.1,Fit trained model
2.7.1,Processing of ToxCast data
2.7.1,Author - Aneesh Pappu
2.7.1,Loading dataframes and editing indices
2.7.1,Loop through rows of hitc matrix and replace codes with smiles strings
2.7.1,get corresponding casn
2.7.1,get corresponding smiles
2.7.1,write to cell
2.7.1,Tidy up and write to csv
2.7.1,TODO(rbharath): Check that this operation is differentiable.
2.7.1,The number of cells which we should theoretically have
2.7.1,The number of cells which we should theoretically have
2.7.1,"Each atom neighbors tensor should be (k, ndim) shaped."
2.7.1,The number of cells which we should theoretically have
2.7.1,TODO(rbharath): The test below only checks that shapes work out.
2.7.1,Need to do a correctness implementation vs. a simple CPU impl.
2.7.1,The number of cells which we should theoretically have
2.7.1,TODO(rbharath): The test below only checks that shapes work out.
2.7.1,Need to do a correctness implementation vs. a simple CPU impl.
2.7.1,The number of cells which we should theoretically have
2.7.1,TODO(rbharath): The test below only checks that shapes work out.
2.7.1,Need to do a correctness implementation vs. a simple CPU impl.
2.7.1,TODO(rbharath): Commenting this out due to weird segfaults
2.7.1,def test_vina_generate_conformers(self):
2.7.1,"""""""Test that Vina Model can generate conformers"""""""
2.7.1,data_dir = os.path.dirname(os.path.realpath(__file__))
2.7.1,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.7.1,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.7.1,max_protein_atoms = 3500
2.7.1,max_ligand_atoms = 100
2.7.1,"print(""Loading protein file"")"
2.7.1,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.7.1,protein_Z = pad_array(
2.7.1,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.7.1,max_protein_atoms)
2.7.1,"print(""Loading ligand file"")"
2.7.1,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.7.1,ligand_Z = pad_array(
2.7.1,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.7.1,max_ligand_atoms)
2.7.1,Associate each atom with cell it belongs to. O(N*n_cells)
2.7.1,"Shape (n_cells, k)"
2.7.1,"Shape (N, 1)"
2.7.1,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.1,"conditions, so does wrapround. O(constant)"
2.7.1,"Shape (n_cells, 26)"
2.7.1,"Shape (N, 26)"
2.7.1,"coords of shape (N, ndim)"
2.7.1,"Shape (N, 26, k, ndim)"
2.7.1,"Shape (N, 26, k)"
2.7.1,"Shape (N, 26, k)"
2.7.1,"Shape (N, 26, k, ndim)"
2.7.1,"For smaller systems especially, the periodic boundary conditions can"
2.7.1,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.7.1,make sure duplicate neighbors are ignored?
2.7.1,TODO(rbharath): How does distance need to be modified here to
2.7.1,account for periodic boundary conditions?
2.7.1,"Shape (N, 26, k)"
2.7.1,"Shape (N, 26*k)"
2.7.1,TODO(rbharath): This will cause an issue with duplicates!
2.7.1,"Shape (N, M)"
2.7.1,"N elts of size (M,) each"
2.7.1,"Shape (N, 26*k)"
2.7.1,"N elts of size (26*k,) each"
2.7.1,"N elts of size (M,) each"
2.7.1,"Shape (N, M)"
2.7.1,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.7.1,"N tensors of shape (n_cells, 1)"
2.7.1,"Shape (N*n_cells, 1) after tile"
2.7.1,"List of N tensors of shape (n_cells, 1)"
2.7.1,Lists of length N
2.7.1,Lists of length n_cells
2.7.1,Get indices of k atoms closest to each cell point
2.7.1,TODO(rbharath): tf.stack for tf 1.0
2.7.1,"Tensor of shape (n_cells, k, ndim)"
2.7.1,atoms_in_cells = tf.stack(atoms_in_cells)
2.7.1,"Tensor of shape (26, k, ndim)"
2.7.1,"Reshape to (26*k, ndim)"
2.7.1,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.7.1,"Dists of shape (26*k, 1)"
2.7.1,"Of shape (k, ndim)"
2.7.1,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.7.1,TODO(rbharath): Change this for tf 1.0
2.7.1,"n_cells tensors of shape (N, 1)"
2.7.1,"Shape (N*n_cells, 1) after tile"
2.7.1,"List of n_cells tensors of shape (N, 1)"
2.7.1,Lists of length n_cells
2.7.1,Lists of length n_cells
2.7.1,Get indices of k atoms closest to each cell point
2.7.1,"n_cells tensors of shape (k, ndim)"
2.7.1,"Tensor of shape (n_cells, k)"
2.7.1,TODO(rbharath):
2.7.1,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.7.1,- Need to group closest atoms amongst cell neighbors
2.7.1,- Need to do another top_k to find indices of closest neighbors.
2.7.1,- Return N lists corresponding to neighbors for every atom.
2.7.1,TODO(rbharath): Do we need to handle periodic boundary conditions
2.7.1,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.7.1,"looking for 26 neighbors, which isn't right for boundary cells in"
2.7.1,the cube.
2.7.1,Number of neighbors of central cube in 3-space is
2.7.1,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.7.1,TODO(rbharath)
2.7.1,n_cells = int(cells.get_shape()[0])
2.7.1,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.7.1,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.7.1,"Tile (a, a, a, b, b, b, etc.)"
2.7.1,"Tile (a, b, c, a, b, c, ...)"
2.7.1,"Lists of n_cells tensors of shape (N, 1)"
2.7.1,Lists of length n_cells
2.7.1,Lists of length n_cells
2.7.1,Get indices of k atoms closest to each cell point
2.7.1,"n_cells tensors of shape (26,)"
2.7.1,TODO(rbharath): Make this handle minibatches
2.7.1,"Shape (N_protein+N_ligand, 3)"
2.7.1,"Shape (N_protein+N_ligand,)"
2.7.1,"Shape (N_protein+N_ligand, M)"
2.7.1,"Shape (N_protein+N_ligand,)"
2.7.1,"Shape (N_protein+N_ligand, 3)"
2.7.1,"Shape (N_protein+N_ligand,)"
2.7.1,"Shape (N_protein+N_ligand, M)"
2.7.1,"Shape (N_protein+N_ligand, M, 3)"
2.7.1,"Shape (N_protein+N_ligand, M)"
2.7.1,"Shape (N_protein+N_ligand, M, 3)"
2.7.1,"Shape (N_protein+N_ligand, M)"
2.7.1,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.7.1,"Shape (N_protein+N_ligand, M)"
2.7.1,"Shape (N_protein+N_ligand, M)"
2.7.1,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.7.1,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.7.1,computing free-energy. This implementation currently uses all interaction
2.7.1,terms. Not sure if this makes a difference.
2.7.1,"Shape (N_protein+N_ligand, M)"
2.7.1,Shape () -- scalar
2.7.1,Keep track of the layers
2.7.1,"For graphical layers, add connectivity placeholders"
2.7.1,Add layer to the layer list
2.7.1,Keep track of the layers
2.7.1,Create graph topology and x
2.7.1,Keep track of the layers
2.7.1,Whether or not we have used the GraphGather layer yet
2.7.1,Update new value of x
2.7.1,Update new value of x
2.7.1,Update new value of x
2.7.1,Get train function
2.7.1,Initialize
2.7.1,################################################################### DEBUG
2.7.1,self.test_label_placeholder = Input(
2.7.1,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.7.1,"name=""label_placeholder""))"
2.7.1,self.test_weight_placeholder = Input(
2.7.1,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.7.1,"name=""weight_placeholder""))"
2.7.1,TODO(rbharath): Should weights for the support be used?
2.7.1,Support labels
2.7.1,self.support_label_placeholder = Input(
2.7.1,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.7.1,"name=""support_label_placeholder""))"
2.7.1,################################################################### DEBUG
2.7.1,Generate dictionary elements for support
2.7.1,Get graph information for test
2.7.1,Generate dictionary elements for test
2.7.1,Perform the optimization
2.7.1,Create different support sets
2.7.1,Get batch to try it out on
2.7.1,"Train on support set, batch pair"
2.7.1,Get featurization for test
2.7.1,"Shape (n_test, n_feat)"
2.7.1,Get featurization for support
2.7.1,"Shape (n_support, n_feat)"
2.7.1,Computes the inner part c() of the kernel
2.7.1,(the inset equation in section 2.1.1 of Matching networks paper).
2.7.1,Normalize
2.7.1,TODO(rbharath): euclidean kernel is broken!
2.7.1,elif self.similarity == 'euclidean':
2.7.1,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.7.1,"Note that gram matrix g has shape (n_test, n_support)"
2.7.1,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.7.1,https://arxiv.org/pdf/1606.04080v1.pdf
2.7.1,"Computes softmax across axis 1, (so sums distances to support set for"
2.7.1,each test entry) to get attention vector
2.7.1,"Shape (n_test, n_support)"
2.7.1,Weighted sum of support labels
2.7.1,"Shape (n_support, 1)"
2.7.1,pred is yhat in eqn (1) of Matching Networks.
2.7.1,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.7.1,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.7.1,"Shape (n_test,)"
2.7.1,Convert to logit space using inverse sigmoid (logit) function
2.7.1,logit function: log(pred) - log(1-pred)
2.7.1,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.7.1,in Cross Entropy calculation.
2.7.1,"Shape (n_test,)"
2.7.1,Get scores
2.7.1,Remove padded elements
2.7.1,Get scores
2.7.1,pred corresponds to prob(example == 1)
2.7.1,Remove padded elements
2.7.1,Get batches
2.7.1,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.7.1,multitask case with missing data...
2.7.1,Join information for all tasks.
2.7.1,TODO(rbharath): Find a way to get rid of this import?
2.7.1,Extract model info
2.7.1,Get graph topology for x
2.7.1,Building outputs
2.7.1,Set epsilon
2.7.1,Initialize
2.7.1,"Path to save checkpoint files, which matches the"
2.7.1,replicated supervisor's default path.
2.7.1,Create target inputs
2.7.1,Get train function
2.7.1,TODO(rbharath): I believe this is total amount of data
2.7.1,Get graph information
2.7.1,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.7.1,the number of labeled data points in target_i. This is to normalize each task
2.7.1,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.7.1,Get other optimizer information
2.7.1,TODO(rbharath): Figure out how to handle phase appropriately
2.7.1,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.7.1,"tensors of shape (batch_size,)"
2.7.1,It's ok to divide by just the batch_size rather than the number of nonzero
2.7.1,examples (effect averages out)
2.7.1,Perform the optimization
2.7.1,TODO(rbharath): Disabling saving for now to try to debug.
2.7.1,run eval data through the model
2.7.1,"Shape (n_samples, n_tasks)"
2.7.1,Create target inputs
2.7.1,TODO(rbharath): Find a way to get rid of this import?
2.7.1,Obtain appropriate loss function
2.7.1,Extract model info
2.7.1,Get graph topology for x
2.7.1,Raw logit outputs
2.7.1,Set epsilon
2.7.1,Initialize
2.7.1,"Path to save checkpoint files, which matches the"
2.7.1,replicated supervisor's default path.
2.7.1,Create target inputs
2.7.1,############################################################### DEBUG
2.7.1,"print(""multitask classifier"")"
2.7.1,"print(""feat"")"
2.7.1,print(feat)
2.7.1,############################################################### DEBUG
2.7.1,Get train function
2.7.1,TODO(rbharath): I believe this is total amount of data
2.7.1,Get graph information
2.7.1,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.7.1,the number of labeled data points in target_i. This is to normalize each task
2.7.1,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.7.1,Get other optimizer information
2.7.1,TODO(rbharath): Figure out how to handle phase appropriately
2.7.1,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.7.1,"tensors of shape (batch_size,)"
2.7.1,Convert the labels into one-hot vector encodings.
2.7.1,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.7.1,un-softmaxed logits rather than softmax outputs.
2.7.1,It's ok to divide by just the batch_size rather than the number of nonzero
2.7.1,examples (effect averages out)
2.7.1,Perform the optimization
2.7.1,TODO(rbharath): Disabling saving for now to try to debug.
2.7.1,run eval data through the model
2.7.1,"Shape (n_samples, n_tasks)"
2.7.1,run eval data through the model
2.7.1,self.n_atoms = n_atoms
2.7.1,Define the list of tensors to be used as topology
2.7.1,Merge mol conv objects
2.7.1,Generate dicts
2.7.1,Define the list of tensors to be used as topology
2.7.1,Extract atom numbers
2.7.1,Generate dicts
2.7.1,molecule * atom(graph) => step => features
2.7.1,molecule * atom(graph) => step
2.7.1,molecule * atom(graph) => step
2.7.1,Define the list of tensors to be used as topology
2.7.1,calculation orders for a batch of molecules
2.7.1,padding atom features vector of each molecule with 0
2.7.1,self.n_atoms = n_atoms
2.7.1,Define the list of tensors to be used as topology
2.7.1,Extract atom numbers
2.7.1,Generate dicts
2.7.1,self.n_atoms = n_atoms
2.7.1,Define the list of tensors to be used as topology
2.7.1,Extract atom numbers
2.7.1,number of atoms in each molecule
2.7.1,index of pair features
2.7.1,number of pairs for each atom
2.7.1,atom features
2.7.1,pair features
2.7.1,Generate dicts
2.7.1,Load Tox21 dataset
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.7.1,Fit trained model
2.7.1,Fit models
2.7.1,Batch size of models
2.7.1,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.7.1,Fit trained model
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,number positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply an attention lstm layer
2.7.1,Number of folds for split
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,number positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply an attention lstm layer
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,number positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply an attention lstm layer
2.7.1,Number of folds for split
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Number of folds for split
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply a residual lstm layer
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply a residual lstm layer
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply a residual lstm layer
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply a residual lstm layer
2.7.1,Number of folds for split
2.7.1,Depth of attention module
2.7.1,number positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,Apply an attention lstm layer
2.7.1,Number of folds for split
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Number of features on conv-mols
2.7.1,Define metric
2.7.1,Train support model on train
2.7.1,Add layers
2.7.1,# Gather Projection
2.7.1,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.7.1,There should be 8 layers in graph_model
2.7.1,assert len(graph_model.layers) == 6
2.7.1,Add layers
2.7.1,Need to add batch-norm separately to test/support due to differing
2.7.1,shapes.
2.7.1,Apply an attention lstm layer
2.7.1,Gather Projection
2.7.1,Add layers
2.7.1,Need to add batch-norm separately to test/support due to differing
2.7.1,shapes.
2.7.1,Apply an attention lstm layer
2.7.1,Gather Projection
2.7.1,Degrees from 1 to max_deg inclusive
2.7.1,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.7.1,"Should have shape (?, deg)"
2.7.1,"Shape of atom_features should be (?, n_feat)"
2.7.1,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.7.1,-*- coding: utf-8 -*-
2.7.1,Save hyperparameters
2.7.1,-*- coding: utf-8 -*-
2.7.1,Save hyperparameters
2.7.1,setup optimizer
2.7.1,setup optimizer
2.7.1,"print(""tasK: %d"" %task)"
2.7.1,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.7.1,"print(""scores"")"
2.7.1,print(scores.size())
2.7.1,"print(""task_label"")"
2.7.1,print(task_label.size())
2.7.1,"task_loss =  self.criterion(scores, task_label)"
2.7.1,"print(""task_loss"")"
2.7.1,print(task_loss.size())
2.7.1,-*- coding: utf-8 -*-
2.7.1,Save hyperparameters
2.7.1,weight decay
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,Turns out there are valid cases where we don't want pad-batches
2.7.1,on by default.
2.7.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.1,Run training op.
2.7.1,############################################################# TIMING
2.7.1,The iterbatches does padding with zero-weight examples on the last batch.
2.7.1,Remove padded examples.
2.7.1,Special case to handle singletasks.
2.7.1,The iterbatches does padding with zero-weight examples on the last batch.
2.7.1,Remove padded examples.
2.7.1,References
2.7.1,Arguments
2.7.1,Aliases.
2.7.1,Aliases.
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,TODO(rbharath): This class does not yet have a
2.7.1,"TensorGraph equivalent, but one may not be required."
2.7.1,"Commented out for now, remove if OK."
2.7.1,class AlternateWeaveLayer(WeaveLayer):
2.7.1,""""""" Alternate implementation of weave module"
2.7.1,"same variables, different graph structures"
2.7.1,""""""""
2.7.1,
2.7.1,"def call(self, x, mask=None):"
2.7.1,"""""""Execute this layer on input tensors."
2.7.1,
2.7.1,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.7.1,
2.7.1,Parameters
2.7.1,----------
2.7.1,x: list
2.7.1,list of Tensors of form described above.
2.7.1,"mask: bool, optional"
2.7.1,Ignored. Present only to shadow superclass call() method.
2.7.1,
2.7.1,Returns
2.7.1,-------
2.7.1,A: Tensor
2.7.1,Tensor of atom_features
2.7.1,P: Tensor
2.7.1,Tensor of pair_features
2.7.1,""""""""
2.7.1,# Add trainable weights
2.7.1,self.build()
2.7.1,
2.7.1,atom_features = x[0]
2.7.1,pair_features = x[1]
2.7.1,
2.7.1,pair_split = x[2]
2.7.1,atom_to_pair = x[4]
2.7.1,
2.7.1,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.7.1,AA = self.activation(AA)
2.7.1,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.7.1,PA = self.activation(PA)
2.7.1,"PA = tf.segment_sum(PA, pair_split)"
2.7.1,
2.7.1,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.7.1,A = self.activation(A)
2.7.1,
2.7.1,if self.update_pair:
2.7.1,AP_ij = tf.matmul(
2.7.1,tf.reshape(
2.7.1,"tf.gather(atom_features, atom_to_pair),"
2.7.1,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.7.1,AP_ij = self.activation(AP_ij)
2.7.1,AP_ji = tf.matmul(
2.7.1,tf.reshape(
2.7.1,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.7.1,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.7.1,AP_ji = self.activation(AP_ji)
2.7.1,
2.7.1,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.7.1,PP = self.activation(PP)
2.7.1,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.7.1,P = self.activation(P)
2.7.1,else:
2.7.1,P = pair_features
2.7.1,
2.7.1,"return A, P"
2.7.1,TODO(rbharath): This class does not yet have a
2.7.1,"TensorGraph equivalent, but one may not be required."
2.7.1,"Commented out for now, remove if OK."
2.7.1,class WeaveConcat(Layer):
2.7.1,""""""""" Concat a batch of molecules into a batch of atoms"
2.7.1,""""""""
2.7.1,
2.7.1,"def __init__(self,"
2.7.1,"batch_size,"
2.7.1,"n_atom_input_feat=50,"
2.7.1,"n_output=128,"
2.7.1,"init='glorot_uniform',"
2.7.1,"activation='tanh',"
2.7.1,**kwargs):
2.7.1,""""""""
2.7.1,Parameters
2.7.1,----------
2.7.1,batch_size: int
2.7.1,number of molecules in a batch
2.7.1,"n_atom_input_feat: int, optional"
2.7.1,Number of features for each atom in input.
2.7.1,"n_output: int, optional"
2.7.1,Number of output features for each atom(concatenated)
2.7.1,"init: str, optional"
2.7.1,Weight initialization for filters.
2.7.1,"activation: str, optional"
2.7.1,Activation function applied
2.7.1,
2.7.1,""""""""
2.7.1,self.batch_size = batch_size
2.7.1,self.n_atom_input_feat = n_atom_input_feat
2.7.1,self.n_output = n_output
2.7.1,self.init = initializations.get(init)  # Set weight initialization
2.7.1,self.activation = activations.get(activation)  # Get activations
2.7.1,"super(WeaveConcat, self).__init__(**kwargs)"
2.7.1,
2.7.1,def build(self):
2.7.1,"""""""""Construct internal trainable weights."
2.7.1,""""""""
2.7.1,
2.7.1,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.7.1,self.b = model_ops.zeros(shape=[
2.7.1,"self.n_output,"
2.7.1,])
2.7.1,
2.7.1,self.trainable_weights = self.W + self.b
2.7.1,
2.7.1,"def call(self, x, mask=None):"
2.7.1,"""""""Execute this layer on input tensors."
2.7.1,
2.7.1,"x = [atom_features, atom_mask]"
2.7.1,
2.7.1,Parameters
2.7.1,----------
2.7.1,x: list
2.7.1,Tensors as listed above
2.7.1,"mask: bool, optional"
2.7.1,Ignored. Present only to shadow superclass call() method.
2.7.1,
2.7.1,Returns
2.7.1,-------
2.7.1,outputs: Tensor
2.7.1,Tensor of concatenated atom features
2.7.1,""""""""
2.7.1,self.build()
2.7.1,atom_features = x[0]
2.7.1,atom_masks = x[1]
2.7.1,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.7.1,A_mask = tf.split(
2.7.1,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.7.1,outputs = tf.concat(
2.7.1,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.7.1,"outputs = tf.matmul(outputs, self.W) + self.b"
2.7.1,outputs = self.activation(outputs)
2.7.1,return outputs
2.7.1,TODO(rbharath): This class does not yet have a
2.7.1,"TensorGraph equivalent, but one may not be required."
2.7.1,"Commented out for now, remove if OK."
2.7.1,class AlternateWeaveGather(WeaveGather):
2.7.1,"""""""Alternate implementation of weave gather layer"
2.7.1,corresponding to AlternateWeaveLayer
2.7.1,""""""""
2.7.1,
2.7.1,"def call(self, x, mask=None):"
2.7.1,"""""""Execute this layer on input tensors."
2.7.1,
2.7.1,"x = [atom_features, atom_split]"
2.7.1,
2.7.1,Parameters
2.7.1,----------
2.7.1,x: list
2.7.1,Tensors as listed above
2.7.1,"mask: bool, optional"
2.7.1,Ignored. Present only to shadow superclass call() method.
2.7.1,
2.7.1,Returns
2.7.1,-------
2.7.1,outputs: Tensor
2.7.1,Tensor of molecular features
2.7.1,""""""""
2.7.1,# Add trainable weights
2.7.1,self.build()
2.7.1,outputs = x[0]
2.7.1,atom_split = x[1]
2.7.1,
2.7.1,if self.gaussian_expand:
2.7.1,outputs = self.gaussian_histogram(outputs)
2.7.1,
2.7.1,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.7.1,
2.7.1,if self.gaussian_expand:
2.7.1,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.7.1,output_molecules = self.activation(output_molecules)
2.7.1,return output_molecules
2.7.1,Each directory holds a range of assay results
2.7.1,Just write NA
2.7.1,"Now, write out the results csv, going line by line through all molecule results"
2.7.1,printing the mol_id
2.7.1,printing the SMILES
2.7.1,Now gzip it
2.7.1,Now remove the intermediate csv
2.7.1,First download all SDF files. We need these to get smiles
2.7.1,Next download all Bioassays
2.7.1,RDKit consistently hangs when trying to read this file
2.7.1,TODO (LESWING) Lazy Load
2.7.1,TODO (LESWING) Lazy Load
2.7.1,from simdna import simulations
2.7.1,define layer out functions
2.7.1,get layer outputs for a positive simulation example
2.7.1,plot layer outputs
2.7.1,highlight motif sites
2.7.1,get a positive and a negative example from the simulation data
2.7.1,"get motif scores, ISM scores, and DeepLIFT scores"
2.7.1,get motif site locations
2.7.1,organize legends
2.7.1,plot scores and highlight motif site locations
2.7.1,initialize fwd and reverse scores to -infinity
2.7.1,"cross-correlate separately for each base,"
2.7.1,for both the PSSM and its reverse complement
2.7.1,sum over the bases
2.7.1,take max of fwd and reverse scores at each position
2.7.1,return 1D view of sequence characters
2.7.1,class SequenceDNN(Model):
2.7.1,""""""""
2.7.1,Sequence DNN models.
2.7.1,
2.7.1,Parameters
2.7.1,----------
2.7.1,"seq_length : int, optional"
2.7.1,length of input sequence.
2.7.1,"keras_model : instance of keras.models.Sequential, optional"
2.7.1,seq_length or keras_model must be specified.
2.7.1,"num_tasks : int, optional"
2.7.1,number of tasks. Default: 1.
2.7.1,num_filters : list[int] | tuple[int]
2.7.1,"number of convolutional filters in each layer. Default: (15,)."
2.7.1,conv_width : list[int] | tuple[int]
2.7.1,"width of each layer's convolutional filters. Default: (15,)."
2.7.1,pool_width : int
2.7.1,width of max pooling after the last layer. Default: 35.
2.7.1,L1 : float
2.7.1,strength of L1 penalty.
2.7.1,dropout : float
2.7.1,dropout probability in every convolutional layer. Default: 0.
2.7.1,verbose: int
2.7.1,"Verbosity level during training. Valida values: 0, 1, 2."
2.7.1,
2.7.1,Returns
2.7.1,-------
2.7.1,Compiled DNN model.
2.7.1,""""""""
2.7.1,
2.7.1,"def __init__(self,"
2.7.1,"seq_length=None,"
2.7.1,"keras_model=None,"
2.7.1,"use_RNN=False,"
2.7.1,"num_tasks=1,"
2.7.1,"num_filters=(15, 15, 15),"
2.7.1,"conv_width=(15, 15, 15),"
2.7.1,"pool_width=35,"
2.7.1,"GRU_size=35,"
2.7.1,"TDD_size=15,"
2.7.1,"L1=0,"
2.7.1,"dropout=0.0,"
2.7.1,"num_epochs=100,"
2.7.1,verbose=1):
2.7.1,self.num_tasks = num_tasks
2.7.1,self.num_epochs = num_epochs
2.7.1,self.verbose = verbose
2.7.1,self.train_metrics = []
2.7.1,self.valid_metrics = []
2.7.1,if keras_model is not None and seq_length is None:
2.7.1,self.model = keras_model
2.7.1,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.7.1,elif seq_length is not None and keras_model is None:
2.7.1,self.model = Sequential()
2.7.1,assert len(num_filters) == len(conv_width)
2.7.1,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.7.1,conv_height = 4 if i == 0 else 1
2.7.1,self.model.add(
2.7.1,Convolution2D(
2.7.1,"nb_filter=nb_filter,"
2.7.1,"nb_row=conv_height,"
2.7.1,"nb_col=nb_col,"
2.7.1,"activation='linear',"
2.7.1,"init='he_normal',"
2.7.1,"input_shape=(1, 4, seq_length),"
2.7.1,"W_regularizer=l1(L1),"
2.7.1,b_regularizer=l1(L1)))
2.7.1,self.model.add(Activation('relu'))
2.7.1,self.model.add(Dropout(dropout))
2.7.1,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.7.1,if use_RNN:
2.7.1,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.7.1,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.7.1,"self.model.add(Permute((2, 1)))"
2.7.1,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.7.1,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.7.1,self.model.add(Flatten())
2.7.1,self.model.add(Dense(output_dim=self.num_tasks))
2.7.1,self.model.add(Activation('sigmoid'))
2.7.1,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.7.1,else:
2.7.1,raise ValueError(
2.7.1,"""Exactly one of seq_length or keras_model must be specified!"")"
2.7.1,
2.7.1,"def train(self,"
2.7.1,"X,"
2.7.1,"y,"
2.7.1,"validation_data,"
2.7.1,"early_stopping_metric='Loss',"
2.7.1,"early_stopping_patience=5,"
2.7.1,save_best_model_to_prefix=None):
2.7.1,if y.dtype != bool:
2.7.1,"assert set(np.unique(y)) == {0, 1}"
2.7.1,y = y.astype(bool)
2.7.1,multitask = y.shape[1] > 1
2.7.1,if not multitask:
2.7.1,num_positives = y.sum()
2.7.1,num_sequences = len(y)
2.7.1,num_negatives = num_sequences - num_positives
2.7.1,if self.verbose >= 1:
2.7.1,print('Training model (* indicates new best result)...')
2.7.1,"X_valid, y_valid = validation_data"
2.7.1,early_stopping_wait = 0
2.7.1,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.7.1,"for epoch in range(1, self.num_epochs + 1):"
2.7.1,self.model.fit(
2.7.1,"X,"
2.7.1,"y,"
2.7.1,"batch_size=128,"
2.7.1,"nb_epoch=1,"
2.7.1,class_weight={
2.7.1,"True: num_sequences / num_positives,"
2.7.1,False: num_sequences / num_negatives
2.7.1,"} if not multitask else None,"
2.7.1,verbose=self.verbose >= 2)
2.7.1,"epoch_train_metrics = self.test(X, y)"
2.7.1,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.7.1,self.train_metrics.append(epoch_train_metrics)
2.7.1,self.valid_metrics.append(epoch_valid_metrics)
2.7.1,if self.verbose >= 1:
2.7.1,print('Epoch {}:'.format(epoch))
2.7.1,print('Train {}'.format(epoch_train_metrics))
2.7.1,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.7.1,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.7.1,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.7.1,if self.verbose >= 1:
2.7.1,print(' *')
2.7.1,best_metric = current_metric
2.7.1,best_epoch = epoch
2.7.1,early_stopping_wait = 0
2.7.1,if save_best_model_to_prefix is not None:
2.7.1,self.save(save_best_model_to_prefix)
2.7.1,else:
2.7.1,if self.verbose >= 1:
2.7.1,print()
2.7.1,if early_stopping_wait >= early_stopping_patience:
2.7.1,break
2.7.1,early_stopping_wait += 1
2.7.1,if self.verbose >= 1:
2.7.1,print('Finished training after {} epochs.'.format(epoch))
2.7.1,if save_best_model_to_prefix is not None:
2.7.1,"print(""The best model's architecture and weights (from epoch {0}) """
2.7.1,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.7.1,"best_epoch, save_best_model_to_prefix))"
2.7.1,
2.7.1,"def predict(self, X):"
2.7.1,"return self.model.predict(X, batch_size=128, verbose=False)"
2.7.1,
2.7.1,def get_sequence_filters(self):
2.7.1,""""""""
2.7.1,Returns 3D array of 2D sequence filters.
2.7.1,""""""""
2.7.1,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.7.1,
2.7.1,"def deeplift(self, X, batch_size=200):"
2.7.1,""""""""
2.7.1,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.7.1,""""""""
2.7.1,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.7.1,from deeplift.conversion import keras_conversion as kc
2.7.1,
2.7.1,# convert to deeplift model and get scoring function
2.7.1,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.7.1,score_func = deeplift_model.get_target_contribs_func(
2.7.1,find_scores_layer_idx=0)
2.7.1,# use a 40% GC reference
2.7.1,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.7.1,# get deeplift scores
2.7.1,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.7.1,for i in range(self.num_tasks):
2.7.1,deeplift_scores[i] = score_func(
2.7.1,"task_idx=i,"
2.7.1,"input_data_list=[X],"
2.7.1,"batch_size=batch_size,"
2.7.1,"progress_update=None,"
2.7.1,input_references_list=input_references)
2.7.1,return deeplift_scores
2.7.1,
2.7.1,"def in_silico_mutagenesis(self, X):"
2.7.1,""""""""
2.7.1,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.7.1,""""""""
2.7.1,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.7.1,wild_type_predictions = self.predict(X)
2.7.1,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.7.1,np.newaxis]
2.7.1,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.7.1,"zip(X, wild_type_predictions)):"
2.7.1,mutated_sequences = np.repeat(
2.7.1,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.7.1,# remove wild-type
2.7.1,arange = np.arange(len(mutated_sequences))
2.7.1,horizontal_cycle = np.tile(
2.7.1,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.7.1,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.7.1,# add mutant
2.7.1,vertical_repeat = np.repeat(
2.7.1,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.7.1,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.7.1,# make mutant predictions
2.7.1,mutated_predictions = self.predict(mutated_sequences)
2.7.1,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.7.1,"(self.num_tasks,))"
2.7.1,mutagenesis_scores[
2.7.1,sequence_index] = wild_type_prediction - mutated_predictions
2.7.1,"return np.rollaxis(mutagenesis_scores, -1)"
2.7.1,
2.7.1,@staticmethod
2.7.1,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.7.1,from dragonn.plot import plot_bases_on_ax
2.7.1,scores = score_func(X).squeeze(
2.7.1,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.7.1,try:
2.7.1,os.makedirs(output_directory)
2.7.1,except OSError:
2.7.1,pass
2.7.1,num_tasks = len(scores)
2.7.1,"for task_index, task_scores in enumerate(scores):"
2.7.1,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.7.1,# sequence_scores is num_bases x sequence_length
2.7.1,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.7.1,plt.clf()
2.7.1,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.7.1,top_axis.plot(
2.7.1,"range(1,"
2.7.1,"len(basewise_max_sequence_scores) + 1),"
2.7.1,basewise_max_sequence_scores)
2.7.1,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.7.1,peak_position = basewise_max_sequence_scores.argmax()
2.7.1,top_axis.axvspan(
2.7.1,"peak_position - peak_width,"
2.7.1,"peak_position + peak_width,"
2.7.1,"color='grey',"
2.7.1,alpha=0.1)
2.7.1,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.7.1,peak_position + peak_width].T
2.7.1,# Set non-max letter_heights to zero
2.7.1,letter_heights = np.zeros_like(peak_sequence_scores)
2.7.1,"letter_heights[np.arange(len(letter_heights)),"
2.7.1,peak_sequence_scores.argmax(axis=1)] = \
2.7.1,basewise_max_sequence_scores[peak_position - peak_width :
2.7.1,peak_position + peak_width]
2.7.1,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.7.1,bottom_axis.set_xticklabels(
2.7.1,tuple(
2.7.1,"map(str,"
2.7.1,"np.arange(peak_position - peak_width,"
2.7.1,peak_position + peak_width + 1))))
2.7.1,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.7.1,plt.xlabel('Position')
2.7.1,plt.ylabel('Score')
2.7.1,plt.savefig(
2.7.1,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.7.1,"sequence_index, '_task_{}'.format(task_index)"
2.7.1,if num_tasks > 1 else '')))
2.7.1,plt.close()
2.7.1,
2.7.1,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.7.1,self._plot_scores(
2.7.1,"X,"
2.7.1,"output_directory,"
2.7.1,"peak_width,"
2.7.1,"score_func=self.deeplift,"
2.7.1,score_name='DeepLift')
2.7.1,
2.7.1,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.7.1,self._plot_scores(
2.7.1,"X,"
2.7.1,"output_directory,"
2.7.1,"peak_width,"
2.7.1,"score_func=self.in_silico_mutagenesis,"
2.7.1,score_name='ISM')
2.7.1,
2.7.1,"def plot_architecture(self, output_file):"
2.7.1,from dragonn.visualize_util import plot as plot_keras_model
2.7.1,"plot_keras_model(self.model, output_file, show_shape=True)"
2.7.1,
2.7.1,"def save(self, save_best_model_to_prefix):"
2.7.1,arch_fname = save_best_model_to_prefix + '.arch.json'
2.7.1,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.7.1,"open(arch_fname, 'w').write(self.model.to_json())"
2.7.1,"self.model.save_weights(weights_fname, overwrite=True)"
2.7.1,
2.7.1,@staticmethod
2.7.1,"def load(arch_fname, weights_fname=None):"
2.7.1,model_json_string = open(arch_fname).read()
2.7.1,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.7.1,if weights_fname is not None:
2.7.1,sequence_dnn.model.load_weights(weights_fname)
2.7.1,return sequence_dnn
2.7.1,create temporary fasta files
2.7.1,run command
2.7.1,remove fasta files
2.7.1,write test fasta file
2.7.1,test gkmsvm
2.7.1,get classification results
2.7.1,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.7.1,"Using canonical smiles for glycine, as in original research paper"
2.7.1,Atom features with padding
2.7.1,A_tilda_k computation
2.7.1,Final feed_dict setup
2.7.1,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.7.1,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.7.1,self.num_node_features)
2.7.1,Fit models
2.7.1,Args
2.7.1,2017 DeepCrystal Technologies - Patrick Hop
2.7.1,
2.7.1,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.7.1,
2.7.1,MIT License - have fun!!
2.7.1,===========================================================
2.7.1,x = F.selu( fc(x) )
2.7.1,x = F.selu( fc(x) )
2.7.1,2017 DeepCrystal Technologies - Patrick Hop
2.7.1,
2.7.1,Data loading a splitting file
2.7.1,
2.7.1,MIT License - have fun!!
2.7.1,===========================================================
2.7.1,Args
2.7.1,TODO (VIGS25): Account for the reload option
2.7.1,Downloading train files
2.7.1,Parsing training data
2.7.1,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.7.1,Test Files loading
2.7.1,One Hot Featurization
2.7.1,Consistency check
2.7.1,Handle output layer
2.7.1,Iterate over all previous tasks.
2.7.1,prev_layers is a list with elements of size
2.7.1,"(batch_size, layer_sizes[i-1])"
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,Save an initial checkpoint.
2.7.1,Turns out there are valid cases where we don't want pad-batches
2.7.1,on by default.
2.7.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.1,Run training op.
2.7.1,Always save a final checkpoint when complete.
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,Note that we divide by the batch size and not the number of
2.7.1,"non-zero weight examples in the batch.  Also, instead of using"
2.7.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.1,calculate with div/sum so it stays on the GPU.
2.7.1,aggregated costs
2.7.1,weight decay
2.7.1,Dummy placeholders
2.7.1,Dummy placeholders
2.7.1,run eval data through the model
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,Handle edge case when batch-size is 1.
2.7.1,Prune away any padding that was added
2.7.1,allow_soft_placement=True allows ops without a GPU implementation
2.7.1,to run on the CPU instead.
2.7.1,!/usr/bin/python
2.7.1,
2.7.1,Copyright 2015 Google Inc.
2.7.1,
2.7.1,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.7.1,you may not use this file except in compliance with the License.
2.7.1,You may obtain a copy of the License at
2.7.1,
2.7.1,http://www.apache.org/licenses/LICENSE-2.0
2.7.1,
2.7.1,"Unless required by applicable law or agreed to in writing, software"
2.7.1,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.7.1,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.7.1,See the License for the specific language governing permissions and
2.7.1,limitations under the License.
2.7.1,parse CheckpointState proto
2.7.1,parse path to actual checkpoint
2.7.1,the provided mask has to be the same shape as features
2.7.1,test k = 1..4
2.7.1,central moments
2.7.1,standardized moments
2.7.1,central across one axis
2.7.1,standardized across one axis
2.7.1,Fit just on task zero
2.7.1,Notice that we keep the session open
2.7.1,Fit on task one
2.7.1,The predictions for task zero should not change after training
2.7.1,on task one.
2.7.1,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.7.1,!/usr/bin/python
2.7.1,
2.7.1,Copyright 2015 Google Inc.
2.7.1,
2.7.1,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.7.1,you may not use this file except in compliance with the License.
2.7.1,You may obtain a copy of the License at
2.7.1,
2.7.1,http://www.apache.org/licenses/LICENSE-2.0
2.7.1,
2.7.1,"Unless required by applicable law or agreed to in writing, software"
2.7.1,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.7.1,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.7.1,See the License for the specific language governing permissions and
2.7.1,limitations under the License.
2.7.1,get the divisor
2.7.1,compute the requested central moment
2.7.1,"note that mean is a raw moment, not a central moment"
2.7.1,TODO(user): median is not implemented yet in TensorFlow
2.7.1,Add the input features.
2.7.1,"layer has shape [None, layer_sizes[i]]"
2.7.1,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.7.1,TODO(rbharath): Might want to make it feasible to have multiple
2.7.1,bypass layers.
2.7.1,Construct task bypass layer
2.7.1,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.7.1,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.7.1,"layer has shape [None, layer_sizes[i]]"
2.7.1,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.7.1,TODO(rbharath): Might want to make it feasible to have multiple
2.7.1,bypass layers.
2.7.1,Construct task bypass layer
2.7.1,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.7.1,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.7.1,Consistency check
2.7.1,Lazily created by _get_shared_session().
2.7.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.7.1,when subclass-overridden methods use the same scopes.
2.7.1,Setup graph
2.7.1,Create placeholders
2.7.1,Handle output layer
2.7.1,Iterate over all previous tasks.
2.7.1,prev_layers is a list with elements of size
2.7.1,"(batch_size, layer_sizes[i-1])"
2.7.1,Note that we divide by the batch size and not the number of
2.7.1,"non-zero weight examples in the batch.  Also, instead of using"
2.7.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.1,calculate with div/sum so it stays on the GPU.
2.7.1,aggregated costs
2.7.1,weight decay
2.7.1,Dummy placeholders
2.7.1,Dummy placeholders
2.7.1,run eval data through the model
2.7.1,"Shape (n_tasks, n__samples)"
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,Handle edge case when batch-size is 1.
2.7.1,with self._get_shared_session(train=True) as sess:
2.7.1,Save an initial checkpoint.
2.7.1,Always save a final checkpoint when complete.
2.7.1,Note that we divide by the batch size and not the number of
2.7.1,"non-zero weight examples in the batch.  Also, instead of using"
2.7.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.1,calculate with div/sum so it stays on the GPU.
2.7.1,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.7.1,Dummy placeholders
2.7.1,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.7.1,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.7.1,Dummy placeholders
2.7.1,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.7.1,allow_soft_placement=True allows ops without a GPU implementation
2.7.1,to run on the CPU instead.
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,Turns out there are valid cases where we don't want pad-batches
2.7.1,on by default.
2.7.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.1,if epoch%checkpoint_interval == checkpoint_interval-1:
2.7.1,"saver.save(sess, self._save_path, global_step=epoch)"
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,"(n_samples, n_classes)"
2.7.1,"(n_samples, n_tasks, n_classes)"
2.7.1,Save hyperparameters
2.7.1,Guard variable to make sure we don't Restore() this model
2.7.1,from a disk checkpoint more than once.
2.7.1,"Path to save checkpoint files, which matches the"
2.7.1,replicated supervisor's default path.
2.7.1,Lazily created by _get_shared_session().
2.7.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.7.1,when subclass-overridden methods use the same scopes.
2.7.1,Setup graph
2.7.1,Note that we divide by the batch size and not the number of
2.7.1,"non-zero weight examples in the batch.  Also, instead of using"
2.7.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.1,calculate with div/sum so it stays on the GPU.
2.7.1,aggregated costs
2.7.1,weight decay
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,Save an initial checkpoint.
2.7.1,Define the code that runs on a separate thread to feed data into the queue.
2.7.1,Main training loop.
2.7.1,Run training op.
2.7.1,We have reached the end of an epoch.
2.7.1,We have reached the end of the data.
2.7.1,Always save a final checkpoint when complete.
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,allow_soft_placement=True allows ops without a GPU implementation
2.7.1,to run on the CPU instead.
2.7.1,gpu memory growth option
2.7.1,gpu memory growth option
2.7.1,TODO(rbharath): Is setting train=False right here?
2.7.1,Discard any padded predictions
2.7.1,The iterbatches does padding with zero-weight examples on the last batch.
2.7.1,Remove padded examples.
2.7.1,Special case to handle singletasks.
2.7.1,The iterbatches does padding with zero-weight examples on the last batch.
2.7.1,Remove padded examples.
2.7.1,TODO(rbharath): Verify this can be safely removed.
2.7.1,"def evaluate(self, dataset, metrics, transformers=[]):"
2.7.1,""""""""
2.7.1,Evaluates the performance of this model on specified dataset.
2.7.1,
2.7.1,Parameters
2.7.1,----------
2.7.1,dataset: dc.data.Dataset
2.7.1,Dataset object.
2.7.1,metric: deepchem.metrics.Metric
2.7.1,Evaluation metric
2.7.1,transformers: list
2.7.1,List of deepchem.transformers.Transformer
2.7.1,Returns
2.7.1,-------
2.7.1,dict
2.7.1,Maps tasks to scores under metric.
2.7.1,""""""""
2.7.1,"evaluator = Evaluator(self, dataset, transformers)"
2.7.1,scores = evaluator.compute_model_performance(metrics)
2.7.1,return scores
2.7.1,checkpoints look like model_dir/model.ckpt-N
2.7.1,"self._save_path is ""model_dir/model.ckpt"""
2.7.1,run eval data through the model
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,run eval data through the model
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,Note that softmax is already applied in construct_grpah
2.7.1,run eval data through the model
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,Handle edge case when batch-size is 1.
2.7.1,Prune away any padding that was added
2.7.1,Handle case of 0-dimensional scalar output
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,inputs placeholder
2.7.1,data preprocessing and augmentation
2.7.1,first conv layer
2.7.1,downsample by max pooling
2.7.1,each module is a residual convolutional block
2.7.1,followed by a convolutional downsample layer
2.7.1,max pooling over the final outcome
2.7.1,fully connected layers
2.7.1,dropout for dense layers
2.7.1,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.7.1,weight decay regularizer
2.7.1,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.7.1,sample cut ratio from a clipped gaussian
2.7.1,train/valid differences
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,Define and build model
2.7.1,model.restore()
2.7.1,Set random seeds
2.7.1,Setup directories
2.7.1,Model constants
2.7.1,Load and transform datasets
2.7.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.1,Atomic convolution variables
2.7.1,at = atomic numbers (atom types)
2.7.1,"radial basis function parameters [cutoff, mean, width]"
2.7.1,Model hyperparameters
2.7.1,Initialize model
2.7.1,Fit model
2.7.1,Evaluate model
2.7.1,Set random seeds
2.7.1,Setup directories
2.7.1,Model constants
2.7.1,Load and transform datasets
2.7.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.1,Atomic convolution variables
2.7.1,at = atomic numbers (atom types)
2.7.1,"radial basis function parameters [cutoff, mean, width]"
2.7.1,Model hyperparameters
2.7.1,Initialize model
2.7.1,Fit model
2.7.1,Evaluate model
2.7.1,Set random seeds
2.7.1,Setup directories
2.7.1,Model constants
2.7.1,Load and transform datasets
2.7.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.1,Atomic convolution variables
2.7.1,at = atomic numbers (atom types)
2.7.1,"radial basis function parameters [cutoff, mean, width]"
2.7.1,Model hyperparameters
2.7.1,Initialize model
2.7.1,Fit model
2.7.1,Evaluate model
2.7.1,Set random seeds
2.7.1,Setup directories
2.7.1,Model constants
2.7.1,Load and transform datasets
2.7.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.1,Atomic convolution variables
2.7.1,at = atomic numbers (atom types)
2.7.1,"radial basis function parameters [cutoff, mean, width]"
2.7.1,Model hyperparameters
2.7.1,Initialize model
2.7.1,Fit model
2.7.1,Evaluate model
2.7.1,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.7.1,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.7.1,test_scores = test_evaluator.compute_model_performance(metric)
2.7.1,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.7.1,param.update(test_scores)
2.7.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.1,for transformer in transformers:
2.7.1,train_dataset = transformer.transform(train_dataset)
2.7.1,test_dataset = transformer.transform(test_dataset)
2.7.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.1,for transformer in transformers:
2.7.1,train_dataset = transformer.transform(train_dataset)
2.7.1,test_dataset = transformer.transform(test_dataset)
2.7.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.1,for transformer in transformers:
2.7.1,train_dataset = transformer.transform(train_dataset)
2.7.1,test_dataset = transformer.transform(test_dataset)
2.7.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.1,for transformer in transformers:
2.7.1,train_dataset = transformer.transform(train_dataset)
2.7.1,test_dataset = transformer.transform(test_dataset)
2.7.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.1,Create some directories for analysis
2.7.1,The base_dir holds the results of all analysis
2.7.1,Make directories to store the raw and featurized datasets.
2.7.1,Load PDBBind dataset
2.7.1,Define featurizers
2.7.1,Currently featurizes with shard_size=1
2.7.1,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.7.1,This could be done with openbabel in python
2.7.1,Compute cells for this molecule. O(constant)
2.7.1,min == max if molecule is planar in some direction
2.7.1,we should still create a bin
2.7.1,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.7.1,Note neighbors contains self!
2.7.1,Associate each atom with cell it belongs to. O(N)
2.7.1,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.1,"conditions, so does wrapround. O(constant)"
2.7.1,"For each atom, loop through all atoms in its cell and neighboring cells."
2.7.1,Accept as neighbors only those within threshold. This computation should be
2.7.1,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.7.1,Sort neighbors by distance
2.7.1,Pick up to max_num_neighbors
2.7.1,Type of data created by this featurizer
2.7.1,assumes that every array is of the same dimension
2.7.1,rem_dataset is remaining portion of dataset
2.7.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.7.1,to k-1.
2.7.1,returns list of per column sum of non zero elements
2.7.1,Compute number of actives needed per task.
2.7.1,loop through each column and obtain index required to splice out for
2.7.1,required fraction of hits
2.7.1,Find the first index where the cumulative number of actives equals
2.7.1,the actives_count
2.7.1,Note that np.where tells us last index required to exceed
2.7.1,"actives_count, so we actually want the following location"
2.7.1,TODO(rbharath): Refactor this split method to match API of other splits (or
2.7.1,potentially refactor those to match this.
2.7.1,Handle edge case where frac_split is 1
2.7.1,Create weight matrices fpor two haves.
2.7.1,copy over up to required index for weight first_split
2.7.1,check out if any rows in either w_1 or w_2 are just zeros
2.7.1,"Obtain original x, y, and w arrays and shuffle"
2.7.1,calculate percent split for valid (out of test and valid)
2.7.1,"split test data into valid and test, treating sub test set also as sparse"
2.7.1,rem_dataset is remaining portion of dataset
2.7.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.7.1,to k-1.
2.7.1,JSG Assert that split fractions can be written as proper fractions over 10.
2.7.1,This can be generalized in the future with some common demoninator determination.
2.7.1,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.7.1,Append remaining examples to train
2.7.1,Sort by increasing MW
2.7.1,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.7.1,for m_idx in cluster:
2.7.1,"continue until we find an active in all the tasks, otherwise we can't"
2.7.1,compute a meaningful AUC
2.7.1,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.7.1,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.7.1,Sort from largest to smallest scaffold sets
2.7.1,Sort from largest to smallest scaffold sets
2.7.1,"(n_samples, n_classes)"
2.7.1,"(n_samples, n_tasks, n_classes)"
2.7.1,Save hyperparameters
2.7.1,Guard variable to make sure we don't Restore() this model
2.7.1,from a disk checkpoint more than once.
2.7.1,"Path to save checkpoint files, which matches the"
2.7.1,replicated supervisor's default path.
2.7.1,Lazily created by _get_shared_session().
2.7.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.7.1,when subclass-overridden methods use the same scopes.
2.7.1,Setup graph
2.7.1,Note that we divide by the batch size and not the number of
2.7.1,"non-zero weight examples in the batch.  Also, instead of using"
2.7.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.1,calculate with div/sum so it stays on the GPU.
2.7.1,aggregated costs
2.7.1,weight decay
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,Save an initial checkpoint.
2.7.1,Turns out there are valid cases where we don't want pad-batches
2.7.1,on by default.
2.7.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.1,Run training op.
2.7.1,Always save a final checkpoint when complete.
2.7.1,############################################################# TIMING
2.7.1,############################################################# TIMING
2.7.1,allow_soft_placement=True allows ops without a GPU implementation
2.7.1,to run on the CPU instead.
2.7.1,TODO(rbharath): Is setting train=False right here?
2.7.1,Discard any padded predictions
2.7.1,The iterbatches does padding with zero-weight examples on the last batch.
2.7.1,Remove padded examples.
2.7.1,Special case to handle singletasks.
2.7.1,The iterbatches does padding with zero-weight examples on the last batch.
2.7.1,Remove padded examples.
2.7.1,TODO(rbharath): Verify this can be safely removed.
2.7.1,"def evaluate(self, dataset, metrics, transformers=[]):"
2.7.1,""""""""
2.7.1,Evaluates the performance of this model on specified dataset.
2.7.1,
2.7.1,Parameters
2.7.1,----------
2.7.1,dataset: dc.data.Dataset
2.7.1,Dataset object.
2.7.1,metric: deepchem.metrics.Metric
2.7.1,Evaluation metric
2.7.1,transformers: list
2.7.1,List of deepchem.transformers.Transformer
2.7.1,Returns
2.7.1,-------
2.7.1,dict
2.7.1,Maps tasks to scores under metric.
2.7.1,""""""""
2.7.1,"evaluator = Evaluator(self, dataset, transformers)"
2.7.1,scores = evaluator.compute_model_performance(metrics)
2.7.1,return scores
2.7.1,checkpoints look like logdir/model.ckpt-N
2.7.1,"self._save_path is ""logdir/model.ckpt"""
2.7.1,run eval data through the model
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,run eval data through the model
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,Note that softmax is already applied in construct_grpah
2.7.1,run eval data through the model
2.7.1,reshape to batch_size x n_tasks x ...
2.7.1,Handle edge case when batch-size is 1.
2.7.1,Prune away any padding that was added
2.7.1,Handle case of 0-dimensional scalar output
2.7.1,Dummy placeholders
2.7.1,Dummy placeholders
2.7.1,## AtomicNet fully-connected layer ops ###
2.7.1,## Atomicnet coordinate transform ops ###
2.7.1,## Atomicnet symmetry function kernel ops ###
2.7.1,## Atomicnet symmetry function ops ###
2.7.1,## Atomcnet symmetry function layer ops ###
2.7.1,We apply the radial pooling filter before atom type conv
2.7.1,to reduce computation
2.7.1,## Misc convenience ops ###
2.7.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.7.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.7.1,"game).  The average reward for any bet is slightly negative, so the best"
2.7.1,strategy is to walk away.
2.7.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.1,Optimize it.
2.7.1,"It should have learned that the expected value is very close to zero, and that the best"
2.7.1,action is to walk away.
2.7.1,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.7.1,get the same result.
2.7.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.1,Run the algorithm.
2.7.1,Save a file checkpoint.
2.7.1,Build the tree.
2.7.1,Compute the final probabilities and expected reward.
2.7.1,Mark this node as terminal
2.7.1,Expand this node.
2.7.1,Select the next action to perform.
2.7.1,Recursively build the tree.
2.7.1,Update statistics for this node.
2.7.1,Configuration file for the Sphinx documentation builder.
2.7.1,
2.7.1,This file only contains a selection of the most common options. For a full
2.7.1,list see the documentation:
2.7.1,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.7.1,-- Path setup --------------------------------------------------------------
2.7.1,"If extensions (or modules to document with autodoc) are in another directory,"
2.7.1,add these directories to sys.path here. If the directory is relative to the
2.7.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.7.1,
2.7.1,-- Project information -----------------------------------------------------
2.7.1,"The full version, including alpha/beta/rc tags"
2.7.1,-- General configuration ---------------------------------------------------
2.7.1,"Add any Sphinx extension module names here, as strings. They can be"
2.7.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.7.1,ones.
2.7.1,Options for autodoc directives
2.7.1,How to represents typehints
2.7.1,"Add any paths that contain templates here, relative to this directory."
2.7.1,The suffix of source filenames.
2.7.1,The master toctree document.
2.7.1,autosectionlabel setting
2.7.1,"List of patterns, relative to source directory, that match files and"
2.7.1,directories to ignore when looking for source files.
2.7.1,This pattern also affects html_static_path and html_extra_path.
2.7.1,"If true, the current module name will be prepended to all description"
2.7.1,unit titles (such as .. function::).
2.7.1,-- Options for HTML output -------------------------------------------------
2.7.1,The theme to use for HTML and HTML Help pages.  See the documentation for
2.7.1,a list of builtin themes.
2.7.1,"Add any paths that contain custom static files (such as style sheets) here,"
2.7.1,"relative to this directory. They are copied after the builtin static files,"
2.7.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.7.1,The name of an image file (relative to this directory) to place at the top
2.7.1,of the sidebar.
2.7.1,Customize the sphinx theme
2.7.1,-- Source code links ---------------------------------------------------
2.7.1,Resolve function for the linkcode extension.
2.7.1,"try to find the file and line number, based on code from numpy:"
2.7.1,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.7.1,lines in the label file have format
2.7.1,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.7.1,"print line[0], line[3]"
2.7.1,"If you push the tag, please remove `.dev`"
2.7.1,Record inputs.
2.7.1,Create the output directory if necessary.
2.7.1,Create the optimizers for meta-optimization and task optimization.
2.7.1,Create a Checkpoint for saving.
2.7.1,Main optimization loop.
2.7.1,Do checkpointing.
2.7.1,flake8: noqa
2.7.1,This is a MetaLearner that learns to generate sine functions with variable
2.7.1,amplitude and phase.
2.7.1,Optimize it.
2.7.1,Test it out on some new tasks and see how it works.
2.7.1,Initially the model should do a bad job of fitting the sine function.
2.7.1,After one step of optimization it should do much better.
2.7.1,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.7.1,get the same result.
2.7.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.1,We know use_pose_generator_scores == False in this case
2.7.1,check whether self.featurizer is instance of ComplexFeaturizer or not
2.7.1,TODO: How to handle the failure here?
2.7.1,TODO(rbharath): The autodock vina source computes surface distances
2.7.1,which take into account the van der Waals radius of each atom type.
2.7.1,"Shape (N, M)"
2.7.1,"Shape (N, M)"
2.7.1,Parse complex
2.7.1,check filetypes
2.7.1,Define locations of log and output files
2.7.1,Write GNINA conf file
2.7.1,Run GNINA
2.7.1,read output and log
2.7.1,Parse complex
2.7.1,Prepare protein
2.7.1,Get protein centroid and range
2.7.1,TODO(rbharath: Does vina divide box dimensions by 2?
2.7.1,Prepare ligand
2.7.1,Write Vina conf file
2.7.1,Define locations of output files
2.7.1,flake8: noqa
2.7.1,We provide no scoring model so the docker won't score
2.7.1,Check only one output since num_modes==1
2.7.1,We provide no scoring model so the docker won't score
2.7.1,Check only one output since num_modes==1
2.7.1,Let's turn on logging since this test will run for a while
2.7.1,Check returned files exist
2.7.1,Let's turn on logging since this test will run for a while
2.7.1,Check returned files exist
2.7.1,"Where d is greater than zero, the repulsion is just zeros"
2.7.1,"When d is 0, this should just be 1"
2.7.1,"When d == 0, the hbond interaction is 0"
2.7.1,The exponential returns 1 when input 0.
2.7.1,This exponential returns 1 when input 3
2.7.1,Let's turn on logging since this test will run for a while
2.7.1,Let's turn on logging since this test will run for a while
2.7.1,Let's turn on logging since this test will run for a while
2.7.1,Let's turn on logging since this test will run for a while
2.7.1,Let's turn on logging since this test will run for a while
2.7.1,Note this may download autodock Vina...
2.7.1,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.7.1,Test that every atom in pocket maps exists
2.7.1,scalar case
2.7.1,per-example case
2.7.1,This is a little arcane but it repeats w across tasks.
2.7.1,"If w.shape == (n_samples, 1) handle it as 1D"
2.7.1,"w.shape == (n_samples, n_tasks)"
2.7.1,scalar case
2.7.1,Handle n_classes/n_task shape ambiguity
2.7.1,Add in task dimension
2.7.1,Insert a task dimension (we know n_tasks=1 from above0
2.7.1,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.7.1,Handle classification. We need to convert labels into one-hot representation.
2.7.1,check whether n_classes is int or not
2.7.1,Handle n_classes/n_task shape ambiguity
2.7.1,Add in task dimension
2.7.1,Make everything 2D so easy to handle
2.7.1,Handle each task separately.
2.7.1,Handle continuous class probabilites of positive class for binary
2.7.1,Fill in class 0 probabilities
2.7.1,Add a task dimension to concatenate on
2.7.1,Handle binary labels
2.7.1,"make y_hot of shape (N, n_classes)"
2.7.1,Add a task dimension to concatenate on
2.7.1,Insert a task dimension
2.7.1,"Now of shape (N,)"
2.7.1,"Now of shape (N, 1)"
2.7.1,"Returns shape (N, n_tasks)"
2.7.1,"Now of shape (N,)"
2.7.1,"Now of shape (N, n_classes)"
2.7.1,"Now of shape (N, 1, n_classes)"
2.7.1,"Returns shape (N, n_tasks, n_classes)"
2.7.1,These are some smart defaults
2.7.1,These are some smart defaults corresponding to sklearn's required
2.7.1,behavior
2.7.1,Attempt some limited shape imputation to find n_tasks
2.7.1,check whether n_tasks is int or not
2.7.1,This is because `normalize_weight_shape` require int value.
2.7.1,FIXME: Incompatible types in assignment
2.7.1,Attempt to convert both into the same type
2.7.1,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.7.1,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.7.1,initialize fwd and reverse scores to -infinity
2.7.1,"cross-correlate separately for each base,"
2.7.1,for both the PSSM and its reverse complement
2.7.1,sum over the bases
2.7.1,take max of fwd and reverse scores at each position
2.7.1,"Shape (N_sequences, num_tasks)"
2.7.1,check whether wild_type_predictions is np.ndarray or not
2.7.1,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.7.1,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.7.1,Mutates every position of the sequence to every letter
2.7.1,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.7.1,Breakdown:
2.7.1,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.7.1,remove wild-type
2.7.1,len(arange) = N_letters * sequence_length
2.7.1,len(horizontal cycle) = N_letters * sequence_length
2.7.1,add mutant
2.7.1,make mutant predictions
2.7.1,check whether wild_type_predictions is np.ndarray or not
2.7.1,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.7.1,validation
2.7.1,flake8: noqa
2.7.1,metric class
2.7.1,metrics utils
2.7.1,sklearn & scipy score function
2.7.1,original score function
2.7.1,Get a random prediction matrix
2.7.1,"Of shape (N, n_classes)"
2.7.1,"Of shape (N, 1, n_classes)"
2.7.1,This has w for each task.
2.7.1,Best score case
2.7.1,Worst score case
2.7.1,best case
2.7.1,duplicate prediction value
2.7.1,Encode motif
2.7.1,"sequences now has shape (3, 4, 5, 1)"
2.7.1,"sequences now has shape (3, 4, 5, 1)"
2.7.1,Construct and train SequenceDNN model
2.7.1,Call in-silico mutagenesis
2.7.1,Construct and train SequenceDNN model
2.7.1,Call in-silico mutagenesis
2.7.1,Check nonzero elements exist
2.7.1,Special case handling of single input
2.7.1,Featurize task results iff they exist.
2.7.1,Filter out examples where featurization failed.
2.7.1,"For prospective data where results are unknown, it"
2.7.1,makes no sense to have y values or weights.
2.7.1,Featurize task results if they exist.
2.7.1,Filter out examples where featurization failed.
2.7.1,"For prospective data where results are unknown, it"
2.7.1,makes no sense to have y values or weights.
2.7.1,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.7.1,The field in which load_sdf_files return value stores smiles
2.7.1,Special case handling of single input
2.7.1,Featurize task results iff they exist.
2.7.1,Filter out examples where featurization failed.
2.7.1,"For prospective data where results are unknown, it"
2.7.1,makes no sense to have y values or weights.
2.7.1,Process legacy toggle
2.7.1,Set attributes
2.7.1,Handle special featurizer cases
2.7.1,Set self.featurizer
2.7.1,"(X, y, w, ids)"
2.7.1,TODO don't convert all sequences into np array (allow shards)
2.7.1,Check if line is a header
2.7.1,Handle empty sequence
2.7.1,Annotate start/stop of sequence
2.7.1,Open index file
2.7.1,create an empty list to store lines in files.
2.7.1,iterate through each line in the input file
2.7.1,If the number of lines iterated through is equal or less than the shard size:
2.7.1,append to list
2.7.1,else yield the list
2.7.1,set the line_number variable to the last line number (num) before 'yield' was called
2.7.1,yield list (shard/batch)
2.7.1,Re-initialize list with the index line to begin a new shard.
2.7.1,Set attributes
2.7.1,Handle special featurizer cases
2.7.1,Set self.featurizer
2.7.1,Set self.return_quality_scores
2.7.1,Featurize sequences
2.7.1,"(X, y , w, ids)"
2.7.1,Featurize sequences
2.7.1,"(X, y , w, ids)"
2.7.1,Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
2.7.1,First line : header description
2.7.1,second line : sequence
2.7.1,third line : more description usually the same as the first line
2.7.1,fourth line: quality scores of the sequence
2.7.1,Second line : add sequence to the sequence array
2.7.1,Fourth line
2.7.1,Handle empty sequence
2.7.1,Annotate start/stop of sequence
2.7.1,Sometimes zip files contain directories within. Traverse directories
2.7.1,TODO(rbharath): Add support for more extensions
2.7.1,Sort image files
2.7.1,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.7.1,Remove support indices
2.7.1,Remove support indices
2.7.1,Remove support indices
2.7.1,Get task specific entries
2.7.1,Now just get weights for this task
2.7.1,Get task specific entries
2.7.1,Now just get weights for this task
2.7.1,Now just get weights for this task
2.7.1,Now just get weights for this task
2.7.1,Split data into pos and neg lists.
2.7.1,No replacement allowed for supports
2.7.1,Handle one-d vs. non one-d feature matrices
2.7.1,Init the iterator
2.7.1,Set initial iterator state
2.7.1,support = self.supports[task][self.trial_num]
2.7.1,Increment and update logic
2.7.1,Init the iterator
2.7.1,Set initial iterator state
2.7.1,support = self.supports[task][self.trial_num]
2.7.1,Increment and update logic
2.7.1,Ensure that every worker will pick the same random order for each epoch.
2.7.1,Ensure that every worker will pick the same random order for each epoch.
2.7.1,"By invariant of when this is called, can assume num_samples > 0"
2.7.1,and num_samples < batch_size
2.7.1,Fill in batch arrays
2.7.1,"By invariant of when this is called, can assume num_samples > 0"
2.7.1,and num_samples < batch_size
2.7.1,Fill in batch arrays
2.7.1,Only the first set of copy will be counted in training loss
2.7.1,Retrieve the first sample so we can determine the dtypes.
2.7.1,Create a Tensorflow Dataset.
2.7.1,Find the X values.
2.7.1,Find the y values.
2.7.1,Find the w values.
2.7.1,Find the ids.
2.7.1,"Set labels to be zero, with zero weights"
2.7.1,The line here assumes that y generated by shard_generator is a numpy array
2.7.1,Load obsolete format -> save in new format
2.7.1,note that this corresponds to the _construct_metadata column order
2.7.1,Create temp directory to store resharded version
2.7.1,Get correct shapes for y/w
2.7.1,Write data in new shards
2.7.1,Handle shapes
2.7.1,Note that this means that DiskDataset resharding currently doesn't
2.7.1,work for datasets that aren't regression/classification.
2.7.1,Handle spillover from last shard
2.7.1,Should have updated to non-legacy metadata
2.7.1,Note that this resets the cache internally
2.7.1,"(ytz): Depending on the application, thread-based pools may be faster"
2.7.1,"than process based pools, since process based pools need to pickle/serialize"
2.7.1,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.7.1,we're actually protected by the GIL.
2.7.1,mp.dummy aliases ThreadPool to Pool
2.7.1,(ytz): this skips everything except possibly the last shard
2.7.1,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.7.1,make a NumpyDataset under the hood
2.7.1,"raw_data = (X, y, w, ids)"
2.7.1,Protect against generator exhaustion
2.7.1,This ensures tasks are consistent for all datasets
2.7.1,determine the shard sizes of the datasets to merge
2.7.1,"otherwise the entire dataset is the ""shard size"""
2.7.1,we must reshard the dataset to have a uniform size
2.7.1,choose the smallest shard size
2.7.1,Get full dataset in memory
2.7.1,Shuffle in memory
2.7.1,Write shuffled shards out to disk
2.7.1,Shuffle the arrays corresponding to each row in metadata_df
2.7.1,Reset cache
2.7.1,See if we have a cached copy of this shard.
2.7.1,"We don't, so load it from disk."
2.7.1,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.7.1,Try to cache this shard for later use.  Since the normal usage pattern is
2.7.1,"a series of passes through the whole dataset, there's no point doing"
2.7.1,anything fancy.  It never makes sense to evict another shard from the
2.7.1,"cache to make room for this one, because we'll probably want that other"
2.7.1,shard again before the next time we want this one.  So just cache as many
2.7.1,as we can and then stop.
2.7.1,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.7.1,Handle edge case with empty indices
2.7.1,We use two loops here. The outer while loop walks over selection shards
2.7.1,(the chunks of the indices to select that should go into separate
2.7.1,"output shards), while the inner for loop walks over the shards in the"
2.7.1,source datasets to select out the shard indices from that  source shard
2.7.1,Find indices which rest in this shard
2.7.1,Need to offset indices to fit within shard_size
2.7.1,Handle empty case where no data from this shard needed
2.7.1,Handle the case of datasets with y/w missing
2.7.1,Break if all indices have been used up already
2.7.1,Note these will be in the sorted order
2.7.1,We need to recover the original ordering. We can do this by using
2.7.1,np.where to find the locatios of the original indices in the sorted
2.7.1,indices.
2.7.1,We know there's only one match for np.where since this is a
2.7.1,"permutation, so the [0][0] pulls out the exact match location."
2.7.1,If shape metadata is available use it to directly compute shape from
2.7.1,metadata
2.7.1,"In absense of shape metadata, fall back to loading data from disk to"
2.7.1,find shape.
2.7.1,Case n_samples should be 1
2.7.1,flake8: noqa
2.7.1,TODO(rbharath): Get rid of * import
2.7.1,Test merging of numpy datasets
2.7.1,Load MUV dataset
2.7.1,Do an approximate comparison since splits are sometimes slightly off from
2.7.1,the exact fraction.
2.7.1,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.7.1,reloading will cause the transform to be reapplied. This is undesirable in
2.7.1,almost all cases. Need to understand a method to fix this.
2.7.1,The shuffling should have switched up the ordering
2.7.1,But all the same entries should still be present
2.7.1,All the data should have same shape
2.7.1,The shuffling should have switched up the ordering
2.7.1,But all the same entries should still be present
2.7.1,All the data should have same shape
2.7.1,The ids should now store the performed permutation. Check that the
2.7.1,original dataset is recoverable.
2.7.1,The ids should now store the performed permutation. Check that the
2.7.1,original dataset is recoverable.
2.7.1,Generate data
2.7.1,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.7.1,around for testing resharding.
2.7.1,Set cache to 0 size to avoid cache hiding errors
2.7.1,Generate data
2.7.1,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.7.1,around for testing resharding.
2.7.1,Set cache to 0 size to avoid cache hiding errors
2.7.1,Featurize emols dataset
2.7.1,example.fasta contains 3 sequences each of length 58.
2.7.1,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.7.1,"There is one ""image channel""."
2.7.1,"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
2.7.1,TODO: test with full uniprot file once sharding support is added.
2.7.1,Generate dummy dataset
2.7.1,Generate dummy dataset
2.7.1,Generate dummy dataset
2.7.1,Set last n_samples/2 weights to 0
2.7.1,Check that no support elements are sample from zero-weight samples
2.7.1,Generate dummy dataset
2.7.1,Generate dummy dataset
2.7.1,Create support generator
2.7.1,Generate dummy dataset
2.7.1,Create support generator
2.7.1,Generate dummy dataset
2.7.1,Assert all support elements have been removed
2.7.1,Generate dummy dataset
2.7.1,Assert all remove elements have been removed
2.7.1,Generate dummy dataset
2.7.1,Assert all support elements have been removed
2.7.1,Generate dummy dataset
2.7.1,Assert all remove elements have been removed
2.7.1,Generate dummy dataset
2.7.1,Set last n_samples/2 weights to 0
2.7.1,Sample from first n_samples/2 elements for support
2.7.1,Should lie within first n_samples/2 samples only
2.7.1,Generate dummy dataset
2.7.1,Create support generator
2.7.1,Generate dummy dataset
2.7.1,This is necessary since from_numpy adds in shape information
2.7.1,This is necessary since from_numpy adds in shape information
2.7.1,This is necessary since from_numpy adds in shape information
2.7.1,Generate data
2.7.1,Generate data
2.7.1,Generate data
2.7.1,Should now have 10 shards
2.7.1,This is the shape of legacy_data
2.7.1,legacy_dataset is a dataset in the legacy format kept around for testing
2.7.1,purposes.
2.7.1,This is the shape of legacy_data_reshard
2.7.1,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.7.1,around for testing
2.7.1,Should now have 10 shards
2.7.1,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.7.1,Test constructor reload works for legacy format
2.7.1,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.7.1,around for testing resharding.
2.7.1,Reshard copy
2.7.1,Check metadata has been updated
2.7.1,First try using images for X.
2.7.1,Now try using images for y.
2.7.1,Transform it
2.7.1,Test on identity matrix
2.7.1,Generate random sparse features dataset
2.7.1,Test edge case with array of all zeros
2.7.1,Test cases where n_samples < 2*n_samples < batch_size
2.7.1,Test cases where n_samples < batch_size
2.7.1,Test case where n_samples == batch_size
2.7.1,Test case for object featurization.
2.7.1,Test case for more complicated object featurization
2.7.1,Test case with multidimensional data
2.7.1,Test cases where n_samples < 2*n_samples < batch_size
2.7.1,Test cases where n_samples < batch_size
2.7.1,Test case where n_samples == batch_size
2.7.1,Test case for object featurization.
2.7.1,Test case for more complicated object featurization
2.7.1,Test case with multidimensional data
2.7.1,Test first resharding worked
2.7.1,Test second resharding worked
2.7.1,approx 1/15! chance of equality
2.7.1,Generate data
2.7.1,Generate data
2.7.1,Transform it
2.7.1,special case to test
2.7.1,deterministic
2.7.1,non-deterministic
2.7.1,we don't know the order in which the shards are iterated in.
2.7.1,Check that we have all the data in
2.7.1,Test iterating in order.
2.7.1,Test iterating out of order.
2.7.1,Test iterating in batches.
2.7.1,Test iterating with multiple workers.
2.7.1,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.7.1,Try specifying particular columns.
2.7.1,Try specifying particular columns
2.7.1,Test id shrinkage
2.7.1,Test task shrinkage
2.7.1,Test max print size
2.7.1,Create image file
2.7.1,Create zip of image file
2.7.1,Create zip of multiple image files
2.7.1,"Create zip of multiple image files, multiple_types"
2.7.1,Create image directory
2.7.1,These are the known dimensions of face.png
2.7.1,These are the known dimensions of face.png
2.7.1,TODO(rbharath): Where are the color channels?
2.7.1,"Since the different files have different shapes, makes an object array"
2.7.1,Splits featurized samples into train/test
2.7.1,Splits featurized samples into train/test
2.7.1,Splits featurized samples into train/test
2.7.1,Splits featurized samples into train/test
2.7.1,Now perform move
2.7.1,Only for debug!
2.7.1,Make directories to store the raw and featurized datasets.
2.7.1,Load dataset
2.7.1,Featurize tox21 dataset
2.7.1,featurization
2.7.1,train/valid split.
2.7.1,singletask load
2.7.1,comparison
2.7.1,Only for debug!
2.7.1,Make directories to store the raw and featurized datasets.
2.7.1,Load dataset
2.7.1,Featurize tox21 dataset
2.7.1,For debugging purposes
2.7.1,multitask load
2.7.1,Do train/valid split.
2.7.1,singletask load
2.7.1,comparison
2.7.1,Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
2.7.1,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.7.1,"Expected shape is now (4, 192, 5)"
2.7.1,Get the labels/weights
2.7.1,Normalize shapes
2.7.1,Remove labels with zero weights
2.7.1,Note that we may have 0 elements of a given class since we remove those
2.7.1,labels with zero weight.
2.7.1,this works because y is 1D
2.7.1,This is the right ratio since int(N/num_c) * num_c \approx N
2.7.1,for all classes
2.7.1,Flattening is safe because of shape check above
2.7.1,Hack to allow for easy unpickling:
2.7.1,http://stefaanlippens.net/pickleproblem
2.7.1,Some transformation must happen
2.7.1,Add this case in to handle non-DiskDataset that should be written to disk
2.7.1,Note that transformers have to be undone in reversed order
2.7.1,Handle division by zero
2.7.1,Handle division by zero
2.7.1,Control for pathological case with no variance.
2.7.1,Handle case with 1 task correctly
2.7.1,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.7.1,Find the task dimension of z
2.7.1,Prevent broadcasting on wrong dimension
2.7.1,BalancingTransformer can only transform weights.
2.7.1,Compute weighting factors from dataset.
2.7.1,Handle 1-D case
2.7.1,Remove labels with zero weights
2.7.1,Note that we may have 0 elements of a given class since we remove those
2.7.1,labels with zero weight. This typically happens in multitask datasets
2.7.1,where some datapoints only have labels for some tasks.
2.7.1,this works because task_y is 1D
2.7.1,This is the right ratio since N_task/num_c * num_c = N_task
2.7.1,for all classes
2.7.1,Set to the class weight computed previously
2.7.1,Need this for transform_y
2.7.1,Handle 1D case
2.7.1,THis reshape is safe because of guard above.
2.7.1,map the indices to labels
2.7.1,generating batch of data by slicing similarity matrix
2.7.1,into 100*reference_dataset_length
2.7.1,concatenate batches of data together
2.7.1,highest similarity is 1: target is in the reference
2.7.1,use the following K points
2.7.1,"highest less than 1: target not in the reference, use top K points"
2.7.1,calculate matrix multiplicatin on slices
2.7.1,concatenate the slices together
2.7.1,list of calculation orders for DAGs
2.7.1,stemming from one specific atom in the molecule
2.7.1,starting from the adjacency list derived by graphconv featurizer
2.7.1,"number of atoms, also number of DAGs"
2.7.1,"DAG on a molecule with k atoms includes k steps of calculation,"
2.7.1,each step calculating graph features for one atom.
2.7.1,`max_atoms` is the maximum number of steps
2.7.1,each iteration generates the DAG starting from atom with index `count`
2.7.1,"list of lists, elements represent the calculation orders"
2.7.1,for atoms in the current graph
2.7.1,starting from the target atom with index `count`
2.7.1,flags of whether the atom is already included in the DAG
2.7.1,atom `count` is in the DAG
2.7.1,recording number of radial propagation steps
2.7.1,"in the fisrt loop, atoms directly connected to `count` will be added"
2.7.1,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.7.1,will be added in the second loop(radial=1).
2.7.1,atoms i-bond away will be added in i-th loop
2.7.1,"when molecules have separate parts, starting from one part,"
2.7.1,it is not possible to include all atoms.
2.7.1,this break quit the loop when going into such condition
2.7.1,reinitialize targets for next iteration
2.7.1,atoms connected to current_atom
2.7.1,generate the dependency map of current DAG
2.7.1,atoms connected to `current_atoms`(and not included in the DAG)
2.7.1,"are added, and will be the `current_atoms` for next iteration."
2.7.1,"DAG starts from the target atom, calculation should go in reverse"
2.7.1,`edge[1]` is the parent of `edge[0]`
2.7.1,"after this loop, `parents[i]` includes all parents of atom i"
2.7.1,manually adding the atom index into its parents list
2.7.1,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.7.1,atoms with less parents(farther from the target atom) come first.
2.7.1,"graph features of atoms without parents will be first calculated,"
2.7.1,then atoms with more parents can be calculated in order
2.7.1,based on previously calculated graph features.
2.7.1,target atom of this DAG will be calculated in the last step
2.7.1,padding with `max_atoms`
2.7.1,padding
2.7.1,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.7.1,which is a max_atoms * max_atoms numpy array after padding
2.7.1,class ANITransformer(Transformer):
2.7.1,"""""""Performs transform from 3D coordinates to ANI symmetry functions"
2.7.1,Note
2.7.1,----
2.7.1,This class requires TensorFlow to be installed.
2.7.1,""""""""
2.7.1,"def __init__(self,"
2.7.1,"max_atoms=23,"
2.7.1,"radial_cutoff=4.6,"
2.7.1,"angular_cutoff=3.1,"
2.7.1,"radial_length=32,"
2.7.1,"angular_length=8,"
2.7.1,"atom_cases=[1, 6, 7, 8, 16],"
2.7.1,"atomic_number_differentiated=True,"
2.7.1,coordinates_in_bohr=True):
2.7.1,""""""""
2.7.1,Only X can be transformed
2.7.1,""""""""
2.7.1,import tensorflow as tf
2.7.1,self.max_atoms = max_atoms
2.7.1,self.radial_cutoff = radial_cutoff
2.7.1,self.angular_cutoff = angular_cutoff
2.7.1,self.radial_length = radial_length
2.7.1,self.angular_length = angular_length
2.7.1,self.atom_cases = atom_cases
2.7.1,self.atomic_number_differentiated = atomic_number_differentiated
2.7.1,self.coordinates_in_bohr = coordinates_in_bohr
2.7.1,self.compute_graph = self.build()
2.7.1,self.sess = tf.Session(graph=self.compute_graph)
2.7.1,self.transform_batch_size = 32
2.7.1,"super(ANITransformer, self).__init__(transform_X=True)"
2.7.1,"def transform_array(self, X, y, w):"
2.7.1,if self.transform_X:
2.7.1,X_out = []
2.7.1,num_transformed = 0
2.7.1,start = 0
2.7.1,batch_size = self.transform_batch_size
2.7.1,while True:
2.7.1,"end = min((start + 1) * batch_size, X.shape[0])"
2.7.1,X_batch = X[(start * batch_size):end]
2.7.1,output = self.sess.run(
2.7.1,"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
2.7.1,X_out.append(output)
2.7.1,num_transformed = num_transformed + X_batch.shape[0]
2.7.1,logger.info('%i samples transformed' % num_transformed)
2.7.1,start += 1
2.7.1,if end >= len(X):
2.7.1,break
2.7.1,"X_new = np.concatenate(X_out, axis=0)"
2.7.1,assert X_new.shape[0] == X.shape[0]
2.7.1,"return (X_new, y, w)"
2.7.1,"def untransform(self, z):"
2.7.1,raise NotImplementedError(
2.7.1,"""Cannot untransform datasets with ANITransformer."")"
2.7.1,def build(self):
2.7.1,""""""" tensorflow computation graph for transform """""""
2.7.1,import tensorflow as tf
2.7.1,graph = tf.Graph()
2.7.1,with graph.as_default():
2.7.1,self.inputs = tf.keras.Input(
2.7.1,"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
2.7.1,"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
2.7.1,flags = tf.sign(atom_numbers)
2.7.1,flags = tf.cast(
2.7.1,"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
2.7.1,"coordinates = self.inputs[:, :, 1:]"
2.7.1,if self.coordinates_in_bohr:
2.7.1,coordinates = coordinates * 0.52917721092
2.7.1,"d = self.distance_matrix(coordinates, flags)"
2.7.1,"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
2.7.1,"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
2.7.1,"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
2.7.1,"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
2.7.1,coordinates)
2.7.1,self.outputs = tf.concat(
2.7.1,[
2.7.1,"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
2.7.1,angular_sym
2.7.1,"],"
2.7.1,axis=2)
2.7.1,return graph
2.7.1,"def distance_matrix(self, coordinates, flags):"
2.7.1,""""""" Generate distance matrix """""""
2.7.1,import tensorflow as tf
2.7.1,max_atoms = self.max_atoms
2.7.1,"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
2.7.1,"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
2.7.1,# Calculate pairwise distance
2.7.1,"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
2.7.1,# Masking for valid atom index
2.7.1,d = d * flags
2.7.1,return d
2.7.1,"def distance_cutoff(self, d, cutoff, flags):"
2.7.1,""""""" Generate distance matrix with trainable cutoff """""""
2.7.1,import tensorflow as tf
2.7.1,# Cutoff with threshold Rc
2.7.1,d_flag = flags * tf.sign(cutoff - d)
2.7.1,d_flag = tf.nn.relu(d_flag)
2.7.1,d_flag = d_flag * tf.expand_dims(
2.7.1,"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
2.7.1,d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
2.7.1,return d * d_flag
2.7.1,"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
2.7.1,""""""" Radial Symmetry Function """""""
2.7.1,import tensorflow as tf
2.7.1,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.7.1,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.7.1,"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
2.7.1,ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
2.7.1,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
2.7.1,"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
2.7.1,length = ita.get_shape().as_list()[-1]
2.7.1,"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
2.7.1,"d = tf.stack([d] * length, axis=3)"
2.7.1,out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
2.7.1,if self.atomic_number_differentiated:
2.7.1,out_tensors = []
2.7.1,for atom_type in self.atom_cases:
2.7.1,selected_atoms = tf.expand_dims(
2.7.1,"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
2.7.1,axis=3)
2.7.1,"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
2.7.1,"return tf.concat(out_tensors, axis=2)"
2.7.1,else:
2.7.1,"return tf.reduce_sum(out, axis=2)"
2.7.1,"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
2.7.1,""""""" Angular Symmetry Function """""""
2.7.1,import tensorflow as tf
2.7.1,max_atoms = self.max_atoms
2.7.1,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.7.1,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.7.1,"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
2.7.1,ita = 3 / (Rs[1] - Rs[0])**2
2.7.1,"thetas = np.linspace(0., np.pi, self.angular_length)"
2.7.1,zeta = float(self.angular_length**2)
2.7.1,"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
2.7.1,"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
2.7.1,"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
2.7.1,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
2.7.1,"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
2.7.1,length = zeta.get_shape().as_list()[-1]
2.7.1,"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
2.7.1,"[coordinates] * max_atoms, 2)"
2.7.1,"R_ij = tf.stack([d] * max_atoms, axis=3)"
2.7.1,"R_ik = tf.stack([d] * max_atoms, axis=2)"
2.7.1,"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
2.7.1,"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
2.7.1,# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.7.1,"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
2.7.1,"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
2.7.1,vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
2.7.1,"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
2.7.1,"R_ij = tf.stack([R_ij] * length, axis=4)"
2.7.1,"R_ik = tf.stack([R_ik] * length, axis=4)"
2.7.1,"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
2.7.1,"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
2.7.1,"theta = tf.stack([theta] * length, axis=4)"
2.7.1,"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
2.7.1,tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
2.7.1,if self.atomic_number_differentiated:
2.7.1,out_tensors = []
2.7.1,"for id_j, atom_type_j in enumerate(self.atom_cases):"
2.7.1,for atom_type_k in self.atom_cases[id_j:]:
2.7.1,"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
2.7.1,"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
2.7.1,selected_atoms = tf.expand_dims(
2.7.1,"tf.expand_dims(selected_atoms, axis=1), axis=4)"
2.7.1,out_tensors.append(
2.7.1,"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
2.7.1,"return tf.concat(out_tensors, axis=2)"
2.7.1,else:
2.7.1,"return tf.reduce_sum(out_tensor, axis=(2, 3))"
2.7.1,def get_num_feats(self):
2.7.1,n_feat = self.outputs.get_shape().as_list()[-1]
2.7.1,return n_feat
2.7.1,flake8: noqa
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a y transformer
2.7.1,Check w is unchanged since this is a y transformer
2.7.1,Check y is now a logarithmic version of itself
2.7.1,Check that untransform does the right thing.
2.7.1,Check ids are unchanged.
2.7.1,Check y is unchanged since this is a X transformer
2.7.1,Check w is unchanged since this is a y transformer
2.7.1,Check y is now a logarithmic version of itself
2.7.1,Check that untransform does the right thing.
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a y transformer
2.7.1,Check w is unchanged since this is a y transformer
2.7.1,Check y is now a logarithmic version of itself
2.7.1,Check that untransform does the right thing.
2.7.1,Tests logarithmic data transformer with selection.
2.7.1,Check ids are unchanged.
2.7.1,Check y is unchanged since this is a X transformer
2.7.1,Check w is unchanged since this is a y transformer
2.7.1,Check y is now a logarithmic version of itself
2.7.1,Check that untransform does the right thing.
2.7.1,Check ids are unchanged.
2.7.1,Check y is unchanged since this is an X transformer
2.7.1,Check w is unchanged since this is an X transformer
2.7.1,Check X is now holding the proper values when sorted.
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is an y transformer
2.7.1,Check w is unchanged since this is an y transformer
2.7.1,Check y is now holding the proper values when sorted.
2.7.1,Check that untransform does the right thing.
2.7.1,Check ids are unchanged.
2.7.1,Check y is unchanged since this is an X transformer
2.7.1,Check w is unchanged since this is an X transformer
2.7.1,Check X is now holding the proper values when sorted.
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a y transformer
2.7.1,Check w is unchanged since this is a y transformer
2.7.1,Check y is now holding the proper values when sorted.
2.7.1,Check ids are unchanged before and after transformation
2.7.1,Check X is unchanged since transform_y is true
2.7.1,Check w is unchanged since transform_y is true
2.7.1,Check minimum and maximum values of transformed y are 0 and 1
2.7.1,Check untransform works correctly
2.7.1,Check ids are unchanged before and after transformation
2.7.1,Check X is unchanged since transform_y is true
2.7.1,Check w is unchanged since transform_y is true
2.7.1,Check minimum and maximum values of transformed y are 0 and 1
2.7.1,Test if dimensionality expansion is handled correctly by untransform
2.7.1,Check ids are unchanged before and after transformation
2.7.1,Check X is unchanged since transform_y is true
2.7.1,Check w is unchanged since transform_y is true
2.7.1,Check minimum and maximum values of transformed y are 0 and 1
2.7.1,Check untransform works correctly
2.7.1,Load mini log-solubility dataset.
2.7.1,The transformer generates n DAGs for a molecule with n
2.7.1,"atoms. These are denoted the ""parents"""
2.7.1,extract only the images (no need of the labels)
2.7.1,reshaping the vector to image
2.7.1,Check Blurring
2.7.1,Check center crop
2.7.1,Check crop
2.7.1,Check convert2gray
2.7.1,Check rotation
2.7.1,Some more test cases for flip
2.7.1,Check flip
2.7.1,Check Scales
2.7.1,Check shift
2.7.1,check gaussian noise
2.7.1,check salt and pepper noise
2.7.1,Check median filter
2.7.1,transforming y should raise an exception
2.7.1,transforming w should raise an exception
2.7.1,transforming X should be okay
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a y transformer
2.7.1,Check w is unchanged since this is a y transformer
2.7.1,"Check that y_t has zero mean, unit std."
2.7.1,Check that untransform does the right thing.
2.7.1,Check ids are unchanged.
2.7.1,Check y is unchanged since this is a X transformer
2.7.1,Check w is unchanged since this is a y transformer
2.7.1,"Check that X_t has zero mean, unit std."
2.7.1,np.set_printoptions(threshold='nan')
2.7.1,Entries with zero std are not normalized
2.7.1,Check that untransform does the right thing.
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a w transformer
2.7.1,Check y is unchanged since this is a w transformer
2.7.1,Assert that entries with zero weight retain zero weight
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a w transformer
2.7.1,Check y is unchanged since this is a w transformer
2.7.1,Assert that entries with zero weight retain zero weight
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a w transformer
2.7.1,Check y is unchanged since this is a w transformer
2.7.1,Assert that entries with zero weight retain zero weight
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a w transformer
2.7.1,Check y is unchanged since this is a w transformer
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is a w transformer
2.7.1,Check y is unchanged since this is a w transformer
2.7.1,Assert that entries with zero weight retain zero weight
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,Check ids are unchanged.
2.7.1,Check y is unchanged since this is an X transformer
2.7.1,Check w is unchanged since this is an X transformer
2.7.1,Check X is now holding the proper values in each column.
2.7.1,Check ids are unchanged.
2.7.1,Check X is unchanged since this is an X transformer
2.7.1,Check w is unchanged since this is an X transformer
2.7.1,Check y is now holding the proper values in each column.
2.7.1,Check that untransform does the right thing.
2.7.1,Check that we have length 8 now with duplication
2.7.1,Check shapes
2.7.1,Check that we have 4 positives and 4 negatives
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,Note that nothing should change in this dataset since weights balance!
2.7.1,Check that still we have length 6
2.7.1,Check shapes
2.7.1,Check that we have 2 positives and 4 negatives
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,Check that we have length 8 now with duplication
2.7.1,Check shapes
2.7.1,Check that we have 4 positives and 4 negatives
2.7.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.1,6-1 imbalance in favor of class 0
2.7.1,Check that we have length 30 now with duplication
2.7.1,Check shapes
2.7.1,Check that we have 6 of each class
2.7.1,Check that sum of all class weights is equal by comparing to 0 weight
2.7.1,Note class imbalance. This will round to 2x duplication for 1
2.7.1,Check that we have length 13 now with duplication
2.7.1,Check shapes
2.7.1,Check that we have 6 positives and 7 negatives
2.7.1,################################################################
2.7.1,save.py is out of date. You should not import any functions from here.
2.7.1,################################################################
2.7.1,flake8: noqa
2.7.1,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.7.1,Walk through the original file and extract ATOM/HETATM lines and
2.7.1,add PDBQT charge annotations.
2.7.1,Remove rotatable bonds from this molecule
2.7.1,Get the connected components now that the rotatable bonds have
2.7.1,been removed.
2.7.1,The root is the largest connected component.
2.7.1,Write the root component
2.7.1,"We've looked at the root, so take note of that"
2.7.1,Compute partial charges on molecule if RDKit Mol
2.7.1,indices to atoms to keep
2.7.1,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.7.1,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.7.1,nonzero contact.
2.7.1,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.7.1,TODO: This is duplicated! Clean up
2.7.1,Updates charges in place
2.7.1,initial embedding
2.7.1,minimization and pruning
2.7.1,always keep lowest-energy conformer
2.7.1,discard conformers after max_conformers is reached
2.7.1,get RMSD to selected conformers
2.7.1,discard conformers within the RMSD threshold
2.7.1,create a new molecule to hold the chosen conformers
2.7.1,this ensures proper conformer IDs and energy-based ordering
2.7.1,False here specifies that water is to be removed
2.7.1,Updates charges in place
2.7.1,TODO: This is wrong. Should return all molecules
2.7.1,TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.7.1,This updates in place
2.7.1,indices of atoms to keep
2.7.1,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.7.1,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.7.1,nonzero contact.
2.7.1,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.7.1,####################################################
2.7.1,Compute partial charges on molecule if rdkit
2.7.1,####################################################
2.7.1,Number of voxels per one edge of box to voxelize.
2.7.1,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.7.1,If interval1 < interval2 entirely
2.7.1,If interval2 < interval1 entirely
2.7.1,Each triangle in the simplices is a set of 3 atoms from
2.7.1,coordinates which forms the vertices of an exterior triangle on
2.7.1,the convex hull of the macromolecule.
2.7.1,Points is the set of atom coordinates that make up this
2.7.1,triangular face on the convex hull
2.7.1,Let's extract x/y/z coords for this face
2.7.1,Let's compute min/max points
2.7.1,"Nitrogen has atomic number 7, and oxygen 8."
2.7.1,If atom is a hydrogen
2.7.1,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.7.1,If atom is a hydrogen
2.7.1,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.7.1,if ring from mol1 is aromatic
2.7.1,...and atom from mol2 is a cation
2.7.1,if angle and distance are correct
2.7.1,count atoms forming a contact
2.7.1,if ring is aromatic
2.7.1,"save its indices, center, and normal"
2.7.1,remember mol1-mol2 pairs we already counted
2.7.1,"if this pair is new, count atoms forming a contact"
2.7.1,"if this pair is new, count atoms forming a contact"
2.7.1,find interacting rings from mol1 and cations from mol2
2.7.1,find interacting cations from mol1 and rings from mol2
2.7.1,merge counters
2.7.1,the line has format
2.7.1,REMARK VINA RESULT: score ...
2.7.1,There is only 1 such line per model so we can append it
2.7.1,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.7.1,Apply common fixes to PDB files
2.7.1,Optimize ligand
2.7.1,"For a node-prediction task, label is not added to edge features and other global features"
2.7.1,because label here is a node-level attribute and not a graph-level attribute
2.7.1,"In this case, the 'y' attribute of GraphData will contain the"
2.7.1,node-level labels.
2.7.1,not a self-loop
2.7.1,Make sure input is a list
2.7.1,FIXME: Incompatible types in assignment
2.7.1,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.7.1,Ensure that metric is wrapped in a list.
2.7.1,This case checks if input is a function then wraps a
2.7.1,dc.metrics.Metric object around it
2.7.1,Process input metrics
2.7.1,Compute multitask metrics
2.7.1,We use y/w to aggregate labels/weights across generator.
2.7.1,This is a KerasModel.
2.7.1,Some datasets have weights
2.7.1,Process predictions and populate y/w lists
2.7.1,Combine labels/weights
2.7.1,Undo data transformations.
2.7.1,Compute multitask metrics
2.7.1,These functions have moved to deepchem.utils_docking_utils
2.7.1,flake8: noqa
2.7.1,The number of elements to print for dataset ids/tasks
2.7.1,"If a dataset contains more than this number of elements, it won't"
2.7.1,print any dataset ids
2.7.1,An activation function for a layer: either a function or the name of a standard activation
2.7.1,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.7.1,"A single value of some type, or multiple values of that type"
2.7.1,The shape of a NumPy array
2.7.1,"A NumPy array, or an object that can be converted to one.  Once we move to"
2.7.1,"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
2.7.1,type of RDKit object
2.7.1,type of Pymatgen object
2.7.1,Generate a random temporary file name
2.7.1,Ensure the file is created
2.7.1,Open the file in the given mode
2.7.1,Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
2.7.1,Structures are stored in .sdf file
2.7.1,"Note: Here, the order of columns is based on the order in which the values"
2.7.1,"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
2.7.1,"tasks above, they occur after `tasks` here."
2.7.1,"FIXME Ideally, we should use something like a dictionary here to keep it independent"
2.7.1,of column ordering.
2.7.1,Reset aggregator
2.7.1,Handle final leftovers for this file
2.7.1,First line of user-specified CSV *must* be header.
2.7.1,"If gzipped, need to compute extension again"
2.7.1,First line of user-specified CSV *must* be header.
2.7.1,The label encoder is given characters for ACGTN
2.7.1,Peak at the first sequence to get the length of the sequence.
2.7.1,init an one-hot vector
2.7.1,"If include_unknown_set is True, set the last index is 1."
2.7.1,################################################################
2.7.1,atom (node) featurization
2.7.1,################################################################
2.7.1,################################################################
2.7.1,bond (edge) featurization
2.7.1,################################################################
2.7.1,One sequence has length longer than others. This should throw a
2.7.1,ValueError.
2.7.1,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.7.1,Loosening atol to see if tests stop failing sporadically
2.7.1,string set
2.7.1,integer set
2.7.1,include_unknown_set is False
2.7.1,include_unknown_set is True
2.7.1,check unknown atoms
2.7.1,check original set
2.7.1,"Generally, =O behaves as an electron acceptor"
2.7.1,we must compute partial charges before using `get_atom_partial_charge`
2.7.1,The C-N bond is a single bond
2.7.1,graph-level labels
2.7.1,node-level labels
2.7.1,graph.y contains node-labels and graph.node_features.shape[0]
2.7.1,holds number of nodes in that graph
2.7.1,TODO test more formats for ligand
2.7.1,TODO test more formats for ligand
2.7.1,adding hydrogens and charges is tested in dc.utils
2.7.1,self.ligand_file is for 3ws9_ligand.sdf
2.7.1,dummy function which can be passed as the parameter f to simultaneous_move and single_move
2.7.1,test for gauss_initialize_position
2.7.1,testing symmetric simultaneous_move
2.7.1,testing asymmetric simultaneous_move
2.7.1,testing symmetric single_move
2.7.1,testing asymmetric single_move
2.7.1,simple flat ring
2.7.1,self.cycle4.Compute2DCoords()
2.7.1,load and sanitize two real molecules
2.7.1,parallel normals
2.7.1,perpendicular normals
2.7.1,too far away
2.7.1,perpendicular normals
2.7.1,parallel normals
2.7.1,too far away
2.7.1,order of the molecules shouldn't matter
2.7.1,with this criteria we should find both types of stacking
2.7.1,parallel normals
2.7.1,perpendicular normals
2.7.1,too far away
2.7.1,def test_compute_cation_pi(self):
2.7.1,"# TODO(rbharath): find better example, currently dicts are empty"
2.7.1,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.7.1,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.7.1,"TODO find better example, currently dicts are empty"
2.7.1,TODO test more formats for ligand
2.7.1,Test on RDKit
2.7.1,3D vector with unit length
2.7.1,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.7.1,"random coords between 0 and 1, so the max possible distance in sqrt(3)"
2.7.1,check if correct distance metric was used
2.7.1,Construct a random class probability matrix
2.7.1,Construct a random class probability matrix
2.7.1,"Note that since no name as provided, metrics are index by order"
2.7.1,given.
2.7.1,"Note that since no name as provided, metrics are index by order"
2.7.1,given.
2.7.1,"Note that since no name as provided, metrics are index by order"
2.7.1,given.
2.7.1,"Note that since no name as provided, metrics are index by order"
2.7.1,given.
2.7.1,TODO: Fix this case with correct thresholding
2.7.1,TODO: Fix this case with correct thresholding
2.7.1,There are 4 faces to the shape created by coords
2.7.1,flake8: noqa
2.7.1,Get the degree id list (which corrects for min_deg)
2.7.1,Get the size of each degree block
2.7.1,Get the the start indices for items in each block
2.7.1,Get the node indices when they are reset when the degree changes
2.7.1,Convert to numpy array
2.7.1,Reorder old atom_features
2.7.1,Reorder old deg lists
2.7.1,Sort membership
2.7.1,Create old to new dictionary. not exactly intuitive
2.7.1,Reorder adjacency lists
2.7.1,Get numpy version of degree list for indexing
2.7.1,"Initialize adj_lists, which supports min_deg = 1 only"
2.7.1,Parse as deg separated
2.7.1,Get indices corresponding to the current degree
2.7.1,Extract and save adjacency list for the current degree
2.7.1,Construct the slice information
2.7.1,Get the cumulative indices after the first index
2.7.1,Set indices with zero sized slices to zero to avoid indexing errors
2.7.1,TODO(rbharath): Can this be removed?
2.7.1,Use random insted of zeros to prevent weird issues with summing to zero
2.7.1,"Combine the features, then sort them by (atom_degree, mol_index)"
2.7.1,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.7.1,Create a map from the original atom indices within each molecule to the
2.7.1,indices in the combined object.
2.7.1,Sort all atoms by degree.
2.7.1,"Get the size of each atom list separated by molecule id, then by degree"
2.7.1,Get the final size of each degree block
2.7.1,"Get the index at which each degree starts, not resetting after each degree"
2.7.1,And not stopping at any specific molecule
2.7.1,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.7.1,first column telling the start indices of each degree block and the
2.7.1,second colum telling the size of each degree block
2.7.1,Determine the membership (atom i belongs to molecule membership[i])
2.7.1,Initialize the new degree separated adjacency lists
2.7.1,Update the old adjacency lists with the new atom indices and then combine
2.7.1,all together
2.7.1,Iterate through all the molecules
2.7.1,Get the adjacency lists for this molecule and current degree id
2.7.1,"Correct all atom indices to the final indices, and then save the"
2.7.1,results into the new adjacency lists
2.7.1,Increment once row is done
2.7.1,Get the final aggregated molecule
2.7.1,"Requriments - transformers, tokenizers"
2.7.1,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.7.1,The vocab may be expanded in the near future
2.7.1,add vocab_file dict
2.7.1,"unk_token=""[UNK]"","
2.7.1,"sep_token=""[SEP]"","
2.7.1,"pad_token=""[PAD]"","
2.7.1,"cls_token=""[CLS]"","
2.7.1,"mask_token=""[MASK]"","
2.7.1,flake8: noqa
2.7.1,Initalize with 1
2.7.1,Replace the hybridization
2.7.1,global possible_hybridization_list
2.7.1,Allow 0 index to correspond to null molecule 1
2.7.1,Correct for null
2.7.1,"print(6-k-1, id)"
2.7.1,Correct for last one
2.7.1,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.7.1,"Handle edge case of self-pairs (i, i)"
2.7.1,Increment by 1 since we don't want 0-indexing
2.7.1,"This creates a matrix of shape (2, num_pairs)"
2.7.1,Get mapping
2.7.1,first `bt_len` features are bond features(if applicable)
2.7.1,For ring pairs outside max pairs distance continue
2.7.1,`bt_len`-th feature is if the pair of atoms are in the same ring
2.7.1,graph distance between two atoms
2.7.1,distance is a matrix of 1-hot encoded distances for all atoms
2.7.1,For ring pairs outside max pairs distance continue
2.7.1,Euclidean distance between atoms
2.7.1,atoms `radial` bonds away from `a1`
2.7.1,atoms less than `radial` bonds away
2.7.1,find atoms `radial`+1 bonds away
2.7.1,create temporary valid ids serving to filter out failed featurizations from every sublist
2.7.1,"of features (i.e. every molecules' frags list), and also totally failed sublists."
2.7.1,This makes output digestable by Loaders
2.7.1,Get the node features
2.7.1,Stack nodes into an array
2.7.1,Get bond lists with reverse edges included
2.7.1,Get canonical adjacency list
2.7.1,"Distance is either graph distance(True) or Euclidean distance(False,"
2.7.1,only support datasets providing Cartesian coordinates)
2.7.1,Set dtype
2.7.1,If includes explicit hydrogens
2.7.1,If uses use_chirality
2.7.1,Atom features
2.7.1,Stack nodes into an array
2.7.1,Get bond lists
2.7.1,Get canonical adjacency list
2.7.1,Calculate pair features
2.7.1,the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
2.7.1,"SMILES is unique, so set a canonical order of atoms"
2.7.1,Add hydrogens and generate a conformation.
2.7.1,Record properties of the molecules.
2.7.1,Create the output object.
2.7.1,"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
2.7.1,flake8: noqa
2.7.1,base classes for featurizers
2.7.1,molecule featurizers
2.7.1,complex featurizers
2.7.1,material featurizers
2.7.1,tokenizers
2.7.1,support classes
2.7.1,for str
2.7.1,for list
2.7.1,validation
2.7.1,skip list
2.7.1,skip path string
2.7.1,main logic
2.7.1,Find a successful featurization
2.7.1,Replace failed featurizations with appropriate array
2.7.1,Special case handling of single molecule
2.7.1,Convert iterables to list
2.7.1,condition if the original atom order is required
2.7.1,"mol must be a RDKit Mol object, so parse a SMILES"
2.7.1,"mol must be a RDKit Mol object, so parse a SMILES"
2.7.1,"SMILES is unique, so set a canonical order of atoms"
2.7.1,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.7.1,atom_name is of format RESX-ATOMTYPE
2.7.1,where X is a 1 to 4 digit number
2.7.1,validate params
2.7.1,"np.max() method works only for a non-empty array, so size of the array should be non-zero"
2.7.1,Adding shapes of kwargs
2.7.1,This assumes that the edge features for self loops are full-zero tensors
2.7.1,In the future we may want to support featurization for self loops
2.7.1,stack features
2.7.1,"before stacking edge_features or node_pos_features,"
2.7.1,we should check whether these are None or not
2.7.1,create new edge index
2.7.1,graph_index indicates which nodes belong to which graph
2.7.1,Setup image
2.7.1,Compute bond properties
2.7.1,Compute atom properties
2.7.1,Setup image
2.7.1,Compute bond properties
2.7.1,Compute atom properties
2.7.1,Reshape done for proper broadcast
2.7.1,"Reshapes, and axes manipulations to facilitate vector processing."
2.7.1,Draw a line between the two atoms.
2.7.1,"The coordinates of this line, are indicated in line_coords"
2.7.1,Turn the line coordinates into image positions
2.7.1,Turn atomic coordinates into image positions
2.7.1,Set the bond line coordinates to the bond property used.
2.7.1,Set the atom positions in image to different atomic properties in channels
2.7.1,With fixed res and img_size some molecules (e.g. long chains) may not fit.
2.7.1,Check whether num_confs >=1 or not
2.7.1,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.7.1,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.7.1,consistent with most QM software packages.
2.7.1,Dimension of atom feature vector
2.7.1,len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
2.7.1,+ 2 at end for is_in_aromatic and mass
2.7.1,dictionary of available feature generators
2.7.1,number of atoms
2.7.1,number of bonds
2.7.1,"mapping from bond index to concat(in_atom, bond) features | initial input is a zero padding"
2.7.1,mapping from atom index to list of indices of incoming bonds
2.7.1,mapping from bond index to the index of the atom the bond is coming from
2.7.1,mapping from bond index to the index of the reverse bond
2.7.1,get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
2.7.1,get bond features
2.7.1,for H2
2.7.1,"not all features are equally long, so used methane as dummy molecule to determine length"
2.7.1,Fix nans in features
2.7.1,add edge list considering a directed graph
2.7.1,get atom features
2.7.1,get edge(bond) features
2.7.1,get edge index
2.7.1,get global features
2.7.1,generate SMILES for fragments
2.7.1,Featurize data using featurize() in parent class
2.7.1,Featurize str data
2.7.1,Extend shorter strings with padding
2.7.1,Padding before and after
2.7.1,Featurize data using featurize() in parent class
2.7.1,Featurize str data
2.7.1,Featurize mol data
2.7.1,Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
2.7.1,"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
2.7.1,load pretrained models
2.7.1,convert errors to zero
2.7.1,flake8: noqa
2.7.1,If partial charges were not computed
2.7.1,construct atom (node) feature
2.7.1,construct edge (bond) index
2.7.1,add edge list considering a directed graph
2.7.1,construct edge (bond) feature
2.7.1,load_sdf_files returns pos as strings but user can also specify
2.7.1,numpy arrays for atom coordinates
2.7.1,The 1.0 float value represents True Boolean
2.7.1,This will return a boolean vector with all entries False
2.7.1,To get the shortest paths between two nodes.
2.7.1,To get info if two nodes belong to the same ring.
2.7.1,Featurizer
2.7.1,initialize
2.7.1,check initialization
2.7.1,creates normalized functions dictionary if normalized features are required
2.7.1,get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
2.7.1,get required distribution_ from `scipy.stats` module.
2.7.1,cdf => cumulative density functions
2.7.1,make the cdf with the parameters.
2.7.1,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.7.1,Check whether num_confs >=1 or not
2.7.1,Convert AtomPositions from Angstrom to bohr (atomic units)
2.7.1,"`(1, max_atoms)` -> `(max_atoms,)`"
2.7.1,bond labels
2.7.1,atom labels
2.7.1,create bond encoders and decoders
2.7.1,create atom encoders and decoders
2.7.1,Special case handling of single molecule
2.7.1,Convert iterables to list
2.7.1,Set up site environment matcher
2.7.1,Graphical option
2.7.1,tolerance for grouping nodes
2.7.1,determine minimum distance between sitetypes.
2.7.1,This is used to determine the existence of an edge
2.7.1,Sort by bond
2.7.1,You want to maximize this in order to make sure every node gets an edge
2.7.1,construct graph
2.7.1,matcher options
2.7.1,construct graph
2.7.1,Add nodes
2.7.1,Add edge. distance is edge attribute
2.7.1,construct graph
2.7.1,Gets the isomorphic mapping. Also the most time consuming part of the code
2.7.1,reconstruct graph after alinging point order
2.7.1,RMSD
2.7.1,Construct one hot encoding
2.7.1,get mapping between all site index to active site index
2.7.1,Get Neighbors
2.7.1,Read Data
2.7.1,get map between two environment
2.7.1,align input to the primitive cell (reference)
2.7.1,apply permutations
2.7.1,remove spectators
2.7.1,map it to active sites
2.7.1,Extract the right number of sites by distance
2.7.1,if PBC condition is fulfilled..
2.7.1,Get full N x N SCM
2.7.1,flake8: noqa
2.7.1,load atom_init.json
2.7.1,check whether the atom feature exists or not
2.7.1,construct bi-directed graph
2.7.1,Increase dimension of distance tensor and apply filter
2.7.1,We compute pairwise contact fingerprints
2.7.1,We compute pairwise contact fingerprints
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.1,We compute pairwise contact fingerprints
2.7.1,"rdks = [frag1[1], frag2[1]]"
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.1,We compute pairwise contact fingerprints
2.7.1,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.7.1,"rdks = [frag1[1], frag2[1]]"
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.1,We compute pairwise contact fingerprints
2.7.1,"rdks = [frag1[1], frag2[1]]"
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.7.1,We compute pairwise contact fingerprints
2.7.1,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.7.1,We compute pairwise contact fingerprints
2.7.1,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.7.1,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.7.1,"xyzs = [frag1_xyz, frag2_xyz]"
2.7.1,"rdks = [frag1[1], frag2[1]]"
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.1,We compute pairwise contact fingerprints
2.7.1,"rdks = [frag1[1], frag2[1]]"
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.1,check if user tries to set removed arguments
2.7.1,list of features that require sanitized molecules
2.7.1,not implemented featurization types
2.7.1,default values
2.7.1,update with cutoffs specified by the user
2.7.1,"each entry is a tuple (is_flat, feature_name)"
2.7.1,list of features that cannot be calculated with specified parameters
2.7.1,this list is used to define <flat/voxel/all>_combined subset
2.7.1,parse provided feature types
2.7.1,flake8: noqa
2.7.1,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.7.1,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.7.1,nonzero contact.
2.7.1,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.7.1,We compute pairwise contact fingerprints
2.7.1,Get coordinates
2.7.1,We compute pairwise contact fingerprints
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.7.1,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.7.1,axis.
2.7.1,Type of data created by this featurizer
2.7.1,TODO(rbharath): Should this return a list?
2.7.1,Type of data created by this featurizer
2.7.1,Currently handles loading failures by returning None
2.7.1,TODO: Is there a better handling procedure?
2.7.1,pad outputs
2.7.1,Deprecation warnings for old atomic conv featurizer name #
2.7.1,We compute pairwise contact fingerprints
2.7.1,Get coordinates
2.7.1,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.7.1,We compute pairwise contact fingerprints
2.7.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.1,decode the source in the mixed and separated cases
2.7.1,number of indices where feature count is more than 1
2.7.1,no normalized feature value should be greater than 1.0
2.7.1,get atom features
2.7.1,mapping from atom index to atom features | initial input is a zero padding
2.7.1,TODO test more formats for ligand
2.7.1,TODO test more formats for ligand
2.7.1,with one conformer
2.7.1,with multiple conformers
2.7.1,include explicit hydrogens
2.7.1,with one conformer
2.7.1,with multiple conformers
2.7.1,include explicit hydrogens
2.7.1,construct edge (bond) index
2.7.1,add edge list considering a directed graph
2.7.1,test for 'MolGraphConvFeaturizer' class
2.7.1,"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
2.7.1,"Requirements - transformers, tokenizers"
2.7.1,no normalized feature value should be greater than 1.0
2.7.1,"assert ""C1=CC=CN=C1"""
2.7.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.1,"assert ""C1=CC=CN=C1"""
2.7.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.1,"assert ""C1=CC=CN=C1"""
2.7.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.1,"assert ""C1=CC=CN=C1"""
2.7.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.1,Test featurizer with atom 3-D coordinates as kwargs
2.7.1,"assert ""C1=CC=CN=C1"""
2.7.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.1,"number of indices, where feature count is more than 1, should be 0"
2.7.1,number of indices where feature count is more than 1
2.7.1,check for separate count and SMILES entries for each fragment
2.7.1,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.7.1,max num atoms instead of exact.
2.7.1,Cutoff in angstroms
2.7.1,"Coords are padded, neighbor list and Z are not"
2.7.1,both reactant and product are null
2.7.1,reactant is null
2.7.1,product is null
2.7.1,valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
2.7.1,"# TODO: This is failing, something about the hydrogen bond counting?"
2.7.1,def test_hydrogen_bond_counter():
2.7.1,current_dir = os.path.dirname(os.path.realpath(__file__))
2.7.1,"protein_file = os.path.join(current_dir, 'data',"
2.7.1,'3ws9_protein_fixer_rdkit.pdb')
2.7.1,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.7.1,
2.7.1,cutoff = 4.5
2.7.1,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.7.1,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.7.1,# TODO: Add shape test
2.7.1,
2.7.1,
2.7.1,"# TODO: This is failing, something about the hydrogen bond counting?"
2.7.1,def test_hydrogen_bond_voxelizer():
2.7.1,current_dir = os.path.dirname(os.path.realpath(__file__))
2.7.1,"protein_file = os.path.join(current_dir, 'data',"
2.7.1,'3ws9_protein_fixer_rdkit.pdb')
2.7.1,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.7.1,
2.7.1,cutoff = 4.5
2.7.1,box_width = 16
2.7.1,voxel_width = 1.0
2.7.1,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.7.1,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.7.1,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.7.1,# TODO: Add shape test
2.7.1,@pytest.mark.linux_only
2.7.1,test if default parameters work
2.7.1,check if use-case from examples works
2.7.1,test if input is flattened when flat features are used
2.7.1,test voxel features
2.7.1,test flat features
2.7.1,check if aromatic features are ignored if sanitize=False
2.7.1,test flattened voxel features
2.7.1,test voxel features
2.7.1,test flat features
2.7.1,test rotations
2.7.1,not support array style inputs
2.7.1,z is kwargs
2.7.1,check convert function
2.7.1,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.7.1,of degree 1 (connected only to central nitrogen).
2.7.1,5 atoms in compound
2.7.1,Get the adjacency lists grouped by degree
2.7.1,The 4 outer atoms connected to central nitrogen
2.7.1,Central nitrogen connected to everything else.
2.7.1,Only one carbon
2.7.1,"No bonds, so degree adjacency lists are empty"
2.7.1,3 carbonds in alkane
2.7.1,Outer two carbonds are connected to central carbon
2.7.1,Central carbon connected to outer two
2.7.1,test featurization
2.7.1,test defeaturization
2.7.1,sanity check; see if something weird does not happen with rdkit
2.7.1,check if original smiles match defeaturized smiles
2.7.1,sanity check; see if something weird does not happen with rdkit
2.7.1,test featurization
2.7.1,test defeaturization
2.7.1,check if original smiles match defeaturized smiles
2.7.1,untransform
2.7.1,untranform
2.7.1,untranform
2.7.1,untranform
2.7.1,untranform
2.7.1,Check the SDF file.
2.7.1,Check the PDB file.
2.7.1,Check the SMILES string.
2.7.1,Set up tests.
2.7.1,Set up testing parameters.
2.7.1,the atom order for 'C' is same in case of canonical and original ordering
2.7.1,Do a manual distance computation and make
2.7.1,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.7.1,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.7.1,Do a manual distance computation and ensure that selected neighbor is
2.7.1,closest since we set max_num_neighbors = 1
2.7.1,Carbon
2.7.1,Test distance 1
2.7.1,Test distance 2
2.7.1,Test alkane
2.7.1,Test distance 1
2.7.1,3 self connections and 2 bonds which are both counted twice because of
2.7.1,symmetry for 7 total
2.7.1,Test distance 2
2.7.1,Everything is connected at this distance
2.7.1,Test alkane
2.7.1,Test distance infinity
2.7.1,Everything is connected at this distance
2.7.1,Test pentane
2.7.1,Test distance infinity
2.7.1,Everything is connected at this distance
2.7.1,Only one carbon
2.7.1,Test feature sizes
2.7.1,"No bonds, so only 1 pair feature (for the self interaction)"
2.7.1,Only 4 atoms
2.7.1,Test feature sizes for chirality
2.7.1,3 carbonds in alkane
2.7.1,Test feature sizes
2.7.1,Should be a 3x3 interaction grid
2.7.1,mol_list = featurizer.featurize(mols)
2.7.1,mol = mol_list[0]
2.7.1,3 carbonds in alkane
2.7.1,Test feature sizes
2.7.1,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.7.1,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.7.1,symmetry)
2.7.1,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.7.1,of degree 1 (connected only to central nitrogen).
2.7.1,import rdkit.Chem
2.7.1,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.7.1,5 atoms in compound
2.7.1,Test feature sizes
2.7.1,Should be a 3x3 interaction grid
2.7.1,Artificial feature array.
2.7.1,0 atoms of degree 0
2.7.1,0 atoms of degree 1
2.7.1,4 atoms of degree 2
2.7.1,0 atoms of degree 3
2.7.1,0 atoms of degree 4
2.7.1,0 atoms of degree 5
2.7.1,0 atoms of degree 6
2.7.1,0 atoms of degree 7
2.7.1,0 atoms of degree 8
2.7.1,0 atoms of degree 9
2.7.1,0 atoms of degree 10
2.7.1,atom 4 has 0 neighbors
2.7.1,atom 0 has 2 neighbors
2.7.1,atom 1 has 2 neighbors
2.7.1,atom 2 has 2 neighbors
2.7.1,atom 3 has 3 neighbors.
2.7.1,Verify that atom features have been sorted by atom degree.
2.7.1,Sorting is done by atom degree as before. So the ordering goes
2.7.1,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.7.1,from new position to old position is
2.7.1,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.7.1,list respects this reordering and returns correct adjacency list.
2.7.1,First example molecule
2.7.1,Artificial feature array.
2.7.1,Second example molecule
2.7.1,Third example molecule
2.7.1,Test agglomerate molecule method
2.7.1,No atoms of degree 0
2.7.1,3 atoms of degree 1
2.7.1,8 atoms of degree 2
2.7.1,1 atom of degree 3
2.7.1,0 atoms of degree 4
2.7.1,0 atoms of degree 5
2.7.1,Check that atoms are only connected to themselves.
2.7.1,Check that there's one atom of each degree.
2.7.1,calculate coordinates
2.7.1,not zero values
2.7.1,Calculate frequency
2.7.1,flake8: noqa
2.7.1,assumes that every array is of the same dimension
2.7.1,rem_dataset is remaining portion of dataset
2.7.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.7.1,to k-1.
2.7.1,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.7.1,validation
2.7.1,skip list
2.7.1,skip path string
2.7.1,main logic
2.7.1,for str
2.7.1,for list
2.7.1,dict is needed in case groups aren't strictly flattened or
2.7.1,hashed by something non-integer like
2.7.1,Figure out how many positive samples we want for each task in each dataset.
2.7.1,Assign the positive samples to datasets.  Since a sample may be positive
2.7.1,"on more than one task, we need to keep track of the effect of each added"
2.7.1,"sample on each task.  To try to keep everything balanced, we cycle through"
2.7.1,"tasks, assigning one positive sample for each one."
2.7.1,We have a sample that hasn't been assigned yet.  Assign it to
2.7.1,whichever set currently has the lowest fraction of its target for
2.7.1,this task.
2.7.1,The remaining samples are negative for all tasks.  Add them to fill out
2.7.1,each set to the correct total number.
2.7.1,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.7.1,JSG Assert that split fractions can be written as proper fractions over 10.
2.7.1,This can be generalized in the future with some common demoninator determination.
2.7.1,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.7.1,Append remaining examples to train
2.7.1,################################################################
2.7.1,Splitter for molecule datasets
2.7.1,################################################################
2.7.1,Sort by increasing MW
2.7.1,calcaulate scaffold sets
2.7.1,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.7.1,Compute fingerprints for all molecules.
2.7.1,Split into two groups: training set and everything else.
2.7.1,Split the second group into validation and test sets.
2.7.1,Begin by assigning the first molecule to the first group.
2.7.1,Return identity if no tuple to split to
2.7.1,Decide which group to assign a molecule to.
2.7.1,Identify the unassigned molecule that is least similar to everything in
2.7.1,the other group.
2.7.1,Add it to the group.
2.7.1,Update the data on unassigned molecules.
2.7.1,Sort from largest to smallest scaffold sets
2.7.1,################################################################
2.7.1,Not well supported splitters
2.7.1,################################################################
2.7.1,All datasets share features and identifiers by assumption.
2.7.1,flake8: noqa
2.7.1,basic splitter
2.7.1,molecule splitter
2.7.1,other splitter
2.7.1,################################################################
2.7.1,Removed API
2.7.1,################################################################
2.7.1,Note that the extra task goes to test
2.7.1,Number tasks per fold
2.7.1,Find the tasks that correspond to this test fold
2.7.1,Assert that all arrays look like they should
2.7.1,"task_type = ""regression"""
2.7.1,0 1 2 3 4 5 6 7 8 9
2.7.1,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.7.1,data. Make a test for properly splitting of sharded data. Perhaps using
2.7.1,reshard() to handle this?
2.7.1,Verify lengths is 10/k == 2
2.7.1,Verify that compounds in this fold are subset of original compounds
2.7.1,Verify that no two folds have overlapping compounds.
2.7.1,Verify lengths is 10/k == 2
2.7.1,Verify that compounds in this fold are subset of original compounds
2.7.1,Verify that no two folds have overlapping compounds.
2.7.1,Verify lengths is 10/k == 2
2.7.1,Verify that compounds in this fold are subset of original compounds
2.7.1,Verify that no two folds have overlapping compounds.
2.7.1,Test singletask case.
2.7.1,The split index should partition dataset in half.
2.7.1,Test singletask case.
2.7.1,Test case where some weights are zero (i.e. masked)
2.7.1,Set half the positives to have zero weight
2.7.1,There are 10 nonzero actives.
2.7.1,"The split index should partition this into half, so expect 5"
2.7.1,The split index should partition the positives for each task roughly in half.
2.7.1,Mask half the examples
2.7.1,The split index should partition dataset in half.
2.7.1,Test singletask case.
2.7.1,Should have split cleanly in half (picked random seed to ensure this)
2.7.1,Check positives are correctly distributed
2.7.1,Test singletask case.
2.7.1,Should have made an 80/10/10 train/valid/test split of actives.
2.7.1,Verify lengths is 100/k == 20
2.7.1,Note: This wouldn't work for multitask str
2.7.1,assert len(fold_dataset) == n_samples/K
2.7.1,Verify that each fold has n_positives/K = 4 positive examples.
2.7.1,Verify that compounds in this fold are subset of original compounds
2.7.1,Verify that no two folds have overlapping compounds.
2.7.1,The amount of datapoints has to be the same
2.7.1,The number of scaffolds generated by the splitter
2.7.1,has to be smaller or equal than number of total molecules
2.7.1,edges logits used during training
2.7.1,nodes logits used during training
2.7.1,edges logits
2.7.1,nodes logits
2.7.1,training of the model
2.7.1,generating compounds
2.7.1,nodes logits used during compound generation
2.7.1,Create the inputs.
2.7.1,Create the generators.
2.7.1,Create the discriminators.
2.7.1,Compute the loss functions.
2.7.1,Create learnable weights for the generators and discriminators.
2.7.1,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.7.1,Compute the weighted errors
2.7.1,Add an entropy term to the loss.
2.7.1,Create the Keras model.
2.7.1,"Every call to fit_generator() will increment global_step, but we only"
2.7.1,"want it to get incremented once for the entire batch, so record the"
2.7.1,value and keep resetting it.
2.7.1,Train the discriminator.
2.7.1,Train the generator.
2.7.1,Write checkpoints and report progress.
2.7.1,Write out final results.
2.7.1,Chain of flows is also a normalizing flow
2.7.1,An instance of tfd.TransformedDistribution
2.7.1,TODO: Incompability between TF and TFP means that TF doesn't track
2.7.1,trainable variables in the flow; must override `_create_gradient_fn`
2.7.1,self._variables = self.flow.trainable_variables
2.7.1,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.7.1,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.7.1,This is for API consistency
2.7.1,extended one of probabilites to binary distribution
2.7.1,extended one of probabilites to binary distribution
2.7.1,-*- coding: utf-8 -*-
2.7.1,"Shape (N_atoms, M_nbrs, ndim)"
2.7.1,"Shape (N_atoms, M_nbrs, ndim)"
2.7.1,"Shape (N_atoms, M_nbrs)"
2.7.1,Generate the nb_affine weights and biases
2.7.1,Extract atom_features
2.7.1,Extract graph topology
2.7.1,Sum all neighbors using adjacency matrix
2.7.1,Get collection of modified atom features
2.7.1,Obtain relevant atoms for this degree
2.7.1,Get self atoms
2.7.1,Apply hidden affine to relevant atoms and append
2.7.1,Determine the min_deg=0 case
2.7.1,Only use the self layer
2.7.1,Combine all atoms back into the list
2.7.1,Tensorflow correctly processes empty lists when using concat
2.7.1,"Sum along neighbors as well as self, and store"
2.7.1,Perform the mol gather
2.7.1,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.7.1,"self.max_degree, self.min_degree)"
2.7.1,Tensorflow correctly processes empty lists when using concat
2.7.1,Get self atoms
2.7.1,"There are no neighbors of this degree, so just create an empty tensor directly."
2.7.1,Expand dims
2.7.1,always deg-1 for deg_adj_lists
2.7.1,Extract graph topology
2.7.1,means that this is second loop of convolution
2.7.1,No other forget biases supported right now.
2.7.1,Taken from Keras code [citation needed]
2.7.1,"x is test set, xp is support set."
2.7.1,Get initializations
2.7.1,Process using attention
2.7.1,"Eqn (4), appendix A.1 of Matching Networks paper"
2.7.1,Generate new attention states
2.7.1,Support set lstm
2.7.1,Test lstm
2.7.1,Get initializations
2.7.1,Rename support
2.7.1,Process support xp using attention
2.7.1,Get linear combination of support set
2.7.1,Process test x using attention
2.7.1,Generate new support attention states
2.7.1,Generate new test attention states
2.7.1,Redefine
2.7.1,Number of rotatable bonds
2.7.1,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.7.1,a difference.
2.7.1,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.7.1,neighbors lists an argument instead of a part of this layer.
2.7.1,"Shape (N, M)"
2.7.1,"Shape (N, M)"
2.7.1,"Shape (N, M)"
2.7.1,Number of grid cells
2.7.1,TODO(rbharath): Support batching
2.7.1,"Shape (n_cells, ndim)"
2.7.1,"List of length N_atoms, each element of different length uniques_i"
2.7.1,"List of length N_atoms, each element of different length uniques_i"
2.7.1,"List of length N_atoms, each a tensor of shape"
2.7.1,"(uniques_i, ndim)"
2.7.1,Add phantom atoms that exist far outside the box
2.7.1,"List of length N_atoms, each of shape (1, ndim)"
2.7.1,TODO(rbharath): How does distance need to be modified here to
2.7.1,account for periodic boundary conditions?
2.7.1,List of length N_atoms each of shape (M_nbrs)
2.7.1,"N_atoms elts of size (M_nbrs,) each"
2.7.1,"Shape (N_atoms, 1)"
2.7.1,Find M_nbrs atoms closest to each cell
2.7.1,"Shape (n_cells, M_nbrs)"
2.7.1,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.1,"conditions, so does wrapround. O(constant)"
2.7.1,"Shape (n_cells, n_nbr_cells)"
2.7.1,"Shape (N_atoms, n_nbr_cells)"
2.7.1,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.7.1,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.7.1,"List of length N_atoms, each element length uniques_i"
2.7.1,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.7.1,element removed to remove self from list of neighbors. Need to verify
2.7.1,this holds more broadly or come up with robust alternative.
2.7.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.1,"Shape (N_atoms*n_cells, ndim) after tile"
2.7.1,Shape (N_atoms*n_cells)
2.7.1,"Shape (n_cells, N_atoms)"
2.7.1,Find k atoms closest to this cell. Notice negative sign since
2.7.1,tf.nn.top_k returns *largest* not smallest.
2.7.1,"Tensor of shape (n_cells, M_nbrs)"
2.7.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.1,"Shape (N_atoms*n_cells, 1) after tile"
2.7.1,9 neighbors in 2-space
2.7.1,TODO(rbharath): Shoddy handling of higher dimensions...
2.7.1,Number of cells for cube in 3-space is
2.7.1,TODO(rbharath): Do we need to handle periodic boundary conditions
2.7.1,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.7.1,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.7.1,the cube.
2.7.1,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.7.1,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.7.1,"Tile (a, a, a, b, b, b, etc.)"
2.7.1,"Tile (a, b, c, a, b, c, ...)"
2.7.1,N: Maximum number of atoms
2.7.1,M: Maximum number of neighbors
2.7.1,d: Number of coordinates/features/filters
2.7.1,B: Batch Size
2.7.1,Compute the distances and radial symmetry functions.
2.7.1,check that there isnt just one or zero inputs
2.7.1,create subspaces
2.7.1,"concatenate subspaces, reshape to size of original input, then stack"
2.7.1,"such that out_tensor has shape (2,?,original_cols)"
2.7.1,creates subspaces the same way it was done in AlphaShare
2.7.1,calculate squared Frobenius norm
2.7.1,"(TODO YTZ:) faster, less memory intensive way"
2.7.1,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.7.1,"r = tf.expand_dims(r, -1)"
2.7.1,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.7.1,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.7.1,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.7.1,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.7.1,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.7.1,Calculate pairwise distance
2.7.1,Cutoff with threshold Rc
2.7.1,return d
2.7.1,tf.stack issues again...
2.7.1,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.7.1,So the Tensor has known dimensions
2.7.1,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.7.1,normalization
2.7.1,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.7.1,and embeddings of atom j(both gone through a hidden layer)
2.7.1,"for atom i, sum the influence from all other atom j in the molecule"
2.7.1,number of inputs each step
2.7.1,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.7.1,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.7.1,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.7.1,`count`-th step
2.7.1,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.7.1,generating index for graph features used in the inputs
2.7.1,"extracting graph features for parents of the target atoms, then flatten"
2.7.1,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.7.1,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.7.1,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.7.1,of shape: (batch_size*max_atoms) * n_graph_features
2.7.1,representing the graph features of target atoms in each graph
2.7.1,index for targe atoms
2.7.1,Extract atom_features
2.7.1,sum all graph outputs
2.7.1,"Default message function: edge network, update function: GRU"
2.7.1,more options to be implemented
2.7.1,Add another value(~-Inf) to prevent error in softmax
2.7.1,Model using this layer must set pad_batches=True
2.7.1,Perform one step of LSTM
2.7.1,task_metadata_rows = {task: [] for task in tasks}
2.7.1,Extract those datapoints which are present for this task
2.7.1,Loading is done on-the-fly
2.7.1,Build the model.
2.7.1,Final atom-layer convolution. Note this differs slightly from the paper
2.7.1,since we use a tanh activation as default. This seems necessary for numerical
2.7.1,stability.
2.7.1,Now fully connected layers
2.7.1,Should this allow for training?
2.7.1,"pair_edges is of shape (2, N)"
2.7.1,number of atoms in each molecule
2.7.1,index of pair features
2.7.1,Get starting pair atoms
2.7.1,number of pairs for each atom
2.7.1,atom features
2.7.1,pair features
2.7.1,Build the model.
2.7.1,Build the model.
2.7.1,calculation orders for a batch of molecules
2.7.1,padding atom features vector of each molecule with 0
2.7.1,Build the model.
2.7.1,number of atoms in each molecule
2.7.1,index of pair features
2.7.1,number of pairs for each atom
2.7.1,atom features
2.7.1,pair features
2.7.1,################### Deprecation warnings for renamed TensorGraph models ####################
2.7.1,Add the input features.
2.7.1,Add the shared dense layers
2.7.1,Add task-specific bypass layers
2.7.1,Add the input features.
2.7.1,Add the shared dense layers
2.7.1,Add task-specific bypass layers
2.7.1,W&B flag support (DEPRECATED)
2.7.1,"If `wandb=True` and no logger is provided, initialize default logger"
2.7.1,Setup and initialize W&B logging
2.7.1,Update config with KerasModel params
2.7.1,Backwards compatibility
2.7.1,The optimizer creates internal variables the first time apply_gradients()
2.7.1,is called for a new set of variables.  If that happens inside a function
2.7.1,"annotated with tf.function it throws an exception, so call it once here."
2.7.1,Main training loop.
2.7.1,"Execute the loss function, accumulating the gradients."
2.7.1,Report progress and write checkpoints.
2.7.1,Capture the last avg_loss in case of return since we're resetting to
2.7.1,0 now
2.7.1,Report final results.
2.7.1,Invoke the model.
2.7.1,Apply tranformers and record results.
2.7.1,Concatenate arrays to create the final results.
2.7.1,Use a GradientTape to compute gradients.
2.7.1,Ensure weights for both models are built.
2.7.1,Define the PyTorch Module that implements the model.
2.7.1,Define the PyTorch Module that implements the model.
2.7.1,Run fit transformers on dummy dataset to determine n_features after transformation
2.7.1,set wandb init arguments
2.7.1,Dataset ids are used to differentiate datasets seen by the logger
2.7.1,log data
2.7.1,Similarity values
2.7.1,Labels for all top K similar samples
2.7.1,Discard any padded predictions
2.7.1,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.7.1,Build the model.
2.7.1,Character embedding
2.7.1,Multiple convolutional layers with different filter widths
2.7.1,Max-over-time pooling
2.7.1,Concat features from all filters(one feature per filter)
2.7.1,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.7.1,SMILES strings
2.7.1,Maximum length is expanded to allow length variation during train and inference
2.7.1,'_' served as delimiter and padding
2.7.1,Initialize common characters as keys
2.7.1,Include space to avoid extra keys
2.7.1,"For 'Cl', 'Br', etc."
2.7.1,"Character not recognized, add to extra_keys"
2.7.1,Add all extra_keys to char_dict
2.7.1,Transform SMILES sequence to integers
2.7.1,Skip all spaces
2.7.1,"For 'Cl', 'Br', etc."
2.7.1,Padding with '_'
2.7.1,################### Deprecation warnings for renamed TensorGraph models ####################
2.7.1,"layer_sizes=[32, 32, 16],"
2.7.1,Add the dense layers
2.7.1,Do a simple greedy search.
2.7.1,Do a beam search with length normalization.
2.7.1,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.7.1,This candidate sequence has already been terminated
2.7.1,Consider all possible tokens we could add to this candidate sequence.
2.7.1,Add the input features.
2.7.1,Handle output layer
2.7.1,Iterate over all previous tasks.
2.7.1,prev_layers is a list with elements of size
2.7.1,"(batch_size, layer_sizes[i-1])"
2.7.1,Log data to Wandb
2.7.1,flake8: noqa
2.7.1,Tensorflow Dependency Models
2.7.1,scikit-learn model
2.7.1,PyTorch models
2.7.1,Pytorch models with torch-geometric dependency
2.7.1,TODO We should clean up DMPNN and remove torch_geometric dependency during import
2.7.1,Pytorch-lightning modules import
2.7.1,Jax models
2.7.1,####################################################################################
2.7.1,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.7.1,####################################################################################
2.7.1,#######################################################################################
2.7.1,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.7.1,#######################################################################################
2.7.1,Last layer sequences not returned.
2.7.1,This is needed because ImageDataGenerator does infinite looping
2.7.1,"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
2.7.1,but turns out to be slightly faster
2.7.1,JAX depend
2.7.1,Main training loop
2.7.1,Capture the last avg_loss in case of return since we're resetting to 0 now
2.7.1,Report final results.
2.7.1,Apply tranformers and record results.
2.7.1,Concatenate arrays to create the final results.
2.7.1,"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
2.7.1,") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
2.7.1,""""""""
2.7.1,"Predict the model's outputs, along with the uncertainty in each one."
2.7.1,The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
2.7.1,It involves repeating the prediction many times with different dropout masks.
2.7.1,The prediction is computed as the average over all the predictions.  The
2.7.1,uncertainty includes both the variation among the predicted values (epistemic
2.7.1,uncertainty) and the model's own estimates for how well it fits the data
2.7.1,(aleatoric uncertainty).  Not all models support uncertainty prediction.
2.7.1,Parameters
2.7.1,----------
2.7.1,dataset: dc.data.Dataset
2.7.1,Dataset to make prediction on
2.7.1,masks: int
2.7.1,the number of dropout masks to average over
2.7.1,Returns
2.7.1,-------
2.7.1,"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
2.7.1,"value of the output, and each element of y_std estimates the standard"
2.7.1,deviation of the corresponding element of y_pred
2.7.1,""""""""
2.7.1,sum_pred: List[np.ndarray] = []
2.7.1,sum_sq_pred: List[np.ndarray] = []
2.7.1,sum_var: List[np.ndarray] = []
2.7.1,for i in range(masks):
2.7.1,generator = self.default_generator(
2.7.1,"dataset, mode='uncertainty', pad_batches=False)"
2.7.1,"results = self._predict(generator, [], True, None)"
2.7.1,if len(sum_pred) == 0:
2.7.1,"for p, v in results:"
2.7.1,sum_pred.append(p)
2.7.1,sum_sq_pred.append(p * p)
2.7.1,sum_var.append(v)
2.7.1,else:
2.7.1,"for j, (p, v) in enumerate(results):"
2.7.1,sum_pred[j] += p
2.7.1,sum_sq_pred[j] += p * p
2.7.1,sum_var[j] += v
2.7.1,output = []
2.7.1,std = []
2.7.1,for i in range(len(sum_pred)):
2.7.1,p = sum_pred[i] / masks
2.7.1,output.append(p)
2.7.1,std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
2.7.1,if len(output) == 1:
2.7.1,"return (output[0], std[0])"
2.7.1,else:
2.7.1,"return list(zip(output, std))"
2.7.1,JAX dependencies
2.7.1,Main training loop
2.7.1,Capture the last avg_loss in case of return since we're resetting to 0 now
2.7.1,Report final results.
2.7.1,Apply tranformers and record results.
2.7.1,Concatenate arrays to create the final results.
2.7.1,flake8:noqa
2.7.1,The PINNModel requires you to create two functions
2.7.1,`create_eval`_fn for letting the model know how to compute the model in inference and
2.7.1,`gradient_fn` for letting model know how to compute the gradient and different regulariser
2.7.1,equation loss depending on the differential equation
2.7.1,defining the Haiku model
2.7.1,"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
2.7.1,"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
2.7.1,which will be used as the differential loss(regulariser)
2.7.1,The expected solution must be as close to cos(x)
2.7.1,Initialize the weights with random values
2.7.1,Forward function which takes the params
2.7.1,Loss Function
2.7.1,JaxModel Working
2.7.1,sample network
2.7.1,Model Initialization
2.7.1,Loss Function
2.7.1,JaxModel Working
2.7.1,sample network
2.7.1,Model Initilisation
2.7.1,Loss Function
2.7.1,JaxModel Working
2.7.1,Model Initilisation
2.7.1,Loss Function
2.7.1,JaxModel Working
2.7.1,Model Initilisation
2.7.1,Loss Function
2.7.1,JaxModel Working
2.7.1,Model Initilisation
2.7.1,Loss Function
2.7.1,JaxModel Working
2.7.1,Each epoch is a single step for this model
2.7.1,@pytest.mark.jax
2.7.1,@pytest.mark.slow
2.7.1,def test_uncertainty():
2.7.1,"""""""Test estimating uncertainty a TorchModel."""""""
2.7.1,n_samples = 30
2.7.1,n_features = 1
2.7.1,noise = 0.1
2.7.1,"X = np.random.rand(n_samples, n_features)"
2.7.1,"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
2.7.1,"dataset = dc.data.NumpyDataset(X, y)"
2.7.1,class Net(hk.Module):
2.7.1,"def __init__(self, output_size: int = 1):"
2.7.1,super().__init__()
2.7.1,"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.7.1,"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.7.1,self.output = hk.Linear(output_size)
2.7.1,self.log_var = hk.Linear(output_size)
2.7.1,"def __call__(self, x):"
2.7.1,x = self._network1(x)
2.7.1,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.7.1,x = self._network2(x)
2.7.1,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.7.1,output = self.output(x)
2.7.1,log_var = self.log_var(x)
2.7.1,var = jnp.exp(log_var)
2.7.1,"return output, var, output, log_var"
2.7.1,def f(x):
2.7.1,net = Net(1)
2.7.1,return net(x)
2.7.1,"def loss(outputs, labels, weights):"
2.7.1,diff = labels[0] - outputs[0]
2.7.1,log_var = outputs[1]
2.7.1,var = jnp.exp(log_var)
2.7.1,return jnp.mean(diff * diff / var + log_var)
2.7.1,class UncertaintyModel(JaxModel):
2.7.1,"def default_generator(self,"
2.7.1,"dataset,"
2.7.1,"epochs=1,"
2.7.1,"mode='fit',"
2.7.1,"deterministic=True,"
2.7.1,pad_batches=True):
2.7.1,for epoch in range(epochs):
2.7.1,"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
2.7.1,"batch_size=self.batch_size,"
2.7.1,"deterministic=deterministic,"
2.7.1,pad_batches=pad_batches):
2.7.1,"yield ([X_b], [y_b], [w_b])"
2.7.1,jm_model = hk.transform(f)
2.7.1,rng = jax.random.PRNGKey(500)
2.7.1,"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
2.7.1,modified_inputs = jnp.array(
2.7.1,[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
2.7.1,"params = jm_model.init(rng, modified_inputs)"
2.7.1,model = UncertaintyModel(
2.7.1,"jm_model.apply,"
2.7.1,"params,"
2.7.1,"loss,"
2.7.1,"output_types=['prediction', 'variance', 'loss', 'loss'],"
2.7.1,learning_rate=0.003)
2.7.1,"model.fit(dataset, nb_epochs=2500)"
2.7.1,"pred, std = model.predict_uncertainty(dataset)"
2.7.1,assert np.mean(np.abs(y - pred)) < 2.0
2.7.1,assert noise < np.mean(std) < 1.0
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,Conv2d and Linear layers test(CNN classification)
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,test if adjacency matrix input is correctly set
2.7.1,test if nodes features matrix input is correctly set
2.7.1,check discriminator shape
2.7.1,check training edges logits shape
2.7.1,check training nodes logits shapes
2.7.1,True will be assigned up successful training attempt
2.7.1,force clear tensor flow backend
2.7.1,create new model
2.7.1,to avoid flake8 E125/yapf incompatibility
2.7.1,generate input
2.7.1,train model
2.7.1,generate sample
2.7.1,check how many valid molecules were created and add to list
2.7.1,finally test if there was at least one valid training session
2.7.1,as the model structure improves this should become more and more strict
2.7.1,Predict the output and uncertainty.
2.7.1,predict datset with no y (ensured by tasks = [])
2.7.1,Predict the output and uncertainty.
2.7.1,The DAG models have high error with dropout
2.7.1,"Despite a lot of effort tweaking it , there appears to be"
2.7.1,a limit to how low the error can go with dropout.
2.7.1,assert mean_error < 0.5 * mean_value
2.7.1,Predict the output and uncertainty.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,load datasets
2.7.1,disable transformer
2.7.1,check train
2.7.1,check predict shape
2.7.1,check overfit
2.7.1,load datasets
2.7.1,disable transformer
2.7.1,check train
2.7.1,check predict shape
2.7.1,check overfit
2.7.1,load datasets
2.7.1,disable transformer
2.7.1,check train
2.7.1,check predict shape
2.7.1,check overfit
2.7.1,reload
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Check same predictions are made.
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Load trained model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload trained model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Reload trained model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload trained model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reloaded Trained Model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Check predictions match on random sample
2.7.1,3D Multivariate Gaussian base distribution
2.7.1,Check that reloaded model can sample from the distribution
2.7.1,Check that density estimation is same for reloaded model
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload trained model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload Trained Model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload Trained Model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload trained model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Check predictions match on random sample
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload trained model
2.7.1,Eval model on train
2.7.1,Check predictions match on random sample
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload trained model
2.7.1,Eval model on train
2.7.1,Check predictions match on random sample
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Reload trained model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Reload trained Model
2.7.1,Check predictions match on random sample
2.7.1,Eval model on train
2.7.1,Reload Trained Model
2.7.1,Check predictions match on random sample
2.7.1,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.7.1,Reload Trained Model
2.7.1,Check predictions match on original dataset
2.7.1,TODO: We need a cleaner usage example for this
2.7.1,Fit trained model
2.7.1,Check predictions match on random sample
2.7.1,Train the model on random sequences.  We aren't training long enough to
2.7.1,"really make it reliable, but I want to keep this test fast, and it should"
2.7.1,still be able to reproduce a reasonable fraction of input sequences.
2.7.1,Test it out.
2.7.1,check predict shape
2.7.1,check overfit
2.7.1,needs change
2.7.1,check predict shape
2.7.1,check overfit
2.7.1,reload
2.7.1,The first pass of the transformation should be 0
2.7.1,Test sampling method
2.7.1,Test log_prob method (this method is used when inverse pass)
2.7.1,Output must be a Nth zero array since nothing is being learned yet
2.7.1,Featurize to assert for tests
2.7.1,Assert errors for sample method
2.7.1,Assert errors for log_prob method
2.7.1,get data
2.7.1,prepare batch (size 1)
2.7.1,initialize the model
2.7.1,get output
2.7.1,get data
2.7.1,prepare batch (size 1)
2.7.1,initialize the model
2.7.1,get output
2.7.1,get data
2.7.1,prepare batch (size 1)
2.7.1,initialize the model
2.7.1,get output
2.7.1,load sample dataset
2.7.1,initialize the model
2.7.1,overfit test
2.7.1,load sample dataset
2.7.1,initialize the model
2.7.1,overfit test
2.7.1,load sample dataset
2.7.1,initialize the model
2.7.1,fit the model
2.7.1,reload the model
2.7.1,There are 4 atoms each of which have 75 atom features
2.7.1,There are 10 pairs with infinity distance and 14 pair features
2.7.1,4 atoms in total
2.7.1,10 pairs in total
2.7.1,10 pairs in total each with start/finish
2.7.1,There are 4 atoms each of which have 75 atom features
2.7.1,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.7.1,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.7.1,connections and accounting for symmetry.)
2.7.1,4 atoms in total
2.7.1,10 pairs in total
2.7.1,The center atom is self connected and to both neighbors so it appears
2.7.1,thrice. The canonical ranking used in MolecularFeaturizer means this
2.7.1,central atom is ranked last in ordering.
2.7.1,10 pairs in total each with start/finish
2.7.1,def test_weave_fit_simple_infinity_distance():
2.7.1,featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
2.7.1,"X = featurizer([""C"", ""CCC""])"
2.7.1,"y = np.array([0, 1.])"
2.7.1,"dataset = dc.data.NumpyDataset(X, y)"
2.7.1,batch_size = 20
2.7.1,model = WeaveModel(
2.7.1,"1,"
2.7.1,"batch_size=batch_size,"
2.7.1,"mode='classification',"
2.7.1,"fully_connected_layer_sizes=[2000, 1000],"
2.7.1,"batch_normalize=True,"
2.7.1,batch_normalize_kwargs={
2.7.1,"""fused"": False,"
2.7.1,"""trainable"": True,"
2.7.1,"""renorm"": True"
2.7.1,"},"
2.7.1,learning_rate=0.0005)
2.7.1,"model.fit(dataset, nb_epoch=200)"
2.7.1,transformers = []
2.7.1,metric = dc.metrics.Metric(
2.7.1,"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
2.7.1,"scores = model.evaluate(dataset, [metric], transformers)"
2.7.1,assert scores['mean-roc_auc_score'] >= 0.9
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,load datasets
2.7.1,initialize model
2.7.1,overfit test
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Fit trained model
2.7.1,Predict the output and uncertainty.
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,xgboost test
2.7.1,fit trained model
2.7.1,eval model on test
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,lightgbm test
2.7.1,fit trained model
2.7.1,eval model on test
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,xgboost test
2.7.1,fit trained model
2.7.1,eval model on test
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,lightgbm test
2.7.1,fit trained model
2.7.1,eval model on test
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,xgboost test
2.7.1,fit trained model
2.7.1,eval model on test
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,lightgbm test
2.7.1,fit trained model
2.7.1,eval model on test
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,xgboost test
2.7.1,fit trained model
2.7.1,reload
2.7.1,check predictions match on test dataset
2.7.1,eval model on test
2.7.1,prepare dataset
2.7.1,global setting
2.7.1,lightgbm test
2.7.1,fit trained model
2.7.1,reload
2.7.1,check predictions match on test dataset
2.7.1,eval model on test
2.7.1,"For simplicity, let's assume both molecules have same number of"
2.7.1,atoms.
2.7.1,Creates a set of dummy features that contain the coordinate and
2.7.1,neighbor-list features required by the AtomicConvModel.
2.7.1,Creates a set of dummy features that contain the coordinate and
2.7.1,neighbor-list features required by the AtomicConvModel.
2.7.1,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.7.1,max num atoms instead of exact.
2.7.1,Cutoff in angstroms
2.7.1,arbitrary label
2.7.1,Run a fitting operation
2.7.1,Testing graphnet for a single graph
2.7.1,Testing for consistency
2.7.1,Testing with a batch of Graphs
2.7.1,"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
2.7.1,the above import raises a ModuleNotFoundError. It is safe to ignore it
2.7.1,since the below tests only run in an environment with pytorch installed.
2.7.1,TODO The test is skipped as FakeGraphGenerator has to be updated
2.7.1,to generate regression labels
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Fit trained model
2.7.1,Eval model on train/test
2.7.1,Fit trained model
2.7.1,Eval model on train/test
2.7.1,Fit trained model
2.7.1,Eval model on train/test
2.7.1,See if it has done a plausible job of learning the distribution.
2.7.1,See if it has done a plausible job of learning the distribution.
2.7.1,See if it has done a plausible job of learning the distribution.
2.7.1,No training has been done after reload
2.7.1,See if it has done a plausible job of learning the distribution.
2.7.1,We have to set the gradient penalty very small because the generator's
2.7.1,"output is only a single number, so the default penalty would constrain"
2.7.1,it far too much.
2.7.1,See if it has done a plausible job of learning the distribution.
2.7.1,We have to set the gradient penalty very small because the generator's
2.7.1,"output is only a single number, so the default penalty would constrain"
2.7.1,it far too much.
2.7.1,See if it has done a plausible job of learning the distribution.
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,n_samples = 100
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Most weights should be close to zero.
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Most weights should be close to zero.
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Predict the output and uncertainty.
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Load mini log-solubility dataset.
2.7.1,Fit trained model
2.7.1,Eval model on train
2.7.1,Check that predicting internal layers works.
2.7.1,Each epoch is a single step for this model
2.7.1,Create two models using the same model directory.
2.7.1,Check that they produce different results.
2.7.1,"Save a checkpoint from the first model and load it into the second one,"
2.7.1,and make sure they now match.
2.7.1,Train a model to overfit the dataset.
2.7.1,"Create an identical model, do a single step of fitting with restore=True,"
2.7.1,and make sure it got restored correctly.
2.7.1,Build a model that predicts uncertainty.
2.7.1,Fit the model and see if its predictions are correct.
2.7.1,Take a tiny step in the direction of s and see if the output changes by
2.7.1,the expected amount.
2.7.1,Load dataset and Models
2.7.1,call model.fit again to test multiple fit() calls
2.7.1,Set up tests.
2.7.1,def test_singletask_to_multitask_classification(self):
2.7.1,n_features = 10
2.7.1,n_tasks = 17
2.7.1,tasks = range(n_tasks)
2.7.1,# Define train dataset
2.7.1,n_train = 100
2.7.1,"X_train = np.random.rand(n_train, n_features)"
2.7.1,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.7.1,w_train = np.ones_like(y_train)
2.7.1,"ids_train = [""C""] * n_train"
2.7.1,train_dataset = dc.data.DiskDataset.from_numpy(
2.7.1,"X_train, y_train, w_train, ids_train)"
2.7.1,# Define test dataset
2.7.1,n_test = 10
2.7.1,"X_test = np.random.rand(n_test, n_features)"
2.7.1,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.7.1,w_test = np.ones_like(y_test)
2.7.1,"ids_test = [""C""] * n_test"
2.7.1,test_dataset = dc.data.DiskDataset.from_numpy(
2.7.1,"X_test, y_test, w_test, ids_test)"
2.7.1,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.7.1,def model_builder(model_dir):
2.7.1,sklearn_model = LogisticRegression()
2.7.1,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.7.1,multitask_model = dc.models.SingletaskToMultitask(
2.7.1,"tasks, model_builder)"
2.7.1,# Fit trained model
2.7.1,multitask_model.fit(train_dataset)
2.7.1,multitask_model.save()
2.7.1,# Eval multitask_model on train/test
2.7.1,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.7.1,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.7.1,Generate data
2.7.1,Cleanup
2.7.1,test for the prepare_input_stream function of Ferminet class
2.7.1,ionic charge initialization test
2.7.1,Train the model while logging the validation ROC AUC.
2.7.1,Parse the log to pull out the AUC scores.
2.7.1,The last reported score should match the current performance of the model.
2.7.1,The highest recorded score should match get_best_score().
2.7.1,Reload the save model and confirm that it matches the best logged score.
2.7.1,Make sure get_best_score() still works when save_dir is not specified
2.7.1,3D Multivariate Gaussian base distribution
2.7.1,Must be float32 for RealNVP
2.7.1,Tests a simple flow of one RealNVP layer.
2.7.1,log likelihoods should be negative
2.7.1,# Fit model
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,x and y are the same tensor (equivalent at every element)
2.7.1,the pairwise inner product of the rows in x and y will always be 1
2.7.1,"the output tensor will be of shape (5,5)"
2.7.1,each row in x1 is orthogonal to each row in x2
2.7.1,the pairwise inner product of the rows in x and y will always be 0
2.7.1,"the output tensor will be of shape (256,256)"
2.7.1,"Creating a second layer should produce different results, since it has"
2.7.1,different random weights.
2.7.1,But evaluating the first layer again should produce the same result as before.
2.7.1,index of pair features
2.7.1,number of pairs for each atom
2.7.1,atom features
2.7.1,pair features
2.7.1,"Outputs should be [A, P]"
2.7.1,atom features
2.7.1,Try without compression
2.7.1,"Outputs should be [mol1_vec, mol2_vec)"
2.7.1,Try with compression
2.7.1,"Outputs should be [mol1_vec, mol2_vec)"
2.7.1,atom features
2.7.1,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.7.1,Gaussian histograms expands into 11 Gaussian buckets.
2.7.1,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.7.1,TODO What should shape[1] be?  It's not documented.
2.7.1,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.7.1,"Creating a second layer should produce different results, since it has"
2.7.1,different random weights.
2.7.1,But evaluating the first layer again should produce the same result as before.
2.7.1,"Creating a second layer should produce different results, since it has"
2.7.1,different random weights.
2.7.1,But evaluating the first layer again should produce the same result as before.
2.7.1,"Creating a second layer should produce different results, since it has"
2.7.1,different random weights.
2.7.1,But evaluating the first layer again should produce the same result as before.
2.7.1,"TODO What should the output shape be?  It's not documented, and there"
2.7.1,are no other test cases for it.
2.7.1,"Creating a second layer should produce different results, since it has"
2.7.1,different random weights.
2.7.1,But evaluating the first layer again should produce the same result as before.
2.7.1,"Creating a second layer should produce different results, since it has"
2.7.1,different random weights.
2.7.1,But evaluating the first layer again should produce the same result as before.
2.7.1,"Recall that the DAG layer expects a MultiConvMol as input,"
2.7.1,"so the ""batch"" is a pooled set of atoms from all the"
2.7.1,"molecules in the batch, just as it is for the graph conv."
2.7.1,This means that n_atoms is the batch-size
2.7.1,dropout_switch = False
2.7.1,dropout_switch
2.7.1,# TODO(rbharath): What is the shape of outputs supposed to be?
2.7.1,"# I'm getting (7, 30) here. Where does 7 come from??"
2.7.1,TODO(rbharath): We need more documentation about why
2.7.1,these numbers work.
2.7.1,"By setting the `box_size` to effectively zero, the result should only contain `nan`."
2.7.1,Check that layer has three trainable parameters.
2.7.1,Check when `box_size` is of wrong dimensionality.
2.7.1,Check when `inputs` is of wrong length.
2.7.1,Create a dataset and an input function for processing it.
2.7.1,Create a dataset and an input function for processing it.
2.7.1,Generate dummy dataset
2.7.1,Fit trained model
2.7.1,Eval model on test
2.7.1,Eval model on train
2.7.1,Fit trained model
2.7.1,Eval model on test
2.7.1,Fit trained model
2.7.1,Eval model on test
2.7.1,Fit trained model
2.7.1,Eval model on test
2.7.1,Fit trained model
2.7.1,Eval model on test
2.7.1,Each epoch is a single step for this model
2.7.1,Create two models using the same model directory.
2.7.1,Check that they produce different results.
2.7.1,"Save a checkpoint from the first model and load it into the second one,"
2.7.1,and make sure they now match.
2.7.1,Train a model to overfit the dataset.
2.7.1,"Create an identical model, do a single step of fitting with restore=True,"
2.7.1,and make sure it got restored correctly.
2.7.1,Build a model that predicts uncertainty.
2.7.1,Fit the model and see if its predictions are correct.
2.7.1,Take a tiny step in the direction of s and see if the output changes by
2.7.1,the expected amount.
2.7.1,Load dataset and Models
2.7.1,call model.fit again to test multiple fit() calls
2.7.1,Train the model on random sequences.  We aren't training long enough to
2.7.1,"really make it reliable, but I want to keep this test fast, and it should"
2.7.1,still be able to reproduce a reasonable fraction of input sequences.
2.7.1,Test it out.
2.7.1,Check that it got at least a quarter of them correct.
2.7.1,Test it out.
2.7.1,Actually training a VAE takes far too long for a unit test.  Just run a
2.7.1,"few steps of training to make sure nothing crashes, then check that the"
2.7.1,results are at least internally consistent.
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,overfit test
2.7.1,test on a small MoleculeNet dataset
2.7.1,load datasets
2.7.1,initialize models
2.7.1,Initialize buffers
2.7.1,Accumulate statistics for Fisher matrices
2.7.1,Initialize buffers
2.7.1,p_grad_mat is of output_dim * input_dim
2.7.1,inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
2.7.1,we always put gradient w.r.t weight in [0]
2.7.1,and w.r.t bias in [1]
2.7.1,do kl clip
2.7.1,PyTorch layers require input and output channels as parameter
2.7.1,"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
2.7.1,"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
2.7.1,initializing layer bias with nn.init gives mypy typecheck error
2.7.1,using the following workaround
2.7.1,residual blocks can only be used when successive layers have the same output shape
2.7.1,Used for converting edges back to their original shape
2.7.1,Compute mean edge features for each node by dst_index (each node
2.7.1,"receives information from edges which have that node as its destination,"
2.7.1,hence the computation uses dst_index to aggregate information)
2.7.1,holding bi-directional edges in case of undirected graphs
2.7.1,coonverting edge features to its original shape
2.7.1,Input
2.7.1,Shared weight matrix across depths (default):
2.7.1,For messages hidden states
2.7.1,For atom hidden states
2.7.1,num_atoms x hidden_size
2.7.1,num_molecules x hidden_size
2.7.1,concat global features
2.7.1,"Shape (N_atoms, M_nbrs, ndim)"
2.7.1,"Shape (N_atoms, M_nbrs, ndim)"
2.7.1,"Shape (N_atoms, M_nbrs)"
2.7.1,Number of grid cells
2.7.1,TODO(rbharath): Support batching
2.7.1,"Shape (n_cells, ndim)"
2.7.1,"List of length N_atoms, each element of different length uniques_i"
2.7.1,"List of length N_atoms, each element of different length uniques_i"
2.7.1,"List of length N_atoms, each a tensor of shape"
2.7.1,"(uniques_i, ndim)"
2.7.1,Add phantom atoms that exist far outside the box
2.7.1,"List of length N_atoms, each of shape (1, ndim)"
2.7.1,TODO(rbharath): How does distance need to be modified here to
2.7.1,account for periodic boundary conditions?
2.7.1,List of length N_atoms each of shape (M_nbrs)
2.7.1,"N_atoms elts of size (M_nbrs,) each"
2.7.1,"Shape (N_atoms, 1)"
2.7.1,Find M_nbrs atoms closest to each cell
2.7.1,"Shape (n_cells, M_nbrs)"
2.7.1,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.1,"conditions, so does wrapround. O(constant)"
2.7.1,"Shape (n_cells, n_nbr_cells)"
2.7.1,"Shape (N_atoms, n_nbr_cells)"
2.7.1,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.7.1,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.7.1,"List of length N_atoms, each element length uniques_i"
2.7.1,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.7.1,element removed to remove self from list of neighbors. Need to verify
2.7.1,this holds more broadly or come up with robust alternative.
2.7.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.1,"Shape (N_atoms*n_cells, ndim) after tile"
2.7.1,Shape (N_atoms*n_cells)
2.7.1,"Shape (n_cells, N_atoms)"
2.7.1,Find k atoms closest to this cell.
2.7.1,"Tensor of shape (n_cells, M_nbrs)"
2.7.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.1,"Shape (N_atoms*n_cells, 1) after tile"
2.7.1,TODO(rbharath): Do we need to handle periodic boundary conditions
2.7.1,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.7.1,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.7.1,the cube.
2.7.1,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.7.1,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.7.1,"Tile (a, a, a, b, b, b, etc.)"
2.7.1,"Tile (a, b, c, a, b, c, ...)"
2.7.1,No other forget biases supported right now.
2.7.1,Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
2.7.1,import torch.nn as nn
2.7.1,from deepchem.models.torch_models import TorchModel
2.7.1,import deepchem.models.optimizers as optim
2.7.1,TODO look for the loss function(Hamiltonian)
2.7.1,dummy function which can be passed as the parameter f. f gives the log probability
2.7.1,TODO replace this function with forward pass of the model in future
2.7.1,"super(Ferminet, self).__init__()"
2.7.1,Initialization for ionic molecules
2.7.1,concatenating distance and vectors arrays
2.7.1,embedding node features
2.7.1,convolutional layer
2.7.1,pooling
2.7.1,for n_tasks == 1 case
2.7.1,mypy check is ignored for global_features as it is not a default attribute
2.7.1,of GraphData. It is created during runtime using **kwargs.
2.7.1,mapping from bond index to the index of the atom (where the bond is coming from)
2.7.1,"mapping from bond index to concat(in_atom, bond) features"
2.7.1,mapping from atom index to list of indicies of incoming bonds
2.7.1,mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
2.7.1,zero padded at the end
2.7.1,get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
2.7.1,padded with -1 at the end
2.7.1,mapping from bond index to the index of the atom (where the bond if going to)
2.7.1,mapping from atom index to list of indicies of incoming bonds
2.7.1,get maximum number of incoming bonds
2.7.1,Make number of incoming bonds equal to maximum number of bonds.
2.7.1,This is done by appending -1 to fill remaining space at each atom indices.
2.7.1,mapping from bond index to the index of the reverse bond
2.7.1,get encoder
2.7.1,get input size for ffn
2.7.1,get output size for ffn
2.7.1,get ffn
2.7.1,Steps to get `molecules_unbatch_key`:
2.7.1,1. Get the tensor containing the indices of first atoms of each molecule
2.7.1,2. Get the tensor containing number of atoms of each molecule
2.7.1,by taking the difference between consecutive indices.
2.7.1,3. Convert the tensor to a list.
2.7.1,num_molecules x (enc_hidden + global_features_size)
2.7.1,ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
2.7.1,"atom feature matrix with shape [number of atoms, number of features]"
2.7.1,concatenated feature vector which contains concatenation of initial atom and bond features
2.7.1,mapping from atom index to list of indicies of incoming bonds
2.7.1,mapping that maps bond index to 'array of indices of the bonds'
2.7.1,incoming at the initial atom of the bond (excluding the reverse bonds)
2.7.1,array of global molecular features
2.7.1,maximum number of incoming bonds in the batch
2.7.1,generate concatenated feature vector and mappings
2.7.1,pad all mappings to maximum number of incoming bonds in the batch
2.7.1,Decide first number of GAT layers
2.7.1,We convert deepchem.feat.GraphData to a PyG graph and then
2.7.1,batch it.
2.7.1,The default_generator method returns an array of dc.feat.GraphData objects
2.7.1,"nested inside a list. To access the nested array of graphs, we are"
2.7.1,indexing by 0 here.
2.7.1,flake8:noqa
2.7.1,Select a device.
2.7.1,W&B logging
2.7.1,"If `wandb=True` and no logger is provided, initialize default logger"
2.7.1,Setup and initialize W&B logging
2.7.1,Update config with KerasModel params
2.7.1,Main training loop.
2.7.1,"Execute the loss function, accumulating the gradients."
2.7.1,Report progress and write checkpoints.
2.7.1,Capture the last avg_loss in case of return since we're resetting to 0 now
2.7.1,Report final results.
2.7.1,Invoke the model.
2.7.1,Apply tranformers and record results.
2.7.1,Concatenate arrays to create the final results.
2.7.1,Compute the gradients.
2.7.1,Save the checkpoint to a file.
2.7.1,Rename and delete older files.
2.7.1,Ensure weights for both models are built.
2.7.1,model is None when reloading a model
2.7.1,Some scikit-learn models don't use weights.
2.7.1,flake8: ignore
2.7.1,flake8:noqa
2.7.1,GDBT doesn't support multi-output(task)
2.7.1,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.7.1,retrain model to whole data using best n_estimators * 1.25
2.7.1,GDBT doesn't support multi-output(task)
2.7.1,########################################
2.7.1,Deprecation warnings for XGBoostModel
2.7.1,########################################
2.7.1,flake8: noqa
2.7.1,-*- coding: utf-8 -*-
2.7.1,Assigning featurizer if not user defined
2.7.1,loading datasets
2.7.1,Assembling train and valid datasets
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,Building tensorflow MultitaskDNN model
2.7.1,Building tensorflow robust MultitaskDNN model
2.7.1,Building scikit logistic regression model
2.7.1,Transform fingerprints to IRV features
2.7.1,Building tensorflow IRV model
2.7.1,Building scikit random forest model
2.7.1,Building scikit learn Kernel SVM model
2.7.1,Building xgboost classification model
2.7.1,Remove token for paddings
2.7.1,Building scikit random forest model
2.7.1,Building scikit learn Kernel Ridge Regression model
2.7.1,Building scikit learn Kernel Ridge Regression model
2.7.1,Building xgboost regression model
2.7.1,Loading hyperparameters
2.7.1,num positive/negative ligands
2.7.1,Set batch sizes for network
2.7.1,Model structure
2.7.1,Traning settings
2.7.1,Fit trained model
2.7.1,Evaluating low data model
2.7.1,-*- coding: utf-8 -*-
2.7.1,Assigning featurizer if not user defined
2.7.1,loading datasets
2.7.1,
2.7.1,Note by @XericZephyr. Reason why I spun off this function:
2.7.1,1. Some model needs dataset information.
2.7.1,2. It offers us possibility to **cache** the dataset
2.7.1,"if the featurizer runs very slow, e.g., GraphConv."
2.7.1,2+. The cache can even happen at Travis CI to accelerate
2.7.1,CI testing.
2.7.1,
2.7.1,loading datasets
2.7.1,!/usr/bin/env python2
2.7.1,-*- coding: utf-8 -*-
2.7.1,"TODO For this dataset and model, the R2-scores are less than 0.3."
2.7.1,This has to be improved.
2.7.1,See: https://github.com/deepchem/deepchem/issues/2776
2.7.1,TODO: Check for this
2.7.1,Download files if they don't exist
2.7.1,Featurize the KINASE dataset
2.7.1,Shuffle the training data
2.7.1,Apply transformations
2.7.1,TIMING
2.7.1,transformers = [
2.7.1,"deepchem.trans.LogTransformer(transform_X=True),"
2.7.1,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.7.1,dataset=train_dataset)]
2.7.1,Set shard size low to avoid memory problems.
2.7.1,TIMING
2.7.1,TIMING
2.7.1,Set some global variables up top
2.7.1,Featurize KAGGLE dataset
2.7.1,TIMING
2.7.1,TIMING
2.7.1,Build the path to the dataset on disk.
2.7.1,Try to reload cached datasets.
2.7.1,Create the dataset
2.7.1,Split and transform the dataset.
2.7.1,. clinical trial toxicity (or absence of toxicity)
2.7.1,. FDA approval status.
2.7.1,Download files if they don't exist
2.7.1,Featurizing datasets
2.7.1,Missing entry removal
2.7.1,Shuffle the training data
2.7.1,Apply transformations
2.7.1,TIMING
2.7.1,TODO: Check if anything needs to be added
2.7.1,Featurize the FACTORS dataset
2.7.1,Shuffle the training data
2.7.1,Apply transformations
2.7.1,TIMING
2.7.1,dict of accepted featurizers for this dataset
2.7.1,modify the returned dicts for your dataset
2.7.1,Names of supported featurizers
2.7.1,dict of accepted transformers
2.7.1,dict of accepted splitters
2.7.1,names of supported splitters
2.7.1,Warning message about this template
2.7.1,Featurize mydataset
2.7.1,Get DeepChem data directory if needed
2.7.1,Check for str args to featurizer and splitter
2.7.1,Reload from disk
2.7.1,First type of supported featurizers
2.7.1,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.7.1,Changer loader to match featurizer and data file type
2.7.1,Featurize dataset
2.7.1,Initialize transformers
2.7.1,"get pdb and sdf filenames, labels and pdbids"
2.7.1,load and featurize each complex
2.7.1,Extract locations of data
2.7.1,Extract labels
2.7.1,Lines have format
2.7.1,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.7.1,"The base-10 logarithm, -log kd/pk"
2.7.1,"def load_pcba_146(featurizer='ECFP',"
2.7.1,"split='random',"
2.7.1,"reload=True,"
2.7.1,"data_dir=None,"
2.7.1,"save_dir=None,"
2.7.1,**kwargs):
2.7.1,return load_pcba_dataset(
2.7.1,"featurizer=featurizer,"
2.7.1,"split=split,"
2.7.1,"reload=reload,"
2.7.1,"assay_file_name=""pcba_146.csv.gz"","
2.7.1,"data_dir=data_dir,"
2.7.1,"save_dir=save_dir,"
2.7.1,**kwargs)
2.7.1,"def load_pcba_2475(featurizer='ECFP',"
2.7.1,"split='random',"
2.7.1,"reload=True,"
2.7.1,"data_dir=None,"
2.7.1,"save_dir=None,"
2.7.1,**kwargs):
2.7.1,return load_pcba_dataset(
2.7.1,"featurizer=featurizer,"
2.7.1,"split=split,"
2.7.1,"reload=reload,"
2.7.1,"assay_file_name=""pcba_2475.csv.gz"","
2.7.1,"data_dir=data_dir,"
2.7.1,"save_dir=save_dir,"
2.7.1,**kwargs)
2.7.1,Range of optimization
2.7.1,We know from guard above that this is an int/float
2.7.1,Specify logfile
2.7.1,Make logdir if it doesn't exist.
2.7.1,setup range
2.7.1,Stores all results
2.7.1,Store all model references so we don't have to reload
2.7.1,Stores all model locations
2.7.1,"param values are always float in BO, so this line converts float to int"
2.7.1,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.7.1,Record hyperparameters
2.7.1,We have already evaluated the model for these hyperparameters.
2.7.1,Add it on to the information needed for the constructor
2.7.1,Not all models have nb_epoch
2.7.1,Some models autosave
2.7.1,Record performances
2.7.1,Store all results
2.7.1,Store reference to model
2.7.1,GPGO maximize performance by default
2.7.1,set performance to its negative value for minimization
2.7.1,Demarcating internal function for readability
2.7.1,execute GPGO
2.7.1,FIXME: Incompatible types in assignment
2.7.1,Let's fetch the model with the best parameters
2.7.1,Compare best model to default hyperparameters
2.7.1,Record hyperparameters
2.7.1,Return default hyperparameters
2.7.1,Construction dictionary mapping hyperparameter names to values
2.7.1,"mypy test throws error, so ignoring it in try"
2.7.1,Not all models have nb_epoch
2.7.1,Some models autosave
2.7.1,arbitrarily return last model
2.7.1,hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
2.7.1,"mypy test throws error, so ignoring it in try"
2.7.1,Not all models have nb_epoch
2.7.1,Some models autosave
2.7.1,Update best validation score so far
2.7.1,"if `hyp_str` not in `all_scores`, store it in `all_scores`"
2.7.1,arbitrarily return last model trained
2.7.1,"If callable, sample it for a maximum n times"
2.7.1,flake8: noqa
2.7.1,"2 model variants, 1 results.txt file"
2.7.1,Generate dummy dataset
2.7.1,Generate dummy dataset
2.7.1,These are per-example multiplier
2.7.1,Test that 2 parameters were optimized
2.7.1,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.7.1,Generate dummy dataset
2.7.1,Define nb_epoch in hyperparam_search function call
2.7.1,"max_iter model variants, 1 results.txt file"
2.7.1,Generate dummy dataset
2.7.1,Generate dummy dataset
2.7.1,These are per-example multiplier
2.7.1,Test that 2 parameters were optimized
2.7.1,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.7.1,Generate dummy dataset
2.7.1,Define nb_epoch in hyperparam_search function call
2.7.1,Generate dummy dataset
2.7.1,Generate dummy dataset
2.7.1,These are per-example multiplier
2.7.1,Test that 2 parameters were optimized
2.7.1,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.7.1,Generate dummy dataset
2.7.1,Have the worker threads generate the rollouts for this iteration.
2.7.1,Perform optimization.
2.7.1,Build the inputs and run the optimizer.
2.7.1,Update the number of steps taken so far and perform checkpointing.
2.7.1,Merge all the rollouts into a single set of arrays.
2.7.1,Iterate slices.
2.7.1,Generate the rollout.
2.7.1,Compute an estimate of the reward for the rest of the episode.
2.7.1,Compute the discounted rewards and advantages.
2.7.1,Convert the actions to one-hot.
2.7.1,Rearrange the states into the proper set of arrays.
2.7.1,Return the processed arrays.
2.7.1,Training loop.
2.7.1,Do checkpointing.
2.7.1,Generate the rollout.
2.7.1,Compute an estimate of the reward for the rest of the episode.
2.7.1,Compute the discounted rewards and advantages.
2.7.1,"Record the actions, converting to one-hot if necessary."
2.7.1,Rearrange the states into the proper set of arrays.
2.7.1,Build the inputs and apply gradients.
2.7.1,Assume all arrays are float32.
2.7.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.7.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.7.1,"game).  The average reward for any bet is slightly negative, so the best"
2.7.1,strategy is to walk away.
2.7.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.1,Optimize it.
2.7.1,"It should have learned that the expected value is very close to zero, and that the best"
2.7.1,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.7.1,top actions).
2.7.1,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.7.1,get the same result.
2.7.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.1,The environment just has a constant state.
2.7.1,The policy includes a single recurrent layer.
2.7.1,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.7.1,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.7.1,"On the first call, the initial state should be all zeros."
2.7.1,It should still be zeros since we didn't save it last time.
2.7.1,It should be different now.
2.7.1,This should be the same as the previous one.
2.7.1,"Now we reset it, so we should get the same result as initially."
2.7.1,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.7.1,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.7.1,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.7.1,at all.  Using hindsight makes it much easier.
2.7.1,A simple policy with two hidden layers.
2.7.1,Optimize it.
2.7.1,Try running it a few times and see if it succeeds.
2.7.1,The state consists of two numbers: a current value and a target value.
2.7.1,The policy just needs to learn to output the target value (or at least
2.7.1,move toward it).
2.7.1,A simple policy with no hidden layers.
2.7.1,Optimize it.
2.7.1,Try running it and see if it reaches the target
2.7.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.7.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.7.1,"game).  The average reward for any bet is slightly negative, so the best"
2.7.1,strategy is to walk away.
2.7.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.1,Optimize it.
2.7.1,"It should have learned that the expected value is very close to zero, and that the best"
2.7.1,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.7.1,top actions).
2.7.1,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.7.1,get the same result.
2.7.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.1,The environment just has a constant state.
2.7.1,The policy includes a single recurrent layer.
2.7.1,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.7.1,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.7.1,"On the first call, the initial state should be all zeros."
2.7.1,It should still be zeros since we didn't save it last time.
2.7.1,It should be different now.
2.7.1,This should be the same as the previous one.
2.7.1,"Now we reset it, so we should get the same result as initially."
2.7.1,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.7.1,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.7.1,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.7.1,at all.  Using hindsight makes it much easier.
2.7.1,A simple policy with two hidden layers.
2.7.1,Optimize it.
2.7.1,Try running it a few times and see if it succeeds.
2.7.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.1,Randomize who goes first
2.7.1,Illegal move -- the square is not empty
2.7.1,Move X
2.7.1,Did X Win
2.7.1,Did O Win
2.7.1,"default channels are ""conda-forge"" and ""omnia"""
2.7.1,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.7.0,Build a nightly package by default.
2.7.0,Environment-specific dependencies.
2.7.0,get the version from deepchem/__init__.py
2.7.0,nightly version : .devYearMonthDayHourMinute
2.7.0,Force to add `.dev` if `--release` option isn't passed when building
2.7.0,!/usr/bin/env python3
2.7.0,-*- coding: utf-8 -*-
2.7.0,Datasets and models used in the benchmark test
2.7.0,"irv, rf, rf_regression should be assigned manually"
2.7.0,Evaluate performances with different training set fraction
2.7.0,Datasets and models used in the benchmark test
2.7.0,Uncomment the two lines below if hyper_parameters are provided
2.7.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.7.0,hyper_parameters = pickle.load(f)
2.7.0,!/usr/bin/env python3
2.7.0,-*- coding: utf-8 -*-
2.7.0,Datasets and models used in the benchmark test
2.7.0,Load Delaney dataset
2.7.0,Get Metric
2.7.0,Fit trained model
2.7.0,Fit trained model
2.7.0,Set numpy seed
2.7.0,##Load data###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Featurize Kinase dataset
2.7.0,##Load data###
2.7.0,num_trials = 5
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,Force matplotlib to not use any Xwindows backend.
2.7.0,##Load data###
2.7.0,the histogram of the data
2.7.0,Set numpy seed
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,num_trials = 5
2.7.0,Set some global variables up top
2.7.0,Fit trained model
2.7.0,Featurize PCBA dataset
2.7.0,Initialize transformers
2.7.0,Fit trained model
2.7.0,Load sider models now
2.7.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.7.0,transformers to predict functions
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,##Load data###
2.7.0,"n_estimators=100, max_features=int(num_features/3),"
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,Fit trained model
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Load tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,This example shows how to use Pandas to load data directly
2.7.0,without using a CSVLoader object. This may be useful if you
2.7.0,want the flexibility of processing your data with Pandas
2.7.0,directly.
2.7.0,Now let's convert from a dataset back to a pandas dataframe
2.7.0,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.7.0,Featurize FACTORS dataset
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,Force matplotlib to not use any Xwindows backend.
2.7.0,##Load data###
2.7.0,the histogram of the data
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Load QM7 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Fit trained model
2.7.0,Fit trained model
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Fit trained model
2.7.0,Load QM8 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Fit trained model
2.7.0,Set numpy seed
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,Load ChEMBL dataset
2.7.0,Fit models
2.7.0,Do setup required for tf/keras models
2.7.0,Number of features on conv-mols
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,DeepCrystal Technologies 2017 - Patrick Hop
2.7.0,MIT License - have fun!!
2.7.0,Set to higher values to get better numbers
2.7.0,======================================================================
2.7.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,Only for debug!
2.7.0,Load Delaney dataset
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Do setup required for tf/keras models
2.7.0,Number of features on conv-mols
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load Delaney dataset
2.7.0,Get Metric
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load Delaney dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load MUV dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Evaluate train/test scores
2.7.0,Load MUV data
2.7.0,Build model
2.7.0,Fit trained model
2.7.0,Evaluate train/test scores
2.7.0,Extract active site
2.7.0,Featurize ligand
2.7.0,Default for CircularFingerprint
2.7.0,Featurize pocket
2.7.0,Note broadcast operation
2.7.0,Compute labels for pockets
2.7.0,Some complexes have labels but no PDB files. Filter these manually
2.7.0,Some of the ligand-names are of form (FMN ox). Use regex
2.7.0,to merge into form (FMN-ox)
2.7.0,Filter if missing PDB files
2.7.0,Load PDBBind dataset
2.7.0,Define featurizers
2.7.0,Featurize Dataset
2.7.0,########################################################## DEBUG
2.7.0,########################################################## DEBUG
2.7.0,For stable runs
2.7.0,Fit trained model
2.7.0,4-fold splits
2.7.0,num positive/negative ligands
2.7.0,10 trials on test-set
2.7.0,Define metric
2.7.0,Get supports on test-set
2.7.0,Compute accuracies
2.7.0,Number of features on conv-mols
2.7.0,Batch size of models
2.7.0,graph_model = dc.nn.SequentialGraph(n_feat)
2.7.0,Fit trained model
2.7.0,Test model
2.7.0,Join information for all tasks.
2.7.0,4-fold splits
2.7.0,num positive/negative ligands
2.7.0,Define metric
2.7.0,Get supports on test-set
2.7.0,Compute accuracies
2.7.0,Number of features on conv-mols
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Test model
2.7.0,Join information for all tasks.
2.7.0,4-fold splits
2.7.0,num positive/negative ligands
2.7.0,10 trials on test-set
2.7.0,Define metric
2.7.0,Get supports on test-set
2.7.0,Compute accuracies
2.7.0,Number of features on conv-mols
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Test model
2.7.0,Join information for all tasks.
2.7.0,4-fold splits
2.7.0,num positive/negative ligands
2.7.0,10 trials on test-set
2.7.0,Define metric
2.7.0,Get supports on test-set
2.7.0,Compute accuracies
2.7.0,Train model on support
2.7.0,Test model
2.7.0,Join information for all tasks.
2.7.0,4-fold splits
2.7.0,num positive/negative ligands
2.7.0,10 trials on test-set
2.7.0,Define metric
2.7.0,Get supports on test-set
2.7.0,Compute accuracies
2.7.0,Train model on support
2.7.0,Test model
2.7.0,Join information for all tasks.
2.7.0,Set some global variables up top
2.7.0,Featurize Tox21 dataset
2.7.0,Initialize transformers
2.7.0,Set some global variables up top
2.7.0,Featurize Tox21 dataset
2.7.0,Initialize transformers
2.7.0,Load MUV dataset
2.7.0,Featurize MUV dataset
2.7.0,Initialize transformers
2.7.0,Load MUV dataset
2.7.0,Featurize MUV dataset
2.7.0,Initialize transformers
2.7.0,Featurize SIDER dataset
2.7.0,Initialize transformers
2.7.0,Featurize SIDER dataset
2.7.0,Initialize transformers
2.7.0,Load the data.
2.7.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.7.0,sparse: most tasks do not include data for most molecules.  It also is very
2.7.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.7.0,create a list of alternating positives and negatives so each batch will have
2.7.0,equal numbers of both.
2.7.0,Define a MetaLearner describing the learning problem.
2.7.0,Run meta-learning on 80% of the tasks.
2.7.0,Validate on the remaining tasks.
2.7.0,4-fold splits
2.7.0,10 positive/negative ligands
2.7.0,10 trials on test-set
2.7.0,Sample supports without replacement (all pos/neg should be different)
2.7.0,Define metric
2.7.0,Get supports on test-set
2.7.0,Compute accuracies
2.7.0,Train model on support
2.7.0,Test model
2.7.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.7.0,Join information for all tasks.
2.7.0,4-fold splits
2.7.0,num positive/negative ligands
2.7.0,Define metric
2.7.0,Get supports on test-set
2.7.0,Compute accuracies
2.7.0,Train model on support
2.7.0,Test model
2.7.0,Join information for all tasks.
2.7.0,replace with your own scratch directory
2.7.0,Number of conformations in each file increases exponentially.
2.7.0,Start with a smaller dataset before continuing. Use all of them
2.7.0,for production
2.7.0,"'ani_gdb_s03.h5',"
2.7.0,"'ani_gdb_s04.h5',"
2.7.0,"'ani_gdb_s05.h5',"
2.7.0,"'ani_gdb_s06.h5',"
2.7.0,"'ani_gdb_s07.h5',"
2.7.0,'ani_gdb_s08.h5'
2.7.0,Extract the data
2.7.0,Print the data
2.7.0,self-interaction energies taken from
2.7.0,https://github.com/isayev/ANI1_dataset README
2.7.0,flush once more at the end
2.7.0,"# For production, set nb_epoch to 100+"
2.7.0,"print(""Train scores"")"
2.7.0,print(train_scores)
2.7.0,"print(""Minimization of a single test set structure:"")"
2.7.0,"print(model.minimize_structure(coords, atomic_nums))"
2.7.0,Written by Roman Zubatyuk and Justin S. Smith
2.7.0,Modified by Yutong Zhao to make python2 compatible
2.7.0,opening file
2.7.0,print(store_loc)
2.7.0,print(type(v[0]))
2.7.0,print(k)
2.7.0,print(path)
2.7.0,Number of conformations in each file increases exponentially.
2.7.0,Start with a smaller dataset before continuing. Use all of them
2.7.0,for production
2.7.0,Extract the data
2.7.0,NOTE THE RENAMING:
2.7.0,Note sensitivity = recall
2.7.0,Load nci dataset
2.7.0,Featurize nci dataset
2.7.0,Initialize transformers
2.7.0,Set some global variables up top
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load hiv dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load hiv dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load delaney dataset
2.7.0,Fit models
2.7.0,Load delaney dataset
2.7.0,Fit models
2.7.0,Fit models
2.7.0,Load delaney dataset
2.7.0,Fit models
2.7.0,TODO: Once improved splitting API is merged in swap to simpler API
2.7.0,The return values are dc.data.Dataset objects so we need to extract
2.7.0,the ids
2.7.0,TODO once improved splitting API is merged in swap out for simpler
2.7.0,API
2.7.0,The return values are dc.data.Dataset objects so we need to extract
2.7.0,the ids
2.7.0,Fit trained model
2.7.0,Load SIDER dataset
2.7.0,Featurize SIDER dataset
2.7.0,Initialize transformers
2.7.0,Featurize permeability dataset
2.7.0,Load Tox21 dataset
2.7.0,Fit trained model
2.7.0,TODO: This should be swapped for simpler splitter API once that's merged in.
2.7.0,The return values are dc.data.Dataset objects so we need to extract
2.7.0,the ids
2.7.0,Only for debug!
2.7.0,Load clintox dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load clintox dataset
2.7.0,Fit models
2.7.0,Do setup required for tf/keras models
2.7.0,Number of features on conv-mols
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,-*- coding: utf-8 -*-
2.7.0,#############################################################################
2.7.0,## save dataset
2.7.0,#############################################################################
2.7.0,## load datasets
2.7.0,load sweetfda
2.7.0,load aact
2.7.0,## fixup smiles for matching
2.7.0,return smiles
2.7.0,map original smiles to converted smiles
2.7.0,"## join dataframes, index on smiles"
2.7.0,map original smiles back
2.7.0,## fill all nan with 0
2.7.0,## construct datasets
2.7.0,store in new datasets
2.7.0,## save datasets
2.7.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.7.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.7.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.7.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.7.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.7.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.7.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.7.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.7.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.7.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.7.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.7.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.7.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.7.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.7.0,For stable runs
2.7.0,Fit trained model
2.7.0,For stable runs
2.7.0,Fit trained model
2.7.0,For stable runs
2.7.0,Fit trained model
2.7.0,TODO The below line should be fixes
2.7.0,See: https://github.com/deepchem/deepchem/issues/2373
2.7.0,model.save()
2.7.0,transformers = [
2.7.0,"dc.trans.LogTransformer(transform_X=True),"
2.7.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.7.0,dataset=train_dataset)]
2.7.0,Featurize UV dataset
2.7.0,##Load data###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Set numpy seed
2.7.0,##Load data###
2.7.0,##Create model###
2.7.0,Use R2 classification metric
2.7.0,Only use for final evaluation
2.7.0,Force matplotlib to not use any Xwindows backend.
2.7.0,##Load data###
2.7.0,the histogram of the data
2.7.0,##Load data###
2.7.0,###################################################### DEBUG
2.7.0,###################################################### DEBUG
2.7.0,Load HOPV dataset
2.7.0,Fit models
2.7.0,Number of features on conv-mols
2.7.0,Batch size of models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load HOPV dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load HOPV dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load HOPV dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Only for debug!
2.7.0,Load HOPV dataset
2.7.0,Fit models
2.7.0,Fit trained model
2.7.0,Load TOXCAST dataset
2.7.0,Featurize TOXCAST dataset
2.7.0,Initialize transformers
2.7.0,Fit trained model
2.7.0,Processing of ToxCast data
2.7.0,Author - Aneesh Pappu
2.7.0,Loading dataframes and editing indices
2.7.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.7.0,get corresponding casn
2.7.0,get corresponding smiles
2.7.0,write to cell
2.7.0,Tidy up and write to csv
2.7.0,TODO(rbharath): Check that this operation is differentiable.
2.7.0,The number of cells which we should theoretically have
2.7.0,The number of cells which we should theoretically have
2.7.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.7.0,The number of cells which we should theoretically have
2.7.0,TODO(rbharath): The test below only checks that shapes work out.
2.7.0,Need to do a correctness implementation vs. a simple CPU impl.
2.7.0,The number of cells which we should theoretically have
2.7.0,TODO(rbharath): The test below only checks that shapes work out.
2.7.0,Need to do a correctness implementation vs. a simple CPU impl.
2.7.0,The number of cells which we should theoretically have
2.7.0,TODO(rbharath): The test below only checks that shapes work out.
2.7.0,Need to do a correctness implementation vs. a simple CPU impl.
2.7.0,TODO(rbharath): Commenting this out due to weird segfaults
2.7.0,def test_vina_generate_conformers(self):
2.7.0,"""""""Test that Vina Model can generate conformers"""""""
2.7.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.7.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.7.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.7.0,max_protein_atoms = 3500
2.7.0,max_ligand_atoms = 100
2.7.0,"print(""Loading protein file"")"
2.7.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.7.0,protein_Z = pad_array(
2.7.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.7.0,max_protein_atoms)
2.7.0,"print(""Loading ligand file"")"
2.7.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.7.0,ligand_Z = pad_array(
2.7.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.7.0,max_ligand_atoms)
2.7.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.7.0,"Shape (n_cells, k)"
2.7.0,"Shape (N, 1)"
2.7.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.0,"conditions, so does wrapround. O(constant)"
2.7.0,"Shape (n_cells, 26)"
2.7.0,"Shape (N, 26)"
2.7.0,"coords of shape (N, ndim)"
2.7.0,"Shape (N, 26, k, ndim)"
2.7.0,"Shape (N, 26, k)"
2.7.0,"Shape (N, 26, k)"
2.7.0,"Shape (N, 26, k, ndim)"
2.7.0,"For smaller systems especially, the periodic boundary conditions can"
2.7.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.7.0,make sure duplicate neighbors are ignored?
2.7.0,TODO(rbharath): How does distance need to be modified here to
2.7.0,account for periodic boundary conditions?
2.7.0,"Shape (N, 26, k)"
2.7.0,"Shape (N, 26*k)"
2.7.0,TODO(rbharath): This will cause an issue with duplicates!
2.7.0,"Shape (N, M)"
2.7.0,"N elts of size (M,) each"
2.7.0,"Shape (N, 26*k)"
2.7.0,"N elts of size (26*k,) each"
2.7.0,"N elts of size (M,) each"
2.7.0,"Shape (N, M)"
2.7.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.7.0,"N tensors of shape (n_cells, 1)"
2.7.0,"Shape (N*n_cells, 1) after tile"
2.7.0,"List of N tensors of shape (n_cells, 1)"
2.7.0,Lists of length N
2.7.0,Lists of length n_cells
2.7.0,Get indices of k atoms closest to each cell point
2.7.0,TODO(rbharath): tf.stack for tf 1.0
2.7.0,"Tensor of shape (n_cells, k, ndim)"
2.7.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.7.0,"Tensor of shape (26, k, ndim)"
2.7.0,"Reshape to (26*k, ndim)"
2.7.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.7.0,"Dists of shape (26*k, 1)"
2.7.0,"Of shape (k, ndim)"
2.7.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.7.0,TODO(rbharath): Change this for tf 1.0
2.7.0,"n_cells tensors of shape (N, 1)"
2.7.0,"Shape (N*n_cells, 1) after tile"
2.7.0,"List of n_cells tensors of shape (N, 1)"
2.7.0,Lists of length n_cells
2.7.0,Lists of length n_cells
2.7.0,Get indices of k atoms closest to each cell point
2.7.0,"n_cells tensors of shape (k, ndim)"
2.7.0,"Tensor of shape (n_cells, k)"
2.7.0,TODO(rbharath):
2.7.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.7.0,- Need to group closest atoms amongst cell neighbors
2.7.0,- Need to do another top_k to find indices of closest neighbors.
2.7.0,- Return N lists corresponding to neighbors for every atom.
2.7.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.7.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.7.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.7.0,the cube.
2.7.0,Number of neighbors of central cube in 3-space is
2.7.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.7.0,TODO(rbharath)
2.7.0,n_cells = int(cells.get_shape()[0])
2.7.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.7.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.7.0,"Tile (a, a, a, b, b, b, etc.)"
2.7.0,"Tile (a, b, c, a, b, c, ...)"
2.7.0,"Lists of n_cells tensors of shape (N, 1)"
2.7.0,Lists of length n_cells
2.7.0,Lists of length n_cells
2.7.0,Get indices of k atoms closest to each cell point
2.7.0,"n_cells tensors of shape (26,)"
2.7.0,TODO(rbharath): Make this handle minibatches
2.7.0,"Shape (N_protein+N_ligand, 3)"
2.7.0,"Shape (N_protein+N_ligand,)"
2.7.0,"Shape (N_protein+N_ligand, M)"
2.7.0,"Shape (N_protein+N_ligand,)"
2.7.0,"Shape (N_protein+N_ligand, 3)"
2.7.0,"Shape (N_protein+N_ligand,)"
2.7.0,"Shape (N_protein+N_ligand, M)"
2.7.0,"Shape (N_protein+N_ligand, M, 3)"
2.7.0,"Shape (N_protein+N_ligand, M)"
2.7.0,"Shape (N_protein+N_ligand, M, 3)"
2.7.0,"Shape (N_protein+N_ligand, M)"
2.7.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.7.0,"Shape (N_protein+N_ligand, M)"
2.7.0,"Shape (N_protein+N_ligand, M)"
2.7.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.7.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.7.0,computing free-energy. This implementation currently uses all interaction
2.7.0,terms. Not sure if this makes a difference.
2.7.0,"Shape (N_protein+N_ligand, M)"
2.7.0,Shape () -- scalar
2.7.0,Keep track of the layers
2.7.0,"For graphical layers, add connectivity placeholders"
2.7.0,Add layer to the layer list
2.7.0,Keep track of the layers
2.7.0,Create graph topology and x
2.7.0,Keep track of the layers
2.7.0,Whether or not we have used the GraphGather layer yet
2.7.0,Update new value of x
2.7.0,Update new value of x
2.7.0,Update new value of x
2.7.0,Get train function
2.7.0,Initialize
2.7.0,################################################################### DEBUG
2.7.0,self.test_label_placeholder = Input(
2.7.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.7.0,"name=""label_placeholder""))"
2.7.0,self.test_weight_placeholder = Input(
2.7.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.7.0,"name=""weight_placeholder""))"
2.7.0,TODO(rbharath): Should weights for the support be used?
2.7.0,Support labels
2.7.0,self.support_label_placeholder = Input(
2.7.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.7.0,"name=""support_label_placeholder""))"
2.7.0,################################################################### DEBUG
2.7.0,Generate dictionary elements for support
2.7.0,Get graph information for test
2.7.0,Generate dictionary elements for test
2.7.0,Perform the optimization
2.7.0,Create different support sets
2.7.0,Get batch to try it out on
2.7.0,"Train on support set, batch pair"
2.7.0,Get featurization for test
2.7.0,"Shape (n_test, n_feat)"
2.7.0,Get featurization for support
2.7.0,"Shape (n_support, n_feat)"
2.7.0,Computes the inner part c() of the kernel
2.7.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.7.0,Normalize
2.7.0,TODO(rbharath): euclidean kernel is broken!
2.7.0,elif self.similarity == 'euclidean':
2.7.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.7.0,"Note that gram matrix g has shape (n_test, n_support)"
2.7.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.7.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.7.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.7.0,each test entry) to get attention vector
2.7.0,"Shape (n_test, n_support)"
2.7.0,Weighted sum of support labels
2.7.0,"Shape (n_support, 1)"
2.7.0,pred is yhat in eqn (1) of Matching Networks.
2.7.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.7.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.7.0,"Shape (n_test,)"
2.7.0,Convert to logit space using inverse sigmoid (logit) function
2.7.0,logit function: log(pred) - log(1-pred)
2.7.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.7.0,in Cross Entropy calculation.
2.7.0,"Shape (n_test,)"
2.7.0,Get scores
2.7.0,Remove padded elements
2.7.0,Get scores
2.7.0,pred corresponds to prob(example == 1)
2.7.0,Remove padded elements
2.7.0,Get batches
2.7.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.7.0,multitask case with missing data...
2.7.0,Join information for all tasks.
2.7.0,TODO(rbharath): Find a way to get rid of this import?
2.7.0,Extract model info
2.7.0,Get graph topology for x
2.7.0,Building outputs
2.7.0,Set epsilon
2.7.0,Initialize
2.7.0,"Path to save checkpoint files, which matches the"
2.7.0,replicated supervisor's default path.
2.7.0,Create target inputs
2.7.0,Get train function
2.7.0,TODO(rbharath): I believe this is total amount of data
2.7.0,Get graph information
2.7.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.7.0,the number of labeled data points in target_i. This is to normalize each task
2.7.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.7.0,Get other optimizer information
2.7.0,TODO(rbharath): Figure out how to handle phase appropriately
2.7.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.7.0,"tensors of shape (batch_size,)"
2.7.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.7.0,examples (effect averages out)
2.7.0,Perform the optimization
2.7.0,TODO(rbharath): Disabling saving for now to try to debug.
2.7.0,run eval data through the model
2.7.0,"Shape (n_samples, n_tasks)"
2.7.0,Create target inputs
2.7.0,TODO(rbharath): Find a way to get rid of this import?
2.7.0,Obtain appropriate loss function
2.7.0,Extract model info
2.7.0,Get graph topology for x
2.7.0,Raw logit outputs
2.7.0,Set epsilon
2.7.0,Initialize
2.7.0,"Path to save checkpoint files, which matches the"
2.7.0,replicated supervisor's default path.
2.7.0,Create target inputs
2.7.0,############################################################### DEBUG
2.7.0,"print(""multitask classifier"")"
2.7.0,"print(""feat"")"
2.7.0,print(feat)
2.7.0,############################################################### DEBUG
2.7.0,Get train function
2.7.0,TODO(rbharath): I believe this is total amount of data
2.7.0,Get graph information
2.7.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.7.0,the number of labeled data points in target_i. This is to normalize each task
2.7.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.7.0,Get other optimizer information
2.7.0,TODO(rbharath): Figure out how to handle phase appropriately
2.7.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.7.0,"tensors of shape (batch_size,)"
2.7.0,Convert the labels into one-hot vector encodings.
2.7.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.7.0,un-softmaxed logits rather than softmax outputs.
2.7.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.7.0,examples (effect averages out)
2.7.0,Perform the optimization
2.7.0,TODO(rbharath): Disabling saving for now to try to debug.
2.7.0,run eval data through the model
2.7.0,"Shape (n_samples, n_tasks)"
2.7.0,run eval data through the model
2.7.0,self.n_atoms = n_atoms
2.7.0,Define the list of tensors to be used as topology
2.7.0,Merge mol conv objects
2.7.0,Generate dicts
2.7.0,Define the list of tensors to be used as topology
2.7.0,Extract atom numbers
2.7.0,Generate dicts
2.7.0,molecule * atom(graph) => step => features
2.7.0,molecule * atom(graph) => step
2.7.0,molecule * atom(graph) => step
2.7.0,Define the list of tensors to be used as topology
2.7.0,calculation orders for a batch of molecules
2.7.0,padding atom features vector of each molecule with 0
2.7.0,self.n_atoms = n_atoms
2.7.0,Define the list of tensors to be used as topology
2.7.0,Extract atom numbers
2.7.0,Generate dicts
2.7.0,self.n_atoms = n_atoms
2.7.0,Define the list of tensors to be used as topology
2.7.0,Extract atom numbers
2.7.0,number of atoms in each molecule
2.7.0,index of pair features
2.7.0,number of pairs for each atom
2.7.0,atom features
2.7.0,pair features
2.7.0,Generate dicts
2.7.0,Load Tox21 dataset
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.7.0,Fit trained model
2.7.0,Fit models
2.7.0,Batch size of models
2.7.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.7.0,Fit trained model
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,number positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply an attention lstm layer
2.7.0,Number of folds for split
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,number positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply an attention lstm layer
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,number positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply an attention lstm layer
2.7.0,Number of folds for split
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Number of folds for split
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply a residual lstm layer
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply a residual lstm layer
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply a residual lstm layer
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply a residual lstm layer
2.7.0,Number of folds for split
2.7.0,Depth of attention module
2.7.0,number positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,Apply an attention lstm layer
2.7.0,Number of folds for split
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Number of features on conv-mols
2.7.0,Define metric
2.7.0,Train support model on train
2.7.0,Add layers
2.7.0,# Gather Projection
2.7.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.7.0,There should be 8 layers in graph_model
2.7.0,assert len(graph_model.layers) == 6
2.7.0,Add layers
2.7.0,Need to add batch-norm separately to test/support due to differing
2.7.0,shapes.
2.7.0,Apply an attention lstm layer
2.7.0,Gather Projection
2.7.0,Add layers
2.7.0,Need to add batch-norm separately to test/support due to differing
2.7.0,shapes.
2.7.0,Apply an attention lstm layer
2.7.0,Gather Projection
2.7.0,Degrees from 1 to max_deg inclusive
2.7.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.7.0,"Should have shape (?, deg)"
2.7.0,"Shape of atom_features should be (?, n_feat)"
2.7.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.7.0,-*- coding: utf-8 -*-
2.7.0,Save hyperparameters
2.7.0,-*- coding: utf-8 -*-
2.7.0,Save hyperparameters
2.7.0,setup optimizer
2.7.0,setup optimizer
2.7.0,"print(""tasK: %d"" %task)"
2.7.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.7.0,"print(""scores"")"
2.7.0,print(scores.size())
2.7.0,"print(""task_label"")"
2.7.0,print(task_label.size())
2.7.0,"task_loss =  self.criterion(scores, task_label)"
2.7.0,"print(""task_loss"")"
2.7.0,print(task_loss.size())
2.7.0,-*- coding: utf-8 -*-
2.7.0,Save hyperparameters
2.7.0,weight decay
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,Turns out there are valid cases where we don't want pad-batches
2.7.0,on by default.
2.7.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.0,Run training op.
2.7.0,############################################################# TIMING
2.7.0,The iterbatches does padding with zero-weight examples on the last batch.
2.7.0,Remove padded examples.
2.7.0,Special case to handle singletasks.
2.7.0,The iterbatches does padding with zero-weight examples on the last batch.
2.7.0,Remove padded examples.
2.7.0,References
2.7.0,Arguments
2.7.0,Aliases.
2.7.0,Aliases.
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,TODO(rbharath): This class does not yet have a
2.7.0,"TensorGraph equivalent, but one may not be required."
2.7.0,"Commented out for now, remove if OK."
2.7.0,class AlternateWeaveLayer(WeaveLayer):
2.7.0,""""""" Alternate implementation of weave module"
2.7.0,"same variables, different graph structures"
2.7.0,""""""""
2.7.0,
2.7.0,"def call(self, x, mask=None):"
2.7.0,"""""""Execute this layer on input tensors."
2.7.0,
2.7.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.7.0,
2.7.0,Parameters
2.7.0,----------
2.7.0,x: list
2.7.0,list of Tensors of form described above.
2.7.0,"mask: bool, optional"
2.7.0,Ignored. Present only to shadow superclass call() method.
2.7.0,
2.7.0,Returns
2.7.0,-------
2.7.0,A: Tensor
2.7.0,Tensor of atom_features
2.7.0,P: Tensor
2.7.0,Tensor of pair_features
2.7.0,""""""""
2.7.0,# Add trainable weights
2.7.0,self.build()
2.7.0,
2.7.0,atom_features = x[0]
2.7.0,pair_features = x[1]
2.7.0,
2.7.0,pair_split = x[2]
2.7.0,atom_to_pair = x[4]
2.7.0,
2.7.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.7.0,AA = self.activation(AA)
2.7.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.7.0,PA = self.activation(PA)
2.7.0,"PA = tf.segment_sum(PA, pair_split)"
2.7.0,
2.7.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.7.0,A = self.activation(A)
2.7.0,
2.7.0,if self.update_pair:
2.7.0,AP_ij = tf.matmul(
2.7.0,tf.reshape(
2.7.0,"tf.gather(atom_features, atom_to_pair),"
2.7.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.7.0,AP_ij = self.activation(AP_ij)
2.7.0,AP_ji = tf.matmul(
2.7.0,tf.reshape(
2.7.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.7.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.7.0,AP_ji = self.activation(AP_ji)
2.7.0,
2.7.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.7.0,PP = self.activation(PP)
2.7.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.7.0,P = self.activation(P)
2.7.0,else:
2.7.0,P = pair_features
2.7.0,
2.7.0,"return A, P"
2.7.0,TODO(rbharath): This class does not yet have a
2.7.0,"TensorGraph equivalent, but one may not be required."
2.7.0,"Commented out for now, remove if OK."
2.7.0,class WeaveConcat(Layer):
2.7.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.7.0,""""""""
2.7.0,
2.7.0,"def __init__(self,"
2.7.0,"batch_size,"
2.7.0,"n_atom_input_feat=50,"
2.7.0,"n_output=128,"
2.7.0,"init='glorot_uniform',"
2.7.0,"activation='tanh',"
2.7.0,**kwargs):
2.7.0,""""""""
2.7.0,Parameters
2.7.0,----------
2.7.0,batch_size: int
2.7.0,number of molecules in a batch
2.7.0,"n_atom_input_feat: int, optional"
2.7.0,Number of features for each atom in input.
2.7.0,"n_output: int, optional"
2.7.0,Number of output features for each atom(concatenated)
2.7.0,"init: str, optional"
2.7.0,Weight initialization for filters.
2.7.0,"activation: str, optional"
2.7.0,Activation function applied
2.7.0,
2.7.0,""""""""
2.7.0,self.batch_size = batch_size
2.7.0,self.n_atom_input_feat = n_atom_input_feat
2.7.0,self.n_output = n_output
2.7.0,self.init = initializations.get(init)  # Set weight initialization
2.7.0,self.activation = activations.get(activation)  # Get activations
2.7.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.7.0,
2.7.0,def build(self):
2.7.0,"""""""""Construct internal trainable weights."
2.7.0,""""""""
2.7.0,
2.7.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.7.0,self.b = model_ops.zeros(shape=[
2.7.0,"self.n_output,"
2.7.0,])
2.7.0,
2.7.0,self.trainable_weights = self.W + self.b
2.7.0,
2.7.0,"def call(self, x, mask=None):"
2.7.0,"""""""Execute this layer on input tensors."
2.7.0,
2.7.0,"x = [atom_features, atom_mask]"
2.7.0,
2.7.0,Parameters
2.7.0,----------
2.7.0,x: list
2.7.0,Tensors as listed above
2.7.0,"mask: bool, optional"
2.7.0,Ignored. Present only to shadow superclass call() method.
2.7.0,
2.7.0,Returns
2.7.0,-------
2.7.0,outputs: Tensor
2.7.0,Tensor of concatenated atom features
2.7.0,""""""""
2.7.0,self.build()
2.7.0,atom_features = x[0]
2.7.0,atom_masks = x[1]
2.7.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.7.0,A_mask = tf.split(
2.7.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.7.0,outputs = tf.concat(
2.7.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.7.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.7.0,outputs = self.activation(outputs)
2.7.0,return outputs
2.7.0,TODO(rbharath): This class does not yet have a
2.7.0,"TensorGraph equivalent, but one may not be required."
2.7.0,"Commented out for now, remove if OK."
2.7.0,class AlternateWeaveGather(WeaveGather):
2.7.0,"""""""Alternate implementation of weave gather layer"
2.7.0,corresponding to AlternateWeaveLayer
2.7.0,""""""""
2.7.0,
2.7.0,"def call(self, x, mask=None):"
2.7.0,"""""""Execute this layer on input tensors."
2.7.0,
2.7.0,"x = [atom_features, atom_split]"
2.7.0,
2.7.0,Parameters
2.7.0,----------
2.7.0,x: list
2.7.0,Tensors as listed above
2.7.0,"mask: bool, optional"
2.7.0,Ignored. Present only to shadow superclass call() method.
2.7.0,
2.7.0,Returns
2.7.0,-------
2.7.0,outputs: Tensor
2.7.0,Tensor of molecular features
2.7.0,""""""""
2.7.0,# Add trainable weights
2.7.0,self.build()
2.7.0,outputs = x[0]
2.7.0,atom_split = x[1]
2.7.0,
2.7.0,if self.gaussian_expand:
2.7.0,outputs = self.gaussian_histogram(outputs)
2.7.0,
2.7.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.7.0,
2.7.0,if self.gaussian_expand:
2.7.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.7.0,output_molecules = self.activation(output_molecules)
2.7.0,return output_molecules
2.7.0,Each directory holds a range of assay results
2.7.0,Just write NA
2.7.0,"Now, write out the results csv, going line by line through all molecule results"
2.7.0,printing the mol_id
2.7.0,printing the SMILES
2.7.0,Now gzip it
2.7.0,Now remove the intermediate csv
2.7.0,First download all SDF files. We need these to get smiles
2.7.0,Next download all Bioassays
2.7.0,RDKit consistently hangs when trying to read this file
2.7.0,TODO (LESWING) Lazy Load
2.7.0,TODO (LESWING) Lazy Load
2.7.0,from simdna import simulations
2.7.0,define layer out functions
2.7.0,get layer outputs for a positive simulation example
2.7.0,plot layer outputs
2.7.0,highlight motif sites
2.7.0,get a positive and a negative example from the simulation data
2.7.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.7.0,get motif site locations
2.7.0,organize legends
2.7.0,plot scores and highlight motif site locations
2.7.0,initialize fwd and reverse scores to -infinity
2.7.0,"cross-correlate separately for each base,"
2.7.0,for both the PSSM and its reverse complement
2.7.0,sum over the bases
2.7.0,take max of fwd and reverse scores at each position
2.7.0,return 1D view of sequence characters
2.7.0,class SequenceDNN(Model):
2.7.0,""""""""
2.7.0,Sequence DNN models.
2.7.0,
2.7.0,Parameters
2.7.0,----------
2.7.0,"seq_length : int, optional"
2.7.0,length of input sequence.
2.7.0,"keras_model : instance of keras.models.Sequential, optional"
2.7.0,seq_length or keras_model must be specified.
2.7.0,"num_tasks : int, optional"
2.7.0,number of tasks. Default: 1.
2.7.0,num_filters : list[int] | tuple[int]
2.7.0,"number of convolutional filters in each layer. Default: (15,)."
2.7.0,conv_width : list[int] | tuple[int]
2.7.0,"width of each layer's convolutional filters. Default: (15,)."
2.7.0,pool_width : int
2.7.0,width of max pooling after the last layer. Default: 35.
2.7.0,L1 : float
2.7.0,strength of L1 penalty.
2.7.0,dropout : float
2.7.0,dropout probability in every convolutional layer. Default: 0.
2.7.0,verbose: int
2.7.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.7.0,
2.7.0,Returns
2.7.0,-------
2.7.0,Compiled DNN model.
2.7.0,""""""""
2.7.0,
2.7.0,"def __init__(self,"
2.7.0,"seq_length=None,"
2.7.0,"keras_model=None,"
2.7.0,"use_RNN=False,"
2.7.0,"num_tasks=1,"
2.7.0,"num_filters=(15, 15, 15),"
2.7.0,"conv_width=(15, 15, 15),"
2.7.0,"pool_width=35,"
2.7.0,"GRU_size=35,"
2.7.0,"TDD_size=15,"
2.7.0,"L1=0,"
2.7.0,"dropout=0.0,"
2.7.0,"num_epochs=100,"
2.7.0,verbose=1):
2.7.0,self.num_tasks = num_tasks
2.7.0,self.num_epochs = num_epochs
2.7.0,self.verbose = verbose
2.7.0,self.train_metrics = []
2.7.0,self.valid_metrics = []
2.7.0,if keras_model is not None and seq_length is None:
2.7.0,self.model = keras_model
2.7.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.7.0,elif seq_length is not None and keras_model is None:
2.7.0,self.model = Sequential()
2.7.0,assert len(num_filters) == len(conv_width)
2.7.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.7.0,conv_height = 4 if i == 0 else 1
2.7.0,self.model.add(
2.7.0,Convolution2D(
2.7.0,"nb_filter=nb_filter,"
2.7.0,"nb_row=conv_height,"
2.7.0,"nb_col=nb_col,"
2.7.0,"activation='linear',"
2.7.0,"init='he_normal',"
2.7.0,"input_shape=(1, 4, seq_length),"
2.7.0,"W_regularizer=l1(L1),"
2.7.0,b_regularizer=l1(L1)))
2.7.0,self.model.add(Activation('relu'))
2.7.0,self.model.add(Dropout(dropout))
2.7.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.7.0,if use_RNN:
2.7.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.7.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.7.0,"self.model.add(Permute((2, 1)))"
2.7.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.7.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.7.0,self.model.add(Flatten())
2.7.0,self.model.add(Dense(output_dim=self.num_tasks))
2.7.0,self.model.add(Activation('sigmoid'))
2.7.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.7.0,else:
2.7.0,raise ValueError(
2.7.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.7.0,
2.7.0,"def train(self,"
2.7.0,"X,"
2.7.0,"y,"
2.7.0,"validation_data,"
2.7.0,"early_stopping_metric='Loss',"
2.7.0,"early_stopping_patience=5,"
2.7.0,save_best_model_to_prefix=None):
2.7.0,if y.dtype != bool:
2.7.0,"assert set(np.unique(y)) == {0, 1}"
2.7.0,y = y.astype(bool)
2.7.0,multitask = y.shape[1] > 1
2.7.0,if not multitask:
2.7.0,num_positives = y.sum()
2.7.0,num_sequences = len(y)
2.7.0,num_negatives = num_sequences - num_positives
2.7.0,if self.verbose >= 1:
2.7.0,print('Training model (* indicates new best result)...')
2.7.0,"X_valid, y_valid = validation_data"
2.7.0,early_stopping_wait = 0
2.7.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.7.0,"for epoch in range(1, self.num_epochs + 1):"
2.7.0,self.model.fit(
2.7.0,"X,"
2.7.0,"y,"
2.7.0,"batch_size=128,"
2.7.0,"nb_epoch=1,"
2.7.0,class_weight={
2.7.0,"True: num_sequences / num_positives,"
2.7.0,False: num_sequences / num_negatives
2.7.0,"} if not multitask else None,"
2.7.0,verbose=self.verbose >= 2)
2.7.0,"epoch_train_metrics = self.test(X, y)"
2.7.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.7.0,self.train_metrics.append(epoch_train_metrics)
2.7.0,self.valid_metrics.append(epoch_valid_metrics)
2.7.0,if self.verbose >= 1:
2.7.0,print('Epoch {}:'.format(epoch))
2.7.0,print('Train {}'.format(epoch_train_metrics))
2.7.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.7.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.7.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.7.0,if self.verbose >= 1:
2.7.0,print(' *')
2.7.0,best_metric = current_metric
2.7.0,best_epoch = epoch
2.7.0,early_stopping_wait = 0
2.7.0,if save_best_model_to_prefix is not None:
2.7.0,self.save(save_best_model_to_prefix)
2.7.0,else:
2.7.0,if self.verbose >= 1:
2.7.0,print()
2.7.0,if early_stopping_wait >= early_stopping_patience:
2.7.0,break
2.7.0,early_stopping_wait += 1
2.7.0,if self.verbose >= 1:
2.7.0,print('Finished training after {} epochs.'.format(epoch))
2.7.0,if save_best_model_to_prefix is not None:
2.7.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.7.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.7.0,"best_epoch, save_best_model_to_prefix))"
2.7.0,
2.7.0,"def predict(self, X):"
2.7.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.7.0,
2.7.0,def get_sequence_filters(self):
2.7.0,""""""""
2.7.0,Returns 3D array of 2D sequence filters.
2.7.0,""""""""
2.7.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.7.0,
2.7.0,"def deeplift(self, X, batch_size=200):"
2.7.0,""""""""
2.7.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.7.0,""""""""
2.7.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.7.0,from deeplift.conversion import keras_conversion as kc
2.7.0,
2.7.0,# convert to deeplift model and get scoring function
2.7.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.7.0,score_func = deeplift_model.get_target_contribs_func(
2.7.0,find_scores_layer_idx=0)
2.7.0,# use a 40% GC reference
2.7.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.7.0,# get deeplift scores
2.7.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.7.0,for i in range(self.num_tasks):
2.7.0,deeplift_scores[i] = score_func(
2.7.0,"task_idx=i,"
2.7.0,"input_data_list=[X],"
2.7.0,"batch_size=batch_size,"
2.7.0,"progress_update=None,"
2.7.0,input_references_list=input_references)
2.7.0,return deeplift_scores
2.7.0,
2.7.0,"def in_silico_mutagenesis(self, X):"
2.7.0,""""""""
2.7.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.7.0,""""""""
2.7.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.7.0,wild_type_predictions = self.predict(X)
2.7.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.7.0,np.newaxis]
2.7.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.7.0,"zip(X, wild_type_predictions)):"
2.7.0,mutated_sequences = np.repeat(
2.7.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.7.0,# remove wild-type
2.7.0,arange = np.arange(len(mutated_sequences))
2.7.0,horizontal_cycle = np.tile(
2.7.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.7.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.7.0,# add mutant
2.7.0,vertical_repeat = np.repeat(
2.7.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.7.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.7.0,# make mutant predictions
2.7.0,mutated_predictions = self.predict(mutated_sequences)
2.7.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.7.0,"(self.num_tasks,))"
2.7.0,mutagenesis_scores[
2.7.0,sequence_index] = wild_type_prediction - mutated_predictions
2.7.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.7.0,
2.7.0,@staticmethod
2.7.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.7.0,from dragonn.plot import plot_bases_on_ax
2.7.0,scores = score_func(X).squeeze(
2.7.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.7.0,try:
2.7.0,os.makedirs(output_directory)
2.7.0,except OSError:
2.7.0,pass
2.7.0,num_tasks = len(scores)
2.7.0,"for task_index, task_scores in enumerate(scores):"
2.7.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.7.0,# sequence_scores is num_bases x sequence_length
2.7.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.7.0,plt.clf()
2.7.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.7.0,top_axis.plot(
2.7.0,"range(1,"
2.7.0,"len(basewise_max_sequence_scores) + 1),"
2.7.0,basewise_max_sequence_scores)
2.7.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.7.0,peak_position = basewise_max_sequence_scores.argmax()
2.7.0,top_axis.axvspan(
2.7.0,"peak_position - peak_width,"
2.7.0,"peak_position + peak_width,"
2.7.0,"color='grey',"
2.7.0,alpha=0.1)
2.7.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.7.0,peak_position + peak_width].T
2.7.0,# Set non-max letter_heights to zero
2.7.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.7.0,"letter_heights[np.arange(len(letter_heights)),"
2.7.0,peak_sequence_scores.argmax(axis=1)] = \
2.7.0,basewise_max_sequence_scores[peak_position - peak_width :
2.7.0,peak_position + peak_width]
2.7.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.7.0,bottom_axis.set_xticklabels(
2.7.0,tuple(
2.7.0,"map(str,"
2.7.0,"np.arange(peak_position - peak_width,"
2.7.0,peak_position + peak_width + 1))))
2.7.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.7.0,plt.xlabel('Position')
2.7.0,plt.ylabel('Score')
2.7.0,plt.savefig(
2.7.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.7.0,"sequence_index, '_task_{}'.format(task_index)"
2.7.0,if num_tasks > 1 else '')))
2.7.0,plt.close()
2.7.0,
2.7.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.7.0,self._plot_scores(
2.7.0,"X,"
2.7.0,"output_directory,"
2.7.0,"peak_width,"
2.7.0,"score_func=self.deeplift,"
2.7.0,score_name='DeepLift')
2.7.0,
2.7.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.7.0,self._plot_scores(
2.7.0,"X,"
2.7.0,"output_directory,"
2.7.0,"peak_width,"
2.7.0,"score_func=self.in_silico_mutagenesis,"
2.7.0,score_name='ISM')
2.7.0,
2.7.0,"def plot_architecture(self, output_file):"
2.7.0,from dragonn.visualize_util import plot as plot_keras_model
2.7.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.7.0,
2.7.0,"def save(self, save_best_model_to_prefix):"
2.7.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.7.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.7.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.7.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.7.0,
2.7.0,@staticmethod
2.7.0,"def load(arch_fname, weights_fname=None):"
2.7.0,model_json_string = open(arch_fname).read()
2.7.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.7.0,if weights_fname is not None:
2.7.0,sequence_dnn.model.load_weights(weights_fname)
2.7.0,return sequence_dnn
2.7.0,create temporary fasta files
2.7.0,run command
2.7.0,remove fasta files
2.7.0,write test fasta file
2.7.0,test gkmsvm
2.7.0,get classification results
2.7.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.7.0,"Using canonical smiles for glycine, as in original research paper"
2.7.0,Atom features with padding
2.7.0,A_tilda_k computation
2.7.0,Final feed_dict setup
2.7.0,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.7.0,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.7.0,self.num_node_features)
2.7.0,Fit models
2.7.0,Args
2.7.0,2017 DeepCrystal Technologies - Patrick Hop
2.7.0,
2.7.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.7.0,
2.7.0,MIT License - have fun!!
2.7.0,===========================================================
2.7.0,x = F.selu( fc(x) )
2.7.0,x = F.selu( fc(x) )
2.7.0,2017 DeepCrystal Technologies - Patrick Hop
2.7.0,
2.7.0,Data loading a splitting file
2.7.0,
2.7.0,MIT License - have fun!!
2.7.0,===========================================================
2.7.0,Args
2.7.0,TODO (VIGS25): Account for the reload option
2.7.0,Downloading train files
2.7.0,Parsing training data
2.7.0,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.7.0,Test Files loading
2.7.0,One Hot Featurization
2.7.0,Consistency check
2.7.0,Handle output layer
2.7.0,Iterate over all previous tasks.
2.7.0,prev_layers is a list with elements of size
2.7.0,"(batch_size, layer_sizes[i-1])"
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,Save an initial checkpoint.
2.7.0,Turns out there are valid cases where we don't want pad-batches
2.7.0,on by default.
2.7.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.0,Run training op.
2.7.0,Always save a final checkpoint when complete.
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,Note that we divide by the batch size and not the number of
2.7.0,"non-zero weight examples in the batch.  Also, instead of using"
2.7.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.0,calculate with div/sum so it stays on the GPU.
2.7.0,aggregated costs
2.7.0,weight decay
2.7.0,Dummy placeholders
2.7.0,Dummy placeholders
2.7.0,run eval data through the model
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,Handle edge case when batch-size is 1.
2.7.0,Prune away any padding that was added
2.7.0,allow_soft_placement=True allows ops without a GPU implementation
2.7.0,to run on the CPU instead.
2.7.0,!/usr/bin/python
2.7.0,
2.7.0,Copyright 2015 Google Inc.
2.7.0,
2.7.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.7.0,you may not use this file except in compliance with the License.
2.7.0,You may obtain a copy of the License at
2.7.0,
2.7.0,http://www.apache.org/licenses/LICENSE-2.0
2.7.0,
2.7.0,"Unless required by applicable law or agreed to in writing, software"
2.7.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.7.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.7.0,See the License for the specific language governing permissions and
2.7.0,limitations under the License.
2.7.0,parse CheckpointState proto
2.7.0,parse path to actual checkpoint
2.7.0,the provided mask has to be the same shape as features
2.7.0,test k = 1..4
2.7.0,central moments
2.7.0,standardized moments
2.7.0,central across one axis
2.7.0,standardized across one axis
2.7.0,Fit just on task zero
2.7.0,Notice that we keep the session open
2.7.0,Fit on task one
2.7.0,The predictions for task zero should not change after training
2.7.0,on task one.
2.7.0,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.7.0,!/usr/bin/python
2.7.0,
2.7.0,Copyright 2015 Google Inc.
2.7.0,
2.7.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.7.0,you may not use this file except in compliance with the License.
2.7.0,You may obtain a copy of the License at
2.7.0,
2.7.0,http://www.apache.org/licenses/LICENSE-2.0
2.7.0,
2.7.0,"Unless required by applicable law or agreed to in writing, software"
2.7.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.7.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.7.0,See the License for the specific language governing permissions and
2.7.0,limitations under the License.
2.7.0,get the divisor
2.7.0,compute the requested central moment
2.7.0,"note that mean is a raw moment, not a central moment"
2.7.0,TODO(user): median is not implemented yet in TensorFlow
2.7.0,Add the input features.
2.7.0,"layer has shape [None, layer_sizes[i]]"
2.7.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.7.0,TODO(rbharath): Might want to make it feasible to have multiple
2.7.0,bypass layers.
2.7.0,Construct task bypass layer
2.7.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.7.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.7.0,"layer has shape [None, layer_sizes[i]]"
2.7.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.7.0,TODO(rbharath): Might want to make it feasible to have multiple
2.7.0,bypass layers.
2.7.0,Construct task bypass layer
2.7.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.7.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.7.0,Consistency check
2.7.0,Lazily created by _get_shared_session().
2.7.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.7.0,when subclass-overridden methods use the same scopes.
2.7.0,Setup graph
2.7.0,Create placeholders
2.7.0,Handle output layer
2.7.0,Iterate over all previous tasks.
2.7.0,prev_layers is a list with elements of size
2.7.0,"(batch_size, layer_sizes[i-1])"
2.7.0,Note that we divide by the batch size and not the number of
2.7.0,"non-zero weight examples in the batch.  Also, instead of using"
2.7.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.0,calculate with div/sum so it stays on the GPU.
2.7.0,aggregated costs
2.7.0,weight decay
2.7.0,Dummy placeholders
2.7.0,Dummy placeholders
2.7.0,run eval data through the model
2.7.0,"Shape (n_tasks, n__samples)"
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,Handle edge case when batch-size is 1.
2.7.0,with self._get_shared_session(train=True) as sess:
2.7.0,Save an initial checkpoint.
2.7.0,Always save a final checkpoint when complete.
2.7.0,Note that we divide by the batch size and not the number of
2.7.0,"non-zero weight examples in the batch.  Also, instead of using"
2.7.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.0,calculate with div/sum so it stays on the GPU.
2.7.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.7.0,Dummy placeholders
2.7.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.7.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.7.0,Dummy placeholders
2.7.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.7.0,allow_soft_placement=True allows ops without a GPU implementation
2.7.0,to run on the CPU instead.
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,Turns out there are valid cases where we don't want pad-batches
2.7.0,on by default.
2.7.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.7.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,"(n_samples, n_classes)"
2.7.0,"(n_samples, n_tasks, n_classes)"
2.7.0,Save hyperparameters
2.7.0,Guard variable to make sure we don't Restore() this model
2.7.0,from a disk checkpoint more than once.
2.7.0,"Path to save checkpoint files, which matches the"
2.7.0,replicated supervisor's default path.
2.7.0,Lazily created by _get_shared_session().
2.7.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.7.0,when subclass-overridden methods use the same scopes.
2.7.0,Setup graph
2.7.0,Note that we divide by the batch size and not the number of
2.7.0,"non-zero weight examples in the batch.  Also, instead of using"
2.7.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.0,calculate with div/sum so it stays on the GPU.
2.7.0,aggregated costs
2.7.0,weight decay
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,Save an initial checkpoint.
2.7.0,Define the code that runs on a separate thread to feed data into the queue.
2.7.0,Main training loop.
2.7.0,Run training op.
2.7.0,We have reached the end of an epoch.
2.7.0,We have reached the end of the data.
2.7.0,Always save a final checkpoint when complete.
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,allow_soft_placement=True allows ops without a GPU implementation
2.7.0,to run on the CPU instead.
2.7.0,gpu memory growth option
2.7.0,gpu memory growth option
2.7.0,TODO(rbharath): Is setting train=False right here?
2.7.0,Discard any padded predictions
2.7.0,The iterbatches does padding with zero-weight examples on the last batch.
2.7.0,Remove padded examples.
2.7.0,Special case to handle singletasks.
2.7.0,The iterbatches does padding with zero-weight examples on the last batch.
2.7.0,Remove padded examples.
2.7.0,TODO(rbharath): Verify this can be safely removed.
2.7.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.7.0,""""""""
2.7.0,Evaluates the performance of this model on specified dataset.
2.7.0,
2.7.0,Parameters
2.7.0,----------
2.7.0,dataset: dc.data.Dataset
2.7.0,Dataset object.
2.7.0,metric: deepchem.metrics.Metric
2.7.0,Evaluation metric
2.7.0,transformers: list
2.7.0,List of deepchem.transformers.Transformer
2.7.0,Returns
2.7.0,-------
2.7.0,dict
2.7.0,Maps tasks to scores under metric.
2.7.0,""""""""
2.7.0,"evaluator = Evaluator(self, dataset, transformers)"
2.7.0,scores = evaluator.compute_model_performance(metrics)
2.7.0,return scores
2.7.0,checkpoints look like model_dir/model.ckpt-N
2.7.0,"self._save_path is ""model_dir/model.ckpt"""
2.7.0,run eval data through the model
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,run eval data through the model
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,Note that softmax is already applied in construct_grpah
2.7.0,run eval data through the model
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,Handle edge case when batch-size is 1.
2.7.0,Prune away any padding that was added
2.7.0,Handle case of 0-dimensional scalar output
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,inputs placeholder
2.7.0,data preprocessing and augmentation
2.7.0,first conv layer
2.7.0,downsample by max pooling
2.7.0,each module is a residual convolutional block
2.7.0,followed by a convolutional downsample layer
2.7.0,max pooling over the final outcome
2.7.0,fully connected layers
2.7.0,dropout for dense layers
2.7.0,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.7.0,weight decay regularizer
2.7.0,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.7.0,sample cut ratio from a clipped gaussian
2.7.0,train/valid differences
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,Define and build model
2.7.0,model.restore()
2.7.0,Set random seeds
2.7.0,Setup directories
2.7.0,Model constants
2.7.0,Load and transform datasets
2.7.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.0,Atomic convolution variables
2.7.0,at = atomic numbers (atom types)
2.7.0,"radial basis function parameters [cutoff, mean, width]"
2.7.0,Model hyperparameters
2.7.0,Initialize model
2.7.0,Fit model
2.7.0,Evaluate model
2.7.0,Set random seeds
2.7.0,Setup directories
2.7.0,Model constants
2.7.0,Load and transform datasets
2.7.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.0,Atomic convolution variables
2.7.0,at = atomic numbers (atom types)
2.7.0,"radial basis function parameters [cutoff, mean, width]"
2.7.0,Model hyperparameters
2.7.0,Initialize model
2.7.0,Fit model
2.7.0,Evaluate model
2.7.0,Set random seeds
2.7.0,Setup directories
2.7.0,Model constants
2.7.0,Load and transform datasets
2.7.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.0,Atomic convolution variables
2.7.0,at = atomic numbers (atom types)
2.7.0,"radial basis function parameters [cutoff, mean, width]"
2.7.0,Model hyperparameters
2.7.0,Initialize model
2.7.0,Fit model
2.7.0,Evaluate model
2.7.0,Set random seeds
2.7.0,Setup directories
2.7.0,Model constants
2.7.0,Load and transform datasets
2.7.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.7.0,Atomic convolution variables
2.7.0,at = atomic numbers (atom types)
2.7.0,"radial basis function parameters [cutoff, mean, width]"
2.7.0,Model hyperparameters
2.7.0,Initialize model
2.7.0,Fit model
2.7.0,Evaluate model
2.7.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.7.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.7.0,test_scores = test_evaluator.compute_model_performance(metric)
2.7.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.7.0,param.update(test_scores)
2.7.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.0,for transformer in transformers:
2.7.0,train_dataset = transformer.transform(train_dataset)
2.7.0,test_dataset = transformer.transform(test_dataset)
2.7.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.0,for transformer in transformers:
2.7.0,train_dataset = transformer.transform(train_dataset)
2.7.0,test_dataset = transformer.transform(test_dataset)
2.7.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.0,for transformer in transformers:
2.7.0,train_dataset = transformer.transform(train_dataset)
2.7.0,test_dataset = transformer.transform(test_dataset)
2.7.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.7.0,for transformer in transformers:
2.7.0,train_dataset = transformer.transform(train_dataset)
2.7.0,test_dataset = transformer.transform(test_dataset)
2.7.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.7.0,Create some directories for analysis
2.7.0,The base_dir holds the results of all analysis
2.7.0,Make directories to store the raw and featurized datasets.
2.7.0,Load PDBBind dataset
2.7.0,Define featurizers
2.7.0,Currently featurizes with shard_size=1
2.7.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.7.0,This could be done with openbabel in python
2.7.0,Compute cells for this molecule. O(constant)
2.7.0,min == max if molecule is planar in some direction
2.7.0,we should still create a bin
2.7.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.7.0,Note neighbors contains self!
2.7.0,Associate each atom with cell it belongs to. O(N)
2.7.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.0,"conditions, so does wrapround. O(constant)"
2.7.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.7.0,Accept as neighbors only those within threshold. This computation should be
2.7.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.7.0,Sort neighbors by distance
2.7.0,Pick up to max_num_neighbors
2.7.0,Type of data created by this featurizer
2.7.0,assumes that every array is of the same dimension
2.7.0,rem_dataset is remaining portion of dataset
2.7.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.7.0,to k-1.
2.7.0,returns list of per column sum of non zero elements
2.7.0,Compute number of actives needed per task.
2.7.0,loop through each column and obtain index required to splice out for
2.7.0,required fraction of hits
2.7.0,Find the first index where the cumulative number of actives equals
2.7.0,the actives_count
2.7.0,Note that np.where tells us last index required to exceed
2.7.0,"actives_count, so we actually want the following location"
2.7.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.7.0,potentially refactor those to match this.
2.7.0,Handle edge case where frac_split is 1
2.7.0,Create weight matrices fpor two haves.
2.7.0,copy over up to required index for weight first_split
2.7.0,check out if any rows in either w_1 or w_2 are just zeros
2.7.0,"Obtain original x, y, and w arrays and shuffle"
2.7.0,calculate percent split for valid (out of test and valid)
2.7.0,"split test data into valid and test, treating sub test set also as sparse"
2.7.0,rem_dataset is remaining portion of dataset
2.7.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.7.0,to k-1.
2.7.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.7.0,This can be generalized in the future with some common demoninator determination.
2.7.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.7.0,Append remaining examples to train
2.7.0,Sort by increasing MW
2.7.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.7.0,for m_idx in cluster:
2.7.0,"continue until we find an active in all the tasks, otherwise we can't"
2.7.0,compute a meaningful AUC
2.7.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.7.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.7.0,Sort from largest to smallest scaffold sets
2.7.0,Sort from largest to smallest scaffold sets
2.7.0,"(n_samples, n_classes)"
2.7.0,"(n_samples, n_tasks, n_classes)"
2.7.0,Save hyperparameters
2.7.0,Guard variable to make sure we don't Restore() this model
2.7.0,from a disk checkpoint more than once.
2.7.0,"Path to save checkpoint files, which matches the"
2.7.0,replicated supervisor's default path.
2.7.0,Lazily created by _get_shared_session().
2.7.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.7.0,when subclass-overridden methods use the same scopes.
2.7.0,Setup graph
2.7.0,Note that we divide by the batch size and not the number of
2.7.0,"non-zero weight examples in the batch.  Also, instead of using"
2.7.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.7.0,calculate with div/sum so it stays on the GPU.
2.7.0,aggregated costs
2.7.0,weight decay
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,Save an initial checkpoint.
2.7.0,Turns out there are valid cases where we don't want pad-batches
2.7.0,on by default.
2.7.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.7.0,Run training op.
2.7.0,Always save a final checkpoint when complete.
2.7.0,############################################################# TIMING
2.7.0,############################################################# TIMING
2.7.0,allow_soft_placement=True allows ops without a GPU implementation
2.7.0,to run on the CPU instead.
2.7.0,TODO(rbharath): Is setting train=False right here?
2.7.0,Discard any padded predictions
2.7.0,The iterbatches does padding with zero-weight examples on the last batch.
2.7.0,Remove padded examples.
2.7.0,Special case to handle singletasks.
2.7.0,The iterbatches does padding with zero-weight examples on the last batch.
2.7.0,Remove padded examples.
2.7.0,TODO(rbharath): Verify this can be safely removed.
2.7.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.7.0,""""""""
2.7.0,Evaluates the performance of this model on specified dataset.
2.7.0,
2.7.0,Parameters
2.7.0,----------
2.7.0,dataset: dc.data.Dataset
2.7.0,Dataset object.
2.7.0,metric: deepchem.metrics.Metric
2.7.0,Evaluation metric
2.7.0,transformers: list
2.7.0,List of deepchem.transformers.Transformer
2.7.0,Returns
2.7.0,-------
2.7.0,dict
2.7.0,Maps tasks to scores under metric.
2.7.0,""""""""
2.7.0,"evaluator = Evaluator(self, dataset, transformers)"
2.7.0,scores = evaluator.compute_model_performance(metrics)
2.7.0,return scores
2.7.0,checkpoints look like logdir/model.ckpt-N
2.7.0,"self._save_path is ""logdir/model.ckpt"""
2.7.0,run eval data through the model
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,run eval data through the model
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,Note that softmax is already applied in construct_grpah
2.7.0,run eval data through the model
2.7.0,reshape to batch_size x n_tasks x ...
2.7.0,Handle edge case when batch-size is 1.
2.7.0,Prune away any padding that was added
2.7.0,Handle case of 0-dimensional scalar output
2.7.0,Dummy placeholders
2.7.0,Dummy placeholders
2.7.0,## AtomicNet fully-connected layer ops ###
2.7.0,## Atomicnet coordinate transform ops ###
2.7.0,## Atomicnet symmetry function kernel ops ###
2.7.0,## Atomicnet symmetry function ops ###
2.7.0,## Atomcnet symmetry function layer ops ###
2.7.0,We apply the radial pooling filter before atom type conv
2.7.0,to reduce computation
2.7.0,## Misc convenience ops ###
2.7.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.7.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.7.0,"game).  The average reward for any bet is slightly negative, so the best"
2.7.0,strategy is to walk away.
2.7.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.0,Optimize it.
2.7.0,"It should have learned that the expected value is very close to zero, and that the best"
2.7.0,action is to walk away.
2.7.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.7.0,get the same result.
2.7.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.0,Run the algorithm.
2.7.0,Save a file checkpoint.
2.7.0,Build the tree.
2.7.0,Compute the final probabilities and expected reward.
2.7.0,Mark this node as terminal
2.7.0,Expand this node.
2.7.0,Select the next action to perform.
2.7.0,Recursively build the tree.
2.7.0,Update statistics for this node.
2.7.0,Configuration file for the Sphinx documentation builder.
2.7.0,
2.7.0,This file only contains a selection of the most common options. For a full
2.7.0,list see the documentation:
2.7.0,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.7.0,-- Path setup --------------------------------------------------------------
2.7.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.7.0,add these directories to sys.path here. If the directory is relative to the
2.7.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.7.0,
2.7.0,-- Project information -----------------------------------------------------
2.7.0,"The full version, including alpha/beta/rc tags"
2.7.0,-- General configuration ---------------------------------------------------
2.7.0,"Add any Sphinx extension module names here, as strings. They can be"
2.7.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.7.0,ones.
2.7.0,Options for autodoc directives
2.7.0,How to represents typehints
2.7.0,"Add any paths that contain templates here, relative to this directory."
2.7.0,The suffix of source filenames.
2.7.0,The master toctree document.
2.7.0,autosectionlabel setting
2.7.0,"List of patterns, relative to source directory, that match files and"
2.7.0,directories to ignore when looking for source files.
2.7.0,This pattern also affects html_static_path and html_extra_path.
2.7.0,"If true, the current module name will be prepended to all description"
2.7.0,unit titles (such as .. function::).
2.7.0,-- Options for HTML output -------------------------------------------------
2.7.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.7.0,a list of builtin themes.
2.7.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.7.0,"relative to this directory. They are copied after the builtin static files,"
2.7.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.7.0,The name of an image file (relative to this directory) to place at the top
2.7.0,of the sidebar.
2.7.0,Customize the sphinx theme
2.7.0,-- Source code links ---------------------------------------------------
2.7.0,Resolve function for the linkcode extension.
2.7.0,"try to find the file and line number, based on code from numpy:"
2.7.0,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.7.0,lines in the label file have format
2.7.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.7.0,"print line[0], line[3]"
2.7.0,"If you push the tag, please remove `.dev`"
2.7.0,Record inputs.
2.7.0,Create the output directory if necessary.
2.7.0,Create the optimizers for meta-optimization and task optimization.
2.7.0,Create a Checkpoint for saving.
2.7.0,Main optimization loop.
2.7.0,Do checkpointing.
2.7.0,flake8: noqa
2.7.0,This is a MetaLearner that learns to generate sine functions with variable
2.7.0,amplitude and phase.
2.7.0,Optimize it.
2.7.0,Test it out on some new tasks and see how it works.
2.7.0,Initially the model should do a bad job of fitting the sine function.
2.7.0,After one step of optimization it should do much better.
2.7.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.7.0,get the same result.
2.7.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.0,We know use_pose_generator_scores == False in this case
2.7.0,check whether self.featurizer is instance of ComplexFeaturizer or not
2.7.0,TODO: How to handle the failure here?
2.7.0,TODO(rbharath): The autodock vina source computes surface distances
2.7.0,which take into account the van der Waals radius of each atom type.
2.7.0,"Shape (N, M)"
2.7.0,"Shape (N, M)"
2.7.0,Parse complex
2.7.0,check filetypes
2.7.0,Define locations of log and output files
2.7.0,Write GNINA conf file
2.7.0,Run GNINA
2.7.0,read output and log
2.7.0,Parse complex
2.7.0,Prepare protein
2.7.0,Get protein centroid and range
2.7.0,TODO(rbharath: Does vina divide box dimensions by 2?
2.7.0,Prepare ligand
2.7.0,Write Vina conf file
2.7.0,Define locations of output files
2.7.0,flake8: noqa
2.7.0,We provide no scoring model so the docker won't score
2.7.0,Check only one output since num_modes==1
2.7.0,We provide no scoring model so the docker won't score
2.7.0,Check only one output since num_modes==1
2.7.0,Let's turn on logging since this test will run for a while
2.7.0,Check returned files exist
2.7.0,Let's turn on logging since this test will run for a while
2.7.0,Check returned files exist
2.7.0,"Where d is greater than zero, the repulsion is just zeros"
2.7.0,"When d is 0, this should just be 1"
2.7.0,"When d == 0, the hbond interaction is 0"
2.7.0,The exponential returns 1 when input 0.
2.7.0,This exponential returns 1 when input 3
2.7.0,Let's turn on logging since this test will run for a while
2.7.0,Let's turn on logging since this test will run for a while
2.7.0,Let's turn on logging since this test will run for a while
2.7.0,Let's turn on logging since this test will run for a while
2.7.0,Let's turn on logging since this test will run for a while
2.7.0,Note this may download autodock Vina...
2.7.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.7.0,Test that every atom in pocket maps exists
2.7.0,scalar case
2.7.0,per-example case
2.7.0,This is a little arcane but it repeats w across tasks.
2.7.0,"If w.shape == (n_samples, 1) handle it as 1D"
2.7.0,"w.shape == (n_samples, n_tasks)"
2.7.0,scalar case
2.7.0,Handle n_classes/n_task shape ambiguity
2.7.0,Add in task dimension
2.7.0,Insert a task dimension (we know n_tasks=1 from above0
2.7.0,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.7.0,Handle classification. We need to convert labels into one-hot representation.
2.7.0,check whether n_classes is int or not
2.7.0,Handle n_classes/n_task shape ambiguity
2.7.0,Add in task dimension
2.7.0,Make everything 2D so easy to handle
2.7.0,Handle each task separately.
2.7.0,Handle continuous class probabilites of positive class for binary
2.7.0,Fill in class 0 probabilities
2.7.0,Add a task dimension to concatenate on
2.7.0,Handle binary labels
2.7.0,"make y_hot of shape (N, n_classes)"
2.7.0,Add a task dimension to concatenate on
2.7.0,Insert a task dimension
2.7.0,"Now of shape (N,)"
2.7.0,"Now of shape (N, 1)"
2.7.0,"Returns shape (N, n_tasks)"
2.7.0,"Now of shape (N,)"
2.7.0,"Now of shape (N, n_classes)"
2.7.0,"Now of shape (N, 1, n_classes)"
2.7.0,"Returns shape (N, n_tasks, n_classes)"
2.7.0,These are some smart defaults
2.7.0,These are some smart defaults corresponding to sklearn's required
2.7.0,behavior
2.7.0,Attempt some limited shape imputation to find n_tasks
2.7.0,check whether n_tasks is int or not
2.7.0,This is because `normalize_weight_shape` require int value.
2.7.0,FIXME: Incompatible types in assignment
2.7.0,Attempt to convert both into the same type
2.7.0,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.7.0,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.7.0,initialize fwd and reverse scores to -infinity
2.7.0,"cross-correlate separately for each base,"
2.7.0,for both the PSSM and its reverse complement
2.7.0,sum over the bases
2.7.0,take max of fwd and reverse scores at each position
2.7.0,"Shape (N_sequences, num_tasks)"
2.7.0,check whether wild_type_predictions is np.ndarray or not
2.7.0,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.7.0,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.7.0,Mutates every position of the sequence to every letter
2.7.0,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.7.0,Breakdown:
2.7.0,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.7.0,remove wild-type
2.7.0,len(arange) = N_letters * sequence_length
2.7.0,len(horizontal cycle) = N_letters * sequence_length
2.7.0,add mutant
2.7.0,make mutant predictions
2.7.0,check whether wild_type_predictions is np.ndarray or not
2.7.0,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.7.0,validation
2.7.0,flake8: noqa
2.7.0,metric class
2.7.0,metrics utils
2.7.0,sklearn & scipy score function
2.7.0,original score function
2.7.0,Get a random prediction matrix
2.7.0,"Of shape (N, n_classes)"
2.7.0,"Of shape (N, 1, n_classes)"
2.7.0,This has w for each task.
2.7.0,Best score case
2.7.0,Worst score case
2.7.0,best case
2.7.0,duplicate prediction value
2.7.0,Encode motif
2.7.0,"sequences now has shape (3, 4, 5, 1)"
2.7.0,"sequences now has shape (3, 4, 5, 1)"
2.7.0,Construct and train SequenceDNN model
2.7.0,Call in-silico mutagenesis
2.7.0,Construct and train SequenceDNN model
2.7.0,Call in-silico mutagenesis
2.7.0,Check nonzero elements exist
2.7.0,Special case handling of single input
2.7.0,Featurize task results iff they exist.
2.7.0,Filter out examples where featurization failed.
2.7.0,"For prospective data where results are unknown, it"
2.7.0,makes no sense to have y values or weights.
2.7.0,Featurize task results if they exist.
2.7.0,Filter out examples where featurization failed.
2.7.0,"For prospective data where results are unknown, it"
2.7.0,makes no sense to have y values or weights.
2.7.0,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.7.0,The field in which load_sdf_files return value stores smiles
2.7.0,Special case handling of single input
2.7.0,Featurize task results iff they exist.
2.7.0,Filter out examples where featurization failed.
2.7.0,"For prospective data where results are unknown, it"
2.7.0,makes no sense to have y values or weights.
2.7.0,Process legacy toggle
2.7.0,Set attributes
2.7.0,Handle special featurizer cases
2.7.0,Set self.featurizer
2.7.0,"(X, y, w, ids)"
2.7.0,TODO don't convert all sequences into np array (allow shards)
2.7.0,Check if line is a header
2.7.0,Handle empty sequence
2.7.0,Annotate start/stop of sequence
2.7.0,Open index file
2.7.0,create an empty list to store lines in files.
2.7.0,iterate through each line in the input file
2.7.0,If the number of lines iterated through is equal or less than the shard size:
2.7.0,append to list
2.7.0,else yield the list
2.7.0,set the line_number variable to the last line number (num) before 'yield' was called
2.7.0,yield list (shard/batch)
2.7.0,Re-initialize list with the index line to begin a new shard.
2.7.0,Set attributes
2.7.0,Handle special featurizer cases
2.7.0,Set self.featurizer
2.7.0,Set self.return_quality_scores
2.7.0,Featurize sequences
2.7.0,"(X, y , w, ids)"
2.7.0,Featurize sequences
2.7.0,"(X, y , w, ids)"
2.7.0,Go through each sequence entity in the fastq_file: each sequence consists of 4 lines
2.7.0,First line : header description
2.7.0,second line : sequence
2.7.0,third line : more description usually the same as the first line
2.7.0,fourth line: quality scores of the sequence
2.7.0,Second line : add sequence to the sequence array
2.7.0,Fourth line
2.7.0,Handle empty sequence
2.7.0,Annotate start/stop of sequence
2.7.0,Sometimes zip files contain directories within. Traverse directories
2.7.0,TODO(rbharath): Add support for more extensions
2.7.0,Sort image files
2.7.0,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.7.0,Remove support indices
2.7.0,Remove support indices
2.7.0,Remove support indices
2.7.0,Get task specific entries
2.7.0,Now just get weights for this task
2.7.0,Get task specific entries
2.7.0,Now just get weights for this task
2.7.0,Now just get weights for this task
2.7.0,Now just get weights for this task
2.7.0,Split data into pos and neg lists.
2.7.0,No replacement allowed for supports
2.7.0,Handle one-d vs. non one-d feature matrices
2.7.0,Init the iterator
2.7.0,Set initial iterator state
2.7.0,support = self.supports[task][self.trial_num]
2.7.0,Increment and update logic
2.7.0,Init the iterator
2.7.0,Set initial iterator state
2.7.0,support = self.supports[task][self.trial_num]
2.7.0,Increment and update logic
2.7.0,Ensure that every worker will pick the same random order for each epoch.
2.7.0,Ensure that every worker will pick the same random order for each epoch.
2.7.0,"By invariant of when this is called, can assume num_samples > 0"
2.7.0,and num_samples < batch_size
2.7.0,Fill in batch arrays
2.7.0,"By invariant of when this is called, can assume num_samples > 0"
2.7.0,and num_samples < batch_size
2.7.0,Fill in batch arrays
2.7.0,Only the first set of copy will be counted in training loss
2.7.0,Retrieve the first sample so we can determine the dtypes.
2.7.0,Create a Tensorflow Dataset.
2.7.0,Find the X values.
2.7.0,Find the y values.
2.7.0,Find the w values.
2.7.0,Find the ids.
2.7.0,"Set labels to be zero, with zero weights"
2.7.0,The line here assumes that y generated by shard_generator is a numpy array
2.7.0,Load obsolete format -> save in new format
2.7.0,note that this corresponds to the _construct_metadata column order
2.7.0,Create temp directory to store resharded version
2.7.0,Get correct shapes for y/w
2.7.0,Write data in new shards
2.7.0,Handle shapes
2.7.0,Note that this means that DiskDataset resharding currently doesn't
2.7.0,work for datasets that aren't regression/classification.
2.7.0,Handle spillover from last shard
2.7.0,Should have updated to non-legacy metadata
2.7.0,Note that this resets the cache internally
2.7.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.7.0,"than process based pools, since process based pools need to pickle/serialize"
2.7.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.7.0,we're actually protected by the GIL.
2.7.0,mp.dummy aliases ThreadPool to Pool
2.7.0,(ytz): this skips everything except possibly the last shard
2.7.0,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.7.0,make a NumpyDataset under the hood
2.7.0,"raw_data = (X, y, w, ids)"
2.7.0,Protect against generator exhaustion
2.7.0,This ensures tasks are consistent for all datasets
2.7.0,determine the shard sizes of the datasets to merge
2.7.0,"otherwise the entire dataset is the ""shard size"""
2.7.0,we must reshard the dataset to have a uniform size
2.7.0,choose the smallest shard size
2.7.0,Get full dataset in memory
2.7.0,Shuffle in memory
2.7.0,Write shuffled shards out to disk
2.7.0,Shuffle the arrays corresponding to each row in metadata_df
2.7.0,Reset cache
2.7.0,See if we have a cached copy of this shard.
2.7.0,"We don't, so load it from disk."
2.7.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.7.0,Try to cache this shard for later use.  Since the normal usage pattern is
2.7.0,"a series of passes through the whole dataset, there's no point doing"
2.7.0,anything fancy.  It never makes sense to evict another shard from the
2.7.0,"cache to make room for this one, because we'll probably want that other"
2.7.0,shard again before the next time we want this one.  So just cache as many
2.7.0,as we can and then stop.
2.7.0,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.7.0,Handle edge case with empty indices
2.7.0,We use two loops here. The outer while loop walks over selection shards
2.7.0,(the chunks of the indices to select that should go into separate
2.7.0,"output shards), while the inner for loop walks over the shards in the"
2.7.0,source datasets to select out the shard indices from that  source shard
2.7.0,Find indices which rest in this shard
2.7.0,Need to offset indices to fit within shard_size
2.7.0,Handle empty case where no data from this shard needed
2.7.0,Handle the case of datasets with y/w missing
2.7.0,Break if all indices have been used up already
2.7.0,Note these will be in the sorted order
2.7.0,We need to recover the original ordering. We can do this by using
2.7.0,np.where to find the locatios of the original indices in the sorted
2.7.0,indices.
2.7.0,We know there's only one match for np.where since this is a
2.7.0,"permutation, so the [0][0] pulls out the exact match location."
2.7.0,If shape metadata is available use it to directly compute shape from
2.7.0,metadata
2.7.0,"In absense of shape metadata, fall back to loading data from disk to"
2.7.0,find shape.
2.7.0,Case n_samples should be 1
2.7.0,flake8: noqa
2.7.0,TODO(rbharath): Get rid of * import
2.7.0,Test merging of numpy datasets
2.7.0,Load MUV dataset
2.7.0,Do an approximate comparison since splits are sometimes slightly off from
2.7.0,the exact fraction.
2.7.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.7.0,reloading will cause the transform to be reapplied. This is undesirable in
2.7.0,almost all cases. Need to understand a method to fix this.
2.7.0,The shuffling should have switched up the ordering
2.7.0,But all the same entries should still be present
2.7.0,All the data should have same shape
2.7.0,The shuffling should have switched up the ordering
2.7.0,But all the same entries should still be present
2.7.0,All the data should have same shape
2.7.0,The ids should now store the performed permutation. Check that the
2.7.0,original dataset is recoverable.
2.7.0,The ids should now store the performed permutation. Check that the
2.7.0,original dataset is recoverable.
2.7.0,Generate data
2.7.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.7.0,around for testing resharding.
2.7.0,Set cache to 0 size to avoid cache hiding errors
2.7.0,Generate data
2.7.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.7.0,around for testing resharding.
2.7.0,Set cache to 0 size to avoid cache hiding errors
2.7.0,Featurize emols dataset
2.7.0,example.fasta contains 3 sequences each of length 58.
2.7.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.7.0,"There is one ""image channel""."
2.7.0,"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
2.7.0,TODO: test with full uniprot file once sharding support is added.
2.7.0,Generate dummy dataset
2.7.0,Generate dummy dataset
2.7.0,Generate dummy dataset
2.7.0,Set last n_samples/2 weights to 0
2.7.0,Check that no support elements are sample from zero-weight samples
2.7.0,Generate dummy dataset
2.7.0,Generate dummy dataset
2.7.0,Create support generator
2.7.0,Generate dummy dataset
2.7.0,Create support generator
2.7.0,Generate dummy dataset
2.7.0,Assert all support elements have been removed
2.7.0,Generate dummy dataset
2.7.0,Assert all remove elements have been removed
2.7.0,Generate dummy dataset
2.7.0,Assert all support elements have been removed
2.7.0,Generate dummy dataset
2.7.0,Assert all remove elements have been removed
2.7.0,Generate dummy dataset
2.7.0,Set last n_samples/2 weights to 0
2.7.0,Sample from first n_samples/2 elements for support
2.7.0,Should lie within first n_samples/2 samples only
2.7.0,Generate dummy dataset
2.7.0,Create support generator
2.7.0,Generate dummy dataset
2.7.0,This is necessary since from_numpy adds in shape information
2.7.0,This is necessary since from_numpy adds in shape information
2.7.0,This is necessary since from_numpy adds in shape information
2.7.0,Generate data
2.7.0,Generate data
2.7.0,Generate data
2.7.0,Should now have 10 shards
2.7.0,This is the shape of legacy_data
2.7.0,legacy_dataset is a dataset in the legacy format kept around for testing
2.7.0,purposes.
2.7.0,This is the shape of legacy_data_reshard
2.7.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.7.0,around for testing
2.7.0,Should now have 10 shards
2.7.0,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.7.0,Test constructor reload works for legacy format
2.7.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.7.0,around for testing resharding.
2.7.0,Reshard copy
2.7.0,Check metadata has been updated
2.7.0,First try using images for X.
2.7.0,Now try using images for y.
2.7.0,Transform it
2.7.0,Test on identity matrix
2.7.0,Generate random sparse features dataset
2.7.0,Test edge case with array of all zeros
2.7.0,Test cases where n_samples < 2*n_samples < batch_size
2.7.0,Test cases where n_samples < batch_size
2.7.0,Test case where n_samples == batch_size
2.7.0,Test case for object featurization.
2.7.0,Test case for more complicated object featurization
2.7.0,Test case with multidimensional data
2.7.0,Test cases where n_samples < 2*n_samples < batch_size
2.7.0,Test cases where n_samples < batch_size
2.7.0,Test case where n_samples == batch_size
2.7.0,Test case for object featurization.
2.7.0,Test case for more complicated object featurization
2.7.0,Test case with multidimensional data
2.7.0,Test first resharding worked
2.7.0,Test second resharding worked
2.7.0,approx 1/15! chance of equality
2.7.0,Generate data
2.7.0,Generate data
2.7.0,Transform it
2.7.0,special case to test
2.7.0,deterministic
2.7.0,non-deterministic
2.7.0,we don't know the order in which the shards are iterated in.
2.7.0,Check that we have all the data in
2.7.0,Test iterating in order.
2.7.0,Test iterating out of order.
2.7.0,Test iterating in batches.
2.7.0,Test iterating with multiple workers.
2.7.0,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.7.0,Try specifying particular columns.
2.7.0,Try specifying particular columns
2.7.0,Test id shrinkage
2.7.0,Test task shrinkage
2.7.0,Test max print size
2.7.0,Create image file
2.7.0,Create zip of image file
2.7.0,Create zip of multiple image files
2.7.0,"Create zip of multiple image files, multiple_types"
2.7.0,Create image directory
2.7.0,These are the known dimensions of face.png
2.7.0,These are the known dimensions of face.png
2.7.0,TODO(rbharath): Where are the color channels?
2.7.0,"Since the different files have different shapes, makes an object array"
2.7.0,Splits featurized samples into train/test
2.7.0,Splits featurized samples into train/test
2.7.0,Splits featurized samples into train/test
2.7.0,Splits featurized samples into train/test
2.7.0,Now perform move
2.7.0,Only for debug!
2.7.0,Make directories to store the raw and featurized datasets.
2.7.0,Load dataset
2.7.0,Featurize tox21 dataset
2.7.0,featurization
2.7.0,train/valid split.
2.7.0,singletask load
2.7.0,comparison
2.7.0,Only for debug!
2.7.0,Make directories to store the raw and featurized datasets.
2.7.0,Load dataset
2.7.0,Featurize tox21 dataset
2.7.0,For debugging purposes
2.7.0,multitask load
2.7.0,Do train/valid split.
2.7.0,singletask load
2.7.0,comparison
2.7.0,Default file contains 4 sequences each of length 192 (excluding the end of line character '\n').
2.7.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.7.0,"Expected shape is now (4, 192, 5)"
2.7.0,Get the labels/weights
2.7.0,Normalize shapes
2.7.0,Remove labels with zero weights
2.7.0,Note that we may have 0 elements of a given class since we remove those
2.7.0,labels with zero weight.
2.7.0,this works because y is 1D
2.7.0,This is the right ratio since int(N/num_c) * num_c \approx N
2.7.0,for all classes
2.7.0,Flattening is safe because of shape check above
2.7.0,Hack to allow for easy unpickling:
2.7.0,http://stefaanlippens.net/pickleproblem
2.7.0,Some transformation must happen
2.7.0,Add this case in to handle non-DiskDataset that should be written to disk
2.7.0,Note that transformers have to be undone in reversed order
2.7.0,Handle division by zero
2.7.0,Handle division by zero
2.7.0,Control for pathological case with no variance.
2.7.0,Handle case with 1 task correctly
2.7.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.7.0,Find the task dimension of z
2.7.0,Prevent broadcasting on wrong dimension
2.7.0,BalancingTransformer can only transform weights.
2.7.0,Compute weighting factors from dataset.
2.7.0,Handle 1-D case
2.7.0,Remove labels with zero weights
2.7.0,Note that we may have 0 elements of a given class since we remove those
2.7.0,labels with zero weight. This typically happens in multitask datasets
2.7.0,where some datapoints only have labels for some tasks.
2.7.0,this works because task_y is 1D
2.7.0,This is the right ratio since N_task/num_c * num_c = N_task
2.7.0,for all classes
2.7.0,Set to the class weight computed previously
2.7.0,Need this for transform_y
2.7.0,Handle 1D case
2.7.0,THis reshape is safe because of guard above.
2.7.0,map the indices to labels
2.7.0,generating batch of data by slicing similarity matrix
2.7.0,into 100*reference_dataset_length
2.7.0,concatenate batches of data together
2.7.0,highest similarity is 1: target is in the reference
2.7.0,use the following K points
2.7.0,"highest less than 1: target not in the reference, use top K points"
2.7.0,calculate matrix multiplicatin on slices
2.7.0,concatenate the slices together
2.7.0,list of calculation orders for DAGs
2.7.0,stemming from one specific atom in the molecule
2.7.0,starting from the adjacency list derived by graphconv featurizer
2.7.0,"number of atoms, also number of DAGs"
2.7.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.7.0,each step calculating graph features for one atom.
2.7.0,`max_atoms` is the maximum number of steps
2.7.0,each iteration generates the DAG starting from atom with index `count`
2.7.0,"list of lists, elements represent the calculation orders"
2.7.0,for atoms in the current graph
2.7.0,starting from the target atom with index `count`
2.7.0,flags of whether the atom is already included in the DAG
2.7.0,atom `count` is in the DAG
2.7.0,recording number of radial propagation steps
2.7.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.7.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.7.0,will be added in the second loop(radial=1).
2.7.0,atoms i-bond away will be added in i-th loop
2.7.0,"when molecules have separate parts, starting from one part,"
2.7.0,it is not possible to include all atoms.
2.7.0,this break quit the loop when going into such condition
2.7.0,reinitialize targets for next iteration
2.7.0,atoms connected to current_atom
2.7.0,generate the dependency map of current DAG
2.7.0,atoms connected to `current_atoms`(and not included in the DAG)
2.7.0,"are added, and will be the `current_atoms` for next iteration."
2.7.0,"DAG starts from the target atom, calculation should go in reverse"
2.7.0,`edge[1]` is the parent of `edge[0]`
2.7.0,"after this loop, `parents[i]` includes all parents of atom i"
2.7.0,manually adding the atom index into its parents list
2.7.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.7.0,atoms with less parents(farther from the target atom) come first.
2.7.0,"graph features of atoms without parents will be first calculated,"
2.7.0,then atoms with more parents can be calculated in order
2.7.0,based on previously calculated graph features.
2.7.0,target atom of this DAG will be calculated in the last step
2.7.0,padding with `max_atoms`
2.7.0,padding
2.7.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.7.0,which is a max_atoms * max_atoms numpy array after padding
2.7.0,class ANITransformer(Transformer):
2.7.0,"""""""Performs transform from 3D coordinates to ANI symmetry functions"
2.7.0,Note
2.7.0,----
2.7.0,This class requires TensorFlow to be installed.
2.7.0,""""""""
2.7.0,"def __init__(self,"
2.7.0,"max_atoms=23,"
2.7.0,"radial_cutoff=4.6,"
2.7.0,"angular_cutoff=3.1,"
2.7.0,"radial_length=32,"
2.7.0,"angular_length=8,"
2.7.0,"atom_cases=[1, 6, 7, 8, 16],"
2.7.0,"atomic_number_differentiated=True,"
2.7.0,coordinates_in_bohr=True):
2.7.0,""""""""
2.7.0,Only X can be transformed
2.7.0,""""""""
2.7.0,import tensorflow as tf
2.7.0,self.max_atoms = max_atoms
2.7.0,self.radial_cutoff = radial_cutoff
2.7.0,self.angular_cutoff = angular_cutoff
2.7.0,self.radial_length = radial_length
2.7.0,self.angular_length = angular_length
2.7.0,self.atom_cases = atom_cases
2.7.0,self.atomic_number_differentiated = atomic_number_differentiated
2.7.0,self.coordinates_in_bohr = coordinates_in_bohr
2.7.0,self.compute_graph = self.build()
2.7.0,self.sess = tf.Session(graph=self.compute_graph)
2.7.0,self.transform_batch_size = 32
2.7.0,"super(ANITransformer, self).__init__(transform_X=True)"
2.7.0,"def transform_array(self, X, y, w):"
2.7.0,if self.transform_X:
2.7.0,X_out = []
2.7.0,num_transformed = 0
2.7.0,start = 0
2.7.0,batch_size = self.transform_batch_size
2.7.0,while True:
2.7.0,"end = min((start + 1) * batch_size, X.shape[0])"
2.7.0,X_batch = X[(start * batch_size):end]
2.7.0,output = self.sess.run(
2.7.0,"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
2.7.0,X_out.append(output)
2.7.0,num_transformed = num_transformed + X_batch.shape[0]
2.7.0,logger.info('%i samples transformed' % num_transformed)
2.7.0,start += 1
2.7.0,if end >= len(X):
2.7.0,break
2.7.0,"X_new = np.concatenate(X_out, axis=0)"
2.7.0,assert X_new.shape[0] == X.shape[0]
2.7.0,"return (X_new, y, w)"
2.7.0,"def untransform(self, z):"
2.7.0,raise NotImplementedError(
2.7.0,"""Cannot untransform datasets with ANITransformer."")"
2.7.0,def build(self):
2.7.0,""""""" tensorflow computation graph for transform """""""
2.7.0,import tensorflow as tf
2.7.0,graph = tf.Graph()
2.7.0,with graph.as_default():
2.7.0,self.inputs = tf.keras.Input(
2.7.0,"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
2.7.0,"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
2.7.0,flags = tf.sign(atom_numbers)
2.7.0,flags = tf.cast(
2.7.0,"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
2.7.0,"coordinates = self.inputs[:, :, 1:]"
2.7.0,if self.coordinates_in_bohr:
2.7.0,coordinates = coordinates * 0.52917721092
2.7.0,"d = self.distance_matrix(coordinates, flags)"
2.7.0,"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
2.7.0,"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
2.7.0,"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
2.7.0,"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
2.7.0,coordinates)
2.7.0,self.outputs = tf.concat(
2.7.0,[
2.7.0,"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
2.7.0,angular_sym
2.7.0,"],"
2.7.0,axis=2)
2.7.0,return graph
2.7.0,"def distance_matrix(self, coordinates, flags):"
2.7.0,""""""" Generate distance matrix """""""
2.7.0,import tensorflow as tf
2.7.0,max_atoms = self.max_atoms
2.7.0,"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
2.7.0,"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
2.7.0,# Calculate pairwise distance
2.7.0,"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
2.7.0,# Masking for valid atom index
2.7.0,d = d * flags
2.7.0,return d
2.7.0,"def distance_cutoff(self, d, cutoff, flags):"
2.7.0,""""""" Generate distance matrix with trainable cutoff """""""
2.7.0,import tensorflow as tf
2.7.0,# Cutoff with threshold Rc
2.7.0,d_flag = flags * tf.sign(cutoff - d)
2.7.0,d_flag = tf.nn.relu(d_flag)
2.7.0,d_flag = d_flag * tf.expand_dims(
2.7.0,"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
2.7.0,d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
2.7.0,return d * d_flag
2.7.0,"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
2.7.0,""""""" Radial Symmetry Function """""""
2.7.0,import tensorflow as tf
2.7.0,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.7.0,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.7.0,"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
2.7.0,ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
2.7.0,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
2.7.0,"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
2.7.0,length = ita.get_shape().as_list()[-1]
2.7.0,"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
2.7.0,"d = tf.stack([d] * length, axis=3)"
2.7.0,out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
2.7.0,if self.atomic_number_differentiated:
2.7.0,out_tensors = []
2.7.0,for atom_type in self.atom_cases:
2.7.0,selected_atoms = tf.expand_dims(
2.7.0,"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
2.7.0,axis=3)
2.7.0,"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
2.7.0,"return tf.concat(out_tensors, axis=2)"
2.7.0,else:
2.7.0,"return tf.reduce_sum(out, axis=2)"
2.7.0,"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
2.7.0,""""""" Angular Symmetry Function """""""
2.7.0,import tensorflow as tf
2.7.0,max_atoms = self.max_atoms
2.7.0,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.7.0,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.7.0,"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
2.7.0,ita = 3 / (Rs[1] - Rs[0])**2
2.7.0,"thetas = np.linspace(0., np.pi, self.angular_length)"
2.7.0,zeta = float(self.angular_length**2)
2.7.0,"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
2.7.0,"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
2.7.0,"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
2.7.0,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
2.7.0,"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
2.7.0,length = zeta.get_shape().as_list()[-1]
2.7.0,"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
2.7.0,"[coordinates] * max_atoms, 2)"
2.7.0,"R_ij = tf.stack([d] * max_atoms, axis=3)"
2.7.0,"R_ik = tf.stack([d] * max_atoms, axis=2)"
2.7.0,"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
2.7.0,"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
2.7.0,# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.7.0,"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
2.7.0,"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
2.7.0,vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
2.7.0,"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
2.7.0,"R_ij = tf.stack([R_ij] * length, axis=4)"
2.7.0,"R_ik = tf.stack([R_ik] * length, axis=4)"
2.7.0,"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
2.7.0,"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
2.7.0,"theta = tf.stack([theta] * length, axis=4)"
2.7.0,"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
2.7.0,tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
2.7.0,if self.atomic_number_differentiated:
2.7.0,out_tensors = []
2.7.0,"for id_j, atom_type_j in enumerate(self.atom_cases):"
2.7.0,for atom_type_k in self.atom_cases[id_j:]:
2.7.0,"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
2.7.0,"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
2.7.0,selected_atoms = tf.expand_dims(
2.7.0,"tf.expand_dims(selected_atoms, axis=1), axis=4)"
2.7.0,out_tensors.append(
2.7.0,"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
2.7.0,"return tf.concat(out_tensors, axis=2)"
2.7.0,else:
2.7.0,"return tf.reduce_sum(out_tensor, axis=(2, 3))"
2.7.0,def get_num_feats(self):
2.7.0,n_feat = self.outputs.get_shape().as_list()[-1]
2.7.0,return n_feat
2.7.0,flake8: noqa
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a y transformer
2.7.0,Check w is unchanged since this is a y transformer
2.7.0,Check y is now a logarithmic version of itself
2.7.0,Check that untransform does the right thing.
2.7.0,Check ids are unchanged.
2.7.0,Check y is unchanged since this is a X transformer
2.7.0,Check w is unchanged since this is a y transformer
2.7.0,Check y is now a logarithmic version of itself
2.7.0,Check that untransform does the right thing.
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a y transformer
2.7.0,Check w is unchanged since this is a y transformer
2.7.0,Check y is now a logarithmic version of itself
2.7.0,Check that untransform does the right thing.
2.7.0,Tests logarithmic data transformer with selection.
2.7.0,Check ids are unchanged.
2.7.0,Check y is unchanged since this is a X transformer
2.7.0,Check w is unchanged since this is a y transformer
2.7.0,Check y is now a logarithmic version of itself
2.7.0,Check that untransform does the right thing.
2.7.0,Check ids are unchanged.
2.7.0,Check y is unchanged since this is an X transformer
2.7.0,Check w is unchanged since this is an X transformer
2.7.0,Check X is now holding the proper values when sorted.
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is an y transformer
2.7.0,Check w is unchanged since this is an y transformer
2.7.0,Check y is now holding the proper values when sorted.
2.7.0,Check that untransform does the right thing.
2.7.0,Check ids are unchanged.
2.7.0,Check y is unchanged since this is an X transformer
2.7.0,Check w is unchanged since this is an X transformer
2.7.0,Check X is now holding the proper values when sorted.
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a y transformer
2.7.0,Check w is unchanged since this is a y transformer
2.7.0,Check y is now holding the proper values when sorted.
2.7.0,Check ids are unchanged before and after transformation
2.7.0,Check X is unchanged since transform_y is true
2.7.0,Check w is unchanged since transform_y is true
2.7.0,Check minimum and maximum values of transformed y are 0 and 1
2.7.0,Check untransform works correctly
2.7.0,Check ids are unchanged before and after transformation
2.7.0,Check X is unchanged since transform_y is true
2.7.0,Check w is unchanged since transform_y is true
2.7.0,Check minimum and maximum values of transformed y are 0 and 1
2.7.0,Test if dimensionality expansion is handled correctly by untransform
2.7.0,Check ids are unchanged before and after transformation
2.7.0,Check X is unchanged since transform_y is true
2.7.0,Check w is unchanged since transform_y is true
2.7.0,Check minimum and maximum values of transformed y are 0 and 1
2.7.0,Check untransform works correctly
2.7.0,Load mini log-solubility dataset.
2.7.0,The transformer generates n DAGs for a molecule with n
2.7.0,"atoms. These are denoted the ""parents"""
2.7.0,extract only the images (no need of the labels)
2.7.0,reshaping the vector to image
2.7.0,Check Blurring
2.7.0,Check center crop
2.7.0,Check crop
2.7.0,Check convert2gray
2.7.0,Check rotation
2.7.0,Some more test cases for flip
2.7.0,Check flip
2.7.0,Check Scales
2.7.0,Check shift
2.7.0,check gaussian noise
2.7.0,check salt and pepper noise
2.7.0,Check median filter
2.7.0,transforming y should raise an exception
2.7.0,transforming w should raise an exception
2.7.0,transforming X should be okay
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a y transformer
2.7.0,Check w is unchanged since this is a y transformer
2.7.0,"Check that y_t has zero mean, unit std."
2.7.0,Check that untransform does the right thing.
2.7.0,Check ids are unchanged.
2.7.0,Check y is unchanged since this is a X transformer
2.7.0,Check w is unchanged since this is a y transformer
2.7.0,"Check that X_t has zero mean, unit std."
2.7.0,np.set_printoptions(threshold='nan')
2.7.0,Entries with zero std are not normalized
2.7.0,Check that untransform does the right thing.
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a w transformer
2.7.0,Check y is unchanged since this is a w transformer
2.7.0,Assert that entries with zero weight retain zero weight
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a w transformer
2.7.0,Check y is unchanged since this is a w transformer
2.7.0,Assert that entries with zero weight retain zero weight
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a w transformer
2.7.0,Check y is unchanged since this is a w transformer
2.7.0,Assert that entries with zero weight retain zero weight
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a w transformer
2.7.0,Check y is unchanged since this is a w transformer
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is a w transformer
2.7.0,Check y is unchanged since this is a w transformer
2.7.0,Assert that entries with zero weight retain zero weight
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,Check ids are unchanged.
2.7.0,Check y is unchanged since this is an X transformer
2.7.0,Check w is unchanged since this is an X transformer
2.7.0,Check X is now holding the proper values in each column.
2.7.0,Check ids are unchanged.
2.7.0,Check X is unchanged since this is an X transformer
2.7.0,Check w is unchanged since this is an X transformer
2.7.0,Check y is now holding the proper values in each column.
2.7.0,Check that untransform does the right thing.
2.7.0,Check that we have length 8 now with duplication
2.7.0,Check shapes
2.7.0,Check that we have 4 positives and 4 negatives
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,Note that nothing should change in this dataset since weights balance!
2.7.0,Check that still we have length 6
2.7.0,Check shapes
2.7.0,Check that we have 2 positives and 4 negatives
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,Check that we have length 8 now with duplication
2.7.0,Check shapes
2.7.0,Check that we have 4 positives and 4 negatives
2.7.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.7.0,6-1 imbalance in favor of class 0
2.7.0,Check that we have length 30 now with duplication
2.7.0,Check shapes
2.7.0,Check that we have 6 of each class
2.7.0,Check that sum of all class weights is equal by comparing to 0 weight
2.7.0,Note class imbalance. This will round to 2x duplication for 1
2.7.0,Check that we have length 13 now with duplication
2.7.0,Check shapes
2.7.0,Check that we have 6 positives and 7 negatives
2.7.0,################################################################
2.7.0,save.py is out of date. You should not import any functions from here.
2.7.0,################################################################
2.7.0,flake8: noqa
2.7.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.7.0,Walk through the original file and extract ATOM/HETATM lines and
2.7.0,add PDBQT charge annotations.
2.7.0,Remove rotatable bonds from this molecule
2.7.0,Get the connected components now that the rotatable bonds have
2.7.0,been removed.
2.7.0,The root is the largest connected component.
2.7.0,Write the root component
2.7.0,"We've looked at the root, so take note of that"
2.7.0,Compute partial charges on molecule if RDKit Mol
2.7.0,indices to atoms to keep
2.7.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.7.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.7.0,nonzero contact.
2.7.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.7.0,TODO: This is duplicated! Clean up
2.7.0,Updates charges in place
2.7.0,initial embedding
2.7.0,minimization and pruning
2.7.0,always keep lowest-energy conformer
2.7.0,discard conformers after max_conformers is reached
2.7.0,get RMSD to selected conformers
2.7.0,discard conformers within the RMSD threshold
2.7.0,create a new molecule to hold the chosen conformers
2.7.0,this ensures proper conformer IDs and energy-based ordering
2.7.0,False here specifies that water is to be removed
2.7.0,Updates charges in place
2.7.0,TODO: This is wrong. Should return all molecules
2.7.0,TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.7.0,This updates in place
2.7.0,indices of atoms to keep
2.7.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.7.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.7.0,nonzero contact.
2.7.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.7.0,####################################################
2.7.0,Compute partial charges on molecule if rdkit
2.7.0,####################################################
2.7.0,Number of voxels per one edge of box to voxelize.
2.7.0,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.7.0,If interval1 < interval2 entirely
2.7.0,If interval2 < interval1 entirely
2.7.0,Each triangle in the simplices is a set of 3 atoms from
2.7.0,coordinates which forms the vertices of an exterior triangle on
2.7.0,the convex hull of the macromolecule.
2.7.0,Points is the set of atom coordinates that make up this
2.7.0,triangular face on the convex hull
2.7.0,Let's extract x/y/z coords for this face
2.7.0,Let's compute min/max points
2.7.0,"Nitrogen has atomic number 7, and oxygen 8."
2.7.0,If atom is a hydrogen
2.7.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.7.0,If atom is a hydrogen
2.7.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.7.0,if ring from mol1 is aromatic
2.7.0,...and atom from mol2 is a cation
2.7.0,if angle and distance are correct
2.7.0,count atoms forming a contact
2.7.0,if ring is aromatic
2.7.0,"save its indices, center, and normal"
2.7.0,remember mol1-mol2 pairs we already counted
2.7.0,"if this pair is new, count atoms forming a contact"
2.7.0,"if this pair is new, count atoms forming a contact"
2.7.0,find interacting rings from mol1 and cations from mol2
2.7.0,find interacting cations from mol1 and rings from mol2
2.7.0,merge counters
2.7.0,the line has format
2.7.0,REMARK VINA RESULT: score ...
2.7.0,There is only 1 such line per model so we can append it
2.7.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.7.0,Apply common fixes to PDB files
2.7.0,Optimize ligand
2.7.0,"For a node-prediction task, label is not added to edge features and other global features"
2.7.0,because label here is a node-level attribute and not a graph-level attribute
2.7.0,"In this case, the 'y' attribute of GraphData will contain the"
2.7.0,node-level labels.
2.7.0,not a self-loop
2.7.0,Make sure input is a list
2.7.0,FIXME: Incompatible types in assignment
2.7.0,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.7.0,Ensure that metric is wrapped in a list.
2.7.0,This case checks if input is a function then wraps a
2.7.0,dc.metrics.Metric object around it
2.7.0,Process input metrics
2.7.0,Compute multitask metrics
2.7.0,We use y/w to aggregate labels/weights across generator.
2.7.0,This is a KerasModel.
2.7.0,Some datasets have weights
2.7.0,Process predictions and populate y/w lists
2.7.0,Combine labels/weights
2.7.0,Undo data transformations.
2.7.0,Compute multitask metrics
2.7.0,These functions have moved to deepchem.utils_docking_utils
2.7.0,flake8: noqa
2.7.0,The number of elements to print for dataset ids/tasks
2.7.0,"If a dataset contains more than this number of elements, it won't"
2.7.0,print any dataset ids
2.7.0,An activation function for a layer: either a function or the name of a standard activation
2.7.0,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.7.0,"A single value of some type, or multiple values of that type"
2.7.0,The shape of a NumPy array
2.7.0,"A NumPy array, or an object that can be converted to one.  Once we move to"
2.7.0,"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
2.7.0,type of RDKit object
2.7.0,type of Pymatgen object
2.7.0,Generate a random temporary file name
2.7.0,Ensure the file is created
2.7.0,Open the file in the given mode
2.7.0,Tasks are either in .sdf.csv file or in the .sdf file itself for QM9 dataset
2.7.0,Structures are stored in .sdf file
2.7.0,"Note: Here, the order of columns is based on the order in which the values"
2.7.0,"are appended to `df_row`. Since pos_x, pos_y, pos_z are appended after appending"
2.7.0,"tasks above, they occur after `tasks` here."
2.7.0,"FIXME Ideally, we should use something like a dictionary here to keep it independent"
2.7.0,of column ordering.
2.7.0,Reset aggregator
2.7.0,Handle final leftovers for this file
2.7.0,First line of user-specified CSV *must* be header.
2.7.0,"If gzipped, need to compute extension again"
2.7.0,First line of user-specified CSV *must* be header.
2.7.0,The label encoder is given characters for ACGTN
2.7.0,Peak at the first sequence to get the length of the sequence.
2.7.0,init an one-hot vector
2.7.0,"If include_unknown_set is True, set the last index is 1."
2.7.0,################################################################
2.7.0,atom (node) featurization
2.7.0,################################################################
2.7.0,################################################################
2.7.0,bond (edge) featurization
2.7.0,################################################################
2.7.0,One sequence has length longer than others. This should throw a
2.7.0,ValueError.
2.7.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.7.0,Loosening atol to see if tests stop failing sporadically
2.7.0,string set
2.7.0,integer set
2.7.0,include_unknown_set is False
2.7.0,include_unknown_set is True
2.7.0,check unknown atoms
2.7.0,check original set
2.7.0,"Generally, =O behaves as an electron acceptor"
2.7.0,we must compute partial charges before using `get_atom_partial_charge`
2.7.0,The C-N bond is a single bond
2.7.0,graph-level labels
2.7.0,node-level labels
2.7.0,graph.y contains node-labels and graph.node_features.shape[0]
2.7.0,holds number of nodes in that graph
2.7.0,TODO test more formats for ligand
2.7.0,TODO test more formats for ligand
2.7.0,adding hydrogens and charges is tested in dc.utils
2.7.0,self.ligand_file is for 3ws9_ligand.sdf
2.7.0,dummy function which can be passed as the parameter f to simultaneous_move and single_move
2.7.0,test for gauss_initialize_position
2.7.0,testing symmetric simultaneous_move
2.7.0,testing asymmetric simultaneous_move
2.7.0,testing symmetric single_move
2.7.0,testing asymmetric single_move
2.7.0,simple flat ring
2.7.0,self.cycle4.Compute2DCoords()
2.7.0,load and sanitize two real molecules
2.7.0,parallel normals
2.7.0,perpendicular normals
2.7.0,too far away
2.7.0,perpendicular normals
2.7.0,parallel normals
2.7.0,too far away
2.7.0,order of the molecules shouldn't matter
2.7.0,with this criteria we should find both types of stacking
2.7.0,parallel normals
2.7.0,perpendicular normals
2.7.0,too far away
2.7.0,def test_compute_cation_pi(self):
2.7.0,"# TODO(rbharath): find better example, currently dicts are empty"
2.7.0,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.7.0,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.7.0,"TODO find better example, currently dicts are empty"
2.7.0,TODO test more formats for ligand
2.7.0,Test on RDKit
2.7.0,3D vector with unit length
2.7.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.7.0,"random coords between 0 and 1, so the max possible distance in sqrt(3)"
2.7.0,check if correct distance metric was used
2.7.0,Construct a random class probability matrix
2.7.0,Construct a random class probability matrix
2.7.0,"Note that since no name as provided, metrics are index by order"
2.7.0,given.
2.7.0,"Note that since no name as provided, metrics are index by order"
2.7.0,given.
2.7.0,"Note that since no name as provided, metrics are index by order"
2.7.0,given.
2.7.0,"Note that since no name as provided, metrics are index by order"
2.7.0,given.
2.7.0,TODO: Fix this case with correct thresholding
2.7.0,TODO: Fix this case with correct thresholding
2.7.0,There are 4 faces to the shape created by coords
2.7.0,flake8: noqa
2.7.0,Get the degree id list (which corrects for min_deg)
2.7.0,Get the size of each degree block
2.7.0,Get the the start indices for items in each block
2.7.0,Get the node indices when they are reset when the degree changes
2.7.0,Convert to numpy array
2.7.0,Reorder old atom_features
2.7.0,Reorder old deg lists
2.7.0,Sort membership
2.7.0,Create old to new dictionary. not exactly intuitive
2.7.0,Reorder adjacency lists
2.7.0,Get numpy version of degree list for indexing
2.7.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.7.0,Parse as deg separated
2.7.0,Get indices corresponding to the current degree
2.7.0,Extract and save adjacency list for the current degree
2.7.0,Construct the slice information
2.7.0,Get the cumulative indices after the first index
2.7.0,Set indices with zero sized slices to zero to avoid indexing errors
2.7.0,TODO(rbharath): Can this be removed?
2.7.0,Use random insted of zeros to prevent weird issues with summing to zero
2.7.0,"Combine the features, then sort them by (atom_degree, mol_index)"
2.7.0,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.7.0,Create a map from the original atom indices within each molecule to the
2.7.0,indices in the combined object.
2.7.0,Sort all atoms by degree.
2.7.0,"Get the size of each atom list separated by molecule id, then by degree"
2.7.0,Get the final size of each degree block
2.7.0,"Get the index at which each degree starts, not resetting after each degree"
2.7.0,And not stopping at any specific molecule
2.7.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.7.0,first column telling the start indices of each degree block and the
2.7.0,second colum telling the size of each degree block
2.7.0,Determine the membership (atom i belongs to molecule membership[i])
2.7.0,Initialize the new degree separated adjacency lists
2.7.0,Update the old adjacency lists with the new atom indices and then combine
2.7.0,all together
2.7.0,Iterate through all the molecules
2.7.0,Get the adjacency lists for this molecule and current degree id
2.7.0,"Correct all atom indices to the final indices, and then save the"
2.7.0,results into the new adjacency lists
2.7.0,Increment once row is done
2.7.0,Get the final aggregated molecule
2.7.0,"Requriments - transformers, tokenizers"
2.7.0,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.7.0,The vocab may be expanded in the near future
2.7.0,add vocab_file dict
2.7.0,"unk_token=""[UNK]"","
2.7.0,"sep_token=""[SEP]"","
2.7.0,"pad_token=""[PAD]"","
2.7.0,"cls_token=""[CLS]"","
2.7.0,"mask_token=""[MASK]"","
2.7.0,flake8: noqa
2.7.0,Initalize with 1
2.7.0,Replace the hybridization
2.7.0,global possible_hybridization_list
2.7.0,Allow 0 index to correspond to null molecule 1
2.7.0,Correct for null
2.7.0,"print(6-k-1, id)"
2.7.0,Correct for last one
2.7.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.7.0,"Handle edge case of self-pairs (i, i)"
2.7.0,Increment by 1 since we don't want 0-indexing
2.7.0,"This creates a matrix of shape (2, num_pairs)"
2.7.0,Get mapping
2.7.0,first `bt_len` features are bond features(if applicable)
2.7.0,For ring pairs outside max pairs distance continue
2.7.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.7.0,graph distance between two atoms
2.7.0,distance is a matrix of 1-hot encoded distances for all atoms
2.7.0,For ring pairs outside max pairs distance continue
2.7.0,Euclidean distance between atoms
2.7.0,atoms `radial` bonds away from `a1`
2.7.0,atoms less than `radial` bonds away
2.7.0,find atoms `radial`+1 bonds away
2.7.0,create temporary valid ids serving to filter out failed featurizations from every sublist
2.7.0,"of features (i.e. every molecules' frags list), and also totally failed sublists."
2.7.0,This makes output digestable by Loaders
2.7.0,Get the node features
2.7.0,Stack nodes into an array
2.7.0,Get bond lists with reverse edges included
2.7.0,Get canonical adjacency list
2.7.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.7.0,only support datasets providing Cartesian coordinates)
2.7.0,Set dtype
2.7.0,If includes explicit hydrogens
2.7.0,If uses use_chirality
2.7.0,Atom features
2.7.0,Stack nodes into an array
2.7.0,Get bond lists
2.7.0,Get canonical adjacency list
2.7.0,Calculate pair features
2.7.0,the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
2.7.0,"SMILES is unique, so set a canonical order of atoms"
2.7.0,Add hydrogens and generate a conformation.
2.7.0,Record properties of the molecules.
2.7.0,Create the output object.
2.7.0,"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
2.7.0,flake8: noqa
2.7.0,base classes for featurizers
2.7.0,molecule featurizers
2.7.0,complex featurizers
2.7.0,material featurizers
2.7.0,tokenizers
2.7.0,support classes
2.7.0,for str
2.7.0,for list
2.7.0,validation
2.7.0,skip list
2.7.0,skip path string
2.7.0,main logic
2.7.0,Find a successful featurization
2.7.0,Replace failed featurizations with appropriate array
2.7.0,Special case handling of single molecule
2.7.0,Convert iterables to list
2.7.0,condition if the original atom order is required
2.7.0,"mol must be a RDKit Mol object, so parse a SMILES"
2.7.0,"mol must be a RDKit Mol object, so parse a SMILES"
2.7.0,"SMILES is unique, so set a canonical order of atoms"
2.7.0,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.7.0,atom_name is of format RESX-ATOMTYPE
2.7.0,where X is a 1 to 4 digit number
2.7.0,validate params
2.7.0,"np.max() method works only for a non-empty array, so size of the array should be non-zero"
2.7.0,Adding shapes of kwargs
2.7.0,This assumes that the edge features for self loops are full-zero tensors
2.7.0,In the future we may want to support featurization for self loops
2.7.0,stack features
2.7.0,"before stacking edge_features or node_pos_features,"
2.7.0,we should check whether these are None or not
2.7.0,create new edge index
2.7.0,graph_index indicates which nodes belong to which graph
2.7.0,Setup image
2.7.0,Compute bond properties
2.7.0,Compute atom properties
2.7.0,Setup image
2.7.0,Compute bond properties
2.7.0,Compute atom properties
2.7.0,Reshape done for proper broadcast
2.7.0,"Reshapes, and axes manipulations to facilitate vector processing."
2.7.0,Draw a line between the two atoms.
2.7.0,"The coordinates of this line, are indicated in line_coords"
2.7.0,Turn the line coordinates into image positions
2.7.0,Turn atomic coordinates into image positions
2.7.0,Set the bond line coordinates to the bond property used.
2.7.0,Set the atom positions in image to different atomic properties in channels
2.7.0,With fixed res and img_size some molecules (e.g. long chains) may not fit.
2.7.0,Check whether num_confs >=1 or not
2.7.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.7.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.7.0,consistent with most QM software packages.
2.7.0,Dimension of atom feature vector
2.7.0,len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION) +1 to include room for unknown set
2.7.0,+ 2 at end for is_in_aromatic and mass
2.7.0,dictionary of available feature generators
2.7.0,number of atoms
2.7.0,number of bonds
2.7.0,"mapping from bond index to concat(in_atom, bond) features | initial input is a zero padding"
2.7.0,mapping from atom index to list of indices of incoming bonds
2.7.0,mapping from bond index to the index of the atom the bond is coming from
2.7.0,mapping from bond index to the index of the reverse bond
2.7.0,get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
2.7.0,get bond features
2.7.0,for H2
2.7.0,"not all features are equally long, so used methane as dummy molecule to determine length"
2.7.0,Fix nans in features
2.7.0,add edge list considering a directed graph
2.7.0,get atom features
2.7.0,get edge(bond) features
2.7.0,get edge index
2.7.0,get global features
2.7.0,generate SMILES for fragments
2.7.0,Featurize data using featurize() in parent class
2.7.0,Featurize str data
2.7.0,Extend shorter strings with padding
2.7.0,Padding before and after
2.7.0,Featurize data using featurize() in parent class
2.7.0,Featurize str data
2.7.0,Featurize mol data
2.7.0,Copied from https://github.com/samoturk/mol2vec/blob/850d944d5f48a58e26ed0264332b5741f72555aa/mol2vec/features.py#L129-L168
2.7.0,"merge identifiers alternating radius to sentence: atom 0 radius0, atom 0 radius 1, etc."
2.7.0,load pretrained models
2.7.0,convert errors to zero
2.7.0,flake8: noqa
2.7.0,If partial charges were not computed
2.7.0,construct atom (node) feature
2.7.0,construct edge (bond) index
2.7.0,add edge list considering a directed graph
2.7.0,construct edge (bond) feature
2.7.0,load_sdf_files returns pos as strings but user can also specify
2.7.0,numpy arrays for atom coordinates
2.7.0,The 1.0 float value represents True Boolean
2.7.0,This will return a boolean vector with all entries False
2.7.0,To get the shortest paths between two nodes.
2.7.0,To get info if two nodes belong to the same ring.
2.7.0,Featurizer
2.7.0,initialize
2.7.0,check initialization
2.7.0,creates normalized functions dictionary if normalized features are required
2.7.0,get sequence of descriptor names and normalization parameters from DescriptorsNormalizationParameters class
2.7.0,get required distribution_ from `scipy.stats` module.
2.7.0,cdf => cumulative density functions
2.7.0,make the cdf with the parameters.
2.7.0,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.7.0,Check whether num_confs >=1 or not
2.7.0,Convert AtomPositions from Angstrom to bohr (atomic units)
2.7.0,"`(1, max_atoms)` -> `(max_atoms,)`"
2.7.0,bond labels
2.7.0,atom labels
2.7.0,create bond encoders and decoders
2.7.0,create atom encoders and decoders
2.7.0,Special case handling of single molecule
2.7.0,Convert iterables to list
2.7.0,Set up site environment matcher
2.7.0,Graphical option
2.7.0,tolerance for grouping nodes
2.7.0,determine minimum distance between sitetypes.
2.7.0,This is used to determine the existence of an edge
2.7.0,Sort by bond
2.7.0,You want to maximize this in order to make sure every node gets an edge
2.7.0,construct graph
2.7.0,matcher options
2.7.0,construct graph
2.7.0,Add nodes
2.7.0,Add edge. distance is edge attribute
2.7.0,construct graph
2.7.0,Gets the isomorphic mapping. Also the most time consuming part of the code
2.7.0,reconstruct graph after alinging point order
2.7.0,RMSD
2.7.0,Construct one hot encoding
2.7.0,get mapping between all site index to active site index
2.7.0,Get Neighbors
2.7.0,Read Data
2.7.0,get map between two environment
2.7.0,align input to the primitive cell (reference)
2.7.0,apply permutations
2.7.0,remove spectators
2.7.0,map it to active sites
2.7.0,Extract the right number of sites by distance
2.7.0,if PBC condition is fulfilled..
2.7.0,Get full N x N SCM
2.7.0,flake8: noqa
2.7.0,load atom_init.json
2.7.0,check whether the atom feature exists or not
2.7.0,construct bi-directed graph
2.7.0,Increase dimension of distance tensor and apply filter
2.7.0,We compute pairwise contact fingerprints
2.7.0,We compute pairwise contact fingerprints
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.0,We compute pairwise contact fingerprints
2.7.0,"rdks = [frag1[1], frag2[1]]"
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.0,We compute pairwise contact fingerprints
2.7.0,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.7.0,"rdks = [frag1[1], frag2[1]]"
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.0,We compute pairwise contact fingerprints
2.7.0,"rdks = [frag1[1], frag2[1]]"
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.7.0,We compute pairwise contact fingerprints
2.7.0,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.7.0,We compute pairwise contact fingerprints
2.7.0,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.7.0,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.7.0,"xyzs = [frag1_xyz, frag2_xyz]"
2.7.0,"rdks = [frag1[1], frag2[1]]"
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.0,We compute pairwise contact fingerprints
2.7.0,"rdks = [frag1[1], frag2[1]]"
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.0,check if user tries to set removed arguments
2.7.0,list of features that require sanitized molecules
2.7.0,not implemented featurization types
2.7.0,default values
2.7.0,update with cutoffs specified by the user
2.7.0,"each entry is a tuple (is_flat, feature_name)"
2.7.0,list of features that cannot be calculated with specified parameters
2.7.0,this list is used to define <flat/voxel/all>_combined subset
2.7.0,parse provided feature types
2.7.0,flake8: noqa
2.7.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.7.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.7.0,nonzero contact.
2.7.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.7.0,We compute pairwise contact fingerprints
2.7.0,Get coordinates
2.7.0,We compute pairwise contact fingerprints
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.7.0,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.7.0,axis.
2.7.0,Type of data created by this featurizer
2.7.0,TODO(rbharath): Should this return a list?
2.7.0,Type of data created by this featurizer
2.7.0,Currently handles loading failures by returning None
2.7.0,TODO: Is there a better handling procedure?
2.7.0,pad outputs
2.7.0,Deprecation warnings for old atomic conv featurizer name #
2.7.0,We compute pairwise contact fingerprints
2.7.0,Get coordinates
2.7.0,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.7.0,We compute pairwise contact fingerprints
2.7.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.7.0,decode the source in the mixed and separated cases
2.7.0,number of indices where feature count is more than 1
2.7.0,no normalized feature value should be greater than 1.0
2.7.0,get atom features
2.7.0,mapping from atom index to atom features | initial input is a zero padding
2.7.0,TODO test more formats for ligand
2.7.0,TODO test more formats for ligand
2.7.0,with one conformer
2.7.0,with multiple conformers
2.7.0,include explicit hydrogens
2.7.0,with one conformer
2.7.0,with multiple conformers
2.7.0,include explicit hydrogens
2.7.0,construct edge (bond) index
2.7.0,add edge list considering a directed graph
2.7.0,test for 'MolGraphConvFeaturizer' class
2.7.0,"for ""C1=CC=CN=C1"" original bond index is not equal to canonical bond index"
2.7.0,"Requirements - transformers, tokenizers"
2.7.0,no normalized feature value should be greater than 1.0
2.7.0,"assert ""C1=CC=CN=C1"""
2.7.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.0,"assert ""C1=CC=CN=C1"""
2.7.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.0,"assert ""C1=CC=CN=C1"""
2.7.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.0,"assert ""C1=CC=CN=C1"""
2.7.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.0,Test featurizer with atom 3-D coordinates as kwargs
2.7.0,"assert ""C1=CC=CN=C1"""
2.7.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.7.0,"number of indices, where feature count is more than 1, should be 0"
2.7.0,number of indices where feature count is more than 1
2.7.0,check for separate count and SMILES entries for each fragment
2.7.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.7.0,max num atoms instead of exact.
2.7.0,Cutoff in angstroms
2.7.0,"Coords are padded, neighbor list and Z are not"
2.7.0,both reactant and product are null
2.7.0,reactant is null
2.7.0,product is null
2.7.0,valid reaction: [CH2:1]=[CH:2][CH:3]=[CH:4][CH2:5][H:6]>> [H:6][CH2:1][CH:2]=[CH:3][CH:4]=[CH2:5]
2.7.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.7.0,def test_hydrogen_bond_counter():
2.7.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.7.0,"protein_file = os.path.join(current_dir, 'data',"
2.7.0,'3ws9_protein_fixer_rdkit.pdb')
2.7.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.7.0,
2.7.0,cutoff = 4.5
2.7.0,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.7.0,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.7.0,# TODO: Add shape test
2.7.0,
2.7.0,
2.7.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.7.0,def test_hydrogen_bond_voxelizer():
2.7.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.7.0,"protein_file = os.path.join(current_dir, 'data',"
2.7.0,'3ws9_protein_fixer_rdkit.pdb')
2.7.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.7.0,
2.7.0,cutoff = 4.5
2.7.0,box_width = 16
2.7.0,voxel_width = 1.0
2.7.0,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.7.0,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.7.0,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.7.0,# TODO: Add shape test
2.7.0,@pytest.mark.linux_only
2.7.0,test if default parameters work
2.7.0,check if use-case from examples works
2.7.0,test if input is flattened when flat features are used
2.7.0,test voxel features
2.7.0,test flat features
2.7.0,check if aromatic features are ignored if sanitize=False
2.7.0,test flattened voxel features
2.7.0,test voxel features
2.7.0,test flat features
2.7.0,test rotations
2.7.0,not support array style inputs
2.7.0,z is kwargs
2.7.0,check convert function
2.7.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.7.0,of degree 1 (connected only to central nitrogen).
2.7.0,5 atoms in compound
2.7.0,Get the adjacency lists grouped by degree
2.7.0,The 4 outer atoms connected to central nitrogen
2.7.0,Central nitrogen connected to everything else.
2.7.0,Only one carbon
2.7.0,"No bonds, so degree adjacency lists are empty"
2.7.0,3 carbonds in alkane
2.7.0,Outer two carbonds are connected to central carbon
2.7.0,Central carbon connected to outer two
2.7.0,test featurization
2.7.0,test defeaturization
2.7.0,sanity check; see if something weird does not happen with rdkit
2.7.0,check if original smiles match defeaturized smiles
2.7.0,sanity check; see if something weird does not happen with rdkit
2.7.0,test featurization
2.7.0,test defeaturization
2.7.0,check if original smiles match defeaturized smiles
2.7.0,untransform
2.7.0,untranform
2.7.0,untranform
2.7.0,untranform
2.7.0,untranform
2.7.0,Check the SDF file.
2.7.0,Check the PDB file.
2.7.0,Check the SMILES string.
2.7.0,Set up tests.
2.7.0,Set up testing parameters.
2.7.0,the atom order for 'C' is same in case of canonical and original ordering
2.7.0,Do a manual distance computation and make
2.7.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.7.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.7.0,Do a manual distance computation and ensure that selected neighbor is
2.7.0,closest since we set max_num_neighbors = 1
2.7.0,Carbon
2.7.0,Test distance 1
2.7.0,Test distance 2
2.7.0,Test alkane
2.7.0,Test distance 1
2.7.0,3 self connections and 2 bonds which are both counted twice because of
2.7.0,symmetry for 7 total
2.7.0,Test distance 2
2.7.0,Everything is connected at this distance
2.7.0,Test alkane
2.7.0,Test distance infinity
2.7.0,Everything is connected at this distance
2.7.0,Test pentane
2.7.0,Test distance infinity
2.7.0,Everything is connected at this distance
2.7.0,Only one carbon
2.7.0,Test feature sizes
2.7.0,"No bonds, so only 1 pair feature (for the self interaction)"
2.7.0,Only 4 atoms
2.7.0,Test feature sizes for chirality
2.7.0,3 carbonds in alkane
2.7.0,Test feature sizes
2.7.0,Should be a 3x3 interaction grid
2.7.0,mol_list = featurizer.featurize(mols)
2.7.0,mol = mol_list[0]
2.7.0,3 carbonds in alkane
2.7.0,Test feature sizes
2.7.0,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.7.0,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.7.0,symmetry)
2.7.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.7.0,of degree 1 (connected only to central nitrogen).
2.7.0,import rdkit.Chem
2.7.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.7.0,5 atoms in compound
2.7.0,Test feature sizes
2.7.0,Should be a 3x3 interaction grid
2.7.0,Artificial feature array.
2.7.0,0 atoms of degree 0
2.7.0,0 atoms of degree 1
2.7.0,4 atoms of degree 2
2.7.0,0 atoms of degree 3
2.7.0,0 atoms of degree 4
2.7.0,0 atoms of degree 5
2.7.0,0 atoms of degree 6
2.7.0,0 atoms of degree 7
2.7.0,0 atoms of degree 8
2.7.0,0 atoms of degree 9
2.7.0,0 atoms of degree 10
2.7.0,atom 4 has 0 neighbors
2.7.0,atom 0 has 2 neighbors
2.7.0,atom 1 has 2 neighbors
2.7.0,atom 2 has 2 neighbors
2.7.0,atom 3 has 3 neighbors.
2.7.0,Verify that atom features have been sorted by atom degree.
2.7.0,Sorting is done by atom degree as before. So the ordering goes
2.7.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.7.0,from new position to old position is
2.7.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.7.0,list respects this reordering and returns correct adjacency list.
2.7.0,First example molecule
2.7.0,Artificial feature array.
2.7.0,Second example molecule
2.7.0,Third example molecule
2.7.0,Test agglomerate molecule method
2.7.0,No atoms of degree 0
2.7.0,3 atoms of degree 1
2.7.0,8 atoms of degree 2
2.7.0,1 atom of degree 3
2.7.0,0 atoms of degree 4
2.7.0,0 atoms of degree 5
2.7.0,Check that atoms are only connected to themselves.
2.7.0,Check that there's one atom of each degree.
2.7.0,calculate coordinates
2.7.0,not zero values
2.7.0,Calculate frequency
2.7.0,flake8: noqa
2.7.0,assumes that every array is of the same dimension
2.7.0,rem_dataset is remaining portion of dataset
2.7.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.7.0,to k-1.
2.7.0,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.7.0,validation
2.7.0,skip list
2.7.0,skip path string
2.7.0,main logic
2.7.0,for str
2.7.0,for list
2.7.0,dict is needed in case groups aren't strictly flattened or
2.7.0,hashed by something non-integer like
2.7.0,Figure out how many positive samples we want for each task in each dataset.
2.7.0,Assign the positive samples to datasets.  Since a sample may be positive
2.7.0,"on more than one task, we need to keep track of the effect of each added"
2.7.0,"sample on each task.  To try to keep everything balanced, we cycle through"
2.7.0,"tasks, assigning one positive sample for each one."
2.7.0,We have a sample that hasn't been assigned yet.  Assign it to
2.7.0,whichever set currently has the lowest fraction of its target for
2.7.0,this task.
2.7.0,The remaining samples are negative for all tasks.  Add them to fill out
2.7.0,each set to the correct total number.
2.7.0,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.7.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.7.0,This can be generalized in the future with some common demoninator determination.
2.7.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.7.0,Append remaining examples to train
2.7.0,################################################################
2.7.0,Splitter for molecule datasets
2.7.0,################################################################
2.7.0,Sort by increasing MW
2.7.0,calcaulate scaffold sets
2.7.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.7.0,Compute fingerprints for all molecules.
2.7.0,Split into two groups: training set and everything else.
2.7.0,Split the second group into validation and test sets.
2.7.0,Begin by assigning the first molecule to the first group.
2.7.0,Return identity if no tuple to split to
2.7.0,Decide which group to assign a molecule to.
2.7.0,Identify the unassigned molecule that is least similar to everything in
2.7.0,the other group.
2.7.0,Add it to the group.
2.7.0,Update the data on unassigned molecules.
2.7.0,Sort from largest to smallest scaffold sets
2.7.0,################################################################
2.7.0,Not well supported splitters
2.7.0,################################################################
2.7.0,All datasets share features and identifiers by assumption.
2.7.0,flake8: noqa
2.7.0,basic splitter
2.7.0,molecule splitter
2.7.0,other splitter
2.7.0,################################################################
2.7.0,Removed API
2.7.0,################################################################
2.7.0,Note that the extra task goes to test
2.7.0,Number tasks per fold
2.7.0,Find the tasks that correspond to this test fold
2.7.0,Assert that all arrays look like they should
2.7.0,"task_type = ""regression"""
2.7.0,0 1 2 3 4 5 6 7 8 9
2.7.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.7.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.7.0,reshard() to handle this?
2.7.0,Verify lengths is 10/k == 2
2.7.0,Verify that compounds in this fold are subset of original compounds
2.7.0,Verify that no two folds have overlapping compounds.
2.7.0,Verify lengths is 10/k == 2
2.7.0,Verify that compounds in this fold are subset of original compounds
2.7.0,Verify that no two folds have overlapping compounds.
2.7.0,Verify lengths is 10/k == 2
2.7.0,Verify that compounds in this fold are subset of original compounds
2.7.0,Verify that no two folds have overlapping compounds.
2.7.0,Test singletask case.
2.7.0,The split index should partition dataset in half.
2.7.0,Test singletask case.
2.7.0,Test case where some weights are zero (i.e. masked)
2.7.0,Set half the positives to have zero weight
2.7.0,There are 10 nonzero actives.
2.7.0,"The split index should partition this into half, so expect 5"
2.7.0,The split index should partition the positives for each task roughly in half.
2.7.0,Mask half the examples
2.7.0,The split index should partition dataset in half.
2.7.0,Test singletask case.
2.7.0,Should have split cleanly in half (picked random seed to ensure this)
2.7.0,Check positives are correctly distributed
2.7.0,Test singletask case.
2.7.0,Should have made an 80/10/10 train/valid/test split of actives.
2.7.0,Verify lengths is 100/k == 20
2.7.0,Note: This wouldn't work for multitask str
2.7.0,assert len(fold_dataset) == n_samples/K
2.7.0,Verify that each fold has n_positives/K = 4 positive examples.
2.7.0,Verify that compounds in this fold are subset of original compounds
2.7.0,Verify that no two folds have overlapping compounds.
2.7.0,The amount of datapoints has to be the same
2.7.0,The number of scaffolds generated by the splitter
2.7.0,has to be smaller or equal than number of total molecules
2.7.0,edges logits used during training
2.7.0,nodes logits used during training
2.7.0,edges logits
2.7.0,nodes logits
2.7.0,training of the model
2.7.0,generating compounds
2.7.0,nodes logits used during compound generation
2.7.0,Create the inputs.
2.7.0,Create the generators.
2.7.0,Create the discriminators.
2.7.0,Compute the loss functions.
2.7.0,Create learnable weights for the generators and discriminators.
2.7.0,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.7.0,Compute the weighted errors
2.7.0,Add an entropy term to the loss.
2.7.0,Create the Keras model.
2.7.0,"Every call to fit_generator() will increment global_step, but we only"
2.7.0,"want it to get incremented once for the entire batch, so record the"
2.7.0,value and keep resetting it.
2.7.0,Train the discriminator.
2.7.0,Train the generator.
2.7.0,Write checkpoints and report progress.
2.7.0,Write out final results.
2.7.0,Chain of flows is also a normalizing flow
2.7.0,An instance of tfd.TransformedDistribution
2.7.0,TODO: Incompability between TF and TFP means that TF doesn't track
2.7.0,trainable variables in the flow; must override `_create_gradient_fn`
2.7.0,self._variables = self.flow.trainable_variables
2.7.0,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.7.0,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.7.0,This is for API consistency
2.7.0,extended one of probabilites to binary distribution
2.7.0,extended one of probabilites to binary distribution
2.7.0,-*- coding: utf-8 -*-
2.7.0,"Shape (N_atoms, M_nbrs, ndim)"
2.7.0,"Shape (N_atoms, M_nbrs, ndim)"
2.7.0,"Shape (N_atoms, M_nbrs)"
2.7.0,Generate the nb_affine weights and biases
2.7.0,Extract atom_features
2.7.0,Extract graph topology
2.7.0,Sum all neighbors using adjacency matrix
2.7.0,Get collection of modified atom features
2.7.0,Obtain relevant atoms for this degree
2.7.0,Get self atoms
2.7.0,Apply hidden affine to relevant atoms and append
2.7.0,Determine the min_deg=0 case
2.7.0,Only use the self layer
2.7.0,Combine all atoms back into the list
2.7.0,Tensorflow correctly processes empty lists when using concat
2.7.0,"Sum along neighbors as well as self, and store"
2.7.0,Perform the mol gather
2.7.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.7.0,"self.max_degree, self.min_degree)"
2.7.0,Tensorflow correctly processes empty lists when using concat
2.7.0,Get self atoms
2.7.0,"There are no neighbors of this degree, so just create an empty tensor directly."
2.7.0,Expand dims
2.7.0,always deg-1 for deg_adj_lists
2.7.0,Extract graph topology
2.7.0,means that this is second loop of convolution
2.7.0,No other forget biases supported right now.
2.7.0,Taken from Keras code [citation needed]
2.7.0,"x is test set, xp is support set."
2.7.0,Get initializations
2.7.0,Process using attention
2.7.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.7.0,Generate new attention states
2.7.0,Support set lstm
2.7.0,Test lstm
2.7.0,Get initializations
2.7.0,Rename support
2.7.0,Process support xp using attention
2.7.0,Get linear combination of support set
2.7.0,Process test x using attention
2.7.0,Generate new support attention states
2.7.0,Generate new test attention states
2.7.0,Redefine
2.7.0,Number of rotatable bonds
2.7.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.7.0,a difference.
2.7.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.7.0,neighbors lists an argument instead of a part of this layer.
2.7.0,"Shape (N, M)"
2.7.0,"Shape (N, M)"
2.7.0,"Shape (N, M)"
2.7.0,Number of grid cells
2.7.0,TODO(rbharath): Support batching
2.7.0,"Shape (n_cells, ndim)"
2.7.0,"List of length N_atoms, each element of different length uniques_i"
2.7.0,"List of length N_atoms, each element of different length uniques_i"
2.7.0,"List of length N_atoms, each a tensor of shape"
2.7.0,"(uniques_i, ndim)"
2.7.0,Add phantom atoms that exist far outside the box
2.7.0,"List of length N_atoms, each of shape (1, ndim)"
2.7.0,TODO(rbharath): How does distance need to be modified here to
2.7.0,account for periodic boundary conditions?
2.7.0,List of length N_atoms each of shape (M_nbrs)
2.7.0,"N_atoms elts of size (M_nbrs,) each"
2.7.0,"Shape (N_atoms, 1)"
2.7.0,Find M_nbrs atoms closest to each cell
2.7.0,"Shape (n_cells, M_nbrs)"
2.7.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.0,"conditions, so does wrapround. O(constant)"
2.7.0,"Shape (n_cells, n_nbr_cells)"
2.7.0,"Shape (N_atoms, n_nbr_cells)"
2.7.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.7.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.7.0,"List of length N_atoms, each element length uniques_i"
2.7.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.7.0,element removed to remove self from list of neighbors. Need to verify
2.7.0,this holds more broadly or come up with robust alternative.
2.7.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.7.0,Shape (N_atoms*n_cells)
2.7.0,"Shape (n_cells, N_atoms)"
2.7.0,Find k atoms closest to this cell. Notice negative sign since
2.7.0,tf.nn.top_k returns *largest* not smallest.
2.7.0,"Tensor of shape (n_cells, M_nbrs)"
2.7.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.0,"Shape (N_atoms*n_cells, 1) after tile"
2.7.0,9 neighbors in 2-space
2.7.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.7.0,Number of cells for cube in 3-space is
2.7.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.7.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.7.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.7.0,the cube.
2.7.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.7.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.7.0,"Tile (a, a, a, b, b, b, etc.)"
2.7.0,"Tile (a, b, c, a, b, c, ...)"
2.7.0,N: Maximum number of atoms
2.7.0,M: Maximum number of neighbors
2.7.0,d: Number of coordinates/features/filters
2.7.0,B: Batch Size
2.7.0,Compute the distances and radial symmetry functions.
2.7.0,check that there isnt just one or zero inputs
2.7.0,create subspaces
2.7.0,"concatenate subspaces, reshape to size of original input, then stack"
2.7.0,"such that out_tensor has shape (2,?,original_cols)"
2.7.0,creates subspaces the same way it was done in AlphaShare
2.7.0,calculate squared Frobenius norm
2.7.0,"(TODO YTZ:) faster, less memory intensive way"
2.7.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.7.0,"r = tf.expand_dims(r, -1)"
2.7.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.7.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.7.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.7.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.7.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.7.0,Calculate pairwise distance
2.7.0,Cutoff with threshold Rc
2.7.0,return d
2.7.0,tf.stack issues again...
2.7.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.7.0,So the Tensor has known dimensions
2.7.0,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.7.0,normalization
2.7.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.7.0,and embeddings of atom j(both gone through a hidden layer)
2.7.0,"for atom i, sum the influence from all other atom j in the molecule"
2.7.0,number of inputs each step
2.7.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.7.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.7.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.7.0,`count`-th step
2.7.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.7.0,generating index for graph features used in the inputs
2.7.0,"extracting graph features for parents of the target atoms, then flatten"
2.7.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.7.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.7.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.7.0,of shape: (batch_size*max_atoms) * n_graph_features
2.7.0,representing the graph features of target atoms in each graph
2.7.0,index for targe atoms
2.7.0,Extract atom_features
2.7.0,sum all graph outputs
2.7.0,"Default message function: edge network, update function: GRU"
2.7.0,more options to be implemented
2.7.0,Add another value(~-Inf) to prevent error in softmax
2.7.0,Model using this layer must set pad_batches=True
2.7.0,Perform one step of LSTM
2.7.0,task_metadata_rows = {task: [] for task in tasks}
2.7.0,Extract those datapoints which are present for this task
2.7.0,Loading is done on-the-fly
2.7.0,Build the model.
2.7.0,Final atom-layer convolution. Note this differs slightly from the paper
2.7.0,since we use a tanh activation as default. This seems necessary for numerical
2.7.0,stability.
2.7.0,Now fully connected layers
2.7.0,Should this allow for training?
2.7.0,"pair_edges is of shape (2, N)"
2.7.0,number of atoms in each molecule
2.7.0,index of pair features
2.7.0,Get starting pair atoms
2.7.0,number of pairs for each atom
2.7.0,atom features
2.7.0,pair features
2.7.0,Build the model.
2.7.0,Build the model.
2.7.0,calculation orders for a batch of molecules
2.7.0,padding atom features vector of each molecule with 0
2.7.0,Build the model.
2.7.0,number of atoms in each molecule
2.7.0,index of pair features
2.7.0,number of pairs for each atom
2.7.0,atom features
2.7.0,pair features
2.7.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.7.0,Add the input features.
2.7.0,Add the shared dense layers
2.7.0,Add task-specific bypass layers
2.7.0,Add the input features.
2.7.0,Add the shared dense layers
2.7.0,Add task-specific bypass layers
2.7.0,W&B flag support (DEPRECATED)
2.7.0,"If `wandb=True` and no logger is provided, initialize default logger"
2.7.0,Setup and initialize W&B logging
2.7.0,Update config with KerasModel params
2.7.0,Backwards compatibility
2.7.0,The optimizer creates internal variables the first time apply_gradients()
2.7.0,is called for a new set of variables.  If that happens inside a function
2.7.0,"annotated with tf.function it throws an exception, so call it once here."
2.7.0,Main training loop.
2.7.0,"Execute the loss function, accumulating the gradients."
2.7.0,Report progress and write checkpoints.
2.7.0,Capture the last avg_loss in case of return since we're resetting to
2.7.0,0 now
2.7.0,Report final results.
2.7.0,Invoke the model.
2.7.0,Apply tranformers and record results.
2.7.0,Concatenate arrays to create the final results.
2.7.0,Use a GradientTape to compute gradients.
2.7.0,Ensure weights for both models are built.
2.7.0,Define the PyTorch Module that implements the model.
2.7.0,Define the PyTorch Module that implements the model.
2.7.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.7.0,set wandb init arguments
2.7.0,Dataset ids are used to differentiate datasets seen by the logger
2.7.0,log data
2.7.0,Similarity values
2.7.0,Labels for all top K similar samples
2.7.0,Discard any padded predictions
2.7.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.7.0,Build the model.
2.7.0,Character embedding
2.7.0,Multiple convolutional layers with different filter widths
2.7.0,Max-over-time pooling
2.7.0,Concat features from all filters(one feature per filter)
2.7.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.7.0,SMILES strings
2.7.0,Maximum length is expanded to allow length variation during train and inference
2.7.0,'_' served as delimiter and padding
2.7.0,Initialize common characters as keys
2.7.0,Include space to avoid extra keys
2.7.0,"For 'Cl', 'Br', etc."
2.7.0,"Character not recognized, add to extra_keys"
2.7.0,Add all extra_keys to char_dict
2.7.0,Transform SMILES sequence to integers
2.7.0,Skip all spaces
2.7.0,"For 'Cl', 'Br', etc."
2.7.0,Padding with '_'
2.7.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.7.0,"layer_sizes=[32, 32, 16],"
2.7.0,Add the dense layers
2.7.0,Do a simple greedy search.
2.7.0,Do a beam search with length normalization.
2.7.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.7.0,This candidate sequence has already been terminated
2.7.0,Consider all possible tokens we could add to this candidate sequence.
2.7.0,Add the input features.
2.7.0,Handle output layer
2.7.0,Iterate over all previous tasks.
2.7.0,prev_layers is a list with elements of size
2.7.0,"(batch_size, layer_sizes[i-1])"
2.7.0,Log data to Wandb
2.7.0,flake8: noqa
2.7.0,Tensorflow Dependency Models
2.7.0,scikit-learn model
2.7.0,PyTorch models
2.7.0,Pytorch models with torch-geometric dependency
2.7.0,TODO We should clean up DMPNN and remove torch_geometric dependency during import
2.7.0,Pytorch-lightning modules import
2.7.0,Jax models
2.7.0,####################################################################################
2.7.0,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.7.0,####################################################################################
2.7.0,#######################################################################################
2.7.0,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.7.0,#######################################################################################
2.7.0,Last layer sequences not returned.
2.7.0,This is needed because ImageDataGenerator does infinite looping
2.7.0,"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
2.7.0,but turns out to be slightly faster
2.7.0,JAX depend
2.7.0,Main training loop
2.7.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.7.0,Report final results.
2.7.0,Apply tranformers and record results.
2.7.0,Concatenate arrays to create the final results.
2.7.0,"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
2.7.0,") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
2.7.0,""""""""
2.7.0,"Predict the model's outputs, along with the uncertainty in each one."
2.7.0,The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
2.7.0,It involves repeating the prediction many times with different dropout masks.
2.7.0,The prediction is computed as the average over all the predictions.  The
2.7.0,uncertainty includes both the variation among the predicted values (epistemic
2.7.0,uncertainty) and the model's own estimates for how well it fits the data
2.7.0,(aleatoric uncertainty).  Not all models support uncertainty prediction.
2.7.0,Parameters
2.7.0,----------
2.7.0,dataset: dc.data.Dataset
2.7.0,Dataset to make prediction on
2.7.0,masks: int
2.7.0,the number of dropout masks to average over
2.7.0,Returns
2.7.0,-------
2.7.0,"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
2.7.0,"value of the output, and each element of y_std estimates the standard"
2.7.0,deviation of the corresponding element of y_pred
2.7.0,""""""""
2.7.0,sum_pred: List[np.ndarray] = []
2.7.0,sum_sq_pred: List[np.ndarray] = []
2.7.0,sum_var: List[np.ndarray] = []
2.7.0,for i in range(masks):
2.7.0,generator = self.default_generator(
2.7.0,"dataset, mode='uncertainty', pad_batches=False)"
2.7.0,"results = self._predict(generator, [], True, None)"
2.7.0,if len(sum_pred) == 0:
2.7.0,"for p, v in results:"
2.7.0,sum_pred.append(p)
2.7.0,sum_sq_pred.append(p * p)
2.7.0,sum_var.append(v)
2.7.0,else:
2.7.0,"for j, (p, v) in enumerate(results):"
2.7.0,sum_pred[j] += p
2.7.0,sum_sq_pred[j] += p * p
2.7.0,sum_var[j] += v
2.7.0,output = []
2.7.0,std = []
2.7.0,for i in range(len(sum_pred)):
2.7.0,p = sum_pred[i] / masks
2.7.0,output.append(p)
2.7.0,std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
2.7.0,if len(output) == 1:
2.7.0,"return (output[0], std[0])"
2.7.0,else:
2.7.0,"return list(zip(output, std))"
2.7.0,JAX dependencies
2.7.0,Main training loop
2.7.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.7.0,Report final results.
2.7.0,Apply tranformers and record results.
2.7.0,Concatenate arrays to create the final results.
2.7.0,flake8:noqa
2.7.0,The PINNModel requires you to create two functions
2.7.0,`create_eval`_fn for letting the model know how to compute the model in inference and
2.7.0,`gradient_fn` for letting model know how to compute the gradient and different regulariser
2.7.0,equation loss depending on the differential equation
2.7.0,defining the Haiku model
2.7.0,"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
2.7.0,"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
2.7.0,which will be used as the differential loss(regulariser)
2.7.0,The expected solution must be as close to cos(x)
2.7.0,Initialize the weights with random values
2.7.0,Forward function which takes the params
2.7.0,Loss Function
2.7.0,JaxModel Working
2.7.0,sample network
2.7.0,Model Initialization
2.7.0,Loss Function
2.7.0,JaxModel Working
2.7.0,sample network
2.7.0,Model Initilisation
2.7.0,Loss Function
2.7.0,JaxModel Working
2.7.0,Model Initilisation
2.7.0,Loss Function
2.7.0,JaxModel Working
2.7.0,Model Initilisation
2.7.0,Loss Function
2.7.0,JaxModel Working
2.7.0,Model Initilisation
2.7.0,Loss Function
2.7.0,JaxModel Working
2.7.0,Each epoch is a single step for this model
2.7.0,@pytest.mark.jax
2.7.0,@pytest.mark.slow
2.7.0,def test_uncertainty():
2.7.0,"""""""Test estimating uncertainty a TorchModel."""""""
2.7.0,n_samples = 30
2.7.0,n_features = 1
2.7.0,noise = 0.1
2.7.0,"X = np.random.rand(n_samples, n_features)"
2.7.0,"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
2.7.0,"dataset = dc.data.NumpyDataset(X, y)"
2.7.0,class Net(hk.Module):
2.7.0,"def __init__(self, output_size: int = 1):"
2.7.0,super().__init__()
2.7.0,"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.7.0,"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.7.0,self.output = hk.Linear(output_size)
2.7.0,self.log_var = hk.Linear(output_size)
2.7.0,"def __call__(self, x):"
2.7.0,x = self._network1(x)
2.7.0,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.7.0,x = self._network2(x)
2.7.0,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.7.0,output = self.output(x)
2.7.0,log_var = self.log_var(x)
2.7.0,var = jnp.exp(log_var)
2.7.0,"return output, var, output, log_var"
2.7.0,def f(x):
2.7.0,net = Net(1)
2.7.0,return net(x)
2.7.0,"def loss(outputs, labels, weights):"
2.7.0,diff = labels[0] - outputs[0]
2.7.0,log_var = outputs[1]
2.7.0,var = jnp.exp(log_var)
2.7.0,return jnp.mean(diff * diff / var + log_var)
2.7.0,class UncertaintyModel(JaxModel):
2.7.0,"def default_generator(self,"
2.7.0,"dataset,"
2.7.0,"epochs=1,"
2.7.0,"mode='fit',"
2.7.0,"deterministic=True,"
2.7.0,pad_batches=True):
2.7.0,for epoch in range(epochs):
2.7.0,"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
2.7.0,"batch_size=self.batch_size,"
2.7.0,"deterministic=deterministic,"
2.7.0,pad_batches=pad_batches):
2.7.0,"yield ([X_b], [y_b], [w_b])"
2.7.0,jm_model = hk.transform(f)
2.7.0,rng = jax.random.PRNGKey(500)
2.7.0,"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
2.7.0,modified_inputs = jnp.array(
2.7.0,[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
2.7.0,"params = jm_model.init(rng, modified_inputs)"
2.7.0,model = UncertaintyModel(
2.7.0,"jm_model.apply,"
2.7.0,"params,"
2.7.0,"loss,"
2.7.0,"output_types=['prediction', 'variance', 'loss', 'loss'],"
2.7.0,learning_rate=0.003)
2.7.0,"model.fit(dataset, nb_epochs=2500)"
2.7.0,"pred, std = model.predict_uncertainty(dataset)"
2.7.0,assert np.mean(np.abs(y - pred)) < 2.0
2.7.0,assert noise < np.mean(std) < 1.0
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,Conv2d and Linear layers test(CNN classification)
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,test if adjacency matrix input is correctly set
2.7.0,test if nodes features matrix input is correctly set
2.7.0,check discriminator shape
2.7.0,check training edges logits shape
2.7.0,check training nodes logits shapes
2.7.0,True will be assigned up successful training attempt
2.7.0,force clear tensor flow backend
2.7.0,create new model
2.7.0,to avoid flake8 E125/yapf incompatibility
2.7.0,generate input
2.7.0,train model
2.7.0,generate sample
2.7.0,check how many valid molecules were created and add to list
2.7.0,finally test if there was at least one valid training session
2.7.0,as the model structure improves this should become more and more strict
2.7.0,Predict the output and uncertainty.
2.7.0,predict datset with no y (ensured by tasks = [])
2.7.0,Predict the output and uncertainty.
2.7.0,The DAG models have high error with dropout
2.7.0,"Despite a lot of effort tweaking it , there appears to be"
2.7.0,a limit to how low the error can go with dropout.
2.7.0,assert mean_error < 0.5 * mean_value
2.7.0,Predict the output and uncertainty.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,load datasets
2.7.0,disable transformer
2.7.0,check train
2.7.0,check predict shape
2.7.0,check overfit
2.7.0,load datasets
2.7.0,disable transformer
2.7.0,check train
2.7.0,check predict shape
2.7.0,check overfit
2.7.0,load datasets
2.7.0,disable transformer
2.7.0,check train
2.7.0,check predict shape
2.7.0,check overfit
2.7.0,reload
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Check same predictions are made.
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Load trained model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload trained model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Reload trained model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload trained model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reloaded Trained Model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Check predictions match on random sample
2.7.0,3D Multivariate Gaussian base distribution
2.7.0,Check that reloaded model can sample from the distribution
2.7.0,Check that density estimation is same for reloaded model
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload trained model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload Trained Model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload Trained Model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload trained model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Check predictions match on random sample
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload trained model
2.7.0,Eval model on train
2.7.0,Check predictions match on random sample
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload trained model
2.7.0,Eval model on train
2.7.0,Check predictions match on random sample
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Reload trained model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Reload trained Model
2.7.0,Check predictions match on random sample
2.7.0,Eval model on train
2.7.0,Reload Trained Model
2.7.0,Check predictions match on random sample
2.7.0,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.7.0,Reload Trained Model
2.7.0,Check predictions match on original dataset
2.7.0,TODO: We need a cleaner usage example for this
2.7.0,Fit trained model
2.7.0,Check predictions match on random sample
2.7.0,Train the model on random sequences.  We aren't training long enough to
2.7.0,"really make it reliable, but I want to keep this test fast, and it should"
2.7.0,still be able to reproduce a reasonable fraction of input sequences.
2.7.0,Test it out.
2.7.0,check predict shape
2.7.0,check overfit
2.7.0,needs change
2.7.0,check predict shape
2.7.0,check overfit
2.7.0,reload
2.7.0,The first pass of the transformation should be 0
2.7.0,Test sampling method
2.7.0,Test log_prob method (this method is used when inverse pass)
2.7.0,Output must be a Nth zero array since nothing is being learned yet
2.7.0,Featurize to assert for tests
2.7.0,Assert errors for sample method
2.7.0,Assert errors for log_prob method
2.7.0,get data
2.7.0,prepare batch (size 1)
2.7.0,initialize the model
2.7.0,get output
2.7.0,get data
2.7.0,prepare batch (size 1)
2.7.0,initialize the model
2.7.0,get output
2.7.0,get data
2.7.0,prepare batch (size 1)
2.7.0,initialize the model
2.7.0,get output
2.7.0,load sample dataset
2.7.0,initialize the model
2.7.0,overfit test
2.7.0,load sample dataset
2.7.0,initialize the model
2.7.0,overfit test
2.7.0,load sample dataset
2.7.0,initialize the model
2.7.0,fit the model
2.7.0,reload the model
2.7.0,There are 4 atoms each of which have 75 atom features
2.7.0,There are 10 pairs with infinity distance and 14 pair features
2.7.0,4 atoms in total
2.7.0,10 pairs in total
2.7.0,10 pairs in total each with start/finish
2.7.0,There are 4 atoms each of which have 75 atom features
2.7.0,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.7.0,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.7.0,connections and accounting for symmetry.)
2.7.0,4 atoms in total
2.7.0,10 pairs in total
2.7.0,The center atom is self connected and to both neighbors so it appears
2.7.0,thrice. The canonical ranking used in MolecularFeaturizer means this
2.7.0,central atom is ranked last in ordering.
2.7.0,10 pairs in total each with start/finish
2.7.0,def test_weave_fit_simple_infinity_distance():
2.7.0,featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
2.7.0,"X = featurizer([""C"", ""CCC""])"
2.7.0,"y = np.array([0, 1.])"
2.7.0,"dataset = dc.data.NumpyDataset(X, y)"
2.7.0,batch_size = 20
2.7.0,model = WeaveModel(
2.7.0,"1,"
2.7.0,"batch_size=batch_size,"
2.7.0,"mode='classification',"
2.7.0,"fully_connected_layer_sizes=[2000, 1000],"
2.7.0,"batch_normalize=True,"
2.7.0,batch_normalize_kwargs={
2.7.0,"""fused"": False,"
2.7.0,"""trainable"": True,"
2.7.0,"""renorm"": True"
2.7.0,"},"
2.7.0,learning_rate=0.0005)
2.7.0,"model.fit(dataset, nb_epoch=200)"
2.7.0,transformers = []
2.7.0,metric = dc.metrics.Metric(
2.7.0,"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
2.7.0,"scores = model.evaluate(dataset, [metric], transformers)"
2.7.0,assert scores['mean-roc_auc_score'] >= 0.9
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,load datasets
2.7.0,initialize model
2.7.0,overfit test
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Fit trained model
2.7.0,Predict the output and uncertainty.
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,xgboost test
2.7.0,fit trained model
2.7.0,eval model on test
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,lightgbm test
2.7.0,fit trained model
2.7.0,eval model on test
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,xgboost test
2.7.0,fit trained model
2.7.0,eval model on test
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,lightgbm test
2.7.0,fit trained model
2.7.0,eval model on test
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,xgboost test
2.7.0,fit trained model
2.7.0,eval model on test
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,lightgbm test
2.7.0,fit trained model
2.7.0,eval model on test
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,xgboost test
2.7.0,fit trained model
2.7.0,reload
2.7.0,check predictions match on test dataset
2.7.0,eval model on test
2.7.0,prepare dataset
2.7.0,global setting
2.7.0,lightgbm test
2.7.0,fit trained model
2.7.0,reload
2.7.0,check predictions match on test dataset
2.7.0,eval model on test
2.7.0,"For simplicity, let's assume both molecules have same number of"
2.7.0,atoms.
2.7.0,Creates a set of dummy features that contain the coordinate and
2.7.0,neighbor-list features required by the AtomicConvModel.
2.7.0,Creates a set of dummy features that contain the coordinate and
2.7.0,neighbor-list features required by the AtomicConvModel.
2.7.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.7.0,max num atoms instead of exact.
2.7.0,Cutoff in angstroms
2.7.0,arbitrary label
2.7.0,Run a fitting operation
2.7.0,Testing graphnet for a single graph
2.7.0,Testing for consistency
2.7.0,Testing with a batch of Graphs
2.7.0,"When pytest runs without pytorch in the environment (ex: as in tensorflow workflow),"
2.7.0,the above import raises a ModuleNotFoundError. It is safe to ignore it
2.7.0,since the below tests only run in an environment with pytorch installed.
2.7.0,TODO The test is skipped as FakeGraphGenerator has to be updated
2.7.0,to generate regression labels
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Fit trained model
2.7.0,Eval model on train/test
2.7.0,Fit trained model
2.7.0,Eval model on train/test
2.7.0,Fit trained model
2.7.0,Eval model on train/test
2.7.0,See if it has done a plausible job of learning the distribution.
2.7.0,See if it has done a plausible job of learning the distribution.
2.7.0,See if it has done a plausible job of learning the distribution.
2.7.0,No training has been done after reload
2.7.0,See if it has done a plausible job of learning the distribution.
2.7.0,We have to set the gradient penalty very small because the generator's
2.7.0,"output is only a single number, so the default penalty would constrain"
2.7.0,it far too much.
2.7.0,See if it has done a plausible job of learning the distribution.
2.7.0,We have to set the gradient penalty very small because the generator's
2.7.0,"output is only a single number, so the default penalty would constrain"
2.7.0,it far too much.
2.7.0,See if it has done a plausible job of learning the distribution.
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,n_samples = 100
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Most weights should be close to zero.
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Most weights should be close to zero.
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Predict the output and uncertainty.
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Load mini log-solubility dataset.
2.7.0,Fit trained model
2.7.0,Eval model on train
2.7.0,Check that predicting internal layers works.
2.7.0,Each epoch is a single step for this model
2.7.0,Create two models using the same model directory.
2.7.0,Check that they produce different results.
2.7.0,"Save a checkpoint from the first model and load it into the second one,"
2.7.0,and make sure they now match.
2.7.0,Train a model to overfit the dataset.
2.7.0,"Create an identical model, do a single step of fitting with restore=True,"
2.7.0,and make sure it got restored correctly.
2.7.0,Build a model that predicts uncertainty.
2.7.0,Fit the model and see if its predictions are correct.
2.7.0,Take a tiny step in the direction of s and see if the output changes by
2.7.0,the expected amount.
2.7.0,Load dataset and Models
2.7.0,call model.fit again to test multiple fit() calls
2.7.0,Set up tests.
2.7.0,def test_singletask_to_multitask_classification(self):
2.7.0,n_features = 10
2.7.0,n_tasks = 17
2.7.0,tasks = range(n_tasks)
2.7.0,# Define train dataset
2.7.0,n_train = 100
2.7.0,"X_train = np.random.rand(n_train, n_features)"
2.7.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.7.0,w_train = np.ones_like(y_train)
2.7.0,"ids_train = [""C""] * n_train"
2.7.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.7.0,"X_train, y_train, w_train, ids_train)"
2.7.0,# Define test dataset
2.7.0,n_test = 10
2.7.0,"X_test = np.random.rand(n_test, n_features)"
2.7.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.7.0,w_test = np.ones_like(y_test)
2.7.0,"ids_test = [""C""] * n_test"
2.7.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.7.0,"X_test, y_test, w_test, ids_test)"
2.7.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.7.0,def model_builder(model_dir):
2.7.0,sklearn_model = LogisticRegression()
2.7.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.7.0,multitask_model = dc.models.SingletaskToMultitask(
2.7.0,"tasks, model_builder)"
2.7.0,# Fit trained model
2.7.0,multitask_model.fit(train_dataset)
2.7.0,multitask_model.save()
2.7.0,# Eval multitask_model on train/test
2.7.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.7.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.7.0,Generate data
2.7.0,Cleanup
2.7.0,test for the prepare_input_stream function of Ferminet class
2.7.0,ionic charge initialization test
2.7.0,Train the model while logging the validation ROC AUC.
2.7.0,Parse the log to pull out the AUC scores.
2.7.0,The last reported score should match the current performance of the model.
2.7.0,The highest recorded score should match get_best_score().
2.7.0,Reload the save model and confirm that it matches the best logged score.
2.7.0,Make sure get_best_score() still works when save_dir is not specified
2.7.0,3D Multivariate Gaussian base distribution
2.7.0,Must be float32 for RealNVP
2.7.0,Tests a simple flow of one RealNVP layer.
2.7.0,log likelihoods should be negative
2.7.0,# Fit model
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,x and y are the same tensor (equivalent at every element)
2.7.0,the pairwise inner product of the rows in x and y will always be 1
2.7.0,"the output tensor will be of shape (5,5)"
2.7.0,each row in x1 is orthogonal to each row in x2
2.7.0,the pairwise inner product of the rows in x and y will always be 0
2.7.0,"the output tensor will be of shape (256,256)"
2.7.0,"Creating a second layer should produce different results, since it has"
2.7.0,different random weights.
2.7.0,But evaluating the first layer again should produce the same result as before.
2.7.0,index of pair features
2.7.0,number of pairs for each atom
2.7.0,atom features
2.7.0,pair features
2.7.0,"Outputs should be [A, P]"
2.7.0,atom features
2.7.0,Try without compression
2.7.0,"Outputs should be [mol1_vec, mol2_vec)"
2.7.0,Try with compression
2.7.0,"Outputs should be [mol1_vec, mol2_vec)"
2.7.0,atom features
2.7.0,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.7.0,Gaussian histograms expands into 11 Gaussian buckets.
2.7.0,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.7.0,TODO What should shape[1] be?  It's not documented.
2.7.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.7.0,"Creating a second layer should produce different results, since it has"
2.7.0,different random weights.
2.7.0,But evaluating the first layer again should produce the same result as before.
2.7.0,"Creating a second layer should produce different results, since it has"
2.7.0,different random weights.
2.7.0,But evaluating the first layer again should produce the same result as before.
2.7.0,"Creating a second layer should produce different results, since it has"
2.7.0,different random weights.
2.7.0,But evaluating the first layer again should produce the same result as before.
2.7.0,"TODO What should the output shape be?  It's not documented, and there"
2.7.0,are no other test cases for it.
2.7.0,"Creating a second layer should produce different results, since it has"
2.7.0,different random weights.
2.7.0,But evaluating the first layer again should produce the same result as before.
2.7.0,"Creating a second layer should produce different results, since it has"
2.7.0,different random weights.
2.7.0,But evaluating the first layer again should produce the same result as before.
2.7.0,"Recall that the DAG layer expects a MultiConvMol as input,"
2.7.0,"so the ""batch"" is a pooled set of atoms from all the"
2.7.0,"molecules in the batch, just as it is for the graph conv."
2.7.0,This means that n_atoms is the batch-size
2.7.0,dropout_switch = False
2.7.0,dropout_switch
2.7.0,# TODO(rbharath): What is the shape of outputs supposed to be?
2.7.0,"# I'm getting (7, 30) here. Where does 7 come from??"
2.7.0,TODO(rbharath): We need more documentation about why
2.7.0,these numbers work.
2.7.0,"By setting the `box_size` to effectively zero, the result should only contain `nan`."
2.7.0,Check that layer has three trainable parameters.
2.7.0,Check when `box_size` is of wrong dimensionality.
2.7.0,Check when `inputs` is of wrong length.
2.7.0,Create a dataset and an input function for processing it.
2.7.0,Create a dataset and an input function for processing it.
2.7.0,Generate dummy dataset
2.7.0,Fit trained model
2.7.0,Eval model on test
2.7.0,Eval model on train
2.7.0,Fit trained model
2.7.0,Eval model on test
2.7.0,Fit trained model
2.7.0,Eval model on test
2.7.0,Fit trained model
2.7.0,Eval model on test
2.7.0,Fit trained model
2.7.0,Eval model on test
2.7.0,Each epoch is a single step for this model
2.7.0,Create two models using the same model directory.
2.7.0,Check that they produce different results.
2.7.0,"Save a checkpoint from the first model and load it into the second one,"
2.7.0,and make sure they now match.
2.7.0,Train a model to overfit the dataset.
2.7.0,"Create an identical model, do a single step of fitting with restore=True,"
2.7.0,and make sure it got restored correctly.
2.7.0,Build a model that predicts uncertainty.
2.7.0,Fit the model and see if its predictions are correct.
2.7.0,Take a tiny step in the direction of s and see if the output changes by
2.7.0,the expected amount.
2.7.0,Load dataset and Models
2.7.0,call model.fit again to test multiple fit() calls
2.7.0,Train the model on random sequences.  We aren't training long enough to
2.7.0,"really make it reliable, but I want to keep this test fast, and it should"
2.7.0,still be able to reproduce a reasonable fraction of input sequences.
2.7.0,Test it out.
2.7.0,Check that it got at least a quarter of them correct.
2.7.0,Test it out.
2.7.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.7.0,"few steps of training to make sure nothing crashes, then check that the"
2.7.0,results are at least internally consistent.
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,overfit test
2.7.0,test on a small MoleculeNet dataset
2.7.0,load datasets
2.7.0,initialize models
2.7.0,Initialize buffers
2.7.0,Accumulate statistics for Fisher matrices
2.7.0,Initialize buffers
2.7.0,p_grad_mat is of output_dim * input_dim
2.7.0,inv((ss')) p_grad_mat inv(aa') = [ Q_g (1/R_g) Q_g^T ] @ p_grad_mat @ [Q_a (1/R_a) Q_a^T]
2.7.0,we always put gradient w.r.t weight in [0]
2.7.0,and w.r.t bias in [1]
2.7.0,do kl clip
2.7.0,PyTorch layers require input and output channels as parameter
2.7.0,"if only one layer to make the model creating loop below work, multiply layer_filters wutg 2"
2.7.0,"Python tuples use 0 based indexing, dims defines number of dimension for convolutional operation"
2.7.0,initializing layer bias with nn.init gives mypy typecheck error
2.7.0,using the following workaround
2.7.0,residual blocks can only be used when successive layers have the same output shape
2.7.0,Used for converting edges back to their original shape
2.7.0,Compute mean edge features for each node by dst_index (each node
2.7.0,"receives information from edges which have that node as its destination,"
2.7.0,hence the computation uses dst_index to aggregate information)
2.7.0,holding bi-directional edges in case of undirected graphs
2.7.0,coonverting edge features to its original shape
2.7.0,Input
2.7.0,Shared weight matrix across depths (default):
2.7.0,For messages hidden states
2.7.0,For atom hidden states
2.7.0,num_atoms x hidden_size
2.7.0,num_molecules x hidden_size
2.7.0,concat global features
2.7.0,"Shape (N_atoms, M_nbrs, ndim)"
2.7.0,"Shape (N_atoms, M_nbrs, ndim)"
2.7.0,"Shape (N_atoms, M_nbrs)"
2.7.0,Number of grid cells
2.7.0,TODO(rbharath): Support batching
2.7.0,"Shape (n_cells, ndim)"
2.7.0,"List of length N_atoms, each element of different length uniques_i"
2.7.0,"List of length N_atoms, each element of different length uniques_i"
2.7.0,"List of length N_atoms, each a tensor of shape"
2.7.0,"(uniques_i, ndim)"
2.7.0,Add phantom atoms that exist far outside the box
2.7.0,"List of length N_atoms, each of shape (1, ndim)"
2.7.0,TODO(rbharath): How does distance need to be modified here to
2.7.0,account for periodic boundary conditions?
2.7.0,List of length N_atoms each of shape (M_nbrs)
2.7.0,"N_atoms elts of size (M_nbrs,) each"
2.7.0,"Shape (N_atoms, 1)"
2.7.0,Find M_nbrs atoms closest to each cell
2.7.0,"Shape (n_cells, M_nbrs)"
2.7.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.7.0,"conditions, so does wrapround. O(constant)"
2.7.0,"Shape (n_cells, n_nbr_cells)"
2.7.0,"Shape (N_atoms, n_nbr_cells)"
2.7.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.7.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.7.0,"List of length N_atoms, each element length uniques_i"
2.7.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.7.0,element removed to remove self from list of neighbors. Need to verify
2.7.0,this holds more broadly or come up with robust alternative.
2.7.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.7.0,Shape (N_atoms*n_cells)
2.7.0,"Shape (n_cells, N_atoms)"
2.7.0,Find k atoms closest to this cell.
2.7.0,"Tensor of shape (n_cells, M_nbrs)"
2.7.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.7.0,"Shape (N_atoms*n_cells, 1) after tile"
2.7.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.7.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.7.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.7.0,the cube.
2.7.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.7.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.7.0,"Tile (a, a, a, b, b, b, etc.)"
2.7.0,"Tile (a, b, c, a, b, c, ...)"
2.7.0,No other forget biases supported right now.
2.7.0,Sum the pairwise-interactions between atoms that are of `atom_type` and its neighbors for each atom type in `atom_types`.
2.7.0,import torch.nn as nn
2.7.0,from deepchem.models.torch_models import TorchModel
2.7.0,import deepchem.models.optimizers as optim
2.7.0,TODO look for the loss function(Hamiltonian)
2.7.0,dummy function which can be passed as the parameter f. f gives the log probability
2.7.0,TODO replace this function with forward pass of the model in future
2.7.0,"super(Ferminet, self).__init__()"
2.7.0,Initialization for ionic molecules
2.7.0,concatenating distance and vectors arrays
2.7.0,embedding node features
2.7.0,convolutional layer
2.7.0,pooling
2.7.0,for n_tasks == 1 case
2.7.0,mypy check is ignored for global_features as it is not a default attribute
2.7.0,of GraphData. It is created during runtime using **kwargs.
2.7.0,mapping from bond index to the index of the atom (where the bond is coming from)
2.7.0,"mapping from bond index to concat(in_atom, bond) features"
2.7.0,mapping from atom index to list of indicies of incoming bonds
2.7.0,mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond (excluding the reverse bonds)
2.7.0,zero padded at the end
2.7.0,get mapping which maps bond index to 'array of indices of the bonds' incoming at the initial atom of the bond
2.7.0,padded with -1 at the end
2.7.0,mapping from bond index to the index of the atom (where the bond if going to)
2.7.0,mapping from atom index to list of indicies of incoming bonds
2.7.0,get maximum number of incoming bonds
2.7.0,Make number of incoming bonds equal to maximum number of bonds.
2.7.0,This is done by appending -1 to fill remaining space at each atom indices.
2.7.0,mapping from bond index to the index of the reverse bond
2.7.0,get encoder
2.7.0,get input size for ffn
2.7.0,get output size for ffn
2.7.0,get ffn
2.7.0,Steps to get `molecules_unbatch_key`:
2.7.0,1. Get the tensor containing the indices of first atoms of each molecule
2.7.0,2. Get the tensor containing number of atoms of each molecule
2.7.0,by taking the difference between consecutive indices.
2.7.0,3. Convert the tensor to a list.
2.7.0,num_molecules x (enc_hidden + global_features_size)
2.7.0,ffn_output (`self.n_tasks` or `self.n_tasks * self.n_classes`)
2.7.0,"atom feature matrix with shape [number of atoms, number of features]"
2.7.0,concatenated feature vector which contains concatenation of initial atom and bond features
2.7.0,mapping from atom index to list of indicies of incoming bonds
2.7.0,mapping that maps bond index to 'array of indices of the bonds'
2.7.0,incoming at the initial atom of the bond (excluding the reverse bonds)
2.7.0,array of global molecular features
2.7.0,maximum number of incoming bonds in the batch
2.7.0,generate concatenated feature vector and mappings
2.7.0,pad all mappings to maximum number of incoming bonds in the batch
2.7.0,Decide first number of GAT layers
2.7.0,We convert deepchem.feat.GraphData to a PyG graph and then
2.7.0,batch it.
2.7.0,The default_generator method returns an array of dc.feat.GraphData objects
2.7.0,"nested inside a list. To access the nested array of graphs, we are"
2.7.0,indexing by 0 here.
2.7.0,flake8:noqa
2.7.0,Select a device.
2.7.0,W&B logging
2.7.0,"If `wandb=True` and no logger is provided, initialize default logger"
2.7.0,Setup and initialize W&B logging
2.7.0,Update config with KerasModel params
2.7.0,Main training loop.
2.7.0,"Execute the loss function, accumulating the gradients."
2.7.0,Report progress and write checkpoints.
2.7.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.7.0,Report final results.
2.7.0,Invoke the model.
2.7.0,Apply tranformers and record results.
2.7.0,Concatenate arrays to create the final results.
2.7.0,Compute the gradients.
2.7.0,Save the checkpoint to a file.
2.7.0,Rename and delete older files.
2.7.0,Ensure weights for both models are built.
2.7.0,model is None when reloading a model
2.7.0,Some scikit-learn models don't use weights.
2.7.0,flake8: ignore
2.7.0,flake8:noqa
2.7.0,GDBT doesn't support multi-output(task)
2.7.0,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.7.0,retrain model to whole data using best n_estimators * 1.25
2.7.0,GDBT doesn't support multi-output(task)
2.7.0,########################################
2.7.0,Deprecation warnings for XGBoostModel
2.7.0,########################################
2.7.0,flake8: noqa
2.7.0,-*- coding: utf-8 -*-
2.7.0,Assigning featurizer if not user defined
2.7.0,loading datasets
2.7.0,Assembling train and valid datasets
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,Building tensorflow MultitaskDNN model
2.7.0,Building tensorflow robust MultitaskDNN model
2.7.0,Building scikit logistic regression model
2.7.0,Transform fingerprints to IRV features
2.7.0,Building tensorflow IRV model
2.7.0,Building scikit random forest model
2.7.0,Building scikit learn Kernel SVM model
2.7.0,Building xgboost classification model
2.7.0,Remove token for paddings
2.7.0,Building scikit random forest model
2.7.0,Building scikit learn Kernel Ridge Regression model
2.7.0,Building scikit learn Kernel Ridge Regression model
2.7.0,Building xgboost regression model
2.7.0,Loading hyperparameters
2.7.0,num positive/negative ligands
2.7.0,Set batch sizes for network
2.7.0,Model structure
2.7.0,Traning settings
2.7.0,Fit trained model
2.7.0,Evaluating low data model
2.7.0,-*- coding: utf-8 -*-
2.7.0,Assigning featurizer if not user defined
2.7.0,loading datasets
2.7.0,
2.7.0,Note by @XericZephyr. Reason why I spun off this function:
2.7.0,1. Some model needs dataset information.
2.7.0,2. It offers us possibility to **cache** the dataset
2.7.0,"if the featurizer runs very slow, e.g., GraphConv."
2.7.0,2+. The cache can even happen at Travis CI to accelerate
2.7.0,CI testing.
2.7.0,
2.7.0,loading datasets
2.7.0,!/usr/bin/env python2
2.7.0,-*- coding: utf-8 -*-
2.7.0,"TODO For this dataset and model, the R2-scores are less than 0.3."
2.7.0,This has to be improved.
2.7.0,See: https://github.com/deepchem/deepchem/issues/2776
2.7.0,TODO: Check for this
2.7.0,Download files if they don't exist
2.7.0,Featurize the KINASE dataset
2.7.0,Shuffle the training data
2.7.0,Apply transformations
2.7.0,TIMING
2.7.0,transformers = [
2.7.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.7.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.7.0,dataset=train_dataset)]
2.7.0,Set shard size low to avoid memory problems.
2.7.0,TIMING
2.7.0,TIMING
2.7.0,Set some global variables up top
2.7.0,Featurize KAGGLE dataset
2.7.0,TIMING
2.7.0,TIMING
2.7.0,Build the path to the dataset on disk.
2.7.0,Try to reload cached datasets.
2.7.0,Create the dataset
2.7.0,Split and transform the dataset.
2.7.0,. clinical trial toxicity (or absence of toxicity)
2.7.0,. FDA approval status.
2.7.0,Download files if they don't exist
2.7.0,Featurizing datasets
2.7.0,Missing entry removal
2.7.0,Shuffle the training data
2.7.0,Apply transformations
2.7.0,TIMING
2.7.0,TODO: Check if anything needs to be added
2.7.0,Featurize the FACTORS dataset
2.7.0,Shuffle the training data
2.7.0,Apply transformations
2.7.0,TIMING
2.7.0,dict of accepted featurizers for this dataset
2.7.0,modify the returned dicts for your dataset
2.7.0,Names of supported featurizers
2.7.0,dict of accepted transformers
2.7.0,dict of accepted splitters
2.7.0,names of supported splitters
2.7.0,Warning message about this template
2.7.0,Featurize mydataset
2.7.0,Get DeepChem data directory if needed
2.7.0,Check for str args to featurizer and splitter
2.7.0,Reload from disk
2.7.0,First type of supported featurizers
2.7.0,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.7.0,Changer loader to match featurizer and data file type
2.7.0,Featurize dataset
2.7.0,Initialize transformers
2.7.0,"get pdb and sdf filenames, labels and pdbids"
2.7.0,load and featurize each complex
2.7.0,Extract locations of data
2.7.0,Extract labels
2.7.0,Lines have format
2.7.0,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.7.0,"The base-10 logarithm, -log kd/pk"
2.7.0,"def load_pcba_146(featurizer='ECFP',"
2.7.0,"split='random',"
2.7.0,"reload=True,"
2.7.0,"data_dir=None,"
2.7.0,"save_dir=None,"
2.7.0,**kwargs):
2.7.0,return load_pcba_dataset(
2.7.0,"featurizer=featurizer,"
2.7.0,"split=split,"
2.7.0,"reload=reload,"
2.7.0,"assay_file_name=""pcba_146.csv.gz"","
2.7.0,"data_dir=data_dir,"
2.7.0,"save_dir=save_dir,"
2.7.0,**kwargs)
2.7.0,"def load_pcba_2475(featurizer='ECFP',"
2.7.0,"split='random',"
2.7.0,"reload=True,"
2.7.0,"data_dir=None,"
2.7.0,"save_dir=None,"
2.7.0,**kwargs):
2.7.0,return load_pcba_dataset(
2.7.0,"featurizer=featurizer,"
2.7.0,"split=split,"
2.7.0,"reload=reload,"
2.7.0,"assay_file_name=""pcba_2475.csv.gz"","
2.7.0,"data_dir=data_dir,"
2.7.0,"save_dir=save_dir,"
2.7.0,**kwargs)
2.7.0,Range of optimization
2.7.0,We know from guard above that this is an int/float
2.7.0,Specify logfile
2.7.0,Make logdir if it doesn't exist.
2.7.0,setup range
2.7.0,Stores all results
2.7.0,Store all model references so we don't have to reload
2.7.0,Stores all model locations
2.7.0,"param values are always float in BO, so this line converts float to int"
2.7.0,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.7.0,Record hyperparameters
2.7.0,We have already evaluated the model for these hyperparameters.
2.7.0,Add it on to the information needed for the constructor
2.7.0,Not all models have nb_epoch
2.7.0,Some models autosave
2.7.0,Record performances
2.7.0,Store all results
2.7.0,Store reference to model
2.7.0,GPGO maximize performance by default
2.7.0,set performance to its negative value for minimization
2.7.0,Demarcating internal function for readability
2.7.0,execute GPGO
2.7.0,FIXME: Incompatible types in assignment
2.7.0,Let's fetch the model with the best parameters
2.7.0,Compare best model to default hyperparameters
2.7.0,Record hyperparameters
2.7.0,Return default hyperparameters
2.7.0,Construction dictionary mapping hyperparameter names to values
2.7.0,"mypy test throws error, so ignoring it in try"
2.7.0,Not all models have nb_epoch
2.7.0,Some models autosave
2.7.0,arbitrarily return last model
2.7.0,hyperparam_list should either be an Iterable sequence or a random sampler with rvs method
2.7.0,"mypy test throws error, so ignoring it in try"
2.7.0,Not all models have nb_epoch
2.7.0,Some models autosave
2.7.0,Update best validation score so far
2.7.0,"if `hyp_str` not in `all_scores`, store it in `all_scores`"
2.7.0,arbitrarily return last model trained
2.7.0,"If callable, sample it for a maximum n times"
2.7.0,flake8: noqa
2.7.0,"2 model variants, 1 results.txt file"
2.7.0,Generate dummy dataset
2.7.0,Generate dummy dataset
2.7.0,These are per-example multiplier
2.7.0,Test that 2 parameters were optimized
2.7.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.7.0,Generate dummy dataset
2.7.0,Define nb_epoch in hyperparam_search function call
2.7.0,"max_iter model variants, 1 results.txt file"
2.7.0,Generate dummy dataset
2.7.0,Generate dummy dataset
2.7.0,These are per-example multiplier
2.7.0,Test that 2 parameters were optimized
2.7.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.7.0,Generate dummy dataset
2.7.0,Define nb_epoch in hyperparam_search function call
2.7.0,Generate dummy dataset
2.7.0,Generate dummy dataset
2.7.0,These are per-example multiplier
2.7.0,Test that 2 parameters were optimized
2.7.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.7.0,Generate dummy dataset
2.7.0,Have the worker threads generate the rollouts for this iteration.
2.7.0,Perform optimization.
2.7.0,Build the inputs and run the optimizer.
2.7.0,Update the number of steps taken so far and perform checkpointing.
2.7.0,Merge all the rollouts into a single set of arrays.
2.7.0,Iterate slices.
2.7.0,Generate the rollout.
2.7.0,Compute an estimate of the reward for the rest of the episode.
2.7.0,Compute the discounted rewards and advantages.
2.7.0,Convert the actions to one-hot.
2.7.0,Rearrange the states into the proper set of arrays.
2.7.0,Return the processed arrays.
2.7.0,Training loop.
2.7.0,Do checkpointing.
2.7.0,Generate the rollout.
2.7.0,Compute an estimate of the reward for the rest of the episode.
2.7.0,Compute the discounted rewards and advantages.
2.7.0,"Record the actions, converting to one-hot if necessary."
2.7.0,Rearrange the states into the proper set of arrays.
2.7.0,Build the inputs and apply gradients.
2.7.0,Assume all arrays are float32.
2.7.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.7.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.7.0,"game).  The average reward for any bet is slightly negative, so the best"
2.7.0,strategy is to walk away.
2.7.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.0,Optimize it.
2.7.0,"It should have learned that the expected value is very close to zero, and that the best"
2.7.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.7.0,top actions).
2.7.0,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.7.0,get the same result.
2.7.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.0,The environment just has a constant state.
2.7.0,The policy includes a single recurrent layer.
2.7.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.7.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.7.0,"On the first call, the initial state should be all zeros."
2.7.0,It should still be zeros since we didn't save it last time.
2.7.0,It should be different now.
2.7.0,This should be the same as the previous one.
2.7.0,"Now we reset it, so we should get the same result as initially."
2.7.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.7.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.7.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.7.0,at all.  Using hindsight makes it much easier.
2.7.0,A simple policy with two hidden layers.
2.7.0,Optimize it.
2.7.0,Try running it a few times and see if it succeeds.
2.7.0,The state consists of two numbers: a current value and a target value.
2.7.0,The policy just needs to learn to output the target value (or at least
2.7.0,move toward it).
2.7.0,A simple policy with no hidden layers.
2.7.0,Optimize it.
2.7.0,Try running it and see if it reaches the target
2.7.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.7.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.7.0,"game).  The average reward for any bet is slightly negative, so the best"
2.7.0,strategy is to walk away.
2.7.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.0,Optimize it.
2.7.0,"It should have learned that the expected value is very close to zero, and that the best"
2.7.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.7.0,top actions).
2.7.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.7.0,get the same result.
2.7.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.7.0,The environment just has a constant state.
2.7.0,The policy includes a single recurrent layer.
2.7.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.7.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.7.0,"On the first call, the initial state should be all zeros."
2.7.0,It should still be zeros since we didn't save it last time.
2.7.0,It should be different now.
2.7.0,This should be the same as the previous one.
2.7.0,"Now we reset it, so we should get the same result as initially."
2.7.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.7.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.7.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.7.0,at all.  Using hindsight makes it much easier.
2.7.0,A simple policy with two hidden layers.
2.7.0,Optimize it.
2.7.0,Try running it a few times and see if it succeeds.
2.7.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.7.0,Randomize who goes first
2.7.0,Illegal move -- the square is not empty
2.7.0,Move X
2.7.0,Did X Win
2.7.0,Did O Win
2.7.0,"default channels are ""conda-forge"" and ""omnia"""
2.7.0,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.6.1,Build a nightly package by default.
2.6.1,Environment-specific dependencies.
2.6.1,get the version from deepchem/__init__.py
2.6.1,nightly version : .devYearMonthDayHourMinute
2.6.1,Force to add `.dev` if `--release` option isn't passed when building
2.6.1,!/usr/bin/env python3
2.6.1,-*- coding: utf-8 -*-
2.6.1,Datasets and models used in the benchmark test
2.6.1,"irv, rf, rf_regression should be assigned manually"
2.6.1,Evaluate performances with different training set fraction
2.6.1,Datasets and models used in the benchmark test
2.6.1,Uncomment the two lines below if hyper_parameters are provided
2.6.1,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.6.1,hyper_parameters = pickle.load(f)
2.6.1,!/usr/bin/env python3
2.6.1,-*- coding: utf-8 -*-
2.6.1,Datasets and models used in the benchmark test
2.6.1,Load Delaney dataset
2.6.1,Get Metric
2.6.1,Fit trained model
2.6.1,Fit trained model
2.6.1,Set numpy seed
2.6.1,##Load data###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Featurize Kinase dataset
2.6.1,##Load data###
2.6.1,num_trials = 5
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,Force matplotlib to not use any Xwindows backend.
2.6.1,##Load data###
2.6.1,the histogram of the data
2.6.1,Set numpy seed
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,num_trials = 5
2.6.1,Set some global variables up top
2.6.1,Fit trained model
2.6.1,Featurize PCBA dataset
2.6.1,Initialize transformers
2.6.1,Fit trained model
2.6.1,Load sider models now
2.6.1,Load sweetlead dataset now. Pass in dataset object and appropriate
2.6.1,transformers to predict functions
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,##Load data###
2.6.1,"n_estimators=100, max_features=int(num_features/3),"
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,Fit trained model
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Load tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,This example shows how to use Pandas to load data directly
2.6.1,without using a CSVLoader object. This may be useful if you
2.6.1,want the flexibility of processing your data with Pandas
2.6.1,directly.
2.6.1,Now let's convert from a dataset back to a pandas dataframe
2.6.1,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.6.1,Featurize FACTORS dataset
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,Force matplotlib to not use any Xwindows backend.
2.6.1,##Load data###
2.6.1,the histogram of the data
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Load QM7 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Fit trained model
2.6.1,Fit trained model
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Fit trained model
2.6.1,Load QM8 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Fit trained model
2.6.1,Set numpy seed
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,Load ChEMBL dataset
2.6.1,Fit models
2.6.1,Do setup required for tf/keras models
2.6.1,Number of features on conv-mols
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,DeepCrystal Technologies 2017 - Patrick Hop
2.6.1,MIT License - have fun!!
2.6.1,Set to higher values to get better numbers
2.6.1,======================================================================
2.6.1,"Run Benchmarks {GC-DNN, SVR, RF}"
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,Only for debug!
2.6.1,Load Delaney dataset
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Do setup required for tf/keras models
2.6.1,Number of features on conv-mols
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load Delaney dataset
2.6.1,Get Metric
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load Delaney dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load MUV dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Evaluate train/test scores
2.6.1,Load MUV data
2.6.1,Build model
2.6.1,Fit trained model
2.6.1,Evaluate train/test scores
2.6.1,Extract active site
2.6.1,Featurize ligand
2.6.1,Default for CircularFingerprint
2.6.1,Featurize pocket
2.6.1,Note broadcast operation
2.6.1,Compute labels for pockets
2.6.1,Some complexes have labels but no PDB files. Filter these manually
2.6.1,Some of the ligand-names are of form (FMN ox). Use regex
2.6.1,to merge into form (FMN-ox)
2.6.1,Filter if missing PDB files
2.6.1,Load PDBBind dataset
2.6.1,Define featurizers
2.6.1,Featurize Dataset
2.6.1,########################################################## DEBUG
2.6.1,########################################################## DEBUG
2.6.1,For stable runs
2.6.1,Fit trained model
2.6.1,4-fold splits
2.6.1,num positive/negative ligands
2.6.1,10 trials on test-set
2.6.1,Define metric
2.6.1,Get supports on test-set
2.6.1,Compute accuracies
2.6.1,Number of features on conv-mols
2.6.1,Batch size of models
2.6.1,graph_model = dc.nn.SequentialGraph(n_feat)
2.6.1,Fit trained model
2.6.1,Test model
2.6.1,Join information for all tasks.
2.6.1,4-fold splits
2.6.1,num positive/negative ligands
2.6.1,Define metric
2.6.1,Get supports on test-set
2.6.1,Compute accuracies
2.6.1,Number of features on conv-mols
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Test model
2.6.1,Join information for all tasks.
2.6.1,4-fold splits
2.6.1,num positive/negative ligands
2.6.1,10 trials on test-set
2.6.1,Define metric
2.6.1,Get supports on test-set
2.6.1,Compute accuracies
2.6.1,Number of features on conv-mols
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Test model
2.6.1,Join information for all tasks.
2.6.1,4-fold splits
2.6.1,num positive/negative ligands
2.6.1,10 trials on test-set
2.6.1,Define metric
2.6.1,Get supports on test-set
2.6.1,Compute accuracies
2.6.1,Train model on support
2.6.1,Test model
2.6.1,Join information for all tasks.
2.6.1,4-fold splits
2.6.1,num positive/negative ligands
2.6.1,10 trials on test-set
2.6.1,Define metric
2.6.1,Get supports on test-set
2.6.1,Compute accuracies
2.6.1,Train model on support
2.6.1,Test model
2.6.1,Join information for all tasks.
2.6.1,Set some global variables up top
2.6.1,Featurize Tox21 dataset
2.6.1,Initialize transformers
2.6.1,Set some global variables up top
2.6.1,Featurize Tox21 dataset
2.6.1,Initialize transformers
2.6.1,Load MUV dataset
2.6.1,Featurize MUV dataset
2.6.1,Initialize transformers
2.6.1,Load MUV dataset
2.6.1,Featurize MUV dataset
2.6.1,Initialize transformers
2.6.1,Featurize SIDER dataset
2.6.1,Initialize transformers
2.6.1,Featurize SIDER dataset
2.6.1,Initialize transformers
2.6.1,Load the data.
2.6.1,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.6.1,sparse: most tasks do not include data for most molecules.  It also is very
2.6.1,"unbalanced: there are many more negatives than positives.  For each task,"
2.6.1,create a list of alternating positives and negatives so each batch will have
2.6.1,equal numbers of both.
2.6.1,Define a MetaLearner describing the learning problem.
2.6.1,Run meta-learning on 80% of the tasks.
2.6.1,Validate on the remaining tasks.
2.6.1,4-fold splits
2.6.1,10 positive/negative ligands
2.6.1,10 trials on test-set
2.6.1,Sample supports without replacement (all pos/neg should be different)
2.6.1,Define metric
2.6.1,Get supports on test-set
2.6.1,Compute accuracies
2.6.1,Train model on support
2.6.1,Test model
2.6.1,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.6.1,Join information for all tasks.
2.6.1,4-fold splits
2.6.1,num positive/negative ligands
2.6.1,Define metric
2.6.1,Get supports on test-set
2.6.1,Compute accuracies
2.6.1,Train model on support
2.6.1,Test model
2.6.1,Join information for all tasks.
2.6.1,replace with your own scratch directory
2.6.1,Number of conformations in each file increases exponentially.
2.6.1,Start with a smaller dataset before continuing. Use all of them
2.6.1,for production
2.6.1,"'ani_gdb_s03.h5',"
2.6.1,"'ani_gdb_s04.h5',"
2.6.1,"'ani_gdb_s05.h5',"
2.6.1,"'ani_gdb_s06.h5',"
2.6.1,"'ani_gdb_s07.h5',"
2.6.1,'ani_gdb_s08.h5'
2.6.1,Extract the data
2.6.1,Print the data
2.6.1,self-interaction energies taken from
2.6.1,https://github.com/isayev/ANI1_dataset README
2.6.1,flush once more at the end
2.6.1,"# For production, set nb_epoch to 100+"
2.6.1,"print(""Train scores"")"
2.6.1,print(train_scores)
2.6.1,"print(""Minimization of a single test set structure:"")"
2.6.1,"print(model.minimize_structure(coords, atomic_nums))"
2.6.1,Written by Roman Zubatyuk and Justin S. Smith
2.6.1,Modified by Yutong Zhao to make python2 compatible
2.6.1,opening file
2.6.1,print(store_loc)
2.6.1,print(type(v[0]))
2.6.1,print(k)
2.6.1,print(path)
2.6.1,Number of conformations in each file increases exponentially.
2.6.1,Start with a smaller dataset before continuing. Use all of them
2.6.1,for production
2.6.1,Extract the data
2.6.1,NOTE THE RENAMING:
2.6.1,Note sensitivity = recall
2.6.1,Load nci dataset
2.6.1,Featurize nci dataset
2.6.1,Initialize transformers
2.6.1,Set some global variables up top
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load hiv dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load hiv dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load delaney dataset
2.6.1,Fit models
2.6.1,Load delaney dataset
2.6.1,Fit models
2.6.1,Fit models
2.6.1,Load delaney dataset
2.6.1,Fit models
2.6.1,TODO: Once improved splitting API is merged in swap to simpler API
2.6.1,The return values are dc.data.Dataset objects so we need to extract
2.6.1,the ids
2.6.1,TODO once improved splitting API is merged in swap out for simpler
2.6.1,API
2.6.1,The return values are dc.data.Dataset objects so we need to extract
2.6.1,the ids
2.6.1,Fit trained model
2.6.1,Load SIDER dataset
2.6.1,Featurize SIDER dataset
2.6.1,Initialize transformers
2.6.1,Featurize permeability dataset
2.6.1,Load Tox21 dataset
2.6.1,Fit trained model
2.6.1,TODO: This should be swapped for simpler splitter API once that's merged in.
2.6.1,The return values are dc.data.Dataset objects so we need to extract
2.6.1,the ids
2.6.1,Only for debug!
2.6.1,Load clintox dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load clintox dataset
2.6.1,Fit models
2.6.1,Do setup required for tf/keras models
2.6.1,Number of features on conv-mols
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,-*- coding: utf-8 -*-
2.6.1,#############################################################################
2.6.1,## save dataset
2.6.1,#############################################################################
2.6.1,## load datasets
2.6.1,load sweetfda
2.6.1,load aact
2.6.1,## fixup smiles for matching
2.6.1,return smiles
2.6.1,map original smiles to converted smiles
2.6.1,"## join dataframes, index on smiles"
2.6.1,map original smiles back
2.6.1,## fill all nan with 0
2.6.1,## construct datasets
2.6.1,store in new datasets
2.6.1,## save datasets
2.6.1,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.6.1,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.6.1,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.6.1,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.6.1,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.6.1,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.6.1,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.6.1,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.6.1,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.6.1,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.6.1,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.6.1,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.6.1,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.6.1,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.6.1,For stable runs
2.6.1,Fit trained model
2.6.1,For stable runs
2.6.1,Fit trained model
2.6.1,For stable runs
2.6.1,Fit trained model
2.6.1,TODO The below line should be fixes
2.6.1,See: https://github.com/deepchem/deepchem/issues/2373
2.6.1,model.save()
2.6.1,transformers = [
2.6.1,"dc.trans.LogTransformer(transform_X=True),"
2.6.1,"dc.trans.NormalizationTransformer(transform_y=True,"
2.6.1,dataset=train_dataset)]
2.6.1,Featurize UV dataset
2.6.1,##Load data###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Set numpy seed
2.6.1,##Load data###
2.6.1,##Create model###
2.6.1,Use R2 classification metric
2.6.1,Only use for final evaluation
2.6.1,Force matplotlib to not use any Xwindows backend.
2.6.1,##Load data###
2.6.1,the histogram of the data
2.6.1,##Load data###
2.6.1,###################################################### DEBUG
2.6.1,###################################################### DEBUG
2.6.1,Load HOPV dataset
2.6.1,Fit models
2.6.1,Number of features on conv-mols
2.6.1,Batch size of models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load HOPV dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load HOPV dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load HOPV dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Only for debug!
2.6.1,Load HOPV dataset
2.6.1,Fit models
2.6.1,Fit trained model
2.6.1,Load TOXCAST dataset
2.6.1,Featurize TOXCAST dataset
2.6.1,Initialize transformers
2.6.1,Fit trained model
2.6.1,Processing of ToxCast data
2.6.1,Author - Aneesh Pappu
2.6.1,Loading dataframes and editing indices
2.6.1,Loop through rows of hitc matrix and replace codes with smiles strings
2.6.1,get corresponding casn
2.6.1,get corresponding smiles
2.6.1,write to cell
2.6.1,Tidy up and write to csv
2.6.1,TODO(rbharath): Check that this operation is differentiable.
2.6.1,The number of cells which we should theoretically have
2.6.1,The number of cells which we should theoretically have
2.6.1,"Each atom neighbors tensor should be (k, ndim) shaped."
2.6.1,The number of cells which we should theoretically have
2.6.1,TODO(rbharath): The test below only checks that shapes work out.
2.6.1,Need to do a correctness implementation vs. a simple CPU impl.
2.6.1,The number of cells which we should theoretically have
2.6.1,TODO(rbharath): The test below only checks that shapes work out.
2.6.1,Need to do a correctness implementation vs. a simple CPU impl.
2.6.1,The number of cells which we should theoretically have
2.6.1,TODO(rbharath): The test below only checks that shapes work out.
2.6.1,Need to do a correctness implementation vs. a simple CPU impl.
2.6.1,TODO(rbharath): Commenting this out due to weird segfaults
2.6.1,def test_vina_generate_conformers(self):
2.6.1,"""""""Test that Vina Model can generate conformers"""""""
2.6.1,data_dir = os.path.dirname(os.path.realpath(__file__))
2.6.1,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.6.1,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.6.1,max_protein_atoms = 3500
2.6.1,max_ligand_atoms = 100
2.6.1,"print(""Loading protein file"")"
2.6.1,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.6.1,protein_Z = pad_array(
2.6.1,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.6.1,max_protein_atoms)
2.6.1,"print(""Loading ligand file"")"
2.6.1,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.6.1,ligand_Z = pad_array(
2.6.1,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.6.1,max_ligand_atoms)
2.6.1,Associate each atom with cell it belongs to. O(N*n_cells)
2.6.1,"Shape (n_cells, k)"
2.6.1,"Shape (N, 1)"
2.6.1,Associate each cell with its neighbor cells. Assumes periodic boundary
2.6.1,"conditions, so does wrapround. O(constant)"
2.6.1,"Shape (n_cells, 26)"
2.6.1,"Shape (N, 26)"
2.6.1,"coords of shape (N, ndim)"
2.6.1,"Shape (N, 26, k, ndim)"
2.6.1,"Shape (N, 26, k)"
2.6.1,"Shape (N, 26, k)"
2.6.1,"Shape (N, 26, k, ndim)"
2.6.1,"For smaller systems especially, the periodic boundary conditions can"
2.6.1,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.6.1,make sure duplicate neighbors are ignored?
2.6.1,TODO(rbharath): How does distance need to be modified here to
2.6.1,account for periodic boundary conditions?
2.6.1,"Shape (N, 26, k)"
2.6.1,"Shape (N, 26*k)"
2.6.1,TODO(rbharath): This will cause an issue with duplicates!
2.6.1,"Shape (N, M)"
2.6.1,"N elts of size (M,) each"
2.6.1,"Shape (N, 26*k)"
2.6.1,"N elts of size (26*k,) each"
2.6.1,"N elts of size (M,) each"
2.6.1,"Shape (N, M)"
2.6.1,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.6.1,"N tensors of shape (n_cells, 1)"
2.6.1,"Shape (N*n_cells, 1) after tile"
2.6.1,"List of N tensors of shape (n_cells, 1)"
2.6.1,Lists of length N
2.6.1,Lists of length n_cells
2.6.1,Get indices of k atoms closest to each cell point
2.6.1,TODO(rbharath): tf.stack for tf 1.0
2.6.1,"Tensor of shape (n_cells, k, ndim)"
2.6.1,atoms_in_cells = tf.stack(atoms_in_cells)
2.6.1,"Tensor of shape (26, k, ndim)"
2.6.1,"Reshape to (26*k, ndim)"
2.6.1,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.6.1,"Dists of shape (26*k, 1)"
2.6.1,"Of shape (k, ndim)"
2.6.1,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.6.1,TODO(rbharath): Change this for tf 1.0
2.6.1,"n_cells tensors of shape (N, 1)"
2.6.1,"Shape (N*n_cells, 1) after tile"
2.6.1,"List of n_cells tensors of shape (N, 1)"
2.6.1,Lists of length n_cells
2.6.1,Lists of length n_cells
2.6.1,Get indices of k atoms closest to each cell point
2.6.1,"n_cells tensors of shape (k, ndim)"
2.6.1,"Tensor of shape (n_cells, k)"
2.6.1,TODO(rbharath):
2.6.1,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.6.1,- Need to group closest atoms amongst cell neighbors
2.6.1,- Need to do another top_k to find indices of closest neighbors.
2.6.1,- Return N lists corresponding to neighbors for every atom.
2.6.1,TODO(rbharath): Do we need to handle periodic boundary conditions
2.6.1,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.6.1,"looking for 26 neighbors, which isn't right for boundary cells in"
2.6.1,the cube.
2.6.1,Number of neighbors of central cube in 3-space is
2.6.1,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.6.1,TODO(rbharath)
2.6.1,n_cells = int(cells.get_shape()[0])
2.6.1,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.6.1,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.6.1,"Tile (a, a, a, b, b, b, etc.)"
2.6.1,"Tile (a, b, c, a, b, c, ...)"
2.6.1,"Lists of n_cells tensors of shape (N, 1)"
2.6.1,Lists of length n_cells
2.6.1,Lists of length n_cells
2.6.1,Get indices of k atoms closest to each cell point
2.6.1,"n_cells tensors of shape (26,)"
2.6.1,TODO(rbharath): Make this handle minibatches
2.6.1,"Shape (N_protein+N_ligand, 3)"
2.6.1,"Shape (N_protein+N_ligand,)"
2.6.1,"Shape (N_protein+N_ligand, M)"
2.6.1,"Shape (N_protein+N_ligand,)"
2.6.1,"Shape (N_protein+N_ligand, 3)"
2.6.1,"Shape (N_protein+N_ligand,)"
2.6.1,"Shape (N_protein+N_ligand, M)"
2.6.1,"Shape (N_protein+N_ligand, M, 3)"
2.6.1,"Shape (N_protein+N_ligand, M)"
2.6.1,"Shape (N_protein+N_ligand, M, 3)"
2.6.1,"Shape (N_protein+N_ligand, M)"
2.6.1,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.6.1,"Shape (N_protein+N_ligand, M)"
2.6.1,"Shape (N_protein+N_ligand, M)"
2.6.1,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.6.1,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.6.1,computing free-energy. This implementation currently uses all interaction
2.6.1,terms. Not sure if this makes a difference.
2.6.1,"Shape (N_protein+N_ligand, M)"
2.6.1,Shape () -- scalar
2.6.1,Keep track of the layers
2.6.1,"For graphical layers, add connectivity placeholders"
2.6.1,Add layer to the layer list
2.6.1,Keep track of the layers
2.6.1,Create graph topology and x
2.6.1,Keep track of the layers
2.6.1,Whether or not we have used the GraphGather layer yet
2.6.1,Update new value of x
2.6.1,Update new value of x
2.6.1,Update new value of x
2.6.1,Get train function
2.6.1,Initialize
2.6.1,################################################################### DEBUG
2.6.1,self.test_label_placeholder = Input(
2.6.1,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.6.1,"name=""label_placeholder""))"
2.6.1,self.test_weight_placeholder = Input(
2.6.1,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.6.1,"name=""weight_placeholder""))"
2.6.1,TODO(rbharath): Should weights for the support be used?
2.6.1,Support labels
2.6.1,self.support_label_placeholder = Input(
2.6.1,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.6.1,"name=""support_label_placeholder""))"
2.6.1,################################################################### DEBUG
2.6.1,Generate dictionary elements for support
2.6.1,Get graph information for test
2.6.1,Generate dictionary elements for test
2.6.1,Perform the optimization
2.6.1,Create different support sets
2.6.1,Get batch to try it out on
2.6.1,"Train on support set, batch pair"
2.6.1,Get featurization for test
2.6.1,"Shape (n_test, n_feat)"
2.6.1,Get featurization for support
2.6.1,"Shape (n_support, n_feat)"
2.6.1,Computes the inner part c() of the kernel
2.6.1,(the inset equation in section 2.1.1 of Matching networks paper).
2.6.1,Normalize
2.6.1,TODO(rbharath): euclidean kernel is broken!
2.6.1,elif self.similarity == 'euclidean':
2.6.1,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.6.1,"Note that gram matrix g has shape (n_test, n_support)"
2.6.1,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.6.1,https://arxiv.org/pdf/1606.04080v1.pdf
2.6.1,"Computes softmax across axis 1, (so sums distances to support set for"
2.6.1,each test entry) to get attention vector
2.6.1,"Shape (n_test, n_support)"
2.6.1,Weighted sum of support labels
2.6.1,"Shape (n_support, 1)"
2.6.1,pred is yhat in eqn (1) of Matching Networks.
2.6.1,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.6.1,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.6.1,"Shape (n_test,)"
2.6.1,Convert to logit space using inverse sigmoid (logit) function
2.6.1,logit function: log(pred) - log(1-pred)
2.6.1,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.6.1,in Cross Entropy calculation.
2.6.1,"Shape (n_test,)"
2.6.1,Get scores
2.6.1,Remove padded elements
2.6.1,Get scores
2.6.1,pred corresponds to prob(example == 1)
2.6.1,Remove padded elements
2.6.1,Get batches
2.6.1,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.6.1,multitask case with missing data...
2.6.1,Join information for all tasks.
2.6.1,TODO(rbharath): Find a way to get rid of this import?
2.6.1,Extract model info
2.6.1,Get graph topology for x
2.6.1,Building outputs
2.6.1,Set epsilon
2.6.1,Initialize
2.6.1,"Path to save checkpoint files, which matches the"
2.6.1,replicated supervisor's default path.
2.6.1,Create target inputs
2.6.1,Get train function
2.6.1,TODO(rbharath): I believe this is total amount of data
2.6.1,Get graph information
2.6.1,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.6.1,the number of labeled data points in target_i. This is to normalize each task
2.6.1,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.6.1,Get other optimizer information
2.6.1,TODO(rbharath): Figure out how to handle phase appropriately
2.6.1,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.6.1,"tensors of shape (batch_size,)"
2.6.1,It's ok to divide by just the batch_size rather than the number of nonzero
2.6.1,examples (effect averages out)
2.6.1,Perform the optimization
2.6.1,TODO(rbharath): Disabling saving for now to try to debug.
2.6.1,run eval data through the model
2.6.1,"Shape (n_samples, n_tasks)"
2.6.1,Create target inputs
2.6.1,TODO(rbharath): Find a way to get rid of this import?
2.6.1,Obtain appropriate loss function
2.6.1,Extract model info
2.6.1,Get graph topology for x
2.6.1,Raw logit outputs
2.6.1,Set epsilon
2.6.1,Initialize
2.6.1,"Path to save checkpoint files, which matches the"
2.6.1,replicated supervisor's default path.
2.6.1,Create target inputs
2.6.1,############################################################### DEBUG
2.6.1,"print(""multitask classifier"")"
2.6.1,"print(""feat"")"
2.6.1,print(feat)
2.6.1,############################################################### DEBUG
2.6.1,Get train function
2.6.1,TODO(rbharath): I believe this is total amount of data
2.6.1,Get graph information
2.6.1,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.6.1,the number of labeled data points in target_i. This is to normalize each task
2.6.1,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.6.1,Get other optimizer information
2.6.1,TODO(rbharath): Figure out how to handle phase appropriately
2.6.1,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.6.1,"tensors of shape (batch_size,)"
2.6.1,Convert the labels into one-hot vector encodings.
2.6.1,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.6.1,un-softmaxed logits rather than softmax outputs.
2.6.1,It's ok to divide by just the batch_size rather than the number of nonzero
2.6.1,examples (effect averages out)
2.6.1,Perform the optimization
2.6.1,TODO(rbharath): Disabling saving for now to try to debug.
2.6.1,run eval data through the model
2.6.1,"Shape (n_samples, n_tasks)"
2.6.1,run eval data through the model
2.6.1,self.n_atoms = n_atoms
2.6.1,Define the list of tensors to be used as topology
2.6.1,Merge mol conv objects
2.6.1,Generate dicts
2.6.1,Define the list of tensors to be used as topology
2.6.1,Extract atom numbers
2.6.1,Generate dicts
2.6.1,molecule * atom(graph) => step => features
2.6.1,molecule * atom(graph) => step
2.6.1,molecule * atom(graph) => step
2.6.1,Define the list of tensors to be used as topology
2.6.1,calculation orders for a batch of molecules
2.6.1,padding atom features vector of each molecule with 0
2.6.1,self.n_atoms = n_atoms
2.6.1,Define the list of tensors to be used as topology
2.6.1,Extract atom numbers
2.6.1,Generate dicts
2.6.1,self.n_atoms = n_atoms
2.6.1,Define the list of tensors to be used as topology
2.6.1,Extract atom numbers
2.6.1,number of atoms in each molecule
2.6.1,index of pair features
2.6.1,number of pairs for each atom
2.6.1,atom features
2.6.1,pair features
2.6.1,Generate dicts
2.6.1,Load Tox21 dataset
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.6.1,Fit trained model
2.6.1,Fit models
2.6.1,Batch size of models
2.6.1,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.6.1,Fit trained model
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,number positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply an attention lstm layer
2.6.1,Number of folds for split
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,number positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply an attention lstm layer
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,number positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply an attention lstm layer
2.6.1,Number of folds for split
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Number of folds for split
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply a residual lstm layer
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply a residual lstm layer
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply a residual lstm layer
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply a residual lstm layer
2.6.1,Number of folds for split
2.6.1,Depth of attention module
2.6.1,number positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,Apply an attention lstm layer
2.6.1,Number of folds for split
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Number of features on conv-mols
2.6.1,Define metric
2.6.1,Train support model on train
2.6.1,Add layers
2.6.1,# Gather Projection
2.6.1,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.6.1,There should be 8 layers in graph_model
2.6.1,assert len(graph_model.layers) == 6
2.6.1,Add layers
2.6.1,Need to add batch-norm separately to test/support due to differing
2.6.1,shapes.
2.6.1,Apply an attention lstm layer
2.6.1,Gather Projection
2.6.1,Add layers
2.6.1,Need to add batch-norm separately to test/support due to differing
2.6.1,shapes.
2.6.1,Apply an attention lstm layer
2.6.1,Gather Projection
2.6.1,Degrees from 1 to max_deg inclusive
2.6.1,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.6.1,"Should have shape (?, deg)"
2.6.1,"Shape of atom_features should be (?, n_feat)"
2.6.1,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.6.1,-*- coding: utf-8 -*-
2.6.1,Save hyperparameters
2.6.1,-*- coding: utf-8 -*-
2.6.1,Save hyperparameters
2.6.1,setup optimizer
2.6.1,setup optimizer
2.6.1,"print(""tasK: %d"" %task)"
2.6.1,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.6.1,"print(""scores"")"
2.6.1,print(scores.size())
2.6.1,"print(""task_label"")"
2.6.1,print(task_label.size())
2.6.1,"task_loss =  self.criterion(scores, task_label)"
2.6.1,"print(""task_loss"")"
2.6.1,print(task_loss.size())
2.6.1,-*- coding: utf-8 -*-
2.6.1,Save hyperparameters
2.6.1,weight decay
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,Turns out there are valid cases where we don't want pad-batches
2.6.1,on by default.
2.6.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.1,Run training op.
2.6.1,############################################################# TIMING
2.6.1,The iterbatches does padding with zero-weight examples on the last batch.
2.6.1,Remove padded examples.
2.6.1,Special case to handle singletasks.
2.6.1,The iterbatches does padding with zero-weight examples on the last batch.
2.6.1,Remove padded examples.
2.6.1,References
2.6.1,Arguments
2.6.1,Aliases.
2.6.1,Aliases.
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,TODO(rbharath): This class does not yet have a
2.6.1,"TensorGraph equivalent, but one may not be required."
2.6.1,"Commented out for now, remove if OK."
2.6.1,class AlternateWeaveLayer(WeaveLayer):
2.6.1,""""""" Alternate implementation of weave module"
2.6.1,"same variables, different graph structures"
2.6.1,""""""""
2.6.1,
2.6.1,"def call(self, x, mask=None):"
2.6.1,"""""""Execute this layer on input tensors."
2.6.1,
2.6.1,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.6.1,
2.6.1,Parameters
2.6.1,----------
2.6.1,x: list
2.6.1,list of Tensors of form described above.
2.6.1,"mask: bool, optional"
2.6.1,Ignored. Present only to shadow superclass call() method.
2.6.1,
2.6.1,Returns
2.6.1,-------
2.6.1,A: Tensor
2.6.1,Tensor of atom_features
2.6.1,P: Tensor
2.6.1,Tensor of pair_features
2.6.1,""""""""
2.6.1,# Add trainable weights
2.6.1,self.build()
2.6.1,
2.6.1,atom_features = x[0]
2.6.1,pair_features = x[1]
2.6.1,
2.6.1,pair_split = x[2]
2.6.1,atom_to_pair = x[4]
2.6.1,
2.6.1,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.6.1,AA = self.activation(AA)
2.6.1,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.6.1,PA = self.activation(PA)
2.6.1,"PA = tf.segment_sum(PA, pair_split)"
2.6.1,
2.6.1,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.6.1,A = self.activation(A)
2.6.1,
2.6.1,if self.update_pair:
2.6.1,AP_ij = tf.matmul(
2.6.1,tf.reshape(
2.6.1,"tf.gather(atom_features, atom_to_pair),"
2.6.1,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.6.1,AP_ij = self.activation(AP_ij)
2.6.1,AP_ji = tf.matmul(
2.6.1,tf.reshape(
2.6.1,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.6.1,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.6.1,AP_ji = self.activation(AP_ji)
2.6.1,
2.6.1,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.6.1,PP = self.activation(PP)
2.6.1,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.6.1,P = self.activation(P)
2.6.1,else:
2.6.1,P = pair_features
2.6.1,
2.6.1,"return A, P"
2.6.1,TODO(rbharath): This class does not yet have a
2.6.1,"TensorGraph equivalent, but one may not be required."
2.6.1,"Commented out for now, remove if OK."
2.6.1,class WeaveConcat(Layer):
2.6.1,""""""""" Concat a batch of molecules into a batch of atoms"
2.6.1,""""""""
2.6.1,
2.6.1,"def __init__(self,"
2.6.1,"batch_size,"
2.6.1,"n_atom_input_feat=50,"
2.6.1,"n_output=128,"
2.6.1,"init='glorot_uniform',"
2.6.1,"activation='tanh',"
2.6.1,**kwargs):
2.6.1,""""""""
2.6.1,Parameters
2.6.1,----------
2.6.1,batch_size: int
2.6.1,number of molecules in a batch
2.6.1,"n_atom_input_feat: int, optional"
2.6.1,Number of features for each atom in input.
2.6.1,"n_output: int, optional"
2.6.1,Number of output features for each atom(concatenated)
2.6.1,"init: str, optional"
2.6.1,Weight initialization for filters.
2.6.1,"activation: str, optional"
2.6.1,Activation function applied
2.6.1,
2.6.1,""""""""
2.6.1,self.batch_size = batch_size
2.6.1,self.n_atom_input_feat = n_atom_input_feat
2.6.1,self.n_output = n_output
2.6.1,self.init = initializations.get(init)  # Set weight initialization
2.6.1,self.activation = activations.get(activation)  # Get activations
2.6.1,"super(WeaveConcat, self).__init__(**kwargs)"
2.6.1,
2.6.1,def build(self):
2.6.1,"""""""""Construct internal trainable weights."
2.6.1,""""""""
2.6.1,
2.6.1,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.6.1,self.b = model_ops.zeros(shape=[
2.6.1,"self.n_output,"
2.6.1,])
2.6.1,
2.6.1,self.trainable_weights = self.W + self.b
2.6.1,
2.6.1,"def call(self, x, mask=None):"
2.6.1,"""""""Execute this layer on input tensors."
2.6.1,
2.6.1,"x = [atom_features, atom_mask]"
2.6.1,
2.6.1,Parameters
2.6.1,----------
2.6.1,x: list
2.6.1,Tensors as listed above
2.6.1,"mask: bool, optional"
2.6.1,Ignored. Present only to shadow superclass call() method.
2.6.1,
2.6.1,Returns
2.6.1,-------
2.6.1,outputs: Tensor
2.6.1,Tensor of concatenated atom features
2.6.1,""""""""
2.6.1,self.build()
2.6.1,atom_features = x[0]
2.6.1,atom_masks = x[1]
2.6.1,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.6.1,A_mask = tf.split(
2.6.1,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.6.1,outputs = tf.concat(
2.6.1,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.6.1,"outputs = tf.matmul(outputs, self.W) + self.b"
2.6.1,outputs = self.activation(outputs)
2.6.1,return outputs
2.6.1,TODO(rbharath): This class does not yet have a
2.6.1,"TensorGraph equivalent, but one may not be required."
2.6.1,"Commented out for now, remove if OK."
2.6.1,class AlternateWeaveGather(WeaveGather):
2.6.1,"""""""Alternate implementation of weave gather layer"
2.6.1,corresponding to AlternateWeaveLayer
2.6.1,""""""""
2.6.1,
2.6.1,"def call(self, x, mask=None):"
2.6.1,"""""""Execute this layer on input tensors."
2.6.1,
2.6.1,"x = [atom_features, atom_split]"
2.6.1,
2.6.1,Parameters
2.6.1,----------
2.6.1,x: list
2.6.1,Tensors as listed above
2.6.1,"mask: bool, optional"
2.6.1,Ignored. Present only to shadow superclass call() method.
2.6.1,
2.6.1,Returns
2.6.1,-------
2.6.1,outputs: Tensor
2.6.1,Tensor of molecular features
2.6.1,""""""""
2.6.1,# Add trainable weights
2.6.1,self.build()
2.6.1,outputs = x[0]
2.6.1,atom_split = x[1]
2.6.1,
2.6.1,if self.gaussian_expand:
2.6.1,outputs = self.gaussian_histogram(outputs)
2.6.1,
2.6.1,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.6.1,
2.6.1,if self.gaussian_expand:
2.6.1,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.6.1,output_molecules = self.activation(output_molecules)
2.6.1,return output_molecules
2.6.1,Each directory holds a range of assay results
2.6.1,Just write NA
2.6.1,"Now, write out the results csv, going line by line through all molecule results"
2.6.1,printing the mol_id
2.6.1,printing the SMILES
2.6.1,Now gzip it
2.6.1,Now remove the intermediate csv
2.6.1,First download all SDF files. We need these to get smiles
2.6.1,Next download all Bioassays
2.6.1,RDKit consistently hangs when trying to read this file
2.6.1,TODO (LESWING) Lazy Load
2.6.1,TODO (LESWING) Lazy Load
2.6.1,from simdna import simulations
2.6.1,define layer out functions
2.6.1,get layer outputs for a positive simulation example
2.6.1,plot layer outputs
2.6.1,highlight motif sites
2.6.1,get a positive and a negative example from the simulation data
2.6.1,"get motif scores, ISM scores, and DeepLIFT scores"
2.6.1,get motif site locations
2.6.1,organize legends
2.6.1,plot scores and highlight motif site locations
2.6.1,initialize fwd and reverse scores to -infinity
2.6.1,"cross-correlate separately for each base,"
2.6.1,for both the PSSM and its reverse complement
2.6.1,sum over the bases
2.6.1,take max of fwd and reverse scores at each position
2.6.1,return 1D view of sequence characters
2.6.1,class SequenceDNN(Model):
2.6.1,""""""""
2.6.1,Sequence DNN models.
2.6.1,
2.6.1,Parameters
2.6.1,----------
2.6.1,"seq_length : int, optional"
2.6.1,length of input sequence.
2.6.1,"keras_model : instance of keras.models.Sequential, optional"
2.6.1,seq_length or keras_model must be specified.
2.6.1,"num_tasks : int, optional"
2.6.1,number of tasks. Default: 1.
2.6.1,num_filters : list[int] | tuple[int]
2.6.1,"number of convolutional filters in each layer. Default: (15,)."
2.6.1,conv_width : list[int] | tuple[int]
2.6.1,"width of each layer's convolutional filters. Default: (15,)."
2.6.1,pool_width : int
2.6.1,width of max pooling after the last layer. Default: 35.
2.6.1,L1 : float
2.6.1,strength of L1 penalty.
2.6.1,dropout : float
2.6.1,dropout probability in every convolutional layer. Default: 0.
2.6.1,verbose: int
2.6.1,"Verbosity level during training. Valida values: 0, 1, 2."
2.6.1,
2.6.1,Returns
2.6.1,-------
2.6.1,Compiled DNN model.
2.6.1,""""""""
2.6.1,
2.6.1,"def __init__(self,"
2.6.1,"seq_length=None,"
2.6.1,"keras_model=None,"
2.6.1,"use_RNN=False,"
2.6.1,"num_tasks=1,"
2.6.1,"num_filters=(15, 15, 15),"
2.6.1,"conv_width=(15, 15, 15),"
2.6.1,"pool_width=35,"
2.6.1,"GRU_size=35,"
2.6.1,"TDD_size=15,"
2.6.1,"L1=0,"
2.6.1,"dropout=0.0,"
2.6.1,"num_epochs=100,"
2.6.1,verbose=1):
2.6.1,self.num_tasks = num_tasks
2.6.1,self.num_epochs = num_epochs
2.6.1,self.verbose = verbose
2.6.1,self.train_metrics = []
2.6.1,self.valid_metrics = []
2.6.1,if keras_model is not None and seq_length is None:
2.6.1,self.model = keras_model
2.6.1,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.6.1,elif seq_length is not None and keras_model is None:
2.6.1,self.model = Sequential()
2.6.1,assert len(num_filters) == len(conv_width)
2.6.1,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.6.1,conv_height = 4 if i == 0 else 1
2.6.1,self.model.add(
2.6.1,Convolution2D(
2.6.1,"nb_filter=nb_filter,"
2.6.1,"nb_row=conv_height,"
2.6.1,"nb_col=nb_col,"
2.6.1,"activation='linear',"
2.6.1,"init='he_normal',"
2.6.1,"input_shape=(1, 4, seq_length),"
2.6.1,"W_regularizer=l1(L1),"
2.6.1,b_regularizer=l1(L1)))
2.6.1,self.model.add(Activation('relu'))
2.6.1,self.model.add(Dropout(dropout))
2.6.1,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.6.1,if use_RNN:
2.6.1,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.6.1,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.6.1,"self.model.add(Permute((2, 1)))"
2.6.1,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.6.1,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.6.1,self.model.add(Flatten())
2.6.1,self.model.add(Dense(output_dim=self.num_tasks))
2.6.1,self.model.add(Activation('sigmoid'))
2.6.1,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.6.1,else:
2.6.1,raise ValueError(
2.6.1,"""Exactly one of seq_length or keras_model must be specified!"")"
2.6.1,
2.6.1,"def train(self,"
2.6.1,"X,"
2.6.1,"y,"
2.6.1,"validation_data,"
2.6.1,"early_stopping_metric='Loss',"
2.6.1,"early_stopping_patience=5,"
2.6.1,save_best_model_to_prefix=None):
2.6.1,if y.dtype != bool:
2.6.1,"assert set(np.unique(y)) == {0, 1}"
2.6.1,y = y.astype(bool)
2.6.1,multitask = y.shape[1] > 1
2.6.1,if not multitask:
2.6.1,num_positives = y.sum()
2.6.1,num_sequences = len(y)
2.6.1,num_negatives = num_sequences - num_positives
2.6.1,if self.verbose >= 1:
2.6.1,print('Training model (* indicates new best result)...')
2.6.1,"X_valid, y_valid = validation_data"
2.6.1,early_stopping_wait = 0
2.6.1,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.6.1,"for epoch in range(1, self.num_epochs + 1):"
2.6.1,self.model.fit(
2.6.1,"X,"
2.6.1,"y,"
2.6.1,"batch_size=128,"
2.6.1,"nb_epoch=1,"
2.6.1,class_weight={
2.6.1,"True: num_sequences / num_positives,"
2.6.1,False: num_sequences / num_negatives
2.6.1,"} if not multitask else None,"
2.6.1,verbose=self.verbose >= 2)
2.6.1,"epoch_train_metrics = self.test(X, y)"
2.6.1,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.6.1,self.train_metrics.append(epoch_train_metrics)
2.6.1,self.valid_metrics.append(epoch_valid_metrics)
2.6.1,if self.verbose >= 1:
2.6.1,print('Epoch {}:'.format(epoch))
2.6.1,print('Train {}'.format(epoch_train_metrics))
2.6.1,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.6.1,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.6.1,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.6.1,if self.verbose >= 1:
2.6.1,print(' *')
2.6.1,best_metric = current_metric
2.6.1,best_epoch = epoch
2.6.1,early_stopping_wait = 0
2.6.1,if save_best_model_to_prefix is not None:
2.6.1,self.save(save_best_model_to_prefix)
2.6.1,else:
2.6.1,if self.verbose >= 1:
2.6.1,print()
2.6.1,if early_stopping_wait >= early_stopping_patience:
2.6.1,break
2.6.1,early_stopping_wait += 1
2.6.1,if self.verbose >= 1:
2.6.1,print('Finished training after {} epochs.'.format(epoch))
2.6.1,if save_best_model_to_prefix is not None:
2.6.1,"print(""The best model's architecture and weights (from epoch {0}) """
2.6.1,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.6.1,"best_epoch, save_best_model_to_prefix))"
2.6.1,
2.6.1,"def predict(self, X):"
2.6.1,"return self.model.predict(X, batch_size=128, verbose=False)"
2.6.1,
2.6.1,def get_sequence_filters(self):
2.6.1,""""""""
2.6.1,Returns 3D array of 2D sequence filters.
2.6.1,""""""""
2.6.1,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.6.1,
2.6.1,"def deeplift(self, X, batch_size=200):"
2.6.1,""""""""
2.6.1,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.6.1,""""""""
2.6.1,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.6.1,from deeplift.conversion import keras_conversion as kc
2.6.1,
2.6.1,# convert to deeplift model and get scoring function
2.6.1,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.6.1,score_func = deeplift_model.get_target_contribs_func(
2.6.1,find_scores_layer_idx=0)
2.6.1,# use a 40% GC reference
2.6.1,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.6.1,# get deeplift scores
2.6.1,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.6.1,for i in range(self.num_tasks):
2.6.1,deeplift_scores[i] = score_func(
2.6.1,"task_idx=i,"
2.6.1,"input_data_list=[X],"
2.6.1,"batch_size=batch_size,"
2.6.1,"progress_update=None,"
2.6.1,input_references_list=input_references)
2.6.1,return deeplift_scores
2.6.1,
2.6.1,"def in_silico_mutagenesis(self, X):"
2.6.1,""""""""
2.6.1,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.6.1,""""""""
2.6.1,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.6.1,wild_type_predictions = self.predict(X)
2.6.1,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.6.1,np.newaxis]
2.6.1,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.6.1,"zip(X, wild_type_predictions)):"
2.6.1,mutated_sequences = np.repeat(
2.6.1,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.6.1,# remove wild-type
2.6.1,arange = np.arange(len(mutated_sequences))
2.6.1,horizontal_cycle = np.tile(
2.6.1,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.6.1,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.6.1,# add mutant
2.6.1,vertical_repeat = np.repeat(
2.6.1,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.6.1,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.6.1,# make mutant predictions
2.6.1,mutated_predictions = self.predict(mutated_sequences)
2.6.1,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.6.1,"(self.num_tasks,))"
2.6.1,mutagenesis_scores[
2.6.1,sequence_index] = wild_type_prediction - mutated_predictions
2.6.1,"return np.rollaxis(mutagenesis_scores, -1)"
2.6.1,
2.6.1,@staticmethod
2.6.1,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.6.1,from dragonn.plot import plot_bases_on_ax
2.6.1,scores = score_func(X).squeeze(
2.6.1,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.6.1,try:
2.6.1,os.makedirs(output_directory)
2.6.1,except OSError:
2.6.1,pass
2.6.1,num_tasks = len(scores)
2.6.1,"for task_index, task_scores in enumerate(scores):"
2.6.1,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.6.1,# sequence_scores is num_bases x sequence_length
2.6.1,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.6.1,plt.clf()
2.6.1,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.6.1,top_axis.plot(
2.6.1,"range(1,"
2.6.1,"len(basewise_max_sequence_scores) + 1),"
2.6.1,basewise_max_sequence_scores)
2.6.1,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.6.1,peak_position = basewise_max_sequence_scores.argmax()
2.6.1,top_axis.axvspan(
2.6.1,"peak_position - peak_width,"
2.6.1,"peak_position + peak_width,"
2.6.1,"color='grey',"
2.6.1,alpha=0.1)
2.6.1,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.6.1,peak_position + peak_width].T
2.6.1,# Set non-max letter_heights to zero
2.6.1,letter_heights = np.zeros_like(peak_sequence_scores)
2.6.1,"letter_heights[np.arange(len(letter_heights)),"
2.6.1,peak_sequence_scores.argmax(axis=1)] = \
2.6.1,basewise_max_sequence_scores[peak_position - peak_width :
2.6.1,peak_position + peak_width]
2.6.1,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.6.1,bottom_axis.set_xticklabels(
2.6.1,tuple(
2.6.1,"map(str,"
2.6.1,"np.arange(peak_position - peak_width,"
2.6.1,peak_position + peak_width + 1))))
2.6.1,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.6.1,plt.xlabel('Position')
2.6.1,plt.ylabel('Score')
2.6.1,plt.savefig(
2.6.1,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.6.1,"sequence_index, '_task_{}'.format(task_index)"
2.6.1,if num_tasks > 1 else '')))
2.6.1,plt.close()
2.6.1,
2.6.1,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.6.1,self._plot_scores(
2.6.1,"X,"
2.6.1,"output_directory,"
2.6.1,"peak_width,"
2.6.1,"score_func=self.deeplift,"
2.6.1,score_name='DeepLift')
2.6.1,
2.6.1,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.6.1,self._plot_scores(
2.6.1,"X,"
2.6.1,"output_directory,"
2.6.1,"peak_width,"
2.6.1,"score_func=self.in_silico_mutagenesis,"
2.6.1,score_name='ISM')
2.6.1,
2.6.1,"def plot_architecture(self, output_file):"
2.6.1,from dragonn.visualize_util import plot as plot_keras_model
2.6.1,"plot_keras_model(self.model, output_file, show_shape=True)"
2.6.1,
2.6.1,"def save(self, save_best_model_to_prefix):"
2.6.1,arch_fname = save_best_model_to_prefix + '.arch.json'
2.6.1,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.6.1,"open(arch_fname, 'w').write(self.model.to_json())"
2.6.1,"self.model.save_weights(weights_fname, overwrite=True)"
2.6.1,
2.6.1,@staticmethod
2.6.1,"def load(arch_fname, weights_fname=None):"
2.6.1,model_json_string = open(arch_fname).read()
2.6.1,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.6.1,if weights_fname is not None:
2.6.1,sequence_dnn.model.load_weights(weights_fname)
2.6.1,return sequence_dnn
2.6.1,create temporary fasta files
2.6.1,run command
2.6.1,remove fasta files
2.6.1,write test fasta file
2.6.1,test gkmsvm
2.6.1,get classification results
2.6.1,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.6.1,"Using canonical smiles for glycine, as in original research paper"
2.6.1,Atom features with padding
2.6.1,A_tilda_k computation
2.6.1,Final feed_dict setup
2.6.1,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.6.1,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.6.1,self.num_node_features)
2.6.1,Fit models
2.6.1,Args
2.6.1,2017 DeepCrystal Technologies - Patrick Hop
2.6.1,
2.6.1,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.6.1,
2.6.1,MIT License - have fun!!
2.6.1,===========================================================
2.6.1,x = F.selu( fc(x) )
2.6.1,x = F.selu( fc(x) )
2.6.1,2017 DeepCrystal Technologies - Patrick Hop
2.6.1,
2.6.1,Data loading a splitting file
2.6.1,
2.6.1,MIT License - have fun!!
2.6.1,===========================================================
2.6.1,Args
2.6.1,TODO (VIGS25): Account for the reload option
2.6.1,Downloading train files
2.6.1,Parsing training data
2.6.1,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.6.1,Test Files loading
2.6.1,One Hot Featurization
2.6.1,Consistency check
2.6.1,Handle output layer
2.6.1,Iterate over all previous tasks.
2.6.1,prev_layers is a list with elements of size
2.6.1,"(batch_size, layer_sizes[i-1])"
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,Save an initial checkpoint.
2.6.1,Turns out there are valid cases where we don't want pad-batches
2.6.1,on by default.
2.6.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.1,Run training op.
2.6.1,Always save a final checkpoint when complete.
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,Note that we divide by the batch size and not the number of
2.6.1,"non-zero weight examples in the batch.  Also, instead of using"
2.6.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.1,calculate with div/sum so it stays on the GPU.
2.6.1,aggregated costs
2.6.1,weight decay
2.6.1,Dummy placeholders
2.6.1,Dummy placeholders
2.6.1,run eval data through the model
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,Handle edge case when batch-size is 1.
2.6.1,Prune away any padding that was added
2.6.1,allow_soft_placement=True allows ops without a GPU implementation
2.6.1,to run on the CPU instead.
2.6.1,!/usr/bin/python
2.6.1,
2.6.1,Copyright 2015 Google Inc.
2.6.1,
2.6.1,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.6.1,you may not use this file except in compliance with the License.
2.6.1,You may obtain a copy of the License at
2.6.1,
2.6.1,http://www.apache.org/licenses/LICENSE-2.0
2.6.1,
2.6.1,"Unless required by applicable law or agreed to in writing, software"
2.6.1,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.6.1,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.6.1,See the License for the specific language governing permissions and
2.6.1,limitations under the License.
2.6.1,parse CheckpointState proto
2.6.1,parse path to actual checkpoint
2.6.1,the provided mask has to be the same shape as features
2.6.1,test k = 1..4
2.6.1,central moments
2.6.1,standardized moments
2.6.1,central across one axis
2.6.1,standardized across one axis
2.6.1,Fit just on task zero
2.6.1,Notice that we keep the session open
2.6.1,Fit on task one
2.6.1,The predictions for task zero should not change after training
2.6.1,on task one.
2.6.1,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.6.1,!/usr/bin/python
2.6.1,
2.6.1,Copyright 2015 Google Inc.
2.6.1,
2.6.1,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.6.1,you may not use this file except in compliance with the License.
2.6.1,You may obtain a copy of the License at
2.6.1,
2.6.1,http://www.apache.org/licenses/LICENSE-2.0
2.6.1,
2.6.1,"Unless required by applicable law or agreed to in writing, software"
2.6.1,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.6.1,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.6.1,See the License for the specific language governing permissions and
2.6.1,limitations under the License.
2.6.1,get the divisor
2.6.1,compute the requested central moment
2.6.1,"note that mean is a raw moment, not a central moment"
2.6.1,TODO(user): median is not implemented yet in TensorFlow
2.6.1,Add the input features.
2.6.1,"layer has shape [None, layer_sizes[i]]"
2.6.1,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.6.1,TODO(rbharath): Might want to make it feasible to have multiple
2.6.1,bypass layers.
2.6.1,Construct task bypass layer
2.6.1,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.6.1,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.6.1,"layer has shape [None, layer_sizes[i]]"
2.6.1,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.6.1,TODO(rbharath): Might want to make it feasible to have multiple
2.6.1,bypass layers.
2.6.1,Construct task bypass layer
2.6.1,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.6.1,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.6.1,Consistency check
2.6.1,Lazily created by _get_shared_session().
2.6.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.6.1,when subclass-overridden methods use the same scopes.
2.6.1,Setup graph
2.6.1,Create placeholders
2.6.1,Handle output layer
2.6.1,Iterate over all previous tasks.
2.6.1,prev_layers is a list with elements of size
2.6.1,"(batch_size, layer_sizes[i-1])"
2.6.1,Note that we divide by the batch size and not the number of
2.6.1,"non-zero weight examples in the batch.  Also, instead of using"
2.6.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.1,calculate with div/sum so it stays on the GPU.
2.6.1,aggregated costs
2.6.1,weight decay
2.6.1,Dummy placeholders
2.6.1,Dummy placeholders
2.6.1,run eval data through the model
2.6.1,"Shape (n_tasks, n__samples)"
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,Handle edge case when batch-size is 1.
2.6.1,with self._get_shared_session(train=True) as sess:
2.6.1,Save an initial checkpoint.
2.6.1,Always save a final checkpoint when complete.
2.6.1,Note that we divide by the batch size and not the number of
2.6.1,"non-zero weight examples in the batch.  Also, instead of using"
2.6.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.1,calculate with div/sum so it stays on the GPU.
2.6.1,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.6.1,Dummy placeholders
2.6.1,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.6.1,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.6.1,Dummy placeholders
2.6.1,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.6.1,allow_soft_placement=True allows ops without a GPU implementation
2.6.1,to run on the CPU instead.
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,Turns out there are valid cases where we don't want pad-batches
2.6.1,on by default.
2.6.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.1,if epoch%checkpoint_interval == checkpoint_interval-1:
2.6.1,"saver.save(sess, self._save_path, global_step=epoch)"
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,"(n_samples, n_classes)"
2.6.1,"(n_samples, n_tasks, n_classes)"
2.6.1,Save hyperparameters
2.6.1,Guard variable to make sure we don't Restore() this model
2.6.1,from a disk checkpoint more than once.
2.6.1,"Path to save checkpoint files, which matches the"
2.6.1,replicated supervisor's default path.
2.6.1,Lazily created by _get_shared_session().
2.6.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.6.1,when subclass-overridden methods use the same scopes.
2.6.1,Setup graph
2.6.1,Note that we divide by the batch size and not the number of
2.6.1,"non-zero weight examples in the batch.  Also, instead of using"
2.6.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.1,calculate with div/sum so it stays on the GPU.
2.6.1,aggregated costs
2.6.1,weight decay
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,Save an initial checkpoint.
2.6.1,Define the code that runs on a separate thread to feed data into the queue.
2.6.1,Main training loop.
2.6.1,Run training op.
2.6.1,We have reached the end of an epoch.
2.6.1,We have reached the end of the data.
2.6.1,Always save a final checkpoint when complete.
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,allow_soft_placement=True allows ops without a GPU implementation
2.6.1,to run on the CPU instead.
2.6.1,gpu memory growth option
2.6.1,gpu memory growth option
2.6.1,TODO(rbharath): Is setting train=False right here?
2.6.1,Discard any padded predictions
2.6.1,The iterbatches does padding with zero-weight examples on the last batch.
2.6.1,Remove padded examples.
2.6.1,Special case to handle singletasks.
2.6.1,The iterbatches does padding with zero-weight examples on the last batch.
2.6.1,Remove padded examples.
2.6.1,TODO(rbharath): Verify this can be safely removed.
2.6.1,"def evaluate(self, dataset, metrics, transformers=[]):"
2.6.1,""""""""
2.6.1,Evaluates the performance of this model on specified dataset.
2.6.1,
2.6.1,Parameters
2.6.1,----------
2.6.1,dataset: dc.data.Dataset
2.6.1,Dataset object.
2.6.1,metric: deepchem.metrics.Metric
2.6.1,Evaluation metric
2.6.1,transformers: list
2.6.1,List of deepchem.transformers.Transformer
2.6.1,Returns
2.6.1,-------
2.6.1,dict
2.6.1,Maps tasks to scores under metric.
2.6.1,""""""""
2.6.1,"evaluator = Evaluator(self, dataset, transformers)"
2.6.1,scores = evaluator.compute_model_performance(metrics)
2.6.1,return scores
2.6.1,checkpoints look like model_dir/model.ckpt-N
2.6.1,"self._save_path is ""model_dir/model.ckpt"""
2.6.1,run eval data through the model
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,run eval data through the model
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,Note that softmax is already applied in construct_grpah
2.6.1,run eval data through the model
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,Handle edge case when batch-size is 1.
2.6.1,Prune away any padding that was added
2.6.1,Handle case of 0-dimensional scalar output
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,inputs placeholder
2.6.1,data preprocessing and augmentation
2.6.1,first conv layer
2.6.1,downsample by max pooling
2.6.1,each module is a residual convolutional block
2.6.1,followed by a convolutional downsample layer
2.6.1,max pooling over the final outcome
2.6.1,fully connected layers
2.6.1,dropout for dense layers
2.6.1,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.6.1,weight decay regularizer
2.6.1,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.6.1,sample cut ratio from a clipped gaussian
2.6.1,train/valid differences
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,Define and build model
2.6.1,model.restore()
2.6.1,Set random seeds
2.6.1,Setup directories
2.6.1,Model constants
2.6.1,Load and transform datasets
2.6.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.1,Atomic convolution variables
2.6.1,at = atomic numbers (atom types)
2.6.1,"radial basis function parameters [cutoff, mean, width]"
2.6.1,Model hyperparameters
2.6.1,Initialize model
2.6.1,Fit model
2.6.1,Evaluate model
2.6.1,Set random seeds
2.6.1,Setup directories
2.6.1,Model constants
2.6.1,Load and transform datasets
2.6.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.1,Atomic convolution variables
2.6.1,at = atomic numbers (atom types)
2.6.1,"radial basis function parameters [cutoff, mean, width]"
2.6.1,Model hyperparameters
2.6.1,Initialize model
2.6.1,Fit model
2.6.1,Evaluate model
2.6.1,Set random seeds
2.6.1,Setup directories
2.6.1,Model constants
2.6.1,Load and transform datasets
2.6.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.1,Atomic convolution variables
2.6.1,at = atomic numbers (atom types)
2.6.1,"radial basis function parameters [cutoff, mean, width]"
2.6.1,Model hyperparameters
2.6.1,Initialize model
2.6.1,Fit model
2.6.1,Evaluate model
2.6.1,Set random seeds
2.6.1,Setup directories
2.6.1,Model constants
2.6.1,Load and transform datasets
2.6.1,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.1,Atomic convolution variables
2.6.1,at = atomic numbers (atom types)
2.6.1,"radial basis function parameters [cutoff, mean, width]"
2.6.1,Model hyperparameters
2.6.1,Initialize model
2.6.1,Fit model
2.6.1,Evaluate model
2.6.1,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.6.1,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.6.1,test_scores = test_evaluator.compute_model_performance(metric)
2.6.1,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.6.1,param.update(test_scores)
2.6.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.1,for transformer in transformers:
2.6.1,train_dataset = transformer.transform(train_dataset)
2.6.1,test_dataset = transformer.transform(test_dataset)
2.6.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.1,for transformer in transformers:
2.6.1,train_dataset = transformer.transform(train_dataset)
2.6.1,test_dataset = transformer.transform(test_dataset)
2.6.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.1,for transformer in transformers:
2.6.1,train_dataset = transformer.transform(train_dataset)
2.6.1,test_dataset = transformer.transform(test_dataset)
2.6.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.1,for transformer in transformers:
2.6.1,train_dataset = transformer.transform(train_dataset)
2.6.1,test_dataset = transformer.transform(test_dataset)
2.6.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.1,Create some directories for analysis
2.6.1,The base_dir holds the results of all analysis
2.6.1,Make directories to store the raw and featurized datasets.
2.6.1,Load PDBBind dataset
2.6.1,Define featurizers
2.6.1,Currently featurizes with shard_size=1
2.6.1,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.6.1,This could be done with openbabel in python
2.6.1,Compute cells for this molecule. O(constant)
2.6.1,min == max if molecule is planar in some direction
2.6.1,we should still create a bin
2.6.1,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.6.1,Note neighbors contains self!
2.6.1,Associate each atom with cell it belongs to. O(N)
2.6.1,Associate each cell with its neighbor cells. Assumes periodic boundary
2.6.1,"conditions, so does wrapround. O(constant)"
2.6.1,"For each atom, loop through all atoms in its cell and neighboring cells."
2.6.1,Accept as neighbors only those within threshold. This computation should be
2.6.1,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.6.1,Sort neighbors by distance
2.6.1,Pick up to max_num_neighbors
2.6.1,Type of data created by this featurizer
2.6.1,assumes that every array is of the same dimension
2.6.1,rem_dataset is remaining portion of dataset
2.6.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.6.1,to k-1.
2.6.1,returns list of per column sum of non zero elements
2.6.1,Compute number of actives needed per task.
2.6.1,loop through each column and obtain index required to splice out for
2.6.1,required fraction of hits
2.6.1,Find the first index where the cumulative number of actives equals
2.6.1,the actives_count
2.6.1,Note that np.where tells us last index required to exceed
2.6.1,"actives_count, so we actually want the following location"
2.6.1,TODO(rbharath): Refactor this split method to match API of other splits (or
2.6.1,potentially refactor those to match this.
2.6.1,Handle edge case where frac_split is 1
2.6.1,Create weight matrices fpor two haves.
2.6.1,copy over up to required index for weight first_split
2.6.1,check out if any rows in either w_1 or w_2 are just zeros
2.6.1,"Obtain original x, y, and w arrays and shuffle"
2.6.1,calculate percent split for valid (out of test and valid)
2.6.1,"split test data into valid and test, treating sub test set also as sparse"
2.6.1,rem_dataset is remaining portion of dataset
2.6.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.6.1,to k-1.
2.6.1,JSG Assert that split fractions can be written as proper fractions over 10.
2.6.1,This can be generalized in the future with some common demoninator determination.
2.6.1,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.6.1,Append remaining examples to train
2.6.1,Sort by increasing MW
2.6.1,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.6.1,for m_idx in cluster:
2.6.1,"continue until we find an active in all the tasks, otherwise we can't"
2.6.1,compute a meaningful AUC
2.6.1,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.6.1,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.6.1,Sort from largest to smallest scaffold sets
2.6.1,Sort from largest to smallest scaffold sets
2.6.1,"(n_samples, n_classes)"
2.6.1,"(n_samples, n_tasks, n_classes)"
2.6.1,Save hyperparameters
2.6.1,Guard variable to make sure we don't Restore() this model
2.6.1,from a disk checkpoint more than once.
2.6.1,"Path to save checkpoint files, which matches the"
2.6.1,replicated supervisor's default path.
2.6.1,Lazily created by _get_shared_session().
2.6.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.6.1,when subclass-overridden methods use the same scopes.
2.6.1,Setup graph
2.6.1,Note that we divide by the batch size and not the number of
2.6.1,"non-zero weight examples in the batch.  Also, instead of using"
2.6.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.1,calculate with div/sum so it stays on the GPU.
2.6.1,aggregated costs
2.6.1,weight decay
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,Save an initial checkpoint.
2.6.1,Turns out there are valid cases where we don't want pad-batches
2.6.1,on by default.
2.6.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.1,Run training op.
2.6.1,Always save a final checkpoint when complete.
2.6.1,############################################################# TIMING
2.6.1,############################################################# TIMING
2.6.1,allow_soft_placement=True allows ops without a GPU implementation
2.6.1,to run on the CPU instead.
2.6.1,TODO(rbharath): Is setting train=False right here?
2.6.1,Discard any padded predictions
2.6.1,The iterbatches does padding with zero-weight examples on the last batch.
2.6.1,Remove padded examples.
2.6.1,Special case to handle singletasks.
2.6.1,The iterbatches does padding with zero-weight examples on the last batch.
2.6.1,Remove padded examples.
2.6.1,TODO(rbharath): Verify this can be safely removed.
2.6.1,"def evaluate(self, dataset, metrics, transformers=[]):"
2.6.1,""""""""
2.6.1,Evaluates the performance of this model on specified dataset.
2.6.1,
2.6.1,Parameters
2.6.1,----------
2.6.1,dataset: dc.data.Dataset
2.6.1,Dataset object.
2.6.1,metric: deepchem.metrics.Metric
2.6.1,Evaluation metric
2.6.1,transformers: list
2.6.1,List of deepchem.transformers.Transformer
2.6.1,Returns
2.6.1,-------
2.6.1,dict
2.6.1,Maps tasks to scores under metric.
2.6.1,""""""""
2.6.1,"evaluator = Evaluator(self, dataset, transformers)"
2.6.1,scores = evaluator.compute_model_performance(metrics)
2.6.1,return scores
2.6.1,checkpoints look like logdir/model.ckpt-N
2.6.1,"self._save_path is ""logdir/model.ckpt"""
2.6.1,run eval data through the model
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,run eval data through the model
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,Note that softmax is already applied in construct_grpah
2.6.1,run eval data through the model
2.6.1,reshape to batch_size x n_tasks x ...
2.6.1,Handle edge case when batch-size is 1.
2.6.1,Prune away any padding that was added
2.6.1,Handle case of 0-dimensional scalar output
2.6.1,Dummy placeholders
2.6.1,Dummy placeholders
2.6.1,## AtomicNet fully-connected layer ops ###
2.6.1,## Atomicnet coordinate transform ops ###
2.6.1,## Atomicnet symmetry function kernel ops ###
2.6.1,## Atomicnet symmetry function ops ###
2.6.1,## Atomcnet symmetry function layer ops ###
2.6.1,We apply the radial pooling filter before atom type conv
2.6.1,to reduce computation
2.6.1,## Misc convenience ops ###
2.6.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.6.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.6.1,"game).  The average reward for any bet is slightly negative, so the best"
2.6.1,strategy is to walk away.
2.6.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.1,Optimize it.
2.6.1,"It should have learned that the expected value is very close to zero, and that the best"
2.6.1,action is to walk away.
2.6.1,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.6.1,get the same result.
2.6.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.1,Run the algorithm.
2.6.1,Save a file checkpoint.
2.6.1,Build the tree.
2.6.1,Compute the final probabilities and expected reward.
2.6.1,Mark this node as terminal
2.6.1,Expand this node.
2.6.1,Select the next action to perform.
2.6.1,Recursively build the tree.
2.6.1,Update statistics for this node.
2.6.1,Configuration file for the Sphinx documentation builder.
2.6.1,
2.6.1,This file only contains a selection of the most common options. For a full
2.6.1,list see the documentation:
2.6.1,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.6.1,-- Path setup --------------------------------------------------------------
2.6.1,"If extensions (or modules to document with autodoc) are in another directory,"
2.6.1,add these directories to sys.path here. If the directory is relative to the
2.6.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.6.1,
2.6.1,-- Project information -----------------------------------------------------
2.6.1,"The full version, including alpha/beta/rc tags"
2.6.1,-- General configuration ---------------------------------------------------
2.6.1,"Add any Sphinx extension module names here, as strings. They can be"
2.6.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.6.1,ones.
2.6.1,Options for autodoc directives
2.6.1,How to represents typehints
2.6.1,"Add any paths that contain templates here, relative to this directory."
2.6.1,The suffix of source filenames.
2.6.1,The master toctree document.
2.6.1,autosectionlabel setting
2.6.1,"List of patterns, relative to source directory, that match files and"
2.6.1,directories to ignore when looking for source files.
2.6.1,This pattern also affects html_static_path and html_extra_path.
2.6.1,"If true, the current module name will be prepended to all description"
2.6.1,unit titles (such as .. function::).
2.6.1,-- Options for HTML output -------------------------------------------------
2.6.1,The theme to use for HTML and HTML Help pages.  See the documentation for
2.6.1,a list of builtin themes.
2.6.1,"Add any paths that contain custom static files (such as style sheets) here,"
2.6.1,"relative to this directory. They are copied after the builtin static files,"
2.6.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.6.1,The name of an image file (relative to this directory) to place at the top
2.6.1,of the sidebar.
2.6.1,Customize the sphinx theme
2.6.1,-- Source code links ---------------------------------------------------
2.6.1,Resolve function for the linkcode extension.
2.6.1,"try to find the file and line number, based on code from numpy:"
2.6.1,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.6.1,lines in the label file have format
2.6.1,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.6.1,"print line[0], line[3]"
2.6.1,"If you push the tag, please remove `.dev`"
2.6.1,Record inputs.
2.6.1,Create the output directory if necessary.
2.6.1,Create the optimizers for meta-optimization and task optimization.
2.6.1,Create a Checkpoint for saving.
2.6.1,Main optimization loop.
2.6.1,Do checkpointing.
2.6.1,flake8: noqa
2.6.1,This is a MetaLearner that learns to generate sine functions with variable
2.6.1,amplitude and phase.
2.6.1,Optimize it.
2.6.1,Test it out on some new tasks and see how it works.
2.6.1,Initially the model should do a bad job of fitting the sine function.
2.6.1,After one step of optimization it should do much better.
2.6.1,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.6.1,get the same result.
2.6.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.1,We know use_pose_generator_scores == False in this case
2.6.1,check whether self.featurizer is instance of ComplexFeaturizer or not
2.6.1,TODO: How to handle the failure here?
2.6.1,TODO(rbharath): The autodock vina source computes surface distances
2.6.1,which take into account the van der Waals radius of each atom type.
2.6.1,"Shape (N, M)"
2.6.1,"Shape (N, M)"
2.6.1,Parse complex
2.6.1,check filetypes
2.6.1,Define locations of log and output files
2.6.1,Write GNINA conf file
2.6.1,Run GNINA
2.6.1,read output and log
2.6.1,Parse complex
2.6.1,Prepare protein
2.6.1,Get protein centroid and range
2.6.1,TODO(rbharath: Does vina divide box dimensions by 2?
2.6.1,Prepare ligand
2.6.1,Write Vina conf file
2.6.1,Define locations of output files
2.6.1,flake8: noqa
2.6.1,We provide no scoring model so the docker won't score
2.6.1,Check only one output since num_modes==1
2.6.1,We provide no scoring model so the docker won't score
2.6.1,Check only one output since num_modes==1
2.6.1,Let's turn on logging since this test will run for a while
2.6.1,Check returned files exist
2.6.1,Let's turn on logging since this test will run for a while
2.6.1,Check returned files exist
2.6.1,"Where d is greater than zero, the repulsion is just zeros"
2.6.1,"When d is 0, this should just be 1"
2.6.1,"When d == 0, the hbond interaction is 0"
2.6.1,The exponential returns 1 when input 0.
2.6.1,This exponential returns 1 when input 3
2.6.1,Let's turn on logging since this test will run for a while
2.6.1,Let's turn on logging since this test will run for a while
2.6.1,Let's turn on logging since this test will run for a while
2.6.1,Let's turn on logging since this test will run for a while
2.6.1,Let's turn on logging since this test will run for a while
2.6.1,Note this may download autodock Vina...
2.6.1,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.6.1,Test that every atom in pocket maps exists
2.6.1,scalar case
2.6.1,per-example case
2.6.1,This is a little arcane but it repeats w across tasks.
2.6.1,"If w.shape == (n_samples, 1) handle it as 1D"
2.6.1,"w.shape == (n_samples, n_tasks)"
2.6.1,scalar case
2.6.1,Handle n_classes/n_task shape ambiguity
2.6.1,Add in task dimension
2.6.1,Insert a task dimension (we know n_tasks=1 from above0
2.6.1,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.6.1,Handle classification. We need to convert labels into one-hot representation.
2.6.1,check whether n_classes is int or not
2.6.1,Handle n_classes/n_task shape ambiguity
2.6.1,Add in task dimension
2.6.1,Make everything 2D so easy to handle
2.6.1,Handle each task separately.
2.6.1,Handle continuous class probabilites of positive class for binary
2.6.1,Fill in class 0 probabilities
2.6.1,Add a task dimension to concatenate on
2.6.1,Handle binary labels
2.6.1,"make y_hot of shape (N, n_classes)"
2.6.1,Add a task dimension to concatenate on
2.6.1,Insert a task dimension
2.6.1,"Now of shape (N,)"
2.6.1,"Now of shape (N, 1)"
2.6.1,"Returns shape (N, n_tasks)"
2.6.1,"Now of shape (N,)"
2.6.1,"Now of shape (N, n_classes)"
2.6.1,"Now of shape (N, 1, n_classes)"
2.6.1,"Returns shape (N, n_tasks, n_classes)"
2.6.1,These are some smart defaults
2.6.1,These are some smart defaults corresponding to sklearn's required
2.6.1,behavior
2.6.1,Attempt some limited shape imputation to find n_tasks
2.6.1,check whether n_tasks is int or not
2.6.1,This is because `normalize_weight_shape` require int value.
2.6.1,FIXME: Incompatible types in assignment
2.6.1,Attempt to convert both into the same type
2.6.1,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.6.1,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.6.1,initialize fwd and reverse scores to -infinity
2.6.1,"cross-correlate separately for each base,"
2.6.1,for both the PSSM and its reverse complement
2.6.1,sum over the bases
2.6.1,take max of fwd and reverse scores at each position
2.6.1,"Shape (N_sequences, num_tasks)"
2.6.1,check whether wild_type_predictions is np.ndarray or not
2.6.1,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.6.1,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.6.1,Mutates every position of the sequence to every letter
2.6.1,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.6.1,Breakdown:
2.6.1,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.6.1,remove wild-type
2.6.1,len(arange) = N_letters * sequence_length
2.6.1,len(horizontal cycle) = N_letters * sequence_length
2.6.1,add mutant
2.6.1,make mutant predictions
2.6.1,check whether wild_type_predictions is np.ndarray or not
2.6.1,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.6.1,validation
2.6.1,flake8: noqa
2.6.1,metric class
2.6.1,metrics utils
2.6.1,sklearn & scipy score function
2.6.1,original score function
2.6.1,Get a random prediction matrix
2.6.1,"Of shape (N, n_classes)"
2.6.1,"Of shape (N, 1, n_classes)"
2.6.1,This has w for each task.
2.6.1,Best score case
2.6.1,Worst score case
2.6.1,best case
2.6.1,duplicate prediction value
2.6.1,Encode motif
2.6.1,"sequences now has shape (3, 4, 5, 1)"
2.6.1,"sequences now has shape (3, 4, 5, 1)"
2.6.1,Construct and train SequenceDNN model
2.6.1,Call in-silico mutagenesis
2.6.1,Construct and train SequenceDNN model
2.6.1,Call in-silico mutagenesis
2.6.1,Check nonzero elements exist
2.6.1,Special case handling of single input
2.6.1,Featurize task results iff they exist.
2.6.1,Filter out examples where featurization failed.
2.6.1,"For prospective data where results are unknown, it"
2.6.1,makes no sense to have y values or weights.
2.6.1,Featurize task results if they exist.
2.6.1,Filter out examples where featurization failed.
2.6.1,"For prospective data where results are unknown, it"
2.6.1,makes no sense to have y values or weights.
2.6.1,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.6.1,The field in which load_sdf_files return value stores smiles
2.6.1,Special case handling of single input
2.6.1,Featurize task results iff they exist.
2.6.1,Filter out examples where featurization failed.
2.6.1,"For prospective data where results are unknown, it"
2.6.1,makes no sense to have y values or weights.
2.6.1,Process legacy toggle
2.6.1,Set attributes
2.6.1,Handle special featurizer cases
2.6.1,Set self.featurizer
2.6.1,"(X, y, w, ids)"
2.6.1,TODO don't convert all sequences into np array (allow shards)
2.6.1,Check if line is a header
2.6.1,Handle empty sequence
2.6.1,TODO log attempts to add empty sequences every shard
2.6.1,Annotate start/stop of sequence
2.6.1,Sometimes zip files contain directories within. Traverse directories
2.6.1,TODO(rbharath): Add support for more extensions
2.6.1,Sort image files
2.6.1,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.6.1,Remove support indices
2.6.1,Remove support indices
2.6.1,Remove support indices
2.6.1,Get task specific entries
2.6.1,Now just get weights for this task
2.6.1,Get task specific entries
2.6.1,Now just get weights for this task
2.6.1,Now just get weights for this task
2.6.1,Now just get weights for this task
2.6.1,Split data into pos and neg lists.
2.6.1,No replacement allowed for supports
2.6.1,Handle one-d vs. non one-d feature matrices
2.6.1,Init the iterator
2.6.1,Set initial iterator state
2.6.1,support = self.supports[task][self.trial_num]
2.6.1,Increment and update logic
2.6.1,Init the iterator
2.6.1,Set initial iterator state
2.6.1,support = self.supports[task][self.trial_num]
2.6.1,Increment and update logic
2.6.1,Ensure that every worker will pick the same random order for each epoch.
2.6.1,Ensure that every worker will pick the same random order for each epoch.
2.6.1,"By invariant of when this is called, can assume num_samples > 0"
2.6.1,and num_samples < batch_size
2.6.1,Fill in batch arrays
2.6.1,"By invariant of when this is called, can assume num_samples > 0"
2.6.1,and num_samples < batch_size
2.6.1,Fill in batch arrays
2.6.1,Only the first set of copy will be counted in training loss
2.6.1,Retrieve the first sample so we can determine the dtypes.
2.6.1,Create a Tensorflow Dataset.
2.6.1,Find the X values.
2.6.1,Find the y values.
2.6.1,Find the w values.
2.6.1,Find the ids.
2.6.1,"Set labels to be zero, with zero weights"
2.6.1,Load obsolete format -> save in new format
2.6.1,note that this corresponds to the _construct_metadata column order
2.6.1,Create temp directory to store resharded version
2.6.1,Get correct shapes for y/w
2.6.1,Write data in new shards
2.6.1,Handle shapes
2.6.1,Note that this means that DiskDataset resharding currently doesn't
2.6.1,work for datasets that aren't regression/classification.
2.6.1,Handle spillover from last shard
2.6.1,Should have updated to non-legacy metadata
2.6.1,Note that this resets the cache internally
2.6.1,"(ytz): Depending on the application, thread-based pools may be faster"
2.6.1,"than process based pools, since process based pools need to pickle/serialize"
2.6.1,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.6.1,we're actually protected by the GIL.
2.6.1,mp.dummy aliases ThreadPool to Pool
2.6.1,(ytz): this skips everything except possibly the last shard
2.6.1,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.6.1,make a NumpyDataset under the hood
2.6.1,"raw_data = (X, y, w, ids)"
2.6.1,Protect against generator exhaustion
2.6.1,This ensures tasks are consistent for all datasets
2.6.1,determine the shard sizes of the datasets to merge
2.6.1,"otherwise the entire dataset is the ""shard size"""
2.6.1,we must reshard the dataset to have a uniform size
2.6.1,choose the smallest shard size
2.6.1,Get full dataset in memory
2.6.1,Shuffle in memory
2.6.1,Write shuffled shards out to disk
2.6.1,Shuffle the arrays corresponding to each row in metadata_df
2.6.1,Reset cache
2.6.1,See if we have a cached copy of this shard.
2.6.1,"We don't, so load it from disk."
2.6.1,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.6.1,Try to cache this shard for later use.  Since the normal usage pattern is
2.6.1,"a series of passes through the whole dataset, there's no point doing"
2.6.1,anything fancy.  It never makes sense to evict another shard from the
2.6.1,"cache to make room for this one, because we'll probably want that other"
2.6.1,shard again before the next time we want this one.  So just cache as many
2.6.1,as we can and then stop.
2.6.1,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.6.1,Handle edge case with empty indices
2.6.1,We use two loops here. The outer while loop walks over selection shards
2.6.1,(the chunks of the indices to select that should go into separate
2.6.1,"output shards), while the inner for loop walks over the shards in the"
2.6.1,source datasets to select out the shard indices from that  source shard
2.6.1,Find indices which rest in this shard
2.6.1,Need to offset indices to fit within shard_size
2.6.1,Handle empty case where no data from this shard needed
2.6.1,Handle the case of datasets with y/w missing
2.6.1,Break if all indices have been used up already
2.6.1,Note these will be in the sorted order
2.6.1,We need to recover the original ordering. We can do this by using
2.6.1,np.where to find the locatios of the original indices in the sorted
2.6.1,indices.
2.6.1,We know there's only one match for np.where since this is a
2.6.1,"permutation, so the [0][0] pulls out the exact match location."
2.6.1,If shape metadata is available use it to directly compute shape from
2.6.1,metadata
2.6.1,"In absense of shape metadata, fall back to loading data from disk to"
2.6.1,find shape.
2.6.1,Case n_samples should be 1
2.6.1,flake8: noqa
2.6.1,TODO(rbharath): Get rid of * import
2.6.1,Test merging of numpy datasets
2.6.1,Load MUV dataset
2.6.1,Do an approximate comparison since splits are sometimes slightly off from
2.6.1,the exact fraction.
2.6.1,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.6.1,reloading will cause the transform to be reapplied. This is undesirable in
2.6.1,almost all cases. Need to understand a method to fix this.
2.6.1,The shuffling should have switched up the ordering
2.6.1,But all the same entries should still be present
2.6.1,All the data should have same shape
2.6.1,The shuffling should have switched up the ordering
2.6.1,But all the same entries should still be present
2.6.1,All the data should have same shape
2.6.1,The ids should now store the performed permutation. Check that the
2.6.1,original dataset is recoverable.
2.6.1,The ids should now store the performed permutation. Check that the
2.6.1,original dataset is recoverable.
2.6.1,Generate data
2.6.1,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.6.1,around for testing resharding.
2.6.1,Set cache to 0 size to avoid cache hiding errors
2.6.1,Generate data
2.6.1,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.6.1,around for testing resharding.
2.6.1,Set cache to 0 size to avoid cache hiding errors
2.6.1,Featurize emols dataset
2.6.1,example.fasta contains 3 sequences each of length 58.
2.6.1,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.6.1,"There is one ""image channel""."
2.6.1,"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
2.6.1,TODO: test with full uniprot file once sharding support is added.
2.6.1,Generate dummy dataset
2.6.1,Generate dummy dataset
2.6.1,Generate dummy dataset
2.6.1,Set last n_samples/2 weights to 0
2.6.1,Check that no support elements are sample from zero-weight samples
2.6.1,Generate dummy dataset
2.6.1,Generate dummy dataset
2.6.1,Create support generator
2.6.1,Generate dummy dataset
2.6.1,Create support generator
2.6.1,Generate dummy dataset
2.6.1,Assert all support elements have been removed
2.6.1,Generate dummy dataset
2.6.1,Assert all remove elements have been removed
2.6.1,Generate dummy dataset
2.6.1,Assert all support elements have been removed
2.6.1,Generate dummy dataset
2.6.1,Assert all remove elements have been removed
2.6.1,Generate dummy dataset
2.6.1,Set last n_samples/2 weights to 0
2.6.1,Sample from first n_samples/2 elements for support
2.6.1,Should lie within first n_samples/2 samples only
2.6.1,Generate dummy dataset
2.6.1,Create support generator
2.6.1,Generate dummy dataset
2.6.1,This is necessary since from_numpy adds in shape information
2.6.1,This is necessary since from_numpy adds in shape information
2.6.1,This is necessary since from_numpy adds in shape information
2.6.1,Generate data
2.6.1,Generate data
2.6.1,Generate data
2.6.1,Should now have 10 shards
2.6.1,This is the shape of legacy_data
2.6.1,legacy_dataset is a dataset in the legacy format kept around for testing
2.6.1,purposes.
2.6.1,This is the shape of legacy_data_reshard
2.6.1,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.6.1,around for testing
2.6.1,Should now have 10 shards
2.6.1,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.6.1,Test constructor reload works for legacy format
2.6.1,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.6.1,around for testing resharding.
2.6.1,Reshard copy
2.6.1,Check metadata has been updated
2.6.1,First try using images for X.
2.6.1,Now try using images for y.
2.6.1,Transform it
2.6.1,Test on identity matrix
2.6.1,Generate random sparse features dataset
2.6.1,Test edge case with array of all zeros
2.6.1,Test cases where n_samples < 2*n_samples < batch_size
2.6.1,Test cases where n_samples < batch_size
2.6.1,Test case where n_samples == batch_size
2.6.1,Test case for object featurization.
2.6.1,Test case for more complicated object featurization
2.6.1,Test case with multidimensional data
2.6.1,Test cases where n_samples < 2*n_samples < batch_size
2.6.1,Test cases where n_samples < batch_size
2.6.1,Test case where n_samples == batch_size
2.6.1,Test case for object featurization.
2.6.1,Test case for more complicated object featurization
2.6.1,Test case with multidimensional data
2.6.1,Test first resharding worked
2.6.1,Test second resharding worked
2.6.1,approx 1/15! chance of equality
2.6.1,Generate data
2.6.1,Generate data
2.6.1,Transform it
2.6.1,special case to test
2.6.1,deterministic
2.6.1,non-deterministic
2.6.1,we don't know the order in which the shards are iterated in.
2.6.1,Check that we have all the data in
2.6.1,Test iterating in order.
2.6.1,Test iterating out of order.
2.6.1,Test iterating in batches.
2.6.1,Test iterating with multiple workers.
2.6.1,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.6.1,Try specifying particular columns.
2.6.1,Test id shrinkage
2.6.1,Test task shrinkage
2.6.1,Test max print size
2.6.1,Create image file
2.6.1,Create zip of image file
2.6.1,Create zip of multiple image files
2.6.1,"Create zip of multiple image files, multiple_types"
2.6.1,Create image directory
2.6.1,These are the known dimensions of face.png
2.6.1,These are the known dimensions of face.png
2.6.1,TODO(rbharath): Where are the color channels?
2.6.1,"Since the different files have different shapes, makes an object array"
2.6.1,Splits featurized samples into train/test
2.6.1,Splits featurized samples into train/test
2.6.1,Splits featurized samples into train/test
2.6.1,Splits featurized samples into train/test
2.6.1,Now perform move
2.6.1,Only for debug!
2.6.1,Make directories to store the raw and featurized datasets.
2.6.1,Load dataset
2.6.1,Featurize tox21 dataset
2.6.1,featurization
2.6.1,train/valid split.
2.6.1,singletask load
2.6.1,comparison
2.6.1,Only for debug!
2.6.1,Make directories to store the raw and featurized datasets.
2.6.1,Load dataset
2.6.1,Featurize tox21 dataset
2.6.1,For debugging purposes
2.6.1,multitask load
2.6.1,Do train/valid split.
2.6.1,singletask load
2.6.1,comparison
2.6.1,Get the labels/weights
2.6.1,Normalize shapes
2.6.1,Remove labels with zero weights
2.6.1,Note that we may have 0 elements of a given class since we remove those
2.6.1,labels with zero weight.
2.6.1,this works because y is 1D
2.6.1,This is the right ratio since int(N/num_c) * num_c \approx N
2.6.1,for all classes
2.6.1,Flattening is safe because of shape check above
2.6.1,Hack to allow for easy unpickling:
2.6.1,http://stefaanlippens.net/pickleproblem
2.6.1,Some transformation must happen
2.6.1,Add this case in to handle non-DiskDataset that should be written to disk
2.6.1,Note that transformers have to be undone in reversed order
2.6.1,Handle division by zero
2.6.1,Handle division by zero
2.6.1,Control for pathological case with no variance.
2.6.1,Handle case with 1 task correctly
2.6.1,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.6.1,Find the task dimension of z
2.6.1,Prevent broadcasting on wrong dimension
2.6.1,BalancingTransformer can only transform weights.
2.6.1,Compute weighting factors from dataset.
2.6.1,Handle 1-D case
2.6.1,Remove labels with zero weights
2.6.1,Note that we may have 0 elements of a given class since we remove those
2.6.1,labels with zero weight. This typically happens in multitask datasets
2.6.1,where some datapoints only have labels for some tasks.
2.6.1,this works because task_y is 1D
2.6.1,This is the right ratio since N_task/num_c * num_c = N_task
2.6.1,for all classes
2.6.1,Set to the class weight computed previously
2.6.1,Need this for transform_y
2.6.1,Handle 1D case
2.6.1,THis reshape is safe because of guard above.
2.6.1,map the indices to labels
2.6.1,generating batch of data by slicing similarity matrix
2.6.1,into 100*reference_dataset_length
2.6.1,concatenate batches of data together
2.6.1,highest similarity is 1: target is in the reference
2.6.1,use the following K points
2.6.1,"highest less than 1: target not in the reference, use top K points"
2.6.1,calculate matrix multiplicatin on slices
2.6.1,concatenate the slices together
2.6.1,list of calculation orders for DAGs
2.6.1,stemming from one specific atom in the molecule
2.6.1,starting from the adjacency list derived by graphconv featurizer
2.6.1,"number of atoms, also number of DAGs"
2.6.1,"DAG on a molecule with k atoms includes k steps of calculation,"
2.6.1,each step calculating graph features for one atom.
2.6.1,`max_atoms` is the maximum number of steps
2.6.1,each iteration generates the DAG starting from atom with index `count`
2.6.1,"list of lists, elements represent the calculation orders"
2.6.1,for atoms in the current graph
2.6.1,starting from the target atom with index `count`
2.6.1,flags of whether the atom is already included in the DAG
2.6.1,atom `count` is in the DAG
2.6.1,recording number of radial propagation steps
2.6.1,"in the fisrt loop, atoms directly connected to `count` will be added"
2.6.1,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.6.1,will be added in the second loop(radial=1).
2.6.1,atoms i-bond away will be added in i-th loop
2.6.1,"when molecules have separate parts, starting from one part,"
2.6.1,it is not possible to include all atoms.
2.6.1,this break quit the loop when going into such condition
2.6.1,reinitialize targets for next iteration
2.6.1,atoms connected to current_atom
2.6.1,generate the dependency map of current DAG
2.6.1,atoms connected to `current_atoms`(and not included in the DAG)
2.6.1,"are added, and will be the `current_atoms` for next iteration."
2.6.1,"DAG starts from the target atom, calculation should go in reverse"
2.6.1,`edge[1]` is the parent of `edge[0]`
2.6.1,"after this loop, `parents[i]` includes all parents of atom i"
2.6.1,manually adding the atom index into its parents list
2.6.1,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.6.1,atoms with less parents(farther from the target atom) come first.
2.6.1,"graph features of atoms without parents will be first calculated,"
2.6.1,then atoms with more parents can be calculated in order
2.6.1,based on previously calculated graph features.
2.6.1,target atom of this DAG will be calculated in the last step
2.6.1,padding with `max_atoms`
2.6.1,padding
2.6.1,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.6.1,which is a max_atoms * max_atoms numpy array after padding
2.6.1,class ANITransformer(Transformer):
2.6.1,"""""""Performs transform from 3D coordinates to ANI symmetry functions"
2.6.1,Note
2.6.1,----
2.6.1,This class requires TensorFlow to be installed.
2.6.1,""""""""
2.6.1,"def __init__(self,"
2.6.1,"max_atoms=23,"
2.6.1,"radial_cutoff=4.6,"
2.6.1,"angular_cutoff=3.1,"
2.6.1,"radial_length=32,"
2.6.1,"angular_length=8,"
2.6.1,"atom_cases=[1, 6, 7, 8, 16],"
2.6.1,"atomic_number_differentiated=True,"
2.6.1,coordinates_in_bohr=True):
2.6.1,""""""""
2.6.1,Only X can be transformed
2.6.1,""""""""
2.6.1,import tensorflow as tf
2.6.1,self.max_atoms = max_atoms
2.6.1,self.radial_cutoff = radial_cutoff
2.6.1,self.angular_cutoff = angular_cutoff
2.6.1,self.radial_length = radial_length
2.6.1,self.angular_length = angular_length
2.6.1,self.atom_cases = atom_cases
2.6.1,self.atomic_number_differentiated = atomic_number_differentiated
2.6.1,self.coordinates_in_bohr = coordinates_in_bohr
2.6.1,self.compute_graph = self.build()
2.6.1,self.sess = tf.Session(graph=self.compute_graph)
2.6.1,self.transform_batch_size = 32
2.6.1,"super(ANITransformer, self).__init__(transform_X=True)"
2.6.1,"def transform_array(self, X, y, w):"
2.6.1,if self.transform_X:
2.6.1,X_out = []
2.6.1,num_transformed = 0
2.6.1,start = 0
2.6.1,batch_size = self.transform_batch_size
2.6.1,while True:
2.6.1,"end = min((start + 1) * batch_size, X.shape[0])"
2.6.1,X_batch = X[(start * batch_size):end]
2.6.1,output = self.sess.run(
2.6.1,"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
2.6.1,X_out.append(output)
2.6.1,num_transformed = num_transformed + X_batch.shape[0]
2.6.1,logger.info('%i samples transformed' % num_transformed)
2.6.1,start += 1
2.6.1,if end >= len(X):
2.6.1,break
2.6.1,"X_new = np.concatenate(X_out, axis=0)"
2.6.1,assert X_new.shape[0] == X.shape[0]
2.6.1,"return (X_new, y, w)"
2.6.1,"def untransform(self, z):"
2.6.1,raise NotImplementedError(
2.6.1,"""Cannot untransform datasets with ANITransformer."")"
2.6.1,def build(self):
2.6.1,""""""" tensorflow computation graph for transform """""""
2.6.1,import tensorflow as tf
2.6.1,graph = tf.Graph()
2.6.1,with graph.as_default():
2.6.1,self.inputs = tf.keras.Input(
2.6.1,"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
2.6.1,"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
2.6.1,flags = tf.sign(atom_numbers)
2.6.1,flags = tf.cast(
2.6.1,"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
2.6.1,"coordinates = self.inputs[:, :, 1:]"
2.6.1,if self.coordinates_in_bohr:
2.6.1,coordinates = coordinates * 0.52917721092
2.6.1,"d = self.distance_matrix(coordinates, flags)"
2.6.1,"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
2.6.1,"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
2.6.1,"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
2.6.1,"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
2.6.1,coordinates)
2.6.1,self.outputs = tf.concat(
2.6.1,[
2.6.1,"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
2.6.1,angular_sym
2.6.1,"],"
2.6.1,axis=2)
2.6.1,return graph
2.6.1,"def distance_matrix(self, coordinates, flags):"
2.6.1,""""""" Generate distance matrix """""""
2.6.1,import tensorflow as tf
2.6.1,max_atoms = self.max_atoms
2.6.1,"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
2.6.1,"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
2.6.1,# Calculate pairwise distance
2.6.1,"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
2.6.1,# Masking for valid atom index
2.6.1,d = d * flags
2.6.1,return d
2.6.1,"def distance_cutoff(self, d, cutoff, flags):"
2.6.1,""""""" Generate distance matrix with trainable cutoff """""""
2.6.1,import tensorflow as tf
2.6.1,# Cutoff with threshold Rc
2.6.1,d_flag = flags * tf.sign(cutoff - d)
2.6.1,d_flag = tf.nn.relu(d_flag)
2.6.1,d_flag = d_flag * tf.expand_dims(
2.6.1,"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
2.6.1,d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
2.6.1,return d * d_flag
2.6.1,"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
2.6.1,""""""" Radial Symmetry Function """""""
2.6.1,import tensorflow as tf
2.6.1,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.6.1,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.6.1,"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
2.6.1,ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
2.6.1,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
2.6.1,"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
2.6.1,length = ita.get_shape().as_list()[-1]
2.6.1,"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
2.6.1,"d = tf.stack([d] * length, axis=3)"
2.6.1,out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
2.6.1,if self.atomic_number_differentiated:
2.6.1,out_tensors = []
2.6.1,for atom_type in self.atom_cases:
2.6.1,selected_atoms = tf.expand_dims(
2.6.1,"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
2.6.1,axis=3)
2.6.1,"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
2.6.1,"return tf.concat(out_tensors, axis=2)"
2.6.1,else:
2.6.1,"return tf.reduce_sum(out, axis=2)"
2.6.1,"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
2.6.1,""""""" Angular Symmetry Function """""""
2.6.1,import tensorflow as tf
2.6.1,max_atoms = self.max_atoms
2.6.1,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.6.1,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.6.1,"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
2.6.1,ita = 3 / (Rs[1] - Rs[0])**2
2.6.1,"thetas = np.linspace(0., np.pi, self.angular_length)"
2.6.1,zeta = float(self.angular_length**2)
2.6.1,"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
2.6.1,"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
2.6.1,"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
2.6.1,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
2.6.1,"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
2.6.1,length = zeta.get_shape().as_list()[-1]
2.6.1,"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
2.6.1,"[coordinates] * max_atoms, 2)"
2.6.1,"R_ij = tf.stack([d] * max_atoms, axis=3)"
2.6.1,"R_ik = tf.stack([d] * max_atoms, axis=2)"
2.6.1,"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
2.6.1,"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
2.6.1,# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.6.1,"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
2.6.1,"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
2.6.1,vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
2.6.1,"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
2.6.1,"R_ij = tf.stack([R_ij] * length, axis=4)"
2.6.1,"R_ik = tf.stack([R_ik] * length, axis=4)"
2.6.1,"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
2.6.1,"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
2.6.1,"theta = tf.stack([theta] * length, axis=4)"
2.6.1,"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
2.6.1,tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
2.6.1,if self.atomic_number_differentiated:
2.6.1,out_tensors = []
2.6.1,"for id_j, atom_type_j in enumerate(self.atom_cases):"
2.6.1,for atom_type_k in self.atom_cases[id_j:]:
2.6.1,"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
2.6.1,"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
2.6.1,selected_atoms = tf.expand_dims(
2.6.1,"tf.expand_dims(selected_atoms, axis=1), axis=4)"
2.6.1,out_tensors.append(
2.6.1,"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
2.6.1,"return tf.concat(out_tensors, axis=2)"
2.6.1,else:
2.6.1,"return tf.reduce_sum(out_tensor, axis=(2, 3))"
2.6.1,def get_num_feats(self):
2.6.1,n_feat = self.outputs.get_shape().as_list()[-1]
2.6.1,return n_feat
2.6.1,flake8: noqa
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a y transformer
2.6.1,Check w is unchanged since this is a y transformer
2.6.1,Check y is now a logarithmic version of itself
2.6.1,Check that untransform does the right thing.
2.6.1,Check ids are unchanged.
2.6.1,Check y is unchanged since this is a X transformer
2.6.1,Check w is unchanged since this is a y transformer
2.6.1,Check y is now a logarithmic version of itself
2.6.1,Check that untransform does the right thing.
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a y transformer
2.6.1,Check w is unchanged since this is a y transformer
2.6.1,Check y is now a logarithmic version of itself
2.6.1,Check that untransform does the right thing.
2.6.1,Tests logarithmic data transformer with selection.
2.6.1,Check ids are unchanged.
2.6.1,Check y is unchanged since this is a X transformer
2.6.1,Check w is unchanged since this is a y transformer
2.6.1,Check y is now a logarithmic version of itself
2.6.1,Check that untransform does the right thing.
2.6.1,Check ids are unchanged.
2.6.1,Check y is unchanged since this is an X transformer
2.6.1,Check w is unchanged since this is an X transformer
2.6.1,Check X is now holding the proper values when sorted.
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is an y transformer
2.6.1,Check w is unchanged since this is an y transformer
2.6.1,Check y is now holding the proper values when sorted.
2.6.1,Check that untransform does the right thing.
2.6.1,Check ids are unchanged.
2.6.1,Check y is unchanged since this is an X transformer
2.6.1,Check w is unchanged since this is an X transformer
2.6.1,Check X is now holding the proper values when sorted.
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a y transformer
2.6.1,Check w is unchanged since this is a y transformer
2.6.1,Check y is now holding the proper values when sorted.
2.6.1,Check ids are unchanged before and after transformation
2.6.1,Check X is unchanged since transform_y is true
2.6.1,Check w is unchanged since transform_y is true
2.6.1,Check minimum and maximum values of transformed y are 0 and 1
2.6.1,Check untransform works correctly
2.6.1,Check ids are unchanged before and after transformation
2.6.1,Check X is unchanged since transform_y is true
2.6.1,Check w is unchanged since transform_y is true
2.6.1,Check minimum and maximum values of transformed y are 0 and 1
2.6.1,Test if dimensionality expansion is handled correctly by untransform
2.6.1,Check ids are unchanged before and after transformation
2.6.1,Check X is unchanged since transform_y is true
2.6.1,Check w is unchanged since transform_y is true
2.6.1,Check minimum and maximum values of transformed y are 0 and 1
2.6.1,Check untransform works correctly
2.6.1,Load mini log-solubility dataset.
2.6.1,The transformer generates n DAGs for a molecule with n
2.6.1,"atoms. These are denoted the ""parents"""
2.6.1,extract only the images (no need of the labels)
2.6.1,reshaping the vector to image
2.6.1,Check Blurring
2.6.1,Check center crop
2.6.1,Check crop
2.6.1,Check convert2gray
2.6.1,Check rotation
2.6.1,Some more test cases for flip
2.6.1,Check flip
2.6.1,Check Scales
2.6.1,Check shift
2.6.1,check gaussian noise
2.6.1,check salt and pepper noise
2.6.1,Check median filter
2.6.1,transforming y should raise an exception
2.6.1,transforming w should raise an exception
2.6.1,transforming X should be okay
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a y transformer
2.6.1,Check w is unchanged since this is a y transformer
2.6.1,"Check that y_t has zero mean, unit std."
2.6.1,Check that untransform does the right thing.
2.6.1,Check ids are unchanged.
2.6.1,Check y is unchanged since this is a X transformer
2.6.1,Check w is unchanged since this is a y transformer
2.6.1,"Check that X_t has zero mean, unit std."
2.6.1,np.set_printoptions(threshold='nan')
2.6.1,Entries with zero std are not normalized
2.6.1,Check that untransform does the right thing.
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a w transformer
2.6.1,Check y is unchanged since this is a w transformer
2.6.1,Assert that entries with zero weight retain zero weight
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a w transformer
2.6.1,Check y is unchanged since this is a w transformer
2.6.1,Assert that entries with zero weight retain zero weight
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a w transformer
2.6.1,Check y is unchanged since this is a w transformer
2.6.1,Assert that entries with zero weight retain zero weight
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a w transformer
2.6.1,Check y is unchanged since this is a w transformer
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is a w transformer
2.6.1,Check y is unchanged since this is a w transformer
2.6.1,Assert that entries with zero weight retain zero weight
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,Check ids are unchanged.
2.6.1,Check y is unchanged since this is an X transformer
2.6.1,Check w is unchanged since this is an X transformer
2.6.1,Check X is now holding the proper values in each column.
2.6.1,Check ids are unchanged.
2.6.1,Check X is unchanged since this is an X transformer
2.6.1,Check w is unchanged since this is an X transformer
2.6.1,Check y is now holding the proper values in each column.
2.6.1,Check that untransform does the right thing.
2.6.1,Check that we have length 8 now with duplication
2.6.1,Check shapes
2.6.1,Check that we have 4 positives and 4 negatives
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,Note that nothing should change in this dataset since weights balance!
2.6.1,Check that still we have length 6
2.6.1,Check shapes
2.6.1,Check that we have 2 positives and 4 negatives
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,Check that we have length 8 now with duplication
2.6.1,Check shapes
2.6.1,Check that we have 4 positives and 4 negatives
2.6.1,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.1,6-1 imbalance in favor of class 0
2.6.1,Check that we have length 30 now with duplication
2.6.1,Check shapes
2.6.1,Check that we have 6 of each class
2.6.1,Check that sum of all class weights is equal by comparing to 0 weight
2.6.1,Note class imbalance. This will round to 2x duplication for 1
2.6.1,Check that we have length 13 now with duplication
2.6.1,Check shapes
2.6.1,Check that we have 6 positives and 7 negatives
2.6.1,################################################################
2.6.1,save.py is out of date. You should not import any functions from here.
2.6.1,################################################################
2.6.1,flake8: noqa
2.6.1,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.6.1,Walk through the original file and extract ATOM/HETATM lines and
2.6.1,add PDBQT charge annotations.
2.6.1,Remove rotatable bonds from this molecule
2.6.1,Get the connected components now that the rotatable bonds have
2.6.1,been removed.
2.6.1,The root is the largest connected component.
2.6.1,Write the root component
2.6.1,"We've looked at the root, so take note of that"
2.6.1,Compute partial charges on molecule if RDKit Mol
2.6.1,indices to atoms to keep
2.6.1,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.6.1,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.6.1,nonzero contact.
2.6.1,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.6.1,TODO: This is duplicated! Clean up
2.6.1,Updates charges in place
2.6.1,initial embedding
2.6.1,minimization and pruning
2.6.1,always keep lowest-energy conformer
2.6.1,discard conformers after max_conformers is reached
2.6.1,get RMSD to selected conformers
2.6.1,discard conformers within the RMSD threshold
2.6.1,create a new molecule to hold the chosen conformers
2.6.1,this ensures proper conformer IDs and energy-based ordering
2.6.1,False here specifies that water is to be removed
2.6.1,Updates charges in place
2.6.1,TODO: This is wrong. Should return all molecules
2.6.1,TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.6.1,This updates in place
2.6.1,indices of atoms to keep
2.6.1,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.6.1,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.6.1,nonzero contact.
2.6.1,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.6.1,####################################################
2.6.1,Compute partial charges on molecule if rdkit
2.6.1,####################################################
2.6.1,Number of voxels per one edge of box to voxelize.
2.6.1,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.6.1,If interval1 < interval2 entirely
2.6.1,If interval2 < interval1 entirely
2.6.1,Each triangle in the simplices is a set of 3 atoms from
2.6.1,coordinates which forms the vertices of an exterior triangle on
2.6.1,the convex hull of the macromolecule.
2.6.1,Points is the set of atom coordinates that make up this
2.6.1,triangular face on the convex hull
2.6.1,Let's extract x/y/z coords for this face
2.6.1,Let's compute min/max points
2.6.1,"Nitrogen has atomic number 7, and oxygen 8."
2.6.1,If atom is a hydrogen
2.6.1,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.6.1,If atom is a hydrogen
2.6.1,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.6.1,if ring from mol1 is aromatic
2.6.1,...and atom from mol2 is a cation
2.6.1,if angle and distance are correct
2.6.1,count atoms forming a contact
2.6.1,if ring is aromatic
2.6.1,"save its indices, center, and normal"
2.6.1,remember mol1-mol2 pairs we already counted
2.6.1,"if this pair is new, count atoms forming a contact"
2.6.1,"if this pair is new, count atoms forming a contact"
2.6.1,find interacting rings from mol1 and cations from mol2
2.6.1,find interacting cations from mol1 and rings from mol2
2.6.1,merge counters
2.6.1,the line has format
2.6.1,REMARK VINA RESULT: score ...
2.6.1,There is only 1 such line per model so we can append it
2.6.1,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.6.1,Apply common fixes to PDB files
2.6.1,Optimize ligand
2.6.1,Make sure input is a list
2.6.1,FIXME: Incompatible types in assignment
2.6.1,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.6.1,Ensure that metric is wrapped in a list.
2.6.1,This case checks if input is a function then wraps a
2.6.1,dc.metrics.Metric object around it
2.6.1,Process input metrics
2.6.1,Compute multitask metrics
2.6.1,We use y/w to aggregate labels/weights across generator.
2.6.1,This is a KerasModel.
2.6.1,Some datasets have weights
2.6.1,Process predictions and populate y/w lists
2.6.1,Combine labels/weights
2.6.1,Undo data transformations.
2.6.1,Compute multitask metrics
2.6.1,These functions have moved to deepchem.utils_docking_utils
2.6.1,flake8: noqa
2.6.1,The number of elements to print for dataset ids/tasks
2.6.1,"If a dataset contains more than this number of elements, it won't"
2.6.1,print any dataset ids
2.6.1,An activation function for a layer: either a function or the name of a standard activation
2.6.1,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.6.1,"A single value of some type, or multiple values of that type"
2.6.1,The shape of a NumPy array
2.6.1,"A NumPy array, or an object that can be converted to one.  Once we move to"
2.6.1,"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
2.6.1,type of RDKit object
2.6.1,type of Pymatgen object
2.6.1,Generate a random temporary file name
2.6.1,Ensure the file is created
2.6.1,Open the file in the given mode
2.6.1,Tasks are either in .sdf.csv file or in the .sdf file itself
2.6.1,Structures are stored in .sdf file
2.6.1,Reset aggregator
2.6.1,Handle final leftovers for this file
2.6.1,First line of user-specified CSV *must* be header.
2.6.1,"If gzipped, need to compute extension again"
2.6.1,First line of user-specified CSV *must* be header.
2.6.1,The label encoder is given characters for ACGTN
2.6.1,Peak at the first sequence to get the length of the sequence.
2.6.1,init an one-hot vector
2.6.1,"If include_unknown_set is True, set the last index is 1."
2.6.1,################################################################
2.6.1,atom (node) featurization
2.6.1,################################################################
2.6.1,################################################################
2.6.1,bond (edge) featurization
2.6.1,################################################################
2.6.1,One sequence has length longer than others. This should throw a
2.6.1,ValueError.
2.6.1,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.6.1,Loosening atol to see if tests stop failing sporadically
2.6.1,string set
2.6.1,integer set
2.6.1,include_unknown_set is False
2.6.1,include_unknown_set is True
2.6.1,check unknown atoms
2.6.1,check original set
2.6.1,"Generally, =O behaves as an electron acceptor"
2.6.1,we must compute partial charges before using `get_atom_partial_charge`
2.6.1,The C-N bond is a single bond
2.6.1,TODO test more formats for ligand
2.6.1,TODO test more formats for ligand
2.6.1,adding hydrogens and charges is tested in dc.utils
2.6.1,self.ligand_file is for 3ws9_ligand.sdf
2.6.1,simple flat ring
2.6.1,self.cycle4.Compute2DCoords()
2.6.1,load and sanitize two real molecules
2.6.1,parallel normals
2.6.1,perpendicular normals
2.6.1,too far away
2.6.1,perpendicular normals
2.6.1,parallel normals
2.6.1,too far away
2.6.1,order of the molecules shouldn't matter
2.6.1,with this criteria we should find both types of stacking
2.6.1,parallel normals
2.6.1,perpendicular normals
2.6.1,too far away
2.6.1,def test_compute_cation_pi(self):
2.6.1,"# TODO(rbharath): find better example, currently dicts are empty"
2.6.1,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.6.1,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.6.1,"TODO find better example, currently dicts are empty"
2.6.1,TODO test more formats for ligand
2.6.1,Test on RDKit
2.6.1,3D vector with unit length
2.6.1,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.6.1,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.6.1,check if correct distance metric was used
2.6.1,Construct a random class probability matrix
2.6.1,Construct a random class probability matrix
2.6.1,"Note that since no name as provided, metrics are index by order"
2.6.1,given.
2.6.1,"Note that since no name as provided, metrics are index by order"
2.6.1,given.
2.6.1,"Note that since no name as provided, metrics are index by order"
2.6.1,given.
2.6.1,"Note that since no name as provided, metrics are index by order"
2.6.1,given.
2.6.1,TODO: Fix this case with correct thresholding
2.6.1,TODO: Fix this case with correct thresholding
2.6.1,There are 4 faces to the shape created by coords
2.6.1,flake8: noqa
2.6.1,Get the degree id list (which corrects for min_deg)
2.6.1,Get the size of each degree block
2.6.1,Get the the start indices for items in each block
2.6.1,Get the node indices when they are reset when the degree changes
2.6.1,Convert to numpy array
2.6.1,Reorder old atom_features
2.6.1,Reorder old deg lists
2.6.1,Sort membership
2.6.1,Create old to new dictionary. not exactly intuitive
2.6.1,Reorder adjacency lists
2.6.1,Get numpy version of degree list for indexing
2.6.1,"Initialize adj_lists, which supports min_deg = 1 only"
2.6.1,Parse as deg separated
2.6.1,Get indices corresponding to the current degree
2.6.1,Extract and save adjacency list for the current degree
2.6.1,Construct the slice information
2.6.1,Get the cumulative indices after the first index
2.6.1,Set indices with zero sized slices to zero to avoid indexing errors
2.6.1,TODO(rbharath): Can this be removed?
2.6.1,Use random insted of zeros to prevent weird issues with summing to zero
2.6.1,"Combine the features, then sort them by (atom_degree, mol_index)"
2.6.1,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.6.1,Create a map from the original atom indices within each molecule to the
2.6.1,indices in the combined object.
2.6.1,Sort all atoms by degree.
2.6.1,"Get the size of each atom list separated by molecule id, then by degree"
2.6.1,Get the final size of each degree block
2.6.1,"Get the index at which each degree starts, not resetting after each degree"
2.6.1,And not stopping at any specific molecule
2.6.1,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.6.1,first column telling the start indices of each degree block and the
2.6.1,second colum telling the size of each degree block
2.6.1,Determine the membership (atom i belongs to molecule membership[i])
2.6.1,Initialize the new degree separated adjacency lists
2.6.1,Update the old adjacency lists with the new atom indices and then combine
2.6.1,all together
2.6.1,Iterate through all the molecules
2.6.1,Get the adjacency lists for this molecule and current degree id
2.6.1,"Correct all atom indices to the final indices, and then save the"
2.6.1,results into the new adjacency lists
2.6.1,Increment once row is done
2.6.1,Get the final aggregated molecule
2.6.1,"Requriments - transformers, tokenizers"
2.6.1,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.6.1,The vocab may be expanded in the near future
2.6.1,add vocab_file dict
2.6.1,"unk_token=""[UNK]"","
2.6.1,"sep_token=""[SEP]"","
2.6.1,"pad_token=""[PAD]"","
2.6.1,"cls_token=""[CLS]"","
2.6.1,"mask_token=""[MASK]"","
2.6.1,take into account special tokens in max length
2.6.1,flake8: noqa
2.6.1,Initalize with 1
2.6.1,Replace the hybridization
2.6.1,global possible_hybridization_list
2.6.1,Allow 0 index to correspond to null molecule 1
2.6.1,Correct for null
2.6.1,"print(6-k-1, id)"
2.6.1,Correct for last one
2.6.1,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.6.1,"Handle edge case of self-pairs (i, i)"
2.6.1,Increment by 1 since we don't want 0-indexing
2.6.1,"This creates a matrix of shape (2, num_pairs)"
2.6.1,Get mapping
2.6.1,first `bt_len` features are bond features(if applicable)
2.6.1,For ring pairs outside max pairs distance continue
2.6.1,`bt_len`-th feature is if the pair of atoms are in the same ring
2.6.1,graph distance between two atoms
2.6.1,distance is a matrix of 1-hot encoded distances for all atoms
2.6.1,For ring pairs outside max pairs distance continue
2.6.1,Euclidean distance between atoms
2.6.1,atoms `radial` bonds away from `a1`
2.6.1,atoms less than `radial` bonds away
2.6.1,find atoms `radial`+1 bonds away
2.6.1,create temporary valid ids serving to filter out failed featurizations from every sublist
2.6.1,"of features (i.e. every molecules' frags list), and also totally failed sublists."
2.6.1,This makes output digestable by Loaders
2.6.1,Get the node features
2.6.1,Stack nodes into an array
2.6.1,Get bond lists with reverse edges included
2.6.1,Get canonical adjacency list
2.6.1,"Distance is either graph distance(True) or Euclidean distance(False,"
2.6.1,only support datasets providing Cartesian coordinates)
2.6.1,Set dtype
2.6.1,If includes explicit hydrogens
2.6.1,If uses use_chirality
2.6.1,Atom features
2.6.1,Stack nodes into an array
2.6.1,Get bond lists
2.6.1,Get canonical adjacency list
2.6.1,Calculate pair features
2.6.1,the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
2.6.1,"SMILES is unique, so set a canonical order of atoms"
2.6.1,Add hydrogens and generate a conformation.
2.6.1,Record properties of the molecules.
2.6.1,Create the output object.
2.6.1,"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
2.6.1,flake8: noqa
2.6.1,base classes for featurizers
2.6.1,molecule featurizers
2.6.1,complex featurizers
2.6.1,material featurizers
2.6.1,tokenizers
2.6.1,support classes
2.6.1,for str
2.6.1,for list
2.6.1,validation
2.6.1,skip list
2.6.1,skip path string
2.6.1,main logic
2.6.1,Find a successful featurization
2.6.1,Replace failed featurizations with appropriate array
2.6.1,Special case handling of single molecule
2.6.1,Convert iterables to list
2.6.1,"mol must be a RDKit Mol object, so parse a SMILES"
2.6.1,"SMILES is unique, so set a canonical order of atoms"
2.6.1,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.6.1,atom_name is of format RESX-ATOMTYPE
2.6.1,where X is a 1 to 4 digit number
2.6.1,validate params
2.6.1,This assumes that the edge features for self loops are full-zero tensors
2.6.1,In the future we may want to support featurization for self loops
2.6.1,stack features
2.6.1,"before stacking edge_features or node_pos_features,"
2.6.1,we should check whether these are None or not
2.6.1,create new edge index
2.6.1,graph_index indicates which nodes belong to which graph
2.6.1,Setup image
2.6.1,Compute bond properties
2.6.1,Compute atom properties
2.6.1,Setup image
2.6.1,Compute bond properties
2.6.1,Compute atom properties
2.6.1,Reshape done for proper broadcast
2.6.1,"Reshapes, and axes manipulations to facilitate vector processing."
2.6.1,Draw a line between the two atoms.
2.6.1,"The coordinates of this line, are indicated in line_coords"
2.6.1,Turn the line coordinates into image positions
2.6.1,Turn atomic coordinates into image positions
2.6.1,Set the bond line coordinates to the bond property used.
2.6.1,Set the atom positions in image to different atomic properties in channels
2.6.1,With fixed res and img_size some molecules (e.g. long chains) may not fit.
2.6.1,Check whether num_confs >=1 or not
2.6.1,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.6.1,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.6.1,consistent with most QM software packages.
2.6.1,generate SMILES for fragments
2.6.1,Extend shorter strings with padding
2.6.1,Padding before and after
2.6.1,Featurize data using featurize() in parent class
2.6.1,Featurize str data
2.6.1,Featurize mol data
2.6.1,load pretrained models
2.6.1,convert errors to zero
2.6.1,flake8: noqa
2.6.1,If partial charges were not computed
2.6.1,construct atom (node) feature
2.6.1,construct edge (bond) index
2.6.1,add edge list considering a directed graph
2.6.1,construct edge (bond) feature
2.6.1,The 1.0 float value represents True Boolean
2.6.1,This will return a boolean vector with all entries False
2.6.1,To get the shortest paths between two nodes.
2.6.1,To get info if two nodes belong to the same ring.
2.6.1,Featurizer
2.6.1,initialize
2.6.1,check initialization
2.6.1,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.6.1,Check whether num_confs >=1 or not
2.6.1,Convert AtomPositions from Angstrom to bohr (atomic units)
2.6.1,"`(1, max_atoms)` -> `(max_atoms,)`"
2.6.1,bond labels
2.6.1,atom labels
2.6.1,create bond encoders and decoders
2.6.1,create atom encoders and decoders
2.6.1,Special case handling of single molecule
2.6.1,Convert iterables to list
2.6.1,Set up site environment matcher
2.6.1,Graphical option
2.6.1,tolerance for grouping nodes
2.6.1,determine minimum distance between sitetypes.
2.6.1,This is used to determine the existence of an edge
2.6.1,Sort by bond
2.6.1,You want to maximize this in order to make sure every node gets an edge
2.6.1,construct graph
2.6.1,matcher options
2.6.1,construct graph
2.6.1,Add nodes
2.6.1,Add edge. distance is edge attribute
2.6.1,construct graph
2.6.1,Gets the isomorphic mapping. Also the most time consuming part of the code
2.6.1,reconstruct graph after alinging point order
2.6.1,RMSD
2.6.1,Construct one hot encoding
2.6.1,get mapping between all site index to active site index
2.6.1,Get Neighbors
2.6.1,Read Data
2.6.1,get map between two environment
2.6.1,align input to the primitive cell (reference)
2.6.1,apply permutations
2.6.1,remove spectators
2.6.1,map it to active sites
2.6.1,Extract the right number of sites by distance
2.6.1,if PBC condition is fulfilled..
2.6.1,Get full N x N SCM
2.6.1,flake8: noqa
2.6.1,load atom_init.json
2.6.1,check whether the atom feature exists or not
2.6.1,construct bi-directed graph
2.6.1,Increase dimension of distance tensor and apply filter
2.6.1,We compute pairwise contact fingerprints
2.6.1,We compute pairwise contact fingerprints
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.1,We compute pairwise contact fingerprints
2.6.1,"rdks = [frag1[1], frag2[1]]"
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.1,We compute pairwise contact fingerprints
2.6.1,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.6.1,"rdks = [frag1[1], frag2[1]]"
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.1,We compute pairwise contact fingerprints
2.6.1,"rdks = [frag1[1], frag2[1]]"
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.6.1,We compute pairwise contact fingerprints
2.6.1,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.6.1,We compute pairwise contact fingerprints
2.6.1,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.6.1,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.6.1,"xyzs = [frag1_xyz, frag2_xyz]"
2.6.1,"rdks = [frag1[1], frag2[1]]"
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.1,We compute pairwise contact fingerprints
2.6.1,"rdks = [frag1[1], frag2[1]]"
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.1,check if user tries to set removed arguments
2.6.1,list of features that require sanitized molecules
2.6.1,not implemented featurization types
2.6.1,default values
2.6.1,update with cutoffs specified by the user
2.6.1,"each entry is a tuple (is_flat, feature_name)"
2.6.1,list of features that cannot be calculated with specified parameters
2.6.1,this list is used to define <flat/voxel/all>_combined subset
2.6.1,parse provided feature types
2.6.1,flake8: noqa
2.6.1,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.6.1,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.6.1,nonzero contact.
2.6.1,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.6.1,We compute pairwise contact fingerprints
2.6.1,Get coordinates
2.6.1,We compute pairwise contact fingerprints
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.6.1,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.6.1,axis.
2.6.1,Type of data created by this featurizer
2.6.1,TODO(rbharath): Should this return a list?
2.6.1,Type of data created by this featurizer
2.6.1,Currently handles loading failures by returning None
2.6.1,TODO: Is there a better handling procedure?
2.6.1,pad outputs
2.6.1,Deprecation warnings for old atomic conv featurizer name #
2.6.1,We compute pairwise contact fingerprints
2.6.1,Get coordinates
2.6.1,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.6.1,We compute pairwise contact fingerprints
2.6.1,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.1,decode the source in the mixed and separated cases
2.6.1,TODO test more formats for ligand
2.6.1,TODO test more formats for ligand
2.6.1,with one conformer
2.6.1,with multiple conformers
2.6.1,include explicit hydrogens
2.6.1,with one conformer
2.6.1,with multiple conformers
2.6.1,include explicit hydrogens
2.6.1,"Requirements - transformers, tokenizers"
2.6.1,"assert ""C1=CC=CN=C1"""
2.6.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.1,"assert ""C1=CC=CN=C1"""
2.6.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.1,"assert ""C1=CC=CN=C1"""
2.6.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.1,"assert ""C1=CC=CN=C1"""
2.6.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.1,"assert ""C1=CC=CN=C1"""
2.6.1,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.1,check for separate count and SMILES entries for each fragment
2.6.1,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.6.1,max num atoms instead of exact.
2.6.1,Cutoff in angstroms
2.6.1,"Coords are padded, neighbor list and Z are not"
2.6.1,"# TODO: This is failing, something about the hydrogen bond counting?"
2.6.1,def test_hydrogen_bond_counter():
2.6.1,current_dir = os.path.dirname(os.path.realpath(__file__))
2.6.1,"protein_file = os.path.join(current_dir, 'data',"
2.6.1,'3ws9_protein_fixer_rdkit.pdb')
2.6.1,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.6.1,
2.6.1,cutoff = 4.5
2.6.1,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.6.1,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.6.1,# TODO: Add shape test
2.6.1,
2.6.1,
2.6.1,"# TODO: This is failing, something about the hydrogen bond counting?"
2.6.1,def test_hydrogen_bond_voxelizer():
2.6.1,current_dir = os.path.dirname(os.path.realpath(__file__))
2.6.1,"protein_file = os.path.join(current_dir, 'data',"
2.6.1,'3ws9_protein_fixer_rdkit.pdb')
2.6.1,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.6.1,
2.6.1,cutoff = 4.5
2.6.1,box_width = 16
2.6.1,voxel_width = 1.0
2.6.1,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.6.1,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.6.1,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.6.1,# TODO: Add shape test
2.6.1,@pytest.mark.linux_only
2.6.1,test if default parameters work
2.6.1,check if use-case from examples works
2.6.1,test if input is flattened when flat features are used
2.6.1,test voxel features
2.6.1,test flat features
2.6.1,check if aromatic features are ignored if sanitize=False
2.6.1,test flattened voxel features
2.6.1,test voxel features
2.6.1,test flat features
2.6.1,test rotations
2.6.1,not support array style inputs
2.6.1,check convert function
2.6.1,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.6.1,of degree 1 (connected only to central nitrogen).
2.6.1,5 atoms in compound
2.6.1,Get the adjacency lists grouped by degree
2.6.1,The 4 outer atoms connected to central nitrogen
2.6.1,Central nitrogen connected to everything else.
2.6.1,Only one carbon
2.6.1,"No bonds, so degree adjacency lists are empty"
2.6.1,3 carbonds in alkane
2.6.1,Outer two carbonds are connected to central carbon
2.6.1,Central carbon connected to outer two
2.6.1,test featurization
2.6.1,test defeaturization
2.6.1,sanity check; see if something weird does not happen with rdkit
2.6.1,check if original smiles match defeaturized smiles
2.6.1,sanity check; see if something weird does not happen with rdkit
2.6.1,test featurization
2.6.1,test defeaturization
2.6.1,check if original smiles match defeaturized smiles
2.6.1,untransform
2.6.1,untranform
2.6.1,untranform
2.6.1,untranform
2.6.1,untranform
2.6.1,Check the SDF file.
2.6.1,Check the PDB file.
2.6.1,Check the SMILES string.
2.6.1,Do a manual distance computation and make
2.6.1,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.6.1,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.6.1,Do a manual distance computation and ensure that selected neighbor is
2.6.1,closest since we set max_num_neighbors = 1
2.6.1,Carbon
2.6.1,Test distance 1
2.6.1,Test distance 2
2.6.1,Test alkane
2.6.1,Test distance 1
2.6.1,3 self connections and 2 bonds which are both counted twice because of
2.6.1,symmetry for 7 total
2.6.1,Test distance 2
2.6.1,Everything is connected at this distance
2.6.1,Test alkane
2.6.1,Test distance infinity
2.6.1,Everything is connected at this distance
2.6.1,Test pentane
2.6.1,Test distance infinity
2.6.1,Everything is connected at this distance
2.6.1,Only one carbon
2.6.1,Test feature sizes
2.6.1,"No bonds, so only 1 pair feature (for the self interaction)"
2.6.1,Only 4 atoms
2.6.1,Test feature sizes for chirality
2.6.1,3 carbonds in alkane
2.6.1,Test feature sizes
2.6.1,Should be a 3x3 interaction grid
2.6.1,mol_list = featurizer.featurize(mols)
2.6.1,mol = mol_list[0]
2.6.1,3 carbonds in alkane
2.6.1,Test feature sizes
2.6.1,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.6.1,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.6.1,symmetry)
2.6.1,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.6.1,of degree 1 (connected only to central nitrogen).
2.6.1,import rdkit.Chem
2.6.1,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.6.1,5 atoms in compound
2.6.1,Test feature sizes
2.6.1,Should be a 3x3 interaction grid
2.6.1,Artificial feature array.
2.6.1,0 atoms of degree 0
2.6.1,0 atoms of degree 1
2.6.1,4 atoms of degree 2
2.6.1,0 atoms of degree 3
2.6.1,0 atoms of degree 4
2.6.1,0 atoms of degree 5
2.6.1,0 atoms of degree 6
2.6.1,0 atoms of degree 7
2.6.1,0 atoms of degree 8
2.6.1,0 atoms of degree 9
2.6.1,0 atoms of degree 10
2.6.1,atom 4 has 0 neighbors
2.6.1,atom 0 has 2 neighbors
2.6.1,atom 1 has 2 neighbors
2.6.1,atom 2 has 2 neighbors
2.6.1,atom 3 has 3 neighbors.
2.6.1,Verify that atom features have been sorted by atom degree.
2.6.1,Sorting is done by atom degree as before. So the ordering goes
2.6.1,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.6.1,from new position to old position is
2.6.1,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.6.1,list respects this reordering and returns correct adjacency list.
2.6.1,First example molecule
2.6.1,Artificial feature array.
2.6.1,Second example molecule
2.6.1,Third example molecule
2.6.1,Test agglomerate molecule method
2.6.1,No atoms of degree 0
2.6.1,3 atoms of degree 1
2.6.1,8 atoms of degree 2
2.6.1,1 atom of degree 3
2.6.1,0 atoms of degree 4
2.6.1,0 atoms of degree 5
2.6.1,Check that atoms are only connected to themselves.
2.6.1,Check that there's one atom of each degree.
2.6.1,calculate coordinates
2.6.1,not zero values
2.6.1,assumes that every array is of the same dimension
2.6.1,rem_dataset is remaining portion of dataset
2.6.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.6.1,to k-1.
2.6.1,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.6.1,validation
2.6.1,skip list
2.6.1,skip path string
2.6.1,main logic
2.6.1,for str
2.6.1,for list
2.6.1,dict is needed in case groups aren't strictly flattened or
2.6.1,hashed by something non-integer like
2.6.1,Figure out how many positive samples we want for each task in each dataset.
2.6.1,Assign the positive samples to datasets.  Since a sample may be positive
2.6.1,"on more than one task, we need to keep track of the effect of each added"
2.6.1,"sample on each task.  To try to keep everything balanced, we cycle through"
2.6.1,"tasks, assigning one positive sample for each one."
2.6.1,We have a sample that hasn't been assigned yet.  Assign it to
2.6.1,whichever set currently has the lowest fraction of its target for
2.6.1,this task.
2.6.1,The remaining samples are negative for all tasks.  Add them to fill out
2.6.1,each set to the correct total number.
2.6.1,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.6.1,JSG Assert that split fractions can be written as proper fractions over 10.
2.6.1,This can be generalized in the future with some common demoninator determination.
2.6.1,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.6.1,Append remaining examples to train
2.6.1,################################################################
2.6.1,Splitter for molecule datasets
2.6.1,################################################################
2.6.1,Sort by increasing MW
2.6.1,calcaulate scaffold sets
2.6.1,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.6.1,Compute fingerprints for all molecules.
2.6.1,Split into two groups: training set and everything else.
2.6.1,Split the second group into validation and test sets.
2.6.1,Begin by assigning the first molecule to the first group.
2.6.1,Decide which group to assign a molecule to.
2.6.1,Identify the unassigned molecule that is least similar to everything in
2.6.1,the other group.
2.6.1,Add it to the group.
2.6.1,Update the data on unassigned molecules.
2.6.1,Sort from largest to smallest scaffold sets
2.6.1,################################################################
2.6.1,Not well supported splitters
2.6.1,################################################################
2.6.1,All datasets share features and identifiers by assumption.
2.6.1,flake8: noqa
2.6.1,basic splitter
2.6.1,molecule splitter
2.6.1,other splitter
2.6.1,################################################################
2.6.1,Removed API
2.6.1,################################################################
2.6.1,Note that the extra task goes to test
2.6.1,Number tasks per fold
2.6.1,Find the tasks that correspond to this test fold
2.6.1,Assert that all arrays look like they should
2.6.1,"task_type = ""regression"""
2.6.1,0 1 2 3 4 5 6 7 8 9
2.6.1,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.6.1,data. Make a test for properly splitting of sharded data. Perhaps using
2.6.1,reshard() to handle this?
2.6.1,Verify lengths is 10/k == 2
2.6.1,Verify that compounds in this fold are subset of original compounds
2.6.1,Verify that no two folds have overlapping compounds.
2.6.1,Verify lengths is 10/k == 2
2.6.1,Verify that compounds in this fold are subset of original compounds
2.6.1,Verify that no two folds have overlapping compounds.
2.6.1,Verify lengths is 10/k == 2
2.6.1,Verify that compounds in this fold are subset of original compounds
2.6.1,Verify that no two folds have overlapping compounds.
2.6.1,Test singletask case.
2.6.1,The split index should partition dataset in half.
2.6.1,Test singletask case.
2.6.1,Test case where some weights are zero (i.e. masked)
2.6.1,Set half the positives to have zero weight
2.6.1,There are 10 nonzero actives.
2.6.1,"The split index should partition this into half, so expect 5"
2.6.1,The split index should partition the positives for each task roughly in half.
2.6.1,Mask half the examples
2.6.1,The split index should partition dataset in half.
2.6.1,Test singletask case.
2.6.1,Should have split cleanly in half (picked random seed to ensure this)
2.6.1,Check positives are correctly distributed
2.6.1,Test singletask case.
2.6.1,Should have made an 80/10/10 train/valid/test split of actives.
2.6.1,Verify lengths is 100/k == 20
2.6.1,Note: This wouldn't work for multitask str
2.6.1,assert len(fold_dataset) == n_samples/K
2.6.1,Verify that each fold has n_positives/K = 4 positive examples.
2.6.1,Verify that compounds in this fold are subset of original compounds
2.6.1,Verify that no two folds have overlapping compounds.
2.6.1,The amount of datapoints has to be the same
2.6.1,The number of scaffolds generated by the splitter
2.6.1,has to be smaller or equal than number of total molecules
2.6.1,Add the input features.
2.6.1,Add the convolutional layers
2.6.1,edges logits used during training
2.6.1,nodes logits used during training
2.6.1,edges logits
2.6.1,nodes logits
2.6.1,training of the model
2.6.1,generating compounds
2.6.1,nodes logits used during compound generation
2.6.1,Create the inputs.
2.6.1,Create the generators.
2.6.1,Create the discriminators.
2.6.1,Compute the loss functions.
2.6.1,Create learnable weights for the generators and discriminators.
2.6.1,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.6.1,Compute the weighted errors
2.6.1,Add an entropy term to the loss.
2.6.1,Create the Keras model.
2.6.1,"Every call to fit_generator() will increment global_step, but we only"
2.6.1,"want it to get incremented once for the entire batch, so record the"
2.6.1,value and keep resetting it.
2.6.1,Train the discriminator.
2.6.1,Train the generator.
2.6.1,Write checkpoints and report progress.
2.6.1,Write out final results.
2.6.1,Chain of flows is also a normalizing flow
2.6.1,An instance of tfd.TransformedDistribution
2.6.1,TODO: Incompability between TF and TFP means that TF doesn't track
2.6.1,trainable variables in the flow; must override `_create_gradient_fn`
2.6.1,self._variables = self.flow.trainable_variables
2.6.1,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.6.1,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.6.1,This is for API consistency
2.6.1,extended one of probabilites to binary distribution
2.6.1,extended one of probabilites to binary distribution
2.6.1,-*- coding: utf-8 -*-
2.6.1,"Shape (N_atoms, M_nbrs, ndim)"
2.6.1,"Shape (N_atoms, M_nbrs, ndim)"
2.6.1,"Shape (N_atoms, M_nbrs)"
2.6.1,Generate the nb_affine weights and biases
2.6.1,Extract atom_features
2.6.1,Extract graph topology
2.6.1,Sum all neighbors using adjacency matrix
2.6.1,Get collection of modified atom features
2.6.1,Obtain relevant atoms for this degree
2.6.1,Get self atoms
2.6.1,Apply hidden affine to relevant atoms and append
2.6.1,Determine the min_deg=0 case
2.6.1,Only use the self layer
2.6.1,Combine all atoms back into the list
2.6.1,Tensorflow correctly processes empty lists when using concat
2.6.1,"Sum along neighbors as well as self, and store"
2.6.1,Perform the mol gather
2.6.1,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.6.1,"self.max_degree, self.min_degree)"
2.6.1,Tensorflow correctly processes empty lists when using concat
2.6.1,Get self atoms
2.6.1,"There are no neighbors of this degree, so just create an empty tensor directly."
2.6.1,Expand dims
2.6.1,always deg-1 for deg_adj_lists
2.6.1,Extract graph topology
2.6.1,means that this is second loop of convolution
2.6.1,No other forget biases supported right now.
2.6.1,Taken from Keras code [citation needed]
2.6.1,"x is test set, xp is support set."
2.6.1,Get initializations
2.6.1,Process using attention
2.6.1,"Eqn (4), appendix A.1 of Matching Networks paper"
2.6.1,Generate new attention states
2.6.1,Support set lstm
2.6.1,Test lstm
2.6.1,Get initializations
2.6.1,Rename support
2.6.1,Process support xp using attention
2.6.1,Get linear combination of support set
2.6.1,Process test x using attention
2.6.1,Generate new support attention states
2.6.1,Generate new test attention states
2.6.1,Redefine
2.6.1,Number of rotatable bonds
2.6.1,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.6.1,a difference.
2.6.1,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.6.1,neighbors lists an argument instead of a part of this layer.
2.6.1,"Shape (N, M)"
2.6.1,"Shape (N, M)"
2.6.1,"Shape (N, M)"
2.6.1,Number of grid cells
2.6.1,TODO(rbharath): Support batching
2.6.1,"Shape (n_cells, ndim)"
2.6.1,"List of length N_atoms, each element of different length uniques_i"
2.6.1,"List of length N_atoms, each element of different length uniques_i"
2.6.1,"List of length N_atoms, each a tensor of shape"
2.6.1,"(uniques_i, ndim)"
2.6.1,Add phantom atoms that exist far outside the box
2.6.1,"List of length N_atoms, each of shape (1, ndim)"
2.6.1,TODO(rbharath): How does distance need to be modified here to
2.6.1,account for periodic boundary conditions?
2.6.1,List of length N_atoms each of shape (M_nbrs)
2.6.1,"N_atoms elts of size (M_nbrs,) each"
2.6.1,"Shape (N_atoms, 1)"
2.6.1,Find M_nbrs atoms closest to each cell
2.6.1,"Shape (n_cells, M_nbrs)"
2.6.1,Associate each cell with its neighbor cells. Assumes periodic boundary
2.6.1,"conditions, so does wrapround. O(constant)"
2.6.1,"Shape (n_cells, n_nbr_cells)"
2.6.1,"Shape (N_atoms, n_nbr_cells)"
2.6.1,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.6.1,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.6.1,"List of length N_atoms, each element length uniques_i"
2.6.1,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.6.1,element removed to remove self from list of neighbors. Need to verify
2.6.1,this holds more broadly or come up with robust alternative.
2.6.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.6.1,"Shape (N_atoms*n_cells, ndim) after tile"
2.6.1,Shape (N_atoms*n_cells)
2.6.1,"Shape (n_cells, N_atoms)"
2.6.1,Find k atoms closest to this cell. Notice negative sign since
2.6.1,tf.nn.top_k returns *largest* not smallest.
2.6.1,"Tensor of shape (n_cells, M_nbrs)"
2.6.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.6.1,"Shape (N_atoms*n_cells, 1) after tile"
2.6.1,9 neighbors in 2-space
2.6.1,TODO(rbharath): Shoddy handling of higher dimensions...
2.6.1,Number of cells for cube in 3-space is
2.6.1,TODO(rbharath): Do we need to handle periodic boundary conditions
2.6.1,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.6.1,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.6.1,the cube.
2.6.1,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.6.1,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.6.1,"Tile (a, a, a, b, b, b, etc.)"
2.6.1,"Tile (a, b, c, a, b, c, ...)"
2.6.1,N: Maximum number of atoms
2.6.1,M: Maximum number of neighbors
2.6.1,d: Number of coordinates/features/filters
2.6.1,B: Batch Size
2.6.1,Compute the distances and radial symmetry functions.
2.6.1,check that there isnt just one or zero inputs
2.6.1,create subspaces
2.6.1,"concatenate subspaces, reshape to size of original input, then stack"
2.6.1,"such that out_tensor has shape (2,?,original_cols)"
2.6.1,creates subspaces the same way it was done in AlphaShare
2.6.1,calculate squared Frobenius norm
2.6.1,"(TODO YTZ:) faster, less memory intensive way"
2.6.1,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.6.1,"r = tf.expand_dims(r, -1)"
2.6.1,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.6.1,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.6.1,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.6.1,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.6.1,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.6.1,Calculate pairwise distance
2.6.1,Cutoff with threshold Rc
2.6.1,return d
2.6.1,tf.stack issues again...
2.6.1,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.6.1,So the Tensor has known dimensions
2.6.1,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.6.1,normalization
2.6.1,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.6.1,and embeddings of atom j(both gone through a hidden layer)
2.6.1,"for atom i, sum the influence from all other atom j in the molecule"
2.6.1,number of inputs each step
2.6.1,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.6.1,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.6.1,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.6.1,`count`-th step
2.6.1,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.6.1,generating index for graph features used in the inputs
2.6.1,"extracting graph features for parents of the target atoms, then flatten"
2.6.1,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.6.1,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.6.1,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.6.1,of shape: (batch_size*max_atoms) * n_graph_features
2.6.1,representing the graph features of target atoms in each graph
2.6.1,index for targe atoms
2.6.1,Extract atom_features
2.6.1,sum all graph outputs
2.6.1,"Default message function: edge network, update function: GRU"
2.6.1,more options to be implemented
2.6.1,Add another value(~-Inf) to prevent error in softmax
2.6.1,Model using this layer must set pad_batches=True
2.6.1,Perform one step of LSTM
2.6.1,task_metadata_rows = {task: [] for task in tasks}
2.6.1,Extract those datapoints which are present for this task
2.6.1,Loading is done on-the-fly
2.6.1,Build the model.
2.6.1,Final atom-layer convolution. Note this differs slightly from the paper
2.6.1,since we use a tanh activation as default. This seems necessary for numerical
2.6.1,stability.
2.6.1,Now fully connected layers
2.6.1,Should this allow for training?
2.6.1,"pair_edges is of shape (2, N)"
2.6.1,number of atoms in each molecule
2.6.1,index of pair features
2.6.1,Get starting pair atoms
2.6.1,number of pairs for each atom
2.6.1,atom features
2.6.1,pair features
2.6.1,Build the model.
2.6.1,Build the model.
2.6.1,calculation orders for a batch of molecules
2.6.1,padding atom features vector of each molecule with 0
2.6.1,Build the model.
2.6.1,number of atoms in each molecule
2.6.1,index of pair features
2.6.1,number of pairs for each atom
2.6.1,atom features
2.6.1,pair features
2.6.1,################### Deprecation warnings for renamed TensorGraph models ####################
2.6.1,Add the input features.
2.6.1,Add the shared dense layers
2.6.1,Add task-specific bypass layers
2.6.1,Add the input features.
2.6.1,Add the shared dense layers
2.6.1,Add task-specific bypass layers
2.6.1,W&B flag support (DEPRECATED)
2.6.1,"If `wandb=True` and no logger is provided, initialize default logger"
2.6.1,Setup and initialize W&B logging
2.6.1,Update config with KerasModel params
2.6.1,Backwards compatibility
2.6.1,The optimizer creates internal variables the first time apply_gradients()
2.6.1,is called for a new set of variables.  If that happens inside a function
2.6.1,"annotated with tf.function it throws an exception, so call it once here."
2.6.1,Main training loop.
2.6.1,"Execute the loss function, accumulating the gradients."
2.6.1,Report progress and write checkpoints.
2.6.1,Capture the last avg_loss in case of return since we're resetting to
2.6.1,0 now
2.6.1,Report final results.
2.6.1,Invoke the model.
2.6.1,Apply tranformers and record results.
2.6.1,Concatenate arrays to create the final results.
2.6.1,Use a GradientTape to compute gradients.
2.6.1,Ensure weights for both models are built.
2.6.1,Define the PyTorch Module that implements the model.
2.6.1,Define the PyTorch Module that implements the model.
2.6.1,Run fit transformers on dummy dataset to determine n_features after transformation
2.6.1,set wandb init arguments
2.6.1,Dataset ids are used to differentiate datasets seen by the logger
2.6.1,log data
2.6.1,Similarity values
2.6.1,Labels for all top K similar samples
2.6.1,Discard any padded predictions
2.6.1,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.6.1,Build the model.
2.6.1,Character embedding
2.6.1,Multiple convolutional layers with different filter widths
2.6.1,Max-over-time pooling
2.6.1,Concat features from all filters(one feature per filter)
2.6.1,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.6.1,SMILES strings
2.6.1,Maximum length is expanded to allow length variation during train and inference
2.6.1,'_' served as delimiter and padding
2.6.1,Initialize common characters as keys
2.6.1,Include space to avoid extra keys
2.6.1,"For 'Cl', 'Br', etc."
2.6.1,"Character not recognized, add to extra_keys"
2.6.1,Add all extra_keys to char_dict
2.6.1,Transform SMILES sequence to integers
2.6.1,Skip all spaces
2.6.1,"For 'Cl', 'Br', etc."
2.6.1,Padding with '_'
2.6.1,################### Deprecation warnings for renamed TensorGraph models ####################
2.6.1,"layer_sizes=[32, 32, 16],"
2.6.1,Add the dense layers
2.6.1,Do a simple greedy search.
2.6.1,Do a beam search with length normalization.
2.6.1,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.6.1,This candidate sequence has already been terminated
2.6.1,Consider all possible tokens we could add to this candidate sequence.
2.6.1,Add the input features.
2.6.1,Handle output layer
2.6.1,Iterate over all previous tasks.
2.6.1,prev_layers is a list with elements of size
2.6.1,"(batch_size, layer_sizes[i-1])"
2.6.1,Log data to Wandb
2.6.1,flake8: noqa
2.6.1,Tensorflow Depedency Models
2.6.1,scikit-learn model
2.6.1,PyTorch models
2.6.1,Jax models
2.6.1,####################################################################################
2.6.1,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.6.1,####################################################################################
2.6.1,#######################################################################################
2.6.1,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.6.1,#######################################################################################
2.6.1,Last layer sequences not returned.
2.6.1,This is needed because ImageDataGenerator does infinite looping
2.6.1,"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
2.6.1,but turns out to be slightly faster
2.6.1,JAX depend
2.6.1,Main training loop
2.6.1,Capture the last avg_loss in case of return since we're resetting to 0 now
2.6.1,Report final results.
2.6.1,Apply tranformers and record results.
2.6.1,Concatenate arrays to create the final results.
2.6.1,"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
2.6.1,") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
2.6.1,""""""""
2.6.1,"Predict the model's outputs, along with the uncertainty in each one."
2.6.1,The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
2.6.1,It involves repeating the prediction many times with different dropout masks.
2.6.1,The prediction is computed as the average over all the predictions.  The
2.6.1,uncertainty includes both the variation among the predicted values (epistemic
2.6.1,uncertainty) and the model's own estimates for how well it fits the data
2.6.1,(aleatoric uncertainty).  Not all models support uncertainty prediction.
2.6.1,Parameters
2.6.1,----------
2.6.1,dataset: dc.data.Dataset
2.6.1,Dataset to make prediction on
2.6.1,masks: int
2.6.1,the number of dropout masks to average over
2.6.1,Returns
2.6.1,-------
2.6.1,"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
2.6.1,"value of the output, and each element of y_std estimates the standard"
2.6.1,deviation of the corresponding element of y_pred
2.6.1,""""""""
2.6.1,sum_pred: List[np.ndarray] = []
2.6.1,sum_sq_pred: List[np.ndarray] = []
2.6.1,sum_var: List[np.ndarray] = []
2.6.1,for i in range(masks):
2.6.1,generator = self.default_generator(
2.6.1,"dataset, mode='uncertainty', pad_batches=False)"
2.6.1,"results = self._predict(generator, [], True, None)"
2.6.1,if len(sum_pred) == 0:
2.6.1,"for p, v in results:"
2.6.1,sum_pred.append(p)
2.6.1,sum_sq_pred.append(p * p)
2.6.1,sum_var.append(v)
2.6.1,else:
2.6.1,"for j, (p, v) in enumerate(results):"
2.6.1,sum_pred[j] += p
2.6.1,sum_sq_pred[j] += p * p
2.6.1,sum_var[j] += v
2.6.1,output = []
2.6.1,std = []
2.6.1,for i in range(len(sum_pred)):
2.6.1,p = sum_pred[i] / masks
2.6.1,output.append(p)
2.6.1,std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
2.6.1,if len(output) == 1:
2.6.1,"return (output[0], std[0])"
2.6.1,else:
2.6.1,"return list(zip(output, std))"
2.6.1,JAX dependencies
2.6.1,Main training loop
2.6.1,Capture the last avg_loss in case of return since we're resetting to 0 now
2.6.1,Report final results.
2.6.1,Apply tranformers and record results.
2.6.1,Concatenate arrays to create the final results.
2.6.1,flake8:noqa
2.6.1,The PINNModel requires you to create two functions
2.6.1,`create_eval`_fn for letting the model know how to compute the model in inference and
2.6.1,`gradient_fn` for letting model know how to compute the gradient and different regulariser
2.6.1,equation loss depending on the differential equation
2.6.1,defining the Haiku model
2.6.1,"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
2.6.1,"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
2.6.1,which will be used as the differential loss(regulariser)
2.6.1,The expected solution must be as close to cos(x)
2.6.1,Initialize the weights with random values
2.6.1,Forward function which takes the params
2.6.1,Loss Function
2.6.1,JaxModel Working
2.6.1,sample network
2.6.1,Model Initialization
2.6.1,Loss Function
2.6.1,JaxModel Working
2.6.1,sample network
2.6.1,Model Initilisation
2.6.1,Loss Function
2.6.1,JaxModel Working
2.6.1,Model Initilisation
2.6.1,Loss Function
2.6.1,JaxModel Working
2.6.1,Model Initilisation
2.6.1,Loss Function
2.6.1,JaxModel Working
2.6.1,Model Initilisation
2.6.1,Loss Function
2.6.1,JaxModel Working
2.6.1,Each epoch is a single step for this model
2.6.1,@pytest.mark.jax
2.6.1,@pytest.mark.slow
2.6.1,def test_uncertainty():
2.6.1,"""""""Test estimating uncertainty a TorchModel."""""""
2.6.1,n_samples = 30
2.6.1,n_features = 1
2.6.1,noise = 0.1
2.6.1,"X = np.random.rand(n_samples, n_features)"
2.6.1,"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
2.6.1,"dataset = dc.data.NumpyDataset(X, y)"
2.6.1,class Net(hk.Module):
2.6.1,"def __init__(self, output_size: int = 1):"
2.6.1,super().__init__()
2.6.1,"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.6.1,"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.6.1,self.output = hk.Linear(output_size)
2.6.1,self.log_var = hk.Linear(output_size)
2.6.1,"def __call__(self, x):"
2.6.1,x = self._network1(x)
2.6.1,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.6.1,x = self._network2(x)
2.6.1,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.6.1,output = self.output(x)
2.6.1,log_var = self.log_var(x)
2.6.1,var = jnp.exp(log_var)
2.6.1,"return output, var, output, log_var"
2.6.1,def f(x):
2.6.1,net = Net(1)
2.6.1,return net(x)
2.6.1,"def loss(outputs, labels, weights):"
2.6.1,diff = labels[0] - outputs[0]
2.6.1,log_var = outputs[1]
2.6.1,var = jnp.exp(log_var)
2.6.1,return jnp.mean(diff * diff / var + log_var)
2.6.1,class UncertaintyModel(JaxModel):
2.6.1,"def default_generator(self,"
2.6.1,"dataset,"
2.6.1,"epochs=1,"
2.6.1,"mode='fit',"
2.6.1,"deterministic=True,"
2.6.1,pad_batches=True):
2.6.1,for epoch in range(epochs):
2.6.1,"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
2.6.1,"batch_size=self.batch_size,"
2.6.1,"deterministic=deterministic,"
2.6.1,pad_batches=pad_batches):
2.6.1,"yield ([X_b], [y_b], [w_b])"
2.6.1,jm_model = hk.transform(f)
2.6.1,rng = jax.random.PRNGKey(500)
2.6.1,"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
2.6.1,modified_inputs = jnp.array(
2.6.1,[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
2.6.1,"params = jm_model.init(rng, modified_inputs)"
2.6.1,model = UncertaintyModel(
2.6.1,"jm_model.apply,"
2.6.1,"params,"
2.6.1,"loss,"
2.6.1,"output_types=['prediction', 'variance', 'loss', 'loss'],"
2.6.1,learning_rate=0.003)
2.6.1,"model.fit(dataset, nb_epochs=2500)"
2.6.1,"pred, std = model.predict_uncertainty(dataset)"
2.6.1,assert np.mean(np.abs(y - pred)) < 2.0
2.6.1,assert noise < np.mean(std) < 1.0
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,test if adjacency matrix input is correctly set
2.6.1,test if nodes features matrix input is correctly set
2.6.1,check discriminator shape
2.6.1,check training edges logits shape
2.6.1,check training nodes logits shapes
2.6.1,True will be assigned up successful training attempt
2.6.1,force clear tensor flow backend
2.6.1,create new model
2.6.1,to avoid flake8 E125/yapf incompatibility
2.6.1,generate input
2.6.1,train model
2.6.1,generate sample
2.6.1,check how many valid molecules were created and add to list
2.6.1,finally test if there was at least one valid training session
2.6.1,as the model structure improves this should become more and more strict
2.6.1,Predict the output and uncertainty.
2.6.1,predict datset with no y (ensured by tasks = [])
2.6.1,Predict the output and uncertainty.
2.6.1,The DAG models have high error with dropout
2.6.1,"Despite a lot of effort tweaking it , there appears to be"
2.6.1,a limit to how low the error can go with dropout.
2.6.1,assert mean_error < 0.5 * mean_value
2.6.1,Predict the output and uncertainty.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,load datasets
2.6.1,disable transformer
2.6.1,check train
2.6.1,check predict shape
2.6.1,check overfit
2.6.1,load datasets
2.6.1,disable transformer
2.6.1,check train
2.6.1,check predict shape
2.6.1,check overfit
2.6.1,load datasets
2.6.1,disable transformer
2.6.1,check train
2.6.1,check predict shape
2.6.1,check overfit
2.6.1,reload
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Check same predictions are made.
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Load trained model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload trained model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Reload trained model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload trained model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reloaded Trained Model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Check predictions match on random sample
2.6.1,3D Multivariate Gaussian base distribution
2.6.1,Check that reloaded model can sample from the distribution
2.6.1,Check that density estimation is same for reloaded model
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload trained model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload Trained Model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload Trained Model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload trained model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Check predictions match on random sample
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload trained model
2.6.1,Eval model on train
2.6.1,Check predictions match on random sample
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload trained model
2.6.1,Eval model on train
2.6.1,Check predictions match on random sample
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Reload trained model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Reload trained Model
2.6.1,Check predictions match on random sample
2.6.1,Eval model on train
2.6.1,Reload Trained Model
2.6.1,Check predictions match on random sample
2.6.1,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.6.1,Reload Trained Model
2.6.1,Check predictions match on original dataset
2.6.1,TODO: We need a cleaner usage example for this
2.6.1,Fit trained model
2.6.1,Check predictions match on random sample
2.6.1,Train the model on random sequences.  We aren't training long enough to
2.6.1,"really make it reliable, but I want to keep this test fast, and it should"
2.6.1,still be able to reproduce a reasonable fraction of input sequences.
2.6.1,Test it out.
2.6.1,check predict shape
2.6.1,check overfit
2.6.1,needs change
2.6.1,check predict shape
2.6.1,check overfit
2.6.1,reload
2.6.1,There are 4 atoms each of which have 75 atom features
2.6.1,There are 10 pairs with infinity distance and 14 pair features
2.6.1,4 atoms in total
2.6.1,10 pairs in total
2.6.1,10 pairs in total each with start/finish
2.6.1,There are 4 atoms each of which have 75 atom features
2.6.1,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.6.1,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.6.1,connections and accounting for symmetry.)
2.6.1,4 atoms in total
2.6.1,10 pairs in total
2.6.1,The center atom is self connected and to both neighbors so it appears
2.6.1,thrice. The canonical ranking used in MolecularFeaturizer means this
2.6.1,central atom is ranked last in ordering.
2.6.1,10 pairs in total each with start/finish
2.6.1,def test_weave_fit_simple_infinity_distance():
2.6.1,featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
2.6.1,"X = featurizer([""C"", ""CCC""])"
2.6.1,"y = np.array([0, 1.])"
2.6.1,"dataset = dc.data.NumpyDataset(X, y)"
2.6.1,batch_size = 20
2.6.1,model = WeaveModel(
2.6.1,"1,"
2.6.1,"batch_size=batch_size,"
2.6.1,"mode='classification',"
2.6.1,"fully_connected_layer_sizes=[2000, 1000],"
2.6.1,"batch_normalize=True,"
2.6.1,batch_normalize_kwargs={
2.6.1,"""fused"": False,"
2.6.1,"""trainable"": True,"
2.6.1,"""renorm"": True"
2.6.1,"},"
2.6.1,learning_rate=0.0005)
2.6.1,"model.fit(dataset, nb_epoch=200)"
2.6.1,transformers = []
2.6.1,metric = dc.metrics.Metric(
2.6.1,"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
2.6.1,"scores = model.evaluate(dataset, [metric], transformers)"
2.6.1,assert scores['mean-roc_auc_score'] >= 0.9
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,load datasets
2.6.1,initialize model
2.6.1,overfit test
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Fit trained model
2.6.1,Predict the output and uncertainty.
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,xgboost test
2.6.1,fit trained model
2.6.1,eval model on test
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,lightgbm test
2.6.1,fit trained model
2.6.1,eval model on test
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,xgboost test
2.6.1,fit trained model
2.6.1,eval model on test
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,lightgbm test
2.6.1,fit trained model
2.6.1,eval model on test
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,xgboost test
2.6.1,fit trained model
2.6.1,eval model on test
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,lightgbm test
2.6.1,fit trained model
2.6.1,eval model on test
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,xgboost test
2.6.1,fit trained model
2.6.1,reload
2.6.1,check predictions match on test dataset
2.6.1,eval model on test
2.6.1,prepare dataset
2.6.1,global setting
2.6.1,lightgbm test
2.6.1,fit trained model
2.6.1,reload
2.6.1,check predictions match on test dataset
2.6.1,eval model on test
2.6.1,"For simplicity, let's assume both molecules have same number of"
2.6.1,atoms.
2.6.1,Creates a set of dummy features that contain the coordinate and
2.6.1,neighbor-list features required by the AtomicConvModel.
2.6.1,Creates a set of dummy features that contain the coordinate and
2.6.1,neighbor-list features required by the AtomicConvModel.
2.6.1,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.6.1,max num atoms instead of exact.
2.6.1,Cutoff in angstroms
2.6.1,arbitrary label
2.6.1,Run a fitting operation
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Fit trained model
2.6.1,Eval model on train/test
2.6.1,Fit trained model
2.6.1,Eval model on train/test
2.6.1,Fit trained model
2.6.1,Eval model on train/test
2.6.1,See if it has done a plausible job of learning the distribution.
2.6.1,See if it has done a plausible job of learning the distribution.
2.6.1,See if it has done a plausible job of learning the distribution.
2.6.1,No training has been done after reload
2.6.1,See if it has done a plausible job of learning the distribution.
2.6.1,We have to set the gradient penalty very small because the generator's
2.6.1,"output is only a single number, so the default penalty would constrain"
2.6.1,it far too much.
2.6.1,See if it has done a plausible job of learning the distribution.
2.6.1,We have to set the gradient penalty very small because the generator's
2.6.1,"output is only a single number, so the default penalty would constrain"
2.6.1,it far too much.
2.6.1,See if it has done a plausible job of learning the distribution.
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,n_samples = 100
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Most weights should be close to zero.
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Most weights should be close to zero.
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Predict the output and uncertainty.
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Load mini log-solubility dataset.
2.6.1,Fit trained model
2.6.1,Eval model on train
2.6.1,Check that predicting internal layers works.
2.6.1,Each epoch is a single step for this model
2.6.1,Create two models using the same model directory.
2.6.1,Check that they produce different results.
2.6.1,"Save a checkpoint from the first model and load it into the second one,"
2.6.1,and make sure they now match.
2.6.1,Train a model to overfit the dataset.
2.6.1,"Create an identical model, do a single step of fitting with restore=True,"
2.6.1,and make sure it got restored correctly.
2.6.1,Build a model that predicts uncertainty.
2.6.1,Fit the model and see if its predictions are correct.
2.6.1,Take a tiny step in the direction of s and see if the output changes by
2.6.1,the expected amount.
2.6.1,Load dataset and Models
2.6.1,call model.fit again to test multiple fit() calls
2.6.1,def test_singletask_to_multitask_classification(self):
2.6.1,n_features = 10
2.6.1,n_tasks = 17
2.6.1,tasks = range(n_tasks)
2.6.1,# Define train dataset
2.6.1,n_train = 100
2.6.1,"X_train = np.random.rand(n_train, n_features)"
2.6.1,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.6.1,w_train = np.ones_like(y_train)
2.6.1,"ids_train = [""C""] * n_train"
2.6.1,train_dataset = dc.data.DiskDataset.from_numpy(
2.6.1,"X_train, y_train, w_train, ids_train)"
2.6.1,# Define test dataset
2.6.1,n_test = 10
2.6.1,"X_test = np.random.rand(n_test, n_features)"
2.6.1,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.6.1,w_test = np.ones_like(y_test)
2.6.1,"ids_test = [""C""] * n_test"
2.6.1,test_dataset = dc.data.DiskDataset.from_numpy(
2.6.1,"X_test, y_test, w_test, ids_test)"
2.6.1,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.6.1,def model_builder(model_dir):
2.6.1,sklearn_model = LogisticRegression()
2.6.1,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.6.1,multitask_model = dc.models.SingletaskToMultitask(
2.6.1,"tasks, model_builder)"
2.6.1,# Fit trained model
2.6.1,multitask_model.fit(train_dataset)
2.6.1,multitask_model.save()
2.6.1,# Eval multitask_model on train/test
2.6.1,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.6.1,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.6.1,Generate data
2.6.1,Cleanup
2.6.1,Train the model while logging the validation ROC AUC.
2.6.1,Parse the log to pull out the AUC scores.
2.6.1,The last reported score should match the current performance of the model.
2.6.1,Reload the save model and confirm that it matches the best logged score.
2.6.1,3D Multivariate Gaussian base distribution
2.6.1,Must be float32 for RealNVP
2.6.1,Tests a simple flow of one RealNVP layer.
2.6.1,log likelihoods should be negative
2.6.1,# Fit model
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,x and y are the same tensor (equivalent at every element)
2.6.1,the pairwise inner product of the rows in x and y will always be 1
2.6.1,"the output tensor will be of shape (5,5)"
2.6.1,each row in x1 is orthogonal to each row in x2
2.6.1,the pairwise inner product of the rows in x and y will always be 0
2.6.1,"the output tensor will be of shape (256,256)"
2.6.1,"Creating a second layer should produce different results, since it has"
2.6.1,different random weights.
2.6.1,But evaluating the first layer again should produce the same result as before.
2.6.1,index of pair features
2.6.1,number of pairs for each atom
2.6.1,atom features
2.6.1,pair features
2.6.1,"Outputs should be [A, P]"
2.6.1,atom features
2.6.1,Try without compression
2.6.1,"Outputs should be [mol1_vec, mol2_vec)"
2.6.1,Try with compression
2.6.1,"Outputs should be [mol1_vec, mol2_vec)"
2.6.1,atom features
2.6.1,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.6.1,Gaussian histograms expands into 11 Gaussian buckets.
2.6.1,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.6.1,TODO What should shape[1] be?  It's not documented.
2.6.1,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.6.1,"Creating a second layer should produce different results, since it has"
2.6.1,different random weights.
2.6.1,But evaluating the first layer again should produce the same result as before.
2.6.1,"Creating a second layer should produce different results, since it has"
2.6.1,different random weights.
2.6.1,But evaluating the first layer again should produce the same result as before.
2.6.1,"Creating a second layer should produce different results, since it has"
2.6.1,different random weights.
2.6.1,But evaluating the first layer again should produce the same result as before.
2.6.1,"TODO What should the output shape be?  It's not documented, and there"
2.6.1,are no other test cases for it.
2.6.1,"Creating a second layer should produce different results, since it has"
2.6.1,different random weights.
2.6.1,But evaluating the first layer again should produce the same result as before.
2.6.1,"Creating a second layer should produce different results, since it has"
2.6.1,different random weights.
2.6.1,But evaluating the first layer again should produce the same result as before.
2.6.1,"Recall that the DAG layer expects a MultiConvMol as input,"
2.6.1,"so the ""batch"" is a pooled set of atoms from all the"
2.6.1,"molecules in the batch, just as it is for the graph conv."
2.6.1,This means that n_atoms is the batch-size
2.6.1,dropout_switch = False
2.6.1,dropout_switch
2.6.1,# TODO(rbharath): What is the shape of outputs supposed to be?
2.6.1,"# I'm getting (7, 30) here. Where does 7 come from??"
2.6.1,TODO(rbharath): We need more documentation about why
2.6.1,these numbers work.
2.6.1,Create a dataset and an input function for processing it.
2.6.1,Create a dataset and an input function for processing it.
2.6.1,Generate dummy dataset
2.6.1,Fit trained model
2.6.1,Eval model on test
2.6.1,Eval model on train
2.6.1,Fit trained model
2.6.1,Eval model on test
2.6.1,Fit trained model
2.6.1,Eval model on test
2.6.1,Fit trained model
2.6.1,Eval model on test
2.6.1,Fit trained model
2.6.1,Eval model on test
2.6.1,Each epoch is a single step for this model
2.6.1,Create two models using the same model directory.
2.6.1,Check that they produce different results.
2.6.1,"Save a checkpoint from the first model and load it into the second one,"
2.6.1,and make sure they now match.
2.6.1,Train a model to overfit the dataset.
2.6.1,"Create an identical model, do a single step of fitting with restore=True,"
2.6.1,and make sure it got restored correctly.
2.6.1,Build a model that predicts uncertainty.
2.6.1,Fit the model and see if its predictions are correct.
2.6.1,Take a tiny step in the direction of s and see if the output changes by
2.6.1,the expected amount.
2.6.1,Load dataset and Models
2.6.1,call model.fit again to test multiple fit() calls
2.6.1,Train the model on random sequences.  We aren't training long enough to
2.6.1,"really make it reliable, but I want to keep this test fast, and it should"
2.6.1,still be able to reproduce a reasonable fraction of input sequences.
2.6.1,Test it out.
2.6.1,Check that it got at least a quarter of them correct.
2.6.1,Test it out.
2.6.1,Actually training a VAE takes far too long for a unit test.  Just run a
2.6.1,"few steps of training to make sure nothing crashes, then check that the"
2.6.1,results are at least internally consistent.
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,overfit test
2.6.1,test on a small MoleculeNet dataset
2.6.1,load datasets
2.6.1,initialize models
2.6.1,embedding node features
2.6.1,convolutional layer
2.6.1,pooling
2.6.1,for n_tasks == 1 case
2.6.1,Decide first number of GAT layers
2.6.1,flake8:noqa
2.6.1,Select a device.
2.6.1,W&B logging
2.6.1,"If `wandb=True` and no logger is provided, initialize default logger"
2.6.1,Setup and initialize W&B logging
2.6.1,Update config with KerasModel params
2.6.1,Main training loop.
2.6.1,"Execute the loss function, accumulating the gradients."
2.6.1,Report progress and write checkpoints.
2.6.1,Capture the last avg_loss in case of return since we're resetting to 0 now
2.6.1,Report final results.
2.6.1,Invoke the model.
2.6.1,Apply tranformers and record results.
2.6.1,Concatenate arrays to create the final results.
2.6.1,Compute the gradients.
2.6.1,Save the checkpoint to a file.
2.6.1,Rename and delete older files.
2.6.1,Ensure weights for both models are built.
2.6.1,Some scikit-learn models don't use weights.
2.6.1,flake8: ignore
2.6.1,GDBT doesn't support multi-output(task)
2.6.1,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.6.1,retrain model to whole data using best n_estimators * 1.25
2.6.1,GDBT doesn't support multi-output(task)
2.6.1,########################################
2.6.1,Deprecation warnings for XGBoostModel
2.6.1,########################################
2.6.1,flake8: noqa
2.6.1,-*- coding: utf-8 -*-
2.6.1,Assigning featurizer if not user defined
2.6.1,loading datasets
2.6.1,Assembling train and valid datasets
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,Building tensorflow MultitaskDNN model
2.6.1,Building tensorflow robust MultitaskDNN model
2.6.1,Building scikit logistic regression model
2.6.1,Transform fingerprints to IRV features
2.6.1,Building tensorflow IRV model
2.6.1,Building scikit random forest model
2.6.1,Building scikit learn Kernel SVM model
2.6.1,Building xgboost classification model
2.6.1,Remove token for paddings
2.6.1,Building scikit random forest model
2.6.1,Building scikit learn Kernel Ridge Regression model
2.6.1,Building scikit learn Kernel Ridge Regression model
2.6.1,Building xgboost regression model
2.6.1,Loading hyperparameters
2.6.1,num positive/negative ligands
2.6.1,Set batch sizes for network
2.6.1,Model structure
2.6.1,Traning settings
2.6.1,Fit trained model
2.6.1,Evaluating low data model
2.6.1,-*- coding: utf-8 -*-
2.6.1,Assigning featurizer if not user defined
2.6.1,loading datasets
2.6.1,
2.6.1,Note by @XericZephyr. Reason why I spun off this function:
2.6.1,1. Some model needs dataset information.
2.6.1,2. It offers us possibility to **cache** the dataset
2.6.1,"if the featurizer runs very slow, e.g., GraphConv."
2.6.1,2+. The cache can even happen at Travis CI to accelerate
2.6.1,CI testing.
2.6.1,
2.6.1,loading datasets
2.6.1,!/usr/bin/env python2
2.6.1,-*- coding: utf-8 -*-
2.6.1,"TODO For this dataset and model, the R2-scores are less than 0.3."
2.6.1,This has to be improved.
2.6.1,See: https://github.com/deepchem/deepchem/issues/2776
2.6.1,TODO: Check for this
2.6.1,Download files if they don't exist
2.6.1,Featurize the KINASE dataset
2.6.1,Shuffle the training data
2.6.1,Apply transformations
2.6.1,TIMING
2.6.1,transformers = [
2.6.1,"deepchem.trans.LogTransformer(transform_X=True),"
2.6.1,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.6.1,dataset=train_dataset)]
2.6.1,Set shard size low to avoid memory problems.
2.6.1,TIMING
2.6.1,TIMING
2.6.1,Set some global variables up top
2.6.1,Featurize KAGGLE dataset
2.6.1,TIMING
2.6.1,TIMING
2.6.1,Build the path to the dataset on disk.
2.6.1,Try to reload cached datasets.
2.6.1,Create the dataset
2.6.1,Split and transform the dataset.
2.6.1,. clinical trial toxicity (or absence of toxicity)
2.6.1,. FDA approval status.
2.6.1,Download files if they don't exist
2.6.1,Featurizing datasets
2.6.1,Missing entry removal
2.6.1,Shuffle the training data
2.6.1,Apply transformations
2.6.1,TIMING
2.6.1,TODO: Check if anything needs to be added
2.6.1,Featurize the FACTORS dataset
2.6.1,Shuffle the training data
2.6.1,Apply transformations
2.6.1,TIMING
2.6.1,dict of accepted featurizers for this dataset
2.6.1,modify the returned dicts for your dataset
2.6.1,Names of supported featurizers
2.6.1,dict of accepted transformers
2.6.1,dict of accepted splitters
2.6.1,names of supported splitters
2.6.1,Warning message about this template
2.6.1,Featurize mydataset
2.6.1,Get DeepChem data directory if needed
2.6.1,Check for str args to featurizer and splitter
2.6.1,Reload from disk
2.6.1,First type of supported featurizers
2.6.1,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.6.1,Changer loader to match featurizer and data file type
2.6.1,Featurize dataset
2.6.1,Initialize transformers
2.6.1,"get pdb and sdf filenames, labels and pdbids"
2.6.1,load and featurize each complex
2.6.1,Extract locations of data
2.6.1,Extract labels
2.6.1,Lines have format
2.6.1,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.6.1,"The base-10 logarithm, -log kd/pk"
2.6.1,"def load_pcba_146(featurizer='ECFP',"
2.6.1,"split='random',"
2.6.1,"reload=True,"
2.6.1,"data_dir=None,"
2.6.1,"save_dir=None,"
2.6.1,**kwargs):
2.6.1,return load_pcba_dataset(
2.6.1,"featurizer=featurizer,"
2.6.1,"split=split,"
2.6.1,"reload=reload,"
2.6.1,"assay_file_name=""pcba_146.csv.gz"","
2.6.1,"data_dir=data_dir,"
2.6.1,"save_dir=save_dir,"
2.6.1,**kwargs)
2.6.1,"def load_pcba_2475(featurizer='ECFP',"
2.6.1,"split='random',"
2.6.1,"reload=True,"
2.6.1,"data_dir=None,"
2.6.1,"save_dir=None,"
2.6.1,**kwargs):
2.6.1,return load_pcba_dataset(
2.6.1,"featurizer=featurizer,"
2.6.1,"split=split,"
2.6.1,"reload=reload,"
2.6.1,"assay_file_name=""pcba_2475.csv.gz"","
2.6.1,"data_dir=data_dir,"
2.6.1,"save_dir=save_dir,"
2.6.1,**kwargs)
2.6.1,Range of optimization
2.6.1,We know from guard above that this is an int/float
2.6.1,Specify logfile
2.6.1,Make logdir if it doesn't exist.
2.6.1,setup range
2.6.1,Stores all results
2.6.1,Store all model references so we don't have to reload
2.6.1,Stores all model locations
2.6.1,"param values are always float in BO, so this line converts float to int"
2.6.1,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.6.1,Record hyperparameters
2.6.1,We have already evaluated the model for these hyperparameters.
2.6.1,Add it on to the information needed for the constructor
2.6.1,Not all models have nb_epoch
2.6.1,Some models autosave
2.6.1,Record performances
2.6.1,Store all results
2.6.1,Store reference to model
2.6.1,GPGO maximize performance by default
2.6.1,set performance to its negative value for minimization
2.6.1,Demarcating internal function for readability
2.6.1,execute GPGO
2.6.1,FIXME: Incompatible types in assignment
2.6.1,Let's fetch the model with the best parameters
2.6.1,Compare best model to default hyperparameters
2.6.1,Record hyperparameters
2.6.1,Return default hyperparameters
2.6.1,Construction dictionary mapping hyperparameter names to values
2.6.1,"mypy test throws error, so ignoring it in try"
2.6.1,Not all models have nb_epoch
2.6.1,Some models autosave
2.6.1,arbitrarily return last model
2.6.1,flake8: noqa
2.6.1,"2 model variants, 1 results.txt file"
2.6.1,Generate dummy dataset
2.6.1,Generate dummy dataset
2.6.1,These are per-example multiplier
2.6.1,Test that 2 parameters were optimized
2.6.1,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.6.1,Generate dummy dataset
2.6.1,Define nb_epoch in hyperparam_search function call
2.6.1,Generate dummy dataset
2.6.1,Generate dummy dataset
2.6.1,These are per-example multiplier
2.6.1,Test that 2 parameters were optimized
2.6.1,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.6.1,Generate dummy dataset
2.6.1,Have the worker threads generate the rollouts for this iteration.
2.6.1,Perform optimization.
2.6.1,Build the inputs and run the optimizer.
2.6.1,Update the number of steps taken so far and perform checkpointing.
2.6.1,Merge all the rollouts into a single set of arrays.
2.6.1,Iterate slices.
2.6.1,Generate the rollout.
2.6.1,Compute an estimate of the reward for the rest of the episode.
2.6.1,Compute the discounted rewards and advantages.
2.6.1,Convert the actions to one-hot.
2.6.1,Rearrange the states into the proper set of arrays.
2.6.1,Return the processed arrays.
2.6.1,Training loop.
2.6.1,Do checkpointing.
2.6.1,Generate the rollout.
2.6.1,Compute an estimate of the reward for the rest of the episode.
2.6.1,Compute the discounted rewards and advantages.
2.6.1,"Record the actions, converting to one-hot if necessary."
2.6.1,Rearrange the states into the proper set of arrays.
2.6.1,Build the inputs and apply gradients.
2.6.1,Assume all arrays are float32.
2.6.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.6.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.6.1,"game).  The average reward for any bet is slightly negative, so the best"
2.6.1,strategy is to walk away.
2.6.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.1,Optimize it.
2.6.1,"It should have learned that the expected value is very close to zero, and that the best"
2.6.1,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.6.1,top actions).
2.6.1,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.6.1,get the same result.
2.6.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.1,The environment just has a constant state.
2.6.1,The policy includes a single recurrent layer.
2.6.1,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.6.1,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.6.1,"On the first call, the initial state should be all zeros."
2.6.1,It should still be zeros since we didn't save it last time.
2.6.1,It should be different now.
2.6.1,This should be the same as the previous one.
2.6.1,"Now we reset it, so we should get the same result as initially."
2.6.1,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.6.1,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.6.1,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.6.1,at all.  Using hindsight makes it much easier.
2.6.1,A simple policy with two hidden layers.
2.6.1,Optimize it.
2.6.1,Try running it a few times and see if it succeeds.
2.6.1,The state consists of two numbers: a current value and a target value.
2.6.1,The policy just needs to learn to output the target value (or at least
2.6.1,move toward it).
2.6.1,A simple policy with no hidden layers.
2.6.1,Optimize it.
2.6.1,Try running it and see if it reaches the target
2.6.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.6.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.6.1,"game).  The average reward for any bet is slightly negative, so the best"
2.6.1,strategy is to walk away.
2.6.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.1,Optimize it.
2.6.1,"It should have learned that the expected value is very close to zero, and that the best"
2.6.1,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.6.1,top actions).
2.6.1,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.6.1,get the same result.
2.6.1,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.1,The environment just has a constant state.
2.6.1,The policy includes a single recurrent layer.
2.6.1,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.6.1,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.6.1,"On the first call, the initial state should be all zeros."
2.6.1,It should still be zeros since we didn't save it last time.
2.6.1,It should be different now.
2.6.1,This should be the same as the previous one.
2.6.1,"Now we reset it, so we should get the same result as initially."
2.6.1,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.6.1,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.6.1,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.6.1,at all.  Using hindsight makes it much easier.
2.6.1,A simple policy with two hidden layers.
2.6.1,Optimize it.
2.6.1,Try running it a few times and see if it succeeds.
2.6.1,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.1,Randomize who goes first
2.6.1,Illegal move -- the square is not empty
2.6.1,Move X
2.6.1,Did X Win
2.6.1,Did O Win
2.6.1,"default channels are ""conda-forge"" and ""omnia"""
2.6.1,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.6.0,Build a nightly package by default.
2.6.0,Environment-specific dependencies.
2.6.0,get the version from deepchem/__init__.py
2.6.0,nightly version : .devYearMonthDayHourMinute
2.6.0,Force to add `.dev` if `--release` option isn't passed when building
2.6.0,!/usr/bin/env python3
2.6.0,-*- coding: utf-8 -*-
2.6.0,Datasets and models used in the benchmark test
2.6.0,"irv, rf, rf_regression should be assigned manually"
2.6.0,Evaluate performances with different training set fraction
2.6.0,Datasets and models used in the benchmark test
2.6.0,Uncomment the two lines below if hyper_parameters are provided
2.6.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.6.0,hyper_parameters = pickle.load(f)
2.6.0,!/usr/bin/env python3
2.6.0,-*- coding: utf-8 -*-
2.6.0,Datasets and models used in the benchmark test
2.6.0,Load Delaney dataset
2.6.0,Get Metric
2.6.0,Fit trained model
2.6.0,Fit trained model
2.6.0,Set numpy seed
2.6.0,##Load data###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Featurize Kinase dataset
2.6.0,##Load data###
2.6.0,num_trials = 5
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,Force matplotlib to not use any Xwindows backend.
2.6.0,##Load data###
2.6.0,the histogram of the data
2.6.0,Set numpy seed
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,num_trials = 5
2.6.0,Set some global variables up top
2.6.0,Fit trained model
2.6.0,Featurize PCBA dataset
2.6.0,Initialize transformers
2.6.0,Fit trained model
2.6.0,Load sider models now
2.6.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.6.0,transformers to predict functions
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,##Load data###
2.6.0,"n_estimators=100, max_features=int(num_features/3),"
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,Fit trained model
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Load tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,This example shows how to use Pandas to load data directly
2.6.0,without using a CSVLoader object. This may be useful if you
2.6.0,want the flexibility of processing your data with Pandas
2.6.0,directly.
2.6.0,Now let's convert from a dataset back to a pandas dataframe
2.6.0,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.6.0,Featurize FACTORS dataset
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,Force matplotlib to not use any Xwindows backend.
2.6.0,##Load data###
2.6.0,the histogram of the data
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Load QM7 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Fit trained model
2.6.0,Fit trained model
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Fit trained model
2.6.0,Load QM8 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Fit trained model
2.6.0,Set numpy seed
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,Load ChEMBL dataset
2.6.0,Fit models
2.6.0,Do setup required for tf/keras models
2.6.0,Number of features on conv-mols
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,DeepCrystal Technologies 2017 - Patrick Hop
2.6.0,MIT License - have fun!!
2.6.0,Set to higher values to get better numbers
2.6.0,======================================================================
2.6.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,Only for debug!
2.6.0,Load Delaney dataset
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Do setup required for tf/keras models
2.6.0,Number of features on conv-mols
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load Delaney dataset
2.6.0,Get Metric
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load Delaney dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load MUV dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Evaluate train/test scores
2.6.0,Load MUV data
2.6.0,Build model
2.6.0,Fit trained model
2.6.0,Evaluate train/test scores
2.6.0,Extract active site
2.6.0,Featurize ligand
2.6.0,Default for CircularFingerprint
2.6.0,Featurize pocket
2.6.0,Note broadcast operation
2.6.0,Compute labels for pockets
2.6.0,Some complexes have labels but no PDB files. Filter these manually
2.6.0,Some of the ligand-names are of form (FMN ox). Use regex
2.6.0,to merge into form (FMN-ox)
2.6.0,Filter if missing PDB files
2.6.0,Load PDBBind dataset
2.6.0,Define featurizers
2.6.0,Featurize Dataset
2.6.0,########################################################## DEBUG
2.6.0,########################################################## DEBUG
2.6.0,For stable runs
2.6.0,Fit trained model
2.6.0,4-fold splits
2.6.0,num positive/negative ligands
2.6.0,10 trials on test-set
2.6.0,Define metric
2.6.0,Get supports on test-set
2.6.0,Compute accuracies
2.6.0,Number of features on conv-mols
2.6.0,Batch size of models
2.6.0,graph_model = dc.nn.SequentialGraph(n_feat)
2.6.0,Fit trained model
2.6.0,Test model
2.6.0,Join information for all tasks.
2.6.0,4-fold splits
2.6.0,num positive/negative ligands
2.6.0,Define metric
2.6.0,Get supports on test-set
2.6.0,Compute accuracies
2.6.0,Number of features on conv-mols
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Test model
2.6.0,Join information for all tasks.
2.6.0,4-fold splits
2.6.0,num positive/negative ligands
2.6.0,10 trials on test-set
2.6.0,Define metric
2.6.0,Get supports on test-set
2.6.0,Compute accuracies
2.6.0,Number of features on conv-mols
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Test model
2.6.0,Join information for all tasks.
2.6.0,4-fold splits
2.6.0,num positive/negative ligands
2.6.0,10 trials on test-set
2.6.0,Define metric
2.6.0,Get supports on test-set
2.6.0,Compute accuracies
2.6.0,Train model on support
2.6.0,Test model
2.6.0,Join information for all tasks.
2.6.0,4-fold splits
2.6.0,num positive/negative ligands
2.6.0,10 trials on test-set
2.6.0,Define metric
2.6.0,Get supports on test-set
2.6.0,Compute accuracies
2.6.0,Train model on support
2.6.0,Test model
2.6.0,Join information for all tasks.
2.6.0,Set some global variables up top
2.6.0,Featurize Tox21 dataset
2.6.0,Initialize transformers
2.6.0,Set some global variables up top
2.6.0,Featurize Tox21 dataset
2.6.0,Initialize transformers
2.6.0,Load MUV dataset
2.6.0,Featurize MUV dataset
2.6.0,Initialize transformers
2.6.0,Load MUV dataset
2.6.0,Featurize MUV dataset
2.6.0,Initialize transformers
2.6.0,Featurize SIDER dataset
2.6.0,Initialize transformers
2.6.0,Featurize SIDER dataset
2.6.0,Initialize transformers
2.6.0,Load the data.
2.6.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.6.0,sparse: most tasks do not include data for most molecules.  It also is very
2.6.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.6.0,create a list of alternating positives and negatives so each batch will have
2.6.0,equal numbers of both.
2.6.0,Define a MetaLearner describing the learning problem.
2.6.0,Run meta-learning on 80% of the tasks.
2.6.0,Validate on the remaining tasks.
2.6.0,4-fold splits
2.6.0,10 positive/negative ligands
2.6.0,10 trials on test-set
2.6.0,Sample supports without replacement (all pos/neg should be different)
2.6.0,Define metric
2.6.0,Get supports on test-set
2.6.0,Compute accuracies
2.6.0,Train model on support
2.6.0,Test model
2.6.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.6.0,Join information for all tasks.
2.6.0,4-fold splits
2.6.0,num positive/negative ligands
2.6.0,Define metric
2.6.0,Get supports on test-set
2.6.0,Compute accuracies
2.6.0,Train model on support
2.6.0,Test model
2.6.0,Join information for all tasks.
2.6.0,replace with your own scratch directory
2.6.0,Number of conformations in each file increases exponentially.
2.6.0,Start with a smaller dataset before continuing. Use all of them
2.6.0,for production
2.6.0,"'ani_gdb_s03.h5',"
2.6.0,"'ani_gdb_s04.h5',"
2.6.0,"'ani_gdb_s05.h5',"
2.6.0,"'ani_gdb_s06.h5',"
2.6.0,"'ani_gdb_s07.h5',"
2.6.0,'ani_gdb_s08.h5'
2.6.0,Extract the data
2.6.0,Print the data
2.6.0,self-interaction energies taken from
2.6.0,https://github.com/isayev/ANI1_dataset README
2.6.0,flush once more at the end
2.6.0,"# For production, set nb_epoch to 100+"
2.6.0,"print(""Train scores"")"
2.6.0,print(train_scores)
2.6.0,"print(""Minimization of a single test set structure:"")"
2.6.0,"print(model.minimize_structure(coords, atomic_nums))"
2.6.0,Written by Roman Zubatyuk and Justin S. Smith
2.6.0,Modified by Yutong Zhao to make python2 compatible
2.6.0,opening file
2.6.0,print(store_loc)
2.6.0,print(type(v[0]))
2.6.0,print(k)
2.6.0,print(path)
2.6.0,Number of conformations in each file increases exponentially.
2.6.0,Start with a smaller dataset before continuing. Use all of them
2.6.0,for production
2.6.0,Extract the data
2.6.0,NOTE THE RENAMING:
2.6.0,Note sensitivity = recall
2.6.0,Load nci dataset
2.6.0,Featurize nci dataset
2.6.0,Initialize transformers
2.6.0,Set some global variables up top
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load hiv dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load hiv dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load delaney dataset
2.6.0,Fit models
2.6.0,Load delaney dataset
2.6.0,Fit models
2.6.0,Fit models
2.6.0,Load delaney dataset
2.6.0,Fit models
2.6.0,TODO: Once improved splitting API is merged in swap to simpler API
2.6.0,The return values are dc.data.Dataset objects so we need to extract
2.6.0,the ids
2.6.0,TODO once improved splitting API is merged in swap out for simpler
2.6.0,API
2.6.0,The return values are dc.data.Dataset objects so we need to extract
2.6.0,the ids
2.6.0,Fit trained model
2.6.0,Load SIDER dataset
2.6.0,Featurize SIDER dataset
2.6.0,Initialize transformers
2.6.0,Featurize permeability dataset
2.6.0,Load Tox21 dataset
2.6.0,Fit trained model
2.6.0,TODO: This should be swapped for simpler splitter API once that's merged in.
2.6.0,The return values are dc.data.Dataset objects so we need to extract
2.6.0,the ids
2.6.0,Only for debug!
2.6.0,Load clintox dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load clintox dataset
2.6.0,Fit models
2.6.0,Do setup required for tf/keras models
2.6.0,Number of features on conv-mols
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,-*- coding: utf-8 -*-
2.6.0,#############################################################################
2.6.0,## save dataset
2.6.0,#############################################################################
2.6.0,## load datasets
2.6.0,load sweetfda
2.6.0,load aact
2.6.0,## fixup smiles for matching
2.6.0,return smiles
2.6.0,map original smiles to converted smiles
2.6.0,"## join dataframes, index on smiles"
2.6.0,map original smiles back
2.6.0,## fill all nan with 0
2.6.0,## construct datasets
2.6.0,store in new datasets
2.6.0,## save datasets
2.6.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.6.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.6.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.6.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.6.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.6.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.6.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.6.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.6.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.6.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.6.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.6.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.6.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.6.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.6.0,For stable runs
2.6.0,Fit trained model
2.6.0,For stable runs
2.6.0,Fit trained model
2.6.0,For stable runs
2.6.0,Fit trained model
2.6.0,TODO The below line should be fixes
2.6.0,See: https://github.com/deepchem/deepchem/issues/2373
2.6.0,model.save()
2.6.0,transformers = [
2.6.0,"dc.trans.LogTransformer(transform_X=True),"
2.6.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.6.0,dataset=train_dataset)]
2.6.0,Featurize UV dataset
2.6.0,##Load data###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Set numpy seed
2.6.0,##Load data###
2.6.0,##Create model###
2.6.0,Use R2 classification metric
2.6.0,Only use for final evaluation
2.6.0,Force matplotlib to not use any Xwindows backend.
2.6.0,##Load data###
2.6.0,the histogram of the data
2.6.0,##Load data###
2.6.0,###################################################### DEBUG
2.6.0,###################################################### DEBUG
2.6.0,Load HOPV dataset
2.6.0,Fit models
2.6.0,Number of features on conv-mols
2.6.0,Batch size of models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load HOPV dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load HOPV dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load HOPV dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Only for debug!
2.6.0,Load HOPV dataset
2.6.0,Fit models
2.6.0,Fit trained model
2.6.0,Load TOXCAST dataset
2.6.0,Featurize TOXCAST dataset
2.6.0,Initialize transformers
2.6.0,Fit trained model
2.6.0,Processing of ToxCast data
2.6.0,Author - Aneesh Pappu
2.6.0,Loading dataframes and editing indices
2.6.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.6.0,get corresponding casn
2.6.0,get corresponding smiles
2.6.0,write to cell
2.6.0,Tidy up and write to csv
2.6.0,TODO(rbharath): Check that this operation is differentiable.
2.6.0,The number of cells which we should theoretically have
2.6.0,The number of cells which we should theoretically have
2.6.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.6.0,The number of cells which we should theoretically have
2.6.0,TODO(rbharath): The test below only checks that shapes work out.
2.6.0,Need to do a correctness implementation vs. a simple CPU impl.
2.6.0,The number of cells which we should theoretically have
2.6.0,TODO(rbharath): The test below only checks that shapes work out.
2.6.0,Need to do a correctness implementation vs. a simple CPU impl.
2.6.0,The number of cells which we should theoretically have
2.6.0,TODO(rbharath): The test below only checks that shapes work out.
2.6.0,Need to do a correctness implementation vs. a simple CPU impl.
2.6.0,TODO(rbharath): Commenting this out due to weird segfaults
2.6.0,def test_vina_generate_conformers(self):
2.6.0,"""""""Test that Vina Model can generate conformers"""""""
2.6.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.6.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.6.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.6.0,max_protein_atoms = 3500
2.6.0,max_ligand_atoms = 100
2.6.0,"print(""Loading protein file"")"
2.6.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.6.0,protein_Z = pad_array(
2.6.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.6.0,max_protein_atoms)
2.6.0,"print(""Loading ligand file"")"
2.6.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.6.0,ligand_Z = pad_array(
2.6.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.6.0,max_ligand_atoms)
2.6.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.6.0,"Shape (n_cells, k)"
2.6.0,"Shape (N, 1)"
2.6.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.6.0,"conditions, so does wrapround. O(constant)"
2.6.0,"Shape (n_cells, 26)"
2.6.0,"Shape (N, 26)"
2.6.0,"coords of shape (N, ndim)"
2.6.0,"Shape (N, 26, k, ndim)"
2.6.0,"Shape (N, 26, k)"
2.6.0,"Shape (N, 26, k)"
2.6.0,"Shape (N, 26, k, ndim)"
2.6.0,"For smaller systems especially, the periodic boundary conditions can"
2.6.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.6.0,make sure duplicate neighbors are ignored?
2.6.0,TODO(rbharath): How does distance need to be modified here to
2.6.0,account for periodic boundary conditions?
2.6.0,"Shape (N, 26, k)"
2.6.0,"Shape (N, 26*k)"
2.6.0,TODO(rbharath): This will cause an issue with duplicates!
2.6.0,"Shape (N, M)"
2.6.0,"N elts of size (M,) each"
2.6.0,"Shape (N, 26*k)"
2.6.0,"N elts of size (26*k,) each"
2.6.0,"N elts of size (M,) each"
2.6.0,"Shape (N, M)"
2.6.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.6.0,"N tensors of shape (n_cells, 1)"
2.6.0,"Shape (N*n_cells, 1) after tile"
2.6.0,"List of N tensors of shape (n_cells, 1)"
2.6.0,Lists of length N
2.6.0,Lists of length n_cells
2.6.0,Get indices of k atoms closest to each cell point
2.6.0,TODO(rbharath): tf.stack for tf 1.0
2.6.0,"Tensor of shape (n_cells, k, ndim)"
2.6.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.6.0,"Tensor of shape (26, k, ndim)"
2.6.0,"Reshape to (26*k, ndim)"
2.6.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.6.0,"Dists of shape (26*k, 1)"
2.6.0,"Of shape (k, ndim)"
2.6.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.6.0,TODO(rbharath): Change this for tf 1.0
2.6.0,"n_cells tensors of shape (N, 1)"
2.6.0,"Shape (N*n_cells, 1) after tile"
2.6.0,"List of n_cells tensors of shape (N, 1)"
2.6.0,Lists of length n_cells
2.6.0,Lists of length n_cells
2.6.0,Get indices of k atoms closest to each cell point
2.6.0,"n_cells tensors of shape (k, ndim)"
2.6.0,"Tensor of shape (n_cells, k)"
2.6.0,TODO(rbharath):
2.6.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.6.0,- Need to group closest atoms amongst cell neighbors
2.6.0,- Need to do another top_k to find indices of closest neighbors.
2.6.0,- Return N lists corresponding to neighbors for every atom.
2.6.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.6.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.6.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.6.0,the cube.
2.6.0,Number of neighbors of central cube in 3-space is
2.6.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.6.0,TODO(rbharath)
2.6.0,n_cells = int(cells.get_shape()[0])
2.6.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.6.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.6.0,"Tile (a, a, a, b, b, b, etc.)"
2.6.0,"Tile (a, b, c, a, b, c, ...)"
2.6.0,"Lists of n_cells tensors of shape (N, 1)"
2.6.0,Lists of length n_cells
2.6.0,Lists of length n_cells
2.6.0,Get indices of k atoms closest to each cell point
2.6.0,"n_cells tensors of shape (26,)"
2.6.0,TODO(rbharath): Make this handle minibatches
2.6.0,"Shape (N_protein+N_ligand, 3)"
2.6.0,"Shape (N_protein+N_ligand,)"
2.6.0,"Shape (N_protein+N_ligand, M)"
2.6.0,"Shape (N_protein+N_ligand,)"
2.6.0,"Shape (N_protein+N_ligand, 3)"
2.6.0,"Shape (N_protein+N_ligand,)"
2.6.0,"Shape (N_protein+N_ligand, M)"
2.6.0,"Shape (N_protein+N_ligand, M, 3)"
2.6.0,"Shape (N_protein+N_ligand, M)"
2.6.0,"Shape (N_protein+N_ligand, M, 3)"
2.6.0,"Shape (N_protein+N_ligand, M)"
2.6.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.6.0,"Shape (N_protein+N_ligand, M)"
2.6.0,"Shape (N_protein+N_ligand, M)"
2.6.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.6.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.6.0,computing free-energy. This implementation currently uses all interaction
2.6.0,terms. Not sure if this makes a difference.
2.6.0,"Shape (N_protein+N_ligand, M)"
2.6.0,Shape () -- scalar
2.6.0,Keep track of the layers
2.6.0,"For graphical layers, add connectivity placeholders"
2.6.0,Add layer to the layer list
2.6.0,Keep track of the layers
2.6.0,Create graph topology and x
2.6.0,Keep track of the layers
2.6.0,Whether or not we have used the GraphGather layer yet
2.6.0,Update new value of x
2.6.0,Update new value of x
2.6.0,Update new value of x
2.6.0,Get train function
2.6.0,Initialize
2.6.0,################################################################### DEBUG
2.6.0,self.test_label_placeholder = Input(
2.6.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.6.0,"name=""label_placeholder""))"
2.6.0,self.test_weight_placeholder = Input(
2.6.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.6.0,"name=""weight_placeholder""))"
2.6.0,TODO(rbharath): Should weights for the support be used?
2.6.0,Support labels
2.6.0,self.support_label_placeholder = Input(
2.6.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.6.0,"name=""support_label_placeholder""))"
2.6.0,################################################################### DEBUG
2.6.0,Generate dictionary elements for support
2.6.0,Get graph information for test
2.6.0,Generate dictionary elements for test
2.6.0,Perform the optimization
2.6.0,Create different support sets
2.6.0,Get batch to try it out on
2.6.0,"Train on support set, batch pair"
2.6.0,Get featurization for test
2.6.0,"Shape (n_test, n_feat)"
2.6.0,Get featurization for support
2.6.0,"Shape (n_support, n_feat)"
2.6.0,Computes the inner part c() of the kernel
2.6.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.6.0,Normalize
2.6.0,TODO(rbharath): euclidean kernel is broken!
2.6.0,elif self.similarity == 'euclidean':
2.6.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.6.0,"Note that gram matrix g has shape (n_test, n_support)"
2.6.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.6.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.6.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.6.0,each test entry) to get attention vector
2.6.0,"Shape (n_test, n_support)"
2.6.0,Weighted sum of support labels
2.6.0,"Shape (n_support, 1)"
2.6.0,pred is yhat in eqn (1) of Matching Networks.
2.6.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.6.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.6.0,"Shape (n_test,)"
2.6.0,Convert to logit space using inverse sigmoid (logit) function
2.6.0,logit function: log(pred) - log(1-pred)
2.6.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.6.0,in Cross Entropy calculation.
2.6.0,"Shape (n_test,)"
2.6.0,Get scores
2.6.0,Remove padded elements
2.6.0,Get scores
2.6.0,pred corresponds to prob(example == 1)
2.6.0,Remove padded elements
2.6.0,Get batches
2.6.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.6.0,multitask case with missing data...
2.6.0,Join information for all tasks.
2.6.0,TODO(rbharath): Find a way to get rid of this import?
2.6.0,Extract model info
2.6.0,Get graph topology for x
2.6.0,Building outputs
2.6.0,Set epsilon
2.6.0,Initialize
2.6.0,"Path to save checkpoint files, which matches the"
2.6.0,replicated supervisor's default path.
2.6.0,Create target inputs
2.6.0,Get train function
2.6.0,TODO(rbharath): I believe this is total amount of data
2.6.0,Get graph information
2.6.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.6.0,the number of labeled data points in target_i. This is to normalize each task
2.6.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.6.0,Get other optimizer information
2.6.0,TODO(rbharath): Figure out how to handle phase appropriately
2.6.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.6.0,"tensors of shape (batch_size,)"
2.6.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.6.0,examples (effect averages out)
2.6.0,Perform the optimization
2.6.0,TODO(rbharath): Disabling saving for now to try to debug.
2.6.0,run eval data through the model
2.6.0,"Shape (n_samples, n_tasks)"
2.6.0,Create target inputs
2.6.0,TODO(rbharath): Find a way to get rid of this import?
2.6.0,Obtain appropriate loss function
2.6.0,Extract model info
2.6.0,Get graph topology for x
2.6.0,Raw logit outputs
2.6.0,Set epsilon
2.6.0,Initialize
2.6.0,"Path to save checkpoint files, which matches the"
2.6.0,replicated supervisor's default path.
2.6.0,Create target inputs
2.6.0,############################################################### DEBUG
2.6.0,"print(""multitask classifier"")"
2.6.0,"print(""feat"")"
2.6.0,print(feat)
2.6.0,############################################################### DEBUG
2.6.0,Get train function
2.6.0,TODO(rbharath): I believe this is total amount of data
2.6.0,Get graph information
2.6.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.6.0,the number of labeled data points in target_i. This is to normalize each task
2.6.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.6.0,Get other optimizer information
2.6.0,TODO(rbharath): Figure out how to handle phase appropriately
2.6.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.6.0,"tensors of shape (batch_size,)"
2.6.0,Convert the labels into one-hot vector encodings.
2.6.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.6.0,un-softmaxed logits rather than softmax outputs.
2.6.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.6.0,examples (effect averages out)
2.6.0,Perform the optimization
2.6.0,TODO(rbharath): Disabling saving for now to try to debug.
2.6.0,run eval data through the model
2.6.0,"Shape (n_samples, n_tasks)"
2.6.0,run eval data through the model
2.6.0,self.n_atoms = n_atoms
2.6.0,Define the list of tensors to be used as topology
2.6.0,Merge mol conv objects
2.6.0,Generate dicts
2.6.0,Define the list of tensors to be used as topology
2.6.0,Extract atom numbers
2.6.0,Generate dicts
2.6.0,molecule * atom(graph) => step => features
2.6.0,molecule * atom(graph) => step
2.6.0,molecule * atom(graph) => step
2.6.0,Define the list of tensors to be used as topology
2.6.0,calculation orders for a batch of molecules
2.6.0,padding atom features vector of each molecule with 0
2.6.0,self.n_atoms = n_atoms
2.6.0,Define the list of tensors to be used as topology
2.6.0,Extract atom numbers
2.6.0,Generate dicts
2.6.0,self.n_atoms = n_atoms
2.6.0,Define the list of tensors to be used as topology
2.6.0,Extract atom numbers
2.6.0,number of atoms in each molecule
2.6.0,index of pair features
2.6.0,number of pairs for each atom
2.6.0,atom features
2.6.0,pair features
2.6.0,Generate dicts
2.6.0,Load Tox21 dataset
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.6.0,Fit trained model
2.6.0,Fit models
2.6.0,Batch size of models
2.6.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.6.0,Fit trained model
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,number positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply an attention lstm layer
2.6.0,Number of folds for split
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,number positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply an attention lstm layer
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,number positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply an attention lstm layer
2.6.0,Number of folds for split
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Number of folds for split
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply a residual lstm layer
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply a residual lstm layer
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply a residual lstm layer
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply a residual lstm layer
2.6.0,Number of folds for split
2.6.0,Depth of attention module
2.6.0,number positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,Apply an attention lstm layer
2.6.0,Number of folds for split
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Number of features on conv-mols
2.6.0,Define metric
2.6.0,Train support model on train
2.6.0,Add layers
2.6.0,# Gather Projection
2.6.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.6.0,There should be 8 layers in graph_model
2.6.0,assert len(graph_model.layers) == 6
2.6.0,Add layers
2.6.0,Need to add batch-norm separately to test/support due to differing
2.6.0,shapes.
2.6.0,Apply an attention lstm layer
2.6.0,Gather Projection
2.6.0,Add layers
2.6.0,Need to add batch-norm separately to test/support due to differing
2.6.0,shapes.
2.6.0,Apply an attention lstm layer
2.6.0,Gather Projection
2.6.0,Degrees from 1 to max_deg inclusive
2.6.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.6.0,"Should have shape (?, deg)"
2.6.0,"Shape of atom_features should be (?, n_feat)"
2.6.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.6.0,-*- coding: utf-8 -*-
2.6.0,Save hyperparameters
2.6.0,-*- coding: utf-8 -*-
2.6.0,Save hyperparameters
2.6.0,setup optimizer
2.6.0,setup optimizer
2.6.0,"print(""tasK: %d"" %task)"
2.6.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.6.0,"print(""scores"")"
2.6.0,print(scores.size())
2.6.0,"print(""task_label"")"
2.6.0,print(task_label.size())
2.6.0,"task_loss =  self.criterion(scores, task_label)"
2.6.0,"print(""task_loss"")"
2.6.0,print(task_loss.size())
2.6.0,-*- coding: utf-8 -*-
2.6.0,Save hyperparameters
2.6.0,weight decay
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,Turns out there are valid cases where we don't want pad-batches
2.6.0,on by default.
2.6.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.0,Run training op.
2.6.0,############################################################# TIMING
2.6.0,The iterbatches does padding with zero-weight examples on the last batch.
2.6.0,Remove padded examples.
2.6.0,Special case to handle singletasks.
2.6.0,The iterbatches does padding with zero-weight examples on the last batch.
2.6.0,Remove padded examples.
2.6.0,References
2.6.0,Arguments
2.6.0,Aliases.
2.6.0,Aliases.
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,TODO(rbharath): This class does not yet have a
2.6.0,"TensorGraph equivalent, but one may not be required."
2.6.0,"Commented out for now, remove if OK."
2.6.0,class AlternateWeaveLayer(WeaveLayer):
2.6.0,""""""" Alternate implementation of weave module"
2.6.0,"same variables, different graph structures"
2.6.0,""""""""
2.6.0,
2.6.0,"def call(self, x, mask=None):"
2.6.0,"""""""Execute this layer on input tensors."
2.6.0,
2.6.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.6.0,
2.6.0,Parameters
2.6.0,----------
2.6.0,x: list
2.6.0,list of Tensors of form described above.
2.6.0,"mask: bool, optional"
2.6.0,Ignored. Present only to shadow superclass call() method.
2.6.0,
2.6.0,Returns
2.6.0,-------
2.6.0,A: Tensor
2.6.0,Tensor of atom_features
2.6.0,P: Tensor
2.6.0,Tensor of pair_features
2.6.0,""""""""
2.6.0,# Add trainable weights
2.6.0,self.build()
2.6.0,
2.6.0,atom_features = x[0]
2.6.0,pair_features = x[1]
2.6.0,
2.6.0,pair_split = x[2]
2.6.0,atom_to_pair = x[4]
2.6.0,
2.6.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.6.0,AA = self.activation(AA)
2.6.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.6.0,PA = self.activation(PA)
2.6.0,"PA = tf.segment_sum(PA, pair_split)"
2.6.0,
2.6.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.6.0,A = self.activation(A)
2.6.0,
2.6.0,if self.update_pair:
2.6.0,AP_ij = tf.matmul(
2.6.0,tf.reshape(
2.6.0,"tf.gather(atom_features, atom_to_pair),"
2.6.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.6.0,AP_ij = self.activation(AP_ij)
2.6.0,AP_ji = tf.matmul(
2.6.0,tf.reshape(
2.6.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.6.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.6.0,AP_ji = self.activation(AP_ji)
2.6.0,
2.6.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.6.0,PP = self.activation(PP)
2.6.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.6.0,P = self.activation(P)
2.6.0,else:
2.6.0,P = pair_features
2.6.0,
2.6.0,"return A, P"
2.6.0,TODO(rbharath): This class does not yet have a
2.6.0,"TensorGraph equivalent, but one may not be required."
2.6.0,"Commented out for now, remove if OK."
2.6.0,class WeaveConcat(Layer):
2.6.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.6.0,""""""""
2.6.0,
2.6.0,"def __init__(self,"
2.6.0,"batch_size,"
2.6.0,"n_atom_input_feat=50,"
2.6.0,"n_output=128,"
2.6.0,"init='glorot_uniform',"
2.6.0,"activation='tanh',"
2.6.0,**kwargs):
2.6.0,""""""""
2.6.0,Parameters
2.6.0,----------
2.6.0,batch_size: int
2.6.0,number of molecules in a batch
2.6.0,"n_atom_input_feat: int, optional"
2.6.0,Number of features for each atom in input.
2.6.0,"n_output: int, optional"
2.6.0,Number of output features for each atom(concatenated)
2.6.0,"init: str, optional"
2.6.0,Weight initialization for filters.
2.6.0,"activation: str, optional"
2.6.0,Activation function applied
2.6.0,
2.6.0,""""""""
2.6.0,self.batch_size = batch_size
2.6.0,self.n_atom_input_feat = n_atom_input_feat
2.6.0,self.n_output = n_output
2.6.0,self.init = initializations.get(init)  # Set weight initialization
2.6.0,self.activation = activations.get(activation)  # Get activations
2.6.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.6.0,
2.6.0,def build(self):
2.6.0,"""""""""Construct internal trainable weights."
2.6.0,""""""""
2.6.0,
2.6.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.6.0,self.b = model_ops.zeros(shape=[
2.6.0,"self.n_output,"
2.6.0,])
2.6.0,
2.6.0,self.trainable_weights = self.W + self.b
2.6.0,
2.6.0,"def call(self, x, mask=None):"
2.6.0,"""""""Execute this layer on input tensors."
2.6.0,
2.6.0,"x = [atom_features, atom_mask]"
2.6.0,
2.6.0,Parameters
2.6.0,----------
2.6.0,x: list
2.6.0,Tensors as listed above
2.6.0,"mask: bool, optional"
2.6.0,Ignored. Present only to shadow superclass call() method.
2.6.0,
2.6.0,Returns
2.6.0,-------
2.6.0,outputs: Tensor
2.6.0,Tensor of concatenated atom features
2.6.0,""""""""
2.6.0,self.build()
2.6.0,atom_features = x[0]
2.6.0,atom_masks = x[1]
2.6.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.6.0,A_mask = tf.split(
2.6.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.6.0,outputs = tf.concat(
2.6.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.6.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.6.0,outputs = self.activation(outputs)
2.6.0,return outputs
2.6.0,TODO(rbharath): This class does not yet have a
2.6.0,"TensorGraph equivalent, but one may not be required."
2.6.0,"Commented out for now, remove if OK."
2.6.0,class AlternateWeaveGather(WeaveGather):
2.6.0,"""""""Alternate implementation of weave gather layer"
2.6.0,corresponding to AlternateWeaveLayer
2.6.0,""""""""
2.6.0,
2.6.0,"def call(self, x, mask=None):"
2.6.0,"""""""Execute this layer on input tensors."
2.6.0,
2.6.0,"x = [atom_features, atom_split]"
2.6.0,
2.6.0,Parameters
2.6.0,----------
2.6.0,x: list
2.6.0,Tensors as listed above
2.6.0,"mask: bool, optional"
2.6.0,Ignored. Present only to shadow superclass call() method.
2.6.0,
2.6.0,Returns
2.6.0,-------
2.6.0,outputs: Tensor
2.6.0,Tensor of molecular features
2.6.0,""""""""
2.6.0,# Add trainable weights
2.6.0,self.build()
2.6.0,outputs = x[0]
2.6.0,atom_split = x[1]
2.6.0,
2.6.0,if self.gaussian_expand:
2.6.0,outputs = self.gaussian_histogram(outputs)
2.6.0,
2.6.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.6.0,
2.6.0,if self.gaussian_expand:
2.6.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.6.0,output_molecules = self.activation(output_molecules)
2.6.0,return output_molecules
2.6.0,Each directory holds a range of assay results
2.6.0,Just write NA
2.6.0,"Now, write out the results csv, going line by line through all molecule results"
2.6.0,printing the mol_id
2.6.0,printing the SMILES
2.6.0,Now gzip it
2.6.0,Now remove the intermediate csv
2.6.0,First download all SDF files. We need these to get smiles
2.6.0,Next download all Bioassays
2.6.0,RDKit consistently hangs when trying to read this file
2.6.0,TODO (LESWING) Lazy Load
2.6.0,TODO (LESWING) Lazy Load
2.6.0,from simdna import simulations
2.6.0,define layer out functions
2.6.0,get layer outputs for a positive simulation example
2.6.0,plot layer outputs
2.6.0,highlight motif sites
2.6.0,get a positive and a negative example from the simulation data
2.6.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.6.0,get motif site locations
2.6.0,organize legends
2.6.0,plot scores and highlight motif site locations
2.6.0,initialize fwd and reverse scores to -infinity
2.6.0,"cross-correlate separately for each base,"
2.6.0,for both the PSSM and its reverse complement
2.6.0,sum over the bases
2.6.0,take max of fwd and reverse scores at each position
2.6.0,return 1D view of sequence characters
2.6.0,class SequenceDNN(Model):
2.6.0,""""""""
2.6.0,Sequence DNN models.
2.6.0,
2.6.0,Parameters
2.6.0,----------
2.6.0,"seq_length : int, optional"
2.6.0,length of input sequence.
2.6.0,"keras_model : instance of keras.models.Sequential, optional"
2.6.0,seq_length or keras_model must be specified.
2.6.0,"num_tasks : int, optional"
2.6.0,number of tasks. Default: 1.
2.6.0,num_filters : list[int] | tuple[int]
2.6.0,"number of convolutional filters in each layer. Default: (15,)."
2.6.0,conv_width : list[int] | tuple[int]
2.6.0,"width of each layer's convolutional filters. Default: (15,)."
2.6.0,pool_width : int
2.6.0,width of max pooling after the last layer. Default: 35.
2.6.0,L1 : float
2.6.0,strength of L1 penalty.
2.6.0,dropout : float
2.6.0,dropout probability in every convolutional layer. Default: 0.
2.6.0,verbose: int
2.6.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.6.0,
2.6.0,Returns
2.6.0,-------
2.6.0,Compiled DNN model.
2.6.0,""""""""
2.6.0,
2.6.0,"def __init__(self,"
2.6.0,"seq_length=None,"
2.6.0,"keras_model=None,"
2.6.0,"use_RNN=False,"
2.6.0,"num_tasks=1,"
2.6.0,"num_filters=(15, 15, 15),"
2.6.0,"conv_width=(15, 15, 15),"
2.6.0,"pool_width=35,"
2.6.0,"GRU_size=35,"
2.6.0,"TDD_size=15,"
2.6.0,"L1=0,"
2.6.0,"dropout=0.0,"
2.6.0,"num_epochs=100,"
2.6.0,verbose=1):
2.6.0,self.num_tasks = num_tasks
2.6.0,self.num_epochs = num_epochs
2.6.0,self.verbose = verbose
2.6.0,self.train_metrics = []
2.6.0,self.valid_metrics = []
2.6.0,if keras_model is not None and seq_length is None:
2.6.0,self.model = keras_model
2.6.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.6.0,elif seq_length is not None and keras_model is None:
2.6.0,self.model = Sequential()
2.6.0,assert len(num_filters) == len(conv_width)
2.6.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.6.0,conv_height = 4 if i == 0 else 1
2.6.0,self.model.add(
2.6.0,Convolution2D(
2.6.0,"nb_filter=nb_filter,"
2.6.0,"nb_row=conv_height,"
2.6.0,"nb_col=nb_col,"
2.6.0,"activation='linear',"
2.6.0,"init='he_normal',"
2.6.0,"input_shape=(1, 4, seq_length),"
2.6.0,"W_regularizer=l1(L1),"
2.6.0,b_regularizer=l1(L1)))
2.6.0,self.model.add(Activation('relu'))
2.6.0,self.model.add(Dropout(dropout))
2.6.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.6.0,if use_RNN:
2.6.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.6.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.6.0,"self.model.add(Permute((2, 1)))"
2.6.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.6.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.6.0,self.model.add(Flatten())
2.6.0,self.model.add(Dense(output_dim=self.num_tasks))
2.6.0,self.model.add(Activation('sigmoid'))
2.6.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.6.0,else:
2.6.0,raise ValueError(
2.6.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.6.0,
2.6.0,"def train(self,"
2.6.0,"X,"
2.6.0,"y,"
2.6.0,"validation_data,"
2.6.0,"early_stopping_metric='Loss',"
2.6.0,"early_stopping_patience=5,"
2.6.0,save_best_model_to_prefix=None):
2.6.0,if y.dtype != bool:
2.6.0,"assert set(np.unique(y)) == {0, 1}"
2.6.0,y = y.astype(bool)
2.6.0,multitask = y.shape[1] > 1
2.6.0,if not multitask:
2.6.0,num_positives = y.sum()
2.6.0,num_sequences = len(y)
2.6.0,num_negatives = num_sequences - num_positives
2.6.0,if self.verbose >= 1:
2.6.0,print('Training model (* indicates new best result)...')
2.6.0,"X_valid, y_valid = validation_data"
2.6.0,early_stopping_wait = 0
2.6.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.6.0,"for epoch in range(1, self.num_epochs + 1):"
2.6.0,self.model.fit(
2.6.0,"X,"
2.6.0,"y,"
2.6.0,"batch_size=128,"
2.6.0,"nb_epoch=1,"
2.6.0,class_weight={
2.6.0,"True: num_sequences / num_positives,"
2.6.0,False: num_sequences / num_negatives
2.6.0,"} if not multitask else None,"
2.6.0,verbose=self.verbose >= 2)
2.6.0,"epoch_train_metrics = self.test(X, y)"
2.6.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.6.0,self.train_metrics.append(epoch_train_metrics)
2.6.0,self.valid_metrics.append(epoch_valid_metrics)
2.6.0,if self.verbose >= 1:
2.6.0,print('Epoch {}:'.format(epoch))
2.6.0,print('Train {}'.format(epoch_train_metrics))
2.6.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.6.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.6.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.6.0,if self.verbose >= 1:
2.6.0,print(' *')
2.6.0,best_metric = current_metric
2.6.0,best_epoch = epoch
2.6.0,early_stopping_wait = 0
2.6.0,if save_best_model_to_prefix is not None:
2.6.0,self.save(save_best_model_to_prefix)
2.6.0,else:
2.6.0,if self.verbose >= 1:
2.6.0,print()
2.6.0,if early_stopping_wait >= early_stopping_patience:
2.6.0,break
2.6.0,early_stopping_wait += 1
2.6.0,if self.verbose >= 1:
2.6.0,print('Finished training after {} epochs.'.format(epoch))
2.6.0,if save_best_model_to_prefix is not None:
2.6.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.6.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.6.0,"best_epoch, save_best_model_to_prefix))"
2.6.0,
2.6.0,"def predict(self, X):"
2.6.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.6.0,
2.6.0,def get_sequence_filters(self):
2.6.0,""""""""
2.6.0,Returns 3D array of 2D sequence filters.
2.6.0,""""""""
2.6.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.6.0,
2.6.0,"def deeplift(self, X, batch_size=200):"
2.6.0,""""""""
2.6.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.6.0,""""""""
2.6.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.6.0,from deeplift.conversion import keras_conversion as kc
2.6.0,
2.6.0,# convert to deeplift model and get scoring function
2.6.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.6.0,score_func = deeplift_model.get_target_contribs_func(
2.6.0,find_scores_layer_idx=0)
2.6.0,# use a 40% GC reference
2.6.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.6.0,# get deeplift scores
2.6.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.6.0,for i in range(self.num_tasks):
2.6.0,deeplift_scores[i] = score_func(
2.6.0,"task_idx=i,"
2.6.0,"input_data_list=[X],"
2.6.0,"batch_size=batch_size,"
2.6.0,"progress_update=None,"
2.6.0,input_references_list=input_references)
2.6.0,return deeplift_scores
2.6.0,
2.6.0,"def in_silico_mutagenesis(self, X):"
2.6.0,""""""""
2.6.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.6.0,""""""""
2.6.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.6.0,wild_type_predictions = self.predict(X)
2.6.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.6.0,np.newaxis]
2.6.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.6.0,"zip(X, wild_type_predictions)):"
2.6.0,mutated_sequences = np.repeat(
2.6.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.6.0,# remove wild-type
2.6.0,arange = np.arange(len(mutated_sequences))
2.6.0,horizontal_cycle = np.tile(
2.6.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.6.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.6.0,# add mutant
2.6.0,vertical_repeat = np.repeat(
2.6.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.6.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.6.0,# make mutant predictions
2.6.0,mutated_predictions = self.predict(mutated_sequences)
2.6.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.6.0,"(self.num_tasks,))"
2.6.0,mutagenesis_scores[
2.6.0,sequence_index] = wild_type_prediction - mutated_predictions
2.6.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.6.0,
2.6.0,@staticmethod
2.6.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.6.0,from dragonn.plot import plot_bases_on_ax
2.6.0,scores = score_func(X).squeeze(
2.6.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.6.0,try:
2.6.0,os.makedirs(output_directory)
2.6.0,except OSError:
2.6.0,pass
2.6.0,num_tasks = len(scores)
2.6.0,"for task_index, task_scores in enumerate(scores):"
2.6.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.6.0,# sequence_scores is num_bases x sequence_length
2.6.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.6.0,plt.clf()
2.6.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.6.0,top_axis.plot(
2.6.0,"range(1,"
2.6.0,"len(basewise_max_sequence_scores) + 1),"
2.6.0,basewise_max_sequence_scores)
2.6.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.6.0,peak_position = basewise_max_sequence_scores.argmax()
2.6.0,top_axis.axvspan(
2.6.0,"peak_position - peak_width,"
2.6.0,"peak_position + peak_width,"
2.6.0,"color='grey',"
2.6.0,alpha=0.1)
2.6.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.6.0,peak_position + peak_width].T
2.6.0,# Set non-max letter_heights to zero
2.6.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.6.0,"letter_heights[np.arange(len(letter_heights)),"
2.6.0,peak_sequence_scores.argmax(axis=1)] = \
2.6.0,basewise_max_sequence_scores[peak_position - peak_width :
2.6.0,peak_position + peak_width]
2.6.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.6.0,bottom_axis.set_xticklabels(
2.6.0,tuple(
2.6.0,"map(str,"
2.6.0,"np.arange(peak_position - peak_width,"
2.6.0,peak_position + peak_width + 1))))
2.6.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.6.0,plt.xlabel('Position')
2.6.0,plt.ylabel('Score')
2.6.0,plt.savefig(
2.6.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.6.0,"sequence_index, '_task_{}'.format(task_index)"
2.6.0,if num_tasks > 1 else '')))
2.6.0,plt.close()
2.6.0,
2.6.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.6.0,self._plot_scores(
2.6.0,"X,"
2.6.0,"output_directory,"
2.6.0,"peak_width,"
2.6.0,"score_func=self.deeplift,"
2.6.0,score_name='DeepLift')
2.6.0,
2.6.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.6.0,self._plot_scores(
2.6.0,"X,"
2.6.0,"output_directory,"
2.6.0,"peak_width,"
2.6.0,"score_func=self.in_silico_mutagenesis,"
2.6.0,score_name='ISM')
2.6.0,
2.6.0,"def plot_architecture(self, output_file):"
2.6.0,from dragonn.visualize_util import plot as plot_keras_model
2.6.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.6.0,
2.6.0,"def save(self, save_best_model_to_prefix):"
2.6.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.6.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.6.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.6.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.6.0,
2.6.0,@staticmethod
2.6.0,"def load(arch_fname, weights_fname=None):"
2.6.0,model_json_string = open(arch_fname).read()
2.6.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.6.0,if weights_fname is not None:
2.6.0,sequence_dnn.model.load_weights(weights_fname)
2.6.0,return sequence_dnn
2.6.0,create temporary fasta files
2.6.0,run command
2.6.0,remove fasta files
2.6.0,write test fasta file
2.6.0,test gkmsvm
2.6.0,get classification results
2.6.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.6.0,"Using canonical smiles for glycine, as in original research paper"
2.6.0,Atom features with padding
2.6.0,A_tilda_k computation
2.6.0,Final feed_dict setup
2.6.0,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.6.0,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.6.0,self.num_node_features)
2.6.0,Fit models
2.6.0,Args
2.6.0,2017 DeepCrystal Technologies - Patrick Hop
2.6.0,
2.6.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.6.0,
2.6.0,MIT License - have fun!!
2.6.0,===========================================================
2.6.0,x = F.selu( fc(x) )
2.6.0,x = F.selu( fc(x) )
2.6.0,2017 DeepCrystal Technologies - Patrick Hop
2.6.0,
2.6.0,Data loading a splitting file
2.6.0,
2.6.0,MIT License - have fun!!
2.6.0,===========================================================
2.6.0,Args
2.6.0,TODO (VIGS25): Account for the reload option
2.6.0,Downloading train files
2.6.0,Parsing training data
2.6.0,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.6.0,Test Files loading
2.6.0,One Hot Featurization
2.6.0,Consistency check
2.6.0,Handle output layer
2.6.0,Iterate over all previous tasks.
2.6.0,prev_layers is a list with elements of size
2.6.0,"(batch_size, layer_sizes[i-1])"
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,Save an initial checkpoint.
2.6.0,Turns out there are valid cases where we don't want pad-batches
2.6.0,on by default.
2.6.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.0,Run training op.
2.6.0,Always save a final checkpoint when complete.
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,Note that we divide by the batch size and not the number of
2.6.0,"non-zero weight examples in the batch.  Also, instead of using"
2.6.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.0,calculate with div/sum so it stays on the GPU.
2.6.0,aggregated costs
2.6.0,weight decay
2.6.0,Dummy placeholders
2.6.0,Dummy placeholders
2.6.0,run eval data through the model
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,Handle edge case when batch-size is 1.
2.6.0,Prune away any padding that was added
2.6.0,allow_soft_placement=True allows ops without a GPU implementation
2.6.0,to run on the CPU instead.
2.6.0,!/usr/bin/python
2.6.0,
2.6.0,Copyright 2015 Google Inc.
2.6.0,
2.6.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.6.0,you may not use this file except in compliance with the License.
2.6.0,You may obtain a copy of the License at
2.6.0,
2.6.0,http://www.apache.org/licenses/LICENSE-2.0
2.6.0,
2.6.0,"Unless required by applicable law or agreed to in writing, software"
2.6.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.6.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.6.0,See the License for the specific language governing permissions and
2.6.0,limitations under the License.
2.6.0,parse CheckpointState proto
2.6.0,parse path to actual checkpoint
2.6.0,the provided mask has to be the same shape as features
2.6.0,test k = 1..4
2.6.0,central moments
2.6.0,standardized moments
2.6.0,central across one axis
2.6.0,standardized across one axis
2.6.0,Fit just on task zero
2.6.0,Notice that we keep the session open
2.6.0,Fit on task one
2.6.0,The predictions for task zero should not change after training
2.6.0,on task one.
2.6.0,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.6.0,!/usr/bin/python
2.6.0,
2.6.0,Copyright 2015 Google Inc.
2.6.0,
2.6.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.6.0,you may not use this file except in compliance with the License.
2.6.0,You may obtain a copy of the License at
2.6.0,
2.6.0,http://www.apache.org/licenses/LICENSE-2.0
2.6.0,
2.6.0,"Unless required by applicable law or agreed to in writing, software"
2.6.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.6.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.6.0,See the License for the specific language governing permissions and
2.6.0,limitations under the License.
2.6.0,get the divisor
2.6.0,compute the requested central moment
2.6.0,"note that mean is a raw moment, not a central moment"
2.6.0,TODO(user): median is not implemented yet in TensorFlow
2.6.0,Add the input features.
2.6.0,"layer has shape [None, layer_sizes[i]]"
2.6.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.6.0,TODO(rbharath): Might want to make it feasible to have multiple
2.6.0,bypass layers.
2.6.0,Construct task bypass layer
2.6.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.6.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.6.0,"layer has shape [None, layer_sizes[i]]"
2.6.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.6.0,TODO(rbharath): Might want to make it feasible to have multiple
2.6.0,bypass layers.
2.6.0,Construct task bypass layer
2.6.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.6.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.6.0,Consistency check
2.6.0,Lazily created by _get_shared_session().
2.6.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.6.0,when subclass-overridden methods use the same scopes.
2.6.0,Setup graph
2.6.0,Create placeholders
2.6.0,Handle output layer
2.6.0,Iterate over all previous tasks.
2.6.0,prev_layers is a list with elements of size
2.6.0,"(batch_size, layer_sizes[i-1])"
2.6.0,Note that we divide by the batch size and not the number of
2.6.0,"non-zero weight examples in the batch.  Also, instead of using"
2.6.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.0,calculate with div/sum so it stays on the GPU.
2.6.0,aggregated costs
2.6.0,weight decay
2.6.0,Dummy placeholders
2.6.0,Dummy placeholders
2.6.0,run eval data through the model
2.6.0,"Shape (n_tasks, n__samples)"
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,Handle edge case when batch-size is 1.
2.6.0,with self._get_shared_session(train=True) as sess:
2.6.0,Save an initial checkpoint.
2.6.0,Always save a final checkpoint when complete.
2.6.0,Note that we divide by the batch size and not the number of
2.6.0,"non-zero weight examples in the batch.  Also, instead of using"
2.6.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.0,calculate with div/sum so it stays on the GPU.
2.6.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.6.0,Dummy placeholders
2.6.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.6.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.6.0,Dummy placeholders
2.6.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.6.0,allow_soft_placement=True allows ops without a GPU implementation
2.6.0,to run on the CPU instead.
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,Turns out there are valid cases where we don't want pad-batches
2.6.0,on by default.
2.6.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.6.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,"(n_samples, n_classes)"
2.6.0,"(n_samples, n_tasks, n_classes)"
2.6.0,Save hyperparameters
2.6.0,Guard variable to make sure we don't Restore() this model
2.6.0,from a disk checkpoint more than once.
2.6.0,"Path to save checkpoint files, which matches the"
2.6.0,replicated supervisor's default path.
2.6.0,Lazily created by _get_shared_session().
2.6.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.6.0,when subclass-overridden methods use the same scopes.
2.6.0,Setup graph
2.6.0,Note that we divide by the batch size and not the number of
2.6.0,"non-zero weight examples in the batch.  Also, instead of using"
2.6.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.0,calculate with div/sum so it stays on the GPU.
2.6.0,aggregated costs
2.6.0,weight decay
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,Save an initial checkpoint.
2.6.0,Define the code that runs on a separate thread to feed data into the queue.
2.6.0,Main training loop.
2.6.0,Run training op.
2.6.0,We have reached the end of an epoch.
2.6.0,We have reached the end of the data.
2.6.0,Always save a final checkpoint when complete.
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,allow_soft_placement=True allows ops without a GPU implementation
2.6.0,to run on the CPU instead.
2.6.0,gpu memory growth option
2.6.0,gpu memory growth option
2.6.0,TODO(rbharath): Is setting train=False right here?
2.6.0,Discard any padded predictions
2.6.0,The iterbatches does padding with zero-weight examples on the last batch.
2.6.0,Remove padded examples.
2.6.0,Special case to handle singletasks.
2.6.0,The iterbatches does padding with zero-weight examples on the last batch.
2.6.0,Remove padded examples.
2.6.0,TODO(rbharath): Verify this can be safely removed.
2.6.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.6.0,""""""""
2.6.0,Evaluates the performance of this model on specified dataset.
2.6.0,
2.6.0,Parameters
2.6.0,----------
2.6.0,dataset: dc.data.Dataset
2.6.0,Dataset object.
2.6.0,metric: deepchem.metrics.Metric
2.6.0,Evaluation metric
2.6.0,transformers: list
2.6.0,List of deepchem.transformers.Transformer
2.6.0,Returns
2.6.0,-------
2.6.0,dict
2.6.0,Maps tasks to scores under metric.
2.6.0,""""""""
2.6.0,"evaluator = Evaluator(self, dataset, transformers)"
2.6.0,scores = evaluator.compute_model_performance(metrics)
2.6.0,return scores
2.6.0,checkpoints look like model_dir/model.ckpt-N
2.6.0,"self._save_path is ""model_dir/model.ckpt"""
2.6.0,run eval data through the model
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,run eval data through the model
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,Note that softmax is already applied in construct_grpah
2.6.0,run eval data through the model
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,Handle edge case when batch-size is 1.
2.6.0,Prune away any padding that was added
2.6.0,Handle case of 0-dimensional scalar output
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,inputs placeholder
2.6.0,data preprocessing and augmentation
2.6.0,first conv layer
2.6.0,downsample by max pooling
2.6.0,each module is a residual convolutional block
2.6.0,followed by a convolutional downsample layer
2.6.0,max pooling over the final outcome
2.6.0,fully connected layers
2.6.0,dropout for dense layers
2.6.0,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.6.0,weight decay regularizer
2.6.0,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.6.0,sample cut ratio from a clipped gaussian
2.6.0,train/valid differences
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,Define and build model
2.6.0,model.restore()
2.6.0,Set random seeds
2.6.0,Setup directories
2.6.0,Model constants
2.6.0,Load and transform datasets
2.6.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.0,Atomic convolution variables
2.6.0,at = atomic numbers (atom types)
2.6.0,"radial basis function parameters [cutoff, mean, width]"
2.6.0,Model hyperparameters
2.6.0,Initialize model
2.6.0,Fit model
2.6.0,Evaluate model
2.6.0,Set random seeds
2.6.0,Setup directories
2.6.0,Model constants
2.6.0,Load and transform datasets
2.6.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.0,Atomic convolution variables
2.6.0,at = atomic numbers (atom types)
2.6.0,"radial basis function parameters [cutoff, mean, width]"
2.6.0,Model hyperparameters
2.6.0,Initialize model
2.6.0,Fit model
2.6.0,Evaluate model
2.6.0,Set random seeds
2.6.0,Setup directories
2.6.0,Model constants
2.6.0,Load and transform datasets
2.6.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.0,Atomic convolution variables
2.6.0,at = atomic numbers (atom types)
2.6.0,"radial basis function parameters [cutoff, mean, width]"
2.6.0,Model hyperparameters
2.6.0,Initialize model
2.6.0,Fit model
2.6.0,Evaluate model
2.6.0,Set random seeds
2.6.0,Setup directories
2.6.0,Model constants
2.6.0,Load and transform datasets
2.6.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.6.0,Atomic convolution variables
2.6.0,at = atomic numbers (atom types)
2.6.0,"radial basis function parameters [cutoff, mean, width]"
2.6.0,Model hyperparameters
2.6.0,Initialize model
2.6.0,Fit model
2.6.0,Evaluate model
2.6.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.6.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.6.0,test_scores = test_evaluator.compute_model_performance(metric)
2.6.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.6.0,param.update(test_scores)
2.6.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.0,for transformer in transformers:
2.6.0,train_dataset = transformer.transform(train_dataset)
2.6.0,test_dataset = transformer.transform(test_dataset)
2.6.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.0,for transformer in transformers:
2.6.0,train_dataset = transformer.transform(train_dataset)
2.6.0,test_dataset = transformer.transform(test_dataset)
2.6.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.0,for transformer in transformers:
2.6.0,train_dataset = transformer.transform(train_dataset)
2.6.0,test_dataset = transformer.transform(test_dataset)
2.6.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.6.0,for transformer in transformers:
2.6.0,train_dataset = transformer.transform(train_dataset)
2.6.0,test_dataset = transformer.transform(test_dataset)
2.6.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.6.0,Create some directories for analysis
2.6.0,The base_dir holds the results of all analysis
2.6.0,Make directories to store the raw and featurized datasets.
2.6.0,Load PDBBind dataset
2.6.0,Define featurizers
2.6.0,Currently featurizes with shard_size=1
2.6.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.6.0,This could be done with openbabel in python
2.6.0,Compute cells for this molecule. O(constant)
2.6.0,min == max if molecule is planar in some direction
2.6.0,we should still create a bin
2.6.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.6.0,Note neighbors contains self!
2.6.0,Associate each atom with cell it belongs to. O(N)
2.6.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.6.0,"conditions, so does wrapround. O(constant)"
2.6.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.6.0,Accept as neighbors only those within threshold. This computation should be
2.6.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.6.0,Sort neighbors by distance
2.6.0,Pick up to max_num_neighbors
2.6.0,Type of data created by this featurizer
2.6.0,assumes that every array is of the same dimension
2.6.0,rem_dataset is remaining portion of dataset
2.6.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.6.0,to k-1.
2.6.0,returns list of per column sum of non zero elements
2.6.0,Compute number of actives needed per task.
2.6.0,loop through each column and obtain index required to splice out for
2.6.0,required fraction of hits
2.6.0,Find the first index where the cumulative number of actives equals
2.6.0,the actives_count
2.6.0,Note that np.where tells us last index required to exceed
2.6.0,"actives_count, so we actually want the following location"
2.6.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.6.0,potentially refactor those to match this.
2.6.0,Handle edge case where frac_split is 1
2.6.0,Create weight matrices fpor two haves.
2.6.0,copy over up to required index for weight first_split
2.6.0,check out if any rows in either w_1 or w_2 are just zeros
2.6.0,"Obtain original x, y, and w arrays and shuffle"
2.6.0,calculate percent split for valid (out of test and valid)
2.6.0,"split test data into valid and test, treating sub test set also as sparse"
2.6.0,rem_dataset is remaining portion of dataset
2.6.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.6.0,to k-1.
2.6.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.6.0,This can be generalized in the future with some common demoninator determination.
2.6.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.6.0,Append remaining examples to train
2.6.0,Sort by increasing MW
2.6.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.6.0,for m_idx in cluster:
2.6.0,"continue until we find an active in all the tasks, otherwise we can't"
2.6.0,compute a meaningful AUC
2.6.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.6.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.6.0,Sort from largest to smallest scaffold sets
2.6.0,Sort from largest to smallest scaffold sets
2.6.0,"(n_samples, n_classes)"
2.6.0,"(n_samples, n_tasks, n_classes)"
2.6.0,Save hyperparameters
2.6.0,Guard variable to make sure we don't Restore() this model
2.6.0,from a disk checkpoint more than once.
2.6.0,"Path to save checkpoint files, which matches the"
2.6.0,replicated supervisor's default path.
2.6.0,Lazily created by _get_shared_session().
2.6.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.6.0,when subclass-overridden methods use the same scopes.
2.6.0,Setup graph
2.6.0,Note that we divide by the batch size and not the number of
2.6.0,"non-zero weight examples in the batch.  Also, instead of using"
2.6.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.6.0,calculate with div/sum so it stays on the GPU.
2.6.0,aggregated costs
2.6.0,weight decay
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,Save an initial checkpoint.
2.6.0,Turns out there are valid cases where we don't want pad-batches
2.6.0,on by default.
2.6.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.6.0,Run training op.
2.6.0,Always save a final checkpoint when complete.
2.6.0,############################################################# TIMING
2.6.0,############################################################# TIMING
2.6.0,allow_soft_placement=True allows ops without a GPU implementation
2.6.0,to run on the CPU instead.
2.6.0,TODO(rbharath): Is setting train=False right here?
2.6.0,Discard any padded predictions
2.6.0,The iterbatches does padding with zero-weight examples on the last batch.
2.6.0,Remove padded examples.
2.6.0,Special case to handle singletasks.
2.6.0,The iterbatches does padding with zero-weight examples on the last batch.
2.6.0,Remove padded examples.
2.6.0,TODO(rbharath): Verify this can be safely removed.
2.6.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.6.0,""""""""
2.6.0,Evaluates the performance of this model on specified dataset.
2.6.0,
2.6.0,Parameters
2.6.0,----------
2.6.0,dataset: dc.data.Dataset
2.6.0,Dataset object.
2.6.0,metric: deepchem.metrics.Metric
2.6.0,Evaluation metric
2.6.0,transformers: list
2.6.0,List of deepchem.transformers.Transformer
2.6.0,Returns
2.6.0,-------
2.6.0,dict
2.6.0,Maps tasks to scores under metric.
2.6.0,""""""""
2.6.0,"evaluator = Evaluator(self, dataset, transformers)"
2.6.0,scores = evaluator.compute_model_performance(metrics)
2.6.0,return scores
2.6.0,checkpoints look like logdir/model.ckpt-N
2.6.0,"self._save_path is ""logdir/model.ckpt"""
2.6.0,run eval data through the model
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,run eval data through the model
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,Note that softmax is already applied in construct_grpah
2.6.0,run eval data through the model
2.6.0,reshape to batch_size x n_tasks x ...
2.6.0,Handle edge case when batch-size is 1.
2.6.0,Prune away any padding that was added
2.6.0,Handle case of 0-dimensional scalar output
2.6.0,Dummy placeholders
2.6.0,Dummy placeholders
2.6.0,## AtomicNet fully-connected layer ops ###
2.6.0,## Atomicnet coordinate transform ops ###
2.6.0,## Atomicnet symmetry function kernel ops ###
2.6.0,## Atomicnet symmetry function ops ###
2.6.0,## Atomcnet symmetry function layer ops ###
2.6.0,We apply the radial pooling filter before atom type conv
2.6.0,to reduce computation
2.6.0,## Misc convenience ops ###
2.6.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.6.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.6.0,"game).  The average reward for any bet is slightly negative, so the best"
2.6.0,strategy is to walk away.
2.6.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.0,Optimize it.
2.6.0,"It should have learned that the expected value is very close to zero, and that the best"
2.6.0,action is to walk away.
2.6.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.6.0,get the same result.
2.6.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.0,Run the algorithm.
2.6.0,Save a file checkpoint.
2.6.0,Build the tree.
2.6.0,Compute the final probabilities and expected reward.
2.6.0,Mark this node as terminal
2.6.0,Expand this node.
2.6.0,Select the next action to perform.
2.6.0,Recursively build the tree.
2.6.0,Update statistics for this node.
2.6.0,Configuration file for the Sphinx documentation builder.
2.6.0,
2.6.0,This file only contains a selection of the most common options. For a full
2.6.0,list see the documentation:
2.6.0,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.6.0,-- Path setup --------------------------------------------------------------
2.6.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.6.0,add these directories to sys.path here. If the directory is relative to the
2.6.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.6.0,
2.6.0,-- Project information -----------------------------------------------------
2.6.0,"The full version, including alpha/beta/rc tags"
2.6.0,-- General configuration ---------------------------------------------------
2.6.0,"Add any Sphinx extension module names here, as strings. They can be"
2.6.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.6.0,ones.
2.6.0,Options for autodoc directives
2.6.0,How to represents typehints
2.6.0,"Add any paths that contain templates here, relative to this directory."
2.6.0,The suffix of source filenames.
2.6.0,The master toctree document.
2.6.0,autosectionlabel setting
2.6.0,"List of patterns, relative to source directory, that match files and"
2.6.0,directories to ignore when looking for source files.
2.6.0,This pattern also affects html_static_path and html_extra_path.
2.6.0,"If true, the current module name will be prepended to all description"
2.6.0,unit titles (such as .. function::).
2.6.0,-- Options for HTML output -------------------------------------------------
2.6.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.6.0,a list of builtin themes.
2.6.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.6.0,"relative to this directory. They are copied after the builtin static files,"
2.6.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.6.0,The name of an image file (relative to this directory) to place at the top
2.6.0,of the sidebar.
2.6.0,Customize the sphinx theme
2.6.0,-- Source code links ---------------------------------------------------
2.6.0,Resolve function for the linkcode extension.
2.6.0,"try to find the file and line number, based on code from numpy:"
2.6.0,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.6.0,lines in the label file have format
2.6.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.6.0,"print line[0], line[3]"
2.6.0,"If you push the tag, please remove `.dev`"
2.6.0,Record inputs.
2.6.0,Create the output directory if necessary.
2.6.0,Create the optimizers for meta-optimization and task optimization.
2.6.0,Create a Checkpoint for saving.
2.6.0,Main optimization loop.
2.6.0,Do checkpointing.
2.6.0,flake8: noqa
2.6.0,This is a MetaLearner that learns to generate sine functions with variable
2.6.0,amplitude and phase.
2.6.0,Optimize it.
2.6.0,Test it out on some new tasks and see how it works.
2.6.0,Initially the model should do a bad job of fitting the sine function.
2.6.0,After one step of optimization it should do much better.
2.6.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.6.0,get the same result.
2.6.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.0,We know use_pose_generator_scores == False in this case
2.6.0,check whether self.featurizer is instance of ComplexFeaturizer or not
2.6.0,TODO: How to handle the failure here?
2.6.0,TODO(rbharath): The autodock vina source computes surface distances
2.6.0,which take into account the van der Waals radius of each atom type.
2.6.0,"Shape (N, M)"
2.6.0,"Shape (N, M)"
2.6.0,Parse complex
2.6.0,check filetypes
2.6.0,Define locations of log and output files
2.6.0,Write GNINA conf file
2.6.0,Run GNINA
2.6.0,read output and log
2.6.0,Parse complex
2.6.0,Prepare protein
2.6.0,Get protein centroid and range
2.6.0,TODO(rbharath: Does vina divide box dimensions by 2?
2.6.0,Prepare ligand
2.6.0,Write Vina conf file
2.6.0,Define locations of output files
2.6.0,flake8: noqa
2.6.0,We provide no scoring model so the docker won't score
2.6.0,Check only one output since num_modes==1
2.6.0,We provide no scoring model so the docker won't score
2.6.0,Check only one output since num_modes==1
2.6.0,Let's turn on logging since this test will run for a while
2.6.0,Check returned files exist
2.6.0,Let's turn on logging since this test will run for a while
2.6.0,Check returned files exist
2.6.0,"Where d is greater than zero, the repulsion is just zeros"
2.6.0,"When d is 0, this should just be 1"
2.6.0,"When d == 0, the hbond interaction is 0"
2.6.0,The exponential returns 1 when input 0.
2.6.0,This exponential returns 1 when input 3
2.6.0,Let's turn on logging since this test will run for a while
2.6.0,Let's turn on logging since this test will run for a while
2.6.0,Let's turn on logging since this test will run for a while
2.6.0,Let's turn on logging since this test will run for a while
2.6.0,Let's turn on logging since this test will run for a while
2.6.0,Note this may download autodock Vina...
2.6.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.6.0,Test that every atom in pocket maps exists
2.6.0,scalar case
2.6.0,per-example case
2.6.0,This is a little arcane but it repeats w across tasks.
2.6.0,"If w.shape == (n_samples, 1) handle it as 1D"
2.6.0,"w.shape == (n_samples, n_tasks)"
2.6.0,scalar case
2.6.0,Handle n_classes/n_task shape ambiguity
2.6.0,Add in task dimension
2.6.0,Insert a task dimension (we know n_tasks=1 from above0
2.6.0,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.6.0,Handle classification. We need to convert labels into one-hot representation.
2.6.0,check whether n_classes is int or not
2.6.0,Handle n_classes/n_task shape ambiguity
2.6.0,Add in task dimension
2.6.0,Make everything 2D so easy to handle
2.6.0,Handle each task separately.
2.6.0,Handle continuous class probabilites of positive class for binary
2.6.0,Fill in class 0 probabilities
2.6.0,Add a task dimension to concatenate on
2.6.0,Handle binary labels
2.6.0,"make y_hot of shape (N, n_classes)"
2.6.0,Add a task dimension to concatenate on
2.6.0,Insert a task dimension
2.6.0,"Now of shape (N,)"
2.6.0,"Now of shape (N, 1)"
2.6.0,"Returns shape (N, n_tasks)"
2.6.0,"Now of shape (N,)"
2.6.0,"Now of shape (N, n_classes)"
2.6.0,"Now of shape (N, 1, n_classes)"
2.6.0,"Returns shape (N, n_tasks, n_classes)"
2.6.0,These are some smart defaults
2.6.0,These are some smart defaults corresponding to sklearn's required
2.6.0,behavior
2.6.0,Attempt some limited shape imputation to find n_tasks
2.6.0,check whether n_tasks is int or not
2.6.0,This is because `normalize_weight_shape` require int value.
2.6.0,FIXME: Incompatible types in assignment
2.6.0,Attempt to convert both into the same type
2.6.0,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.6.0,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.6.0,initialize fwd and reverse scores to -infinity
2.6.0,"cross-correlate separately for each base,"
2.6.0,for both the PSSM and its reverse complement
2.6.0,sum over the bases
2.6.0,take max of fwd and reverse scores at each position
2.6.0,"Shape (N_sequences, num_tasks)"
2.6.0,check whether wild_type_predictions is np.ndarray or not
2.6.0,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.6.0,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.6.0,Mutates every position of the sequence to every letter
2.6.0,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.6.0,Breakdown:
2.6.0,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.6.0,remove wild-type
2.6.0,len(arange) = N_letters * sequence_length
2.6.0,len(horizontal cycle) = N_letters * sequence_length
2.6.0,add mutant
2.6.0,make mutant predictions
2.6.0,check whether wild_type_predictions is np.ndarray or not
2.6.0,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.6.0,validation
2.6.0,flake8: noqa
2.6.0,metric class
2.6.0,metrics utils
2.6.0,sklearn & scipy score function
2.6.0,original score function
2.6.0,Get a random prediction matrix
2.6.0,"Of shape (N, n_classes)"
2.6.0,"Of shape (N, 1, n_classes)"
2.6.0,This has w for each task.
2.6.0,Best score case
2.6.0,Worst score case
2.6.0,best case
2.6.0,duplicate prediction value
2.6.0,Encode motif
2.6.0,"sequences now has shape (3, 4, 5, 1)"
2.6.0,"sequences now has shape (3, 4, 5, 1)"
2.6.0,Construct and train SequenceDNN model
2.6.0,Call in-silico mutagenesis
2.6.0,Construct and train SequenceDNN model
2.6.0,Call in-silico mutagenesis
2.6.0,Check nonzero elements exist
2.6.0,Special case handling of single input
2.6.0,Featurize task results iff they exist.
2.6.0,Filter out examples where featurization failed.
2.6.0,"For prospective data where results are unknown, it"
2.6.0,makes no sense to have y values or weights.
2.6.0,Featurize task results if they exist.
2.6.0,Filter out examples where featurization failed.
2.6.0,"For prospective data where results are unknown, it"
2.6.0,makes no sense to have y values or weights.
2.6.0,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.6.0,The field in which load_sdf_files return value stores smiles
2.6.0,Special case handling of single input
2.6.0,Featurize task results iff they exist.
2.6.0,Filter out examples where featurization failed.
2.6.0,"For prospective data where results are unknown, it"
2.6.0,makes no sense to have y values or weights.
2.6.0,Process legacy toggle
2.6.0,Set attributes
2.6.0,Handle special featurizer cases
2.6.0,Set self.featurizer
2.6.0,"(X, y, w, ids)"
2.6.0,TODO don't convert all sequences into np array (allow shards)
2.6.0,Check if line is a header
2.6.0,Handle empty sequence
2.6.0,TODO log attempts to add empty sequences every shard
2.6.0,Annotate start/stop of sequence
2.6.0,Sometimes zip files contain directories within. Traverse directories
2.6.0,TODO(rbharath): Add support for more extensions
2.6.0,Sort image files
2.6.0,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.6.0,Remove support indices
2.6.0,Remove support indices
2.6.0,Remove support indices
2.6.0,Get task specific entries
2.6.0,Now just get weights for this task
2.6.0,Get task specific entries
2.6.0,Now just get weights for this task
2.6.0,Now just get weights for this task
2.6.0,Now just get weights for this task
2.6.0,Split data into pos and neg lists.
2.6.0,No replacement allowed for supports
2.6.0,Handle one-d vs. non one-d feature matrices
2.6.0,Init the iterator
2.6.0,Set initial iterator state
2.6.0,support = self.supports[task][self.trial_num]
2.6.0,Increment and update logic
2.6.0,Init the iterator
2.6.0,Set initial iterator state
2.6.0,support = self.supports[task][self.trial_num]
2.6.0,Increment and update logic
2.6.0,Ensure that every worker will pick the same random order for each epoch.
2.6.0,Ensure that every worker will pick the same random order for each epoch.
2.6.0,"By invariant of when this is called, can assume num_samples > 0"
2.6.0,and num_samples < batch_size
2.6.0,Fill in batch arrays
2.6.0,"By invariant of when this is called, can assume num_samples > 0"
2.6.0,and num_samples < batch_size
2.6.0,Fill in batch arrays
2.6.0,Only the first set of copy will be counted in training loss
2.6.0,Retrieve the first sample so we can determine the dtypes.
2.6.0,Create a Tensorflow Dataset.
2.6.0,Find the X values.
2.6.0,Find the y values.
2.6.0,Find the w values.
2.6.0,Find the ids.
2.6.0,"Set labels to be zero, with zero weights"
2.6.0,Load obsolete format -> save in new format
2.6.0,note that this corresponds to the _construct_metadata column order
2.6.0,Create temp directory to store resharded version
2.6.0,Get correct shapes for y/w
2.6.0,Write data in new shards
2.6.0,Handle shapes
2.6.0,Note that this means that DiskDataset resharding currently doesn't
2.6.0,work for datasets that aren't regression/classification.
2.6.0,Handle spillover from last shard
2.6.0,Should have updated to non-legacy metadata
2.6.0,Note that this resets the cache internally
2.6.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.6.0,"than process based pools, since process based pools need to pickle/serialize"
2.6.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.6.0,we're actually protected by the GIL.
2.6.0,mp.dummy aliases ThreadPool to Pool
2.6.0,(ytz): this skips everything except possibly the last shard
2.6.0,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.6.0,make a NumpyDataset under the hood
2.6.0,"raw_data = (X, y, w, ids)"
2.6.0,Protect against generator exhaustion
2.6.0,This ensures tasks are consistent for all datasets
2.6.0,determine the shard sizes of the datasets to merge
2.6.0,"otherwise the entire dataset is the ""shard size"""
2.6.0,we must reshard the dataset to have a uniform size
2.6.0,choose the smallest shard size
2.6.0,Get full dataset in memory
2.6.0,Shuffle in memory
2.6.0,Write shuffled shards out to disk
2.6.0,Shuffle the arrays corresponding to each row in metadata_df
2.6.0,Reset cache
2.6.0,See if we have a cached copy of this shard.
2.6.0,"We don't, so load it from disk."
2.6.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.6.0,Try to cache this shard for later use.  Since the normal usage pattern is
2.6.0,"a series of passes through the whole dataset, there's no point doing"
2.6.0,anything fancy.  It never makes sense to evict another shard from the
2.6.0,"cache to make room for this one, because we'll probably want that other"
2.6.0,shard again before the next time we want this one.  So just cache as many
2.6.0,as we can and then stop.
2.6.0,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.6.0,Handle edge case with empty indices
2.6.0,We use two loops here. The outer while loop walks over selection shards
2.6.0,(the chunks of the indices to select that should go into separate
2.6.0,"output shards), while the inner for loop walks over the shards in the"
2.6.0,source datasets to select out the shard indices from that  source shard
2.6.0,Find indices which rest in this shard
2.6.0,Need to offset indices to fit within shard_size
2.6.0,Handle empty case where no data from this shard needed
2.6.0,Handle the case of datasets with y/w missing
2.6.0,Break if all indices have been used up already
2.6.0,Note these will be in the sorted order
2.6.0,We need to recover the original ordering. We can do this by using
2.6.0,np.where to find the locatios of the original indices in the sorted
2.6.0,indices.
2.6.0,We know there's only one match for np.where since this is a
2.6.0,"permutation, so the [0][0] pulls out the exact match location."
2.6.0,If shape metadata is available use it to directly compute shape from
2.6.0,metadata
2.6.0,"In absense of shape metadata, fall back to loading data from disk to"
2.6.0,find shape.
2.6.0,Case n_samples should be 1
2.6.0,flake8: noqa
2.6.0,TODO(rbharath): Get rid of * import
2.6.0,Test merging of numpy datasets
2.6.0,Load MUV dataset
2.6.0,Do an approximate comparison since splits are sometimes slightly off from
2.6.0,the exact fraction.
2.6.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.6.0,reloading will cause the transform to be reapplied. This is undesirable in
2.6.0,almost all cases. Need to understand a method to fix this.
2.6.0,The shuffling should have switched up the ordering
2.6.0,But all the same entries should still be present
2.6.0,All the data should have same shape
2.6.0,The shuffling should have switched up the ordering
2.6.0,But all the same entries should still be present
2.6.0,All the data should have same shape
2.6.0,The ids should now store the performed permutation. Check that the
2.6.0,original dataset is recoverable.
2.6.0,The ids should now store the performed permutation. Check that the
2.6.0,original dataset is recoverable.
2.6.0,Generate data
2.6.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.6.0,around for testing resharding.
2.6.0,Set cache to 0 size to avoid cache hiding errors
2.6.0,Generate data
2.6.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.6.0,around for testing resharding.
2.6.0,Set cache to 0 size to avoid cache hiding errors
2.6.0,Featurize emols dataset
2.6.0,example.fasta contains 3 sequences each of length 58.
2.6.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.6.0,"There is one ""image channel""."
2.6.0,"Due to FASTALoader redesign, expected shape is now (3, 58, 5)"
2.6.0,TODO: test with full uniprot file once sharding support is added.
2.6.0,Generate dummy dataset
2.6.0,Generate dummy dataset
2.6.0,Generate dummy dataset
2.6.0,Set last n_samples/2 weights to 0
2.6.0,Check that no support elements are sample from zero-weight samples
2.6.0,Generate dummy dataset
2.6.0,Generate dummy dataset
2.6.0,Create support generator
2.6.0,Generate dummy dataset
2.6.0,Create support generator
2.6.0,Generate dummy dataset
2.6.0,Assert all support elements have been removed
2.6.0,Generate dummy dataset
2.6.0,Assert all remove elements have been removed
2.6.0,Generate dummy dataset
2.6.0,Assert all support elements have been removed
2.6.0,Generate dummy dataset
2.6.0,Assert all remove elements have been removed
2.6.0,Generate dummy dataset
2.6.0,Set last n_samples/2 weights to 0
2.6.0,Sample from first n_samples/2 elements for support
2.6.0,Should lie within first n_samples/2 samples only
2.6.0,Generate dummy dataset
2.6.0,Create support generator
2.6.0,Generate dummy dataset
2.6.0,This is necessary since from_numpy adds in shape information
2.6.0,This is necessary since from_numpy adds in shape information
2.6.0,This is necessary since from_numpy adds in shape information
2.6.0,Generate data
2.6.0,Generate data
2.6.0,Generate data
2.6.0,Should now have 10 shards
2.6.0,This is the shape of legacy_data
2.6.0,legacy_dataset is a dataset in the legacy format kept around for testing
2.6.0,purposes.
2.6.0,This is the shape of legacy_data_reshard
2.6.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.6.0,around for testing
2.6.0,Should now have 10 shards
2.6.0,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.6.0,Test constructor reload works for legacy format
2.6.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.6.0,around for testing resharding.
2.6.0,Reshard copy
2.6.0,Check metadata has been updated
2.6.0,First try using images for X.
2.6.0,Now try using images for y.
2.6.0,Transform it
2.6.0,Test on identity matrix
2.6.0,Generate random sparse features dataset
2.6.0,Test edge case with array of all zeros
2.6.0,Test cases where n_samples < 2*n_samples < batch_size
2.6.0,Test cases where n_samples < batch_size
2.6.0,Test case where n_samples == batch_size
2.6.0,Test case for object featurization.
2.6.0,Test case for more complicated object featurization
2.6.0,Test case with multidimensional data
2.6.0,Test cases where n_samples < 2*n_samples < batch_size
2.6.0,Test cases where n_samples < batch_size
2.6.0,Test case where n_samples == batch_size
2.6.0,Test case for object featurization.
2.6.0,Test case for more complicated object featurization
2.6.0,Test case with multidimensional data
2.6.0,Test first resharding worked
2.6.0,Test second resharding worked
2.6.0,approx 1/15! chance of equality
2.6.0,Generate data
2.6.0,Generate data
2.6.0,Transform it
2.6.0,special case to test
2.6.0,deterministic
2.6.0,non-deterministic
2.6.0,we don't know the order in which the shards are iterated in.
2.6.0,Check that we have all the data in
2.6.0,Test iterating in order.
2.6.0,Test iterating out of order.
2.6.0,Test iterating in batches.
2.6.0,Test iterating with multiple workers.
2.6.0,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.6.0,Try specifying particular columns.
2.6.0,Test id shrinkage
2.6.0,Test task shrinkage
2.6.0,Test max print size
2.6.0,Create image file
2.6.0,Create zip of image file
2.6.0,Create zip of multiple image files
2.6.0,"Create zip of multiple image files, multiple_types"
2.6.0,Create image directory
2.6.0,These are the known dimensions of face.png
2.6.0,These are the known dimensions of face.png
2.6.0,TODO(rbharath): Where are the color channels?
2.6.0,"Since the different files have different shapes, makes an object array"
2.6.0,Splits featurized samples into train/test
2.6.0,Splits featurized samples into train/test
2.6.0,Splits featurized samples into train/test
2.6.0,Splits featurized samples into train/test
2.6.0,Now perform move
2.6.0,Only for debug!
2.6.0,Make directories to store the raw and featurized datasets.
2.6.0,Load dataset
2.6.0,Featurize tox21 dataset
2.6.0,featurization
2.6.0,train/valid split.
2.6.0,singletask load
2.6.0,comparison
2.6.0,Only for debug!
2.6.0,Make directories to store the raw and featurized datasets.
2.6.0,Load dataset
2.6.0,Featurize tox21 dataset
2.6.0,For debugging purposes
2.6.0,multitask load
2.6.0,Do train/valid split.
2.6.0,singletask load
2.6.0,comparison
2.6.0,Get the labels/weights
2.6.0,Normalize shapes
2.6.0,Remove labels with zero weights
2.6.0,Note that we may have 0 elements of a given class since we remove those
2.6.0,labels with zero weight.
2.6.0,this works because y is 1D
2.6.0,This is the right ratio since int(N/num_c) * num_c \approx N
2.6.0,for all classes
2.6.0,Flattening is safe because of shape check above
2.6.0,Hack to allow for easy unpickling:
2.6.0,http://stefaanlippens.net/pickleproblem
2.6.0,Some transformation must happen
2.6.0,Add this case in to handle non-DiskDataset that should be written to disk
2.6.0,Note that transformers have to be undone in reversed order
2.6.0,Handle division by zero
2.6.0,Handle division by zero
2.6.0,Control for pathological case with no variance.
2.6.0,Handle case with 1 task correctly
2.6.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.6.0,Find the task dimension of z
2.6.0,Prevent broadcasting on wrong dimension
2.6.0,BalancingTransformer can only transform weights.
2.6.0,Compute weighting factors from dataset.
2.6.0,Handle 1-D case
2.6.0,Remove labels with zero weights
2.6.0,Note that we may have 0 elements of a given class since we remove those
2.6.0,labels with zero weight. This typically happens in multitask datasets
2.6.0,where some datapoints only have labels for some tasks.
2.6.0,this works because task_y is 1D
2.6.0,This is the right ratio since N_task/num_c * num_c = N_task
2.6.0,for all classes
2.6.0,Set to the class weight computed previously
2.6.0,Need this for transform_y
2.6.0,Handle 1D case
2.6.0,THis reshape is safe because of guard above.
2.6.0,map the indices to labels
2.6.0,generating batch of data by slicing similarity matrix
2.6.0,into 100*reference_dataset_length
2.6.0,concatenate batches of data together
2.6.0,highest similarity is 1: target is in the reference
2.6.0,use the following K points
2.6.0,"highest less than 1: target not in the reference, use top K points"
2.6.0,calculate matrix multiplicatin on slices
2.6.0,concatenate the slices together
2.6.0,list of calculation orders for DAGs
2.6.0,stemming from one specific atom in the molecule
2.6.0,starting from the adjacency list derived by graphconv featurizer
2.6.0,"number of atoms, also number of DAGs"
2.6.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.6.0,each step calculating graph features for one atom.
2.6.0,`max_atoms` is the maximum number of steps
2.6.0,each iteration generates the DAG starting from atom with index `count`
2.6.0,"list of lists, elements represent the calculation orders"
2.6.0,for atoms in the current graph
2.6.0,starting from the target atom with index `count`
2.6.0,flags of whether the atom is already included in the DAG
2.6.0,atom `count` is in the DAG
2.6.0,recording number of radial propagation steps
2.6.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.6.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.6.0,will be added in the second loop(radial=1).
2.6.0,atoms i-bond away will be added in i-th loop
2.6.0,"when molecules have separate parts, starting from one part,"
2.6.0,it is not possible to include all atoms.
2.6.0,this break quit the loop when going into such condition
2.6.0,reinitialize targets for next iteration
2.6.0,atoms connected to current_atom
2.6.0,generate the dependency map of current DAG
2.6.0,atoms connected to `current_atoms`(and not included in the DAG)
2.6.0,"are added, and will be the `current_atoms` for next iteration."
2.6.0,"DAG starts from the target atom, calculation should go in reverse"
2.6.0,`edge[1]` is the parent of `edge[0]`
2.6.0,"after this loop, `parents[i]` includes all parents of atom i"
2.6.0,manually adding the atom index into its parents list
2.6.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.6.0,atoms with less parents(farther from the target atom) come first.
2.6.0,"graph features of atoms without parents will be first calculated,"
2.6.0,then atoms with more parents can be calculated in order
2.6.0,based on previously calculated graph features.
2.6.0,target atom of this DAG will be calculated in the last step
2.6.0,padding with `max_atoms`
2.6.0,padding
2.6.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.6.0,which is a max_atoms * max_atoms numpy array after padding
2.6.0,class ANITransformer(Transformer):
2.6.0,"""""""Performs transform from 3D coordinates to ANI symmetry functions"
2.6.0,Note
2.6.0,----
2.6.0,This class requires TensorFlow to be installed.
2.6.0,""""""""
2.6.0,"def __init__(self,"
2.6.0,"max_atoms=23,"
2.6.0,"radial_cutoff=4.6,"
2.6.0,"angular_cutoff=3.1,"
2.6.0,"radial_length=32,"
2.6.0,"angular_length=8,"
2.6.0,"atom_cases=[1, 6, 7, 8, 16],"
2.6.0,"atomic_number_differentiated=True,"
2.6.0,coordinates_in_bohr=True):
2.6.0,""""""""
2.6.0,Only X can be transformed
2.6.0,""""""""
2.6.0,import tensorflow as tf
2.6.0,self.max_atoms = max_atoms
2.6.0,self.radial_cutoff = radial_cutoff
2.6.0,self.angular_cutoff = angular_cutoff
2.6.0,self.radial_length = radial_length
2.6.0,self.angular_length = angular_length
2.6.0,self.atom_cases = atom_cases
2.6.0,self.atomic_number_differentiated = atomic_number_differentiated
2.6.0,self.coordinates_in_bohr = coordinates_in_bohr
2.6.0,self.compute_graph = self.build()
2.6.0,self.sess = tf.Session(graph=self.compute_graph)
2.6.0,self.transform_batch_size = 32
2.6.0,"super(ANITransformer, self).__init__(transform_X=True)"
2.6.0,"def transform_array(self, X, y, w):"
2.6.0,if self.transform_X:
2.6.0,X_out = []
2.6.0,num_transformed = 0
2.6.0,start = 0
2.6.0,batch_size = self.transform_batch_size
2.6.0,while True:
2.6.0,"end = min((start + 1) * batch_size, X.shape[0])"
2.6.0,X_batch = X[(start * batch_size):end]
2.6.0,output = self.sess.run(
2.6.0,"[self.outputs], feed_dict={self.inputs: X_batch})[0]"
2.6.0,X_out.append(output)
2.6.0,num_transformed = num_transformed + X_batch.shape[0]
2.6.0,logger.info('%i samples transformed' % num_transformed)
2.6.0,start += 1
2.6.0,if end >= len(X):
2.6.0,break
2.6.0,"X_new = np.concatenate(X_out, axis=0)"
2.6.0,assert X_new.shape[0] == X.shape[0]
2.6.0,"return (X_new, y, w)"
2.6.0,"def untransform(self, z):"
2.6.0,raise NotImplementedError(
2.6.0,"""Cannot untransform datasets with ANITransformer."")"
2.6.0,def build(self):
2.6.0,""""""" tensorflow computation graph for transform """""""
2.6.0,import tensorflow as tf
2.6.0,graph = tf.Graph()
2.6.0,with graph.as_default():
2.6.0,self.inputs = tf.keras.Input(
2.6.0,"dtype=tf.float32, shape=(None, self.max_atoms, 4))"
2.6.0,"atom_numbers = tf.cast(self.inputs[:, :, 0], tf.int32)"
2.6.0,flags = tf.sign(atom_numbers)
2.6.0,flags = tf.cast(
2.6.0,"tf.expand_dims(flags, 1) * tf.expand_dims(flags, 2), tf.float32)"
2.6.0,"coordinates = self.inputs[:, :, 1:]"
2.6.0,if self.coordinates_in_bohr:
2.6.0,coordinates = coordinates * 0.52917721092
2.6.0,"d = self.distance_matrix(coordinates, flags)"
2.6.0,"d_radial_cutoff = self.distance_cutoff(d, self.radial_cutoff, flags)"
2.6.0,"d_angular_cutoff = self.distance_cutoff(d, self.angular_cutoff, flags)"
2.6.0,"radial_sym = self.radial_symmetry(d_radial_cutoff, d, atom_numbers)"
2.6.0,"angular_sym = self.angular_symmetry(d_angular_cutoff, d, atom_numbers,"
2.6.0,coordinates)
2.6.0,self.outputs = tf.concat(
2.6.0,[
2.6.0,"tf.cast(tf.expand_dims(atom_numbers, 2), tf.float32), radial_sym,"
2.6.0,angular_sym
2.6.0,"],"
2.6.0,axis=2)
2.6.0,return graph
2.6.0,"def distance_matrix(self, coordinates, flags):"
2.6.0,""""""" Generate distance matrix """""""
2.6.0,import tensorflow as tf
2.6.0,max_atoms = self.max_atoms
2.6.0,"tensor1 = tf.stack([coordinates] * max_atoms, axis=1)"
2.6.0,"tensor2 = tf.stack([coordinates] * max_atoms, axis=2)"
2.6.0,# Calculate pairwise distance
2.6.0,"d = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"
2.6.0,# Masking for valid atom index
2.6.0,d = d * flags
2.6.0,return d
2.6.0,"def distance_cutoff(self, d, cutoff, flags):"
2.6.0,""""""" Generate distance matrix with trainable cutoff """""""
2.6.0,import tensorflow as tf
2.6.0,# Cutoff with threshold Rc
2.6.0,d_flag = flags * tf.sign(cutoff - d)
2.6.0,d_flag = tf.nn.relu(d_flag)
2.6.0,d_flag = d_flag * tf.expand_dims(
2.6.0,"tf.expand_dims((1 - tf.eye(self.max_atoms)), 0), -1)"
2.6.0,d = 0.5 * (tf.cos(np.pi * d / cutoff) + 1)
2.6.0,return d * d_flag
2.6.0,"def radial_symmetry(self, d_cutoff, d, atom_numbers):"
2.6.0,""""""" Radial Symmetry Function """""""
2.6.0,import tensorflow as tf
2.6.0,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.6.0,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.6.0,"Rs = np.linspace(0., self.radial_cutoff, self.radial_length)"
2.6.0,ita = np.ones_like(Rs) * 3 / (Rs[1] - Rs[0])**2
2.6.0,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, -1)), tf.float32)"
2.6.0,"ita = tf.cast(np.reshape(ita, (1, 1, 1, -1)), tf.float32)"
2.6.0,length = ita.get_shape().as_list()[-1]
2.6.0,"d_cutoff = tf.stack([d_cutoff] * length, axis=3)"
2.6.0,"d = tf.stack([d] * length, axis=3)"
2.6.0,out = tf.exp(-ita * tf.square(d - Rs)) * d_cutoff
2.6.0,if self.atomic_number_differentiated:
2.6.0,out_tensors = []
2.6.0,for atom_type in self.atom_cases:
2.6.0,selected_atoms = tf.expand_dims(
2.6.0,"tf.expand_dims(atom_numbers_embedded[:, :, atom_type], axis=1),"
2.6.0,axis=3)
2.6.0,"out_tensors.append(tf.reduce_sum(out * selected_atoms, axis=2))"
2.6.0,"return tf.concat(out_tensors, axis=2)"
2.6.0,else:
2.6.0,"return tf.reduce_sum(out, axis=2)"
2.6.0,"def angular_symmetry(self, d_cutoff, d, atom_numbers, coordinates):"
2.6.0,""""""" Angular Symmetry Function """""""
2.6.0,import tensorflow as tf
2.6.0,max_atoms = self.max_atoms
2.6.0,embedding = tf.eye(np.max(self.atom_cases) + 1)
2.6.0,"atom_numbers_embedded = tf.nn.embedding_lookup(embedding, atom_numbers)"
2.6.0,"Rs = np.linspace(0., self.angular_cutoff, self.angular_length)"
2.6.0,ita = 3 / (Rs[1] - Rs[0])**2
2.6.0,"thetas = np.linspace(0., np.pi, self.angular_length)"
2.6.0,zeta = float(self.angular_length**2)
2.6.0,"ita, zeta, Rs, thetas = np.meshgrid(ita, zeta, Rs, thetas)"
2.6.0,"zeta = tf.cast(np.reshape(zeta, (1, 1, 1, 1, -1)), tf.float32)"
2.6.0,"ita = tf.cast(np.reshape(ita, (1, 1, 1, 1, -1)), tf.float32)"
2.6.0,"Rs = tf.cast(np.reshape(Rs, (1, 1, 1, 1, -1)), tf.float32)"
2.6.0,"thetas = tf.cast(np.reshape(thetas, (1, 1, 1, 1, -1)), tf.float32)"
2.6.0,length = zeta.get_shape().as_list()[-1]
2.6.0,"vector_distances = tf.stack([coordinates] * max_atoms, 1) - tf.stack("
2.6.0,"[coordinates] * max_atoms, 2)"
2.6.0,"R_ij = tf.stack([d] * max_atoms, axis=3)"
2.6.0,"R_ik = tf.stack([d] * max_atoms, axis=2)"
2.6.0,"f_R_ij = tf.stack([d_cutoff] * max_atoms, axis=3)"
2.6.0,"f_R_ik = tf.stack([d_cutoff] * max_atoms, axis=2)"
2.6.0,# Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.6.0,"vector_mul = tf.reduce_sum(tf.stack([vector_distances] * max_atoms, axis=3) * \"
2.6.0,"tf.stack([vector_distances] * max_atoms, axis=2), axis=4)"
2.6.0,vector_mul = vector_mul * tf.sign(f_R_ij) * tf.sign(f_R_ik)
2.6.0,"theta = tf.acos(tf.math.divide(vector_mul, R_ij * R_ik + 1e-5))"
2.6.0,"R_ij = tf.stack([R_ij] * length, axis=4)"
2.6.0,"R_ik = tf.stack([R_ik] * length, axis=4)"
2.6.0,"f_R_ij = tf.stack([f_R_ij] * length, axis=4)"
2.6.0,"f_R_ik = tf.stack([f_R_ik] * length, axis=4)"
2.6.0,"theta = tf.stack([theta] * length, axis=4)"
2.6.0,"out_tensor = tf.pow((1. + tf.cos(theta - thetas)) / 2., zeta) * \"
2.6.0,tf.exp(-ita * tf.square((R_ij + R_ik) / 2. - Rs)) * f_R_ij * f_R_ik * 2
2.6.0,if self.atomic_number_differentiated:
2.6.0,out_tensors = []
2.6.0,"for id_j, atom_type_j in enumerate(self.atom_cases):"
2.6.0,for atom_type_k in self.atom_cases[id_j:]:
2.6.0,"selected_atoms = tf.stack([atom_numbers_embedded[:, :, atom_type_j]] * max_atoms, axis=2) * \"
2.6.0,"tf.stack([atom_numbers_embedded[:, :, atom_type_k]] * max_atoms, axis=1)"
2.6.0,selected_atoms = tf.expand_dims(
2.6.0,"tf.expand_dims(selected_atoms, axis=1), axis=4)"
2.6.0,out_tensors.append(
2.6.0,"tf.reduce_sum(out_tensor * selected_atoms, axis=(2, 3)))"
2.6.0,"return tf.concat(out_tensors, axis=2)"
2.6.0,else:
2.6.0,"return tf.reduce_sum(out_tensor, axis=(2, 3))"
2.6.0,def get_num_feats(self):
2.6.0,n_feat = self.outputs.get_shape().as_list()[-1]
2.6.0,return n_feat
2.6.0,flake8: noqa
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a y transformer
2.6.0,Check w is unchanged since this is a y transformer
2.6.0,Check y is now a logarithmic version of itself
2.6.0,Check that untransform does the right thing.
2.6.0,Check ids are unchanged.
2.6.0,Check y is unchanged since this is a X transformer
2.6.0,Check w is unchanged since this is a y transformer
2.6.0,Check y is now a logarithmic version of itself
2.6.0,Check that untransform does the right thing.
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a y transformer
2.6.0,Check w is unchanged since this is a y transformer
2.6.0,Check y is now a logarithmic version of itself
2.6.0,Check that untransform does the right thing.
2.6.0,Tests logarithmic data transformer with selection.
2.6.0,Check ids are unchanged.
2.6.0,Check y is unchanged since this is a X transformer
2.6.0,Check w is unchanged since this is a y transformer
2.6.0,Check y is now a logarithmic version of itself
2.6.0,Check that untransform does the right thing.
2.6.0,Check ids are unchanged.
2.6.0,Check y is unchanged since this is an X transformer
2.6.0,Check w is unchanged since this is an X transformer
2.6.0,Check X is now holding the proper values when sorted.
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is an y transformer
2.6.0,Check w is unchanged since this is an y transformer
2.6.0,Check y is now holding the proper values when sorted.
2.6.0,Check that untransform does the right thing.
2.6.0,Check ids are unchanged.
2.6.0,Check y is unchanged since this is an X transformer
2.6.0,Check w is unchanged since this is an X transformer
2.6.0,Check X is now holding the proper values when sorted.
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a y transformer
2.6.0,Check w is unchanged since this is a y transformer
2.6.0,Check y is now holding the proper values when sorted.
2.6.0,Check ids are unchanged before and after transformation
2.6.0,Check X is unchanged since transform_y is true
2.6.0,Check w is unchanged since transform_y is true
2.6.0,Check minimum and maximum values of transformed y are 0 and 1
2.6.0,Check untransform works correctly
2.6.0,Check ids are unchanged before and after transformation
2.6.0,Check X is unchanged since transform_y is true
2.6.0,Check w is unchanged since transform_y is true
2.6.0,Check minimum and maximum values of transformed y are 0 and 1
2.6.0,Test if dimensionality expansion is handled correctly by untransform
2.6.0,Check ids are unchanged before and after transformation
2.6.0,Check X is unchanged since transform_y is true
2.6.0,Check w is unchanged since transform_y is true
2.6.0,Check minimum and maximum values of transformed y are 0 and 1
2.6.0,Check untransform works correctly
2.6.0,Load mini log-solubility dataset.
2.6.0,The transformer generates n DAGs for a molecule with n
2.6.0,"atoms. These are denoted the ""parents"""
2.6.0,extract only the images (no need of the labels)
2.6.0,reshaping the vector to image
2.6.0,Check Blurring
2.6.0,Check center crop
2.6.0,Check crop
2.6.0,Check convert2gray
2.6.0,Check rotation
2.6.0,Some more test cases for flip
2.6.0,Check flip
2.6.0,Check Scales
2.6.0,Check shift
2.6.0,check gaussian noise
2.6.0,check salt and pepper noise
2.6.0,Check median filter
2.6.0,transforming y should raise an exception
2.6.0,transforming w should raise an exception
2.6.0,transforming X should be okay
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a y transformer
2.6.0,Check w is unchanged since this is a y transformer
2.6.0,"Check that y_t has zero mean, unit std."
2.6.0,Check that untransform does the right thing.
2.6.0,Check ids are unchanged.
2.6.0,Check y is unchanged since this is a X transformer
2.6.0,Check w is unchanged since this is a y transformer
2.6.0,"Check that X_t has zero mean, unit std."
2.6.0,np.set_printoptions(threshold='nan')
2.6.0,Entries with zero std are not normalized
2.6.0,Check that untransform does the right thing.
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a w transformer
2.6.0,Check y is unchanged since this is a w transformer
2.6.0,Assert that entries with zero weight retain zero weight
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a w transformer
2.6.0,Check y is unchanged since this is a w transformer
2.6.0,Assert that entries with zero weight retain zero weight
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a w transformer
2.6.0,Check y is unchanged since this is a w transformer
2.6.0,Assert that entries with zero weight retain zero weight
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a w transformer
2.6.0,Check y is unchanged since this is a w transformer
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is a w transformer
2.6.0,Check y is unchanged since this is a w transformer
2.6.0,Assert that entries with zero weight retain zero weight
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,Check ids are unchanged.
2.6.0,Check y is unchanged since this is an X transformer
2.6.0,Check w is unchanged since this is an X transformer
2.6.0,Check X is now holding the proper values in each column.
2.6.0,Check ids are unchanged.
2.6.0,Check X is unchanged since this is an X transformer
2.6.0,Check w is unchanged since this is an X transformer
2.6.0,Check y is now holding the proper values in each column.
2.6.0,Check that untransform does the right thing.
2.6.0,Check that we have length 8 now with duplication
2.6.0,Check shapes
2.6.0,Check that we have 4 positives and 4 negatives
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,Note that nothing should change in this dataset since weights balance!
2.6.0,Check that still we have length 6
2.6.0,Check shapes
2.6.0,Check that we have 2 positives and 4 negatives
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,Check that we have length 8 now with duplication
2.6.0,Check shapes
2.6.0,Check that we have 4 positives and 4 negatives
2.6.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.6.0,6-1 imbalance in favor of class 0
2.6.0,Check that we have length 30 now with duplication
2.6.0,Check shapes
2.6.0,Check that we have 6 of each class
2.6.0,Check that sum of all class weights is equal by comparing to 0 weight
2.6.0,Note class imbalance. This will round to 2x duplication for 1
2.6.0,Check that we have length 13 now with duplication
2.6.0,Check shapes
2.6.0,Check that we have 6 positives and 7 negatives
2.6.0,################################################################
2.6.0,save.py is out of date. You should not import any functions from here.
2.6.0,################################################################
2.6.0,flake8: noqa
2.6.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.6.0,Walk through the original file and extract ATOM/HETATM lines and
2.6.0,add PDBQT charge annotations.
2.6.0,Remove rotatable bonds from this molecule
2.6.0,Get the connected components now that the rotatable bonds have
2.6.0,been removed.
2.6.0,The root is the largest connected component.
2.6.0,Write the root component
2.6.0,"We've looked at the root, so take note of that"
2.6.0,Compute partial charges on molecule if RDKit Mol
2.6.0,indices to atoms to keep
2.6.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.6.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.6.0,nonzero contact.
2.6.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.6.0,TODO: This is duplicated! Clean up
2.6.0,Updates charges in place
2.6.0,initial embedding
2.6.0,minimization and pruning
2.6.0,always keep lowest-energy conformer
2.6.0,discard conformers after max_conformers is reached
2.6.0,get RMSD to selected conformers
2.6.0,discard conformers within the RMSD threshold
2.6.0,create a new molecule to hold the chosen conformers
2.6.0,this ensures proper conformer IDs and energy-based ordering
2.6.0,False here specifies that water is to be removed
2.6.0,Updates charges in place
2.6.0,TODO: This is wrong. Should return all molecules
2.6.0,TODO: Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.6.0,This updates in place
2.6.0,indices of atoms to keep
2.6.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.6.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.6.0,nonzero contact.
2.6.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.6.0,####################################################
2.6.0,Compute partial charges on molecule if rdkit
2.6.0,####################################################
2.6.0,Number of voxels per one edge of box to voxelize.
2.6.0,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.6.0,If interval1 < interval2 entirely
2.6.0,If interval2 < interval1 entirely
2.6.0,Each triangle in the simplices is a set of 3 atoms from
2.6.0,coordinates which forms the vertices of an exterior triangle on
2.6.0,the convex hull of the macromolecule.
2.6.0,Points is the set of atom coordinates that make up this
2.6.0,triangular face on the convex hull
2.6.0,Let's extract x/y/z coords for this face
2.6.0,Let's compute min/max points
2.6.0,"Nitrogen has atomic number 7, and oxygen 8."
2.6.0,If atom is a hydrogen
2.6.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.6.0,If atom is a hydrogen
2.6.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.6.0,if ring from mol1 is aromatic
2.6.0,...and atom from mol2 is a cation
2.6.0,if angle and distance are correct
2.6.0,count atoms forming a contact
2.6.0,if ring is aromatic
2.6.0,"save its indices, center, and normal"
2.6.0,remember mol1-mol2 pairs we already counted
2.6.0,"if this pair is new, count atoms forming a contact"
2.6.0,"if this pair is new, count atoms forming a contact"
2.6.0,find interacting rings from mol1 and cations from mol2
2.6.0,find interacting cations from mol1 and rings from mol2
2.6.0,merge counters
2.6.0,the line has format
2.6.0,REMARK VINA RESULT: score ...
2.6.0,There is only 1 such line per model so we can append it
2.6.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.6.0,Apply common fixes to PDB files
2.6.0,Optimize ligand
2.6.0,Make sure input is a list
2.6.0,FIXME: Incompatible types in assignment
2.6.0,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.6.0,Ensure that metric is wrapped in a list.
2.6.0,This case checks if input is a function then wraps a
2.6.0,dc.metrics.Metric object around it
2.6.0,Process input metrics
2.6.0,Compute multitask metrics
2.6.0,We use y/w to aggregate labels/weights across generator.
2.6.0,This is a KerasModel.
2.6.0,Some datasets have weights
2.6.0,Process predictions and populate y/w lists
2.6.0,Combine labels/weights
2.6.0,Undo data transformations.
2.6.0,Compute multitask metrics
2.6.0,These functions have moved to deepchem.utils_docking_utils
2.6.0,flake8: noqa
2.6.0,The number of elements to print for dataset ids/tasks
2.6.0,"If a dataset contains more than this number of elements, it won't"
2.6.0,print any dataset ids
2.6.0,An activation function for a layer: either a function or the name of a standard activation
2.6.0,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.6.0,"A single value of some type, or multiple values of that type"
2.6.0,The shape of a NumPy array
2.6.0,"A NumPy array, or an object that can be converted to one.  Once we move to"
2.6.0,"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
2.6.0,type of RDKit object
2.6.0,type of Pymatgen object
2.6.0,Generate a random temporary file name
2.6.0,Ensure the file is created
2.6.0,Open the file in the given mode
2.6.0,Tasks are either in .sdf.csv file or in the .sdf file itself
2.6.0,Structures are stored in .sdf file
2.6.0,Reset aggregator
2.6.0,Handle final leftovers for this file
2.6.0,First line of user-specified CSV *must* be header.
2.6.0,"If gzipped, need to compute extension again"
2.6.0,First line of user-specified CSV *must* be header.
2.6.0,The label encoder is given characters for ACGTN
2.6.0,Peak at the first sequence to get the length of the sequence.
2.6.0,init an one-hot vector
2.6.0,"If include_unknown_set is True, set the last index is 1."
2.6.0,################################################################
2.6.0,atom (node) featurization
2.6.0,################################################################
2.6.0,################################################################
2.6.0,bond (edge) featurization
2.6.0,################################################################
2.6.0,One sequence has length longer than others. This should throw a
2.6.0,ValueError.
2.6.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.6.0,Loosening atol to see if tests stop failing sporadically
2.6.0,string set
2.6.0,integer set
2.6.0,include_unknown_set is False
2.6.0,include_unknown_set is True
2.6.0,check unknown atoms
2.6.0,check original set
2.6.0,"Generally, =O behaves as an electron acceptor"
2.6.0,we must compute partial charges before using `get_atom_partial_charge`
2.6.0,The C-N bond is a single bond
2.6.0,TODO test more formats for ligand
2.6.0,TODO test more formats for ligand
2.6.0,adding hydrogens and charges is tested in dc.utils
2.6.0,self.ligand_file is for 3ws9_ligand.sdf
2.6.0,simple flat ring
2.6.0,self.cycle4.Compute2DCoords()
2.6.0,load and sanitize two real molecules
2.6.0,parallel normals
2.6.0,perpendicular normals
2.6.0,too far away
2.6.0,perpendicular normals
2.6.0,parallel normals
2.6.0,too far away
2.6.0,order of the molecules shouldn't matter
2.6.0,with this criteria we should find both types of stacking
2.6.0,parallel normals
2.6.0,perpendicular normals
2.6.0,too far away
2.6.0,def test_compute_cation_pi(self):
2.6.0,"# TODO(rbharath): find better example, currently dicts are empty"
2.6.0,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.6.0,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.6.0,"TODO find better example, currently dicts are empty"
2.6.0,TODO test more formats for ligand
2.6.0,Test on RDKit
2.6.0,3D vector with unit length
2.6.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.6.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.6.0,check if correct distance metric was used
2.6.0,Construct a random class probability matrix
2.6.0,Construct a random class probability matrix
2.6.0,"Note that since no name as provided, metrics are index by order"
2.6.0,given.
2.6.0,"Note that since no name as provided, metrics are index by order"
2.6.0,given.
2.6.0,"Note that since no name as provided, metrics are index by order"
2.6.0,given.
2.6.0,"Note that since no name as provided, metrics are index by order"
2.6.0,given.
2.6.0,TODO: Fix this case with correct thresholding
2.6.0,TODO: Fix this case with correct thresholding
2.6.0,There are 4 faces to the shape created by coords
2.6.0,flake8: noqa
2.6.0,Get the degree id list (which corrects for min_deg)
2.6.0,Get the size of each degree block
2.6.0,Get the the start indices for items in each block
2.6.0,Get the node indices when they are reset when the degree changes
2.6.0,Convert to numpy array
2.6.0,Reorder old atom_features
2.6.0,Reorder old deg lists
2.6.0,Sort membership
2.6.0,Create old to new dictionary. not exactly intuitive
2.6.0,Reorder adjacency lists
2.6.0,Get numpy version of degree list for indexing
2.6.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.6.0,Parse as deg separated
2.6.0,Get indices corresponding to the current degree
2.6.0,Extract and save adjacency list for the current degree
2.6.0,Construct the slice information
2.6.0,Get the cumulative indices after the first index
2.6.0,Set indices with zero sized slices to zero to avoid indexing errors
2.6.0,TODO(rbharath): Can this be removed?
2.6.0,Use random insted of zeros to prevent weird issues with summing to zero
2.6.0,"Combine the features, then sort them by (atom_degree, mol_index)"
2.6.0,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.6.0,Create a map from the original atom indices within each molecule to the
2.6.0,indices in the combined object.
2.6.0,Sort all atoms by degree.
2.6.0,"Get the size of each atom list separated by molecule id, then by degree"
2.6.0,Get the final size of each degree block
2.6.0,"Get the index at which each degree starts, not resetting after each degree"
2.6.0,And not stopping at any specific molecule
2.6.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.6.0,first column telling the start indices of each degree block and the
2.6.0,second colum telling the size of each degree block
2.6.0,Determine the membership (atom i belongs to molecule membership[i])
2.6.0,Initialize the new degree separated adjacency lists
2.6.0,Update the old adjacency lists with the new atom indices and then combine
2.6.0,all together
2.6.0,Iterate through all the molecules
2.6.0,Get the adjacency lists for this molecule and current degree id
2.6.0,"Correct all atom indices to the final indices, and then save the"
2.6.0,results into the new adjacency lists
2.6.0,Increment once row is done
2.6.0,Get the final aggregated molecule
2.6.0,"Requriments - transformers, tokenizers"
2.6.0,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.6.0,The vocab may be expanded in the near future
2.6.0,add vocab_file dict
2.6.0,"unk_token=""[UNK]"","
2.6.0,"sep_token=""[SEP]"","
2.6.0,"pad_token=""[PAD]"","
2.6.0,"cls_token=""[CLS]"","
2.6.0,"mask_token=""[MASK]"","
2.6.0,take into account special tokens in max length
2.6.0,flake8: noqa
2.6.0,Initalize with 1
2.6.0,Replace the hybridization
2.6.0,global possible_hybridization_list
2.6.0,Allow 0 index to correspond to null molecule 1
2.6.0,Correct for null
2.6.0,"print(6-k-1, id)"
2.6.0,Correct for last one
2.6.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.6.0,"Handle edge case of self-pairs (i, i)"
2.6.0,Increment by 1 since we don't want 0-indexing
2.6.0,"This creates a matrix of shape (2, num_pairs)"
2.6.0,Get mapping
2.6.0,first `bt_len` features are bond features(if applicable)
2.6.0,For ring pairs outside max pairs distance continue
2.6.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.6.0,graph distance between two atoms
2.6.0,distance is a matrix of 1-hot encoded distances for all atoms
2.6.0,For ring pairs outside max pairs distance continue
2.6.0,Euclidean distance between atoms
2.6.0,atoms `radial` bonds away from `a1`
2.6.0,atoms less than `radial` bonds away
2.6.0,find atoms `radial`+1 bonds away
2.6.0,create temporary valid ids serving to filter out failed featurizations from every sublist
2.6.0,"of features (i.e. every molecules' frags list), and also totally failed sublists."
2.6.0,This makes output digestable by Loaders
2.6.0,Get the node features
2.6.0,Stack nodes into an array
2.6.0,Get bond lists with reverse edges included
2.6.0,Get canonical adjacency list
2.6.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.6.0,only support datasets providing Cartesian coordinates)
2.6.0,Set dtype
2.6.0,If includes explicit hydrogens
2.6.0,If uses use_chirality
2.6.0,Atom features
2.6.0,Stack nodes into an array
2.6.0,Get bond lists
2.6.0,Get canonical adjacency list
2.6.0,Calculate pair features
2.6.0,the encoding is natively a dictionary with keys 'input_ids' and 'attention_mask'
2.6.0,"SMILES is unique, so set a canonical order of atoms"
2.6.0,Add hydrogens and generate a conformation.
2.6.0,Record properties of the molecules.
2.6.0,Create the output object.
2.6.0,"the encoding is natively a dictionary with keys 'input_ids', 'token_type_ids', and 'attention_mask'"
2.6.0,flake8: noqa
2.6.0,base classes for featurizers
2.6.0,molecule featurizers
2.6.0,complex featurizers
2.6.0,material featurizers
2.6.0,tokenizers
2.6.0,support classes
2.6.0,for str
2.6.0,for list
2.6.0,validation
2.6.0,skip list
2.6.0,skip path string
2.6.0,main logic
2.6.0,Find a successful featurization
2.6.0,Replace failed featurizations with appropriate array
2.6.0,Special case handling of single molecule
2.6.0,Convert iterables to list
2.6.0,"mol must be a RDKit Mol object, so parse a SMILES"
2.6.0,"SMILES is unique, so set a canonical order of atoms"
2.6.0,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.6.0,atom_name is of format RESX-ATOMTYPE
2.6.0,where X is a 1 to 4 digit number
2.6.0,validate params
2.6.0,This assumes that the edge features for self loops are full-zero tensors
2.6.0,In the future we may want to support featurization for self loops
2.6.0,stack features
2.6.0,"before stacking edge_features or node_pos_features,"
2.6.0,we should check whether these are None or not
2.6.0,create new edge index
2.6.0,graph_index indicates which nodes belong to which graph
2.6.0,Setup image
2.6.0,Compute bond properties
2.6.0,Compute atom properties
2.6.0,Setup image
2.6.0,Compute bond properties
2.6.0,Compute atom properties
2.6.0,Reshape done for proper broadcast
2.6.0,"Reshapes, and axes manipulations to facilitate vector processing."
2.6.0,Draw a line between the two atoms.
2.6.0,"The coordinates of this line, are indicated in line_coords"
2.6.0,Turn the line coordinates into image positions
2.6.0,Turn atomic coordinates into image positions
2.6.0,Set the bond line coordinates to the bond property used.
2.6.0,Set the atom positions in image to different atomic properties in channels
2.6.0,With fixed res and img_size some molecules (e.g. long chains) may not fit.
2.6.0,Check whether num_confs >=1 or not
2.6.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.6.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.6.0,consistent with most QM software packages.
2.6.0,generate SMILES for fragments
2.6.0,Extend shorter strings with padding
2.6.0,Padding before and after
2.6.0,Featurize data using featurize() in parent class
2.6.0,Featurize str data
2.6.0,Featurize mol data
2.6.0,load pretrained models
2.6.0,convert errors to zero
2.6.0,flake8: noqa
2.6.0,If partial charges were not computed
2.6.0,construct atom (node) feature
2.6.0,construct edge (bond) index
2.6.0,add edge list considering a directed graph
2.6.0,construct edge (bond) feature
2.6.0,The 1.0 float value represents True Boolean
2.6.0,This will return a boolean vector with all entries False
2.6.0,To get the shortest paths between two nodes.
2.6.0,To get info if two nodes belong to the same ring.
2.6.0,Featurizer
2.6.0,initialize
2.6.0,check initialization
2.6.0,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.6.0,Check whether num_confs >=1 or not
2.6.0,Convert AtomPositions from Angstrom to bohr (atomic units)
2.6.0,"`(1, max_atoms)` -> `(max_atoms,)`"
2.6.0,bond labels
2.6.0,atom labels
2.6.0,create bond encoders and decoders
2.6.0,create atom encoders and decoders
2.6.0,Special case handling of single molecule
2.6.0,Convert iterables to list
2.6.0,Set up site environment matcher
2.6.0,Graphical option
2.6.0,tolerance for grouping nodes
2.6.0,determine minimum distance between sitetypes.
2.6.0,This is used to determine the existence of an edge
2.6.0,Sort by bond
2.6.0,You want to maximize this in order to make sure every node gets an edge
2.6.0,construct graph
2.6.0,matcher options
2.6.0,construct graph
2.6.0,Add nodes
2.6.0,Add edge. distance is edge attribute
2.6.0,construct graph
2.6.0,Gets the isomorphic mapping. Also the most time consuming part of the code
2.6.0,reconstruct graph after alinging point order
2.6.0,RMSD
2.6.0,Construct one hot encoding
2.6.0,get mapping between all site index to active site index
2.6.0,Get Neighbors
2.6.0,Read Data
2.6.0,get map between two environment
2.6.0,align input to the primitive cell (reference)
2.6.0,apply permutations
2.6.0,remove spectators
2.6.0,map it to active sites
2.6.0,Extract the right number of sites by distance
2.6.0,if PBC condition is fulfilled..
2.6.0,Get full N x N SCM
2.6.0,flake8: noqa
2.6.0,load atom_init.json
2.6.0,check whether the atom feature exists or not
2.6.0,construct bi-directed graph
2.6.0,Increase dimension of distance tensor and apply filter
2.6.0,We compute pairwise contact fingerprints
2.6.0,We compute pairwise contact fingerprints
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.0,We compute pairwise contact fingerprints
2.6.0,"rdks = [frag1[1], frag2[1]]"
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.0,We compute pairwise contact fingerprints
2.6.0,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.6.0,"rdks = [frag1[1], frag2[1]]"
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.0,We compute pairwise contact fingerprints
2.6.0,"rdks = [frag1[1], frag2[1]]"
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.6.0,We compute pairwise contact fingerprints
2.6.0,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.6.0,We compute pairwise contact fingerprints
2.6.0,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.6.0,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.6.0,"xyzs = [frag1_xyz, frag2_xyz]"
2.6.0,"rdks = [frag1[1], frag2[1]]"
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.0,We compute pairwise contact fingerprints
2.6.0,"rdks = [frag1[1], frag2[1]]"
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.0,check if user tries to set removed arguments
2.6.0,list of features that require sanitized molecules
2.6.0,not implemented featurization types
2.6.0,default values
2.6.0,update with cutoffs specified by the user
2.6.0,"each entry is a tuple (is_flat, feature_name)"
2.6.0,list of features that cannot be calculated with specified parameters
2.6.0,this list is used to define <flat/voxel/all>_combined subset
2.6.0,parse provided feature types
2.6.0,flake8: noqa
2.6.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.6.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.6.0,nonzero contact.
2.6.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.6.0,We compute pairwise contact fingerprints
2.6.0,Get coordinates
2.6.0,We compute pairwise contact fingerprints
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.6.0,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.6.0,axis.
2.6.0,Type of data created by this featurizer
2.6.0,TODO(rbharath): Should this return a list?
2.6.0,Type of data created by this featurizer
2.6.0,Currently handles loading failures by returning None
2.6.0,TODO: Is there a better handling procedure?
2.6.0,pad outputs
2.6.0,Deprecation warnings for old atomic conv featurizer name #
2.6.0,We compute pairwise contact fingerprints
2.6.0,Get coordinates
2.6.0,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.6.0,We compute pairwise contact fingerprints
2.6.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.6.0,decode the source in the mixed and separated cases
2.6.0,TODO test more formats for ligand
2.6.0,TODO test more formats for ligand
2.6.0,with one conformer
2.6.0,with multiple conformers
2.6.0,include explicit hydrogens
2.6.0,with one conformer
2.6.0,with multiple conformers
2.6.0,include explicit hydrogens
2.6.0,"Requirements - transformers, tokenizers"
2.6.0,"assert ""C1=CC=CN=C1"""
2.6.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.0,"assert ""C1=CC=CN=C1"""
2.6.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.0,"assert ""C1=CC=CN=C1"""
2.6.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.0,"assert ""C1=CC=CN=C1"""
2.6.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.0,"assert ""C1=CC=CN=C1"""
2.6.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.6.0,check for separate count and SMILES entries for each fragment
2.6.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.6.0,max num atoms instead of exact.
2.6.0,Cutoff in angstroms
2.6.0,"Coords are padded, neighbor list and Z are not"
2.6.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.6.0,def test_hydrogen_bond_counter():
2.6.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.6.0,"protein_file = os.path.join(current_dir, 'data',"
2.6.0,'3ws9_protein_fixer_rdkit.pdb')
2.6.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.6.0,
2.6.0,cutoff = 4.5
2.6.0,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.6.0,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.6.0,# TODO: Add shape test
2.6.0,
2.6.0,
2.6.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.6.0,def test_hydrogen_bond_voxelizer():
2.6.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.6.0,"protein_file = os.path.join(current_dir, 'data',"
2.6.0,'3ws9_protein_fixer_rdkit.pdb')
2.6.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.6.0,
2.6.0,cutoff = 4.5
2.6.0,box_width = 16
2.6.0,voxel_width = 1.0
2.6.0,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.6.0,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.6.0,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.6.0,# TODO: Add shape test
2.6.0,@pytest.mark.linux_only
2.6.0,test if default parameters work
2.6.0,check if use-case from examples works
2.6.0,test if input is flattened when flat features are used
2.6.0,test voxel features
2.6.0,test flat features
2.6.0,check if aromatic features are ignored if sanitize=False
2.6.0,test flattened voxel features
2.6.0,test voxel features
2.6.0,test flat features
2.6.0,test rotations
2.6.0,not support array style inputs
2.6.0,check convert function
2.6.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.6.0,of degree 1 (connected only to central nitrogen).
2.6.0,5 atoms in compound
2.6.0,Get the adjacency lists grouped by degree
2.6.0,The 4 outer atoms connected to central nitrogen
2.6.0,Central nitrogen connected to everything else.
2.6.0,Only one carbon
2.6.0,"No bonds, so degree adjacency lists are empty"
2.6.0,3 carbonds in alkane
2.6.0,Outer two carbonds are connected to central carbon
2.6.0,Central carbon connected to outer two
2.6.0,test featurization
2.6.0,test defeaturization
2.6.0,sanity check; see if something weird does not happen with rdkit
2.6.0,check if original smiles match defeaturized smiles
2.6.0,sanity check; see if something weird does not happen with rdkit
2.6.0,test featurization
2.6.0,test defeaturization
2.6.0,check if original smiles match defeaturized smiles
2.6.0,untransform
2.6.0,untranform
2.6.0,untranform
2.6.0,untranform
2.6.0,untranform
2.6.0,Check the SDF file.
2.6.0,Check the PDB file.
2.6.0,Check the SMILES string.
2.6.0,Do a manual distance computation and make
2.6.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.6.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.6.0,Do a manual distance computation and ensure that selected neighbor is
2.6.0,closest since we set max_num_neighbors = 1
2.6.0,Carbon
2.6.0,Test distance 1
2.6.0,Test distance 2
2.6.0,Test alkane
2.6.0,Test distance 1
2.6.0,3 self connections and 2 bonds which are both counted twice because of
2.6.0,symmetry for 7 total
2.6.0,Test distance 2
2.6.0,Everything is connected at this distance
2.6.0,Test alkane
2.6.0,Test distance infinity
2.6.0,Everything is connected at this distance
2.6.0,Test pentane
2.6.0,Test distance infinity
2.6.0,Everything is connected at this distance
2.6.0,Only one carbon
2.6.0,Test feature sizes
2.6.0,"No bonds, so only 1 pair feature (for the self interaction)"
2.6.0,Only 4 atoms
2.6.0,Test feature sizes for chirality
2.6.0,3 carbonds in alkane
2.6.0,Test feature sizes
2.6.0,Should be a 3x3 interaction grid
2.6.0,mol_list = featurizer.featurize(mols)
2.6.0,mol = mol_list[0]
2.6.0,3 carbonds in alkane
2.6.0,Test feature sizes
2.6.0,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.6.0,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.6.0,symmetry)
2.6.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.6.0,of degree 1 (connected only to central nitrogen).
2.6.0,import rdkit.Chem
2.6.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.6.0,5 atoms in compound
2.6.0,Test feature sizes
2.6.0,Should be a 3x3 interaction grid
2.6.0,Artificial feature array.
2.6.0,0 atoms of degree 0
2.6.0,0 atoms of degree 1
2.6.0,4 atoms of degree 2
2.6.0,0 atoms of degree 3
2.6.0,0 atoms of degree 4
2.6.0,0 atoms of degree 5
2.6.0,0 atoms of degree 6
2.6.0,0 atoms of degree 7
2.6.0,0 atoms of degree 8
2.6.0,0 atoms of degree 9
2.6.0,0 atoms of degree 10
2.6.0,atom 4 has 0 neighbors
2.6.0,atom 0 has 2 neighbors
2.6.0,atom 1 has 2 neighbors
2.6.0,atom 2 has 2 neighbors
2.6.0,atom 3 has 3 neighbors.
2.6.0,Verify that atom features have been sorted by atom degree.
2.6.0,Sorting is done by atom degree as before. So the ordering goes
2.6.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.6.0,from new position to old position is
2.6.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.6.0,list respects this reordering and returns correct adjacency list.
2.6.0,First example molecule
2.6.0,Artificial feature array.
2.6.0,Second example molecule
2.6.0,Third example molecule
2.6.0,Test agglomerate molecule method
2.6.0,No atoms of degree 0
2.6.0,3 atoms of degree 1
2.6.0,8 atoms of degree 2
2.6.0,1 atom of degree 3
2.6.0,0 atoms of degree 4
2.6.0,0 atoms of degree 5
2.6.0,Check that atoms are only connected to themselves.
2.6.0,Check that there's one atom of each degree.
2.6.0,calculate coordinates
2.6.0,not zero values
2.6.0,assumes that every array is of the same dimension
2.6.0,rem_dataset is remaining portion of dataset
2.6.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.6.0,to k-1.
2.6.0,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.6.0,validation
2.6.0,skip list
2.6.0,skip path string
2.6.0,main logic
2.6.0,for str
2.6.0,for list
2.6.0,dict is needed in case groups aren't strictly flattened or
2.6.0,hashed by something non-integer like
2.6.0,Figure out how many positive samples we want for each task in each dataset.
2.6.0,Assign the positive samples to datasets.  Since a sample may be positive
2.6.0,"on more than one task, we need to keep track of the effect of each added"
2.6.0,"sample on each task.  To try to keep everything balanced, we cycle through"
2.6.0,"tasks, assigning one positive sample for each one."
2.6.0,We have a sample that hasn't been assigned yet.  Assign it to
2.6.0,whichever set currently has the lowest fraction of its target for
2.6.0,this task.
2.6.0,The remaining samples are negative for all tasks.  Add them to fill out
2.6.0,each set to the correct total number.
2.6.0,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.6.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.6.0,This can be generalized in the future with some common demoninator determination.
2.6.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.6.0,Append remaining examples to train
2.6.0,################################################################
2.6.0,Splitter for molecule datasets
2.6.0,################################################################
2.6.0,Sort by increasing MW
2.6.0,calcaulate scaffold sets
2.6.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.6.0,Compute fingerprints for all molecules.
2.6.0,Split into two groups: training set and everything else.
2.6.0,Split the second group into validation and test sets.
2.6.0,Begin by assigning the first molecule to the first group.
2.6.0,Decide which group to assign a molecule to.
2.6.0,Identify the unassigned molecule that is least similar to everything in
2.6.0,the other group.
2.6.0,Add it to the group.
2.6.0,Update the data on unassigned molecules.
2.6.0,Sort from largest to smallest scaffold sets
2.6.0,################################################################
2.6.0,Not well supported splitters
2.6.0,################################################################
2.6.0,All datasets share features and identifiers by assumption.
2.6.0,flake8: noqa
2.6.0,basic splitter
2.6.0,molecule splitter
2.6.0,other splitter
2.6.0,################################################################
2.6.0,Removed API
2.6.0,################################################################
2.6.0,Note that the extra task goes to test
2.6.0,Number tasks per fold
2.6.0,Find the tasks that correspond to this test fold
2.6.0,Assert that all arrays look like they should
2.6.0,"task_type = ""regression"""
2.6.0,0 1 2 3 4 5 6 7 8 9
2.6.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.6.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.6.0,reshard() to handle this?
2.6.0,Verify lengths is 10/k == 2
2.6.0,Verify that compounds in this fold are subset of original compounds
2.6.0,Verify that no two folds have overlapping compounds.
2.6.0,Verify lengths is 10/k == 2
2.6.0,Verify that compounds in this fold are subset of original compounds
2.6.0,Verify that no two folds have overlapping compounds.
2.6.0,Verify lengths is 10/k == 2
2.6.0,Verify that compounds in this fold are subset of original compounds
2.6.0,Verify that no two folds have overlapping compounds.
2.6.0,Test singletask case.
2.6.0,The split index should partition dataset in half.
2.6.0,Test singletask case.
2.6.0,Test case where some weights are zero (i.e. masked)
2.6.0,Set half the positives to have zero weight
2.6.0,There are 10 nonzero actives.
2.6.0,"The split index should partition this into half, so expect 5"
2.6.0,The split index should partition the positives for each task roughly in half.
2.6.0,Mask half the examples
2.6.0,The split index should partition dataset in half.
2.6.0,Test singletask case.
2.6.0,Should have split cleanly in half (picked random seed to ensure this)
2.6.0,Check positives are correctly distributed
2.6.0,Test singletask case.
2.6.0,Should have made an 80/10/10 train/valid/test split of actives.
2.6.0,Verify lengths is 100/k == 20
2.6.0,Note: This wouldn't work for multitask str
2.6.0,assert len(fold_dataset) == n_samples/K
2.6.0,Verify that each fold has n_positives/K = 4 positive examples.
2.6.0,Verify that compounds in this fold are subset of original compounds
2.6.0,Verify that no two folds have overlapping compounds.
2.6.0,The amount of datapoints has to be the same
2.6.0,The number of scaffolds generated by the splitter
2.6.0,has to be smaller or equal than number of total molecules
2.6.0,Add the input features.
2.6.0,Add the convolutional layers
2.6.0,edges logits used during training
2.6.0,nodes logits used during training
2.6.0,edges logits
2.6.0,nodes logits
2.6.0,training of the model
2.6.0,generating compounds
2.6.0,nodes logits used during compound generation
2.6.0,Create the inputs.
2.6.0,Create the generators.
2.6.0,Create the discriminators.
2.6.0,Compute the loss functions.
2.6.0,Create learnable weights for the generators and discriminators.
2.6.0,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.6.0,Compute the weighted errors
2.6.0,Add an entropy term to the loss.
2.6.0,Create the Keras model.
2.6.0,"Every call to fit_generator() will increment global_step, but we only"
2.6.0,"want it to get incremented once for the entire batch, so record the"
2.6.0,value and keep resetting it.
2.6.0,Train the discriminator.
2.6.0,Train the generator.
2.6.0,Write checkpoints and report progress.
2.6.0,Write out final results.
2.6.0,Chain of flows is also a normalizing flow
2.6.0,An instance of tfd.TransformedDistribution
2.6.0,TODO: Incompability between TF and TFP means that TF doesn't track
2.6.0,trainable variables in the flow; must override `_create_gradient_fn`
2.6.0,self._variables = self.flow.trainable_variables
2.6.0,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.6.0,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.6.0,This is for API consistency
2.6.0,extended one of probabilites to binary distribution
2.6.0,extended one of probabilites to binary distribution
2.6.0,-*- coding: utf-8 -*-
2.6.0,"Shape (N_atoms, M_nbrs, ndim)"
2.6.0,"Shape (N_atoms, M_nbrs, ndim)"
2.6.0,"Shape (N_atoms, M_nbrs)"
2.6.0,Generate the nb_affine weights and biases
2.6.0,Extract atom_features
2.6.0,Extract graph topology
2.6.0,Sum all neighbors using adjacency matrix
2.6.0,Get collection of modified atom features
2.6.0,Obtain relevant atoms for this degree
2.6.0,Get self atoms
2.6.0,Apply hidden affine to relevant atoms and append
2.6.0,Determine the min_deg=0 case
2.6.0,Only use the self layer
2.6.0,Combine all atoms back into the list
2.6.0,Tensorflow correctly processes empty lists when using concat
2.6.0,"Sum along neighbors as well as self, and store"
2.6.0,Perform the mol gather
2.6.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.6.0,"self.max_degree, self.min_degree)"
2.6.0,Tensorflow correctly processes empty lists when using concat
2.6.0,Get self atoms
2.6.0,"There are no neighbors of this degree, so just create an empty tensor directly."
2.6.0,Expand dims
2.6.0,always deg-1 for deg_adj_lists
2.6.0,Extract graph topology
2.6.0,means that this is second loop of convolution
2.6.0,No other forget biases supported right now.
2.6.0,Taken from Keras code [citation needed]
2.6.0,"x is test set, xp is support set."
2.6.0,Get initializations
2.6.0,Process using attention
2.6.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.6.0,Generate new attention states
2.6.0,Support set lstm
2.6.0,Test lstm
2.6.0,Get initializations
2.6.0,Rename support
2.6.0,Process support xp using attention
2.6.0,Get linear combination of support set
2.6.0,Process test x using attention
2.6.0,Generate new support attention states
2.6.0,Generate new test attention states
2.6.0,Redefine
2.6.0,Number of rotatable bonds
2.6.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.6.0,a difference.
2.6.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.6.0,neighbors lists an argument instead of a part of this layer.
2.6.0,"Shape (N, M)"
2.6.0,"Shape (N, M)"
2.6.0,"Shape (N, M)"
2.6.0,Number of grid cells
2.6.0,TODO(rbharath): Support batching
2.6.0,"Shape (n_cells, ndim)"
2.6.0,"List of length N_atoms, each element of different length uniques_i"
2.6.0,"List of length N_atoms, each element of different length uniques_i"
2.6.0,"List of length N_atoms, each a tensor of shape"
2.6.0,"(uniques_i, ndim)"
2.6.0,Add phantom atoms that exist far outside the box
2.6.0,"List of length N_atoms, each of shape (1, ndim)"
2.6.0,TODO(rbharath): How does distance need to be modified here to
2.6.0,account for periodic boundary conditions?
2.6.0,List of length N_atoms each of shape (M_nbrs)
2.6.0,"N_atoms elts of size (M_nbrs,) each"
2.6.0,"Shape (N_atoms, 1)"
2.6.0,Find M_nbrs atoms closest to each cell
2.6.0,"Shape (n_cells, M_nbrs)"
2.6.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.6.0,"conditions, so does wrapround. O(constant)"
2.6.0,"Shape (n_cells, n_nbr_cells)"
2.6.0,"Shape (N_atoms, n_nbr_cells)"
2.6.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.6.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.6.0,"List of length N_atoms, each element length uniques_i"
2.6.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.6.0,element removed to remove self from list of neighbors. Need to verify
2.6.0,this holds more broadly or come up with robust alternative.
2.6.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.6.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.6.0,Shape (N_atoms*n_cells)
2.6.0,"Shape (n_cells, N_atoms)"
2.6.0,Find k atoms closest to this cell. Notice negative sign since
2.6.0,tf.nn.top_k returns *largest* not smallest.
2.6.0,"Tensor of shape (n_cells, M_nbrs)"
2.6.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.6.0,"Shape (N_atoms*n_cells, 1) after tile"
2.6.0,9 neighbors in 2-space
2.6.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.6.0,Number of cells for cube in 3-space is
2.6.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.6.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.6.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.6.0,the cube.
2.6.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.6.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.6.0,"Tile (a, a, a, b, b, b, etc.)"
2.6.0,"Tile (a, b, c, a, b, c, ...)"
2.6.0,N: Maximum number of atoms
2.6.0,M: Maximum number of neighbors
2.6.0,d: Number of coordinates/features/filters
2.6.0,B: Batch Size
2.6.0,Compute the distances and radial symmetry functions.
2.6.0,check that there isnt just one or zero inputs
2.6.0,create subspaces
2.6.0,"concatenate subspaces, reshape to size of original input, then stack"
2.6.0,"such that out_tensor has shape (2,?,original_cols)"
2.6.0,creates subspaces the same way it was done in AlphaShare
2.6.0,calculate squared Frobenius norm
2.6.0,"(TODO YTZ:) faster, less memory intensive way"
2.6.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.6.0,"r = tf.expand_dims(r, -1)"
2.6.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.6.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.6.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.6.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.6.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.6.0,Calculate pairwise distance
2.6.0,Cutoff with threshold Rc
2.6.0,return d
2.6.0,tf.stack issues again...
2.6.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.6.0,So the Tensor has known dimensions
2.6.0,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.6.0,normalization
2.6.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.6.0,and embeddings of atom j(both gone through a hidden layer)
2.6.0,"for atom i, sum the influence from all other atom j in the molecule"
2.6.0,number of inputs each step
2.6.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.6.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.6.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.6.0,`count`-th step
2.6.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.6.0,generating index for graph features used in the inputs
2.6.0,"extracting graph features for parents of the target atoms, then flatten"
2.6.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.6.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.6.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.6.0,of shape: (batch_size*max_atoms) * n_graph_features
2.6.0,representing the graph features of target atoms in each graph
2.6.0,index for targe atoms
2.6.0,Extract atom_features
2.6.0,sum all graph outputs
2.6.0,"Default message function: edge network, update function: GRU"
2.6.0,more options to be implemented
2.6.0,Add another value(~-Inf) to prevent error in softmax
2.6.0,Model using this layer must set pad_batches=True
2.6.0,Perform one step of LSTM
2.6.0,task_metadata_rows = {task: [] for task in tasks}
2.6.0,Extract those datapoints which are present for this task
2.6.0,Loading is done on-the-fly
2.6.0,Build the model.
2.6.0,Final atom-layer convolution. Note this differs slightly from the paper
2.6.0,since we use a tanh activation as default. This seems necessary for numerical
2.6.0,stability.
2.6.0,Now fully connected layers
2.6.0,Should this allow for training?
2.6.0,"pair_edges is of shape (2, N)"
2.6.0,number of atoms in each molecule
2.6.0,index of pair features
2.6.0,Get starting pair atoms
2.6.0,number of pairs for each atom
2.6.0,atom features
2.6.0,pair features
2.6.0,Build the model.
2.6.0,Build the model.
2.6.0,calculation orders for a batch of molecules
2.6.0,padding atom features vector of each molecule with 0
2.6.0,Build the model.
2.6.0,number of atoms in each molecule
2.6.0,index of pair features
2.6.0,number of pairs for each atom
2.6.0,atom features
2.6.0,pair features
2.6.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.6.0,Add the input features.
2.6.0,Add the shared dense layers
2.6.0,Add task-specific bypass layers
2.6.0,Add the input features.
2.6.0,Add the shared dense layers
2.6.0,Add task-specific bypass layers
2.6.0,W&B flag support (DEPRECATED)
2.6.0,"If `wandb=True` and no logger is provided, initialize default logger"
2.6.0,Setup and initialize W&B logging
2.6.0,Update config with KerasModel params
2.6.0,Backwards compatibility
2.6.0,The optimizer creates internal variables the first time apply_gradients()
2.6.0,is called for a new set of variables.  If that happens inside a function
2.6.0,"annotated with tf.function it throws an exception, so call it once here."
2.6.0,Main training loop.
2.6.0,"Execute the loss function, accumulating the gradients."
2.6.0,Report progress and write checkpoints.
2.6.0,Capture the last avg_loss in case of return since we're resetting to
2.6.0,0 now
2.6.0,Report final results.
2.6.0,Invoke the model.
2.6.0,Apply tranformers and record results.
2.6.0,Concatenate arrays to create the final results.
2.6.0,Use a GradientTape to compute gradients.
2.6.0,Ensure weights for both models are built.
2.6.0,Define the PyTorch Module that implements the model.
2.6.0,Define the PyTorch Module that implements the model.
2.6.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.6.0,set wandb init arguments
2.6.0,Dataset ids are used to differentiate datasets seen by the logger
2.6.0,log data
2.6.0,Similarity values
2.6.0,Labels for all top K similar samples
2.6.0,Discard any padded predictions
2.6.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.6.0,Build the model.
2.6.0,Character embedding
2.6.0,Multiple convolutional layers with different filter widths
2.6.0,Max-over-time pooling
2.6.0,Concat features from all filters(one feature per filter)
2.6.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.6.0,SMILES strings
2.6.0,Maximum length is expanded to allow length variation during train and inference
2.6.0,'_' served as delimiter and padding
2.6.0,Initialize common characters as keys
2.6.0,Include space to avoid extra keys
2.6.0,"For 'Cl', 'Br', etc."
2.6.0,"Character not recognized, add to extra_keys"
2.6.0,Add all extra_keys to char_dict
2.6.0,Transform SMILES sequence to integers
2.6.0,Skip all spaces
2.6.0,"For 'Cl', 'Br', etc."
2.6.0,Padding with '_'
2.6.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.6.0,"layer_sizes=[32, 32, 16],"
2.6.0,Add the dense layers
2.6.0,Do a simple greedy search.
2.6.0,Do a beam search with length normalization.
2.6.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.6.0,This candidate sequence has already been terminated
2.6.0,Consider all possible tokens we could add to this candidate sequence.
2.6.0,Add the input features.
2.6.0,Handle output layer
2.6.0,Iterate over all previous tasks.
2.6.0,prev_layers is a list with elements of size
2.6.0,"(batch_size, layer_sizes[i-1])"
2.6.0,Log data to Wandb
2.6.0,flake8: noqa
2.6.0,Tensorflow Depedency Models
2.6.0,scikit-learn model
2.6.0,PyTorch models
2.6.0,Jax models
2.6.0,####################################################################################
2.6.0,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.6.0,####################################################################################
2.6.0,#######################################################################################
2.6.0,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.6.0,#######################################################################################
2.6.0,Last layer sequences not returned.
2.6.0,This is needed because ImageDataGenerator does infinite looping
2.6.0,"this is equivalent to einsum('...c,cd->...d', inputs, weights)"
2.6.0,but turns out to be slightly faster
2.6.0,JAX depend
2.6.0,Main training loop
2.6.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.6.0,Report final results.
2.6.0,Apply tranformers and record results.
2.6.0,Concatenate arrays to create the final results.
2.6.0,"def predict_uncertainty(self, dataset: Dataset, masks: int = 50"
2.6.0,") -> OneOrMany[Tuple[np.ndarray, np.ndarray]]:"
2.6.0,""""""""
2.6.0,"Predict the model's outputs, along with the uncertainty in each one."
2.6.0,The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.
2.6.0,It involves repeating the prediction many times with different dropout masks.
2.6.0,The prediction is computed as the average over all the predictions.  The
2.6.0,uncertainty includes both the variation among the predicted values (epistemic
2.6.0,uncertainty) and the model's own estimates for how well it fits the data
2.6.0,(aleatoric uncertainty).  Not all models support uncertainty prediction.
2.6.0,Parameters
2.6.0,----------
2.6.0,dataset: dc.data.Dataset
2.6.0,Dataset to make prediction on
2.6.0,masks: int
2.6.0,the number of dropout masks to average over
2.6.0,Returns
2.6.0,-------
2.6.0,"for each output, a tuple (y_pred, y_std) where y_pred is the predicted"
2.6.0,"value of the output, and each element of y_std estimates the standard"
2.6.0,deviation of the corresponding element of y_pred
2.6.0,""""""""
2.6.0,sum_pred: List[np.ndarray] = []
2.6.0,sum_sq_pred: List[np.ndarray] = []
2.6.0,sum_var: List[np.ndarray] = []
2.6.0,for i in range(masks):
2.6.0,generator = self.default_generator(
2.6.0,"dataset, mode='uncertainty', pad_batches=False)"
2.6.0,"results = self._predict(generator, [], True, None)"
2.6.0,if len(sum_pred) == 0:
2.6.0,"for p, v in results:"
2.6.0,sum_pred.append(p)
2.6.0,sum_sq_pred.append(p * p)
2.6.0,sum_var.append(v)
2.6.0,else:
2.6.0,"for j, (p, v) in enumerate(results):"
2.6.0,sum_pred[j] += p
2.6.0,sum_sq_pred[j] += p * p
2.6.0,sum_var[j] += v
2.6.0,output = []
2.6.0,std = []
2.6.0,for i in range(len(sum_pred)):
2.6.0,p = sum_pred[i] / masks
2.6.0,output.append(p)
2.6.0,std.append(np.sqrt(sum_sq_pred[i] / masks - p * p + sum_var[i] / masks))
2.6.0,if len(output) == 1:
2.6.0,"return (output[0], std[0])"
2.6.0,else:
2.6.0,"return list(zip(output, std))"
2.6.0,JAX dependencies
2.6.0,Main training loop
2.6.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.6.0,Report final results.
2.6.0,Apply tranformers and record results.
2.6.0,Concatenate arrays to create the final results.
2.6.0,flake8:noqa
2.6.0,The PINNModel requires you to create two functions
2.6.0,`create_eval`_fn for letting the model know how to compute the model in inference and
2.6.0,`gradient_fn` for letting model know how to compute the gradient and different regulariser
2.6.0,equation loss depending on the differential equation
2.6.0,defining the Haiku model
2.6.0,"giving an initial boundary condition at 5 points between [-pi, pi] which will be used in l2 loss"
2.6.0,"defining our training data. We feed 100 points between [-pi, pi] without the labels,"
2.6.0,which will be used as the differential loss(regulariser)
2.6.0,The expected solution must be as close to cos(x)
2.6.0,Initialize the weights with random values
2.6.0,Forward function which takes the params
2.6.0,Loss Function
2.6.0,JaxModel Working
2.6.0,sample network
2.6.0,Model Initialization
2.6.0,Loss Function
2.6.0,JaxModel Working
2.6.0,sample network
2.6.0,Model Initilisation
2.6.0,Loss Function
2.6.0,JaxModel Working
2.6.0,Model Initilisation
2.6.0,Loss Function
2.6.0,JaxModel Working
2.6.0,Model Initilisation
2.6.0,Loss Function
2.6.0,JaxModel Working
2.6.0,Model Initilisation
2.6.0,Loss Function
2.6.0,JaxModel Working
2.6.0,Each epoch is a single step for this model
2.6.0,@pytest.mark.jax
2.6.0,@pytest.mark.slow
2.6.0,def test_uncertainty():
2.6.0,"""""""Test estimating uncertainty a TorchModel."""""""
2.6.0,n_samples = 30
2.6.0,n_features = 1
2.6.0,noise = 0.1
2.6.0,"X = np.random.rand(n_samples, n_features)"
2.6.0,"y = (10 * X + np.random.normal(scale=noise, size=(n_samples, n_features)))"
2.6.0,"dataset = dc.data.NumpyDataset(X, y)"
2.6.0,class Net(hk.Module):
2.6.0,"def __init__(self, output_size: int = 1):"
2.6.0,super().__init__()
2.6.0,"self._network1 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.6.0,"self._network2 = hk.Sequential([hk.Linear(200), jax.nn.relu])"
2.6.0,self.output = hk.Linear(output_size)
2.6.0,self.log_var = hk.Linear(output_size)
2.6.0,"def __call__(self, x):"
2.6.0,x = self._network1(x)
2.6.0,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.6.0,x = self._network2(x)
2.6.0,"x = hk.dropout(hk.next_rng_key(), 0.1, x)"
2.6.0,output = self.output(x)
2.6.0,log_var = self.log_var(x)
2.6.0,var = jnp.exp(log_var)
2.6.0,"return output, var, output, log_var"
2.6.0,def f(x):
2.6.0,net = Net(1)
2.6.0,return net(x)
2.6.0,"def loss(outputs, labels, weights):"
2.6.0,diff = labels[0] - outputs[0]
2.6.0,log_var = outputs[1]
2.6.0,var = jnp.exp(log_var)
2.6.0,return jnp.mean(diff * diff / var + log_var)
2.6.0,class UncertaintyModel(JaxModel):
2.6.0,"def default_generator(self,"
2.6.0,"dataset,"
2.6.0,"epochs=1,"
2.6.0,"mode='fit',"
2.6.0,"deterministic=True,"
2.6.0,pad_batches=True):
2.6.0,for epoch in range(epochs):
2.6.0,"for (X_b, y_b, w_b, ids_b) in dataset.iterbatches("
2.6.0,"batch_size=self.batch_size,"
2.6.0,"deterministic=deterministic,"
2.6.0,pad_batches=pad_batches):
2.6.0,"yield ([X_b], [y_b], [w_b])"
2.6.0,jm_model = hk.transform(f)
2.6.0,rng = jax.random.PRNGKey(500)
2.6.0,"inputs, _, _, _ = next(iter(dataset.iterbatches(batch_size=100)))"
2.6.0,modified_inputs = jnp.array(
2.6.0,[x.astype(np.float32) if x.dtype == np.float64 else x for x in inputs])
2.6.0,"params = jm_model.init(rng, modified_inputs)"
2.6.0,model = UncertaintyModel(
2.6.0,"jm_model.apply,"
2.6.0,"params,"
2.6.0,"loss,"
2.6.0,"output_types=['prediction', 'variance', 'loss', 'loss'],"
2.6.0,learning_rate=0.003)
2.6.0,"model.fit(dataset, nb_epochs=2500)"
2.6.0,"pred, std = model.predict_uncertainty(dataset)"
2.6.0,assert np.mean(np.abs(y - pred)) < 2.0
2.6.0,assert noise < np.mean(std) < 1.0
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,test if adjacency matrix input is correctly set
2.6.0,test if nodes features matrix input is correctly set
2.6.0,check discriminator shape
2.6.0,check training edges logits shape
2.6.0,check training nodes logits shapes
2.6.0,True will be assigned up successful training attempt
2.6.0,force clear tensor flow backend
2.6.0,create new model
2.6.0,to avoid flake8 E125/yapf incompatibility
2.6.0,generate input
2.6.0,train model
2.6.0,generate sample
2.6.0,check how many valid molecules were created and add to list
2.6.0,finally test if there was at least one valid training session
2.6.0,as the model structure improves this should become more and more strict
2.6.0,Predict the output and uncertainty.
2.6.0,predict datset with no y (ensured by tasks = [])
2.6.0,Predict the output and uncertainty.
2.6.0,The DAG models have high error with dropout
2.6.0,"Despite a lot of effort tweaking it , there appears to be"
2.6.0,a limit to how low the error can go with dropout.
2.6.0,assert mean_error < 0.5 * mean_value
2.6.0,Predict the output and uncertainty.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,load datasets
2.6.0,disable transformer
2.6.0,check train
2.6.0,check predict shape
2.6.0,check overfit
2.6.0,load datasets
2.6.0,disable transformer
2.6.0,check train
2.6.0,check predict shape
2.6.0,check overfit
2.6.0,load datasets
2.6.0,disable transformer
2.6.0,check train
2.6.0,check predict shape
2.6.0,check overfit
2.6.0,reload
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Check same predictions are made.
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Load trained model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload trained model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Reload trained model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload trained model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reloaded Trained Model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Check predictions match on random sample
2.6.0,3D Multivariate Gaussian base distribution
2.6.0,Check that reloaded model can sample from the distribution
2.6.0,Check that density estimation is same for reloaded model
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload trained model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload Trained Model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload Trained Model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload trained model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Check predictions match on random sample
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload trained model
2.6.0,Eval model on train
2.6.0,Check predictions match on random sample
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload trained model
2.6.0,Eval model on train
2.6.0,Check predictions match on random sample
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Reload trained model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Reload trained Model
2.6.0,Check predictions match on random sample
2.6.0,Eval model on train
2.6.0,Reload Trained Model
2.6.0,Check predictions match on random sample
2.6.0,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.6.0,Reload Trained Model
2.6.0,Check predictions match on original dataset
2.6.0,TODO: We need a cleaner usage example for this
2.6.0,Fit trained model
2.6.0,Check predictions match on random sample
2.6.0,Train the model on random sequences.  We aren't training long enough to
2.6.0,"really make it reliable, but I want to keep this test fast, and it should"
2.6.0,still be able to reproduce a reasonable fraction of input sequences.
2.6.0,Test it out.
2.6.0,check predict shape
2.6.0,check overfit
2.6.0,needs change
2.6.0,check predict shape
2.6.0,check overfit
2.6.0,reload
2.6.0,There are 4 atoms each of which have 75 atom features
2.6.0,There are 10 pairs with infinity distance and 14 pair features
2.6.0,4 atoms in total
2.6.0,10 pairs in total
2.6.0,10 pairs in total each with start/finish
2.6.0,There are 4 atoms each of which have 75 atom features
2.6.0,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.6.0,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.6.0,connections and accounting for symmetry.)
2.6.0,4 atoms in total
2.6.0,10 pairs in total
2.6.0,The center atom is self connected and to both neighbors so it appears
2.6.0,thrice. The canonical ranking used in MolecularFeaturizer means this
2.6.0,central atom is ranked last in ordering.
2.6.0,10 pairs in total each with start/finish
2.6.0,def test_weave_fit_simple_infinity_distance():
2.6.0,featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
2.6.0,"X = featurizer([""C"", ""CCC""])"
2.6.0,"y = np.array([0, 1.])"
2.6.0,"dataset = dc.data.NumpyDataset(X, y)"
2.6.0,batch_size = 20
2.6.0,model = WeaveModel(
2.6.0,"1,"
2.6.0,"batch_size=batch_size,"
2.6.0,"mode='classification',"
2.6.0,"fully_connected_layer_sizes=[2000, 1000],"
2.6.0,"batch_normalize=True,"
2.6.0,batch_normalize_kwargs={
2.6.0,"""fused"": False,"
2.6.0,"""trainable"": True,"
2.6.0,"""renorm"": True"
2.6.0,"},"
2.6.0,learning_rate=0.0005)
2.6.0,"model.fit(dataset, nb_epoch=200)"
2.6.0,transformers = []
2.6.0,metric = dc.metrics.Metric(
2.6.0,"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
2.6.0,"scores = model.evaluate(dataset, [metric], transformers)"
2.6.0,assert scores['mean-roc_auc_score'] >= 0.9
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,load datasets
2.6.0,initialize model
2.6.0,overfit test
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Fit trained model
2.6.0,Predict the output and uncertainty.
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,xgboost test
2.6.0,fit trained model
2.6.0,eval model on test
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,lightgbm test
2.6.0,fit trained model
2.6.0,eval model on test
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,xgboost test
2.6.0,fit trained model
2.6.0,eval model on test
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,lightgbm test
2.6.0,fit trained model
2.6.0,eval model on test
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,xgboost test
2.6.0,fit trained model
2.6.0,eval model on test
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,lightgbm test
2.6.0,fit trained model
2.6.0,eval model on test
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,xgboost test
2.6.0,fit trained model
2.6.0,reload
2.6.0,check predictions match on test dataset
2.6.0,eval model on test
2.6.0,prepare dataset
2.6.0,global setting
2.6.0,lightgbm test
2.6.0,fit trained model
2.6.0,reload
2.6.0,check predictions match on test dataset
2.6.0,eval model on test
2.6.0,"For simplicity, let's assume both molecules have same number of"
2.6.0,atoms.
2.6.0,Creates a set of dummy features that contain the coordinate and
2.6.0,neighbor-list features required by the AtomicConvModel.
2.6.0,Creates a set of dummy features that contain the coordinate and
2.6.0,neighbor-list features required by the AtomicConvModel.
2.6.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.6.0,max num atoms instead of exact.
2.6.0,Cutoff in angstroms
2.6.0,arbitrary label
2.6.0,Run a fitting operation
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Fit trained model
2.6.0,Eval model on train/test
2.6.0,Fit trained model
2.6.0,Eval model on train/test
2.6.0,Fit trained model
2.6.0,Eval model on train/test
2.6.0,See if it has done a plausible job of learning the distribution.
2.6.0,See if it has done a plausible job of learning the distribution.
2.6.0,See if it has done a plausible job of learning the distribution.
2.6.0,No training has been done after reload
2.6.0,See if it has done a plausible job of learning the distribution.
2.6.0,We have to set the gradient penalty very small because the generator's
2.6.0,"output is only a single number, so the default penalty would constrain"
2.6.0,it far too much.
2.6.0,See if it has done a plausible job of learning the distribution.
2.6.0,We have to set the gradient penalty very small because the generator's
2.6.0,"output is only a single number, so the default penalty would constrain"
2.6.0,it far too much.
2.6.0,See if it has done a plausible job of learning the distribution.
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,n_samples = 100
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Most weights should be close to zero.
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Most weights should be close to zero.
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Predict the output and uncertainty.
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Load mini log-solubility dataset.
2.6.0,Fit trained model
2.6.0,Eval model on train
2.6.0,Check that predicting internal layers works.
2.6.0,Each epoch is a single step for this model
2.6.0,Create two models using the same model directory.
2.6.0,Check that they produce different results.
2.6.0,"Save a checkpoint from the first model and load it into the second one,"
2.6.0,and make sure they now match.
2.6.0,Train a model to overfit the dataset.
2.6.0,"Create an identical model, do a single step of fitting with restore=True,"
2.6.0,and make sure it got restored correctly.
2.6.0,Build a model that predicts uncertainty.
2.6.0,Fit the model and see if its predictions are correct.
2.6.0,Take a tiny step in the direction of s and see if the output changes by
2.6.0,the expected amount.
2.6.0,Load dataset and Models
2.6.0,call model.fit again to test multiple fit() calls
2.6.0,def test_singletask_to_multitask_classification(self):
2.6.0,n_features = 10
2.6.0,n_tasks = 17
2.6.0,tasks = range(n_tasks)
2.6.0,# Define train dataset
2.6.0,n_train = 100
2.6.0,"X_train = np.random.rand(n_train, n_features)"
2.6.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.6.0,w_train = np.ones_like(y_train)
2.6.0,"ids_train = [""C""] * n_train"
2.6.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.6.0,"X_train, y_train, w_train, ids_train)"
2.6.0,# Define test dataset
2.6.0,n_test = 10
2.6.0,"X_test = np.random.rand(n_test, n_features)"
2.6.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.6.0,w_test = np.ones_like(y_test)
2.6.0,"ids_test = [""C""] * n_test"
2.6.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.6.0,"X_test, y_test, w_test, ids_test)"
2.6.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.6.0,def model_builder(model_dir):
2.6.0,sklearn_model = LogisticRegression()
2.6.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.6.0,multitask_model = dc.models.SingletaskToMultitask(
2.6.0,"tasks, model_builder)"
2.6.0,# Fit trained model
2.6.0,multitask_model.fit(train_dataset)
2.6.0,multitask_model.save()
2.6.0,# Eval multitask_model on train/test
2.6.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.6.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.6.0,Generate data
2.6.0,Cleanup
2.6.0,Train the model while logging the validation ROC AUC.
2.6.0,Parse the log to pull out the AUC scores.
2.6.0,The last reported score should match the current performance of the model.
2.6.0,Reload the save model and confirm that it matches the best logged score.
2.6.0,3D Multivariate Gaussian base distribution
2.6.0,Must be float32 for RealNVP
2.6.0,Tests a simple flow of one RealNVP layer.
2.6.0,log likelihoods should be negative
2.6.0,# Fit model
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,x and y are the same tensor (equivalent at every element)
2.6.0,the pairwise inner product of the rows in x and y will always be 1
2.6.0,"the output tensor will be of shape (5,5)"
2.6.0,each row in x1 is orthogonal to each row in x2
2.6.0,the pairwise inner product of the rows in x and y will always be 0
2.6.0,"the output tensor will be of shape (256,256)"
2.6.0,"Creating a second layer should produce different results, since it has"
2.6.0,different random weights.
2.6.0,But evaluating the first layer again should produce the same result as before.
2.6.0,index of pair features
2.6.0,number of pairs for each atom
2.6.0,atom features
2.6.0,pair features
2.6.0,"Outputs should be [A, P]"
2.6.0,atom features
2.6.0,Try without compression
2.6.0,"Outputs should be [mol1_vec, mol2_vec)"
2.6.0,Try with compression
2.6.0,"Outputs should be [mol1_vec, mol2_vec)"
2.6.0,atom features
2.6.0,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.6.0,Gaussian histograms expands into 11 Gaussian buckets.
2.6.0,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.6.0,TODO What should shape[1] be?  It's not documented.
2.6.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.6.0,"Creating a second layer should produce different results, since it has"
2.6.0,different random weights.
2.6.0,But evaluating the first layer again should produce the same result as before.
2.6.0,"Creating a second layer should produce different results, since it has"
2.6.0,different random weights.
2.6.0,But evaluating the first layer again should produce the same result as before.
2.6.0,"Creating a second layer should produce different results, since it has"
2.6.0,different random weights.
2.6.0,But evaluating the first layer again should produce the same result as before.
2.6.0,"TODO What should the output shape be?  It's not documented, and there"
2.6.0,are no other test cases for it.
2.6.0,"Creating a second layer should produce different results, since it has"
2.6.0,different random weights.
2.6.0,But evaluating the first layer again should produce the same result as before.
2.6.0,"Creating a second layer should produce different results, since it has"
2.6.0,different random weights.
2.6.0,But evaluating the first layer again should produce the same result as before.
2.6.0,"Recall that the DAG layer expects a MultiConvMol as input,"
2.6.0,"so the ""batch"" is a pooled set of atoms from all the"
2.6.0,"molecules in the batch, just as it is for the graph conv."
2.6.0,This means that n_atoms is the batch-size
2.6.0,dropout_switch = False
2.6.0,dropout_switch
2.6.0,# TODO(rbharath): What is the shape of outputs supposed to be?
2.6.0,"# I'm getting (7, 30) here. Where does 7 come from??"
2.6.0,TODO(rbharath): We need more documentation about why
2.6.0,these numbers work.
2.6.0,Create a dataset and an input function for processing it.
2.6.0,Create a dataset and an input function for processing it.
2.6.0,Generate dummy dataset
2.6.0,Fit trained model
2.6.0,Eval model on test
2.6.0,Eval model on train
2.6.0,Fit trained model
2.6.0,Eval model on test
2.6.0,Fit trained model
2.6.0,Eval model on test
2.6.0,Fit trained model
2.6.0,Eval model on test
2.6.0,Fit trained model
2.6.0,Eval model on test
2.6.0,Each epoch is a single step for this model
2.6.0,Create two models using the same model directory.
2.6.0,Check that they produce different results.
2.6.0,"Save a checkpoint from the first model and load it into the second one,"
2.6.0,and make sure they now match.
2.6.0,Train a model to overfit the dataset.
2.6.0,"Create an identical model, do a single step of fitting with restore=True,"
2.6.0,and make sure it got restored correctly.
2.6.0,Build a model that predicts uncertainty.
2.6.0,Fit the model and see if its predictions are correct.
2.6.0,Take a tiny step in the direction of s and see if the output changes by
2.6.0,the expected amount.
2.6.0,Load dataset and Models
2.6.0,call model.fit again to test multiple fit() calls
2.6.0,Train the model on random sequences.  We aren't training long enough to
2.6.0,"really make it reliable, but I want to keep this test fast, and it should"
2.6.0,still be able to reproduce a reasonable fraction of input sequences.
2.6.0,Test it out.
2.6.0,Check that it got at least a quarter of them correct.
2.6.0,Test it out.
2.6.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.6.0,"few steps of training to make sure nothing crashes, then check that the"
2.6.0,results are at least internally consistent.
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,overfit test
2.6.0,test on a small MoleculeNet dataset
2.6.0,load datasets
2.6.0,initialize models
2.6.0,embedding node features
2.6.0,convolutional layer
2.6.0,pooling
2.6.0,for n_tasks == 1 case
2.6.0,Decide first number of GAT layers
2.6.0,flake8:noqa
2.6.0,Select a device.
2.6.0,W&B logging
2.6.0,"If `wandb=True` and no logger is provided, initialize default logger"
2.6.0,Setup and initialize W&B logging
2.6.0,Update config with KerasModel params
2.6.0,Main training loop.
2.6.0,"Execute the loss function, accumulating the gradients."
2.6.0,Report progress and write checkpoints.
2.6.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.6.0,Report final results.
2.6.0,Invoke the model.
2.6.0,Apply tranformers and record results.
2.6.0,Concatenate arrays to create the final results.
2.6.0,Compute the gradients.
2.6.0,Save the checkpoint to a file.
2.6.0,Rename and delete older files.
2.6.0,Ensure weights for both models are built.
2.6.0,Some scikit-learn models don't use weights.
2.6.0,flake8: ignore
2.6.0,GDBT doesn't support multi-output(task)
2.6.0,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.6.0,retrain model to whole data using best n_estimators * 1.25
2.6.0,GDBT doesn't support multi-output(task)
2.6.0,########################################
2.6.0,Deprecation warnings for XGBoostModel
2.6.0,########################################
2.6.0,flake8: noqa
2.6.0,-*- coding: utf-8 -*-
2.6.0,Assigning featurizer if not user defined
2.6.0,loading datasets
2.6.0,Assembling train and valid datasets
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,Building tensorflow MultitaskDNN model
2.6.0,Building tensorflow robust MultitaskDNN model
2.6.0,Building scikit logistic regression model
2.6.0,Transform fingerprints to IRV features
2.6.0,Building tensorflow IRV model
2.6.0,Building scikit random forest model
2.6.0,Building scikit learn Kernel SVM model
2.6.0,Building xgboost classification model
2.6.0,Remove token for paddings
2.6.0,Building scikit random forest model
2.6.0,Building scikit learn Kernel Ridge Regression model
2.6.0,Building scikit learn Kernel Ridge Regression model
2.6.0,Building xgboost regression model
2.6.0,Loading hyperparameters
2.6.0,num positive/negative ligands
2.6.0,Set batch sizes for network
2.6.0,Model structure
2.6.0,Traning settings
2.6.0,Fit trained model
2.6.0,Evaluating low data model
2.6.0,-*- coding: utf-8 -*-
2.6.0,Assigning featurizer if not user defined
2.6.0,loading datasets
2.6.0,
2.6.0,Note by @XericZephyr. Reason why I spun off this function:
2.6.0,1. Some model needs dataset information.
2.6.0,2. It offers us possibility to **cache** the dataset
2.6.0,"if the featurizer runs very slow, e.g., GraphConv."
2.6.0,2+. The cache can even happen at Travis CI to accelerate
2.6.0,CI testing.
2.6.0,
2.6.0,loading datasets
2.6.0,!/usr/bin/env python2
2.6.0,-*- coding: utf-8 -*-
2.6.0,"TODO For this dataset and model, the R2-scores are less than 0.3."
2.6.0,This has to be improved.
2.6.0,See: https://github.com/deepchem/deepchem/issues/2776
2.6.0,TODO: Check for this
2.6.0,Download files if they don't exist
2.6.0,Featurize the KINASE dataset
2.6.0,Shuffle the training data
2.6.0,Apply transformations
2.6.0,TIMING
2.6.0,transformers = [
2.6.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.6.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.6.0,dataset=train_dataset)]
2.6.0,Set shard size low to avoid memory problems.
2.6.0,TIMING
2.6.0,TIMING
2.6.0,Set some global variables up top
2.6.0,Featurize KAGGLE dataset
2.6.0,TIMING
2.6.0,TIMING
2.6.0,Build the path to the dataset on disk.
2.6.0,Try to reload cached datasets.
2.6.0,Create the dataset
2.6.0,Split and transform the dataset.
2.6.0,. clinical trial toxicity (or absence of toxicity)
2.6.0,. FDA approval status.
2.6.0,Download files if they don't exist
2.6.0,Featurizing datasets
2.6.0,Missing entry removal
2.6.0,Shuffle the training data
2.6.0,Apply transformations
2.6.0,TIMING
2.6.0,TODO: Check if anything needs to be added
2.6.0,Featurize the FACTORS dataset
2.6.0,Shuffle the training data
2.6.0,Apply transformations
2.6.0,TIMING
2.6.0,dict of accepted featurizers for this dataset
2.6.0,modify the returned dicts for your dataset
2.6.0,Names of supported featurizers
2.6.0,dict of accepted transformers
2.6.0,dict of accepted splitters
2.6.0,names of supported splitters
2.6.0,Warning message about this template
2.6.0,Featurize mydataset
2.6.0,Get DeepChem data directory if needed
2.6.0,Check for str args to featurizer and splitter
2.6.0,Reload from disk
2.6.0,First type of supported featurizers
2.6.0,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.6.0,Changer loader to match featurizer and data file type
2.6.0,Featurize dataset
2.6.0,Initialize transformers
2.6.0,"get pdb and sdf filenames, labels and pdbids"
2.6.0,load and featurize each complex
2.6.0,Extract locations of data
2.6.0,Extract labels
2.6.0,Lines have format
2.6.0,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.6.0,"The base-10 logarithm, -log kd/pk"
2.6.0,"def load_pcba_146(featurizer='ECFP',"
2.6.0,"split='random',"
2.6.0,"reload=True,"
2.6.0,"data_dir=None,"
2.6.0,"save_dir=None,"
2.6.0,**kwargs):
2.6.0,return load_pcba_dataset(
2.6.0,"featurizer=featurizer,"
2.6.0,"split=split,"
2.6.0,"reload=reload,"
2.6.0,"assay_file_name=""pcba_146.csv.gz"","
2.6.0,"data_dir=data_dir,"
2.6.0,"save_dir=save_dir,"
2.6.0,**kwargs)
2.6.0,"def load_pcba_2475(featurizer='ECFP',"
2.6.0,"split='random',"
2.6.0,"reload=True,"
2.6.0,"data_dir=None,"
2.6.0,"save_dir=None,"
2.6.0,**kwargs):
2.6.0,return load_pcba_dataset(
2.6.0,"featurizer=featurizer,"
2.6.0,"split=split,"
2.6.0,"reload=reload,"
2.6.0,"assay_file_name=""pcba_2475.csv.gz"","
2.6.0,"data_dir=data_dir,"
2.6.0,"save_dir=save_dir,"
2.6.0,**kwargs)
2.6.0,Range of optimization
2.6.0,We know from guard above that this is an int/float
2.6.0,Specify logfile
2.6.0,Make logdir if it doesn't exist.
2.6.0,setup range
2.6.0,Stores all results
2.6.0,Store all model references so we don't have to reload
2.6.0,Stores all model locations
2.6.0,"param values are always float in BO, so this line converts float to int"
2.6.0,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.6.0,Record hyperparameters
2.6.0,We have already evaluated the model for these hyperparameters.
2.6.0,Add it on to the information needed for the constructor
2.6.0,Not all models have nb_epoch
2.6.0,Some models autosave
2.6.0,Record performances
2.6.0,Store all results
2.6.0,Store reference to model
2.6.0,GPGO maximize performance by default
2.6.0,set performance to its negative value for minimization
2.6.0,Demarcating internal function for readability
2.6.0,execute GPGO
2.6.0,FIXME: Incompatible types in assignment
2.6.0,Let's fetch the model with the best parameters
2.6.0,Compare best model to default hyperparameters
2.6.0,Record hyperparameters
2.6.0,Return default hyperparameters
2.6.0,Construction dictionary mapping hyperparameter names to values
2.6.0,"mypy test throws error, so ignoring it in try"
2.6.0,Not all models have nb_epoch
2.6.0,Some models autosave
2.6.0,arbitrarily return last model
2.6.0,flake8: noqa
2.6.0,"2 model variants, 1 results.txt file"
2.6.0,Generate dummy dataset
2.6.0,Generate dummy dataset
2.6.0,These are per-example multiplier
2.6.0,Test that 2 parameters were optimized
2.6.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.6.0,Generate dummy dataset
2.6.0,Define nb_epoch in hyperparam_search function call
2.6.0,Generate dummy dataset
2.6.0,Generate dummy dataset
2.6.0,These are per-example multiplier
2.6.0,Test that 2 parameters were optimized
2.6.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.6.0,Generate dummy dataset
2.6.0,Have the worker threads generate the rollouts for this iteration.
2.6.0,Perform optimization.
2.6.0,Build the inputs and run the optimizer.
2.6.0,Update the number of steps taken so far and perform checkpointing.
2.6.0,Merge all the rollouts into a single set of arrays.
2.6.0,Iterate slices.
2.6.0,Generate the rollout.
2.6.0,Compute an estimate of the reward for the rest of the episode.
2.6.0,Compute the discounted rewards and advantages.
2.6.0,Convert the actions to one-hot.
2.6.0,Rearrange the states into the proper set of arrays.
2.6.0,Return the processed arrays.
2.6.0,Training loop.
2.6.0,Do checkpointing.
2.6.0,Generate the rollout.
2.6.0,Compute an estimate of the reward for the rest of the episode.
2.6.0,Compute the discounted rewards and advantages.
2.6.0,"Record the actions, converting to one-hot if necessary."
2.6.0,Rearrange the states into the proper set of arrays.
2.6.0,Build the inputs and apply gradients.
2.6.0,Assume all arrays are float32.
2.6.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.6.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.6.0,"game).  The average reward for any bet is slightly negative, so the best"
2.6.0,strategy is to walk away.
2.6.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.0,Optimize it.
2.6.0,"It should have learned that the expected value is very close to zero, and that the best"
2.6.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.6.0,top actions).
2.6.0,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.6.0,get the same result.
2.6.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.0,The environment just has a constant state.
2.6.0,The policy includes a single recurrent layer.
2.6.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.6.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.6.0,"On the first call, the initial state should be all zeros."
2.6.0,It should still be zeros since we didn't save it last time.
2.6.0,It should be different now.
2.6.0,This should be the same as the previous one.
2.6.0,"Now we reset it, so we should get the same result as initially."
2.6.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.6.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.6.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.6.0,at all.  Using hindsight makes it much easier.
2.6.0,A simple policy with two hidden layers.
2.6.0,Optimize it.
2.6.0,Try running it a few times and see if it succeeds.
2.6.0,The state consists of two numbers: a current value and a target value.
2.6.0,The policy just needs to learn to output the target value (or at least
2.6.0,move toward it).
2.6.0,A simple policy with no hidden layers.
2.6.0,Optimize it.
2.6.0,Try running it and see if it reaches the target
2.6.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.6.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.6.0,"game).  The average reward for any bet is slightly negative, so the best"
2.6.0,strategy is to walk away.
2.6.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.0,Optimize it.
2.6.0,"It should have learned that the expected value is very close to zero, and that the best"
2.6.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.6.0,top actions).
2.6.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.6.0,get the same result.
2.6.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.6.0,The environment just has a constant state.
2.6.0,The policy includes a single recurrent layer.
2.6.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.6.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.6.0,"On the first call, the initial state should be all zeros."
2.6.0,It should still be zeros since we didn't save it last time.
2.6.0,It should be different now.
2.6.0,This should be the same as the previous one.
2.6.0,"Now we reset it, so we should get the same result as initially."
2.6.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.6.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.6.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.6.0,at all.  Using hindsight makes it much easier.
2.6.0,A simple policy with two hidden layers.
2.6.0,Optimize it.
2.6.0,Try running it a few times and see if it succeeds.
2.6.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.6.0,Randomize who goes first
2.6.0,Illegal move -- the square is not empty
2.6.0,Move X
2.6.0,Did X Win
2.6.0,Did O Win
2.6.0,"default channels are ""conda-forge"" and ""omnia"""
2.6.0,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.5.0,Build a nightly package by default.
2.5.0,get the version from deepchem/__init__.py
2.5.0,nightly version : .devYearMonthDayHourMinute
2.5.0,Force to add `.dev` if `--release` option isn't passed when building
2.5.0,!/usr/bin/env python3
2.5.0,-*- coding: utf-8 -*-
2.5.0,Datasets and models used in the benchmark test
2.5.0,"irv, rf, rf_regression should be assigned manually"
2.5.0,Evaluate performances with different training set fraction
2.5.0,Datasets and models used in the benchmark test
2.5.0,Uncomment the two lines below if hyper_parameters are provided
2.5.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.5.0,hyper_parameters = pickle.load(f)
2.5.0,!/usr/bin/env python3
2.5.0,-*- coding: utf-8 -*-
2.5.0,Datasets and models used in the benchmark test
2.5.0,Load Delaney dataset
2.5.0,Get Metric
2.5.0,Fit trained model
2.5.0,Fit trained model
2.5.0,Set numpy seed
2.5.0,##Load data###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Featurize Kinase dataset
2.5.0,##Load data###
2.5.0,num_trials = 5
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,Force matplotlib to not use any Xwindows backend.
2.5.0,##Load data###
2.5.0,the histogram of the data
2.5.0,Set numpy seed
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,num_trials = 5
2.5.0,Set some global variables up top
2.5.0,Fit trained model
2.5.0,Featurize PCBA dataset
2.5.0,Initialize transformers
2.5.0,Fit trained model
2.5.0,Load sider models now
2.5.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.5.0,transformers to predict functions
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,##Load data###
2.5.0,"n_estimators=100, max_features=int(num_features/3),"
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,Fit trained model
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Load tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,This example shows how to use Pandas to load data directly
2.5.0,without using a CSVLoader object. This may be useful if you
2.5.0,want the flexibility of processing your data with Pandas
2.5.0,directly.
2.5.0,Now let's convert from a dataset back to a pandas dataframe
2.5.0,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.5.0,Featurize FACTORS dataset
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,Force matplotlib to not use any Xwindows backend.
2.5.0,##Load data###
2.5.0,the histogram of the data
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Load QM7 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Fit trained model
2.5.0,Fit trained model
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Fit trained model
2.5.0,Load QM8 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Fit trained model
2.5.0,Set numpy seed
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,Load ChEMBL dataset
2.5.0,Fit models
2.5.0,Do setup required for tf/keras models
2.5.0,Number of features on conv-mols
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,DeepCrystal Technologies 2017 - Patrick Hop
2.5.0,MIT License - have fun!!
2.5.0,Set to higher values to get better numbers
2.5.0,======================================================================
2.5.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,Only for debug!
2.5.0,Load Delaney dataset
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Do setup required for tf/keras models
2.5.0,Number of features on conv-mols
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load Delaney dataset
2.5.0,Get Metric
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load Delaney dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load MUV dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Evaluate train/test scores
2.5.0,Load MUV data
2.5.0,Build model
2.5.0,Fit trained model
2.5.0,Evaluate train/test scores
2.5.0,Extract active site
2.5.0,Featurize ligand
2.5.0,Default for CircularFingerprint
2.5.0,Featurize pocket
2.5.0,Note broadcast operation
2.5.0,Compute labels for pockets
2.5.0,Some complexes have labels but no PDB files. Filter these manually
2.5.0,Some of the ligand-names are of form (FMN ox). Use regex
2.5.0,to merge into form (FMN-ox)
2.5.0,Filter if missing PDB files
2.5.0,Load PDBBind dataset
2.5.0,Define featurizers
2.5.0,Featurize Dataset
2.5.0,########################################################## DEBUG
2.5.0,########################################################## DEBUG
2.5.0,For stable runs
2.5.0,Fit trained model
2.5.0,4-fold splits
2.5.0,num positive/negative ligands
2.5.0,10 trials on test-set
2.5.0,Define metric
2.5.0,Get supports on test-set
2.5.0,Compute accuracies
2.5.0,Number of features on conv-mols
2.5.0,Batch size of models
2.5.0,graph_model = dc.nn.SequentialGraph(n_feat)
2.5.0,Fit trained model
2.5.0,Test model
2.5.0,Join information for all tasks.
2.5.0,4-fold splits
2.5.0,num positive/negative ligands
2.5.0,Define metric
2.5.0,Get supports on test-set
2.5.0,Compute accuracies
2.5.0,Number of features on conv-mols
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Test model
2.5.0,Join information for all tasks.
2.5.0,4-fold splits
2.5.0,num positive/negative ligands
2.5.0,10 trials on test-set
2.5.0,Define metric
2.5.0,Get supports on test-set
2.5.0,Compute accuracies
2.5.0,Number of features on conv-mols
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Test model
2.5.0,Join information for all tasks.
2.5.0,4-fold splits
2.5.0,num positive/negative ligands
2.5.0,10 trials on test-set
2.5.0,Define metric
2.5.0,Get supports on test-set
2.5.0,Compute accuracies
2.5.0,Train model on support
2.5.0,Test model
2.5.0,Join information for all tasks.
2.5.0,4-fold splits
2.5.0,num positive/negative ligands
2.5.0,10 trials on test-set
2.5.0,Define metric
2.5.0,Get supports on test-set
2.5.0,Compute accuracies
2.5.0,Train model on support
2.5.0,Test model
2.5.0,Join information for all tasks.
2.5.0,Set some global variables up top
2.5.0,Featurize Tox21 dataset
2.5.0,Initialize transformers
2.5.0,Set some global variables up top
2.5.0,Featurize Tox21 dataset
2.5.0,Initialize transformers
2.5.0,Load MUV dataset
2.5.0,Featurize MUV dataset
2.5.0,Initialize transformers
2.5.0,Load MUV dataset
2.5.0,Featurize MUV dataset
2.5.0,Initialize transformers
2.5.0,Featurize SIDER dataset
2.5.0,Initialize transformers
2.5.0,Featurize SIDER dataset
2.5.0,Initialize transformers
2.5.0,Load the data.
2.5.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.5.0,sparse: most tasks do not include data for most molecules.  It also is very
2.5.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.5.0,create a list of alternating positives and negatives so each batch will have
2.5.0,equal numbers of both.
2.5.0,Define a MetaLearner describing the learning problem.
2.5.0,Run meta-learning on 80% of the tasks.
2.5.0,Validate on the remaining tasks.
2.5.0,4-fold splits
2.5.0,10 positive/negative ligands
2.5.0,10 trials on test-set
2.5.0,Sample supports without replacement (all pos/neg should be different)
2.5.0,Define metric
2.5.0,Get supports on test-set
2.5.0,Compute accuracies
2.5.0,Train model on support
2.5.0,Test model
2.5.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.5.0,Join information for all tasks.
2.5.0,4-fold splits
2.5.0,num positive/negative ligands
2.5.0,Define metric
2.5.0,Get supports on test-set
2.5.0,Compute accuracies
2.5.0,Train model on support
2.5.0,Test model
2.5.0,Join information for all tasks.
2.5.0,replace with your own scratch directory
2.5.0,Number of conformations in each file increases exponentially.
2.5.0,Start with a smaller dataset before continuing. Use all of them
2.5.0,for production
2.5.0,"'ani_gdb_s03.h5',"
2.5.0,"'ani_gdb_s04.h5',"
2.5.0,"'ani_gdb_s05.h5',"
2.5.0,"'ani_gdb_s06.h5',"
2.5.0,"'ani_gdb_s07.h5',"
2.5.0,'ani_gdb_s08.h5'
2.5.0,Extract the data
2.5.0,Print the data
2.5.0,self-interaction energies taken from
2.5.0,https://github.com/isayev/ANI1_dataset README
2.5.0,flush once more at the end
2.5.0,"# For production, set nb_epoch to 100+"
2.5.0,"print(""Train scores"")"
2.5.0,print(train_scores)
2.5.0,"print(""Minimization of a single test set structure:"")"
2.5.0,"print(model.minimize_structure(coords, atomic_nums))"
2.5.0,Written by Roman Zubatyuk and Justin S. Smith
2.5.0,Modified by Yutong Zhao to make python2 compatible
2.5.0,opening file
2.5.0,print(store_loc)
2.5.0,print(type(v[0]))
2.5.0,print(k)
2.5.0,print(path)
2.5.0,Number of conformations in each file increases exponentially.
2.5.0,Start with a smaller dataset before continuing. Use all of them
2.5.0,for production
2.5.0,Extract the data
2.5.0,NOTE THE RENAMING:
2.5.0,Note sensitivity = recall
2.5.0,Load nci dataset
2.5.0,Featurize nci dataset
2.5.0,Initialize transformers
2.5.0,Set some global variables up top
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load hiv dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load hiv dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load delaney dataset
2.5.0,Fit models
2.5.0,Load delaney dataset
2.5.0,Fit models
2.5.0,Fit models
2.5.0,Load delaney dataset
2.5.0,Fit models
2.5.0,TODO: Once improved splitting API is merged in swap to simpler API
2.5.0,The return values are dc.data.Dataset objects so we need to extract
2.5.0,the ids
2.5.0,TODO once improved splitting API is merged in swap out for simpler
2.5.0,API
2.5.0,The return values are dc.data.Dataset objects so we need to extract
2.5.0,the ids
2.5.0,Fit trained model
2.5.0,Load SIDER dataset
2.5.0,Featurize SIDER dataset
2.5.0,Initialize transformers
2.5.0,Featurize permeability dataset
2.5.0,Load Tox21 dataset
2.5.0,Fit trained model
2.5.0,TODO: This should be swapped for simpler splitter API once that's merged in.
2.5.0,The return values are dc.data.Dataset objects so we need to extract
2.5.0,the ids
2.5.0,Only for debug!
2.5.0,Load clintox dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load clintox dataset
2.5.0,Fit models
2.5.0,Do setup required for tf/keras models
2.5.0,Number of features on conv-mols
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,-*- coding: utf-8 -*-
2.5.0,#############################################################################
2.5.0,## save dataset
2.5.0,#############################################################################
2.5.0,## load datasets
2.5.0,load sweetfda
2.5.0,load aact
2.5.0,## fixup smiles for matching
2.5.0,return smiles
2.5.0,map original smiles to converted smiles
2.5.0,"## join dataframes, index on smiles"
2.5.0,map original smiles back
2.5.0,## fill all nan with 0
2.5.0,## construct datasets
2.5.0,store in new datasets
2.5.0,## save datasets
2.5.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.5.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.5.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.5.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.5.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.5.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.5.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.5.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.5.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.5.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.5.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.5.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.5.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.5.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.5.0,For stable runs
2.5.0,Fit trained model
2.5.0,For stable runs
2.5.0,Fit trained model
2.5.0,For stable runs
2.5.0,Fit trained model
2.5.0,transformers = [
2.5.0,"dc.trans.LogTransformer(transform_X=True),"
2.5.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.5.0,dataset=train_dataset)]
2.5.0,Featurize UV dataset
2.5.0,##Load data###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Set numpy seed
2.5.0,##Load data###
2.5.0,##Create model###
2.5.0,Use R2 classification metric
2.5.0,Only use for final evaluation
2.5.0,Force matplotlib to not use any Xwindows backend.
2.5.0,##Load data###
2.5.0,the histogram of the data
2.5.0,##Load data###
2.5.0,###################################################### DEBUG
2.5.0,###################################################### DEBUG
2.5.0,Load HOPV dataset
2.5.0,Fit models
2.5.0,Number of features on conv-mols
2.5.0,Batch size of models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load HOPV dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load HOPV dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load HOPV dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Only for debug!
2.5.0,Load HOPV dataset
2.5.0,Fit models
2.5.0,Fit trained model
2.5.0,Load TOXCAST dataset
2.5.0,Featurize TOXCAST dataset
2.5.0,Initialize transformers
2.5.0,Fit trained model
2.5.0,Processing of ToxCast data
2.5.0,Author - Aneesh Pappu
2.5.0,Loading dataframes and editing indices
2.5.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.5.0,get corresponding casn
2.5.0,get corresponding smiles
2.5.0,write to cell
2.5.0,Tidy up and write to csv
2.5.0,TODO(rbharath): Check that this operation is differentiable.
2.5.0,The number of cells which we should theoretically have
2.5.0,The number of cells which we should theoretically have
2.5.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.5.0,The number of cells which we should theoretically have
2.5.0,TODO(rbharath): The test below only checks that shapes work out.
2.5.0,Need to do a correctness implementation vs. a simple CPU impl.
2.5.0,The number of cells which we should theoretically have
2.5.0,TODO(rbharath): The test below only checks that shapes work out.
2.5.0,Need to do a correctness implementation vs. a simple CPU impl.
2.5.0,The number of cells which we should theoretically have
2.5.0,TODO(rbharath): The test below only checks that shapes work out.
2.5.0,Need to do a correctness implementation vs. a simple CPU impl.
2.5.0,TODO(rbharath): Commenting this out due to weird segfaults
2.5.0,def test_vina_generate_conformers(self):
2.5.0,"""""""Test that Vina Model can generate conformers"""""""
2.5.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.5.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.5.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.5.0,max_protein_atoms = 3500
2.5.0,max_ligand_atoms = 100
2.5.0,"print(""Loading protein file"")"
2.5.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.5.0,protein_Z = pad_array(
2.5.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.5.0,max_protein_atoms)
2.5.0,"print(""Loading ligand file"")"
2.5.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.5.0,ligand_Z = pad_array(
2.5.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.5.0,max_ligand_atoms)
2.5.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.5.0,"Shape (n_cells, k)"
2.5.0,"Shape (N, 1)"
2.5.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.5.0,"conditions, so does wrapround. O(constant)"
2.5.0,"Shape (n_cells, 26)"
2.5.0,"Shape (N, 26)"
2.5.0,"coords of shape (N, ndim)"
2.5.0,"Shape (N, 26, k, ndim)"
2.5.0,"Shape (N, 26, k)"
2.5.0,"Shape (N, 26, k)"
2.5.0,"Shape (N, 26, k, ndim)"
2.5.0,"For smaller systems especially, the periodic boundary conditions can"
2.5.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.5.0,make sure duplicate neighbors are ignored?
2.5.0,TODO(rbharath): How does distance need to be modified here to
2.5.0,account for periodic boundary conditions?
2.5.0,"Shape (N, 26, k)"
2.5.0,"Shape (N, 26*k)"
2.5.0,TODO(rbharath): This will cause an issue with duplicates!
2.5.0,"Shape (N, M)"
2.5.0,"N elts of size (M,) each"
2.5.0,"Shape (N, 26*k)"
2.5.0,"N elts of size (26*k,) each"
2.5.0,"N elts of size (M,) each"
2.5.0,"Shape (N, M)"
2.5.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.5.0,"N tensors of shape (n_cells, 1)"
2.5.0,"Shape (N*n_cells, 1) after tile"
2.5.0,"List of N tensors of shape (n_cells, 1)"
2.5.0,Lists of length N
2.5.0,Lists of length n_cells
2.5.0,Get indices of k atoms closest to each cell point
2.5.0,TODO(rbharath): tf.stack for tf 1.0
2.5.0,"Tensor of shape (n_cells, k, ndim)"
2.5.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.5.0,"Tensor of shape (26, k, ndim)"
2.5.0,"Reshape to (26*k, ndim)"
2.5.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.5.0,"Dists of shape (26*k, 1)"
2.5.0,"Of shape (k, ndim)"
2.5.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.5.0,TODO(rbharath): Change this for tf 1.0
2.5.0,"n_cells tensors of shape (N, 1)"
2.5.0,"Shape (N*n_cells, 1) after tile"
2.5.0,"List of n_cells tensors of shape (N, 1)"
2.5.0,Lists of length n_cells
2.5.0,Lists of length n_cells
2.5.0,Get indices of k atoms closest to each cell point
2.5.0,"n_cells tensors of shape (k, ndim)"
2.5.0,"Tensor of shape (n_cells, k)"
2.5.0,TODO(rbharath):
2.5.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.5.0,- Need to group closest atoms amongst cell neighbors
2.5.0,- Need to do another top_k to find indices of closest neighbors.
2.5.0,- Return N lists corresponding to neighbors for every atom.
2.5.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.5.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.5.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.5.0,the cube.
2.5.0,Number of neighbors of central cube in 3-space is
2.5.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.5.0,TODO(rbharath)
2.5.0,n_cells = int(cells.get_shape()[0])
2.5.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.5.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.5.0,"Tile (a, a, a, b, b, b, etc.)"
2.5.0,"Tile (a, b, c, a, b, c, ...)"
2.5.0,"Lists of n_cells tensors of shape (N, 1)"
2.5.0,Lists of length n_cells
2.5.0,Lists of length n_cells
2.5.0,Get indices of k atoms closest to each cell point
2.5.0,"n_cells tensors of shape (26,)"
2.5.0,TODO(rbharath): Make this handle minibatches
2.5.0,"Shape (N_protein+N_ligand, 3)"
2.5.0,"Shape (N_protein+N_ligand,)"
2.5.0,"Shape (N_protein+N_ligand, M)"
2.5.0,"Shape (N_protein+N_ligand,)"
2.5.0,"Shape (N_protein+N_ligand, 3)"
2.5.0,"Shape (N_protein+N_ligand,)"
2.5.0,"Shape (N_protein+N_ligand, M)"
2.5.0,"Shape (N_protein+N_ligand, M, 3)"
2.5.0,"Shape (N_protein+N_ligand, M)"
2.5.0,"Shape (N_protein+N_ligand, M, 3)"
2.5.0,"Shape (N_protein+N_ligand, M)"
2.5.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.5.0,"Shape (N_protein+N_ligand, M)"
2.5.0,"Shape (N_protein+N_ligand, M)"
2.5.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.5.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.5.0,computing free-energy. This implementation currently uses all interaction
2.5.0,terms. Not sure if this makes a difference.
2.5.0,"Shape (N_protein+N_ligand, M)"
2.5.0,Shape () -- scalar
2.5.0,Keep track of the layers
2.5.0,"For graphical layers, add connectivity placeholders"
2.5.0,Add layer to the layer list
2.5.0,Keep track of the layers
2.5.0,Create graph topology and x
2.5.0,Keep track of the layers
2.5.0,Whether or not we have used the GraphGather layer yet
2.5.0,Update new value of x
2.5.0,Update new value of x
2.5.0,Update new value of x
2.5.0,Get train function
2.5.0,Initialize
2.5.0,################################################################### DEBUG
2.5.0,self.test_label_placeholder = Input(
2.5.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.5.0,"name=""label_placeholder""))"
2.5.0,self.test_weight_placeholder = Input(
2.5.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.5.0,"name=""weight_placeholder""))"
2.5.0,TODO(rbharath): Should weights for the support be used?
2.5.0,Support labels
2.5.0,self.support_label_placeholder = Input(
2.5.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.5.0,"name=""support_label_placeholder""))"
2.5.0,################################################################### DEBUG
2.5.0,Generate dictionary elements for support
2.5.0,Get graph information for test
2.5.0,Generate dictionary elements for test
2.5.0,Perform the optimization
2.5.0,Create different support sets
2.5.0,Get batch to try it out on
2.5.0,"Train on support set, batch pair"
2.5.0,Get featurization for test
2.5.0,"Shape (n_test, n_feat)"
2.5.0,Get featurization for support
2.5.0,"Shape (n_support, n_feat)"
2.5.0,Computes the inner part c() of the kernel
2.5.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.5.0,Normalize
2.5.0,TODO(rbharath): euclidean kernel is broken!
2.5.0,elif self.similarity == 'euclidean':
2.5.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.5.0,"Note that gram matrix g has shape (n_test, n_support)"
2.5.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.5.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.5.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.5.0,each test entry) to get attention vector
2.5.0,"Shape (n_test, n_support)"
2.5.0,Weighted sum of support labels
2.5.0,"Shape (n_support, 1)"
2.5.0,pred is yhat in eqn (1) of Matching Networks.
2.5.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.5.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.5.0,"Shape (n_test,)"
2.5.0,Convert to logit space using inverse sigmoid (logit) function
2.5.0,logit function: log(pred) - log(1-pred)
2.5.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.5.0,in Cross Entropy calculation.
2.5.0,"Shape (n_test,)"
2.5.0,Get scores
2.5.0,Remove padded elements
2.5.0,Get scores
2.5.0,pred corresponds to prob(example == 1)
2.5.0,Remove padded elements
2.5.0,Get batches
2.5.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.5.0,multitask case with missing data...
2.5.0,Join information for all tasks.
2.5.0,TODO(rbharath): Find a way to get rid of this import?
2.5.0,Extract model info
2.5.0,Get graph topology for x
2.5.0,Building outputs
2.5.0,Set epsilon
2.5.0,Initialize
2.5.0,"Path to save checkpoint files, which matches the"
2.5.0,replicated supervisor's default path.
2.5.0,Create target inputs
2.5.0,Get train function
2.5.0,TODO(rbharath): I believe this is total amount of data
2.5.0,Get graph information
2.5.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.5.0,the number of labeled data points in target_i. This is to normalize each task
2.5.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.5.0,Get other optimizer information
2.5.0,TODO(rbharath): Figure out how to handle phase appropriately
2.5.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.5.0,"tensors of shape (batch_size,)"
2.5.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.5.0,examples (effect averages out)
2.5.0,Perform the optimization
2.5.0,TODO(rbharath): Disabling saving for now to try to debug.
2.5.0,run eval data through the model
2.5.0,"Shape (n_samples, n_tasks)"
2.5.0,Create target inputs
2.5.0,TODO(rbharath): Find a way to get rid of this import?
2.5.0,Obtain appropriate loss function
2.5.0,Extract model info
2.5.0,Get graph topology for x
2.5.0,Raw logit outputs
2.5.0,Set epsilon
2.5.0,Initialize
2.5.0,"Path to save checkpoint files, which matches the"
2.5.0,replicated supervisor's default path.
2.5.0,Create target inputs
2.5.0,############################################################### DEBUG
2.5.0,"print(""multitask classifier"")"
2.5.0,"print(""feat"")"
2.5.0,print(feat)
2.5.0,############################################################### DEBUG
2.5.0,Get train function
2.5.0,TODO(rbharath): I believe this is total amount of data
2.5.0,Get graph information
2.5.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.5.0,the number of labeled data points in target_i. This is to normalize each task
2.5.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.5.0,Get other optimizer information
2.5.0,TODO(rbharath): Figure out how to handle phase appropriately
2.5.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.5.0,"tensors of shape (batch_size,)"
2.5.0,Convert the labels into one-hot vector encodings.
2.5.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.5.0,un-softmaxed logits rather than softmax outputs.
2.5.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.5.0,examples (effect averages out)
2.5.0,Perform the optimization
2.5.0,TODO(rbharath): Disabling saving for now to try to debug.
2.5.0,run eval data through the model
2.5.0,"Shape (n_samples, n_tasks)"
2.5.0,run eval data through the model
2.5.0,self.n_atoms = n_atoms
2.5.0,Define the list of tensors to be used as topology
2.5.0,Merge mol conv objects
2.5.0,Generate dicts
2.5.0,Define the list of tensors to be used as topology
2.5.0,Extract atom numbers
2.5.0,Generate dicts
2.5.0,molecule * atom(graph) => step => features
2.5.0,molecule * atom(graph) => step
2.5.0,molecule * atom(graph) => step
2.5.0,Define the list of tensors to be used as topology
2.5.0,calculation orders for a batch of molecules
2.5.0,padding atom features vector of each molecule with 0
2.5.0,self.n_atoms = n_atoms
2.5.0,Define the list of tensors to be used as topology
2.5.0,Extract atom numbers
2.5.0,Generate dicts
2.5.0,self.n_atoms = n_atoms
2.5.0,Define the list of tensors to be used as topology
2.5.0,Extract atom numbers
2.5.0,number of atoms in each molecule
2.5.0,index of pair features
2.5.0,number of pairs for each atom
2.5.0,atom features
2.5.0,pair features
2.5.0,Generate dicts
2.5.0,Load Tox21 dataset
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.5.0,Fit trained model
2.5.0,Fit models
2.5.0,Batch size of models
2.5.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.5.0,Fit trained model
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,number positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply an attention lstm layer
2.5.0,Number of folds for split
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,number positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply an attention lstm layer
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,number positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply an attention lstm layer
2.5.0,Number of folds for split
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Number of folds for split
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply a residual lstm layer
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply a residual lstm layer
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply a residual lstm layer
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply a residual lstm layer
2.5.0,Number of folds for split
2.5.0,Depth of attention module
2.5.0,number positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,Apply an attention lstm layer
2.5.0,Number of folds for split
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Number of features on conv-mols
2.5.0,Define metric
2.5.0,Train support model on train
2.5.0,Add layers
2.5.0,# Gather Projection
2.5.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.5.0,There should be 8 layers in graph_model
2.5.0,assert len(graph_model.layers) == 6
2.5.0,Add layers
2.5.0,Need to add batch-norm separately to test/support due to differing
2.5.0,shapes.
2.5.0,Apply an attention lstm layer
2.5.0,Gather Projection
2.5.0,Add layers
2.5.0,Need to add batch-norm separately to test/support due to differing
2.5.0,shapes.
2.5.0,Apply an attention lstm layer
2.5.0,Gather Projection
2.5.0,Degrees from 1 to max_deg inclusive
2.5.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.5.0,"Should have shape (?, deg)"
2.5.0,"Shape of atom_features should be (?, n_feat)"
2.5.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.5.0,-*- coding: utf-8 -*-
2.5.0,Save hyperparameters
2.5.0,-*- coding: utf-8 -*-
2.5.0,Save hyperparameters
2.5.0,setup optimizer
2.5.0,setup optimizer
2.5.0,"print(""tasK: %d"" %task)"
2.5.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.5.0,"print(""scores"")"
2.5.0,print(scores.size())
2.5.0,"print(""task_label"")"
2.5.0,print(task_label.size())
2.5.0,"task_loss =  self.criterion(scores, task_label)"
2.5.0,"print(""task_loss"")"
2.5.0,print(task_loss.size())
2.5.0,-*- coding: utf-8 -*-
2.5.0,Save hyperparameters
2.5.0,weight decay
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Turns out there are valid cases where we don't want pad-batches
2.5.0,on by default.
2.5.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.5.0,Run training op.
2.5.0,############################################################# TIMING
2.5.0,The iterbatches does padding with zero-weight examples on the last batch.
2.5.0,Remove padded examples.
2.5.0,Special case to handle singletasks.
2.5.0,The iterbatches does padding with zero-weight examples on the last batch.
2.5.0,Remove padded examples.
2.5.0,References
2.5.0,Arguments
2.5.0,Aliases.
2.5.0,Aliases.
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,TODO(rbharath): This class does not yet have a
2.5.0,"TensorGraph equivalent, but one may not be required."
2.5.0,"Commented out for now, remove if OK."
2.5.0,class AlternateWeaveLayer(WeaveLayer):
2.5.0,""""""" Alternate implementation of weave module"
2.5.0,"same variables, different graph structures"
2.5.0,""""""""
2.5.0,
2.5.0,"def call(self, x, mask=None):"
2.5.0,"""""""Execute this layer on input tensors."
2.5.0,
2.5.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.5.0,
2.5.0,Parameters
2.5.0,----------
2.5.0,x: list
2.5.0,list of Tensors of form described above.
2.5.0,"mask: bool, optional"
2.5.0,Ignored. Present only to shadow superclass call() method.
2.5.0,
2.5.0,Returns
2.5.0,-------
2.5.0,A: Tensor
2.5.0,Tensor of atom_features
2.5.0,P: Tensor
2.5.0,Tensor of pair_features
2.5.0,""""""""
2.5.0,# Add trainable weights
2.5.0,self.build()
2.5.0,
2.5.0,atom_features = x[0]
2.5.0,pair_features = x[1]
2.5.0,
2.5.0,pair_split = x[2]
2.5.0,atom_to_pair = x[4]
2.5.0,
2.5.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.5.0,AA = self.activation(AA)
2.5.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.5.0,PA = self.activation(PA)
2.5.0,"PA = tf.segment_sum(PA, pair_split)"
2.5.0,
2.5.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.5.0,A = self.activation(A)
2.5.0,
2.5.0,if self.update_pair:
2.5.0,AP_ij = tf.matmul(
2.5.0,tf.reshape(
2.5.0,"tf.gather(atom_features, atom_to_pair),"
2.5.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.5.0,AP_ij = self.activation(AP_ij)
2.5.0,AP_ji = tf.matmul(
2.5.0,tf.reshape(
2.5.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.5.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.5.0,AP_ji = self.activation(AP_ji)
2.5.0,
2.5.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.5.0,PP = self.activation(PP)
2.5.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.5.0,P = self.activation(P)
2.5.0,else:
2.5.0,P = pair_features
2.5.0,
2.5.0,"return A, P"
2.5.0,TODO(rbharath): This class does not yet have a
2.5.0,"TensorGraph equivalent, but one may not be required."
2.5.0,"Commented out for now, remove if OK."
2.5.0,class WeaveConcat(Layer):
2.5.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.5.0,""""""""
2.5.0,
2.5.0,"def __init__(self,"
2.5.0,"batch_size,"
2.5.0,"n_atom_input_feat=50,"
2.5.0,"n_output=128,"
2.5.0,"init='glorot_uniform',"
2.5.0,"activation='tanh',"
2.5.0,**kwargs):
2.5.0,""""""""
2.5.0,Parameters
2.5.0,----------
2.5.0,batch_size: int
2.5.0,number of molecules in a batch
2.5.0,"n_atom_input_feat: int, optional"
2.5.0,Number of features for each atom in input.
2.5.0,"n_output: int, optional"
2.5.0,Number of output features for each atom(concatenated)
2.5.0,"init: str, optional"
2.5.0,Weight initialization for filters.
2.5.0,"activation: str, optional"
2.5.0,Activation function applied
2.5.0,
2.5.0,""""""""
2.5.0,self.batch_size = batch_size
2.5.0,self.n_atom_input_feat = n_atom_input_feat
2.5.0,self.n_output = n_output
2.5.0,self.init = initializations.get(init)  # Set weight initialization
2.5.0,self.activation = activations.get(activation)  # Get activations
2.5.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.5.0,
2.5.0,def build(self):
2.5.0,"""""""""Construct internal trainable weights."
2.5.0,""""""""
2.5.0,
2.5.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.5.0,self.b = model_ops.zeros(shape=[
2.5.0,"self.n_output,"
2.5.0,])
2.5.0,
2.5.0,self.trainable_weights = self.W + self.b
2.5.0,
2.5.0,"def call(self, x, mask=None):"
2.5.0,"""""""Execute this layer on input tensors."
2.5.0,
2.5.0,"x = [atom_features, atom_mask]"
2.5.0,
2.5.0,Parameters
2.5.0,----------
2.5.0,x: list
2.5.0,Tensors as listed above
2.5.0,"mask: bool, optional"
2.5.0,Ignored. Present only to shadow superclass call() method.
2.5.0,
2.5.0,Returns
2.5.0,-------
2.5.0,outputs: Tensor
2.5.0,Tensor of concatenated atom features
2.5.0,""""""""
2.5.0,self.build()
2.5.0,atom_features = x[0]
2.5.0,atom_masks = x[1]
2.5.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.5.0,A_mask = tf.split(
2.5.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.5.0,outputs = tf.concat(
2.5.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.5.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.5.0,outputs = self.activation(outputs)
2.5.0,return outputs
2.5.0,TODO(rbharath): This class does not yet have a
2.5.0,"TensorGraph equivalent, but one may not be required."
2.5.0,"Commented out for now, remove if OK."
2.5.0,class AlternateWeaveGather(WeaveGather):
2.5.0,"""""""Alternate implementation of weave gather layer"
2.5.0,corresponding to AlternateWeaveLayer
2.5.0,""""""""
2.5.0,
2.5.0,"def call(self, x, mask=None):"
2.5.0,"""""""Execute this layer on input tensors."
2.5.0,
2.5.0,"x = [atom_features, atom_split]"
2.5.0,
2.5.0,Parameters
2.5.0,----------
2.5.0,x: list
2.5.0,Tensors as listed above
2.5.0,"mask: bool, optional"
2.5.0,Ignored. Present only to shadow superclass call() method.
2.5.0,
2.5.0,Returns
2.5.0,-------
2.5.0,outputs: Tensor
2.5.0,Tensor of molecular features
2.5.0,""""""""
2.5.0,# Add trainable weights
2.5.0,self.build()
2.5.0,outputs = x[0]
2.5.0,atom_split = x[1]
2.5.0,
2.5.0,if self.gaussian_expand:
2.5.0,outputs = self.gaussian_histogram(outputs)
2.5.0,
2.5.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.5.0,
2.5.0,if self.gaussian_expand:
2.5.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.5.0,output_molecules = self.activation(output_molecules)
2.5.0,return output_molecules
2.5.0,Each directory holds a range of assay results
2.5.0,Just write NA
2.5.0,"Now, write out the results csv, going line by line through all molecule results"
2.5.0,printing the mol_id
2.5.0,printing the SMILES
2.5.0,Now gzip it
2.5.0,Now remove the intermediate csv
2.5.0,First download all SDF files. We need these to get smiles
2.5.0,Next download all Bioassays
2.5.0,RDKit consistently hangs when trying to read this file
2.5.0,TODO (LESWING) Lazy Load
2.5.0,TODO (LESWING) Lazy Load
2.5.0,from simdna import simulations
2.5.0,define layer out functions
2.5.0,get layer outputs for a positive simulation example
2.5.0,plot layer outputs
2.5.0,highlight motif sites
2.5.0,get a positive and a negative example from the simulation data
2.5.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.5.0,get motif site locations
2.5.0,organize legends
2.5.0,plot scores and highlight motif site locations
2.5.0,initialize fwd and reverse scores to -infinity
2.5.0,"cross-correlate separately for each base,"
2.5.0,for both the PSSM and its reverse complement
2.5.0,sum over the bases
2.5.0,take max of fwd and reverse scores at each position
2.5.0,return 1D view of sequence characters
2.5.0,class SequenceDNN(Model):
2.5.0,""""""""
2.5.0,Sequence DNN models.
2.5.0,
2.5.0,Parameters
2.5.0,----------
2.5.0,"seq_length : int, optional"
2.5.0,length of input sequence.
2.5.0,"keras_model : instance of keras.models.Sequential, optional"
2.5.0,seq_length or keras_model must be specified.
2.5.0,"num_tasks : int, optional"
2.5.0,number of tasks. Default: 1.
2.5.0,num_filters : list[int] | tuple[int]
2.5.0,"number of convolutional filters in each layer. Default: (15,)."
2.5.0,conv_width : list[int] | tuple[int]
2.5.0,"width of each layer's convolutional filters. Default: (15,)."
2.5.0,pool_width : int
2.5.0,width of max pooling after the last layer. Default: 35.
2.5.0,L1 : float
2.5.0,strength of L1 penalty.
2.5.0,dropout : float
2.5.0,dropout probability in every convolutional layer. Default: 0.
2.5.0,verbose: int
2.5.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.5.0,
2.5.0,Returns
2.5.0,-------
2.5.0,Compiled DNN model.
2.5.0,""""""""
2.5.0,
2.5.0,"def __init__(self,"
2.5.0,"seq_length=None,"
2.5.0,"keras_model=None,"
2.5.0,"use_RNN=False,"
2.5.0,"num_tasks=1,"
2.5.0,"num_filters=(15, 15, 15),"
2.5.0,"conv_width=(15, 15, 15),"
2.5.0,"pool_width=35,"
2.5.0,"GRU_size=35,"
2.5.0,"TDD_size=15,"
2.5.0,"L1=0,"
2.5.0,"dropout=0.0,"
2.5.0,"num_epochs=100,"
2.5.0,verbose=1):
2.5.0,self.num_tasks = num_tasks
2.5.0,self.num_epochs = num_epochs
2.5.0,self.verbose = verbose
2.5.0,self.train_metrics = []
2.5.0,self.valid_metrics = []
2.5.0,if keras_model is not None and seq_length is None:
2.5.0,self.model = keras_model
2.5.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.5.0,elif seq_length is not None and keras_model is None:
2.5.0,self.model = Sequential()
2.5.0,assert len(num_filters) == len(conv_width)
2.5.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.5.0,conv_height = 4 if i == 0 else 1
2.5.0,self.model.add(
2.5.0,Convolution2D(
2.5.0,"nb_filter=nb_filter,"
2.5.0,"nb_row=conv_height,"
2.5.0,"nb_col=nb_col,"
2.5.0,"activation='linear',"
2.5.0,"init='he_normal',"
2.5.0,"input_shape=(1, 4, seq_length),"
2.5.0,"W_regularizer=l1(L1),"
2.5.0,b_regularizer=l1(L1)))
2.5.0,self.model.add(Activation('relu'))
2.5.0,self.model.add(Dropout(dropout))
2.5.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.5.0,if use_RNN:
2.5.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.5.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.5.0,"self.model.add(Permute((2, 1)))"
2.5.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.5.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.5.0,self.model.add(Flatten())
2.5.0,self.model.add(Dense(output_dim=self.num_tasks))
2.5.0,self.model.add(Activation('sigmoid'))
2.5.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.5.0,else:
2.5.0,raise ValueError(
2.5.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.5.0,
2.5.0,"def train(self,"
2.5.0,"X,"
2.5.0,"y,"
2.5.0,"validation_data,"
2.5.0,"early_stopping_metric='Loss',"
2.5.0,"early_stopping_patience=5,"
2.5.0,save_best_model_to_prefix=None):
2.5.0,if y.dtype != bool:
2.5.0,"assert set(np.unique(y)) == {0, 1}"
2.5.0,y = y.astype(bool)
2.5.0,multitask = y.shape[1] > 1
2.5.0,if not multitask:
2.5.0,num_positives = y.sum()
2.5.0,num_sequences = len(y)
2.5.0,num_negatives = num_sequences - num_positives
2.5.0,if self.verbose >= 1:
2.5.0,print('Training model (* indicates new best result)...')
2.5.0,"X_valid, y_valid = validation_data"
2.5.0,early_stopping_wait = 0
2.5.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.5.0,"for epoch in range(1, self.num_epochs + 1):"
2.5.0,self.model.fit(
2.5.0,"X,"
2.5.0,"y,"
2.5.0,"batch_size=128,"
2.5.0,"nb_epoch=1,"
2.5.0,class_weight={
2.5.0,"True: num_sequences / num_positives,"
2.5.0,False: num_sequences / num_negatives
2.5.0,"} if not multitask else None,"
2.5.0,verbose=self.verbose >= 2)
2.5.0,"epoch_train_metrics = self.test(X, y)"
2.5.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.5.0,self.train_metrics.append(epoch_train_metrics)
2.5.0,self.valid_metrics.append(epoch_valid_metrics)
2.5.0,if self.verbose >= 1:
2.5.0,print('Epoch {}:'.format(epoch))
2.5.0,print('Train {}'.format(epoch_train_metrics))
2.5.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.5.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.5.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.5.0,if self.verbose >= 1:
2.5.0,print(' *')
2.5.0,best_metric = current_metric
2.5.0,best_epoch = epoch
2.5.0,early_stopping_wait = 0
2.5.0,if save_best_model_to_prefix is not None:
2.5.0,self.save(save_best_model_to_prefix)
2.5.0,else:
2.5.0,if self.verbose >= 1:
2.5.0,print()
2.5.0,if early_stopping_wait >= early_stopping_patience:
2.5.0,break
2.5.0,early_stopping_wait += 1
2.5.0,if self.verbose >= 1:
2.5.0,print('Finished training after {} epochs.'.format(epoch))
2.5.0,if save_best_model_to_prefix is not None:
2.5.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.5.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.5.0,"best_epoch, save_best_model_to_prefix))"
2.5.0,
2.5.0,"def predict(self, X):"
2.5.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.5.0,
2.5.0,def get_sequence_filters(self):
2.5.0,""""""""
2.5.0,Returns 3D array of 2D sequence filters.
2.5.0,""""""""
2.5.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.5.0,
2.5.0,"def deeplift(self, X, batch_size=200):"
2.5.0,""""""""
2.5.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.5.0,""""""""
2.5.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.5.0,from deeplift.conversion import keras_conversion as kc
2.5.0,
2.5.0,# convert to deeplift model and get scoring function
2.5.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.5.0,score_func = deeplift_model.get_target_contribs_func(
2.5.0,find_scores_layer_idx=0)
2.5.0,# use a 40% GC reference
2.5.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.5.0,# get deeplift scores
2.5.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.5.0,for i in range(self.num_tasks):
2.5.0,deeplift_scores[i] = score_func(
2.5.0,"task_idx=i,"
2.5.0,"input_data_list=[X],"
2.5.0,"batch_size=batch_size,"
2.5.0,"progress_update=None,"
2.5.0,input_references_list=input_references)
2.5.0,return deeplift_scores
2.5.0,
2.5.0,"def in_silico_mutagenesis(self, X):"
2.5.0,""""""""
2.5.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.5.0,""""""""
2.5.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.5.0,wild_type_predictions = self.predict(X)
2.5.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.5.0,np.newaxis]
2.5.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.5.0,"zip(X, wild_type_predictions)):"
2.5.0,mutated_sequences = np.repeat(
2.5.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.5.0,# remove wild-type
2.5.0,arange = np.arange(len(mutated_sequences))
2.5.0,horizontal_cycle = np.tile(
2.5.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.5.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.5.0,# add mutant
2.5.0,vertical_repeat = np.repeat(
2.5.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.5.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.5.0,# make mutant predictions
2.5.0,mutated_predictions = self.predict(mutated_sequences)
2.5.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.5.0,"(self.num_tasks,))"
2.5.0,mutagenesis_scores[
2.5.0,sequence_index] = wild_type_prediction - mutated_predictions
2.5.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.5.0,
2.5.0,@staticmethod
2.5.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.5.0,from dragonn.plot import plot_bases_on_ax
2.5.0,scores = score_func(X).squeeze(
2.5.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.5.0,try:
2.5.0,os.makedirs(output_directory)
2.5.0,except OSError:
2.5.0,pass
2.5.0,num_tasks = len(scores)
2.5.0,"for task_index, task_scores in enumerate(scores):"
2.5.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.5.0,# sequence_scores is num_bases x sequence_length
2.5.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.5.0,plt.clf()
2.5.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.5.0,top_axis.plot(
2.5.0,"range(1,"
2.5.0,"len(basewise_max_sequence_scores) + 1),"
2.5.0,basewise_max_sequence_scores)
2.5.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.5.0,peak_position = basewise_max_sequence_scores.argmax()
2.5.0,top_axis.axvspan(
2.5.0,"peak_position - peak_width,"
2.5.0,"peak_position + peak_width,"
2.5.0,"color='grey',"
2.5.0,alpha=0.1)
2.5.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.5.0,peak_position + peak_width].T
2.5.0,# Set non-max letter_heights to zero
2.5.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.5.0,"letter_heights[np.arange(len(letter_heights)),"
2.5.0,peak_sequence_scores.argmax(axis=1)] = \
2.5.0,basewise_max_sequence_scores[peak_position - peak_width :
2.5.0,peak_position + peak_width]
2.5.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.5.0,bottom_axis.set_xticklabels(
2.5.0,tuple(
2.5.0,"map(str,"
2.5.0,"np.arange(peak_position - peak_width,"
2.5.0,peak_position + peak_width + 1))))
2.5.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.5.0,plt.xlabel('Position')
2.5.0,plt.ylabel('Score')
2.5.0,plt.savefig(
2.5.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.5.0,"sequence_index, '_task_{}'.format(task_index)"
2.5.0,if num_tasks > 1 else '')))
2.5.0,plt.close()
2.5.0,
2.5.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.5.0,self._plot_scores(
2.5.0,"X,"
2.5.0,"output_directory,"
2.5.0,"peak_width,"
2.5.0,"score_func=self.deeplift,"
2.5.0,score_name='DeepLift')
2.5.0,
2.5.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.5.0,self._plot_scores(
2.5.0,"X,"
2.5.0,"output_directory,"
2.5.0,"peak_width,"
2.5.0,"score_func=self.in_silico_mutagenesis,"
2.5.0,score_name='ISM')
2.5.0,
2.5.0,"def plot_architecture(self, output_file):"
2.5.0,from dragonn.visualize_util import plot as plot_keras_model
2.5.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.5.0,
2.5.0,"def save(self, save_best_model_to_prefix):"
2.5.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.5.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.5.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.5.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.5.0,
2.5.0,@staticmethod
2.5.0,"def load(arch_fname, weights_fname=None):"
2.5.0,model_json_string = open(arch_fname).read()
2.5.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.5.0,if weights_fname is not None:
2.5.0,sequence_dnn.model.load_weights(weights_fname)
2.5.0,return sequence_dnn
2.5.0,create temporary fasta files
2.5.0,run command
2.5.0,remove fasta files
2.5.0,write test fasta file
2.5.0,test gkmsvm
2.5.0,get classification results
2.5.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.5.0,"Using canonical smiles for glycine, as in original research paper"
2.5.0,Atom features with padding
2.5.0,A_tilda_k computation
2.5.0,Final feed_dict setup
2.5.0,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.5.0,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.5.0,self.num_node_features)
2.5.0,Fit models
2.5.0,Args
2.5.0,2017 DeepCrystal Technologies - Patrick Hop
2.5.0,
2.5.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.5.0,
2.5.0,MIT License - have fun!!
2.5.0,===========================================================
2.5.0,x = F.selu( fc(x) )
2.5.0,x = F.selu( fc(x) )
2.5.0,2017 DeepCrystal Technologies - Patrick Hop
2.5.0,
2.5.0,Data loading a splitting file
2.5.0,
2.5.0,MIT License - have fun!!
2.5.0,===========================================================
2.5.0,Args
2.5.0,TODO (VIGS25): Account for the reload option
2.5.0,Downloading train files
2.5.0,Parsing training data
2.5.0,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.5.0,Test Files loading
2.5.0,One Hot Featurization
2.5.0,Consistency check
2.5.0,Handle output layer
2.5.0,Iterate over all previous tasks.
2.5.0,prev_layers is a list with elements of size
2.5.0,"(batch_size, layer_sizes[i-1])"
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Save an initial checkpoint.
2.5.0,Turns out there are valid cases where we don't want pad-batches
2.5.0,on by default.
2.5.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.5.0,Run training op.
2.5.0,Always save a final checkpoint when complete.
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Note that we divide by the batch size and not the number of
2.5.0,"non-zero weight examples in the batch.  Also, instead of using"
2.5.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.5.0,calculate with div/sum so it stays on the GPU.
2.5.0,aggregated costs
2.5.0,weight decay
2.5.0,Dummy placeholders
2.5.0,Dummy placeholders
2.5.0,run eval data through the model
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,Handle edge case when batch-size is 1.
2.5.0,Prune away any padding that was added
2.5.0,allow_soft_placement=True allows ops without a GPU implementation
2.5.0,to run on the CPU instead.
2.5.0,!/usr/bin/python
2.5.0,
2.5.0,Copyright 2015 Google Inc.
2.5.0,
2.5.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.5.0,you may not use this file except in compliance with the License.
2.5.0,You may obtain a copy of the License at
2.5.0,
2.5.0,http://www.apache.org/licenses/LICENSE-2.0
2.5.0,
2.5.0,"Unless required by applicable law or agreed to in writing, software"
2.5.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.5.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.5.0,See the License for the specific language governing permissions and
2.5.0,limitations under the License.
2.5.0,parse CheckpointState proto
2.5.0,parse path to actual checkpoint
2.5.0,the provided mask has to be the same shape as features
2.5.0,test k = 1..4
2.5.0,central moments
2.5.0,standardized moments
2.5.0,central across one axis
2.5.0,standardized across one axis
2.5.0,Fit just on task zero
2.5.0,Notice that we keep the session open
2.5.0,Fit on task one
2.5.0,The predictions for task zero should not change after training
2.5.0,on task one.
2.5.0,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.5.0,!/usr/bin/python
2.5.0,
2.5.0,Copyright 2015 Google Inc.
2.5.0,
2.5.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.5.0,you may not use this file except in compliance with the License.
2.5.0,You may obtain a copy of the License at
2.5.0,
2.5.0,http://www.apache.org/licenses/LICENSE-2.0
2.5.0,
2.5.0,"Unless required by applicable law or agreed to in writing, software"
2.5.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.5.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.5.0,See the License for the specific language governing permissions and
2.5.0,limitations under the License.
2.5.0,get the divisor
2.5.0,compute the requested central moment
2.5.0,"note that mean is a raw moment, not a central moment"
2.5.0,TODO(user): median is not implemented yet in TensorFlow
2.5.0,Add the input features.
2.5.0,"layer has shape [None, layer_sizes[i]]"
2.5.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.5.0,TODO(rbharath): Might want to make it feasible to have multiple
2.5.0,bypass layers.
2.5.0,Construct task bypass layer
2.5.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.5.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.5.0,"layer has shape [None, layer_sizes[i]]"
2.5.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.5.0,TODO(rbharath): Might want to make it feasible to have multiple
2.5.0,bypass layers.
2.5.0,Construct task bypass layer
2.5.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.5.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.5.0,Consistency check
2.5.0,Lazily created by _get_shared_session().
2.5.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.5.0,when subclass-overridden methods use the same scopes.
2.5.0,Setup graph
2.5.0,Create placeholders
2.5.0,Handle output layer
2.5.0,Iterate over all previous tasks.
2.5.0,prev_layers is a list with elements of size
2.5.0,"(batch_size, layer_sizes[i-1])"
2.5.0,Note that we divide by the batch size and not the number of
2.5.0,"non-zero weight examples in the batch.  Also, instead of using"
2.5.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.5.0,calculate with div/sum so it stays on the GPU.
2.5.0,aggregated costs
2.5.0,weight decay
2.5.0,Dummy placeholders
2.5.0,Dummy placeholders
2.5.0,run eval data through the model
2.5.0,"Shape (n_tasks, n__samples)"
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,Handle edge case when batch-size is 1.
2.5.0,with self._get_shared_session(train=True) as sess:
2.5.0,Save an initial checkpoint.
2.5.0,Always save a final checkpoint when complete.
2.5.0,Note that we divide by the batch size and not the number of
2.5.0,"non-zero weight examples in the batch.  Also, instead of using"
2.5.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.5.0,calculate with div/sum so it stays on the GPU.
2.5.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.5.0,Dummy placeholders
2.5.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.5.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.5.0,Dummy placeholders
2.5.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.5.0,allow_soft_placement=True allows ops without a GPU implementation
2.5.0,to run on the CPU instead.
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Turns out there are valid cases where we don't want pad-batches
2.5.0,on by default.
2.5.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.5.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.5.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,"(n_samples, n_classes)"
2.5.0,"(n_samples, n_tasks, n_classes)"
2.5.0,Save hyperparameters
2.5.0,Guard variable to make sure we don't Restore() this model
2.5.0,from a disk checkpoint more than once.
2.5.0,"Path to save checkpoint files, which matches the"
2.5.0,replicated supervisor's default path.
2.5.0,Lazily created by _get_shared_session().
2.5.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.5.0,when subclass-overridden methods use the same scopes.
2.5.0,Setup graph
2.5.0,Note that we divide by the batch size and not the number of
2.5.0,"non-zero weight examples in the batch.  Also, instead of using"
2.5.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.5.0,calculate with div/sum so it stays on the GPU.
2.5.0,aggregated costs
2.5.0,weight decay
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Save an initial checkpoint.
2.5.0,Define the code that runs on a separate thread to feed data into the queue.
2.5.0,Main training loop.
2.5.0,Run training op.
2.5.0,We have reached the end of an epoch.
2.5.0,We have reached the end of the data.
2.5.0,Always save a final checkpoint when complete.
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,allow_soft_placement=True allows ops without a GPU implementation
2.5.0,to run on the CPU instead.
2.5.0,gpu memory growth option
2.5.0,gpu memory growth option
2.5.0,TODO(rbharath): Is setting train=False right here?
2.5.0,Discard any padded predictions
2.5.0,The iterbatches does padding with zero-weight examples on the last batch.
2.5.0,Remove padded examples.
2.5.0,Special case to handle singletasks.
2.5.0,The iterbatches does padding with zero-weight examples on the last batch.
2.5.0,Remove padded examples.
2.5.0,TODO(rbharath): Verify this can be safely removed.
2.5.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.5.0,""""""""
2.5.0,Evaluates the performance of this model on specified dataset.
2.5.0,
2.5.0,Parameters
2.5.0,----------
2.5.0,dataset: dc.data.Dataset
2.5.0,Dataset object.
2.5.0,metric: deepchem.metrics.Metric
2.5.0,Evaluation metric
2.5.0,transformers: list
2.5.0,List of deepchem.transformers.Transformer
2.5.0,Returns
2.5.0,-------
2.5.0,dict
2.5.0,Maps tasks to scores under metric.
2.5.0,""""""""
2.5.0,"evaluator = Evaluator(self, dataset, transformers)"
2.5.0,scores = evaluator.compute_model_performance(metrics)
2.5.0,return scores
2.5.0,checkpoints look like model_dir/model.ckpt-N
2.5.0,"self._save_path is ""model_dir/model.ckpt"""
2.5.0,run eval data through the model
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,run eval data through the model
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,Note that softmax is already applied in construct_grpah
2.5.0,run eval data through the model
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,Handle edge case when batch-size is 1.
2.5.0,Prune away any padding that was added
2.5.0,Handle case of 0-dimensional scalar output
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,inputs placeholder
2.5.0,data preprocessing and augmentation
2.5.0,first conv layer
2.5.0,downsample by max pooling
2.5.0,each module is a residual convolutional block
2.5.0,followed by a convolutional downsample layer
2.5.0,max pooling over the final outcome
2.5.0,fully connected layers
2.5.0,dropout for dense layers
2.5.0,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.5.0,weight decay regularizer
2.5.0,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.5.0,sample cut ratio from a clipped gaussian
2.5.0,train/valid differences
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,Define and build model
2.5.0,model.restore()
2.5.0,Set random seeds
2.5.0,Setup directories
2.5.0,Model constants
2.5.0,Load and transform datasets
2.5.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.5.0,Atomic convolution variables
2.5.0,at = atomic numbers (atom types)
2.5.0,"radial basis function parameters [cutoff, mean, width]"
2.5.0,Model hyperparameters
2.5.0,Initialize model
2.5.0,Fit model
2.5.0,Evaluate model
2.5.0,Set random seeds
2.5.0,Setup directories
2.5.0,Model constants
2.5.0,Load and transform datasets
2.5.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.5.0,Atomic convolution variables
2.5.0,at = atomic numbers (atom types)
2.5.0,"radial basis function parameters [cutoff, mean, width]"
2.5.0,Model hyperparameters
2.5.0,Initialize model
2.5.0,Fit model
2.5.0,Evaluate model
2.5.0,Set random seeds
2.5.0,Setup directories
2.5.0,Model constants
2.5.0,Load and transform datasets
2.5.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.5.0,Atomic convolution variables
2.5.0,at = atomic numbers (atom types)
2.5.0,"radial basis function parameters [cutoff, mean, width]"
2.5.0,Model hyperparameters
2.5.0,Initialize model
2.5.0,Fit model
2.5.0,Evaluate model
2.5.0,Set random seeds
2.5.0,Setup directories
2.5.0,Model constants
2.5.0,Load and transform datasets
2.5.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.5.0,Atomic convolution variables
2.5.0,at = atomic numbers (atom types)
2.5.0,"radial basis function parameters [cutoff, mean, width]"
2.5.0,Model hyperparameters
2.5.0,Initialize model
2.5.0,Fit model
2.5.0,Evaluate model
2.5.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.5.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.5.0,test_scores = test_evaluator.compute_model_performance(metric)
2.5.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.5.0,param.update(test_scores)
2.5.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.5.0,for transformer in transformers:
2.5.0,train_dataset = transformer.transform(train_dataset)
2.5.0,test_dataset = transformer.transform(test_dataset)
2.5.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.5.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.5.0,for transformer in transformers:
2.5.0,train_dataset = transformer.transform(train_dataset)
2.5.0,test_dataset = transformer.transform(test_dataset)
2.5.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.5.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.5.0,for transformer in transformers:
2.5.0,train_dataset = transformer.transform(train_dataset)
2.5.0,test_dataset = transformer.transform(test_dataset)
2.5.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.5.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.5.0,for transformer in transformers:
2.5.0,train_dataset = transformer.transform(train_dataset)
2.5.0,test_dataset = transformer.transform(test_dataset)
2.5.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.5.0,Create some directories for analysis
2.5.0,The base_dir holds the results of all analysis
2.5.0,Make directories to store the raw and featurized datasets.
2.5.0,Load PDBBind dataset
2.5.0,Define featurizers
2.5.0,Currently featurizes with shard_size=1
2.5.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.5.0,This could be done with openbabel in python
2.5.0,Compute cells for this molecule. O(constant)
2.5.0,min == max if molecule is planar in some direction
2.5.0,we should still create a bin
2.5.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.5.0,Note neighbors contains self!
2.5.0,Associate each atom with cell it belongs to. O(N)
2.5.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.5.0,"conditions, so does wrapround. O(constant)"
2.5.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.5.0,Accept as neighbors only those within threshold. This computation should be
2.5.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.5.0,Sort neighbors by distance
2.5.0,Pick up to max_num_neighbors
2.5.0,Type of data created by this featurizer
2.5.0,assumes that every array is of the same dimension
2.5.0,rem_dataset is remaining portion of dataset
2.5.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.5.0,to k-1.
2.5.0,returns list of per column sum of non zero elements
2.5.0,Compute number of actives needed per task.
2.5.0,loop through each column and obtain index required to splice out for
2.5.0,required fraction of hits
2.5.0,Find the first index where the cumulative number of actives equals
2.5.0,the actives_count
2.5.0,Note that np.where tells us last index required to exceed
2.5.0,"actives_count, so we actually want the following location"
2.5.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.5.0,potentially refactor those to match this.
2.5.0,Handle edge case where frac_split is 1
2.5.0,Create weight matrices fpor two haves.
2.5.0,copy over up to required index for weight first_split
2.5.0,check out if any rows in either w_1 or w_2 are just zeros
2.5.0,"Obtain original x, y, and w arrays and shuffle"
2.5.0,calculate percent split for valid (out of test and valid)
2.5.0,"split test data into valid and test, treating sub test set also as sparse"
2.5.0,rem_dataset is remaining portion of dataset
2.5.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.5.0,to k-1.
2.5.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.5.0,This can be generalized in the future with some common demoninator determination.
2.5.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.5.0,Append remaining examples to train
2.5.0,Sort by increasing MW
2.5.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.5.0,for m_idx in cluster:
2.5.0,"continue until we find an active in all the tasks, otherwise we can't"
2.5.0,compute a meaningful AUC
2.5.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.5.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.5.0,Sort from largest to smallest scaffold sets
2.5.0,Sort from largest to smallest scaffold sets
2.5.0,"(n_samples, n_classes)"
2.5.0,"(n_samples, n_tasks, n_classes)"
2.5.0,Save hyperparameters
2.5.0,Guard variable to make sure we don't Restore() this model
2.5.0,from a disk checkpoint more than once.
2.5.0,"Path to save checkpoint files, which matches the"
2.5.0,replicated supervisor's default path.
2.5.0,Lazily created by _get_shared_session().
2.5.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.5.0,when subclass-overridden methods use the same scopes.
2.5.0,Setup graph
2.5.0,Note that we divide by the batch size and not the number of
2.5.0,"non-zero weight examples in the batch.  Also, instead of using"
2.5.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.5.0,calculate with div/sum so it stays on the GPU.
2.5.0,aggregated costs
2.5.0,weight decay
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Save an initial checkpoint.
2.5.0,Turns out there are valid cases where we don't want pad-batches
2.5.0,on by default.
2.5.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.5.0,Run training op.
2.5.0,Always save a final checkpoint when complete.
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,allow_soft_placement=True allows ops without a GPU implementation
2.5.0,to run on the CPU instead.
2.5.0,TODO(rbharath): Is setting train=False right here?
2.5.0,Discard any padded predictions
2.5.0,The iterbatches does padding with zero-weight examples on the last batch.
2.5.0,Remove padded examples.
2.5.0,Special case to handle singletasks.
2.5.0,The iterbatches does padding with zero-weight examples on the last batch.
2.5.0,Remove padded examples.
2.5.0,TODO(rbharath): Verify this can be safely removed.
2.5.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.5.0,""""""""
2.5.0,Evaluates the performance of this model on specified dataset.
2.5.0,
2.5.0,Parameters
2.5.0,----------
2.5.0,dataset: dc.data.Dataset
2.5.0,Dataset object.
2.5.0,metric: deepchem.metrics.Metric
2.5.0,Evaluation metric
2.5.0,transformers: list
2.5.0,List of deepchem.transformers.Transformer
2.5.0,Returns
2.5.0,-------
2.5.0,dict
2.5.0,Maps tasks to scores under metric.
2.5.0,""""""""
2.5.0,"evaluator = Evaluator(self, dataset, transformers)"
2.5.0,scores = evaluator.compute_model_performance(metrics)
2.5.0,return scores
2.5.0,checkpoints look like logdir/model.ckpt-N
2.5.0,"self._save_path is ""logdir/model.ckpt"""
2.5.0,run eval data through the model
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,run eval data through the model
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,Note that softmax is already applied in construct_grpah
2.5.0,run eval data through the model
2.5.0,reshape to batch_size x n_tasks x ...
2.5.0,Handle edge case when batch-size is 1.
2.5.0,Prune away any padding that was added
2.5.0,Handle case of 0-dimensional scalar output
2.5.0,Dummy placeholders
2.5.0,Dummy placeholders
2.5.0,## AtomicNet fully-connected layer ops ###
2.5.0,## Atomicnet coordinate transform ops ###
2.5.0,## Atomicnet symmetry function kernel ops ###
2.5.0,## Atomicnet symmetry function ops ###
2.5.0,## Atomcnet symmetry function layer ops ###
2.5.0,We apply the radial pooling filter before atom type conv
2.5.0,to reduce computation
2.5.0,## Misc convenience ops ###
2.5.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.5.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.5.0,"game).  The average reward for any bet is slightly negative, so the best"
2.5.0,strategy is to walk away.
2.5.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.5.0,Optimize it.
2.5.0,"It should have learned that the expected value is very close to zero, and that the best"
2.5.0,action is to walk away.
2.5.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.5.0,get the same result.
2.5.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.5.0,Run the algorithm.
2.5.0,Save a file checkpoint.
2.5.0,Build the tree.
2.5.0,Compute the final probabilities and expected reward.
2.5.0,Mark this node as terminal
2.5.0,Expand this node.
2.5.0,Select the next action to perform.
2.5.0,Recursively build the tree.
2.5.0,Update statistics for this node.
2.5.0,Configuration file for the Sphinx documentation builder.
2.5.0,
2.5.0,This file only contains a selection of the most common options. For a full
2.5.0,list see the documentation:
2.5.0,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.5.0,-- Path setup --------------------------------------------------------------
2.5.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.5.0,add these directories to sys.path here. If the directory is relative to the
2.5.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.5.0,
2.5.0,-- Project information -----------------------------------------------------
2.5.0,"The full version, including alpha/beta/rc tags"
2.5.0,-- General configuration ---------------------------------------------------
2.5.0,"Add any Sphinx extension module names here, as strings. They can be"
2.5.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.5.0,ones.
2.5.0,Options for autodoc directives
2.5.0,How to represents typehints
2.5.0,"Add any paths that contain templates here, relative to this directory."
2.5.0,The suffix of source filenames.
2.5.0,The master toctree document.
2.5.0,autosectionlabel setting
2.5.0,"List of patterns, relative to source directory, that match files and"
2.5.0,directories to ignore when looking for source files.
2.5.0,This pattern also affects html_static_path and html_extra_path.
2.5.0,"If true, the current module name will be prepended to all description"
2.5.0,unit titles (such as .. function::).
2.5.0,-- Options for HTML output -------------------------------------------------
2.5.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.5.0,a list of builtin themes.
2.5.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.5.0,"relative to this directory. They are copied after the builtin static files,"
2.5.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.5.0,The name of an image file (relative to this directory) to place at the top
2.5.0,of the sidebar.
2.5.0,Customize the sphinx theme
2.5.0,-- Source code links ---------------------------------------------------
2.5.0,Resolve function for the linkcode extension.
2.5.0,"try to find the file and line number, based on code from numpy:"
2.5.0,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.5.0,lines in the label file have format
2.5.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.5.0,"print line[0], line[3]"
2.5.0,"If you push the tag, please remove `.dev`"
2.5.0,Record inputs.
2.5.0,Create the output directory if necessary.
2.5.0,Create the optimizers for meta-optimization and task optimization.
2.5.0,Create a Checkpoint for saving.
2.5.0,Main optimization loop.
2.5.0,Do checkpointing.
2.5.0,flake8: noqa
2.5.0,This is a MetaLearner that learns to generate sine functions with variable
2.5.0,amplitude and phase.
2.5.0,Optimize it.
2.5.0,Test it out on some new tasks and see how it works.
2.5.0,Initially the model should do a bad job of fitting the sine function.
2.5.0,After one step of optimization it should do much better.
2.5.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.5.0,get the same result.
2.5.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.5.0,We know use_pose_generator_scores == False in this case
2.5.0,check whether self.featurizer is instance of ComplexFeaturizer or not
2.5.0,TODO: How to handle the failure here?
2.5.0,TODO(rbharath): The autodock vina source computes surface distances
2.5.0,which take into account the van der Waals radius of each atom type.
2.5.0,"Shape (N, M)"
2.5.0,"Shape (N, M)"
2.5.0,Parse complex
2.5.0,check filetypes
2.5.0,Define locations of log and output files
2.5.0,Write GNINA conf file
2.5.0,Run GNINA
2.5.0,read output and log
2.5.0,Parse complex
2.5.0,Prepare protein
2.5.0,Get protein centroid and range
2.5.0,TODO(rbharath: Does vina divide box dimensions by 2?
2.5.0,Prepare ligand
2.5.0,Write Vina conf file
2.5.0,Define locations of log and output files
2.5.0,"I'm not sure why specifying the args as a list fails on other platforms,"
2.5.0,but for some reason it only works if I pass it as a string.
2.5.0,FIXME: Incompatible types in assignment
2.5.0,FIXME: We should use `subprocess.run` instead of `call`
2.5.0,flake8: noqa
2.5.0,We provide no scoring model so the docker won't score
2.5.0,Check only one output since num_modes==1
2.5.0,We provide no scoring model so the docker won't score
2.5.0,Check only one output since num_modes==1
2.5.0,Let's turn on logging since this test will run for a while
2.5.0,Check returned files exist
2.5.0,Let's turn on logging since this test will run for a while
2.5.0,Check returned files exist
2.5.0,"Where d is greater than zero, the repulsion is just zeros"
2.5.0,"When d is 0, this should just be 1"
2.5.0,"When d == 0, the hbond interaction is 0"
2.5.0,The exponential returns 1 when input 0.
2.5.0,This exponential returns 1 when input 3
2.5.0,Let's turn on logging since this test will run for a while
2.5.0,Let's turn on logging since this test will run for a while
2.5.0,Let's turn on logging since this test will run for a while
2.5.0,Let's turn on logging since this test will run for a while
2.5.0,Let's turn on logging since this test will run for a while
2.5.0,Note this may download autodock Vina...
2.5.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.5.0,Test that every atom in pocket maps exists
2.5.0,scalar case
2.5.0,per-example case
2.5.0,This is a little arcane but it repeats w across tasks.
2.5.0,"If w.shape == (n_samples, 1) handle it as 1D"
2.5.0,"w.shape == (n_samples, n_tasks)"
2.5.0,scalar case
2.5.0,Handle n_classes/n_task shape ambiguity
2.5.0,Add in task dimension
2.5.0,Insert a task dimension (we know n_tasks=1 from above0
2.5.0,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.5.0,Handle classification. We need to convert labels into one-hot representation.
2.5.0,check whether n_classes is int or not
2.5.0,Handle n_classes/n_task shape ambiguity
2.5.0,Add in task dimension
2.5.0,Make everything 2D so easy to handle
2.5.0,Handle each task separately.
2.5.0,Handle continuous class probabilites of positive class for binary
2.5.0,Fill in class 0 probabilities
2.5.0,Add a task dimension to concatenate on
2.5.0,Handle binary labels
2.5.0,"make y_hot of shape (N, n_classes)"
2.5.0,Add a task dimension to concatenate on
2.5.0,Insert a task dimension
2.5.0,"Now of shape (N,)"
2.5.0,"Now of shape (N, 1)"
2.5.0,"Returns shape (N, n_tasks)"
2.5.0,"Now of shape (N,)"
2.5.0,"Now of shape (N, n_classes)"
2.5.0,"Now of shape (N, 1, n_classes)"
2.5.0,"Returns shape (N, n_tasks, n_classes)"
2.5.0,These are some smart defaults
2.5.0,These are some smart defaults corresponding to sklearn's required
2.5.0,behavior
2.5.0,Attempt some limited shape imputation to find n_tasks
2.5.0,check whether n_tasks is int or not
2.5.0,This is because `normalize_weight_shape` require int value.
2.5.0,FIXME: Incompatible types in assignment
2.5.0,Attempt to convert both into the same type
2.5.0,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.5.0,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.5.0,initialize fwd and reverse scores to -infinity
2.5.0,"cross-correlate separately for each base,"
2.5.0,for both the PSSM and its reverse complement
2.5.0,sum over the bases
2.5.0,take max of fwd and reverse scores at each position
2.5.0,"Shape (N_sequences, num_tasks)"
2.5.0,check whether wild_type_predictions is np.ndarray or not
2.5.0,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.5.0,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.5.0,Mutates every position of the sequence to every letter
2.5.0,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.5.0,Breakdown:
2.5.0,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.5.0,remove wild-type
2.5.0,len(arange) = N_letters * sequence_length
2.5.0,len(horizontal cycle) = N_letters * sequence_length
2.5.0,add mutant
2.5.0,make mutant predictions
2.5.0,check whether wild_type_predictions is np.ndarray or not
2.5.0,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.5.0,validation
2.5.0,flake8: noqa
2.5.0,metric class
2.5.0,metrics utils
2.5.0,sklearn & scipy score function
2.5.0,original score function
2.5.0,Get a random prediction matrix
2.5.0,"Of shape (N, n_classes)"
2.5.0,"Of shape (N, 1, n_classes)"
2.5.0,This has w for each task.
2.5.0,Best score case
2.5.0,Worst score case
2.5.0,best case
2.5.0,duplicate prediction value
2.5.0,Encode motif
2.5.0,"sequences now has shape (3, 4, 5, 1)"
2.5.0,"sequences now has shape (3, 4, 5, 1)"
2.5.0,Construct and train SequenceDNN model
2.5.0,Call in-silico mutagenesis
2.5.0,Construct and train SequenceDNN model
2.5.0,Call in-silico mutagenesis
2.5.0,Check nonzero elements exist
2.5.0,Special case handling of single input
2.5.0,Featurize task results iff they exist.
2.5.0,Filter out examples where featurization failed.
2.5.0,"For prospective data where results are unknown, it"
2.5.0,makes no sense to have y values or weights.
2.5.0,Featurize task results if they exist.
2.5.0,Filter out examples where featurization failed.
2.5.0,"For prospective data where results are unknown, it"
2.5.0,makes no sense to have y values or weights.
2.5.0,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.5.0,The field in which load_sdf_files return value stores smiles
2.5.0,"(X, y, w, ids)"
2.5.0,Sometimes zip files contain directories within. Traverse directories
2.5.0,TODO(rbharath): Add support for more extensions
2.5.0,Sort image files
2.5.0,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.5.0,Remove support indices
2.5.0,Remove support indices
2.5.0,Remove support indices
2.5.0,Get task specific entries
2.5.0,Now just get weights for this task
2.5.0,Get task specific entries
2.5.0,Now just get weights for this task
2.5.0,Now just get weights for this task
2.5.0,Now just get weights for this task
2.5.0,Split data into pos and neg lists.
2.5.0,No replacement allowed for supports
2.5.0,Handle one-d vs. non one-d feature matrices
2.5.0,Init the iterator
2.5.0,Set initial iterator state
2.5.0,support = self.supports[task][self.trial_num]
2.5.0,Increment and update logic
2.5.0,Init the iterator
2.5.0,Set initial iterator state
2.5.0,support = self.supports[task][self.trial_num]
2.5.0,Increment and update logic
2.5.0,Ensure that every worker will pick the same random order for each epoch.
2.5.0,Ensure that every worker will pick the same random order for each epoch.
2.5.0,"By invariant of when this is called, can assume num_samples > 0"
2.5.0,and num_samples < batch_size
2.5.0,Fill in batch arrays
2.5.0,"By invariant of when this is called, can assume num_samples > 0"
2.5.0,and num_samples < batch_size
2.5.0,Fill in batch arrays
2.5.0,Only the first set of copy will be counted in training loss
2.5.0,Retrieve the first sample so we can determine the dtypes.
2.5.0,Create a Tensorflow Dataset.
2.5.0,Find the X values.
2.5.0,Find the y values.
2.5.0,Find the w values.
2.5.0,Find the ids.
2.5.0,"Set labels to be zero, with zero weights"
2.5.0,Load obsolete format -> save in new format
2.5.0,note that this corresponds to the _construct_metadata column order
2.5.0,Create temp directory to store resharded version
2.5.0,Get correct shapes for y/w
2.5.0,Write data in new shards
2.5.0,Handle shapes
2.5.0,Note that this means that DiskDataset resharding currently doesn't
2.5.0,work for datasets that aren't regression/classification.
2.5.0,Handle spillover from last shard
2.5.0,Should have updated to non-legacy metadata
2.5.0,Note that this resets the cache internally
2.5.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.5.0,"than process based pools, since process based pools need to pickle/serialize"
2.5.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.5.0,we're actually protected by the GIL.
2.5.0,mp.dummy aliases ThreadPool to Pool
2.5.0,(ytz): this skips everything except possibly the last shard
2.5.0,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.5.0,make a NumpyDataset under the hood
2.5.0,"raw_data = (X, y, w, ids)"
2.5.0,Protect against generator exhaustion
2.5.0,This ensures tasks are consistent for all datasets
2.5.0,Get full dataset in memory
2.5.0,Shuffle in memory
2.5.0,Write shuffled shards out to disk
2.5.0,Shuffle the arrays corresponding to each row in metadata_df
2.5.0,Reset cache
2.5.0,See if we have a cached copy of this shard.
2.5.0,"We don't, so load it from disk."
2.5.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.5.0,Try to cache this shard for later use.  Since the normal usage pattern is
2.5.0,"a series of passes through the whole dataset, there's no point doing"
2.5.0,anything fancy.  It never makes sense to evict another shard from the
2.5.0,"cache to make room for this one, because we'll probably want that other"
2.5.0,shard again before the next time we want this one.  So just cache as many
2.5.0,as we can and then stop.
2.5.0,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.5.0,Handle edge case with empty indices
2.5.0,We use two loops here. The outer while loop walks over selection shards
2.5.0,(the chunks of the indices to select that should go into separate
2.5.0,"output shards), while the inner for loop walks over the shards in the"
2.5.0,source datasets to select out the shard indices from that  source shard
2.5.0,Find indices which rest in this shard
2.5.0,Need to offset indices to fit within shard_size
2.5.0,Handle empty case where no data from this shard needed
2.5.0,Handle the case of datasets with y/w missing
2.5.0,Break if all indices have been used up already
2.5.0,Note these will be in the sorted order
2.5.0,We need to recover the original ordering. We can do this by using
2.5.0,np.where to find the locatios of the original indices in the sorted
2.5.0,indices.
2.5.0,We know there's only one match for np.where since this is a
2.5.0,"permutation, so the [0][0] pulls out the exact match location."
2.5.0,If shape metadata is available use it to directly compute shape from
2.5.0,metadata
2.5.0,"In absense of shape metadata, fall back to loading data from disk to"
2.5.0,find shape.
2.5.0,Case n_samples should be 1
2.5.0,flake8: noqa
2.5.0,TODO(rbharath): Get rid of * import
2.5.0,Load MUV dataset
2.5.0,Do an approximate comparison since splits are sometimes slightly off from
2.5.0,the exact fraction.
2.5.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.5.0,reloading will cause the transform to be reapplied. This is undesirable in
2.5.0,almost all cases. Need to understand a method to fix this.
2.5.0,The shuffling should have switched up the ordering
2.5.0,But all the same entries should still be present
2.5.0,All the data should have same shape
2.5.0,The shuffling should have switched up the ordering
2.5.0,But all the same entries should still be present
2.5.0,All the data should have same shape
2.5.0,The ids should now store the performed permutation. Check that the
2.5.0,original dataset is recoverable.
2.5.0,The ids should now store the performed permutation. Check that the
2.5.0,original dataset is recoverable.
2.5.0,Generate data
2.5.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.5.0,around for testing resharding.
2.5.0,Set cache to 0 size to avoid cache hiding errors
2.5.0,Generate data
2.5.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.5.0,around for testing resharding.
2.5.0,Set cache to 0 size to avoid cache hiding errors
2.5.0,Featurize emols dataset
2.5.0,example.fasta contains 3 sequences each of length 58.
2.5.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.5.0,"There is one ""image channel""."
2.5.0,Generate dummy dataset
2.5.0,Generate dummy dataset
2.5.0,Generate dummy dataset
2.5.0,Set last n_samples/2 weights to 0
2.5.0,Check that no support elements are sample from zero-weight samples
2.5.0,Generate dummy dataset
2.5.0,Generate dummy dataset
2.5.0,Create support generator
2.5.0,Generate dummy dataset
2.5.0,Create support generator
2.5.0,Generate dummy dataset
2.5.0,Assert all support elements have been removed
2.5.0,Generate dummy dataset
2.5.0,Assert all remove elements have been removed
2.5.0,Generate dummy dataset
2.5.0,Assert all support elements have been removed
2.5.0,Generate dummy dataset
2.5.0,Assert all remove elements have been removed
2.5.0,Generate dummy dataset
2.5.0,Set last n_samples/2 weights to 0
2.5.0,Sample from first n_samples/2 elements for support
2.5.0,Should lie within first n_samples/2 samples only
2.5.0,Generate dummy dataset
2.5.0,Create support generator
2.5.0,Generate dummy dataset
2.5.0,This is necessary since from_numpy adds in shape information
2.5.0,This is necessary since from_numpy adds in shape information
2.5.0,This is necessary since from_numpy adds in shape information
2.5.0,Generate data
2.5.0,Generate data
2.5.0,Generate data
2.5.0,Should now have 10 shards
2.5.0,This is the shape of legacy_data
2.5.0,legacy_dataset is a dataset in the legacy format kept around for testing
2.5.0,purposes.
2.5.0,This is the shape of legacy_data_reshard
2.5.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.5.0,around for testing
2.5.0,Should now have 10 shards
2.5.0,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.5.0,Test constructor reload works for legacy format
2.5.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.5.0,around for testing resharding.
2.5.0,Reshard copy
2.5.0,Check metadata has been updated
2.5.0,First try using images for X.
2.5.0,Now try using images for y.
2.5.0,Transform it
2.5.0,Test on identity matrix
2.5.0,Generate random sparse features dataset
2.5.0,Test edge case with array of all zeros
2.5.0,Test cases where n_samples < 2*n_samples < batch_size
2.5.0,Test cases where n_samples < batch_size
2.5.0,Test case where n_samples == batch_size
2.5.0,Test case for object featurization.
2.5.0,Test case for more complicated object featurization
2.5.0,Test case with multidimensional data
2.5.0,Test cases where n_samples < 2*n_samples < batch_size
2.5.0,Test cases where n_samples < batch_size
2.5.0,Test case where n_samples == batch_size
2.5.0,Test case for object featurization.
2.5.0,Test case for more complicated object featurization
2.5.0,Test case with multidimensional data
2.5.0,Test first resharding worked
2.5.0,Test second resharding worked
2.5.0,approx 1/15! chance of equality
2.5.0,Generate data
2.5.0,Generate data
2.5.0,Transform it
2.5.0,special case to test
2.5.0,deterministic
2.5.0,non-deterministic
2.5.0,we don't know the order in which the shards are iterated in.
2.5.0,Check that we have all the data in
2.5.0,Test iterating in order.
2.5.0,Test iterating out of order.
2.5.0,Test iterating in batches.
2.5.0,Test iterating with multiple workers.
2.5.0,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.5.0,Try specifying particular columns.
2.5.0,Test id shrinkage
2.5.0,Test task shrinkage
2.5.0,Test max print size
2.5.0,Create image file
2.5.0,Create zip of image file
2.5.0,Create zip of multiple image files
2.5.0,"Create zip of multiple image files, multiple_types"
2.5.0,Create image directory
2.5.0,These are the known dimensions of face.png
2.5.0,These are the known dimensions of face.png
2.5.0,TODO(rbharath): Where are the color channels?
2.5.0,"Since the different files have different shapes, makes an object array"
2.5.0,Splits featurized samples into train/test
2.5.0,Splits featurized samples into train/test
2.5.0,Splits featurized samples into train/test
2.5.0,Splits featurized samples into train/test
2.5.0,Now perform move
2.5.0,Only for debug!
2.5.0,Make directories to store the raw and featurized datasets.
2.5.0,Load dataset
2.5.0,Featurize tox21 dataset
2.5.0,featurization
2.5.0,train/valid split.
2.5.0,singletask load
2.5.0,comparison
2.5.0,Only for debug!
2.5.0,Make directories to store the raw and featurized datasets.
2.5.0,Load dataset
2.5.0,Featurize tox21 dataset
2.5.0,For debugging purposes
2.5.0,multitask load
2.5.0,Do train/valid split.
2.5.0,singletask load
2.5.0,comparison
2.5.0,Get the labels/weights
2.5.0,Normalize shapes
2.5.0,Remove labels with zero weights
2.5.0,Note that we may have 0 elements of a given class since we remove those
2.5.0,labels with zero weight.
2.5.0,this works because y is 1D
2.5.0,This is the right ratio since int(N/num_c) * num_c \approx N
2.5.0,for all classes
2.5.0,Flattening is safe because of shape check above
2.5.0,Hack to allow for easy unpickling:
2.5.0,http://stefaanlippens.net/pickleproblem
2.5.0,Some transformation must happen
2.5.0,Add this case in to handle non-DiskDataset that should be written to disk
2.5.0,Note that transformers have to be undone in reversed order
2.5.0,Handle division by zero
2.5.0,Handle division by zero
2.5.0,Control for pathological case with no variance.
2.5.0,Handle case with 1 task correctly
2.5.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.5.0,Find the task dimension of z
2.5.0,Prevent broadcasting on wrong dimension
2.5.0,BalancingTransformer can only transform weights.
2.5.0,Compute weighting factors from dataset.
2.5.0,Handle 1-D case
2.5.0,Remove labels with zero weights
2.5.0,Note that we may have 0 elements of a given class since we remove those
2.5.0,labels with zero weight. This typically happens in multitask datasets
2.5.0,where some datapoints only have labels for some tasks.
2.5.0,this works because task_y is 1D
2.5.0,This is the right ratio since N_task/num_c * num_c = N_task
2.5.0,for all classes
2.5.0,Set to the class weight computed previously
2.5.0,Need this for transform_y
2.5.0,Handle 1D case
2.5.0,THis reshape is safe because of guard above.
2.5.0,map the indices to labels
2.5.0,generating batch of data by slicing similarity matrix
2.5.0,into 100*reference_dataset_length
2.5.0,concatenate batches of data together
2.5.0,highest similarity is 1: target is in the reference
2.5.0,use the following K points
2.5.0,"highest less than 1: target not in the reference, use top K points"
2.5.0,calculate matrix multiplicatin on slices
2.5.0,concatenate the slices together
2.5.0,list of calculation orders for DAGs
2.5.0,stemming from one specific atom in the molecule
2.5.0,starting from the adjacency list derived by graphconv featurizer
2.5.0,"number of atoms, also number of DAGs"
2.5.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.5.0,each step calculating graph features for one atom.
2.5.0,`max_atoms` is the maximum number of steps
2.5.0,each iteration generates the DAG starting from atom with index `count`
2.5.0,"list of lists, elements represent the calculation orders"
2.5.0,for atoms in the current graph
2.5.0,starting from the target atom with index `count`
2.5.0,flags of whether the atom is already included in the DAG
2.5.0,atom `count` is in the DAG
2.5.0,recording number of radial propagation steps
2.5.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.5.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.5.0,will be added in the second loop(radial=1).
2.5.0,atoms i-bond away will be added in i-th loop
2.5.0,"when molecules have separate parts, starting from one part,"
2.5.0,it is not possible to include all atoms.
2.5.0,this break quit the loop when going into such condition
2.5.0,reinitialize targets for next iteration
2.5.0,atoms connected to current_atom
2.5.0,generate the dependency map of current DAG
2.5.0,atoms connected to `current_atoms`(and not included in the DAG)
2.5.0,"are added, and will be the `current_atoms` for next iteration."
2.5.0,"DAG starts from the target atom, calculation should go in reverse"
2.5.0,`edge[1]` is the parent of `edge[0]`
2.5.0,"after this loop, `parents[i]` includes all parents of atom i"
2.5.0,manually adding the atom index into its parents list
2.5.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.5.0,atoms with less parents(farther from the target atom) come first.
2.5.0,"graph features of atoms without parents will be first calculated,"
2.5.0,then atoms with more parents can be calculated in order
2.5.0,based on previously calculated graph features.
2.5.0,target atom of this DAG will be calculated in the last step
2.5.0,padding with `max_atoms`
2.5.0,padding
2.5.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.5.0,which is a max_atoms * max_atoms numpy array after padding
2.5.0,Calculate pairwise distance
2.5.0,Masking for valid atom index
2.5.0,Cutoff with threshold Rc
2.5.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.5.0,flake8: noqa
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a y transformer
2.5.0,Check w is unchanged since this is a y transformer
2.5.0,Check y is now a logarithmic version of itself
2.5.0,Check that untransform does the right thing.
2.5.0,Check ids are unchanged.
2.5.0,Check y is unchanged since this is a X transformer
2.5.0,Check w is unchanged since this is a y transformer
2.5.0,Check y is now a logarithmic version of itself
2.5.0,Check that untransform does the right thing.
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a y transformer
2.5.0,Check w is unchanged since this is a y transformer
2.5.0,Check y is now a logarithmic version of itself
2.5.0,Check that untransform does the right thing.
2.5.0,Tests logarithmic data transformer with selection.
2.5.0,Check ids are unchanged.
2.5.0,Check y is unchanged since this is a X transformer
2.5.0,Check w is unchanged since this is a y transformer
2.5.0,Check y is now a logarithmic version of itself
2.5.0,Check that untransform does the right thing.
2.5.0,Check ids are unchanged.
2.5.0,Check y is unchanged since this is an X transformer
2.5.0,Check w is unchanged since this is an X transformer
2.5.0,Check X is now holding the proper values when sorted.
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is an y transformer
2.5.0,Check w is unchanged since this is an y transformer
2.5.0,Check y is now holding the proper values when sorted.
2.5.0,Check that untransform does the right thing.
2.5.0,Check ids are unchanged.
2.5.0,Check y is unchanged since this is an X transformer
2.5.0,Check w is unchanged since this is an X transformer
2.5.0,Check X is now holding the proper values when sorted.
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a y transformer
2.5.0,Check w is unchanged since this is a y transformer
2.5.0,Check y is now holding the proper values when sorted.
2.5.0,Check ids are unchanged before and after transformation
2.5.0,Check X is unchanged since transform_y is true
2.5.0,Check w is unchanged since transform_y is true
2.5.0,Check minimum and maximum values of transformed y are 0 and 1
2.5.0,Check untransform works correctly
2.5.0,Check ids are unchanged before and after transformation
2.5.0,Check X is unchanged since transform_y is true
2.5.0,Check w is unchanged since transform_y is true
2.5.0,Check minimum and maximum values of transformed y are 0 and 1
2.5.0,Test if dimensionality expansion is handled correctly by untransform
2.5.0,Check ids are unchanged before and after transformation
2.5.0,Check X is unchanged since transform_y is true
2.5.0,Check w is unchanged since transform_y is true
2.5.0,Check minimum and maximum values of transformed y are 0 and 1
2.5.0,Check untransform works correctly
2.5.0,Load mini log-solubility dataset.
2.5.0,The transformer generates n DAGs for a molecule with n
2.5.0,"atoms. These are denoted the ""parents"""
2.5.0,extract only the images (no need of the labels)
2.5.0,reshaping the vector to image
2.5.0,Check Blurring
2.5.0,Check center crop
2.5.0,Check crop
2.5.0,Check convert2gray
2.5.0,Check rotation
2.5.0,Some more test cases for flip
2.5.0,Check flip
2.5.0,Check Scales
2.5.0,Check shift
2.5.0,check gaussian noise
2.5.0,check salt and pepper noise
2.5.0,Check median filter
2.5.0,transforming y should raise an exception
2.5.0,transforming w should raise an exception
2.5.0,transforming X should be okay
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a y transformer
2.5.0,Check w is unchanged since this is a y transformer
2.5.0,"Check that y_t has zero mean, unit std."
2.5.0,Check that untransform does the right thing.
2.5.0,Check ids are unchanged.
2.5.0,Check y is unchanged since this is a X transformer
2.5.0,Check w is unchanged since this is a y transformer
2.5.0,"Check that X_t has zero mean, unit std."
2.5.0,np.set_printoptions(threshold='nan')
2.5.0,Entries with zero std are not normalized
2.5.0,Check that untransform does the right thing.
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a w transformer
2.5.0,Check y is unchanged since this is a w transformer
2.5.0,Assert that entries with zero weight retain zero weight
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a w transformer
2.5.0,Check y is unchanged since this is a w transformer
2.5.0,Assert that entries with zero weight retain zero weight
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a w transformer
2.5.0,Check y is unchanged since this is a w transformer
2.5.0,Assert that entries with zero weight retain zero weight
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a w transformer
2.5.0,Check y is unchanged since this is a w transformer
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is a w transformer
2.5.0,Check y is unchanged since this is a w transformer
2.5.0,Assert that entries with zero weight retain zero weight
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,Check ids are unchanged.
2.5.0,Check y is unchanged since this is an X transformer
2.5.0,Check w is unchanged since this is an X transformer
2.5.0,Check X is now holding the proper values in each column.
2.5.0,Check ids are unchanged.
2.5.0,Check X is unchanged since this is an X transformer
2.5.0,Check w is unchanged since this is an X transformer
2.5.0,Check y is now holding the proper values in each column.
2.5.0,Check that untransform does the right thing.
2.5.0,Check that we have length 8 now with duplication
2.5.0,Check shapes
2.5.0,Check that we have 4 positives and 4 negatives
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,Note that nothing should change in this dataset since weights balance!
2.5.0,Check that still we have length 6
2.5.0,Check shapes
2.5.0,Check that we have 2 positives and 4 negatives
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,Check that we have length 8 now with duplication
2.5.0,Check shapes
2.5.0,Check that we have 4 positives and 4 negatives
2.5.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.5.0,6-1 imbalance in favor of class 0
2.5.0,Check that we have length 30 now with duplication
2.5.0,Check shapes
2.5.0,Check that we have 6 of each class
2.5.0,Check that sum of all class weights is equal by comparing to 0 weight
2.5.0,Note class imbalance. This will round to 2x duplication for 1
2.5.0,Check that we have length 13 now with duplication
2.5.0,Check shapes
2.5.0,Check that we have 6 positives and 7 negatives
2.5.0,################################################################
2.5.0,save.py is out of date. You should not import any functions from here.
2.5.0,################################################################
2.5.0,flake8: noqa
2.5.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.5.0,Walk through the original file and extract ATOM/HETATM lines and
2.5.0,add PDBQT charge annotations.
2.5.0,Remove rotatable bonds from this molecule
2.5.0,Get the connected components now that the rotatable bonds have
2.5.0,been removed.
2.5.0,The root is the largest connected component.
2.5.0,Write the root component
2.5.0,"We've looked at the root, so take note of that"
2.5.0,Compute partial charges on molecule if RDKit Mol
2.5.0,indices to atoms to keep
2.5.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.5.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.5.0,nonzero contact.
2.5.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.5.0,TODO: This is duplicated! Clean up
2.5.0,Updates charges in place
2.5.0,initial embedding
2.5.0,minimization and pruning
2.5.0,always keep lowest-energy conformer
2.5.0,discard conformers after max_conformers is reached
2.5.0,get RMSD to selected conformers
2.5.0,discard conformers within the RMSD threshold
2.5.0,create a new molecule to hold the chosen conformers
2.5.0,this ensures proper conformer IDs and energy-based ordering
2.5.0,False here specifies that water is to be removed
2.5.0,Updates charges in place
2.5.0,TODO: This is wrong. Should return all molecules
2.5.0,Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.5.0,This updates in place
2.5.0,indices of atoms to keep
2.5.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.5.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.5.0,nonzero contact.
2.5.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.5.0,####################################################
2.5.0,Compute partial charges on molecule if rdkit
2.5.0,####################################################
2.5.0,Number of voxels per one edge of box to voxelize.
2.5.0,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.5.0,If interval1 < interval2 entirely
2.5.0,If interval2 < interval1 entirely
2.5.0,Each triangle in the simplices is a set of 3 atoms from
2.5.0,coordinates which forms the vertices of an exterior triangle on
2.5.0,the convex hull of the macromolecule.
2.5.0,Points is the set of atom coordinates that make up this
2.5.0,triangular face on the convex hull
2.5.0,Let's extract x/y/z coords for this face
2.5.0,Let's compute min/max points
2.5.0,"Nitrogen has atomic number 7, and oxygen 8."
2.5.0,If atom is a hydrogen
2.5.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.5.0,If atom is a hydrogen
2.5.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.5.0,if ring from mol1 is aromatic
2.5.0,...and atom from mol2 is a cation
2.5.0,if angle and distance are correct
2.5.0,count atoms forming a contact
2.5.0,if ring is aromatic
2.5.0,"save its indices, center, and normal"
2.5.0,remember mol1-mol2 pairs we already counted
2.5.0,"if this pair is new, count atoms forming a contact"
2.5.0,"if this pair is new, count atoms forming a contact"
2.5.0,find interacting rings from mol1 and cations from mol2
2.5.0,find interacting cations from mol1 and rings from mol2
2.5.0,merge counters
2.5.0,the line has format
2.5.0,REMARK VINA RESULT: score ...
2.5.0,There is only 1 such line per model so we can append it
2.5.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.5.0,Apply common fixes to PDB files
2.5.0,Optimize ligand
2.5.0,Make sure input is a list
2.5.0,FIXME: Incompatible types in assignment
2.5.0,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.5.0,Ensure that metric is wrapped in a list.
2.5.0,This case checks if input is a function then wraps a
2.5.0,dc.metrics.Metric object around it
2.5.0,Process input metrics
2.5.0,Compute multitask metrics
2.5.0,We use y/w to aggregate labels/weights across generator.
2.5.0,This is a KerasModel.
2.5.0,Some datasets have weights
2.5.0,Process predictions and populate y/w lists
2.5.0,Combine labels/weights
2.5.0,Undo data transformations.
2.5.0,Compute multitask metrics
2.5.0,These functions have moved to deepchem.utils_docking_utils
2.5.0,flake8: noqa
2.5.0,The number of elements to print for dataset ids/tasks
2.5.0,"If a dataset contains more than this number of elements, it won't"
2.5.0,print any dataset ids
2.5.0,An activation function for a Keras layer: either a TensorFlow function or the name of a standard activation
2.5.0,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.5.0,"A single value of some type, or multiple values of that type"
2.5.0,The shape of a NumPy array
2.5.0,"A NumPy array, or an object that can be converted to one.  Once we move to"
2.5.0,"requiring NumPy 1.20, we should replace this with numpy.typing.ArrayLike."
2.5.0,type of RDKit object
2.5.0,type of Pymatgen object
2.5.0,Generate a random temporary file name
2.5.0,Ensure the file is created
2.5.0,Open the file in the given mode
2.5.0,Tasks are either in .sdf.csv file or in the .sdf file itself
2.5.0,Structures are stored in .sdf file
2.5.0,Reset aggregator
2.5.0,Handle final leftovers for this file
2.5.0,First line of user-specified CSV *must* be header.
2.5.0,"If gzipped, need to compute extension again"
2.5.0,First line of user-specified CSV *must* be header.
2.5.0,The label encoder is given characters for ACGTN
2.5.0,Peak at the first sequence to get the length of the sequence.
2.5.0,init an one-hot vector
2.5.0,"If include_unknown_set is True, set the last index is 1."
2.5.0,################################################################
2.5.0,atom (node) featurization
2.5.0,################################################################
2.5.0,################################################################
2.5.0,bond (edge) featurization
2.5.0,################################################################
2.5.0,One sequence has length longer than others. This should throw a
2.5.0,ValueError.
2.5.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.5.0,Loosening atol to see if tests stop failing sporadically
2.5.0,string set
2.5.0,integer set
2.5.0,include_unknown_set is False
2.5.0,include_unknown_set is True
2.5.0,check unknown atoms
2.5.0,check original set
2.5.0,"Generally, =O behaves as an electron acceptor"
2.5.0,we must compute partial charges before using `get_atom_partial_charge`
2.5.0,The C-N bond is a single bond
2.5.0,TODO test more formats for ligand
2.5.0,TODO test more formats for ligand
2.5.0,adding hydrogens and charges is tested in dc.utils
2.5.0,self.ligand_file is for 3ws9_ligand.sdf
2.5.0,simple flat ring
2.5.0,self.cycle4.Compute2DCoords()
2.5.0,load and sanitize two real molecules
2.5.0,parallel normals
2.5.0,perpendicular normals
2.5.0,too far away
2.5.0,perpendicular normals
2.5.0,parallel normals
2.5.0,too far away
2.5.0,order of the molecules shouldn't matter
2.5.0,with this criteria we should find both types of stacking
2.5.0,parallel normals
2.5.0,perpendicular normals
2.5.0,too far away
2.5.0,def test_compute_cation_pi(self):
2.5.0,"# TODO(rbharath): find better example, currently dicts are empty"
2.5.0,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.5.0,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.5.0,"TODO find better example, currently dicts are empty"
2.5.0,TODO test more formats for ligand
2.5.0,Test on RDKit
2.5.0,3D vector with unit length
2.5.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.5.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.5.0,check if correct distance metric was used
2.5.0,Construct a random class probability matrix
2.5.0,Construct a random class probability matrix
2.5.0,"Note that since no name as provided, metrics are index by order"
2.5.0,given.
2.5.0,"Note that since no name as provided, metrics are index by order"
2.5.0,given.
2.5.0,"Note that since no name as provided, metrics are index by order"
2.5.0,given.
2.5.0,"Note that since no name as provided, metrics are index by order"
2.5.0,given.
2.5.0,TODO: Fix this case with correct thresholding
2.5.0,TODO: Fix this case with correct thresholding
2.5.0,There are 4 faces to the shape created by coords
2.5.0,flake8: noqa
2.5.0,Get the degree id list (which corrects for min_deg)
2.5.0,Get the size of each degree block
2.5.0,Get the the start indices for items in each block
2.5.0,Get the node indices when they are reset when the degree changes
2.5.0,Convert to numpy array
2.5.0,Reorder old atom_features
2.5.0,Reorder old deg lists
2.5.0,Sort membership
2.5.0,Create old to new dictionary. not exactly intuitive
2.5.0,Reorder adjacency lists
2.5.0,Get numpy version of degree list for indexing
2.5.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.5.0,Parse as deg separated
2.5.0,Get indices corresponding to the current degree
2.5.0,Extract and save adjacency list for the current degree
2.5.0,Construct the slice information
2.5.0,Get the cumulative indices after the first index
2.5.0,Set indices with zero sized slices to zero to avoid indexing errors
2.5.0,TODO(rbharath): Can this be removed?
2.5.0,Use random insted of zeros to prevent weird issues with summing to zero
2.5.0,"Combine the features, then sort them by (atom_degree, mol_index)"
2.5.0,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.5.0,Create a map from the original atom indices within each molecule to the
2.5.0,indices in the combined object.
2.5.0,Sort all atoms by degree.
2.5.0,"Get the size of each atom list separated by molecule id, then by degree"
2.5.0,Get the final size of each degree block
2.5.0,"Get the index at which each degree starts, not resetting after each degree"
2.5.0,And not stopping at any specific molecule
2.5.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.5.0,first column telling the start indices of each degree block and the
2.5.0,second colum telling the size of each degree block
2.5.0,Determine the membership (atom i belongs to molecule membership[i])
2.5.0,Initialize the new degree separated adjacency lists
2.5.0,Update the old adjacency lists with the new atom indices and then combine
2.5.0,all together
2.5.0,Iterate through all the molecules
2.5.0,Get the adjacency lists for this molecule and current degree id
2.5.0,"Correct all atom indices to the final indices, and then save the"
2.5.0,results into the new adjacency lists
2.5.0,Increment once row is done
2.5.0,Get the final aggregated molecule
2.5.0,"Requriments - transformers, tokenizers"
2.5.0,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.5.0,The vocab may be expanded in the near future
2.5.0,"|-|\+|\\|\/|:|~|@|\?|>>?|\*|\$|\%[0-9]{2}|[0-9])"""""""
2.5.0,add vocab_file dict
2.5.0,"unk_token=""[UNK]"","
2.5.0,"sep_token=""[SEP]"","
2.5.0,"pad_token=""[PAD]"","
2.5.0,"cls_token=""[CLS]"","
2.5.0,"mask_token=""[MASK]"","
2.5.0,take into account special tokens in max length
2.5.0,flake8: noqa
2.5.0,Initalize with 1
2.5.0,Replace the hybridization
2.5.0,global possible_hybridization_list
2.5.0,Allow 0 index to correspond to null molecule 1
2.5.0,Correct for null
2.5.0,"print(6-k-1, id)"
2.5.0,Correct for last one
2.5.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.5.0,"Handle edge case of self-pairs (i, i)"
2.5.0,Increment by 1 since we don't want 0-indexing
2.5.0,"This creates a matrix of shape (2, num_pairs)"
2.5.0,Get mapping
2.5.0,first `bt_len` features are bond features(if applicable)
2.5.0,For ring pairs outside max pairs distance continue
2.5.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.5.0,graph distance between two atoms
2.5.0,distance is a matrix of 1-hot encoded distances for all atoms
2.5.0,For ring pairs outside max pairs distance continue
2.5.0,Euclidean distance between atoms
2.5.0,atoms `radial` bonds away from `a1`
2.5.0,atoms less than `radial` bonds away
2.5.0,find atoms `radial`+1 bonds away
2.5.0,create temporary valid ids serving to filter out failed featurizations from every sublist
2.5.0,"of features (i.e. every molecules' frags list), and also totally failed sublists."
2.5.0,This makes output digestable by Loaders
2.5.0,Get the node features
2.5.0,Stack nodes into an array
2.5.0,Get bond lists with reverse edges included
2.5.0,Get canonical adjacency list
2.5.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.5.0,only support datasets providing Cartesian coordinates)
2.5.0,Set dtype
2.5.0,If includes explicit hydrogens
2.5.0,If uses use_chirality
2.5.0,Atom features
2.5.0,Stack nodes into an array
2.5.0,Get bond lists
2.5.0,Get canonical adjacency list
2.5.0,Calculate pair features
2.5.0,TODO (VIGS25): Complete the description
2.5.0,Handle loading failures which return None
2.5.0,Fit atomic conv model
2.5.0,Add the Atomic Convolution layers to fetches
2.5.0,Extract the atomic convolution features
2.5.0,"SMILES is unique, so set a canonical order of atoms"
2.5.0,Add hydrogens and generate a conformation.
2.5.0,Record properties of the molecules.
2.5.0,Create the output object.
2.5.0,flake8: noqa
2.5.0,base classes for featurizers
2.5.0,molecule featurizers
2.5.0,complex featurizers
2.5.0,material featurizers
2.5.0,support classes
2.5.0,for str
2.5.0,for list
2.5.0,validation
2.5.0,skip list
2.5.0,skip path string
2.5.0,main logic
2.5.0,Find a successful featurization
2.5.0,Replace failed featurizations with appropriate array
2.5.0,Special case handling of single molecule
2.5.0,Convert iterables to list
2.5.0,"mol must be a RDKit Mol object, so parse a SMILES"
2.5.0,"SMILES is unique, so set a canonical order of atoms"
2.5.0,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.5.0,atom_name is of format RESX-ATOMTYPE
2.5.0,where X is a 1 to 4 digit number
2.5.0,validate params
2.5.0,This assumes that the edge features for self loops are full-zero tensors
2.5.0,In the future we may want to support featurization for self loops
2.5.0,stack features
2.5.0,"before stacking edge_features or node_pos_features,"
2.5.0,we should check whether these are None or not
2.5.0,create new edge index
2.5.0,graph_index indicates which nodes belong to which graph
2.5.0,Setup image
2.5.0,Compute bond properties
2.5.0,Compute atom properties
2.5.0,Setup image
2.5.0,Compute bond properties
2.5.0,Compute atom properties
2.5.0,Reshape done for proper broadcast
2.5.0,"Reshapes, and axes manipulations to facilitate vector processing."
2.5.0,Draw a line between the two atoms.
2.5.0,"The coordinates of this line, are indicated in line_coords"
2.5.0,Turn the line coordinates into image positions
2.5.0,Set the bond line coordinates to the bond property used.
2.5.0,Turn atomic coordinates into image positions
2.5.0,Set the atom positions in image to different atomic properties in channels
2.5.0,Check whether num_confs >=1 or not
2.5.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.5.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.5.0,consistent with most QM software packages.
2.5.0,generate SMILES for fragments
2.5.0,Extend shorter strings with padding
2.5.0,Padding before and after
2.5.0,validation
2.5.0,load pretrained models
2.5.0,convert errors to zero
2.5.0,flake8: noqa
2.5.0,If partial charges were not computed
2.5.0,construct atom (node) feature
2.5.0,construct edge (bond) index
2.5.0,add edge list considering a directed graph
2.5.0,construct edge (bond) feature
2.5.0,initialize
2.5.0,check initialization
2.5.0,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.5.0,Check whether num_confs >=1 or not
2.5.0,Convert AtomPositions from Angstrom to bohr (atomic units)
2.5.0,"`(1, max_atoms)` -> `(max_atoms,)`"
2.5.0,bond labels
2.5.0,atom labels
2.5.0,create bond encoders and decoders
2.5.0,create atom encoders and decoders
2.5.0,Special case handling of single molecule
2.5.0,Convert iterables to list
2.5.0,Set up site environment matcher
2.5.0,Graphical option
2.5.0,tolerance for grouping nodes
2.5.0,determine minimum distance between sitetypes.
2.5.0,This is used to determine the existence of an edge
2.5.0,Sort by bond
2.5.0,You want to maximize this in order to make sure every node gets an edge
2.5.0,construct graph
2.5.0,matcher options
2.5.0,construct graph
2.5.0,Add nodes
2.5.0,Add edge. distance is edge attribute
2.5.0,construct graph
2.5.0,Gets the isomorphic mapping. Also the most time consuming part of the code
2.5.0,reconstruct graph after alinging point order
2.5.0,RMSD
2.5.0,Construct one hot encoding
2.5.0,get mapping between all site index to active site index
2.5.0,Get Neighbors
2.5.0,Read Data
2.5.0,get map between two environment
2.5.0,align input to the primitive cell (reference)
2.5.0,apply permutations
2.5.0,remove spectators
2.5.0,map it to active sites
2.5.0,Extract the right number of sites by distance
2.5.0,if PBC condition is fulfilled..
2.5.0,Get full N x N SCM
2.5.0,flake8: noqa
2.5.0,load atom_init.json
2.5.0,check whether the atom feature exists or not
2.5.0,construct bi-directed graph
2.5.0,Increase dimension of distance tensor and apply filter
2.5.0,We compute pairwise contact fingerprints
2.5.0,We compute pairwise contact fingerprints
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.5.0,We compute pairwise contact fingerprints
2.5.0,"rdks = [frag1[1], frag2[1]]"
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.5.0,We compute pairwise contact fingerprints
2.5.0,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.5.0,"rdks = [frag1[1], frag2[1]]"
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.5.0,We compute pairwise contact fingerprints
2.5.0,"rdks = [frag1[1], frag2[1]]"
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.5.0,We compute pairwise contact fingerprints
2.5.0,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.5.0,We compute pairwise contact fingerprints
2.5.0,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.5.0,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.5.0,"xyzs = [frag1_xyz, frag2_xyz]"
2.5.0,"rdks = [frag1[1], frag2[1]]"
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.5.0,We compute pairwise contact fingerprints
2.5.0,"rdks = [frag1[1], frag2[1]]"
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.5.0,check if user tries to set removed arguments
2.5.0,list of features that require sanitized molecules
2.5.0,not implemented featurization types
2.5.0,default values
2.5.0,update with cutoffs specified by the user
2.5.0,"each entry is a tuple (is_flat, feature_name)"
2.5.0,list of features that cannot be calculated with specified parameters
2.5.0,this list is used to define <flat/voxel/all>_combined subset
2.5.0,parse provided feature types
2.5.0,flake8: noqa
2.5.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.5.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.5.0,nonzero contact.
2.5.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.5.0,We compute pairwise contact fingerprints
2.5.0,Get coordinates
2.5.0,We compute pairwise contact fingerprints
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.5.0,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.5.0,axis.
2.5.0,Type of data created by this featurizer
2.5.0,TODO(rbharath): Should this return a list?
2.5.0,Type of data created by this featurizer
2.5.0,Currently handles loading failures by returning None
2.5.0,TODO: Is there a better handling procedure?
2.5.0,pad outputs
2.5.0,Deprecation warnings for old atomic conv featurizer name #
2.5.0,We compute pairwise contact fingerprints
2.5.0,Get coordinates
2.5.0,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.5.0,We compute pairwise contact fingerprints
2.5.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.5.0,TODO test more formats for ligand
2.5.0,TODO test more formats for ligand
2.5.0,with one conformer
2.5.0,with multiple conformers
2.5.0,include explicit hydrogens
2.5.0,with one conformer
2.5.0,with multiple conformers
2.5.0,include explicit hydrogens
2.5.0,"Requirements - transformers, tokenizers"
2.5.0,"assert ""C1=CC=CN=C1"""
2.5.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.5.0,"assert ""C1=CC=CN=C1"""
2.5.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.5.0,"assert ""C1=CC=CN=C1"""
2.5.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.5.0,"assert ""C1=CC=CN=C1"""
2.5.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.5.0,check for separate count and SMILES entries for each fragment
2.5.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.5.0,max num atoms instead of exact.
2.5.0,Cutoff in angstroms
2.5.0,"Coords are padded, neighbor list and Z are not"
2.5.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.5.0,def test_hydrogen_bond_counter():
2.5.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.5.0,"protein_file = os.path.join(current_dir, 'data',"
2.5.0,'3ws9_protein_fixer_rdkit.pdb')
2.5.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.5.0,
2.5.0,cutoff = 4.5
2.5.0,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.5.0,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.5.0,# TODO: Add shape test
2.5.0,
2.5.0,
2.5.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.5.0,def test_hydrogen_bond_voxelizer():
2.5.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.5.0,"protein_file = os.path.join(current_dir, 'data',"
2.5.0,'3ws9_protein_fixer_rdkit.pdb')
2.5.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.5.0,
2.5.0,cutoff = 4.5
2.5.0,box_width = 16
2.5.0,voxel_width = 1.0
2.5.0,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.5.0,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.5.0,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.5.0,# TODO: Add shape test
2.5.0,@pytest.mark.linux_only
2.5.0,test if default parameters work
2.5.0,check if use-case from examples works
2.5.0,test if input is flattened when flat features are used
2.5.0,test voxel features
2.5.0,test flat features
2.5.0,check if aromatic features are ignored if sanitize=False
2.5.0,test flattened voxel features
2.5.0,test voxel features
2.5.0,test flat features
2.5.0,test rotations
2.5.0,not support array style inputs
2.5.0,check convert function
2.5.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.5.0,of degree 1 (connected only to central nitrogen).
2.5.0,5 atoms in compound
2.5.0,Get the adjacency lists grouped by degree
2.5.0,The 4 outer atoms connected to central nitrogen
2.5.0,Central nitrogen connected to everything else.
2.5.0,Only one carbon
2.5.0,"No bonds, so degree adjacency lists are empty"
2.5.0,3 carbonds in alkane
2.5.0,Outer two carbonds are connected to central carbon
2.5.0,Central carbon connected to outer two
2.5.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.5.0,max num atoms instead of exact.
2.5.0,Cutoff in angstroms
2.5.0,test featurization
2.5.0,test defeaturization
2.5.0,sanity check; see if something weird does not happen with rdkit
2.5.0,check if original smiles match defeaturized smiles
2.5.0,sanity check; see if something weird does not happen with rdkit
2.5.0,test featurization
2.5.0,test defeaturization
2.5.0,check if original smiles match defeaturized smiles
2.5.0,untranform
2.5.0,untranform
2.5.0,untranform
2.5.0,Check the SDF file.
2.5.0,Check the PDB file.
2.5.0,Check the SMILES string.
2.5.0,Do a manual distance computation and make
2.5.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.5.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.5.0,Do a manual distance computation and ensure that selected neighbor is
2.5.0,closest since we set max_num_neighbors = 1
2.5.0,Carbon
2.5.0,Test distance 1
2.5.0,Test distance 2
2.5.0,Test alkane
2.5.0,Test distance 1
2.5.0,3 self connections and 2 bonds which are both counted twice because of
2.5.0,symmetry for 7 total
2.5.0,Test distance 2
2.5.0,Everything is connected at this distance
2.5.0,Test alkane
2.5.0,Test distance infinity
2.5.0,Everything is connected at this distance
2.5.0,Test pentane
2.5.0,Test distance infinity
2.5.0,Everything is connected at this distance
2.5.0,Only one carbon
2.5.0,Test feature sizes
2.5.0,"No bonds, so only 1 pair feature (for the self interaction)"
2.5.0,Only 4 atoms
2.5.0,Test feature sizes for chirality
2.5.0,3 carbonds in alkane
2.5.0,Test feature sizes
2.5.0,Should be a 3x3 interaction grid
2.5.0,mol_list = featurizer.featurize(mols)
2.5.0,mol = mol_list[0]
2.5.0,3 carbonds in alkane
2.5.0,Test feature sizes
2.5.0,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.5.0,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.5.0,symmetry)
2.5.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.5.0,of degree 1 (connected only to central nitrogen).
2.5.0,import rdkit.Chem
2.5.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.5.0,5 atoms in compound
2.5.0,Test feature sizes
2.5.0,Should be a 3x3 interaction grid
2.5.0,Artificial feature array.
2.5.0,0 atoms of degree 0
2.5.0,0 atoms of degree 1
2.5.0,4 atoms of degree 2
2.5.0,0 atoms of degree 3
2.5.0,0 atoms of degree 4
2.5.0,0 atoms of degree 5
2.5.0,0 atoms of degree 6
2.5.0,0 atoms of degree 7
2.5.0,0 atoms of degree 8
2.5.0,0 atoms of degree 9
2.5.0,0 atoms of degree 10
2.5.0,atom 4 has 0 neighbors
2.5.0,atom 0 has 2 neighbors
2.5.0,atom 1 has 2 neighbors
2.5.0,atom 2 has 2 neighbors
2.5.0,atom 3 has 3 neighbors.
2.5.0,Verify that atom features have been sorted by atom degree.
2.5.0,Sorting is done by atom degree as before. So the ordering goes
2.5.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.5.0,from new position to old position is
2.5.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.5.0,list respects this reordering and returns correct adjacency list.
2.5.0,First example molecule
2.5.0,Artificial feature array.
2.5.0,Second example molecule
2.5.0,Third example molecule
2.5.0,Test agglomerate molecule method
2.5.0,No atoms of degree 0
2.5.0,3 atoms of degree 1
2.5.0,8 atoms of degree 2
2.5.0,1 atom of degree 3
2.5.0,0 atoms of degree 4
2.5.0,0 atoms of degree 5
2.5.0,Check that atoms are only connected to themselves.
2.5.0,Check that there's one atom of each degree.
2.5.0,calculate coordinates
2.5.0,not zero values
2.5.0,assumes that every array is of the same dimension
2.5.0,rem_dataset is remaining portion of dataset
2.5.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.5.0,to k-1.
2.5.0,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.5.0,validation
2.5.0,skip list
2.5.0,skip path string
2.5.0,main logic
2.5.0,for str
2.5.0,for list
2.5.0,dict is needed in case groups aren't strictly flattened or
2.5.0,hashed by something non-integer like
2.5.0,Figure out how many positive samples we want for each task in each dataset.
2.5.0,Assign the positive samples to datasets.  Since a sample may be positive
2.5.0,"on more than one task, we need to keep track of the effect of each added"
2.5.0,"sample on each task.  To try to keep everything balanced, we cycle through"
2.5.0,"tasks, assigning one positive sample for each one."
2.5.0,We have a sample that hasn't been assigned yet.  Assign it to
2.5.0,whichever set currently has the lowest fraction of its target for
2.5.0,this task.
2.5.0,The remaining samples are negative for all tasks.  Add them to fill out
2.5.0,each set to the correct total number.
2.5.0,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.5.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.5.0,This can be generalized in the future with some common demoninator determination.
2.5.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.5.0,Append remaining examples to train
2.5.0,################################################################
2.5.0,Splitter for molecule datasets
2.5.0,################################################################
2.5.0,Sort by increasing MW
2.5.0,calcaulate scaffold sets
2.5.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.5.0,Compute fingerprints for all molecules.
2.5.0,Split into two groups: training set and everything else.
2.5.0,Split the second group into validation and test sets.
2.5.0,Begin by assigning the first molecule to the first group.
2.5.0,Decide which group to assign a molecule to.
2.5.0,Identify the unassigned molecule that is least similar to everything in
2.5.0,the other group.
2.5.0,Add it to the group.
2.5.0,Update the data on unassigned molecules.
2.5.0,Sort from largest to smallest scaffold sets
2.5.0,################################################################
2.5.0,Not well supported splitters
2.5.0,################################################################
2.5.0,All datasets share features and identifiers by assumption.
2.5.0,flake8: noqa
2.5.0,basic splitter
2.5.0,molecule splitter
2.5.0,other splitter
2.5.0,################################################################
2.5.0,Removed API
2.5.0,################################################################
2.5.0,Note that the extra task goes to test
2.5.0,Number tasks per fold
2.5.0,Find the tasks that correspond to this test fold
2.5.0,Assert that all arrays look like they should
2.5.0,"task_type = ""regression"""
2.5.0,0 1 2 3 4 5 6 7 8 9
2.5.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.5.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.5.0,reshard() to handle this?
2.5.0,Verify lengths is 10/k == 2
2.5.0,Verify that compounds in this fold are subset of original compounds
2.5.0,Verify that no two folds have overlapping compounds.
2.5.0,Verify lengths is 10/k == 2
2.5.0,Verify that compounds in this fold are subset of original compounds
2.5.0,Verify that no two folds have overlapping compounds.
2.5.0,Verify lengths is 10/k == 2
2.5.0,Verify that compounds in this fold are subset of original compounds
2.5.0,Verify that no two folds have overlapping compounds.
2.5.0,Test singletask case.
2.5.0,The split index should partition dataset in half.
2.5.0,Test singletask case.
2.5.0,Test case where some weights are zero (i.e. masked)
2.5.0,Set half the positives to have zero weight
2.5.0,There are 10 nonzero actives.
2.5.0,"The split index should partition this into half, so expect 5"
2.5.0,The split index should partition the positives for each task roughly in half.
2.5.0,Mask half the examples
2.5.0,The split index should partition dataset in half.
2.5.0,Test singletask case.
2.5.0,Should have split cleanly in half (picked random seed to ensure this)
2.5.0,Check positives are correctly distributed
2.5.0,Test singletask case.
2.5.0,Should have made an 80/10/10 train/valid/test split of actives.
2.5.0,Verify lengths is 100/k == 20
2.5.0,Note: This wouldn't work for multitask str
2.5.0,assert len(fold_dataset) == n_samples/K
2.5.0,Verify that each fold has n_positives/K = 4 positive examples.
2.5.0,Verify that compounds in this fold are subset of original compounds
2.5.0,Verify that no two folds have overlapping compounds.
2.5.0,The amount of datapoints has to be the same
2.5.0,The number of scaffolds generated by the splitter
2.5.0,has to be smaller or equal than number of total molecules
2.5.0,Add the input features.
2.5.0,Add the convolutional layers
2.5.0,Create the inputs.
2.5.0,Create the generators.
2.5.0,Create the discriminators.
2.5.0,Compute the loss functions.
2.5.0,Create learnable weights for the generators and discriminators.
2.5.0,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.5.0,Compute the weighted errors
2.5.0,Add an entropy term to the loss.
2.5.0,Create the Keras model.
2.5.0,"Every call to fit_generator() will increment global_step, but we only"
2.5.0,"want it to get incremented once for the entire batch, so record the"
2.5.0,value and keep resetting it.
2.5.0,Train the discriminator.
2.5.0,Train the generator.
2.5.0,Write checkpoints and report progress.
2.5.0,Write out final results.
2.5.0,Chain of flows is also a normalizing flow
2.5.0,An instance of tfd.TransformedDistribution
2.5.0,TODO: Incompability between TF and TFP means that TF doesn't track
2.5.0,trainable variables in the flow; must override `_create_gradient_fn`
2.5.0,self._variables = self.flow.trainable_variables
2.5.0,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.5.0,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.5.0,This is for API consistency
2.5.0,extended one of probabilites to binary distribution
2.5.0,extended one of probabilites to binary distribution
2.5.0,-*- coding: utf-8 -*-
2.5.0,"Shape (N_atoms, M_nbrs, ndim)"
2.5.0,"Shape (N_atoms, M_nbrs, ndim)"
2.5.0,"Shape (N_atoms, M_nbrs)"
2.5.0,Generate the nb_affine weights and biases
2.5.0,Extract atom_features
2.5.0,Extract graph topology
2.5.0,Sum all neighbors using adjacency matrix
2.5.0,Get collection of modified atom features
2.5.0,Obtain relevant atoms for this degree
2.5.0,Get self atoms
2.5.0,Apply hidden affine to relevant atoms and append
2.5.0,Determine the min_deg=0 case
2.5.0,Only use the self layer
2.5.0,Combine all atoms back into the list
2.5.0,Tensorflow correctly processes empty lists when using concat
2.5.0,"Sum along neighbors as well as self, and store"
2.5.0,Perform the mol gather
2.5.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.5.0,"self.max_degree, self.min_degree)"
2.5.0,Tensorflow correctly processes empty lists when using concat
2.5.0,Get self atoms
2.5.0,"There are no neighbors of this degree, so just create an empty tensor directly."
2.5.0,Expand dims
2.5.0,always deg-1 for deg_adj_lists
2.5.0,Extract graph topology
2.5.0,means that this is second loop of convolution
2.5.0,No other forget biases supported right now.
2.5.0,Taken from Keras code [citation needed]
2.5.0,"x is test set, xp is support set."
2.5.0,Get initializations
2.5.0,Process using attention
2.5.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.5.0,Generate new attention states
2.5.0,Support set lstm
2.5.0,Test lstm
2.5.0,Get initializations
2.5.0,Rename support
2.5.0,Process support xp using attention
2.5.0,Get linear combination of support set
2.5.0,Process test x using attention
2.5.0,Generate new support attention states
2.5.0,Generate new test attention states
2.5.0,Redefine
2.5.0,Number of rotatable bonds
2.5.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.5.0,a difference.
2.5.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.5.0,neighbors lists an argument instead of a part of this layer.
2.5.0,"Shape (N, M)"
2.5.0,"Shape (N, M)"
2.5.0,"Shape (N, M)"
2.5.0,Number of grid cells
2.5.0,TODO(rbharath): Support batching
2.5.0,"Shape (n_cells, ndim)"
2.5.0,"List of length N_atoms, each element of different length uniques_i"
2.5.0,"List of length N_atoms, each element of different length uniques_i"
2.5.0,"List of length N_atoms, each a tensor of shape"
2.5.0,"(uniques_i, ndim)"
2.5.0,Add phantom atoms that exist far outside the box
2.5.0,"List of length N_atoms, each of shape (1, ndim)"
2.5.0,TODO(rbharath): How does distance need to be modified here to
2.5.0,account for periodic boundary conditions?
2.5.0,List of length N_atoms each of shape (M_nbrs)
2.5.0,"N_atoms elts of size (M_nbrs,) each"
2.5.0,"Shape (N_atoms, 1)"
2.5.0,Find M_nbrs atoms closest to each cell
2.5.0,"Shape (n_cells, M_nbrs)"
2.5.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.5.0,"conditions, so does wrapround. O(constant)"
2.5.0,"Shape (n_cells, n_nbr_cells)"
2.5.0,"Shape (N_atoms, n_nbr_cells)"
2.5.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.5.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.5.0,"List of length N_atoms, each element length uniques_i"
2.5.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.5.0,element removed to remove self from list of neighbors. Need to verify
2.5.0,this holds more broadly or come up with robust alternative.
2.5.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.5.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.5.0,Shape (N_atoms*n_cells)
2.5.0,"Shape (n_cells, N_atoms)"
2.5.0,Find k atoms closest to this cell. Notice negative sign since
2.5.0,tf.nn.top_k returns *largest* not smallest.
2.5.0,"Tensor of shape (n_cells, M_nbrs)"
2.5.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.5.0,"Shape (N_atoms*n_cells, 1) after tile"
2.5.0,9 neighbors in 2-space
2.5.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.5.0,Number of cells for cube in 3-space is
2.5.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.5.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.5.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.5.0,the cube.
2.5.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.5.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.5.0,"Tile (a, a, a, b, b, b, etc.)"
2.5.0,"Tile (a, b, c, a, b, c, ...)"
2.5.0,N: Maximum number of atoms
2.5.0,M: Maximum number of neighbors
2.5.0,d: Number of coordinates/features/filters
2.5.0,B: Batch Size
2.5.0,Compute the distances and radial symmetry functions.
2.5.0,check that there isnt just one or zero inputs
2.5.0,create subspaces
2.5.0,"concatenate subspaces, reshape to size of original input, then stack"
2.5.0,"such that out_tensor has shape (2,?,original_cols)"
2.5.0,creates subspaces the same way it was done in AlphaShare
2.5.0,calculate squared Frobenius norm
2.5.0,"(TODO YTZ:) faster, less memory intensive way"
2.5.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.5.0,"r = tf.expand_dims(r, -1)"
2.5.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.5.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.5.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.5.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.5.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.5.0,Calculate pairwise distance
2.5.0,Cutoff with threshold Rc
2.5.0,return d
2.5.0,tf.stack issues again...
2.5.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.5.0,So the Tensor has known dimensions
2.5.0,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.5.0,normalization
2.5.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.5.0,and embeddings of atom j(both gone through a hidden layer)
2.5.0,"for atom i, sum the influence from all other atom j in the molecule"
2.5.0,number of inputs each step
2.5.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.5.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.5.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.5.0,`count`-th step
2.5.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.5.0,generating index for graph features used in the inputs
2.5.0,"extracting graph features for parents of the target atoms, then flatten"
2.5.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.5.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.5.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.5.0,of shape: (batch_size*max_atoms) * n_graph_features
2.5.0,representing the graph features of target atoms in each graph
2.5.0,index for targe atoms
2.5.0,Extract atom_features
2.5.0,sum all graph outputs
2.5.0,"Default message function: edge network, update function: GRU"
2.5.0,more options to be implemented
2.5.0,Add another value(~-Inf) to prevent error in softmax
2.5.0,Model using this layer must set pad_batches=True
2.5.0,Perform one step of LSTM
2.5.0,task_metadata_rows = {task: [] for task in tasks}
2.5.0,Extract those datapoints which are present for this task
2.5.0,Loading is done on-the-fly
2.5.0,Build the model.
2.5.0,Final atom-layer convolution. Note this differs slightly from the paper
2.5.0,since we use a tanh activation as default. This seems necessary for numerical
2.5.0,stability.
2.5.0,Now fully connected layers
2.5.0,Should this allow for training?
2.5.0,"pair_edges is of shape (2, N)"
2.5.0,number of atoms in each molecule
2.5.0,index of pair features
2.5.0,Get starting pair atoms
2.5.0,number of pairs for each atom
2.5.0,atom features
2.5.0,pair features
2.5.0,Build the model.
2.5.0,Build the model.
2.5.0,calculation orders for a batch of molecules
2.5.0,padding atom features vector of each molecule with 0
2.5.0,Build the model.
2.5.0,number of atoms in each molecule
2.5.0,index of pair features
2.5.0,number of pairs for each atom
2.5.0,atom features
2.5.0,pair features
2.5.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.5.0,Add the input features.
2.5.0,Add the shared dense layers
2.5.0,Add task-specific bypass layers
2.5.0,Add the input features.
2.5.0,Add the shared dense layers
2.5.0,Add task-specific bypass layers
2.5.0,W&B logging
2.5.0,Backwards compatibility
2.5.0,The optimizer creates internal variables the first time apply_gradients()
2.5.0,is called for a new set of variables.  If that happens inside a function
2.5.0,"annotated with tf.function it throws an exception, so call it once here."
2.5.0,Main training loop.
2.5.0,"Execute the loss function, accumulating the gradients."
2.5.0,Report progress and write checkpoints.
2.5.0,Capture the last avg_loss in case of return since we're resetting to
2.5.0,0 now
2.5.0,Report final results.
2.5.0,Invoke the model.
2.5.0,Apply tranformers and record results.
2.5.0,Concatenate arrays to create the final results.
2.5.0,Use a GradientTape to compute gradients.
2.5.0,Ensure weights for both models are built.
2.5.0,Add the input features.
2.5.0,Add the dense layers
2.5.0,Add the input features.
2.5.0,Add the dense layers
2.5.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.5.0,Similarity values
2.5.0,Labels for all top K similar samples
2.5.0,Discard any padded predictions
2.5.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.5.0,Build the model.
2.5.0,Character embedding
2.5.0,Multiple convolutional layers with different filter widths
2.5.0,Max-over-time pooling
2.5.0,Concat features from all filters(one feature per filter)
2.5.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.5.0,SMILES strings
2.5.0,Maximum length is expanded to allow length variation during train and inference
2.5.0,'_' served as delimiter and padding
2.5.0,Initialize common characters as keys
2.5.0,Include space to avoid extra keys
2.5.0,"For 'Cl', 'Br', etc."
2.5.0,"Character not recognized, add to extra_keys"
2.5.0,Add all extra_keys to char_dict
2.5.0,Transform SMILES sequence to integers
2.5.0,Skip all spaces
2.5.0,"For 'Cl', 'Br', etc."
2.5.0,Padding with '_'
2.5.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.5.0,"layer_sizes=[32, 32, 16],"
2.5.0,Add the dense layers
2.5.0,Do a simple greedy search.
2.5.0,Do a beam search with length normalization.
2.5.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.5.0,This candidate sequence has already been terminated
2.5.0,Consider all possible tokens we could add to this candidate sequence.
2.5.0,Add the input features.
2.5.0,Handle output layer
2.5.0,Iterate over all previous tasks.
2.5.0,prev_layers is a list with elements of size
2.5.0,"(batch_size, layer_sizes[i-1])"
2.5.0,flake8: noqa
2.5.0,scikit-learn model
2.5.0,PyTorch models
2.5.0,####################################################################################
2.5.0,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.5.0,####################################################################################
2.5.0,#######################################################################################
2.5.0,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.5.0,#######################################################################################
2.5.0,Last layer sequences not returned.
2.5.0,This is needed because ImageDataGenerator does infinite looping
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,Predict the output and uncertainty.
2.5.0,predict datset with no y (ensured by tasks = [])
2.5.0,Predict the output and uncertainty.
2.5.0,The DAG models have high error with dropout
2.5.0,"Despite a lot of effort tweaking it , there appears to be"
2.5.0,a limit to how low the error can go with dropout.
2.5.0,assert mean_error < 0.5 * mean_value
2.5.0,Predict the output and uncertainty.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,load datasets
2.5.0,disable transformer
2.5.0,check train
2.5.0,check predict shape
2.5.0,check overfit
2.5.0,load datasets
2.5.0,disable transformer
2.5.0,check train
2.5.0,check predict shape
2.5.0,check overfit
2.5.0,load datasets
2.5.0,disable transformer
2.5.0,check train
2.5.0,check predict shape
2.5.0,check overfit
2.5.0,reload
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Check same predictions are made.
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Load trained model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload trained model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Reload trained model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload trained model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reloaded Trained Model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,3D Multivariate Gaussian base distribution
2.5.0,Check that reloaded model can sample from the distribution
2.5.0,Check that density estimation is same for reloaded model
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload trained model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload Trained Model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload Trained Model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload trained model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Check predictions match on random sample
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload trained model
2.5.0,Eval model on train
2.5.0,Check predictions match on random sample
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload trained model
2.5.0,Eval model on train
2.5.0,Check predictions match on random sample
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Reload trained model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Reload trained Model
2.5.0,Check predictions match on random sample
2.5.0,Eval model on train
2.5.0,Reload Trained Model
2.5.0,Check predictions match on random sample
2.5.0,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.5.0,Reload Trained Model
2.5.0,Check predictions match on original dataset
2.5.0,TODO: We need a cleaner usage example for this
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Check predictions match on random sample
2.5.0,Train the model on random sequences.  We aren't training long enough to
2.5.0,"really make it reliable, but I want to keep this test fast, and it should"
2.5.0,still be able to reproduce a reasonable fraction of input sequences.
2.5.0,Test it out.
2.5.0,check predict shape
2.5.0,check overfit
2.5.0,needs change
2.5.0,check predict shape
2.5.0,check overfit
2.5.0,reload
2.5.0,There are 4 atoms each of which have 75 atom features
2.5.0,There are 10 pairs with infinity distance and 14 pair features
2.5.0,4 atoms in total
2.5.0,10 pairs in total
2.5.0,10 pairs in total each with start/finish
2.5.0,There are 4 atoms each of which have 75 atom features
2.5.0,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.5.0,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.5.0,connections and accounting for symmetry.)
2.5.0,4 atoms in total
2.5.0,10 pairs in total
2.5.0,The center atom is self connected and to both neighbors so it appears
2.5.0,thrice. The canonical ranking used in MolecularFeaturizer means this
2.5.0,central atom is ranked last in ordering.
2.5.0,10 pairs in total each with start/finish
2.5.0,def test_weave_fit_simple_infinity_distance():
2.5.0,featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
2.5.0,"X = featurizer([""C"", ""CCC""])"
2.5.0,"y = np.array([0, 1.])"
2.5.0,"dataset = dc.data.NumpyDataset(X, y)"
2.5.0,batch_size = 20
2.5.0,model = WeaveModel(
2.5.0,"1,"
2.5.0,"batch_size=batch_size,"
2.5.0,"mode='classification',"
2.5.0,"fully_connected_layer_sizes=[2000, 1000],"
2.5.0,"batch_normalize=True,"
2.5.0,batch_normalize_kwargs={
2.5.0,"""fused"": False,"
2.5.0,"""trainable"": True,"
2.5.0,"""renorm"": True"
2.5.0,"},"
2.5.0,learning_rate=0.0005)
2.5.0,"model.fit(dataset, nb_epoch=200)"
2.5.0,transformers = []
2.5.0,metric = dc.metrics.Metric(
2.5.0,"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
2.5.0,"scores = model.evaluate(dataset, [metric], transformers)"
2.5.0,assert scores['mean-roc_auc_score'] >= 0.9
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Fit trained model
2.5.0,Predict the output and uncertainty.
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,xgboost test
2.5.0,fit trained model
2.5.0,eval model on test
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,lightgbm test
2.5.0,fit trained model
2.5.0,eval model on test
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,xgboost test
2.5.0,fit trained model
2.5.0,eval model on test
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,lightgbm test
2.5.0,fit trained model
2.5.0,eval model on test
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,xgboost test
2.5.0,fit trained model
2.5.0,eval model on test
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,lightgbm test
2.5.0,fit trained model
2.5.0,eval model on test
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,xgboost test
2.5.0,fit trained model
2.5.0,reload
2.5.0,check predictions match on test dataset
2.5.0,eval model on test
2.5.0,prepare dataset
2.5.0,global setting
2.5.0,lightgbm test
2.5.0,fit trained model
2.5.0,reload
2.5.0,check predictions match on test dataset
2.5.0,eval model on test
2.5.0,"For simplicity, let's assume both molecules have same number of"
2.5.0,atoms.
2.5.0,Creates a set of dummy features that contain the coordinate and
2.5.0,neighbor-list features required by the AtomicConvModel.
2.5.0,Creates a set of dummy features that contain the coordinate and
2.5.0,neighbor-list features required by the AtomicConvModel.
2.5.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.5.0,max num atoms instead of exact.
2.5.0,Cutoff in angstroms
2.5.0,arbitrary label
2.5.0,Run a fitting operation
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Fit trained model
2.5.0,Eval model on train/test
2.5.0,Fit trained model
2.5.0,Eval model on train/test
2.5.0,Fit trained model
2.5.0,Eval model on train/test
2.5.0,See if it has done a plausible job of learning the distribution.
2.5.0,See if it has done a plausible job of learning the distribution.
2.5.0,See if it has done a plausible job of learning the distribution.
2.5.0,No training has been done after reload
2.5.0,See if it has done a plausible job of learning the distribution.
2.5.0,We have to set the gradient penalty very small because the generator's
2.5.0,"output is only a single number, so the default penalty would constrain"
2.5.0,it far too much.
2.5.0,See if it has done a plausible job of learning the distribution.
2.5.0,We have to set the gradient penalty very small because the generator's
2.5.0,"output is only a single number, so the default penalty would constrain"
2.5.0,it far too much.
2.5.0,See if it has done a plausible job of learning the distribution.
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,n_samples = 100
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Predict the output and uncertainty.
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Load mini log-solubility dataset.
2.5.0,Fit trained model
2.5.0,Eval model on train
2.5.0,Check that predicting internal layers works.
2.5.0,Each epoch is a single step for this model
2.5.0,Create two models using the same model directory.
2.5.0,Check that they produce different results.
2.5.0,"Save a checkpoint from the first model and load it into the second one,"
2.5.0,and make sure they now match.
2.5.0,Train a model to overfit the dataset.
2.5.0,"Create an identical model, do a single step of fitting with restore=True,"
2.5.0,and make sure it got restored correctly.
2.5.0,Build a model that predicts uncertainty.
2.5.0,Fit the model and see if its predictions are correct.
2.5.0,Take a tiny step in the direction of s and see if the output changes by
2.5.0,the expected amount.
2.5.0,def test_singletask_to_multitask_classification(self):
2.5.0,n_features = 10
2.5.0,n_tasks = 17
2.5.0,tasks = range(n_tasks)
2.5.0,# Define train dataset
2.5.0,n_train = 100
2.5.0,"X_train = np.random.rand(n_train, n_features)"
2.5.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.5.0,w_train = np.ones_like(y_train)
2.5.0,"ids_train = [""C""] * n_train"
2.5.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.5.0,"X_train, y_train, w_train, ids_train)"
2.5.0,# Define test dataset
2.5.0,n_test = 10
2.5.0,"X_test = np.random.rand(n_test, n_features)"
2.5.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.5.0,w_test = np.ones_like(y_test)
2.5.0,"ids_test = [""C""] * n_test"
2.5.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.5.0,"X_test, y_test, w_test, ids_test)"
2.5.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.5.0,def model_builder(model_dir):
2.5.0,sklearn_model = LogisticRegression()
2.5.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.5.0,multitask_model = dc.models.SingletaskToMultitask(
2.5.0,"tasks, model_builder)"
2.5.0,# Fit trained model
2.5.0,multitask_model.fit(train_dataset)
2.5.0,multitask_model.save()
2.5.0,# Eval multitask_model on train/test
2.5.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.5.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.5.0,Generate data
2.5.0,Cleanup
2.5.0,Train the model while logging the validation ROC AUC.
2.5.0,Parse the log to pull out the AUC scores.
2.5.0,The last reported score should match the current performance of the model.
2.5.0,Reload the save model and confirm that it matches the best logged score.
2.5.0,3D Multivariate Gaussian base distribution
2.5.0,Must be float32 for RealNVP
2.5.0,Tests a simple flow of one RealNVP layer.
2.5.0,log likelihoods should be negative
2.5.0,# Fit model
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,x and y are the same tensor (equivalent at every element)
2.5.0,the pairwise inner product of the rows in x and y will always be 1
2.5.0,"the output tensor will be of shape (5,5)"
2.5.0,each row in x1 is orthogonal to each row in x2
2.5.0,the pairwise inner product of the rows in x and y will always be 0
2.5.0,"the output tensor will be of shape (256,256)"
2.5.0,"Creating a second layer should produce different results, since it has"
2.5.0,different random weights.
2.5.0,But evaluating the first layer again should produce the same result as before.
2.5.0,index of pair features
2.5.0,number of pairs for each atom
2.5.0,atom features
2.5.0,pair features
2.5.0,"Outputs should be [A, P]"
2.5.0,atom features
2.5.0,Try without compression
2.5.0,"Outputs should be [mol1_vec, mol2_vec)"
2.5.0,Try with compression
2.5.0,"Outputs should be [mol1_vec, mol2_vec)"
2.5.0,atom features
2.5.0,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.5.0,Gaussian histograms expands into 11 Gaussian buckets.
2.5.0,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.5.0,TODO What should shape[1] be?  It's not documented.
2.5.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.5.0,"Creating a second layer should produce different results, since it has"
2.5.0,different random weights.
2.5.0,But evaluating the first layer again should produce the same result as before.
2.5.0,"Creating a second layer should produce different results, since it has"
2.5.0,different random weights.
2.5.0,But evaluating the first layer again should produce the same result as before.
2.5.0,"Creating a second layer should produce different results, since it has"
2.5.0,different random weights.
2.5.0,But evaluating the first layer again should produce the same result as before.
2.5.0,"TODO What should the output shape be?  It's not documented, and there"
2.5.0,are no other test cases for it.
2.5.0,"Creating a second layer should produce different results, since it has"
2.5.0,different random weights.
2.5.0,But evaluating the first layer again should produce the same result as before.
2.5.0,"Creating a second layer should produce different results, since it has"
2.5.0,different random weights.
2.5.0,But evaluating the first layer again should produce the same result as before.
2.5.0,"Recall that the DAG layer expects a MultiConvMol as input,"
2.5.0,"so the ""batch"" is a pooled set of atoms from all the"
2.5.0,"molecules in the batch, just as it is for the graph conv."
2.5.0,This means that n_atoms is the batch-size
2.5.0,dropout_switch = False
2.5.0,dropout_switch
2.5.0,# TODO(rbharath): What is the shape of outputs supposed to be?
2.5.0,"# I'm getting (7, 30) here. Where does 7 come from??"
2.5.0,TODO(rbharath): We need more documentation about why
2.5.0,these numbers work.
2.5.0,Create a dataset and an input function for processing it.
2.5.0,Create a dataset and an input function for processing it.
2.5.0,Generate dummy dataset
2.5.0,Fit trained model
2.5.0,Eval model on test
2.5.0,Eval model on train
2.5.0,Fit trained model
2.5.0,Eval model on test
2.5.0,Fit trained model
2.5.0,Eval model on test
2.5.0,Fit trained model
2.5.0,Eval model on test
2.5.0,Fit trained model
2.5.0,Eval model on test
2.5.0,Each epoch is a single step for this model
2.5.0,Create two models using the same model directory.
2.5.0,Check that they produce different results.
2.5.0,"Save a checkpoint from the first model and load it into the second one,"
2.5.0,and make sure they now match.
2.5.0,Train a model to overfit the dataset.
2.5.0,"Create an identical model, do a single step of fitting with restore=True,"
2.5.0,and make sure it got restored correctly.
2.5.0,Build a model that predicts uncertainty.
2.5.0,Fit the model and see if its predictions are correct.
2.5.0,Take a tiny step in the direction of s and see if the output changes by
2.5.0,the expected amount.
2.5.0,Train the model on random sequences.  We aren't training long enough to
2.5.0,"really make it reliable, but I want to keep this test fast, and it should"
2.5.0,still be able to reproduce a reasonable fraction of input sequences.
2.5.0,Test it out.
2.5.0,Check that it got at least a quarter of them correct.
2.5.0,Test it out.
2.5.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.5.0,"few steps of training to make sure nothing crashes, then check that the"
2.5.0,results are at least internally consistent.
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,overfit test
2.5.0,test on a small MoleculeNet dataset
2.5.0,load datasets
2.5.0,initialize models
2.5.0,embedding node features
2.5.0,convolutional layer
2.5.0,pooling
2.5.0,for n_tasks == 1 case
2.5.0,Decide first number of GAT layers
2.5.0,flake8:noqa
2.5.0,Select a device.
2.5.0,W&B logging
2.5.0,Main training loop.
2.5.0,"Execute the loss function, accumulating the gradients."
2.5.0,Report progress and write checkpoints.
2.5.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.5.0,Report final results.
2.5.0,Invoke the model.
2.5.0,Apply tranformers and record results.
2.5.0,Concatenate arrays to create the final results.
2.5.0,Compute the gradients.
2.5.0,Save the checkpoint to a file.
2.5.0,Rename and delete older files.
2.5.0,Ensure weights for both models are built.
2.5.0,Some scikit-learn models don't use weights.
2.5.0,flake8: ignore
2.5.0,GDBT doesn't support multi-output(task)
2.5.0,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.5.0,retrain model to whole data using best n_estimators * 1.25
2.5.0,GDBT doesn't support multi-output(task)
2.5.0,########################################
2.5.0,Deprecation warnings for XGBoostModel
2.5.0,########################################
2.5.0,flake8: noqa
2.5.0,-*- coding: utf-8 -*-
2.5.0,Assigning featurizer if not user defined
2.5.0,loading datasets
2.5.0,Assembling train and valid datasets
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,Building tensorflow MultitaskDNN model
2.5.0,Building tensorflow robust MultitaskDNN model
2.5.0,Building scikit logistic regression model
2.5.0,Transform fingerprints to IRV features
2.5.0,Building tensorflow IRV model
2.5.0,Building scikit random forest model
2.5.0,Building scikit learn Kernel SVM model
2.5.0,Building xgboost classification model
2.5.0,Remove token for paddings
2.5.0,Building scikit random forest model
2.5.0,Building scikit learn Kernel Ridge Regression model
2.5.0,Building scikit learn Kernel Ridge Regression model
2.5.0,Building xgboost regression model
2.5.0,Loading hyperparameters
2.5.0,num positive/negative ligands
2.5.0,Set batch sizes for network
2.5.0,Model structure
2.5.0,Traning settings
2.5.0,Fit trained model
2.5.0,Evaluating low data model
2.5.0,-*- coding: utf-8 -*-
2.5.0,Assigning featurizer if not user defined
2.5.0,loading datasets
2.5.0,
2.5.0,Note by @XericZephyr. Reason why I spun off this function:
2.5.0,1. Some model needs dataset information.
2.5.0,2. It offers us possibility to **cache** the dataset
2.5.0,"if the featurizer runs very slow, e.g., GraphConv."
2.5.0,2+. The cache can even happen at Travis CI to accelerate
2.5.0,CI testing.
2.5.0,
2.5.0,loading datasets
2.5.0,!/usr/bin/env python2
2.5.0,-*- coding: utf-8 -*-
2.5.0,TODO: Check for this
2.5.0,Download files if they don't exist
2.5.0,Featurize the KINASE dataset
2.5.0,Shuffle the training data
2.5.0,Apply transformations
2.5.0,#### TIMING ######
2.5.0,transformers = [
2.5.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.5.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.5.0,dataset=train_dataset)]
2.5.0,Set shard size low to avoid memory problems.
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Set some global variables up top
2.5.0,Featurize KAGGLE dataset
2.5.0,############################################################# TIMING
2.5.0,############################################################# TIMING
2.5.0,Build the path to the dataset on disk.
2.5.0,Try to reload cached datasets.
2.5.0,Create the dataset
2.5.0,Split and transform the dataset.
2.5.0,. clinical trial toxicity (or absence of toxicity)
2.5.0,. FDA approval status.
2.5.0,Download files if they don't exist
2.5.0,Featurizing datasets
2.5.0,Missing entry removal
2.5.0,Shuffle the training data
2.5.0,Apply transformations
2.5.0,#### TIMING ###########
2.5.0,TODO: Check if anything needs to be added
2.5.0,Featurize the FACTORS dataset
2.5.0,Shuffle the training data
2.5.0,Apply transformations
2.5.0,######### TIMING ################
2.5.0,Most reaction dataset ML tasks train the prediction of products from
2.5.0,"ractants. Both of these are contained in the rxn object that is output,"
2.5.0,"so there is no ""tasks"" field."
2.5.0,Download USPTO dataset
2.5.0,Unzip
2.5.0,Unzipped file is a tap seperated values file (despite the .txt)
2.5.0,The first element in the row is the reaction smarts
2.5.0,"Sometimes smarts have extraneous information at end of form """
2.5.0,"|f:0"" that causes parsing to fail. Not sure what this information"
2.5.0,"is, but just ignoring for now."
2.5.0,Make up dummy labels since DiskDataset.from_numpy doesn't allow
2.5.0,creation from just features for now.
2.5.0,TODO: This dataset isn't saved to disk so reload doesn't happen.
2.5.0,dict of accepted featurizers for this dataset
2.5.0,modify the returned dicts for your dataset
2.5.0,Names of supported featurizers
2.5.0,dict of accepted transformers
2.5.0,dict of accepted splitters
2.5.0,names of supported splitters
2.5.0,Warning message about this template
2.5.0,Featurize mydataset
2.5.0,Get DeepChem data directory if needed
2.5.0,Check for str args to featurizer and splitter
2.5.0,Reload from disk
2.5.0,First type of supported featurizers
2.5.0,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.5.0,Changer loader to match featurizer and data file type
2.5.0,Featurize dataset
2.5.0,Initialize transformers
2.5.0,"get pdb and sdf filenames, labels and pdbids"
2.5.0,load and featurize each complex
2.5.0,Extract locations of data
2.5.0,Extract labels
2.5.0,Lines have format
2.5.0,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.5.0,"The base-10 logarithm, -log kd/pk"
2.5.0,"def load_pcba_146(featurizer='ECFP',"
2.5.0,"split='random',"
2.5.0,"reload=True,"
2.5.0,"data_dir=None,"
2.5.0,"save_dir=None,"
2.5.0,**kwargs):
2.5.0,return load_pcba_dataset(
2.5.0,"featurizer=featurizer,"
2.5.0,"split=split,"
2.5.0,"reload=reload,"
2.5.0,"assay_file_name=""pcba_146.csv.gz"","
2.5.0,"data_dir=data_dir,"
2.5.0,"save_dir=save_dir,"
2.5.0,**kwargs)
2.5.0,"def load_pcba_2475(featurizer='ECFP',"
2.5.0,"split='random',"
2.5.0,"reload=True,"
2.5.0,"data_dir=None,"
2.5.0,"save_dir=None,"
2.5.0,**kwargs):
2.5.0,return load_pcba_dataset(
2.5.0,"featurizer=featurizer,"
2.5.0,"split=split,"
2.5.0,"reload=reload,"
2.5.0,"assay_file_name=""pcba_2475.csv.gz"","
2.5.0,"data_dir=data_dir,"
2.5.0,"save_dir=save_dir,"
2.5.0,**kwargs)
2.5.0,def test_qm9_loader():
2.5.0,current_dir = os.path.dirname(os.path.abspath(__file__))
2.5.0,"tasks, datasets, transformers = load_qm9("
2.5.0,"reload=False,"
2.5.0,"data_dir=current_dir,"
2.5.0,"featurizer='ECFP',"
2.5.0,splitter_kwargs={
2.5.0,"'seed': 42,"
2.5.0,"'frac_train': 0.6,"
2.5.0,"'frac_valid': 0.2,"
2.5.0,'frac_test': 0.2
2.5.0,})
2.5.0,
2.5.0,assert len(tasks) == 12
2.5.0,assert tasks[0] == 'mu'
2.5.0,"assert datasets[0].X.shape == (8, 1024)"
2.5.0,def test_zinc15_loader():
2.5.0,current_dir = os.path.dirname(os.path.abspath(__file__))
2.5.0,
2.5.0,"tasks, datasets, transformers = load_zinc15("
2.5.0,"reload=False,"
2.5.0,"data_dir=current_dir,"
2.5.0,splitter_kwargs={
2.5.0,"'seed': 42,"
2.5.0,"'frac_train': 0.6,"
2.5.0,"'frac_valid': 0.2,"
2.5.0,'frac_test': 0.2
2.5.0,})
2.5.0,
2.5.0,test_vec = np.array([
2.5.0,"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,"
2.5.0,"0.0, -1.224744871391589, 0.0, 0.0, 0.0, 0.0, 2.0, -0.5, 0.0, 0.0, 0.0,"
2.5.0,"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0"
2.5.0,])
2.5.0,
2.5.0,"train, val, test = datasets"
2.5.0,"assert tasks == ['mwt', 'logp', 'reactive']"
2.5.0,"assert train.X.shape == (3, 100, 35)"
2.5.0,"assert np.allclose(train.X[0][0], test_vec, atol=0.01)"
2.5.0,
2.5.0,"if os.path.exists(os.path.join(current_dir, 'zinc15_250K_2D.csv')):"
2.5.0,"os.remove(os.path.join(current_dir, 'zinc15_250K_2D.csv'))"
2.5.0,Range of optimization
2.5.0,We know from guard above that this is an int/float
2.5.0,Specify logfile
2.5.0,Make logdir if it doesn't exist.
2.5.0,setup range
2.5.0,Stores all results
2.5.0,Store all model references so we don't have to reload
2.5.0,Stores all model locations
2.5.0,"param values are always float in BO, so this line converts float to int"
2.5.0,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.5.0,Record hyperparameters
2.5.0,Add it on to the information needed for the constructor
2.5.0,Not all models have nb_epoch
2.5.0,Some models autosave
2.5.0,Record performances
2.5.0,Store all results
2.5.0,Store reference to model
2.5.0,GPGO maximize performance by default
2.5.0,set performance to its negative value for minimization
2.5.0,Demarcating internal function for readability
2.5.0,execute GPGO
2.5.0,FIXME: Incompatible types in assignment
2.5.0,Let's fetch the model with the best parameters
2.5.0,Compare best model to default hyperparameters
2.5.0,Record hyperparameters
2.5.0,Return default hyperparameters
2.5.0,Construction dictionary mapping hyperparameter names to values
2.5.0,"mypy test throws error, so ignoring it in try"
2.5.0,Not all models have nb_epoch
2.5.0,Some models autosave
2.5.0,arbitrarily return last model
2.5.0,flake8: noqa
2.5.0,Generate dummy dataset
2.5.0,Generate dummy dataset
2.5.0,These are per-example multiplier
2.5.0,Test that 2 parameters were optimized
2.5.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.5.0,Generate dummy dataset
2.5.0,Define nb_epoch in hyperparam_search function call
2.5.0,Generate dummy dataset
2.5.0,Generate dummy dataset
2.5.0,These are per-example multiplier
2.5.0,Test that 2 parameters were optimized
2.5.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.5.0,Generate dummy dataset
2.5.0,Have the worker threads generate the rollouts for this iteration.
2.5.0,Perform optimization.
2.5.0,Build the inputs and run the optimizer.
2.5.0,Update the number of steps taken so far and perform checkpointing.
2.5.0,Merge all the rollouts into a single set of arrays.
2.5.0,Iterate slices.
2.5.0,Generate the rollout.
2.5.0,Compute an estimate of the reward for the rest of the episode.
2.5.0,Compute the discounted rewards and advantages.
2.5.0,Convert the actions to one-hot.
2.5.0,Rearrange the states into the proper set of arrays.
2.5.0,Return the processed arrays.
2.5.0,Training loop.
2.5.0,Do checkpointing.
2.5.0,Generate the rollout.
2.5.0,Compute an estimate of the reward for the rest of the episode.
2.5.0,Compute the discounted rewards and advantages.
2.5.0,"Record the actions, converting to one-hot if necessary."
2.5.0,Rearrange the states into the proper set of arrays.
2.5.0,Build the inputs and apply gradients.
2.5.0,Assume all arrays are float32.
2.5.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.5.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.5.0,"game).  The average reward for any bet is slightly negative, so the best"
2.5.0,strategy is to walk away.
2.5.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.5.0,Optimize it.
2.5.0,"It should have learned that the expected value is very close to zero, and that the best"
2.5.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.5.0,top actions).
2.5.0,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.5.0,get the same result.
2.5.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.5.0,The environment just has a constant state.
2.5.0,The policy includes a single recurrent layer.
2.5.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.5.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.5.0,"On the first call, the initial state should be all zeros."
2.5.0,It should still be zeros since we didn't save it last time.
2.5.0,It should be different now.
2.5.0,This should be the same as the previous one.
2.5.0,"Now we reset it, so we should get the same result as initially."
2.5.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.5.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.5.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.5.0,at all.  Using hindsight makes it much easier.
2.5.0,A simple policy with two hidden layers.
2.5.0,Optimize it.
2.5.0,Try running it a few times and see if it succeeds.
2.5.0,The state consists of two numbers: a current value and a target value.
2.5.0,The policy just needs to learn to output the target value (or at least
2.5.0,move toward it).
2.5.0,A simple policy with no hidden layers.
2.5.0,Optimize it.
2.5.0,Try running it and see if it reaches the target
2.5.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.5.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.5.0,"game).  The average reward for any bet is slightly negative, so the best"
2.5.0,strategy is to walk away.
2.5.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.5.0,Optimize it.
2.5.0,"It should have learned that the expected value is very close to zero, and that the best"
2.5.0,"action is to walk away.  (To keep the test fast, we allow that to be either of the two"
2.5.0,top actions).
2.5.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.5.0,get the same result.
2.5.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.5.0,The environment just has a constant state.
2.5.0,The policy includes a single recurrent layer.
2.5.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.5.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.5.0,"On the first call, the initial state should be all zeros."
2.5.0,It should still be zeros since we didn't save it last time.
2.5.0,It should be different now.
2.5.0,This should be the same as the previous one.
2.5.0,"Now we reset it, so we should get the same result as initially."
2.5.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.5.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.5.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.5.0,at all.  Using hindsight makes it much easier.
2.5.0,A simple policy with two hidden layers.
2.5.0,Optimize it.
2.5.0,Try running it a few times and see if it succeeds.
2.5.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.5.0,Randomize who goes first
2.5.0,Illegal move -- the square is not empty
2.5.0,Move X
2.5.0,Did X Win
2.5.0,Did O Win
2.5.0,"default channels are ""conda-forge"" and ""omnia"""
2.5.0,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.4.0,Build a nightly package by default.
2.4.0,get the version from deepchem/__init__.py
2.4.0,nightly version : .devYearMonthDayHourMinute
2.4.0,Force to add `.dev` if `--release` option isn't passed when building
2.4.0,!/usr/bin/env python3
2.4.0,-*- coding: utf-8 -*-
2.4.0,Datasets and models used in the benchmark test
2.4.0,"irv, rf, rf_regression should be assigned manually"
2.4.0,Evaluate performances with different training set fraction
2.4.0,Datasets and models used in the benchmark test
2.4.0,Uncomment the two lines below if hyper_parameters are provided
2.4.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.4.0,hyper_parameters = pickle.load(f)
2.4.0,!/usr/bin/env python3
2.4.0,-*- coding: utf-8 -*-
2.4.0,Datasets and models used in the benchmark test
2.4.0,Load Delaney dataset
2.4.0,Get Metric
2.4.0,Fit trained model
2.4.0,Fit trained model
2.4.0,Set numpy seed
2.4.0,##Load data###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Featurize Kinase dataset
2.4.0,##Load data###
2.4.0,num_trials = 5
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,Force matplotlib to not use any Xwindows backend.
2.4.0,##Load data###
2.4.0,the histogram of the data
2.4.0,Set numpy seed
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,num_trials = 5
2.4.0,Set some global variables up top
2.4.0,Fit trained model
2.4.0,Featurize PCBA dataset
2.4.0,Initialize transformers
2.4.0,Fit trained model
2.4.0,Load sider models now
2.4.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.4.0,transformers to predict functions
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,##Load data###
2.4.0,"n_estimators=100, max_features=int(num_features/3),"
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,Fit trained model
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Load tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,This example shows how to use Pandas to load data directly
2.4.0,without using a CSVLoader object. This may be useful if you
2.4.0,want the flexibility of processing your data with Pandas
2.4.0,directly.
2.4.0,Now let's convert from a dataset back to a pandas dataframe
2.4.0,"This example shows how to load data from a SDF file into DeepChem. The data in this SDF file is stored in field ""LogP(RRCK)"""
2.4.0,Featurize FACTORS dataset
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,Force matplotlib to not use any Xwindows backend.
2.4.0,##Load data###
2.4.0,the histogram of the data
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Load QM7 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Fit trained model
2.4.0,Fit trained model
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Fit trained model
2.4.0,Load QM8 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Fit trained model
2.4.0,Set numpy seed
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,Load ChEMBL dataset
2.4.0,Fit models
2.4.0,Do setup required for tf/keras models
2.4.0,Number of features on conv-mols
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,DeepCrystal Technologies 2017 - Patrick Hop
2.4.0,MIT License - have fun!!
2.4.0,Set to higher values to get better numbers
2.4.0,======================================================================
2.4.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,Only for debug!
2.4.0,Load Delaney dataset
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Do setup required for tf/keras models
2.4.0,Number of features on conv-mols
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load Delaney dataset
2.4.0,Get Metric
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load Delaney dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load MUV dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Evaluate train/test scores
2.4.0,Load MUV data
2.4.0,Build model
2.4.0,Fit trained model
2.4.0,Evaluate train/test scores
2.4.0,Extract active site
2.4.0,Featurize ligand
2.4.0,Default for CircularFingerprint
2.4.0,Featurize pocket
2.4.0,Note broadcast operation
2.4.0,Compute labels for pockets
2.4.0,Some complexes have labels but no PDB files. Filter these manually
2.4.0,Some of the ligand-names are of form (FMN ox). Use regex
2.4.0,to merge into form (FMN-ox)
2.4.0,Filter if missing PDB files
2.4.0,Load PDBBind dataset
2.4.0,Define featurizers
2.4.0,Featurize Dataset
2.4.0,########################################################## DEBUG
2.4.0,########################################################## DEBUG
2.4.0,For stable runs
2.4.0,Fit trained model
2.4.0,4-fold splits
2.4.0,num positive/negative ligands
2.4.0,10 trials on test-set
2.4.0,Define metric
2.4.0,Get supports on test-set
2.4.0,Compute accuracies
2.4.0,Number of features on conv-mols
2.4.0,Batch size of models
2.4.0,graph_model = dc.nn.SequentialGraph(n_feat)
2.4.0,Fit trained model
2.4.0,Test model
2.4.0,Join information for all tasks.
2.4.0,4-fold splits
2.4.0,num positive/negative ligands
2.4.0,Define metric
2.4.0,Get supports on test-set
2.4.0,Compute accuracies
2.4.0,Number of features on conv-mols
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Test model
2.4.0,Join information for all tasks.
2.4.0,4-fold splits
2.4.0,num positive/negative ligands
2.4.0,10 trials on test-set
2.4.0,Define metric
2.4.0,Get supports on test-set
2.4.0,Compute accuracies
2.4.0,Number of features on conv-mols
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Test model
2.4.0,Join information for all tasks.
2.4.0,4-fold splits
2.4.0,num positive/negative ligands
2.4.0,10 trials on test-set
2.4.0,Define metric
2.4.0,Get supports on test-set
2.4.0,Compute accuracies
2.4.0,Train model on support
2.4.0,Test model
2.4.0,Join information for all tasks.
2.4.0,4-fold splits
2.4.0,num positive/negative ligands
2.4.0,10 trials on test-set
2.4.0,Define metric
2.4.0,Get supports on test-set
2.4.0,Compute accuracies
2.4.0,Train model on support
2.4.0,Test model
2.4.0,Join information for all tasks.
2.4.0,Set some global variables up top
2.4.0,Featurize Tox21 dataset
2.4.0,Initialize transformers
2.4.0,Set some global variables up top
2.4.0,Featurize Tox21 dataset
2.4.0,Initialize transformers
2.4.0,Load MUV dataset
2.4.0,Featurize MUV dataset
2.4.0,Initialize transformers
2.4.0,Load MUV dataset
2.4.0,Featurize MUV dataset
2.4.0,Initialize transformers
2.4.0,Featurize SIDER dataset
2.4.0,Initialize transformers
2.4.0,Featurize SIDER dataset
2.4.0,Initialize transformers
2.4.0,Load the data.
2.4.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.4.0,sparse: most tasks do not include data for most molecules.  It also is very
2.4.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.4.0,create a list of alternating positives and negatives so each batch will have
2.4.0,equal numbers of both.
2.4.0,Define a MetaLearner describing the learning problem.
2.4.0,Run meta-learning on 80% of the tasks.
2.4.0,Validate on the remaining tasks.
2.4.0,4-fold splits
2.4.0,10 positive/negative ligands
2.4.0,10 trials on test-set
2.4.0,Sample supports without replacement (all pos/neg should be different)
2.4.0,Define metric
2.4.0,Get supports on test-set
2.4.0,Compute accuracies
2.4.0,Train model on support
2.4.0,Test model
2.4.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.4.0,Join information for all tasks.
2.4.0,4-fold splits
2.4.0,num positive/negative ligands
2.4.0,Define metric
2.4.0,Get supports on test-set
2.4.0,Compute accuracies
2.4.0,Train model on support
2.4.0,Test model
2.4.0,Join information for all tasks.
2.4.0,replace with your own scratch directory
2.4.0,Number of conformations in each file increases exponentially.
2.4.0,Start with a smaller dataset before continuing. Use all of them
2.4.0,for production
2.4.0,"'ani_gdb_s03.h5',"
2.4.0,"'ani_gdb_s04.h5',"
2.4.0,"'ani_gdb_s05.h5',"
2.4.0,"'ani_gdb_s06.h5',"
2.4.0,"'ani_gdb_s07.h5',"
2.4.0,'ani_gdb_s08.h5'
2.4.0,Extract the data
2.4.0,Print the data
2.4.0,self-interaction energies taken from
2.4.0,https://github.com/isayev/ANI1_dataset README
2.4.0,flush once more at the end
2.4.0,"# For production, set nb_epoch to 100+"
2.4.0,"print(""Train scores"")"
2.4.0,print(train_scores)
2.4.0,"print(""Minimization of a single test set structure:"")"
2.4.0,"print(model.minimize_structure(coords, atomic_nums))"
2.4.0,Written by Roman Zubatyuk and Justin S. Smith
2.4.0,Modified by Yutong Zhao to make python2 compatible
2.4.0,opening file
2.4.0,print(store_loc)
2.4.0,print(type(v[0]))
2.4.0,print(k)
2.4.0,print(path)
2.4.0,Number of conformations in each file increases exponentially.
2.4.0,Start with a smaller dataset before continuing. Use all of them
2.4.0,for production
2.4.0,Extract the data
2.4.0,NOTE THE RENAMING:
2.4.0,Note sensitivity = recall
2.4.0,Load nci dataset
2.4.0,Featurize nci dataset
2.4.0,Initialize transformers
2.4.0,Set some global variables up top
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load hiv dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load hiv dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load delaney dataset
2.4.0,Fit models
2.4.0,Load delaney dataset
2.4.0,Fit models
2.4.0,Fit models
2.4.0,Load delaney dataset
2.4.0,Fit models
2.4.0,TODO: Once improved splitting API is merged in swap to simpler API
2.4.0,The return values are dc.data.Dataset objects so we need to extract
2.4.0,the ids
2.4.0,TODO once improved splitting API is merged in swap out for simpler
2.4.0,API
2.4.0,The return values are dc.data.Dataset objects so we need to extract
2.4.0,the ids
2.4.0,Fit trained model
2.4.0,Load SIDER dataset
2.4.0,Featurize SIDER dataset
2.4.0,Initialize transformers
2.4.0,Featurize permeability dataset
2.4.0,Load Tox21 dataset
2.4.0,Fit trained model
2.4.0,TODO: This should be swapped for simpler splitter API once that's merged in.
2.4.0,The return values are dc.data.Dataset objects so we need to extract
2.4.0,the ids
2.4.0,Only for debug!
2.4.0,Load clintox dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load clintox dataset
2.4.0,Fit models
2.4.0,Do setup required for tf/keras models
2.4.0,Number of features on conv-mols
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,-*- coding: utf-8 -*-
2.4.0,#############################################################################
2.4.0,## save dataset
2.4.0,#############################################################################
2.4.0,## load datasets
2.4.0,load sweetfda
2.4.0,load aact
2.4.0,## fixup smiles for matching
2.4.0,return smiles
2.4.0,map original smiles to converted smiles
2.4.0,"## join dataframes, index on smiles"
2.4.0,map original smiles back
2.4.0,## fill all nan with 0
2.4.0,## construct datasets
2.4.0,store in new datasets
2.4.0,## save datasets
2.4.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.4.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.4.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.4.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.4.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.4.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.4.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.4.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.4.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.4.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.4.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.4.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.4.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.4.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.4.0,For stable runs
2.4.0,Fit trained model
2.4.0,For stable runs
2.4.0,Fit trained model
2.4.0,For stable runs
2.4.0,Fit trained model
2.4.0,transformers = [
2.4.0,"dc.trans.LogTransformer(transform_X=True),"
2.4.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.4.0,dataset=train_dataset)]
2.4.0,Featurize UV dataset
2.4.0,##Load data###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Set numpy seed
2.4.0,##Load data###
2.4.0,##Create model###
2.4.0,Use R2 classification metric
2.4.0,Only use for final evaluation
2.4.0,Force matplotlib to not use any Xwindows backend.
2.4.0,##Load data###
2.4.0,the histogram of the data
2.4.0,##Load data###
2.4.0,###################################################### DEBUG
2.4.0,###################################################### DEBUG
2.4.0,Load HOPV dataset
2.4.0,Fit models
2.4.0,Number of features on conv-mols
2.4.0,Batch size of models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load HOPV dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load HOPV dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load HOPV dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Only for debug!
2.4.0,Load HOPV dataset
2.4.0,Fit models
2.4.0,Fit trained model
2.4.0,Load TOXCAST dataset
2.4.0,Featurize TOXCAST dataset
2.4.0,Initialize transformers
2.4.0,Fit trained model
2.4.0,Processing of ToxCast data
2.4.0,Author - Aneesh Pappu
2.4.0,Loading dataframes and editing indices
2.4.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.4.0,get corresponding casn
2.4.0,get corresponding smiles
2.4.0,write to cell
2.4.0,Tidy up and write to csv
2.4.0,TODO(rbharath): Check that this operation is differentiable.
2.4.0,The number of cells which we should theoretically have
2.4.0,The number of cells which we should theoretically have
2.4.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.4.0,The number of cells which we should theoretically have
2.4.0,TODO(rbharath): The test below only checks that shapes work out.
2.4.0,Need to do a correctness implementation vs. a simple CPU impl.
2.4.0,The number of cells which we should theoretically have
2.4.0,TODO(rbharath): The test below only checks that shapes work out.
2.4.0,Need to do a correctness implementation vs. a simple CPU impl.
2.4.0,The number of cells which we should theoretically have
2.4.0,TODO(rbharath): The test below only checks that shapes work out.
2.4.0,Need to do a correctness implementation vs. a simple CPU impl.
2.4.0,TODO(rbharath): Commenting this out due to weird segfaults
2.4.0,def test_vina_generate_conformers(self):
2.4.0,"""""""Test that Vina Model can generate conformers"""""""
2.4.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.4.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.4.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.4.0,max_protein_atoms = 3500
2.4.0,max_ligand_atoms = 100
2.4.0,"print(""Loading protein file"")"
2.4.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.4.0,protein_Z = pad_array(
2.4.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.4.0,max_protein_atoms)
2.4.0,"print(""Loading ligand file"")"
2.4.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.4.0,ligand_Z = pad_array(
2.4.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.4.0,max_ligand_atoms)
2.4.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.4.0,"Shape (n_cells, k)"
2.4.0,"Shape (N, 1)"
2.4.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.4.0,"conditions, so does wrapround. O(constant)"
2.4.0,"Shape (n_cells, 26)"
2.4.0,"Shape (N, 26)"
2.4.0,"coords of shape (N, ndim)"
2.4.0,"Shape (N, 26, k, ndim)"
2.4.0,"Shape (N, 26, k)"
2.4.0,"Shape (N, 26, k)"
2.4.0,"Shape (N, 26, k, ndim)"
2.4.0,"For smaller systems especially, the periodic boundary conditions can"
2.4.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.4.0,make sure duplicate neighbors are ignored?
2.4.0,TODO(rbharath): How does distance need to be modified here to
2.4.0,account for periodic boundary conditions?
2.4.0,"Shape (N, 26, k)"
2.4.0,"Shape (N, 26*k)"
2.4.0,TODO(rbharath): This will cause an issue with duplicates!
2.4.0,"Shape (N, M)"
2.4.0,"N elts of size (M,) each"
2.4.0,"Shape (N, 26*k)"
2.4.0,"N elts of size (26*k,) each"
2.4.0,"N elts of size (M,) each"
2.4.0,"Shape (N, M)"
2.4.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.4.0,"N tensors of shape (n_cells, 1)"
2.4.0,"Shape (N*n_cells, 1) after tile"
2.4.0,"List of N tensors of shape (n_cells, 1)"
2.4.0,Lists of length N
2.4.0,Lists of length n_cells
2.4.0,Get indices of k atoms closest to each cell point
2.4.0,TODO(rbharath): tf.stack for tf 1.0
2.4.0,"Tensor of shape (n_cells, k, ndim)"
2.4.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.4.0,"Tensor of shape (26, k, ndim)"
2.4.0,"Reshape to (26*k, ndim)"
2.4.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.4.0,"Dists of shape (26*k, 1)"
2.4.0,"Of shape (k, ndim)"
2.4.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.4.0,TODO(rbharath): Change this for tf 1.0
2.4.0,"n_cells tensors of shape (N, 1)"
2.4.0,"Shape (N*n_cells, 1) after tile"
2.4.0,"List of n_cells tensors of shape (N, 1)"
2.4.0,Lists of length n_cells
2.4.0,Lists of length n_cells
2.4.0,Get indices of k atoms closest to each cell point
2.4.0,"n_cells tensors of shape (k, ndim)"
2.4.0,"Tensor of shape (n_cells, k)"
2.4.0,TODO(rbharath):
2.4.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.4.0,- Need to group closest atoms amongst cell neighbors
2.4.0,- Need to do another top_k to find indices of closest neighbors.
2.4.0,- Return N lists corresponding to neighbors for every atom.
2.4.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.4.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.4.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.4.0,the cube.
2.4.0,Number of neighbors of central cube in 3-space is
2.4.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.4.0,TODO(rbharath)
2.4.0,n_cells = int(cells.get_shape()[0])
2.4.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.4.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.4.0,"Tile (a, a, a, b, b, b, etc.)"
2.4.0,"Tile (a, b, c, a, b, c, ...)"
2.4.0,"Lists of n_cells tensors of shape (N, 1)"
2.4.0,Lists of length n_cells
2.4.0,Lists of length n_cells
2.4.0,Get indices of k atoms closest to each cell point
2.4.0,"n_cells tensors of shape (26,)"
2.4.0,TODO(rbharath): Make this handle minibatches
2.4.0,"Shape (N_protein+N_ligand, 3)"
2.4.0,"Shape (N_protein+N_ligand,)"
2.4.0,"Shape (N_protein+N_ligand, M)"
2.4.0,"Shape (N_protein+N_ligand,)"
2.4.0,"Shape (N_protein+N_ligand, 3)"
2.4.0,"Shape (N_protein+N_ligand,)"
2.4.0,"Shape (N_protein+N_ligand, M)"
2.4.0,"Shape (N_protein+N_ligand, M, 3)"
2.4.0,"Shape (N_protein+N_ligand, M)"
2.4.0,"Shape (N_protein+N_ligand, M, 3)"
2.4.0,"Shape (N_protein+N_ligand, M)"
2.4.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.4.0,"Shape (N_protein+N_ligand, M)"
2.4.0,"Shape (N_protein+N_ligand, M)"
2.4.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.4.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.4.0,computing free-energy. This implementation currently uses all interaction
2.4.0,terms. Not sure if this makes a difference.
2.4.0,"Shape (N_protein+N_ligand, M)"
2.4.0,Shape () -- scalar
2.4.0,Keep track of the layers
2.4.0,"For graphical layers, add connectivity placeholders"
2.4.0,Add layer to the layer list
2.4.0,Keep track of the layers
2.4.0,Create graph topology and x
2.4.0,Keep track of the layers
2.4.0,Whether or not we have used the GraphGather layer yet
2.4.0,Update new value of x
2.4.0,Update new value of x
2.4.0,Update new value of x
2.4.0,Get train function
2.4.0,Initialize
2.4.0,################################################################### DEBUG
2.4.0,self.test_label_placeholder = Input(
2.4.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.4.0,"name=""label_placeholder""))"
2.4.0,self.test_weight_placeholder = Input(
2.4.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.4.0,"name=""weight_placeholder""))"
2.4.0,TODO(rbharath): Should weights for the support be used?
2.4.0,Support labels
2.4.0,self.support_label_placeholder = Input(
2.4.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.4.0,"name=""support_label_placeholder""))"
2.4.0,################################################################### DEBUG
2.4.0,Generate dictionary elements for support
2.4.0,Get graph information for test
2.4.0,Generate dictionary elements for test
2.4.0,Perform the optimization
2.4.0,Create different support sets
2.4.0,Get batch to try it out on
2.4.0,"Train on support set, batch pair"
2.4.0,Get featurization for test
2.4.0,"Shape (n_test, n_feat)"
2.4.0,Get featurization for support
2.4.0,"Shape (n_support, n_feat)"
2.4.0,Computes the inner part c() of the kernel
2.4.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.4.0,Normalize
2.4.0,TODO(rbharath): euclidean kernel is broken!
2.4.0,elif self.similarity == 'euclidean':
2.4.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.4.0,"Note that gram matrix g has shape (n_test, n_support)"
2.4.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.4.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.4.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.4.0,each test entry) to get attention vector
2.4.0,"Shape (n_test, n_support)"
2.4.0,Weighted sum of support labels
2.4.0,"Shape (n_support, 1)"
2.4.0,pred is yhat in eqn (1) of Matching Networks.
2.4.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.4.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.4.0,"Shape (n_test,)"
2.4.0,Convert to logit space using inverse sigmoid (logit) function
2.4.0,logit function: log(pred) - log(1-pred)
2.4.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.4.0,in Cross Entropy calculation.
2.4.0,"Shape (n_test,)"
2.4.0,Get scores
2.4.0,Remove padded elements
2.4.0,Get scores
2.4.0,pred corresponds to prob(example == 1)
2.4.0,Remove padded elements
2.4.0,Get batches
2.4.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.4.0,multitask case with missing data...
2.4.0,Join information for all tasks.
2.4.0,TODO(rbharath): Find a way to get rid of this import?
2.4.0,Extract model info
2.4.0,Get graph topology for x
2.4.0,Building outputs
2.4.0,Set epsilon
2.4.0,Initialize
2.4.0,"Path to save checkpoint files, which matches the"
2.4.0,replicated supervisor's default path.
2.4.0,Create target inputs
2.4.0,Get train function
2.4.0,TODO(rbharath): I believe this is total amount of data
2.4.0,Get graph information
2.4.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.4.0,the number of labeled data points in target_i. This is to normalize each task
2.4.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.4.0,Get other optimizer information
2.4.0,TODO(rbharath): Figure out how to handle phase appropriately
2.4.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.4.0,"tensors of shape (batch_size,)"
2.4.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.4.0,examples (effect averages out)
2.4.0,Perform the optimization
2.4.0,TODO(rbharath): Disabling saving for now to try to debug.
2.4.0,run eval data through the model
2.4.0,"Shape (n_samples, n_tasks)"
2.4.0,Create target inputs
2.4.0,TODO(rbharath): Find a way to get rid of this import?
2.4.0,Obtain appropriate loss function
2.4.0,Extract model info
2.4.0,Get graph topology for x
2.4.0,Raw logit outputs
2.4.0,Set epsilon
2.4.0,Initialize
2.4.0,"Path to save checkpoint files, which matches the"
2.4.0,replicated supervisor's default path.
2.4.0,Create target inputs
2.4.0,############################################################### DEBUG
2.4.0,"print(""multitask classifier"")"
2.4.0,"print(""feat"")"
2.4.0,print(feat)
2.4.0,############################################################### DEBUG
2.4.0,Get train function
2.4.0,TODO(rbharath): I believe this is total amount of data
2.4.0,Get graph information
2.4.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.4.0,the number of labeled data points in target_i. This is to normalize each task
2.4.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.4.0,Get other optimizer information
2.4.0,TODO(rbharath): Figure out how to handle phase appropriately
2.4.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.4.0,"tensors of shape (batch_size,)"
2.4.0,Convert the labels into one-hot vector encodings.
2.4.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.4.0,un-softmaxed logits rather than softmax outputs.
2.4.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.4.0,examples (effect averages out)
2.4.0,Perform the optimization
2.4.0,TODO(rbharath): Disabling saving for now to try to debug.
2.4.0,run eval data through the model
2.4.0,"Shape (n_samples, n_tasks)"
2.4.0,run eval data through the model
2.4.0,self.n_atoms = n_atoms
2.4.0,Define the list of tensors to be used as topology
2.4.0,Merge mol conv objects
2.4.0,Generate dicts
2.4.0,Define the list of tensors to be used as topology
2.4.0,Extract atom numbers
2.4.0,Generate dicts
2.4.0,molecule * atom(graph) => step => features
2.4.0,molecule * atom(graph) => step
2.4.0,molecule * atom(graph) => step
2.4.0,Define the list of tensors to be used as topology
2.4.0,calculation orders for a batch of molecules
2.4.0,padding atom features vector of each molecule with 0
2.4.0,self.n_atoms = n_atoms
2.4.0,Define the list of tensors to be used as topology
2.4.0,Extract atom numbers
2.4.0,Generate dicts
2.4.0,self.n_atoms = n_atoms
2.4.0,Define the list of tensors to be used as topology
2.4.0,Extract atom numbers
2.4.0,number of atoms in each molecule
2.4.0,index of pair features
2.4.0,number of pairs for each atom
2.4.0,atom features
2.4.0,pair features
2.4.0,Generate dicts
2.4.0,Load Tox21 dataset
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.4.0,Fit trained model
2.4.0,Fit models
2.4.0,Batch size of models
2.4.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.4.0,Fit trained model
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,number positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply an attention lstm layer
2.4.0,Number of folds for split
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,number positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply an attention lstm layer
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,number positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply an attention lstm layer
2.4.0,Number of folds for split
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Number of folds for split
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply a residual lstm layer
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply a residual lstm layer
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply a residual lstm layer
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply a residual lstm layer
2.4.0,Number of folds for split
2.4.0,Depth of attention module
2.4.0,number positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,Apply an attention lstm layer
2.4.0,Number of folds for split
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Number of features on conv-mols
2.4.0,Define metric
2.4.0,Train support model on train
2.4.0,Add layers
2.4.0,# Gather Projection
2.4.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.4.0,There should be 8 layers in graph_model
2.4.0,assert len(graph_model.layers) == 6
2.4.0,Add layers
2.4.0,Need to add batch-norm separately to test/support due to differing
2.4.0,shapes.
2.4.0,Apply an attention lstm layer
2.4.0,Gather Projection
2.4.0,Add layers
2.4.0,Need to add batch-norm separately to test/support due to differing
2.4.0,shapes.
2.4.0,Apply an attention lstm layer
2.4.0,Gather Projection
2.4.0,Degrees from 1 to max_deg inclusive
2.4.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.4.0,"Should have shape (?, deg)"
2.4.0,"Shape of atom_features should be (?, n_feat)"
2.4.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.4.0,-*- coding: utf-8 -*-
2.4.0,Save hyperparameters
2.4.0,-*- coding: utf-8 -*-
2.4.0,Save hyperparameters
2.4.0,setup optimizer
2.4.0,setup optimizer
2.4.0,"print(""tasK: %d"" %task)"
2.4.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.4.0,"print(""scores"")"
2.4.0,print(scores.size())
2.4.0,"print(""task_label"")"
2.4.0,print(task_label.size())
2.4.0,"task_loss =  self.criterion(scores, task_label)"
2.4.0,"print(""task_loss"")"
2.4.0,print(task_loss.size())
2.4.0,-*- coding: utf-8 -*-
2.4.0,Save hyperparameters
2.4.0,weight decay
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Turns out there are valid cases where we don't want pad-batches
2.4.0,on by default.
2.4.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.4.0,Run training op.
2.4.0,############################################################# TIMING
2.4.0,The iterbatches does padding with zero-weight examples on the last batch.
2.4.0,Remove padded examples.
2.4.0,Special case to handle singletasks.
2.4.0,The iterbatches does padding with zero-weight examples on the last batch.
2.4.0,Remove padded examples.
2.4.0,References
2.4.0,Arguments
2.4.0,Aliases.
2.4.0,Aliases.
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,TODO(rbharath): This class does not yet have a
2.4.0,"TensorGraph equivalent, but one may not be required."
2.4.0,"Commented out for now, remove if OK."
2.4.0,class AlternateWeaveLayer(WeaveLayer):
2.4.0,""""""" Alternate implementation of weave module"
2.4.0,"same variables, different graph structures"
2.4.0,""""""""
2.4.0,
2.4.0,"def call(self, x, mask=None):"
2.4.0,"""""""Execute this layer on input tensors."
2.4.0,
2.4.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.4.0,
2.4.0,Parameters
2.4.0,----------
2.4.0,x: list
2.4.0,list of Tensors of form described above.
2.4.0,"mask: bool, optional"
2.4.0,Ignored. Present only to shadow superclass call() method.
2.4.0,
2.4.0,Returns
2.4.0,-------
2.4.0,A: Tensor
2.4.0,Tensor of atom_features
2.4.0,P: Tensor
2.4.0,Tensor of pair_features
2.4.0,""""""""
2.4.0,# Add trainable weights
2.4.0,self.build()
2.4.0,
2.4.0,atom_features = x[0]
2.4.0,pair_features = x[1]
2.4.0,
2.4.0,pair_split = x[2]
2.4.0,atom_to_pair = x[4]
2.4.0,
2.4.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.4.0,AA = self.activation(AA)
2.4.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.4.0,PA = self.activation(PA)
2.4.0,"PA = tf.segment_sum(PA, pair_split)"
2.4.0,
2.4.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.4.0,A = self.activation(A)
2.4.0,
2.4.0,if self.update_pair:
2.4.0,AP_ij = tf.matmul(
2.4.0,tf.reshape(
2.4.0,"tf.gather(atom_features, atom_to_pair),"
2.4.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.4.0,AP_ij = self.activation(AP_ij)
2.4.0,AP_ji = tf.matmul(
2.4.0,tf.reshape(
2.4.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.4.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.4.0,AP_ji = self.activation(AP_ji)
2.4.0,
2.4.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.4.0,PP = self.activation(PP)
2.4.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.4.0,P = self.activation(P)
2.4.0,else:
2.4.0,P = pair_features
2.4.0,
2.4.0,"return A, P"
2.4.0,TODO(rbharath): This class does not yet have a
2.4.0,"TensorGraph equivalent, but one may not be required."
2.4.0,"Commented out for now, remove if OK."
2.4.0,class WeaveConcat(Layer):
2.4.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.4.0,""""""""
2.4.0,
2.4.0,"def __init__(self,"
2.4.0,"batch_size,"
2.4.0,"n_atom_input_feat=50,"
2.4.0,"n_output=128,"
2.4.0,"init='glorot_uniform',"
2.4.0,"activation='tanh',"
2.4.0,**kwargs):
2.4.0,""""""""
2.4.0,Parameters
2.4.0,----------
2.4.0,batch_size: int
2.4.0,number of molecules in a batch
2.4.0,"n_atom_input_feat: int, optional"
2.4.0,Number of features for each atom in input.
2.4.0,"n_output: int, optional"
2.4.0,Number of output features for each atom(concatenated)
2.4.0,"init: str, optional"
2.4.0,Weight initialization for filters.
2.4.0,"activation: str, optional"
2.4.0,Activation function applied
2.4.0,
2.4.0,""""""""
2.4.0,self.batch_size = batch_size
2.4.0,self.n_atom_input_feat = n_atom_input_feat
2.4.0,self.n_output = n_output
2.4.0,self.init = initializations.get(init)  # Set weight initialization
2.4.0,self.activation = activations.get(activation)  # Get activations
2.4.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.4.0,
2.4.0,def build(self):
2.4.0,"""""""""Construct internal trainable weights."
2.4.0,""""""""
2.4.0,
2.4.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.4.0,self.b = model_ops.zeros(shape=[
2.4.0,"self.n_output,"
2.4.0,])
2.4.0,
2.4.0,self.trainable_weights = self.W + self.b
2.4.0,
2.4.0,"def call(self, x, mask=None):"
2.4.0,"""""""Execute this layer on input tensors."
2.4.0,
2.4.0,"x = [atom_features, atom_mask]"
2.4.0,
2.4.0,Parameters
2.4.0,----------
2.4.0,x: list
2.4.0,Tensors as listed above
2.4.0,"mask: bool, optional"
2.4.0,Ignored. Present only to shadow superclass call() method.
2.4.0,
2.4.0,Returns
2.4.0,-------
2.4.0,outputs: Tensor
2.4.0,Tensor of concatenated atom features
2.4.0,""""""""
2.4.0,self.build()
2.4.0,atom_features = x[0]
2.4.0,atom_masks = x[1]
2.4.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.4.0,A_mask = tf.split(
2.4.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.4.0,outputs = tf.concat(
2.4.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.4.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.4.0,outputs = self.activation(outputs)
2.4.0,return outputs
2.4.0,TODO(rbharath): This class does not yet have a
2.4.0,"TensorGraph equivalent, but one may not be required."
2.4.0,"Commented out for now, remove if OK."
2.4.0,class AlternateWeaveGather(WeaveGather):
2.4.0,"""""""Alternate implementation of weave gather layer"
2.4.0,corresponding to AlternateWeaveLayer
2.4.0,""""""""
2.4.0,
2.4.0,"def call(self, x, mask=None):"
2.4.0,"""""""Execute this layer on input tensors."
2.4.0,
2.4.0,"x = [atom_features, atom_split]"
2.4.0,
2.4.0,Parameters
2.4.0,----------
2.4.0,x: list
2.4.0,Tensors as listed above
2.4.0,"mask: bool, optional"
2.4.0,Ignored. Present only to shadow superclass call() method.
2.4.0,
2.4.0,Returns
2.4.0,-------
2.4.0,outputs: Tensor
2.4.0,Tensor of molecular features
2.4.0,""""""""
2.4.0,# Add trainable weights
2.4.0,self.build()
2.4.0,outputs = x[0]
2.4.0,atom_split = x[1]
2.4.0,
2.4.0,if self.gaussian_expand:
2.4.0,outputs = self.gaussian_histogram(outputs)
2.4.0,
2.4.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.4.0,
2.4.0,if self.gaussian_expand:
2.4.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.4.0,output_molecules = self.activation(output_molecules)
2.4.0,return output_molecules
2.4.0,Each directory holds a range of assay results
2.4.0,Just write NA
2.4.0,"Now, write out the results csv, going line by line through all molecule results"
2.4.0,printing the mol_id
2.4.0,printing the SMILES
2.4.0,Now gzip it
2.4.0,Now remove the intermediate csv
2.4.0,First download all SDF files. We need these to get smiles
2.4.0,Next download all Bioassays
2.4.0,RDKit consistently hangs when trying to read this file
2.4.0,TODO (LESWING) Lazy Load
2.4.0,TODO (LESWING) Lazy Load
2.4.0,from simdna import simulations
2.4.0,define layer out functions
2.4.0,get layer outputs for a positive simulation example
2.4.0,plot layer outputs
2.4.0,highlight motif sites
2.4.0,get a positive and a negative example from the simulation data
2.4.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.4.0,get motif site locations
2.4.0,organize legends
2.4.0,plot scores and highlight motif site locations
2.4.0,initialize fwd and reverse scores to -infinity
2.4.0,"cross-correlate separately for each base,"
2.4.0,for both the PSSM and its reverse complement
2.4.0,sum over the bases
2.4.0,take max of fwd and reverse scores at each position
2.4.0,return 1D view of sequence characters
2.4.0,class SequenceDNN(Model):
2.4.0,""""""""
2.4.0,Sequence DNN models.
2.4.0,
2.4.0,Parameters
2.4.0,----------
2.4.0,"seq_length : int, optional"
2.4.0,length of input sequence.
2.4.0,"keras_model : instance of keras.models.Sequential, optional"
2.4.0,seq_length or keras_model must be specified.
2.4.0,"num_tasks : int, optional"
2.4.0,number of tasks. Default: 1.
2.4.0,num_filters : list[int] | tuple[int]
2.4.0,"number of convolutional filters in each layer. Default: (15,)."
2.4.0,conv_width : list[int] | tuple[int]
2.4.0,"width of each layer's convolutional filters. Default: (15,)."
2.4.0,pool_width : int
2.4.0,width of max pooling after the last layer. Default: 35.
2.4.0,L1 : float
2.4.0,strength of L1 penalty.
2.4.0,dropout : float
2.4.0,dropout probability in every convolutional layer. Default: 0.
2.4.0,verbose: int
2.4.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.4.0,
2.4.0,Returns
2.4.0,-------
2.4.0,Compiled DNN model.
2.4.0,""""""""
2.4.0,
2.4.0,"def __init__(self,"
2.4.0,"seq_length=None,"
2.4.0,"keras_model=None,"
2.4.0,"use_RNN=False,"
2.4.0,"num_tasks=1,"
2.4.0,"num_filters=(15, 15, 15),"
2.4.0,"conv_width=(15, 15, 15),"
2.4.0,"pool_width=35,"
2.4.0,"GRU_size=35,"
2.4.0,"TDD_size=15,"
2.4.0,"L1=0,"
2.4.0,"dropout=0.0,"
2.4.0,"num_epochs=100,"
2.4.0,verbose=1):
2.4.0,self.num_tasks = num_tasks
2.4.0,self.num_epochs = num_epochs
2.4.0,self.verbose = verbose
2.4.0,self.train_metrics = []
2.4.0,self.valid_metrics = []
2.4.0,if keras_model is not None and seq_length is None:
2.4.0,self.model = keras_model
2.4.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.4.0,elif seq_length is not None and keras_model is None:
2.4.0,self.model = Sequential()
2.4.0,assert len(num_filters) == len(conv_width)
2.4.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.4.0,conv_height = 4 if i == 0 else 1
2.4.0,self.model.add(
2.4.0,Convolution2D(
2.4.0,"nb_filter=nb_filter,"
2.4.0,"nb_row=conv_height,"
2.4.0,"nb_col=nb_col,"
2.4.0,"activation='linear',"
2.4.0,"init='he_normal',"
2.4.0,"input_shape=(1, 4, seq_length),"
2.4.0,"W_regularizer=l1(L1),"
2.4.0,b_regularizer=l1(L1)))
2.4.0,self.model.add(Activation('relu'))
2.4.0,self.model.add(Dropout(dropout))
2.4.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.4.0,if use_RNN:
2.4.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.4.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.4.0,"self.model.add(Permute((2, 1)))"
2.4.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.4.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.4.0,self.model.add(Flatten())
2.4.0,self.model.add(Dense(output_dim=self.num_tasks))
2.4.0,self.model.add(Activation('sigmoid'))
2.4.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.4.0,else:
2.4.0,raise ValueError(
2.4.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.4.0,
2.4.0,"def train(self,"
2.4.0,"X,"
2.4.0,"y,"
2.4.0,"validation_data,"
2.4.0,"early_stopping_metric='Loss',"
2.4.0,"early_stopping_patience=5,"
2.4.0,save_best_model_to_prefix=None):
2.4.0,if y.dtype != bool:
2.4.0,"assert set(np.unique(y)) == {0, 1}"
2.4.0,y = y.astype(bool)
2.4.0,multitask = y.shape[1] > 1
2.4.0,if not multitask:
2.4.0,num_positives = y.sum()
2.4.0,num_sequences = len(y)
2.4.0,num_negatives = num_sequences - num_positives
2.4.0,if self.verbose >= 1:
2.4.0,print('Training model (* indicates new best result)...')
2.4.0,"X_valid, y_valid = validation_data"
2.4.0,early_stopping_wait = 0
2.4.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.4.0,"for epoch in range(1, self.num_epochs + 1):"
2.4.0,self.model.fit(
2.4.0,"X,"
2.4.0,"y,"
2.4.0,"batch_size=128,"
2.4.0,"nb_epoch=1,"
2.4.0,class_weight={
2.4.0,"True: num_sequences / num_positives,"
2.4.0,False: num_sequences / num_negatives
2.4.0,"} if not multitask else None,"
2.4.0,verbose=self.verbose >= 2)
2.4.0,"epoch_train_metrics = self.test(X, y)"
2.4.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.4.0,self.train_metrics.append(epoch_train_metrics)
2.4.0,self.valid_metrics.append(epoch_valid_metrics)
2.4.0,if self.verbose >= 1:
2.4.0,print('Epoch {}:'.format(epoch))
2.4.0,print('Train {}'.format(epoch_train_metrics))
2.4.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.4.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.4.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.4.0,if self.verbose >= 1:
2.4.0,print(' *')
2.4.0,best_metric = current_metric
2.4.0,best_epoch = epoch
2.4.0,early_stopping_wait = 0
2.4.0,if save_best_model_to_prefix is not None:
2.4.0,self.save(save_best_model_to_prefix)
2.4.0,else:
2.4.0,if self.verbose >= 1:
2.4.0,print()
2.4.0,if early_stopping_wait >= early_stopping_patience:
2.4.0,break
2.4.0,early_stopping_wait += 1
2.4.0,if self.verbose >= 1:
2.4.0,print('Finished training after {} epochs.'.format(epoch))
2.4.0,if save_best_model_to_prefix is not None:
2.4.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.4.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.4.0,"best_epoch, save_best_model_to_prefix))"
2.4.0,
2.4.0,"def predict(self, X):"
2.4.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.4.0,
2.4.0,def get_sequence_filters(self):
2.4.0,""""""""
2.4.0,Returns 3D array of 2D sequence filters.
2.4.0,""""""""
2.4.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.4.0,
2.4.0,"def deeplift(self, X, batch_size=200):"
2.4.0,""""""""
2.4.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.4.0,""""""""
2.4.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.4.0,from deeplift.conversion import keras_conversion as kc
2.4.0,
2.4.0,# convert to deeplift model and get scoring function
2.4.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.4.0,score_func = deeplift_model.get_target_contribs_func(
2.4.0,find_scores_layer_idx=0)
2.4.0,# use a 40% GC reference
2.4.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.4.0,# get deeplift scores
2.4.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.4.0,for i in range(self.num_tasks):
2.4.0,deeplift_scores[i] = score_func(
2.4.0,"task_idx=i,"
2.4.0,"input_data_list=[X],"
2.4.0,"batch_size=batch_size,"
2.4.0,"progress_update=None,"
2.4.0,input_references_list=input_references)
2.4.0,return deeplift_scores
2.4.0,
2.4.0,"def in_silico_mutagenesis(self, X):"
2.4.0,""""""""
2.4.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.4.0,""""""""
2.4.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.4.0,wild_type_predictions = self.predict(X)
2.4.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.4.0,np.newaxis]
2.4.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.4.0,"zip(X, wild_type_predictions)):"
2.4.0,mutated_sequences = np.repeat(
2.4.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.4.0,# remove wild-type
2.4.0,arange = np.arange(len(mutated_sequences))
2.4.0,horizontal_cycle = np.tile(
2.4.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.4.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.4.0,# add mutant
2.4.0,vertical_repeat = np.repeat(
2.4.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.4.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.4.0,# make mutant predictions
2.4.0,mutated_predictions = self.predict(mutated_sequences)
2.4.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.4.0,"(self.num_tasks,))"
2.4.0,mutagenesis_scores[
2.4.0,sequence_index] = wild_type_prediction - mutated_predictions
2.4.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.4.0,
2.4.0,@staticmethod
2.4.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.4.0,from dragonn.plot import plot_bases_on_ax
2.4.0,scores = score_func(X).squeeze(
2.4.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.4.0,try:
2.4.0,os.makedirs(output_directory)
2.4.0,except OSError:
2.4.0,pass
2.4.0,num_tasks = len(scores)
2.4.0,"for task_index, task_scores in enumerate(scores):"
2.4.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.4.0,# sequence_scores is num_bases x sequence_length
2.4.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.4.0,plt.clf()
2.4.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.4.0,top_axis.plot(
2.4.0,"range(1,"
2.4.0,"len(basewise_max_sequence_scores) + 1),"
2.4.0,basewise_max_sequence_scores)
2.4.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.4.0,peak_position = basewise_max_sequence_scores.argmax()
2.4.0,top_axis.axvspan(
2.4.0,"peak_position - peak_width,"
2.4.0,"peak_position + peak_width,"
2.4.0,"color='grey',"
2.4.0,alpha=0.1)
2.4.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.4.0,peak_position + peak_width].T
2.4.0,# Set non-max letter_heights to zero
2.4.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.4.0,"letter_heights[np.arange(len(letter_heights)),"
2.4.0,peak_sequence_scores.argmax(axis=1)] = \
2.4.0,basewise_max_sequence_scores[peak_position - peak_width :
2.4.0,peak_position + peak_width]
2.4.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.4.0,bottom_axis.set_xticklabels(
2.4.0,tuple(
2.4.0,"map(str,"
2.4.0,"np.arange(peak_position - peak_width,"
2.4.0,peak_position + peak_width + 1))))
2.4.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.4.0,plt.xlabel('Position')
2.4.0,plt.ylabel('Score')
2.4.0,plt.savefig(
2.4.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.4.0,"sequence_index, '_task_{}'.format(task_index)"
2.4.0,if num_tasks > 1 else '')))
2.4.0,plt.close()
2.4.0,
2.4.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.4.0,self._plot_scores(
2.4.0,"X,"
2.4.0,"output_directory,"
2.4.0,"peak_width,"
2.4.0,"score_func=self.deeplift,"
2.4.0,score_name='DeepLift')
2.4.0,
2.4.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.4.0,self._plot_scores(
2.4.0,"X,"
2.4.0,"output_directory,"
2.4.0,"peak_width,"
2.4.0,"score_func=self.in_silico_mutagenesis,"
2.4.0,score_name='ISM')
2.4.0,
2.4.0,"def plot_architecture(self, output_file):"
2.4.0,from dragonn.visualize_util import plot as plot_keras_model
2.4.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.4.0,
2.4.0,"def save(self, save_best_model_to_prefix):"
2.4.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.4.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.4.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.4.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.4.0,
2.4.0,@staticmethod
2.4.0,"def load(arch_fname, weights_fname=None):"
2.4.0,model_json_string = open(arch_fname).read()
2.4.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.4.0,if weights_fname is not None:
2.4.0,sequence_dnn.model.load_weights(weights_fname)
2.4.0,return sequence_dnn
2.4.0,create temporary fasta files
2.4.0,run command
2.4.0,remove fasta files
2.4.0,write test fasta file
2.4.0,test gkmsvm
2.4.0,get classification results
2.4.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.4.0,"Using canonical smiles for glycine, as in original research paper"
2.4.0,Atom features with padding
2.4.0,A_tilda_k computation
2.4.0,Final feed_dict setup
2.4.0,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.4.0,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.4.0,self.num_node_features)
2.4.0,Fit models
2.4.0,Args
2.4.0,2017 DeepCrystal Technologies - Patrick Hop
2.4.0,
2.4.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.4.0,
2.4.0,MIT License - have fun!!
2.4.0,===========================================================
2.4.0,x = F.selu( fc(x) )
2.4.0,x = F.selu( fc(x) )
2.4.0,2017 DeepCrystal Technologies - Patrick Hop
2.4.0,
2.4.0,Data loading a splitting file
2.4.0,
2.4.0,MIT License - have fun!!
2.4.0,===========================================================
2.4.0,Args
2.4.0,TODO (VIGS25): Account for the reload option
2.4.0,Downloading train files
2.4.0,Parsing training data
2.4.0,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.4.0,Test Files loading
2.4.0,One Hot Featurization
2.4.0,Consistency check
2.4.0,Handle output layer
2.4.0,Iterate over all previous tasks.
2.4.0,prev_layers is a list with elements of size
2.4.0,"(batch_size, layer_sizes[i-1])"
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Save an initial checkpoint.
2.4.0,Turns out there are valid cases where we don't want pad-batches
2.4.0,on by default.
2.4.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.4.0,Run training op.
2.4.0,Always save a final checkpoint when complete.
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Note that we divide by the batch size and not the number of
2.4.0,"non-zero weight examples in the batch.  Also, instead of using"
2.4.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.4.0,calculate with div/sum so it stays on the GPU.
2.4.0,aggregated costs
2.4.0,weight decay
2.4.0,Dummy placeholders
2.4.0,Dummy placeholders
2.4.0,run eval data through the model
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,Handle edge case when batch-size is 1.
2.4.0,Prune away any padding that was added
2.4.0,allow_soft_placement=True allows ops without a GPU implementation
2.4.0,to run on the CPU instead.
2.4.0,!/usr/bin/python
2.4.0,
2.4.0,Copyright 2015 Google Inc.
2.4.0,
2.4.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.4.0,you may not use this file except in compliance with the License.
2.4.0,You may obtain a copy of the License at
2.4.0,
2.4.0,http://www.apache.org/licenses/LICENSE-2.0
2.4.0,
2.4.0,"Unless required by applicable law or agreed to in writing, software"
2.4.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.4.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.4.0,See the License for the specific language governing permissions and
2.4.0,limitations under the License.
2.4.0,parse CheckpointState proto
2.4.0,parse path to actual checkpoint
2.4.0,the provided mask has to be the same shape as features
2.4.0,test k = 1..4
2.4.0,central moments
2.4.0,standardized moments
2.4.0,central across one axis
2.4.0,standardized across one axis
2.4.0,Fit just on task zero
2.4.0,Notice that we keep the session open
2.4.0,Fit on task one
2.4.0,The predictions for task zero should not change after training
2.4.0,on task one.
2.4.0,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.4.0,!/usr/bin/python
2.4.0,
2.4.0,Copyright 2015 Google Inc.
2.4.0,
2.4.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.4.0,you may not use this file except in compliance with the License.
2.4.0,You may obtain a copy of the License at
2.4.0,
2.4.0,http://www.apache.org/licenses/LICENSE-2.0
2.4.0,
2.4.0,"Unless required by applicable law or agreed to in writing, software"
2.4.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.4.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.4.0,See the License for the specific language governing permissions and
2.4.0,limitations under the License.
2.4.0,get the divisor
2.4.0,compute the requested central moment
2.4.0,"note that mean is a raw moment, not a central moment"
2.4.0,TODO(user): median is not implemented yet in TensorFlow
2.4.0,Add the input features.
2.4.0,"layer has shape [None, layer_sizes[i]]"
2.4.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.4.0,TODO(rbharath): Might want to make it feasible to have multiple
2.4.0,bypass layers.
2.4.0,Construct task bypass layer
2.4.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.4.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.4.0,"layer has shape [None, layer_sizes[i]]"
2.4.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.4.0,TODO(rbharath): Might want to make it feasible to have multiple
2.4.0,bypass layers.
2.4.0,Construct task bypass layer
2.4.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.4.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.4.0,Consistency check
2.4.0,Lazily created by _get_shared_session().
2.4.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.4.0,when subclass-overridden methods use the same scopes.
2.4.0,Setup graph
2.4.0,Create placeholders
2.4.0,Handle output layer
2.4.0,Iterate over all previous tasks.
2.4.0,prev_layers is a list with elements of size
2.4.0,"(batch_size, layer_sizes[i-1])"
2.4.0,Note that we divide by the batch size and not the number of
2.4.0,"non-zero weight examples in the batch.  Also, instead of using"
2.4.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.4.0,calculate with div/sum so it stays on the GPU.
2.4.0,aggregated costs
2.4.0,weight decay
2.4.0,Dummy placeholders
2.4.0,Dummy placeholders
2.4.0,run eval data through the model
2.4.0,"Shape (n_tasks, n__samples)"
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,Handle edge case when batch-size is 1.
2.4.0,with self._get_shared_session(train=True) as sess:
2.4.0,Save an initial checkpoint.
2.4.0,Always save a final checkpoint when complete.
2.4.0,Note that we divide by the batch size and not the number of
2.4.0,"non-zero weight examples in the batch.  Also, instead of using"
2.4.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.4.0,calculate with div/sum so it stays on the GPU.
2.4.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.4.0,Dummy placeholders
2.4.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.4.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.4.0,Dummy placeholders
2.4.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.4.0,allow_soft_placement=True allows ops without a GPU implementation
2.4.0,to run on the CPU instead.
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Turns out there are valid cases where we don't want pad-batches
2.4.0,on by default.
2.4.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.4.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.4.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,"(n_samples, n_classes)"
2.4.0,"(n_samples, n_tasks, n_classes)"
2.4.0,Save hyperparameters
2.4.0,Guard variable to make sure we don't Restore() this model
2.4.0,from a disk checkpoint more than once.
2.4.0,"Path to save checkpoint files, which matches the"
2.4.0,replicated supervisor's default path.
2.4.0,Lazily created by _get_shared_session().
2.4.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.4.0,when subclass-overridden methods use the same scopes.
2.4.0,Setup graph
2.4.0,Note that we divide by the batch size and not the number of
2.4.0,"non-zero weight examples in the batch.  Also, instead of using"
2.4.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.4.0,calculate with div/sum so it stays on the GPU.
2.4.0,aggregated costs
2.4.0,weight decay
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Save an initial checkpoint.
2.4.0,Define the code that runs on a separate thread to feed data into the queue.
2.4.0,Main training loop.
2.4.0,Run training op.
2.4.0,We have reached the end of an epoch.
2.4.0,We have reached the end of the data.
2.4.0,Always save a final checkpoint when complete.
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,allow_soft_placement=True allows ops without a GPU implementation
2.4.0,to run on the CPU instead.
2.4.0,gpu memory growth option
2.4.0,gpu memory growth option
2.4.0,TODO(rbharath): Is setting train=False right here?
2.4.0,Discard any padded predictions
2.4.0,The iterbatches does padding with zero-weight examples on the last batch.
2.4.0,Remove padded examples.
2.4.0,Special case to handle singletasks.
2.4.0,The iterbatches does padding with zero-weight examples on the last batch.
2.4.0,Remove padded examples.
2.4.0,TODO(rbharath): Verify this can be safely removed.
2.4.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.4.0,""""""""
2.4.0,Evaluates the performance of this model on specified dataset.
2.4.0,
2.4.0,Parameters
2.4.0,----------
2.4.0,dataset: dc.data.Dataset
2.4.0,Dataset object.
2.4.0,metric: deepchem.metrics.Metric
2.4.0,Evaluation metric
2.4.0,transformers: list
2.4.0,List of deepchem.transformers.Transformer
2.4.0,Returns
2.4.0,-------
2.4.0,dict
2.4.0,Maps tasks to scores under metric.
2.4.0,""""""""
2.4.0,"evaluator = Evaluator(self, dataset, transformers)"
2.4.0,scores = evaluator.compute_model_performance(metrics)
2.4.0,return scores
2.4.0,checkpoints look like model_dir/model.ckpt-N
2.4.0,"self._save_path is ""model_dir/model.ckpt"""
2.4.0,run eval data through the model
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,run eval data through the model
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,Note that softmax is already applied in construct_grpah
2.4.0,run eval data through the model
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,Handle edge case when batch-size is 1.
2.4.0,Prune away any padding that was added
2.4.0,Handle case of 0-dimensional scalar output
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,inputs placeholder
2.4.0,data preprocessing and augmentation
2.4.0,first conv layer
2.4.0,downsample by max pooling
2.4.0,each module is a residual convolutional block
2.4.0,followed by a convolutional downsample layer
2.4.0,max pooling over the final outcome
2.4.0,fully connected layers
2.4.0,dropout for dense layers
2.4.0,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.4.0,weight decay regularizer
2.4.0,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.4.0,sample cut ratio from a clipped gaussian
2.4.0,train/valid differences
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,Define and build model
2.4.0,model.restore()
2.4.0,Set random seeds
2.4.0,Setup directories
2.4.0,Model constants
2.4.0,Load and transform datasets
2.4.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.4.0,Atomic convolution variables
2.4.0,at = atomic numbers (atom types)
2.4.0,"radial basis function parameters [cutoff, mean, width]"
2.4.0,Model hyperparameters
2.4.0,Initialize model
2.4.0,Fit model
2.4.0,Evaluate model
2.4.0,Set random seeds
2.4.0,Setup directories
2.4.0,Model constants
2.4.0,Load and transform datasets
2.4.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.4.0,Atomic convolution variables
2.4.0,at = atomic numbers (atom types)
2.4.0,"radial basis function parameters [cutoff, mean, width]"
2.4.0,Model hyperparameters
2.4.0,Initialize model
2.4.0,Fit model
2.4.0,Evaluate model
2.4.0,Set random seeds
2.4.0,Setup directories
2.4.0,Model constants
2.4.0,Load and transform datasets
2.4.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.4.0,Atomic convolution variables
2.4.0,at = atomic numbers (atom types)
2.4.0,"radial basis function parameters [cutoff, mean, width]"
2.4.0,Model hyperparameters
2.4.0,Initialize model
2.4.0,Fit model
2.4.0,Evaluate model
2.4.0,Set random seeds
2.4.0,Setup directories
2.4.0,Model constants
2.4.0,Load and transform datasets
2.4.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.4.0,Atomic convolution variables
2.4.0,at = atomic numbers (atom types)
2.4.0,"radial basis function parameters [cutoff, mean, width]"
2.4.0,Model hyperparameters
2.4.0,Initialize model
2.4.0,Fit model
2.4.0,Evaluate model
2.4.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.4.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.4.0,test_scores = test_evaluator.compute_model_performance(metric)
2.4.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.4.0,param.update(test_scores)
2.4.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.4.0,for transformer in transformers:
2.4.0,train_dataset = transformer.transform(train_dataset)
2.4.0,test_dataset = transformer.transform(test_dataset)
2.4.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.4.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.4.0,for transformer in transformers:
2.4.0,train_dataset = transformer.transform(train_dataset)
2.4.0,test_dataset = transformer.transform(test_dataset)
2.4.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.4.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.4.0,for transformer in transformers:
2.4.0,train_dataset = transformer.transform(train_dataset)
2.4.0,test_dataset = transformer.transform(test_dataset)
2.4.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.4.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.4.0,for transformer in transformers:
2.4.0,train_dataset = transformer.transform(train_dataset)
2.4.0,test_dataset = transformer.transform(test_dataset)
2.4.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.4.0,Create some directories for analysis
2.4.0,The base_dir holds the results of all analysis
2.4.0,Make directories to store the raw and featurized datasets.
2.4.0,Load PDBBind dataset
2.4.0,Define featurizers
2.4.0,Currently featurizes with shard_size=1
2.4.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.4.0,This could be done with openbabel in python
2.4.0,Compute cells for this molecule. O(constant)
2.4.0,min == max if molecule is planar in some direction
2.4.0,we should still create a bin
2.4.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.4.0,Note neighbors contains self!
2.4.0,Associate each atom with cell it belongs to. O(N)
2.4.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.4.0,"conditions, so does wrapround. O(constant)"
2.4.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.4.0,Accept as neighbors only those within threshold. This computation should be
2.4.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.4.0,Sort neighbors by distance
2.4.0,Pick up to max_num_neighbors
2.4.0,Type of data created by this featurizer
2.4.0,assumes that every array is of the same dimension
2.4.0,rem_dataset is remaining portion of dataset
2.4.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.4.0,to k-1.
2.4.0,returns list of per column sum of non zero elements
2.4.0,Compute number of actives needed per task.
2.4.0,loop through each column and obtain index required to splice out for
2.4.0,required fraction of hits
2.4.0,Find the first index where the cumulative number of actives equals
2.4.0,the actives_count
2.4.0,Note that np.where tells us last index required to exceed
2.4.0,"actives_count, so we actually want the following location"
2.4.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.4.0,potentially refactor those to match this.
2.4.0,Handle edge case where frac_split is 1
2.4.0,Create weight matrices fpor two haves.
2.4.0,copy over up to required index for weight first_split
2.4.0,check out if any rows in either w_1 or w_2 are just zeros
2.4.0,"Obtain original x, y, and w arrays and shuffle"
2.4.0,calculate percent split for valid (out of test and valid)
2.4.0,"split test data into valid and test, treating sub test set also as sparse"
2.4.0,rem_dataset is remaining portion of dataset
2.4.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.4.0,to k-1.
2.4.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.4.0,This can be generalized in the future with some common demoninator determination.
2.4.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.4.0,Append remaining examples to train
2.4.0,Sort by increasing MW
2.4.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.4.0,for m_idx in cluster:
2.4.0,"continue until we find an active in all the tasks, otherwise we can't"
2.4.0,compute a meaningful AUC
2.4.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.4.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.4.0,Sort from largest to smallest scaffold sets
2.4.0,Sort from largest to smallest scaffold sets
2.4.0,"(n_samples, n_classes)"
2.4.0,"(n_samples, n_tasks, n_classes)"
2.4.0,Save hyperparameters
2.4.0,Guard variable to make sure we don't Restore() this model
2.4.0,from a disk checkpoint more than once.
2.4.0,"Path to save checkpoint files, which matches the"
2.4.0,replicated supervisor's default path.
2.4.0,Lazily created by _get_shared_session().
2.4.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.4.0,when subclass-overridden methods use the same scopes.
2.4.0,Setup graph
2.4.0,Note that we divide by the batch size and not the number of
2.4.0,"non-zero weight examples in the batch.  Also, instead of using"
2.4.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.4.0,calculate with div/sum so it stays on the GPU.
2.4.0,aggregated costs
2.4.0,weight decay
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Save an initial checkpoint.
2.4.0,Turns out there are valid cases where we don't want pad-batches
2.4.0,on by default.
2.4.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.4.0,Run training op.
2.4.0,Always save a final checkpoint when complete.
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,allow_soft_placement=True allows ops without a GPU implementation
2.4.0,to run on the CPU instead.
2.4.0,TODO(rbharath): Is setting train=False right here?
2.4.0,Discard any padded predictions
2.4.0,The iterbatches does padding with zero-weight examples on the last batch.
2.4.0,Remove padded examples.
2.4.0,Special case to handle singletasks.
2.4.0,The iterbatches does padding with zero-weight examples on the last batch.
2.4.0,Remove padded examples.
2.4.0,TODO(rbharath): Verify this can be safely removed.
2.4.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.4.0,""""""""
2.4.0,Evaluates the performance of this model on specified dataset.
2.4.0,
2.4.0,Parameters
2.4.0,----------
2.4.0,dataset: dc.data.Dataset
2.4.0,Dataset object.
2.4.0,metric: deepchem.metrics.Metric
2.4.0,Evaluation metric
2.4.0,transformers: list
2.4.0,List of deepchem.transformers.Transformer
2.4.0,Returns
2.4.0,-------
2.4.0,dict
2.4.0,Maps tasks to scores under metric.
2.4.0,""""""""
2.4.0,"evaluator = Evaluator(self, dataset, transformers)"
2.4.0,scores = evaluator.compute_model_performance(metrics)
2.4.0,return scores
2.4.0,checkpoints look like logdir/model.ckpt-N
2.4.0,"self._save_path is ""logdir/model.ckpt"""
2.4.0,run eval data through the model
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,run eval data through the model
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,Note that softmax is already applied in construct_grpah
2.4.0,run eval data through the model
2.4.0,reshape to batch_size x n_tasks x ...
2.4.0,Handle edge case when batch-size is 1.
2.4.0,Prune away any padding that was added
2.4.0,Handle case of 0-dimensional scalar output
2.4.0,Dummy placeholders
2.4.0,Dummy placeholders
2.4.0,## AtomicNet fully-connected layer ops ###
2.4.0,## Atomicnet coordinate transform ops ###
2.4.0,## Atomicnet symmetry function kernel ops ###
2.4.0,## Atomicnet symmetry function ops ###
2.4.0,## Atomcnet symmetry function layer ops ###
2.4.0,We apply the radial pooling filter before atom type conv
2.4.0,to reduce computation
2.4.0,## Misc convenience ops ###
2.4.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.4.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.4.0,"game).  The average reward for any bet is slightly negative, so the best"
2.4.0,strategy is to walk away.
2.4.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.4.0,Optimize it.
2.4.0,"It should have learned that the expected value is very close to zero, and that the best"
2.4.0,action is to walk away.
2.4.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.4.0,get the same result.
2.4.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.4.0,Run the algorithm.
2.4.0,Save a file checkpoint.
2.4.0,Build the tree.
2.4.0,Compute the final probabilities and expected reward.
2.4.0,Mark this node as terminal
2.4.0,Expand this node.
2.4.0,Select the next action to perform.
2.4.0,Recursively build the tree.
2.4.0,Update statistics for this node.
2.4.0,Configuration file for the Sphinx documentation builder.
2.4.0,
2.4.0,This file only contains a selection of the most common options. For a full
2.4.0,list see the documentation:
2.4.0,https://www.sphinx-doc.org/en/master/usage/configuration.html
2.4.0,-- Path setup --------------------------------------------------------------
2.4.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.4.0,add these directories to sys.path here. If the directory is relative to the
2.4.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.4.0,
2.4.0,-- Project information -----------------------------------------------------
2.4.0,"The full version, including alpha/beta/rc tags"
2.4.0,-- General configuration ---------------------------------------------------
2.4.0,"Add any Sphinx extension module names here, as strings. They can be"
2.4.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.4.0,ones.
2.4.0,Options for autodoc directives
2.4.0,How to represents typehints.
2.4.0,"Add any paths that contain templates here, relative to this directory."
2.4.0,The suffix of source filenames.
2.4.0,The master toctree document.
2.4.0,autosectionlabel setting
2.4.0,"List of patterns, relative to source directory, that match files and"
2.4.0,directories to ignore when looking for source files.
2.4.0,This pattern also affects html_static_path and html_extra_path.
2.4.0,"If true, the current module name will be prepended to all description"
2.4.0,unit titles (such as .. function::).
2.4.0,-- Options for HTML output -------------------------------------------------
2.4.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.4.0,a list of builtin themes.
2.4.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.4.0,"relative to this directory. They are copied after the builtin static files,"
2.4.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.4.0,The name of an image file (relative to this directory) to place at the top
2.4.0,of the sidebar.
2.4.0,Customize the sphinx theme
2.4.0,-- Source code links ---------------------------------------------------
2.4.0,Resolve function for the linkcode extension.
2.4.0,"try to find the file and line number, based on code from numpy:"
2.4.0,https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286
2.4.0,lines in the label file have format
2.4.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.4.0,"print line[0], line[3]"
2.4.0,"If you push the tag, please remove `.dev`"
2.4.0,Record inputs.
2.4.0,Create the output directory if necessary.
2.4.0,Create the optimizers for meta-optimization and task optimization.
2.4.0,Create a Checkpoint for saving.
2.4.0,Main optimization loop.
2.4.0,Do checkpointing.
2.4.0,flake8: noqa
2.4.0,This is a MetaLearner that learns to generate sine functions with variable
2.4.0,amplitude and phase.
2.4.0,Optimize it.
2.4.0,Test it out on some new tasks and see how it works.
2.4.0,Initially the model should do a bad job of fitting the sine function.
2.4.0,After one step of optimization it should do much better.
2.4.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.4.0,get the same result.
2.4.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.4.0,We know use_pose_generator_scores == False in this case
2.4.0,check whether self.featurizer is instance of ComplexFeaturizer or not
2.4.0,TODO: How to handle the failure here?
2.4.0,TODO(rbharath): The autodock vina source computes surface distances
2.4.0,which take into account the van der Waals radius of each atom type.
2.4.0,"Shape (N, M)"
2.4.0,"Shape (N, M)"
2.4.0,Parse complex
2.4.0,Prepare protein
2.4.0,Get protein centroid and range
2.4.0,TODO(rbharath: Does vina divide box dimensions by 2?
2.4.0,Prepare protein
2.4.0,Write Vina conf file
2.4.0,Define locations of log and output files
2.4.0,"I'm not sure why specifying the args as a list fails on other platforms,"
2.4.0,but for some reason it only works if I pass it as a string.
2.4.0,FIXME: Incompatible types in assignment
2.4.0,FIXME: We should use `subprocess.run` instead of `call`
2.4.0,flake8: noqa
2.4.0,We provide no scoring model so the docker won't score
2.4.0,Check only one output since num_modes==1
2.4.0,We provide no scoring model so the docker won't score
2.4.0,Check only one output since num_modes==1
2.4.0,Let's turn on logging since this test will run for a while
2.4.0,Check returned files exist
2.4.0,Let's turn on logging since this test will run for a while
2.4.0,Check returned files exist
2.4.0,"Where d is greater than zero, the repulsion is just zeros"
2.4.0,"When d is 0, this should just be 1"
2.4.0,"When d == 0, the hbond interaction is 0"
2.4.0,The exponential returns 1 when input 0.
2.4.0,This exponential returns 1 when input 3
2.4.0,Let's turn on logging since this test will run for a while
2.4.0,Let's turn on logging since this test will run for a while
2.4.0,Let's turn on logging since this test will run for a while
2.4.0,Let's turn on logging since this test will run for a while
2.4.0,Note this may download autodock Vina...
2.4.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.4.0,Test that every atom in pocket maps exists
2.4.0,scalar case
2.4.0,per-example case
2.4.0,This is a little arcane but it repeats w across tasks.
2.4.0,"If w.shape == (n_samples, 1) handle it as 1D"
2.4.0,"w.shape == (n_samples, n_tasks)"
2.4.0,scalar case
2.4.0,Handle n_classes/n_task shape ambiguity
2.4.0,Add in task dimension
2.4.0,Insert a task dimension (we know n_tasks=1 from above0
2.4.0,"If 3D and last dimension isn't 1, assume this is one-hot encoded and return as-is."
2.4.0,Handle classification. We need to convert labels into one-hot representation.
2.4.0,check whether n_classes is int or not
2.4.0,Handle n_classes/n_task shape ambiguity
2.4.0,Add in task dimension
2.4.0,Make everything 2D so easy to handle
2.4.0,Handle each task separately.
2.4.0,Handle continuous class probabilites of positive class for binary
2.4.0,Fill in class 0 probabilities
2.4.0,Add a task dimension to concatenate on
2.4.0,Handle binary labels
2.4.0,"make y_hot of shape (N, n_classes)"
2.4.0,Add a task dimension to concatenate on
2.4.0,Insert a task dimension
2.4.0,"Now of shape (N,)"
2.4.0,"Now of shape (N, 1)"
2.4.0,"Returns shape (N, n_tasks)"
2.4.0,"Now of shape (N,)"
2.4.0,"Now of shape (N, n_classes)"
2.4.0,"Now of shape (N, 1, n_classes)"
2.4.0,"Returns shape (N, n_tasks, n_classes)"
2.4.0,These are some smart defaults
2.4.0,These are some smart defaults corresponding to sklearn's required
2.4.0,behavior
2.4.0,Attempt some limited shape imputation to find n_tasks
2.4.0,check whether n_tasks is int or not
2.4.0,This is because `normalize_weight_shape` require int value.
2.4.0,FIXME: Incompatible types in assignment
2.4.0,DEPRECATED. WILL BE REMOVED IN NEXT DEEPCHEM VERSION
2.4.0,DEPRECATED. WILL BE REMOVED IN NEXT DEEPCHEM VERSION
2.4.0,Attempt to convert both into the same type
2.4.0,if len(y_true.shape) != 2 or len(y_pred.shape) != 2 or y_true.shape != y_pred.shape:
2.4.0,"raise ValueError(""For classification metrics, y_true and y_pred must both be of shape (N, n_classes)"")"
2.4.0,initialize fwd and reverse scores to -infinity
2.4.0,"cross-correlate separately for each base,"
2.4.0,for both the PSSM and its reverse complement
2.4.0,sum over the bases
2.4.0,take max of fwd and reverse scores at each position
2.4.0,"Shape (N_sequences, num_tasks)"
2.4.0,check whether wild_type_predictions is np.ndarray or not
2.4.0,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.4.0,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.4.0,Mutates every position of the sequence to every letter
2.4.0,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.4.0,Breakdown:
2.4.0,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.4.0,remove wild-type
2.4.0,len(arange) = N_letters * sequence_length
2.4.0,len(horizontal cycle) = N_letters * sequence_length
2.4.0,add mutant
2.4.0,make mutant predictions
2.4.0,check whether wild_type_predictions is np.ndarray or not
2.4.0,kappa_score is an alias for `sklearn.metrics.cohen_kappa_score`
2.4.0,validation
2.4.0,flake8: noqa
2.4.0,metric class
2.4.0,metrics utils
2.4.0,sklearn & scipy score function
2.4.0,original score function
2.4.0,Get a random prediction matrix
2.4.0,"Of shape (N, n_classes)"
2.4.0,"Of shape (N, 1, n_classes)"
2.4.0,This has w for each task.
2.4.0,Best score case
2.4.0,Worst score case
2.4.0,best case
2.4.0,duplicate prediction value
2.4.0,Encode motif
2.4.0,"sequences now has shape (3, 4, 5, 1)"
2.4.0,"sequences now has shape (3, 4, 5, 1)"
2.4.0,Construct and train SequenceDNN model
2.4.0,Call in-silico mutagenesis
2.4.0,Construct and train SequenceDNN model
2.4.0,Call in-silico mutagenesis
2.4.0,Check nonzero elements exist
2.4.0,Special case handling of single input
2.4.0,Featurize task results iff they exist.
2.4.0,Filter out examples where featurization failed.
2.4.0,"For prospective data where results are unknown, it"
2.4.0,makes no sense to have y values or weights.
2.4.0,Featurize task results if they exist.
2.4.0,Filter out examples where featurization failed.
2.4.0,"For prospective data where results are unknown, it"
2.4.0,makes no sense to have y values or weights.
2.4.0,The field in which dc.utils.save.load_sdf_files stores RDKit mol objects
2.4.0,The field in which load_sdf_files return value stores smiles
2.4.0,"(X, y, w, ids)"
2.4.0,Sometimes zip files contain directories within. Traverse directories
2.4.0,TODO(rbharath): Add support for more extensions
2.4.0,Sort image files
2.4.0,"FIXME: Signature of ""_featurize_shard"" incompatible with supertype ""DataLoader"""
2.4.0,Remove support indices
2.4.0,Remove support indices
2.4.0,Remove support indices
2.4.0,Get task specific entries
2.4.0,Now just get weights for this task
2.4.0,Get task specific entries
2.4.0,Now just get weights for this task
2.4.0,Now just get weights for this task
2.4.0,Now just get weights for this task
2.4.0,Split data into pos and neg lists.
2.4.0,No replacement allowed for supports
2.4.0,Handle one-d vs. non one-d feature matrices
2.4.0,Init the iterator
2.4.0,Set initial iterator state
2.4.0,support = self.supports[task][self.trial_num]
2.4.0,Increment and update logic
2.4.0,Init the iterator
2.4.0,Set initial iterator state
2.4.0,support = self.supports[task][self.trial_num]
2.4.0,Increment and update logic
2.4.0,Ensure that every worker will pick the same random order for each epoch.
2.4.0,Ensure that every worker will pick the same random order for each epoch.
2.4.0,"By invariant of when this is called, can assume num_samples > 0"
2.4.0,and num_samples < batch_size
2.4.0,Fill in batch arrays
2.4.0,"By invariant of when this is called, can assume num_samples > 0"
2.4.0,and num_samples < batch_size
2.4.0,Fill in batch arrays
2.4.0,Only the first set of copy will be counted in training loss
2.4.0,Retrieve the first sample so we can determine the dtypes.
2.4.0,Create a Tensorflow Dataset.
2.4.0,Find the X values.
2.4.0,Find the y values.
2.4.0,Find the w values.
2.4.0,Find the ids.
2.4.0,"Set labels to be zero, with zero weights"
2.4.0,Load obsolete format -> save in new format
2.4.0,note that this corresponds to the _construct_metadata column order
2.4.0,Create temp directory to store resharded version
2.4.0,Get correct shapes for y/w
2.4.0,Write data in new shards
2.4.0,Handle shapes
2.4.0,Note that this means that DiskDataset resharding currently doesn't
2.4.0,work for datasets that aren't regression/classification.
2.4.0,Handle spillover from last shard
2.4.0,Should have updated to non-legacy metadata
2.4.0,Note that this resets the cache internally
2.4.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.4.0,"than process based pools, since process based pools need to pickle/serialize"
2.4.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.4.0,we're actually protected by the GIL.
2.4.0,mp.dummy aliases ThreadPool to Pool
2.4.0,(ytz): this skips everything except possibly the last shard
2.4.0,"To unify shape handling so from_numpy behaves like NumpyDataset, we just"
2.4.0,make a NumpyDataset under the hood
2.4.0,"raw_data = (X, y, w, ids)"
2.4.0,Protect against generator exhaustion
2.4.0,This ensures tasks are consistent for all datasets
2.4.0,Get full dataset in memory
2.4.0,Shuffle in memory
2.4.0,Write shuffled shards out to disk
2.4.0,Shuffle the arrays corresponding to each row in metadata_df
2.4.0,Reset cache
2.4.0,See if we have a cached copy of this shard.
2.4.0,"We don't, so load it from disk."
2.4.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.4.0,Try to cache this shard for later use.  Since the normal usage pattern is
2.4.0,"a series of passes through the whole dataset, there's no point doing"
2.4.0,anything fancy.  It never makes sense to evict another shard from the
2.4.0,"cache to make room for this one, because we'll probably want that other"
2.4.0,shard again before the next time we want this one.  So just cache as many
2.4.0,as we can and then stop.
2.4.0,"When outputting a NumpyDataset, we have 1 in-memory shard"
2.4.0,Handle edge case with empty indices
2.4.0,We use two loops here. The outer while loop walks over selection shards
2.4.0,(the chunks of the indices to select that should go into separate
2.4.0,"output shards), while the inner for loop walks over the shards in the"
2.4.0,source datasets to select out the shard indices from that  source shard
2.4.0,Find indices which rest in this shard
2.4.0,Need to offset indices to fit within shard_size
2.4.0,Handle empty case where no data from this shard needed
2.4.0,Handle the case of datasets with y/w missing
2.4.0,Break if all indices have been used up already
2.4.0,Note these will be in the sorted order
2.4.0,We need to recover the original ordering. We can do this by using
2.4.0,np.where to find the locatios of the original indices in the sorted
2.4.0,indices.
2.4.0,We know there's only one match for np.where since this is a
2.4.0,"permutation, so the [0][0] pulls out the exact match location."
2.4.0,If shape metadata is available use it to directly compute shape from
2.4.0,metadata
2.4.0,"In absense of shape metadata, fall back to loading data from disk to"
2.4.0,find shape.
2.4.0,Case n_samples should be 1
2.4.0,flake8: noqa
2.4.0,TODO(rbharath): Get rid of * import
2.4.0,Load MUV dataset
2.4.0,Do an approximate comparison since splits are sometimes slightly off from
2.4.0,the exact fraction.
2.4.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.4.0,reloading will cause the transform to be reapplied. This is undesirable in
2.4.0,almost all cases. Need to understand a method to fix this.
2.4.0,The shuffling should have switched up the ordering
2.4.0,But all the same entries should still be present
2.4.0,All the data should have same shape
2.4.0,The shuffling should have switched up the ordering
2.4.0,But all the same entries should still be present
2.4.0,All the data should have same shape
2.4.0,The ids should now store the performed permutation. Check that the
2.4.0,original dataset is recoverable.
2.4.0,The ids should now store the performed permutation. Check that the
2.4.0,original dataset is recoverable.
2.4.0,Generate data
2.4.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.4.0,around for testing resharding.
2.4.0,Set cache to 0 size to avoid cache hiding errors
2.4.0,Generate data
2.4.0,legacy_dataset_reshard is a shared dataset in the legacy format kept
2.4.0,around for testing resharding.
2.4.0,Set cache to 0 size to avoid cache hiding errors
2.4.0,Featurize emols dataset
2.4.0,example.fasta contains 3 sequences each of length 58.
2.4.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.4.0,"There is one ""image channel""."
2.4.0,Generate dummy dataset
2.4.0,Generate dummy dataset
2.4.0,Generate dummy dataset
2.4.0,Set last n_samples/2 weights to 0
2.4.0,Check that no support elements are sample from zero-weight samples
2.4.0,Generate dummy dataset
2.4.0,Generate dummy dataset
2.4.0,Create support generator
2.4.0,Generate dummy dataset
2.4.0,Create support generator
2.4.0,Generate dummy dataset
2.4.0,Assert all support elements have been removed
2.4.0,Generate dummy dataset
2.4.0,Assert all remove elements have been removed
2.4.0,Generate dummy dataset
2.4.0,Assert all support elements have been removed
2.4.0,Generate dummy dataset
2.4.0,Assert all remove elements have been removed
2.4.0,Generate dummy dataset
2.4.0,Set last n_samples/2 weights to 0
2.4.0,Sample from first n_samples/2 elements for support
2.4.0,Should lie within first n_samples/2 samples only
2.4.0,Generate dummy dataset
2.4.0,Create support generator
2.4.0,Generate dummy dataset
2.4.0,This is necessary since from_numpy adds in shape information
2.4.0,This is necessary since from_numpy adds in shape information
2.4.0,This is necessary since from_numpy adds in shape information
2.4.0,Generate data
2.4.0,Generate data
2.4.0,Generate data
2.4.0,Should now have 10 shards
2.4.0,This is the shape of legacy_data
2.4.0,legacy_dataset is a dataset in the legacy format kept around for testing
2.4.0,purposes.
2.4.0,This is the shape of legacy_data_reshard
2.4.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.4.0,around for testing
2.4.0,Should now have 10 shards
2.4.0,legacy_dataset is a dataset in the legacy format kept around for testing purposes.
2.4.0,Test constructor reload works for legacy format
2.4.0,legacy_dataset_reshard is a sharded dataset in the legacy format kept
2.4.0,around for testing resharding.
2.4.0,Reshard copy
2.4.0,Check metadata has been updated
2.4.0,First try using images for X.
2.4.0,Now try using images for y.
2.4.0,Transform it
2.4.0,Test on identity matrix
2.4.0,Generate random sparse features dataset
2.4.0,Test edge case with array of all zeros
2.4.0,Test cases where n_samples < 2*n_samples < batch_size
2.4.0,Test cases where n_samples < batch_size
2.4.0,Test case where n_samples == batch_size
2.4.0,Test case for object featurization.
2.4.0,Test case for more complicated object featurization
2.4.0,Test case with multidimensional data
2.4.0,Test cases where n_samples < 2*n_samples < batch_size
2.4.0,Test cases where n_samples < batch_size
2.4.0,Test case where n_samples == batch_size
2.4.0,Test case for object featurization.
2.4.0,Test case for more complicated object featurization
2.4.0,Test case with multidimensional data
2.4.0,Test first resharding worked
2.4.0,Test second resharding worked
2.4.0,approx 1/15! chance of equality
2.4.0,Generate data
2.4.0,Generate data
2.4.0,Transform it
2.4.0,special case to test
2.4.0,deterministic
2.4.0,non-deterministic
2.4.0,we don't know the order in which the shards are iterated in.
2.4.0,Check that we have all the data in
2.4.0,Test iterating in order.
2.4.0,Test iterating out of order.
2.4.0,Test iterating in batches.
2.4.0,Test iterating with multiple workers.
2.4.0,A round trip from Dataset to DataFrame to Dataset should produce identical arrays.
2.4.0,Try specifying particular columns.
2.4.0,Test id shrinkage
2.4.0,Test task shrinkage
2.4.0,Test max print size
2.4.0,Create image file
2.4.0,Create zip of image file
2.4.0,Create zip of multiple image files
2.4.0,"Create zip of multiple image files, multiple_types"
2.4.0,Create image directory
2.4.0,These are the known dimensions of face.png
2.4.0,These are the known dimensions of face.png
2.4.0,TODO(rbharath): Where are the color channels?
2.4.0,"Since the different files have different shapes, makes an object array"
2.4.0,Splits featurized samples into train/test
2.4.0,Splits featurized samples into train/test
2.4.0,Splits featurized samples into train/test
2.4.0,Splits featurized samples into train/test
2.4.0,Now perform move
2.4.0,Only for debug!
2.4.0,Make directories to store the raw and featurized datasets.
2.4.0,Load dataset
2.4.0,Featurize tox21 dataset
2.4.0,featurization
2.4.0,train/valid split.
2.4.0,singletask load
2.4.0,comparison
2.4.0,Only for debug!
2.4.0,Make directories to store the raw and featurized datasets.
2.4.0,Load dataset
2.4.0,Featurize tox21 dataset
2.4.0,For debugging purposes
2.4.0,multitask load
2.4.0,Do train/valid split.
2.4.0,singletask load
2.4.0,comparison
2.4.0,Get the labels/weights
2.4.0,Normalize shapes
2.4.0,Remove labels with zero weights
2.4.0,Note that we may have 0 elements of a given class since we remove those
2.4.0,labels with zero weight.
2.4.0,this works because y is 1D
2.4.0,This is the right ratio since int(N/num_c) * num_c \approx N
2.4.0,for all classes
2.4.0,Flattening is safe because of shape check above
2.4.0,Hack to allow for easy unpickling:
2.4.0,http://stefaanlippens.net/pickleproblem
2.4.0,Some transformation must happen
2.4.0,Add this case in to handle non-DiskDataset that should be written to disk
2.4.0,Note that transformers have to be undone in reversed order
2.4.0,Handle division by zero
2.4.0,Handle division by zero
2.4.0,Control for pathological case with no variance.
2.4.0,Handle case with 1 task correctly
2.4.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.4.0,Find the task dimension of z
2.4.0,Prevent broadcasting on wrong dimension
2.4.0,BalancingTransformer can only transform weights.
2.4.0,Compute weighting factors from dataset.
2.4.0,Handle 1-D case
2.4.0,Remove labels with zero weights
2.4.0,Note that we may have 0 elements of a given class since we remove those
2.4.0,labels with zero weight. This typically happens in multitask datasets
2.4.0,where some datapoints only have labels for some tasks.
2.4.0,this works because task_y is 1D
2.4.0,This is the right ratio since N_task/num_c * num_c = N_task
2.4.0,for all classes
2.4.0,Set to the class weight computed previously
2.4.0,Need this for transform_y
2.4.0,Handle 1D case
2.4.0,THis reshape is safe because of guard above.
2.4.0,map the indices to labels
2.4.0,generating batch of data by slicing similarity matrix
2.4.0,into 100*reference_dataset_length
2.4.0,concatenate batches of data together
2.4.0,highest similarity is 1: target is in the reference
2.4.0,use the following K points
2.4.0,"highest less than 1: target not in the reference, use top K points"
2.4.0,calculate matrix multiplicatin on slices
2.4.0,concatenate the slices together
2.4.0,list of calculation orders for DAGs
2.4.0,stemming from one specific atom in the molecule
2.4.0,starting from the adjacency list derived by graphconv featurizer
2.4.0,"number of atoms, also number of DAGs"
2.4.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.4.0,each step calculating graph features for one atom.
2.4.0,`max_atoms` is the maximum number of steps
2.4.0,each iteration generates the DAG starting from atom with index `count`
2.4.0,"list of lists, elements represent the calculation orders"
2.4.0,for atoms in the current graph
2.4.0,starting from the target atom with index `count`
2.4.0,flags of whether the atom is already included in the DAG
2.4.0,atom `count` is in the DAG
2.4.0,recording number of radial propagation steps
2.4.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.4.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.4.0,will be added in the second loop(radial=1).
2.4.0,atoms i-bond away will be added in i-th loop
2.4.0,"when molecules have separate parts, starting from one part,"
2.4.0,it is not possible to include all atoms.
2.4.0,this break quit the loop when going into such condition
2.4.0,reinitialize targets for next iteration
2.4.0,atoms connected to current_atom
2.4.0,generate the dependency map of current DAG
2.4.0,atoms connected to `current_atoms`(and not included in the DAG)
2.4.0,"are added, and will be the `current_atoms` for next iteration."
2.4.0,"DAG starts from the target atom, calculation should go in reverse"
2.4.0,`edge[1]` is the parent of `edge[0]`
2.4.0,"after this loop, `parents[i]` includes all parents of atom i"
2.4.0,manually adding the atom index into its parents list
2.4.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.4.0,atoms with less parents(farther from the target atom) come first.
2.4.0,"graph features of atoms without parents will be first calculated,"
2.4.0,then atoms with more parents can be calculated in order
2.4.0,based on previously calculated graph features.
2.4.0,target atom of this DAG will be calculated in the last step
2.4.0,padding with `max_atoms`
2.4.0,padding
2.4.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.4.0,which is a max_atoms * max_atoms numpy array after padding
2.4.0,Calculate pairwise distance
2.4.0,Masking for valid atom index
2.4.0,Cutoff with threshold Rc
2.4.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.4.0,flake8: noqa
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a y transformer
2.4.0,Check w is unchanged since this is a y transformer
2.4.0,Check y is now a logarithmic version of itself
2.4.0,Check that untransform does the right thing.
2.4.0,Check ids are unchanged.
2.4.0,Check y is unchanged since this is a X transformer
2.4.0,Check w is unchanged since this is a y transformer
2.4.0,Check y is now a logarithmic version of itself
2.4.0,Check that untransform does the right thing.
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a y transformer
2.4.0,Check w is unchanged since this is a y transformer
2.4.0,Check y is now a logarithmic version of itself
2.4.0,Check that untransform does the right thing.
2.4.0,Tests logarithmic data transformer with selection.
2.4.0,Check ids are unchanged.
2.4.0,Check y is unchanged since this is a X transformer
2.4.0,Check w is unchanged since this is a y transformer
2.4.0,Check y is now a logarithmic version of itself
2.4.0,Check that untransform does the right thing.
2.4.0,Check ids are unchanged.
2.4.0,Check y is unchanged since this is an X transformer
2.4.0,Check w is unchanged since this is an X transformer
2.4.0,Check X is now holding the proper values when sorted.
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is an y transformer
2.4.0,Check w is unchanged since this is an y transformer
2.4.0,Check y is now holding the proper values when sorted.
2.4.0,Check that untransform does the right thing.
2.4.0,Check ids are unchanged.
2.4.0,Check y is unchanged since this is an X transformer
2.4.0,Check w is unchanged since this is an X transformer
2.4.0,Check X is now holding the proper values when sorted.
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a y transformer
2.4.0,Check w is unchanged since this is a y transformer
2.4.0,Check y is now holding the proper values when sorted.
2.4.0,Check ids are unchanged before and after transformation
2.4.0,Check X is unchanged since transform_y is true
2.4.0,Check w is unchanged since transform_y is true
2.4.0,Check minimum and maximum values of transformed y are 0 and 1
2.4.0,Check untransform works correctly
2.4.0,Check ids are unchanged before and after transformation
2.4.0,Check X is unchanged since transform_y is true
2.4.0,Check w is unchanged since transform_y is true
2.4.0,Check minimum and maximum values of transformed y are 0 and 1
2.4.0,Test if dimensionality expansion is handled correctly by untransform
2.4.0,Check ids are unchanged before and after transformation
2.4.0,Check X is unchanged since transform_y is true
2.4.0,Check w is unchanged since transform_y is true
2.4.0,Check minimum and maximum values of transformed y are 0 and 1
2.4.0,Check untransform works correctly
2.4.0,Load mini log-solubility dataset.
2.4.0,The transformer generates n DAGs for a molecule with n
2.4.0,"atoms. These are denoted the ""parents"""
2.4.0,extract only the images (no need of the labels)
2.4.0,reshaping the vector to image
2.4.0,Check Blurring
2.4.0,Check center crop
2.4.0,Check crop
2.4.0,Check convert2gray
2.4.0,Check rotation
2.4.0,Some more test cases for flip
2.4.0,Check flip
2.4.0,Check Scales
2.4.0,Check shift
2.4.0,check gaussian noise
2.4.0,check salt and pepper noise
2.4.0,Check median filter
2.4.0,transforming y should raise an exception
2.4.0,transforming w should raise an exception
2.4.0,transforming X should be okay
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a y transformer
2.4.0,Check w is unchanged since this is a y transformer
2.4.0,"Check that y_t has zero mean, unit std."
2.4.0,Check that untransform does the right thing.
2.4.0,Check ids are unchanged.
2.4.0,Check y is unchanged since this is a X transformer
2.4.0,Check w is unchanged since this is a y transformer
2.4.0,"Check that X_t has zero mean, unit std."
2.4.0,np.set_printoptions(threshold='nan')
2.4.0,Entries with zero std are not normalized
2.4.0,Check that untransform does the right thing.
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a w transformer
2.4.0,Check y is unchanged since this is a w transformer
2.4.0,Assert that entries with zero weight retain zero weight
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a w transformer
2.4.0,Check y is unchanged since this is a w transformer
2.4.0,Assert that entries with zero weight retain zero weight
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a w transformer
2.4.0,Check y is unchanged since this is a w transformer
2.4.0,Assert that entries with zero weight retain zero weight
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a w transformer
2.4.0,Check y is unchanged since this is a w transformer
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is a w transformer
2.4.0,Check y is unchanged since this is a w transformer
2.4.0,Assert that entries with zero weight retain zero weight
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,Check ids are unchanged.
2.4.0,Check y is unchanged since this is an X transformer
2.4.0,Check w is unchanged since this is an X transformer
2.4.0,Check X is now holding the proper values in each column.
2.4.0,Check ids are unchanged.
2.4.0,Check X is unchanged since this is an X transformer
2.4.0,Check w is unchanged since this is an X transformer
2.4.0,Check y is now holding the proper values in each column.
2.4.0,Check that untransform does the right thing.
2.4.0,Check that we have length 8 now with duplication
2.4.0,Check shapes
2.4.0,Check that we have 4 positives and 4 negatives
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,Note that nothing should change in this dataset since weights balance!
2.4.0,Check that still we have length 6
2.4.0,Check shapes
2.4.0,Check that we have 2 positives and 4 negatives
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,Check that we have length 8 now with duplication
2.4.0,Check shapes
2.4.0,Check that we have 4 positives and 4 negatives
2.4.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.4.0,6-1 imbalance in favor of class 0
2.4.0,Check that we have length 30 now with duplication
2.4.0,Check shapes
2.4.0,Check that we have 6 of each class
2.4.0,Check that sum of all class weights is equal by comparing to 0 weight
2.4.0,Note class imbalance. This will round to 2x duplication for 1
2.4.0,Check that we have length 13 now with duplication
2.4.0,Check shapes
2.4.0,Check that we have 6 positives and 7 negatives
2.4.0,################################################################
2.4.0,save.py is out of date. You should not import any functions from here.
2.4.0,################################################################
2.4.0,flake8: noqa
2.4.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""__iter__"" (not iterable)"
2.4.0,Walk through the original file and extract ATOM/HETATM lines and
2.4.0,add PDBQT charge annotations.
2.4.0,Remove rotatable bonds from this molecule
2.4.0,Get the connected components now that the rotatable bonds have
2.4.0,been removed.
2.4.0,The root is the largest connected component.
2.4.0,Write the root component
2.4.0,"We've looked at the root, so take note of that"
2.4.0,Compute partial charges on molecule if RDKit Mol
2.4.0,indices to atoms to keep
2.4.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.4.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.4.0,nonzero contact.
2.4.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.4.0,TODO: This is duplicated! Clean up
2.4.0,Updates charges in place
2.4.0,initial embedding
2.4.0,minimization and pruning
2.4.0,always keep lowest-energy conformer
2.4.0,discard conformers after max_conformers is reached
2.4.0,get RMSD to selected conformers
2.4.0,discard conformers within the RMSD threshold
2.4.0,create a new molecule to hold the chosen conformers
2.4.0,this ensures proper conformer IDs and energy-based ordering
2.4.0,False here specifies that water is to be removed
2.4.0,Updates charges in place
2.4.0,TODO: This is wrong. Should return all molecules
2.4.0,Ideally we should catch AtomValenceException but Travis seems to choke on it for some reason.
2.4.0,This updates in place
2.4.0,indices of atoms to keep
2.4.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.4.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.4.0,nonzero contact.
2.4.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.4.0,####################################################
2.4.0,Compute partial charges on molecule if rdkit
2.4.0,####################################################
2.4.0,Number of voxels per one edge of box to voxelize.
2.4.0,"FIXME: Argument 1 of ""__eq__"" is incompatible with supertype ""object"""
2.4.0,If interval1 < interval2 entirely
2.4.0,If interval2 < interval1 entirely
2.4.0,Each triangle in the simplices is a set of 3 atoms from
2.4.0,coordinates which forms the vertices of an exterior triangle on
2.4.0,the convex hull of the macromolecule.
2.4.0,Points is the set of atom coordinates that make up this
2.4.0,triangular face on the convex hull
2.4.0,Let's extract x/y/z coords for this face
2.4.0,Let's compute min/max points
2.4.0,"Nitrogen has atomic number 7, and oxygen 8."
2.4.0,If atom is a hydrogen
2.4.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.4.0,If atom is a hydrogen
2.4.0,"O-H distance is 0.96 A, N-H is 1.01 A. See http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html"
2.4.0,if ring from mol1 is aromatic
2.4.0,...and atom from mol2 is a cation
2.4.0,if angle and distance are correct
2.4.0,count atoms forming a contact
2.4.0,if ring is aromatic
2.4.0,"save its indices, center, and normal"
2.4.0,remember mol1-mol2 pairs we already counted
2.4.0,"if this pair is new, count atoms forming a contact"
2.4.0,"if this pair is new, count atoms forming a contact"
2.4.0,find interacting rings from mol1 and cations from mol2
2.4.0,find interacting cations from mol1 and rings from mol2
2.4.0,merge counters
2.4.0,Make sure input is a list
2.4.0,FIXME: Incompatible types in assignment
2.4.0,"FIXME: Argument 1 to ""enumerate"" has incompatible type"
2.4.0,Ensure that metric is wrapped in a list.
2.4.0,This case checks if input is a function then wraps a
2.4.0,dc.metrics.Metric object around it
2.4.0,Process input metrics
2.4.0,Compute multitask metrics
2.4.0,We use y/w to aggregate labels/weights across generator.
2.4.0,This is a KerasModel.
2.4.0,Some datasets have weights
2.4.0,Process predictions and populate y/w lists
2.4.0,Combine labels/weights
2.4.0,Undo data transformations.
2.4.0,Compute multitask metrics
2.4.0,the line has format
2.4.0,REMARK VINA RESULT: score ...
2.4.0,There is only 1 such line per model so we can append it
2.4.0,"FIXME: Item ""None"" of ""Optional[List[str]]"" has no attribute ""append"""
2.4.0,Apply common fixes to PDB files
2.4.0,Optimize ligand
2.4.0,flake8: noqa
2.4.0,The number of elements to print for dataset ids/tasks
2.4.0,"If a dataset contains more than this number of elements, it won't"
2.4.0,print any dataset ids
2.4.0,An activation function for a Keras layer: either a TensorFlow function or the name of a standard activation
2.4.0,"A loss function for use with KerasModel or TorchModel: f(outputs, labels, weights)"
2.4.0,"A single value of some type, or multiple values of that type"
2.4.0,The shape of a NumPy array
2.4.0,type of RDKit object
2.4.0,type of Pymatgen object
2.4.0,Generate a random temporary file name
2.4.0,Ensure the file is created
2.4.0,Open the file in the given mode
2.4.0,Tasks are either in .sdf.csv file or in the .sdf file itself
2.4.0,Structures are stored in .sdf file
2.4.0,Reset aggregator
2.4.0,Handle final leftovers for this file
2.4.0,First line of user-specified CSV *must* be header.
2.4.0,"If gzipped, need to compute extension again"
2.4.0,First line of user-specified CSV *must* be header.
2.4.0,The label encoder is given characters for ACGTN
2.4.0,Peak at the first sequence to get the length of the sequence.
2.4.0,init an one-hot vector
2.4.0,"If include_unknown_set is True, set the last index is 1."
2.4.0,################################################################
2.4.0,atom (node) featurization
2.4.0,################################################################
2.4.0,################################################################
2.4.0,bond (edge) featurization
2.4.0,################################################################
2.4.0,One sequence has length longer than others. This should throw a
2.4.0,ValueError.
2.4.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.4.0,Loosening atol to see if tests stop failing sporadically
2.4.0,string set
2.4.0,integer set
2.4.0,include_unknown_set is False
2.4.0,include_unknown_set is True
2.4.0,check unknown atoms
2.4.0,check original set
2.4.0,"Generally, =O behaves as an electron acceptor"
2.4.0,we must compute partial charges before using `get_atom_partial_charge`
2.4.0,The C-N bond is a single bond
2.4.0,TODO test more formats for ligand
2.4.0,adding hydrogens and charges is tested in dc.utils
2.4.0,self.ligand_file is for 3ws9_ligand.sdf
2.4.0,simple flat ring
2.4.0,self.cycle4.Compute2DCoords()
2.4.0,load and sanitize two real molecules
2.4.0,parallel normals
2.4.0,perpendicular normals
2.4.0,too far away
2.4.0,perpendicular normals
2.4.0,parallel normals
2.4.0,too far away
2.4.0,order of the molecules shouldn't matter
2.4.0,with this criteria we should find both types of stacking
2.4.0,parallel normals
2.4.0,perpendicular normals
2.4.0,too far away
2.4.0,def test_compute_cation_pi(self):
2.4.0,"# TODO(rbharath): find better example, currently dicts are empty"
2.4.0,"dicts1 = compute_cation_pi(self.prot, self.lig)"
2.4.0,"dicts2 = compute_cation_pi(self.lig, self.prot)"
2.4.0,"TODO find better example, currently dicts are empty"
2.4.0,TODO test more formats for ligand
2.4.0,Test on RDKit
2.4.0,3D vector with unit length
2.4.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.4.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.4.0,check if correct distance metric was used
2.4.0,TODO test more formats for ligand
2.4.0,Construct a random class probability matrix
2.4.0,Construct a random class probability matrix
2.4.0,"Note that since no name as provided, metrics are index by order"
2.4.0,given.
2.4.0,"Note that since no name as provided, metrics are index by order"
2.4.0,given.
2.4.0,"Note that since no name as provided, metrics are index by order"
2.4.0,given.
2.4.0,"Note that since no name as provided, metrics are index by order"
2.4.0,given.
2.4.0,TODO: Fix this case with correct thresholding
2.4.0,TODO: Fix this case with correct thresholding
2.4.0,There are 4 faces to the shape created by coords
2.4.0,flake8: noqa
2.4.0,Get the degree id list (which corrects for min_deg)
2.4.0,Get the size of each degree block
2.4.0,Get the the start indices for items in each block
2.4.0,Get the node indices when they are reset when the degree changes
2.4.0,Convert to numpy array
2.4.0,Reorder old atom_features
2.4.0,Reorder old deg lists
2.4.0,Sort membership
2.4.0,Create old to new dictionary. not exactly intuitive
2.4.0,Reorder adjacency lists
2.4.0,Get numpy version of degree list for indexing
2.4.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.4.0,Parse as deg separated
2.4.0,Get indices corresponding to the current degree
2.4.0,Extract and save adjacency list for the current degree
2.4.0,Construct the slice information
2.4.0,Get the cumulative indices after the first index
2.4.0,Set indices with zero sized slices to zero to avoid indexing errors
2.4.0,TODO(rbharath): Can this be removed?
2.4.0,Use random insted of zeros to prevent weird issues with summing to zero
2.4.0,"Combine the features, then sort them by (atom_degree, mol_index)"
2.4.0,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.4.0,Create a map from the original atom indices within each molecule to the
2.4.0,indices in the combined object.
2.4.0,Sort all atoms by degree.
2.4.0,"Get the size of each atom list separated by molecule id, then by degree"
2.4.0,Get the final size of each degree block
2.4.0,"Get the index at which each degree starts, not resetting after each degree"
2.4.0,And not stopping at any specific molecule
2.4.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.4.0,first column telling the start indices of each degree block and the
2.4.0,second colum telling the size of each degree block
2.4.0,Determine the membership (atom i belongs to molecule membership[i])
2.4.0,Initialize the new degree separated adjacency lists
2.4.0,Update the old adjacency lists with the new atom indices and then combine
2.4.0,all together
2.4.0,Iterate through all the molecules
2.4.0,Get the adjacency lists for this molecule and current degree id
2.4.0,"Correct all atom indices to the final indices, and then save the"
2.4.0,results into the new adjacency lists
2.4.0,Increment once row is done
2.4.0,Get the final aggregated molecule
2.4.0,"Requriments - transformers, tokenizers"
2.4.0,"Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset."
2.4.0,The vocab may be expanded in the near future
2.4.0,"|-|\+|\\|\/|:|~|@|\?|>>?|\*|\$|\%[0-9]{2}|[0-9])"""""""
2.4.0,add vocab_file dict
2.4.0,"unk_token=""[UNK]"","
2.4.0,"sep_token=""[SEP]"","
2.4.0,"pad_token=""[PAD]"","
2.4.0,"cls_token=""[CLS]"","
2.4.0,"mask_token=""[MASK]"","
2.4.0,take into account special tokens in max length
2.4.0,flake8: noqa
2.4.0,Initalize with 1
2.4.0,Replace the hybridization
2.4.0,global possible_hybridization_list
2.4.0,Allow 0 index to correspond to null molecule 1
2.4.0,Correct for null
2.4.0,"print(6-k-1, id)"
2.4.0,Correct for last one
2.4.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.4.0,"Handle edge case of self-pairs (i, i)"
2.4.0,Increment by 1 since we don't want 0-indexing
2.4.0,"This creates a matrix of shape (2, num_pairs)"
2.4.0,Get mapping
2.4.0,first `bt_len` features are bond features(if applicable)
2.4.0,For ring pairs outside max pairs distance continue
2.4.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.4.0,graph distance between two atoms
2.4.0,distance is a matrix of 1-hot encoded distances for all atoms
2.4.0,For ring pairs outside max pairs distance continue
2.4.0,Euclidean distance between atoms
2.4.0,atoms `radial` bonds away from `a1`
2.4.0,atoms less than `radial` bonds away
2.4.0,find atoms `radial`+1 bonds away
2.4.0,Get the node features
2.4.0,Stack nodes into an array
2.4.0,Get bond lists with reverse edges included
2.4.0,Get canonical adjacency list
2.4.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.4.0,only support datasets providing Cartesian coordinates)
2.4.0,Set dtype
2.4.0,If includes explicit hydrogens
2.4.0,If uses use_chirality
2.4.0,Atom features
2.4.0,Stack nodes into an array
2.4.0,Get bond lists
2.4.0,Get canonical adjacency list
2.4.0,Calculate pair features
2.4.0,TODO (VIGS25): Complete the description
2.4.0,Handle loading failures which return None
2.4.0,Fit atomic conv model
2.4.0,Add the Atomic Convolution layers to fetches
2.4.0,Extract the atomic convolution features
2.4.0,flake8: noqa
2.4.0,base classes for featurizers
2.4.0,molecule featurizers
2.4.0,complex featurizers
2.4.0,material featurizers
2.4.0,for str
2.4.0,for list
2.4.0,validation
2.4.0,skip list
2.4.0,skip path string
2.4.0,main logic
2.4.0,Special case handling of single molecule
2.4.0,Convert iterables to list
2.4.0,"mol must be a RDKit Mol object, so parse a SMILES"
2.4.0,"SMILES is unique, so set a canonical order of atoms"
2.4.0,"FIXME: Signature of ""featurize"" incompatible with supertype ""Featurizer"""
2.4.0,atom_name is of format RESX-ATOMTYPE
2.4.0,where X is a 1 to 4 digit number
2.4.0,validate params
2.4.0,This assumes that the edge features for self loops are full-zero tensors
2.4.0,In the future we may want to support featurization for self loops
2.4.0,stack features
2.4.0,"before stacking edge_features or node_pos_features,"
2.4.0,we should check whether these are None or not
2.4.0,create new edge index
2.4.0,graph_index indicates which nodes belong to which graph
2.4.0,Setup image
2.4.0,Compute bond properties
2.4.0,Compute atom properties
2.4.0,Setup image
2.4.0,Compute bond properties
2.4.0,Compute atom properties
2.4.0,Reshape done for proper broadcast
2.4.0,"Reshapes, and axes manipulations to facilitate vector processing."
2.4.0,Draw a line between the two atoms.
2.4.0,"The coordinates of this line, are indicated in line_coords"
2.4.0,Turn the line coordinates into image positions
2.4.0,Set the bond line coordinates to the bond property used.
2.4.0,Turn atomic coordinates into image positions
2.4.0,Set the atom positions in image to different atomic properties in channels
2.4.0,Check whether num_confs >=1 or not
2.4.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.4.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.4.0,consistent with most QM software packages.
2.4.0,generate SMILES for fragments
2.4.0,Extend shorter strings with padding
2.4.0,Padding before and after
2.4.0,validation
2.4.0,load pretrained models
2.4.0,convert errors to zero
2.4.0,flake8: noqa
2.4.0,If partial charges were not computed
2.4.0,construct atom (node) feature
2.4.0,construct edge (bond) index
2.4.0,add edge list considering a directed graph
2.4.0,construct edge (bond) feature
2.4.0,initialize
2.4.0,check initialization
2.4.0,"`(1, max_atoms, max_atoms)` -> `(max_atoms, max_atoms)`"
2.4.0,Check whether num_confs >=1 or not
2.4.0,Convert AtomPositions from Angstrom to bohr (atomic units)
2.4.0,"`(1, max_atoms)` -> `(max_atoms,)`"
2.4.0,Get full N x N SCM
2.4.0,flake8: noqa
2.4.0,load atom_init.json
2.4.0,check whether the atom feature exists or not
2.4.0,construct bi-directed graph
2.4.0,Increase dimension of distance tensor and apply filter
2.4.0,We compute pairwise contact fingerprints
2.4.0,We compute pairwise contact fingerprints
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.4.0,We compute pairwise contact fingerprints
2.4.0,"rdks = [frag1[1], frag2[1]]"
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.4.0,We compute pairwise contact fingerprints
2.4.0,"distances = compute_pairwise_distances(frag1[0], frag2[0])"
2.4.0,"rdks = [frag1[1], frag2[1]]"
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.4.0,We compute pairwise contact fingerprints
2.4.0,"rdks = [frag1[1], frag2[1]]"
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 2) so we should concatenate on the last axis."
2.4.0,We compute pairwise contact fingerprints
2.4.0,"centroid = compute_contact_centroid(fragments, cutoff=self.cutoff)"
2.4.0,We compute pairwise contact fingerprints
2.4.0,"frag1_xyz = subtract_centroid(frag1[0], centroid)"
2.4.0,"frag2_xyz = subtract_centroid(frag2[0], centroid)"
2.4.0,"xyzs = [frag1_xyz, frag2_xyz]"
2.4.0,"rdks = [frag1[1], frag2[1]]"
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.4.0,We compute pairwise contact fingerprints
2.4.0,"rdks = [frag1[1], frag2[1]]"
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.4.0,flake8: noqa
2.4.0,if ring is aromatic
2.4.0,"save its indices, center, and normal"
2.4.0,remember protein-ligand pairs we already counted
2.4.0,"if this pair is new, count atoms forming a contact"
2.4.0,"if this pair is new, count atoms forming a contact"
2.4.0,if ring from mol1 is aromatic
2.4.0,...and atom from mol2 is a cation
2.4.0,if angle and distance are correct
2.4.0,count atoms forming a contact
2.4.0,find interacting rings from protein and cations from ligand
2.4.0,find interacting cations from protein and rings from ligand
2.4.0,merge counters
2.4.0,TODO(LESWING)
2.4.0,check if user tries to set removed arguments
2.4.0,list of features that require sanitized molecules
2.4.0,not implemented featurization types
2.4.0,default values
2.4.0,update with cutoffs specified by the user
2.4.0,"each entry is a tuple (is_flat, feature_name)"
2.4.0,list of features that cannot be calculated with specified parameters
2.4.0,this list is used to define <flat/voxel/all>_combined subset
2.4.0,parse provided feature types
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,TODO(rbharath): Is this squeeze OK?
2.4.0,flake8: noqa
2.4.0,"contacts is of form (x_coords, y_coords), a tuple of 2 lists"
2.4.0,"contacts[0] is the x_coords, that is the frag1 atoms that have"
2.4.0,nonzero contact.
2.4.0,"contacts[1] is the y_coords, the frag2 atoms with nonzero contacts"
2.4.0,We compute pairwise contact fingerprints
2.4.0,Get coordinates
2.4.0,We compute pairwise contact fingerprints
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge,"
2.4.0,"voxels_per_edge, num_feat) so we should concatenate on the last"
2.4.0,axis.
2.4.0,Type of data created by this featurizer
2.4.0,TODO(rbharath): Should this return a list?
2.4.0,Type of data created by this featurizer
2.4.0,Currently handles loading failures by returning None
2.4.0,TODO: Is there a better handling procedure?
2.4.0,We compute pairwise contact fingerprints
2.4.0,Get coordinates
2.4.0,"distances = compute_pairwise_distances(prot_xyz, lig_xyz)"
2.4.0,We compute pairwise contact fingerprints
2.4.0,"Features are of shape (voxels_per_edge, voxels_per_edge, voxels_per_edge, 1) so we should concatenate on the last axis."
2.4.0,TODO test more formats for ligand
2.4.0,TODO test more formats for ligand
2.4.0,with one conformer
2.4.0,with multiple conformers
2.4.0,include explicit hydrogens
2.4.0,with one conformer
2.4.0,with multiple conformers
2.4.0,include explicit hydrogens
2.4.0,"Requirements - transformers, tokenizers"
2.4.0,"assert ""C1=CC=CN=C1"""
2.4.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.4.0,"assert ""C1=CC=CN=C1"""
2.4.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.4.0,"assert ""C1=CC=CN=C1"""
2.4.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.4.0,"assert ""C1=CC=CN=C1"""
2.4.0,"assert ""O=C(NCc1cc(OC)c(O)cc1)CCCC/C=C/C(C)C"""
2.4.0,check for separate count and SMILES entries for each fragment
2.4.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.4.0,def test_hydrogen_bond_counter():
2.4.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.4.0,"protein_file = os.path.join(current_dir, 'data',"
2.4.0,'3ws9_protein_fixer_rdkit.pdb')
2.4.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.4.0,
2.4.0,cutoff = 4.5
2.4.0,featurizer = dc.feat.HydrogenBondCounter(cutoff=cutoff)
2.4.0,"features, failures = featurizer.featurize([ligand_file], [protein_file])"
2.4.0,# TODO: Add shape test
2.4.0,
2.4.0,
2.4.0,"# TODO: This is failing, something about the hydrogen bond counting?"
2.4.0,def test_hydrogen_bond_voxelizer():
2.4.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.4.0,"protein_file = os.path.join(current_dir, 'data',"
2.4.0,'3ws9_protein_fixer_rdkit.pdb')
2.4.0,"ligand_file = os.path.join(current_dir, 'data', '3ws9_ligand.sdf')"
2.4.0,
2.4.0,cutoff = 4.5
2.4.0,box_width = 16
2.4.0,voxel_width = 1.0
2.4.0,voxelizer = dc.feat.HydrogenBondVoxelizer(
2.4.0,"cutoff=cutoff, box_width=box_width, voxel_width=voxel_width)"
2.4.0,"features, failures = voxelizer.featurize([ligand_file], [protein_file])"
2.4.0,# TODO: Add shape test
2.4.0,TODO test more formats for ligand
2.4.0,adding hydrogens and charges is tested in dc.utils
2.4.0,3D vector with unit length
2.4.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.4.0,check if distances do not change
2.4.0,check if it works for molecules with different numbers of atoms
2.4.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.4.0,check if correct distance metric was used
2.4.0,"20 points with coords between -5 and 5, centered at 0"
2.4.0,indices are positive
2.4.0,coordinates were properly translated and scaled
2.4.0,for coordinates outside of the box function should properly transform them
2.4.0,to indices and warn the user
2.4.0,"TODO check if function warns. There is assertWarns method in unittest,"
2.4.0,but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
2.4.0,"20 points with coords between -5 and 5, centered at 0"
2.4.0,3 pairs of indices
2.4.0,simple flat ring
2.4.0,load and sanitize two real molecules
2.4.0,FIXME might break with different version of rdkit
2.4.0,FIXME might break with different version of rdkit
2.4.0,parallel normals
2.4.0,perpendicular normals
2.4.0,too far away
2.4.0,perpendicular normals
2.4.0,parallel normals
2.4.0,too far away
2.4.0,order of the molecules shouldn't matter
2.4.0,with this criteria we should find both types of stacking
2.4.0,parallel normals
2.4.0,perpendicular normals
2.4.0,too far away
2.4.0,"TODO find better example, currently dicts are empty"
2.4.0,"TODO find better example, currently dicts are empty"
2.4.0,TODO test if dict contains smiles
2.4.0,check if results are the same if we provide precomputed distances
2.4.0,...but first check if we actually got two dicts
2.4.0,check if we get less features with smaller distance cutoff
2.4.0,ligands are typically small so all atoms might be present
2.4.0,check if using different ecfp_degree changes anything
2.4.0,TODO upperbound?
2.4.0,test if default parameters work
2.4.0,check if use-case from examples works
2.4.0,test if input is flattened when flat features are used
2.4.0,test voxel features
2.4.0,test flat features
2.4.0,check if aromatic features are ignores if sanitize=False
2.4.0,"protein is too big for the box, some features should be missing"
2.4.0,whole ligand should fit in the box
2.4.0,not support array style inputs
2.4.0,check convert function
2.4.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.4.0,of degree 1 (connected only to central nitrogen).
2.4.0,5 atoms in compound
2.4.0,Get the adjacency lists grouped by degree
2.4.0,The 4 outer atoms connected to central nitrogen
2.4.0,Central nitrogen connected to everything else.
2.4.0,Only one carbon
2.4.0,"No bonds, so degree adjacency lists are empty"
2.4.0,3 carbonds in alkane
2.4.0,Outer two carbonds are connected to central carbon
2.4.0,Central carbon connected to outer two
2.4.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.4.0,max num atoms instead of exact.
2.4.0,Cutoff in angstroms
2.4.0,untranform
2.4.0,untranform
2.4.0,untranform
2.4.0,Do a manual distance computation and make
2.4.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.4.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.4.0,Do a manual distance computation and ensure that selected neighbor is
2.4.0,closest since we set max_num_neighbors = 1
2.4.0,TODO(rbharath): This test will be uncommented in the next PR up on the docket.
2.4.0,def test_full_complex_featurization(self):
2.4.0,"""""""Unit test for ComplexNeighborListFragmentAtomicCoordinates."""""""
2.4.0,dir_path = os.path.dirname(os.path.realpath(__file__))
2.4.0,"ligand_file = os.path.join(dir_path, ""data/3zso_ligand_hyd.pdb"")"
2.4.0,"protein_file = os.path.join(dir_path, ""data/3zso_protein.pdb"")"
2.4.0,"# Pulled from PDB files. For larger datasets with more PDBs, would use"
2.4.0,# max num atoms instead of exact.
2.4.0,frag1_num_atoms = 44  # for ligand atoms
2.4.0,frag2_num_atoms = 2336  # for protein atoms
2.4.0,complex_num_atoms = 2380  # in total
2.4.0,max_num_neighbors = 4
2.4.0,# Cutoff in angstroms
2.4.0,neighbor_cutoff = 4
2.4.0,complex_featurizer = ComplexNeighborListFragmentAtomicCoordinates(
2.4.0,"frag1_num_atoms, frag2_num_atoms, complex_num_atoms, max_num_neighbors,"
2.4.0,neighbor_cutoff)
2.4.0,"(frag1_coords, frag1_neighbor_list, frag1_z, frag2_coords,"
2.4.0,"frag2_neighbor_list, frag2_z, complex_coords,"
2.4.0,"complex_neighbor_list, complex_z) = complex_featurizer._featurize_complex("
2.4.0,"ligand_file, protein_file)"
2.4.0,
2.4.0,"assert frag1_coords.shape == (frag1_num_atoms, 3)"
2.4.0,self.assertEqual(
2.4.0,"sorted(list(frag1_neighbor_list.keys())), list(range(frag1_num_atoms)))"
2.4.0,"self.assertEqual(frag1_z.shape, (frag1_num_atoms,))"
2.4.0,
2.4.0,"self.assertEqual(frag2_coords.shape, (frag2_num_atoms, 3))"
2.4.0,self.assertEqual(
2.4.0,"sorted(list(frag2_neighbor_list.keys())), list(range(frag2_num_atoms)))"
2.4.0,"self.assertEqual(frag2_z.shape, (frag2_num_atoms,))"
2.4.0,
2.4.0,"self.assertEqual(complex_coords.shape, (complex_num_atoms, 3))"
2.4.0,self.assertEqual(
2.4.0,"sorted(list(complex_neighbor_list.keys())),"
2.4.0,list(range(complex_num_atoms)))
2.4.0,"self.assertEqual(complex_z.shape, (complex_num_atoms,))"
2.4.0,Carbon
2.4.0,Test distance 1
2.4.0,Test distance 2
2.4.0,Test alkane
2.4.0,Test distance 1
2.4.0,3 self connections and 2 bonds which are both counted twice because of
2.4.0,symmetry for 7 total
2.4.0,Test distance 2
2.4.0,Everything is connected at this distance
2.4.0,Test alkane
2.4.0,Test distance infinity
2.4.0,Everything is connected at this distance
2.4.0,Test pentane
2.4.0,Test distance infinity
2.4.0,Everything is connected at this distance
2.4.0,Only one carbon
2.4.0,Test feature sizes
2.4.0,"No bonds, so only 1 pair feature (for the self interaction)"
2.4.0,Only 4 atoms
2.4.0,Test feature sizes for chirality
2.4.0,3 carbonds in alkane
2.4.0,Test feature sizes
2.4.0,Should be a 3x3 interaction grid
2.4.0,mol_list = featurizer.featurize(mols)
2.4.0,mol = mol_list[0]
2.4.0,3 carbonds in alkane
2.4.0,Test feature sizes
2.4.0,Should be a 7x14 interaction grid since there are 7 pairs within graph
2.4.0,distance 1 (3 self interactions plus 2 bonds counted twice because of
2.4.0,symmetry)
2.4.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.4.0,of degree 1 (connected only to central nitrogen).
2.4.0,import rdkit.Chem
2.4.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.4.0,5 atoms in compound
2.4.0,Test feature sizes
2.4.0,Should be a 3x3 interaction grid
2.4.0,Artificial feature array.
2.4.0,0 atoms of degree 0
2.4.0,0 atoms of degree 1
2.4.0,4 atoms of degree 2
2.4.0,0 atoms of degree 3
2.4.0,0 atoms of degree 4
2.4.0,0 atoms of degree 5
2.4.0,0 atoms of degree 6
2.4.0,0 atoms of degree 7
2.4.0,0 atoms of degree 8
2.4.0,0 atoms of degree 9
2.4.0,0 atoms of degree 10
2.4.0,atom 4 has 0 neighbors
2.4.0,atom 0 has 2 neighbors
2.4.0,atom 1 has 2 neighbors
2.4.0,atom 2 has 2 neighbors
2.4.0,atom 3 has 3 neighbors.
2.4.0,Verify that atom features have been sorted by atom degree.
2.4.0,Sorting is done by atom degree as before. So the ordering goes
2.4.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.4.0,from new position to old position is
2.4.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.4.0,list respects this reordering and returns correct adjacency list.
2.4.0,First example molecule
2.4.0,Artificial feature array.
2.4.0,Second example molecule
2.4.0,Third example molecule
2.4.0,Test agglomerate molecule method
2.4.0,No atoms of degree 0
2.4.0,3 atoms of degree 1
2.4.0,8 atoms of degree 2
2.4.0,1 atom of degree 3
2.4.0,0 atoms of degree 4
2.4.0,0 atoms of degree 5
2.4.0,Check that atoms are only connected to themselves.
2.4.0,Check that there's one atom of each degree.
2.4.0,calculate coordinates
2.4.0,not zero values
2.4.0,assumes that every array is of the same dimension
2.4.0,rem_dataset is remaining portion of dataset
2.4.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.4.0,to k-1.
2.4.0,"FIXME: Incompatible types in assignment (expression has type ""Dataset"", variable has type ""DiskDataset"")"
2.4.0,validation
2.4.0,skip list
2.4.0,skip path string
2.4.0,main logic
2.4.0,for str
2.4.0,for list
2.4.0,dict is needed in case groups aren't strictly flattened or
2.4.0,hashed by something non-integer like
2.4.0,Figure out how many positive samples we want for each task in each dataset.
2.4.0,Assign the positive samples to datasets.  Since a sample may be positive
2.4.0,"on more than one task, we need to keep track of the effect of each added"
2.4.0,"sample on each task.  To try to keep everything balanced, we cycle through"
2.4.0,"tasks, assigning one positive sample for each one."
2.4.0,We have a sample that hasn't been assigned yet.  Assign it to
2.4.0,whichever set currently has the lowest fraction of its target for
2.4.0,this task.
2.4.0,The remaining samples are negative for all tasks.  Add them to fill out
2.4.0,each set to the correct total number.
2.4.0,"FIXME: Signature of ""k_fold_split"" incompatible with supertype ""Splitter"""
2.4.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.4.0,This can be generalized in the future with some common demoninator determination.
2.4.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.4.0,Append remaining examples to train
2.4.0,################################################################
2.4.0,Splitter for molecule datasets
2.4.0,################################################################
2.4.0,Sort by increasing MW
2.4.0,calcaulate scaffold sets
2.4.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.4.0,Compute fingerprints for all molecules.
2.4.0,Split into two groups: test training set and everything else.
2.4.0,Split the second group into validation and test sets.
2.4.0,Begin by assigning the first molecule to the first group.
2.4.0,Decide which group to assign a molecule to.
2.4.0,Identify the unassigned molecule that is least similar to everything in
2.4.0,the other group.
2.4.0,Add it to the group.
2.4.0,Update the data on unassigned molecules.
2.4.0,Sort from largest to smallest scaffold sets
2.4.0,################################################################
2.4.0,Not well supported splitters
2.4.0,################################################################
2.4.0,All datasets share features and identifiers by assumption.
2.4.0,flake8: noqa
2.4.0,basic splitter
2.4.0,molecule splitter
2.4.0,other splitter
2.4.0,################################################################
2.4.0,Removed API
2.4.0,################################################################
2.4.0,Note that the extra task goes to test
2.4.0,Number tasks per fold
2.4.0,Find the tasks that correspond to this test fold
2.4.0,Assert that all arrays look like they should
2.4.0,"task_type = ""regression"""
2.4.0,0 1 2 3 4 5 6 7 8 9
2.4.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.4.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.4.0,reshard() to handle this?
2.4.0,Verify lengths is 10/k == 2
2.4.0,Verify that compounds in this fold are subset of original compounds
2.4.0,Verify that no two folds have overlapping compounds.
2.4.0,Verify lengths is 10/k == 2
2.4.0,Verify that compounds in this fold are subset of original compounds
2.4.0,Verify that no two folds have overlapping compounds.
2.4.0,Verify lengths is 10/k == 2
2.4.0,Verify that compounds in this fold are subset of original compounds
2.4.0,Verify that no two folds have overlapping compounds.
2.4.0,Test singletask case.
2.4.0,The split index should partition dataset in half.
2.4.0,Test singletask case.
2.4.0,Test case where some weights are zero (i.e. masked)
2.4.0,Set half the positives to have zero weight
2.4.0,There are 10 nonzero actives.
2.4.0,"The split index should partition this into half, so expect 5"
2.4.0,The split index should partition the positives for each task roughly in half.
2.4.0,Mask half the examples
2.4.0,The split index should partition dataset in half.
2.4.0,Test singletask case.
2.4.0,Should have split cleanly in half (picked random seed to ensure this)
2.4.0,Check positives are correctly distributed
2.4.0,Test singletask case.
2.4.0,Should have made an 80/10/10 train/valid/test split of actives.
2.4.0,Verify lengths is 100/k == 20
2.4.0,Note: This wouldn't work for multitask str
2.4.0,assert len(fold_dataset) == n_samples/K
2.4.0,Verify that each fold has n_positives/K = 4 positive examples.
2.4.0,Verify that compounds in this fold are subset of original compounds
2.4.0,Verify that no two folds have overlapping compounds.
2.4.0,The amount of datapoints has to be the same
2.4.0,The number of scaffolds generated by the splitter
2.4.0,has to be smaller or equal than number of total molecules
2.4.0,Add the input features.
2.4.0,Add the convolutional layers
2.4.0,Create the inputs.
2.4.0,Create the generators.
2.4.0,Create the discriminators.
2.4.0,Compute the loss functions.
2.4.0,Create learnable weights for the generators and discriminators.
2.4.0,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.4.0,Compute the weighted errors
2.4.0,Add an entropy term to the loss.
2.4.0,Create the Keras model.
2.4.0,"Every call to fit_generator() will increment global_step, but we only"
2.4.0,"want it to get incremented once for the entire batch, so record the"
2.4.0,value and keep resetting it.
2.4.0,Train the discriminator.
2.4.0,Train the generator.
2.4.0,Write checkpoints and report progress.
2.4.0,Write out final results.
2.4.0,Chain of flows is also a normalizing flow
2.4.0,An instance of tfd.TransformedDistribution
2.4.0,TODO: Incompability between TF and TFP means that TF doesn't track
2.4.0,trainable variables in the flow; must override `_create_gradient_fn`
2.4.0,self._variables = self.flow.trainable_variables
2.4.0,"Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)"
2.4.0,"CrossEntropyLoss only supports (batch_size, classes, tasks)"
2.4.0,This is for API consistency
2.4.0,extended one of probabilites to binary distribution
2.4.0,extended one of probabilites to binary distribution
2.4.0,-*- coding: utf-8 -*-
2.4.0,"Shape (N_atoms, M_nbrs, ndim)"
2.4.0,"Shape (N_atoms, M_nbrs, ndim)"
2.4.0,"Shape (N_atoms, M_nbrs)"
2.4.0,Generate the nb_affine weights and biases
2.4.0,Extract atom_features
2.4.0,Extract graph topology
2.4.0,Sum all neighbors using adjacency matrix
2.4.0,Get collection of modified atom features
2.4.0,Obtain relevant atoms for this degree
2.4.0,Get self atoms
2.4.0,Apply hidden affine to relevant atoms and append
2.4.0,Determine the min_deg=0 case
2.4.0,Only use the self layer
2.4.0,Combine all atoms back into the list
2.4.0,Tensorflow correctly processes empty lists when using concat
2.4.0,"Sum along neighbors as well as self, and store"
2.4.0,Perform the mol gather
2.4.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.4.0,"self.max_degree, self.min_degree)"
2.4.0,Tensorflow correctly processes empty lists when using concat
2.4.0,Get self atoms
2.4.0,"There are no neighbors of this degree, so just create an empty tensor directly."
2.4.0,Expand dims
2.4.0,always deg-1 for deg_adj_lists
2.4.0,Extract graph topology
2.4.0,No other forget biases supported right now.
2.4.0,Taken from Keras code [citation needed]
2.4.0,"x is test set, xp is support set."
2.4.0,Get initializations
2.4.0,Process using attention
2.4.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.4.0,Generate new attention states
2.4.0,Support set lstm
2.4.0,Test lstm
2.4.0,Get initializations
2.4.0,Rename support
2.4.0,Process support xp using attention
2.4.0,Get linear combination of support set
2.4.0,Process test x using attention
2.4.0,Generate new support attention states
2.4.0,Generate new test attention states
2.4.0,Redefine
2.4.0,Number of rotatable bonds
2.4.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.4.0,a difference.
2.4.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.4.0,neighbors lists an argument instead of a part of this layer.
2.4.0,"Shape (N, M)"
2.4.0,"Shape (N, M)"
2.4.0,"Shape (N, M)"
2.4.0,Number of grid cells
2.4.0,TODO(rbharath): Support batching
2.4.0,"Shape (n_cells, ndim)"
2.4.0,"List of length N_atoms, each element of different length uniques_i"
2.4.0,"List of length N_atoms, each element of different length uniques_i"
2.4.0,"List of length N_atoms, each a tensor of shape"
2.4.0,"(uniques_i, ndim)"
2.4.0,Add phantom atoms that exist far outside the box
2.4.0,"List of length N_atoms, each of shape (1, ndim)"
2.4.0,TODO(rbharath): How does distance need to be modified here to
2.4.0,account for periodic boundary conditions?
2.4.0,List of length N_atoms each of shape (M_nbrs)
2.4.0,"N_atoms elts of size (M_nbrs,) each"
2.4.0,"Shape (N_atoms, 1)"
2.4.0,Find M_nbrs atoms closest to each cell
2.4.0,"Shape (n_cells, M_nbrs)"
2.4.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.4.0,"conditions, so does wrapround. O(constant)"
2.4.0,"Shape (n_cells, n_nbr_cells)"
2.4.0,"Shape (N_atoms, n_nbr_cells)"
2.4.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.4.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.4.0,"List of length N_atoms, each element length uniques_i"
2.4.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.4.0,element removed to remove self from list of neighbors. Need to verify
2.4.0,this holds more broadly or come up with robust alternative.
2.4.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.4.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.4.0,Shape (N_atoms*n_cells)
2.4.0,"Shape (n_cells, N_atoms)"
2.4.0,Find k atoms closest to this cell. Notice negative sign since
2.4.0,tf.nn.top_k returns *largest* not smallest.
2.4.0,"Tensor of shape (n_cells, M_nbrs)"
2.4.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.4.0,"Shape (N_atoms*n_cells, 1) after tile"
2.4.0,9 neighbors in 2-space
2.4.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.4.0,Number of cells for cube in 3-space is
2.4.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.4.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.4.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.4.0,the cube.
2.4.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.4.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.4.0,"Tile (a, a, a, b, b, b, etc.)"
2.4.0,"Tile (a, b, c, a, b, c, ...)"
2.4.0,N: Maximum number of atoms
2.4.0,M: Maximum number of neighbors
2.4.0,d: Number of coordinates/features/filters
2.4.0,B: Batch Size
2.4.0,Compute the distances and radial symmetry functions.
2.4.0,check that there isnt just one or zero inputs
2.4.0,create subspaces
2.4.0,"concatenate subspaces, reshape to size of original input, then stack"
2.4.0,"such that out_tensor has shape (2,?,original_cols)"
2.4.0,creates subspaces the same way it was done in AlphaShare
2.4.0,calculate squared Frobenius norm
2.4.0,"(TODO YTZ:) faster, less memory intensive way"
2.4.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.4.0,"r = tf.expand_dims(r, -1)"
2.4.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.4.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.4.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.4.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.4.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.4.0,Calculate pairwise distance
2.4.0,Cutoff with threshold Rc
2.4.0,return d
2.4.0,tf.stack issues again...
2.4.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.4.0,So the Tensor has known dimensions
2.4.0,Note that AP_ij and AP_ji share the same self.AP_bn batch
2.4.0,normalization
2.4.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.4.0,and embeddings of atom j(both gone through a hidden layer)
2.4.0,"for atom i, sum the influence from all other atom j in the molecule"
2.4.0,number of inputs each step
2.4.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.4.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.4.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.4.0,`count`-th step
2.4.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.4.0,generating index for graph features used in the inputs
2.4.0,"extracting graph features for parents of the target atoms, then flatten"
2.4.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.4.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.4.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.4.0,of shape: (batch_size*max_atoms) * n_graph_features
2.4.0,representing the graph features of target atoms in each graph
2.4.0,index for targe atoms
2.4.0,Extract atom_features
2.4.0,sum all graph outputs
2.4.0,"Default message function: edge network, update function: GRU"
2.4.0,more options to be implemented
2.4.0,Add another value(~-Inf) to prevent error in softmax
2.4.0,Model using this layer must set pad_batches=True
2.4.0,Perform one step of LSTM
2.4.0,task_metadata_rows = {task: [] for task in tasks}
2.4.0,Extract those datapoints which are present for this task
2.4.0,Loading is done on-the-fly
2.4.0,Build the model.
2.4.0,Final atom-layer convolution. Note this differs slightly from the paper
2.4.0,since we use a tanh activation as default. This seems necessary for numerical
2.4.0,stability.
2.4.0,Now fully connected layers
2.4.0,Should this allow for training?
2.4.0,"pair_edges is of shape (2, N)"
2.4.0,number of atoms in each molecule
2.4.0,index of pair features
2.4.0,Get starting pair atoms
2.4.0,number of pairs for each atom
2.4.0,atom features
2.4.0,pair features
2.4.0,Build the model.
2.4.0,Build the model.
2.4.0,calculation orders for a batch of molecules
2.4.0,padding atom features vector of each molecule with 0
2.4.0,Build the model.
2.4.0,number of atoms in each molecule
2.4.0,index of pair features
2.4.0,number of pairs for each atom
2.4.0,atom features
2.4.0,pair features
2.4.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.4.0,Add the input features.
2.4.0,Add the shared dense layers
2.4.0,Add task-specific bypass layers
2.4.0,Add the input features.
2.4.0,Add the shared dense layers
2.4.0,Add task-specific bypass layers
2.4.0,W&B logging
2.4.0,Backwards compatibility
2.4.0,The optimizer creates internal variables the first time apply_gradients()
2.4.0,is called for a new set of variables.  If that happens inside a function
2.4.0,"annotated with tf.function it throws an exception, so call it once here."
2.4.0,Main training loop.
2.4.0,"Execute the loss function, accumulating the gradients."
2.4.0,Report progress and write checkpoints.
2.4.0,Capture the last avg_loss in case of return since we're resetting to
2.4.0,0 now
2.4.0,Report final results.
2.4.0,Invoke the model.
2.4.0,Apply tranformers and record results.
2.4.0,Concatenate arrays to create the final results.
2.4.0,Use a GradientTape to compute gradients.
2.4.0,Ensure weights for both models are built.
2.4.0,Add the input features.
2.4.0,Add the dense layers
2.4.0,Add the input features.
2.4.0,Add the dense layers
2.4.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.4.0,Similarity values
2.4.0,Labels for all top K similar samples
2.4.0,Discard any padded predictions
2.4.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.4.0,Build the model.
2.4.0,Character embedding
2.4.0,Multiple convolutional layers with different filter widths
2.4.0,Max-over-time pooling
2.4.0,Concat features from all filters(one feature per filter)
2.4.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.4.0,SMILES strings
2.4.0,Maximum length is expanded to allow length variation during train and inference
2.4.0,'_' served as delimiter and padding
2.4.0,Initialize common characters as keys
2.4.0,Include space to avoid extra keys
2.4.0,"For 'Cl', 'Br', etc."
2.4.0,"Character not recognized, add to extra_keys"
2.4.0,Add all extra_keys to char_dict
2.4.0,Transform SMILES sequence to integers
2.4.0,Skip all spaces
2.4.0,"For 'Cl', 'Br', etc."
2.4.0,Padding with '_'
2.4.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.4.0,TODO: Turning off queue for now. Safe to re-activate?
2.4.0,Do a simple greedy search.
2.4.0,Do a beam search with length normalization.
2.4.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.4.0,This candidate sequence has already been terminated
2.4.0,Consider all possible tokens we could add to this candidate sequence.
2.4.0,Add the input features.
2.4.0,Handle output layer
2.4.0,Iterate over all previous tasks.
2.4.0,prev_layers is a list with elements of size
2.4.0,"(batch_size, layer_sizes[i-1])"
2.4.0,flake8: noqa
2.4.0,scikit-learn model
2.4.0,PyTorch models
2.4.0,####################################################################################
2.4.0,Compatibility imports for renamed XGBoost models. Remove below with DeepChem 3.0.
2.4.0,####################################################################################
2.4.0,#######################################################################################
2.4.0,Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0.
2.4.0,#######################################################################################
2.4.0,Last layer sequences not returned.
2.4.0,This is needed because ImageDataGenerator does infinite looping
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,Predict the output and uncertainty.
2.4.0,Predict the output and uncertainty.
2.4.0,The DAG models have high error with dropout
2.4.0,"Despite a lot of effort tweaking it , there appears to be"
2.4.0,a limit to how low the error can go with dropout.
2.4.0,assert mean_error < 0.5 * mean_value
2.4.0,Predict the output and uncertainty.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,load datasets
2.4.0,disable transformer
2.4.0,check train
2.4.0,check predict shape
2.4.0,check overfit
2.4.0,load datasets
2.4.0,disable transformer
2.4.0,check train
2.4.0,check predict shape
2.4.0,check overfit
2.4.0,load datasets
2.4.0,disable transformer
2.4.0,check train
2.4.0,check predict shape
2.4.0,check overfit
2.4.0,reload
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Check same predictions are made.
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Load trained model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload trained model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Reload trained model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload trained model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reloaded Trained Model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,3D Multivariate Gaussian base distribution
2.4.0,Check that reloaded model can sample from the distribution
2.4.0,Check that density estimation is same for reloaded model
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload trained model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload Trained Model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload Trained Model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload trained model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Check predictions match on random sample
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload trained model
2.4.0,Eval model on train
2.4.0,Check predictions match on random sample
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload trained model
2.4.0,Eval model on train
2.4.0,Check predictions match on random sample
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Reload trained model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Reload trained Model
2.4.0,Check predictions match on random sample
2.4.0,Eval model on train
2.4.0,Reload Trained Model
2.4.0,Check predictions match on random sample
2.4.0,TODO: This test is a little awkward. The Smiles2Vec model awkwardly depends on a dataset_file being available on disk. This needs to be cleaned up to match the standard model handling API.
2.4.0,Reload Trained Model
2.4.0,Check predictions match on original dataset
2.4.0,TODO: We need a cleaner usage example for this
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Check predictions match on random sample
2.4.0,Train the model on random sequences.  We aren't training long enough to
2.4.0,"really make it reliable, but I want to keep this test fast, and it should"
2.4.0,still be able to reproduce a reasonable fraction of input sequences.
2.4.0,Test it out.
2.4.0,There are 4 atoms each of which have 75 atom features
2.4.0,There are 10 pairs with infinity distance and 14 pair features
2.4.0,4 atoms in total
2.4.0,10 pairs in total
2.4.0,10 pairs in total each with start/finish
2.4.0,There are 4 atoms each of which have 75 atom features
2.4.0,"There are 8 pairs with distance 1 and 14 pair features. (To see why 8,"
2.4.0,"there's the self pair for ""C"". For ""CCC"" there are 7 pairs including self"
2.4.0,connections and accounting for symmetry.)
2.4.0,4 atoms in total
2.4.0,10 pairs in total
2.4.0,The center atom is self connected and to both neighbors so it appears
2.4.0,thrice. The canonical ranking used in MolecularFeaturizer means this
2.4.0,central atom is ranked last in ordering.
2.4.0,10 pairs in total each with start/finish
2.4.0,def test_weave_fit_simple_infinity_distance():
2.4.0,featurizer = dc.feat.WeaveFeaturizer(max_pair_distance=None)
2.4.0,"X = featurizer([""C"", ""CCC""])"
2.4.0,"y = np.array([0, 1.])"
2.4.0,"dataset = dc.data.NumpyDataset(X, y)"
2.4.0,batch_size = 20
2.4.0,model = WeaveModel(
2.4.0,"1,"
2.4.0,"batch_size=batch_size,"
2.4.0,"mode='classification',"
2.4.0,"fully_connected_layer_sizes=[2000, 1000],"
2.4.0,"batch_normalize=True,"
2.4.0,batch_normalize_kwargs={
2.4.0,"""fused"": False,"
2.4.0,"""trainable"": True,"
2.4.0,"""renorm"": True"
2.4.0,"},"
2.4.0,learning_rage=0.0005)
2.4.0,"model.fit(dataset, nb_epoch=200)"
2.4.0,transformers = []
2.4.0,metric = dc.metrics.Metric(
2.4.0,"dc.metrics.roc_auc_score, np.mean, mode=""classification"")"
2.4.0,"scores = model.evaluate(dataset, [metric], transformers)"
2.4.0,assert scores['mean-roc_auc_score'] >= 0.9
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Fit trained model
2.4.0,Predict the output and uncertainty.
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,xgboost test
2.4.0,fit trained model
2.4.0,eval model on test
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,lightgbm test
2.4.0,fit trained model
2.4.0,eval model on test
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,xgboost test
2.4.0,fit trained model
2.4.0,eval model on test
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,lightgbm test
2.4.0,fit trained model
2.4.0,eval model on test
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,xgboost test
2.4.0,fit trained model
2.4.0,eval model on test
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,lightgbm test
2.4.0,fit trained model
2.4.0,eval model on test
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,xgboost test
2.4.0,fit trained model
2.4.0,reload
2.4.0,check predictions match on test dataset
2.4.0,eval model on test
2.4.0,prepare dataset
2.4.0,global setting
2.4.0,lightgbm test
2.4.0,fit trained model
2.4.0,reload
2.4.0,check predictions match on test dataset
2.4.0,eval model on test
2.4.0,"For simplicity, let's assume both molecules have same number of"
2.4.0,atoms.
2.4.0,Creates a set of dummy features that contain the coordinate and
2.4.0,neighbor-list features required by the AtomicConvModel.
2.4.0,"frag2_z = np.random.rand(N_atoms, 3)"
2.4.0,"For simplicity, let's assume both molecules have same number of"
2.4.0,atoms.
2.4.0,Creates a set of dummy features that contain the coordinate and
2.4.0,neighbor-list features required by the AtomicConvModel.
2.4.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.4.0,max num atoms instead of exact.
2.4.0,Cutoff in angstroms
2.4.0,arbitrary label
2.4.0,Run a fitting operation
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Fit trained model
2.4.0,Eval model on train/test
2.4.0,Fit trained model
2.4.0,Eval model on train/test
2.4.0,Fit trained model
2.4.0,Eval model on train/test
2.4.0,See if it has done a plausible job of learning the distribution.
2.4.0,See if it has done a plausible job of learning the distribution.
2.4.0,See if it has done a plausible job of learning the distribution.
2.4.0,No training has been done after reload
2.4.0,See if it has done a plausible job of learning the distribution.
2.4.0,We have to set the gradient penalty very small because the generator's
2.4.0,"output is only a single number, so the default penalty would constrain"
2.4.0,it far too much.
2.4.0,See if it has done a plausible job of learning the distribution.
2.4.0,We have to set the gradient penalty very small because the generator's
2.4.0,"output is only a single number, so the default penalty would constrain"
2.4.0,it far too much.
2.4.0,See if it has done a plausible job of learning the distribution.
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,n_samples = 100
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Predict the output and uncertainty.
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Load mini log-solubility dataset.
2.4.0,Fit trained model
2.4.0,Eval model on train
2.4.0,Check that predicting internal layers works.
2.4.0,Each epoch is a single step for this model
2.4.0,Create two models using the same model directory.
2.4.0,Check that they produce different results.
2.4.0,"Save a checkpoint from the first model and load it into the second one,"
2.4.0,and make sure they now match.
2.4.0,Train a model to overfit the dataset.
2.4.0,"Create an identical model, do a single step of fitting with restore=True,"
2.4.0,and make sure it got restored correctly.
2.4.0,Build a model that predicts uncertainty.
2.4.0,Fit the model and see if its predictions are correct.
2.4.0,Take a tiny step in the direction of s and see if the output changes by
2.4.0,the expected amount.
2.4.0,def test_singletask_to_multitask_classification(self):
2.4.0,n_features = 10
2.4.0,n_tasks = 17
2.4.0,tasks = range(n_tasks)
2.4.0,# Define train dataset
2.4.0,n_train = 100
2.4.0,"X_train = np.random.rand(n_train, n_features)"
2.4.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.4.0,w_train = np.ones_like(y_train)
2.4.0,"ids_train = [""C""] * n_train"
2.4.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.4.0,"X_train, y_train, w_train, ids_train)"
2.4.0,# Define test dataset
2.4.0,n_test = 10
2.4.0,"X_test = np.random.rand(n_test, n_features)"
2.4.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.4.0,w_test = np.ones_like(y_test)
2.4.0,"ids_test = [""C""] * n_test"
2.4.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.4.0,"X_test, y_test, w_test, ids_test)"
2.4.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.4.0,def model_builder(model_dir):
2.4.0,sklearn_model = LogisticRegression()
2.4.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.4.0,multitask_model = dc.models.SingletaskToMultitask(
2.4.0,"tasks, model_builder)"
2.4.0,# Fit trained model
2.4.0,multitask_model.fit(train_dataset)
2.4.0,multitask_model.save()
2.4.0,# Eval multitask_model on train/test
2.4.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.4.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.4.0,Generate data
2.4.0,Cleanup
2.4.0,Train the model while logging the validation ROC AUC.
2.4.0,Parse the log to pull out the AUC scores.
2.4.0,The last reported score should match the current performance of the model.
2.4.0,Reload the save model and confirm that it matches the best logged score.
2.4.0,3D Multivariate Gaussian base distribution
2.4.0,Must be float32 for RealNVP
2.4.0,Tests a simple flow of one RealNVP layer.
2.4.0,log likelihoods should be negative
2.4.0,# Fit model
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,x and y are the same tensor (equivalent at every element)
2.4.0,the pairwise inner product of the rows in x and y will always be 1
2.4.0,"the output tensor will be of shape (5,5)"
2.4.0,each row in x1 is orthogonal to each row in x2
2.4.0,the pairwise inner product of the rows in x and y will always be 0
2.4.0,"the output tensor will be of shape (256,256)"
2.4.0,"Creating a second layer should produce different results, since it has"
2.4.0,different random weights.
2.4.0,But evaluating the first layer again should produce the same result as before.
2.4.0,index of pair features
2.4.0,number of pairs for each atom
2.4.0,atom features
2.4.0,pair features
2.4.0,"Outputs should be [A, P]"
2.4.0,atom features
2.4.0,Try without compression
2.4.0,"Outputs should be [mol1_vec, mol2_vec)"
2.4.0,Try with compression
2.4.0,"Outputs should be [mol1_vec, mol2_vec)"
2.4.0,atom features
2.4.0,"per_mol_features = tf.math.segment_sum(inputs[0], inputs[1])"
2.4.0,Gaussian histograms expands into 11 Gaussian buckets.
2.4.0,"assert np.array(outputs[1]).shape == (11 * 75,)"
2.4.0,TODO What should shape[1] be?  It's not documented.
2.4.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.4.0,"Creating a second layer should produce different results, since it has"
2.4.0,different random weights.
2.4.0,But evaluating the first layer again should produce the same result as before.
2.4.0,"Creating a second layer should produce different results, since it has"
2.4.0,different random weights.
2.4.0,But evaluating the first layer again should produce the same result as before.
2.4.0,"Creating a second layer should produce different results, since it has"
2.4.0,different random weights.
2.4.0,But evaluating the first layer again should produce the same result as before.
2.4.0,"TODO What should the output shape be?  It's not documented, and there"
2.4.0,are no other test cases for it.
2.4.0,"Creating a second layer should produce different results, since it has"
2.4.0,different random weights.
2.4.0,But evaluating the first layer again should produce the same result as before.
2.4.0,"Creating a second layer should produce different results, since it has"
2.4.0,different random weights.
2.4.0,But evaluating the first layer again should produce the same result as before.
2.4.0,"Recall that the DAG layer expects a MultiConvMol as input,"
2.4.0,"so the ""batch"" is a pooled set of atoms from all the"
2.4.0,"molecules in the batch, just as it is for the graph conv."
2.4.0,This means that n_atoms is the batch-size
2.4.0,dropout_switch = False
2.4.0,dropout_switch
2.4.0,# TODO(rbharath): What is the shape of outputs supposed to be?
2.4.0,"# I'm getting (7, 30) here. Where does 7 come from??"
2.4.0,TODO(rbharath): We need more documentation about why
2.4.0,these numbers work.
2.4.0,Create a dataset and an input function for processing it.
2.4.0,Create a dataset and an input function for processing it.
2.4.0,Generate dummy dataset
2.4.0,Fit trained model
2.4.0,Eval model on test
2.4.0,Eval model on train
2.4.0,Fit trained model
2.4.0,Eval model on test
2.4.0,Fit trained model
2.4.0,Eval model on test
2.4.0,Fit trained model
2.4.0,Eval model on test
2.4.0,Fit trained model
2.4.0,Eval model on test
2.4.0,Each epoch is a single step for this model
2.4.0,Create two models using the same model directory.
2.4.0,Check that they produce different results.
2.4.0,"Save a checkpoint from the first model and load it into the second one,"
2.4.0,and make sure they now match.
2.4.0,Train a model to overfit the dataset.
2.4.0,"Create an identical model, do a single step of fitting with restore=True,"
2.4.0,and make sure it got restored correctly.
2.4.0,Build a model that predicts uncertainty.
2.4.0,Fit the model and see if its predictions are correct.
2.4.0,Take a tiny step in the direction of s and see if the output changes by
2.4.0,the expected amount.
2.4.0,Train the model on random sequences.  We aren't training long enough to
2.4.0,"really make it reliable, but I want to keep this test fast, and it should"
2.4.0,still be able to reproduce a reasonable fraction of input sequences.
2.4.0,Test it out.
2.4.0,Check that it got at least a quarter of them correct.
2.4.0,Test it out.
2.4.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.4.0,"few steps of training to make sure nothing crashes, then check that the"
2.4.0,results are at least internally consistent.
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,overfit test
2.4.0,test on a small MoleculeNet dataset
2.4.0,load datasets
2.4.0,initialize models
2.4.0,embedding node features
2.4.0,convolutional layer
2.4.0,pooling
2.4.0,for n_tasks == 1 case
2.4.0,Decide first number of GAT layers
2.4.0,flake8:noqa
2.4.0,Select a device.
2.4.0,W&B logging
2.4.0,Main training loop.
2.4.0,"Execute the loss function, accumulating the gradients."
2.4.0,Report progress and write checkpoints.
2.4.0,Capture the last avg_loss in case of return since we're resetting to 0 now
2.4.0,Report final results.
2.4.0,Invoke the model.
2.4.0,Apply tranformers and record results.
2.4.0,Concatenate arrays to create the final results.
2.4.0,Compute the gradients.
2.4.0,Save the checkpoint to a file.
2.4.0,Rename and delete older files.
2.4.0,Ensure weights for both models are built.
2.4.0,Some scikit-learn models don't use weights.
2.4.0,flake8: ignore
2.4.0,GDBT doesn't support multi-output(task)
2.4.0,Find optimal n_estimators based on original learning_rate and early_stopping_rounds
2.4.0,retrain model to whole data using best n_estimators * 1.25
2.4.0,GDBT doesn't support multi-output(task)
2.4.0,########################################
2.4.0,Deprecation warnings for XGBoostModel
2.4.0,########################################
2.4.0,flake8: noqa
2.4.0,-*- coding: utf-8 -*-
2.4.0,Assigning featurizer if not user defined
2.4.0,loading datasets
2.4.0,Assembling train and valid datasets
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,Building tensorflow MultitaskDNN model
2.4.0,Building tensorflow robust MultitaskDNN model
2.4.0,Building scikit logistic regression model
2.4.0,Transform fingerprints to IRV features
2.4.0,Building tensorflow IRV model
2.4.0,Building scikit random forest model
2.4.0,Building scikit learn Kernel SVM model
2.4.0,Building xgboost classification model
2.4.0,Remove token for paddings
2.4.0,Building scikit random forest model
2.4.0,Building scikit learn Kernel Ridge Regression model
2.4.0,Building scikit learn Kernel Ridge Regression model
2.4.0,Building xgboost regression model
2.4.0,Loading hyperparameters
2.4.0,num positive/negative ligands
2.4.0,Set batch sizes for network
2.4.0,Model structure
2.4.0,Traning settings
2.4.0,Fit trained model
2.4.0,Evaluating low data model
2.4.0,-*- coding: utf-8 -*-
2.4.0,Assigning featurizer if not user defined
2.4.0,loading datasets
2.4.0,
2.4.0,Note by @XericZephyr. Reason why I spun off this function:
2.4.0,1. Some model needs dataset information.
2.4.0,2. It offers us possibility to **cache** the dataset
2.4.0,"if the featurizer runs very slow, e.g., GraphConv."
2.4.0,2+. The cache can even happen at Travis CI to accelerate
2.4.0,CI testing.
2.4.0,
2.4.0,loading datasets
2.4.0,!/usr/bin/env python2
2.4.0,-*- coding: utf-8 -*-
2.4.0,TODO: Check for this
2.4.0,Download files if they don't exist
2.4.0,Featurize the KINASE dataset
2.4.0,Shuffle the training data
2.4.0,Apply transformations
2.4.0,#### TIMING ######
2.4.0,transformers = [
2.4.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.4.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.4.0,dataset=train_dataset)]
2.4.0,Set shard size low to avoid memory problems.
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Set some global variables up top
2.4.0,Featurize KAGGLE dataset
2.4.0,############################################################# TIMING
2.4.0,############################################################# TIMING
2.4.0,Build the path to the dataset on disk.
2.4.0,Try to reload cached datasets.
2.4.0,Create the dataset
2.4.0,Split and transform the dataset.
2.4.0,. clinical trial toxicity (or absence of toxicity)
2.4.0,. FDA approval status.
2.4.0,Download files if they don't exist
2.4.0,Featurizing datasets
2.4.0,Missing entry removal
2.4.0,Shuffle the training data
2.4.0,Apply transformations
2.4.0,#### TIMING ###########
2.4.0,TODO: Check if anything needs to be added
2.4.0,Featurize the FACTORS dataset
2.4.0,Shuffle the training data
2.4.0,Apply transformations
2.4.0,######### TIMING ################
2.4.0,Most reaction dataset ML tasks train the prediction of products from
2.4.0,"ractants. Both of these are contained in the rxn object that is output,"
2.4.0,"so there is no ""tasks"" field."
2.4.0,Download USPTO dataset
2.4.0,Unzip
2.4.0,Unzipped file is a tap seperated values file (despite the .txt)
2.4.0,The first element in the row is the reaction smarts
2.4.0,"Sometimes smarts have extraneous information at end of form """
2.4.0,"|f:0"" that causes parsing to fail. Not sure what this information"
2.4.0,"is, but just ignoring for now."
2.4.0,Make up dummy labels since DiskDataset.from_numpy doesn't allow
2.4.0,creation from just features for now.
2.4.0,TODO: This dataset isn't saved to disk so reload doesn't happen.
2.4.0,dict of accepted featurizers for this dataset
2.4.0,modify the returned dicts for your dataset
2.4.0,Names of supported featurizers
2.4.0,dict of accepted transformers
2.4.0,dict of accepted splitters
2.4.0,names of supported splitters
2.4.0,Warning message about this template
2.4.0,Featurize mydataset
2.4.0,Get DeepChem data directory if needed
2.4.0,Check for str args to featurizer and splitter
2.4.0,Reload from disk
2.4.0,First type of supported featurizers
2.4.0,"If featurizer requires a non-CSV file format, load .tar.gz file"
2.4.0,Changer loader to match featurizer and data file type
2.4.0,Featurize dataset
2.4.0,Initialize transformers
2.4.0,"get pdb and sdf filenames, labels and pdbids"
2.4.0,load and featurize each complex
2.4.0,Extract locations of data
2.4.0,Extract labels
2.4.0,Lines have format
2.4.0,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.4.0,"The base-10 logarithm, -log kd/pk"
2.4.0,"def load_pcba_146(featurizer='ECFP',"
2.4.0,"split='random',"
2.4.0,"reload=True,"
2.4.0,"data_dir=None,"
2.4.0,"save_dir=None,"
2.4.0,**kwargs):
2.4.0,return load_pcba_dataset(
2.4.0,"featurizer=featurizer,"
2.4.0,"split=split,"
2.4.0,"reload=reload,"
2.4.0,"assay_file_name=""pcba_146.csv.gz"","
2.4.0,"data_dir=data_dir,"
2.4.0,"save_dir=save_dir,"
2.4.0,**kwargs)
2.4.0,"def load_pcba_2475(featurizer='ECFP',"
2.4.0,"split='random',"
2.4.0,"reload=True,"
2.4.0,"data_dir=None,"
2.4.0,"save_dir=None,"
2.4.0,**kwargs):
2.4.0,return load_pcba_dataset(
2.4.0,"featurizer=featurizer,"
2.4.0,"split=split,"
2.4.0,"reload=reload,"
2.4.0,"assay_file_name=""pcba_2475.csv.gz"","
2.4.0,"data_dir=data_dir,"
2.4.0,"save_dir=save_dir,"
2.4.0,**kwargs)
2.4.0,def test_qm9_loader():
2.4.0,current_dir = os.path.dirname(os.path.abspath(__file__))
2.4.0,"tasks, datasets, transformers = load_qm9("
2.4.0,"reload=False,"
2.4.0,"data_dir=current_dir,"
2.4.0,"featurizer='ECFP',"
2.4.0,splitter_kwargs={
2.4.0,"'seed': 42,"
2.4.0,"'frac_train': 0.6,"
2.4.0,"'frac_valid': 0.2,"
2.4.0,'frac_test': 0.2
2.4.0,})
2.4.0,
2.4.0,assert len(tasks) == 12
2.4.0,assert tasks[0] == 'mu'
2.4.0,"assert datasets[0].X.shape == (8, 1024)"
2.4.0,def test_zinc15_loader():
2.4.0,current_dir = os.path.dirname(os.path.abspath(__file__))
2.4.0,
2.4.0,"tasks, datasets, transformers = load_zinc15("
2.4.0,"reload=False,"
2.4.0,"data_dir=current_dir,"
2.4.0,splitter_kwargs={
2.4.0,"'seed': 42,"
2.4.0,"'frac_train': 0.6,"
2.4.0,"'frac_valid': 0.2,"
2.4.0,'frac_test': 0.2
2.4.0,})
2.4.0,
2.4.0,test_vec = np.array([
2.4.0,"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,"
2.4.0,"0.0, -1.224744871391589, 0.0, 0.0, 0.0, 0.0, 2.0, -0.5, 0.0, 0.0, 0.0,"
2.4.0,"0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0"
2.4.0,])
2.4.0,
2.4.0,"train, val, test = datasets"
2.4.0,"assert tasks == ['mwt', 'logp', 'reactive']"
2.4.0,"assert train.X.shape == (3, 100, 35)"
2.4.0,"assert np.allclose(train.X[0][0], test_vec, atol=0.01)"
2.4.0,
2.4.0,"if os.path.exists(os.path.join(current_dir, 'zinc15_250K_2D.csv')):"
2.4.0,"os.remove(os.path.join(current_dir, 'zinc15_250K_2D.csv'))"
2.4.0,Range of optimization
2.4.0,We know from guard above that this is an int/float
2.4.0,Specify logfile
2.4.0,Make logdir if it doesn't exist.
2.4.0,setup range
2.4.0,Stores all results
2.4.0,Store all model references so we don't have to reload
2.4.0,Stores all model locations
2.4.0,Demarcating internal function for readability
2.4.0,"param values are always float in BO, so this line converts float to int"
2.4.0,see : https://github.com/josejimenezluna/pyGPGO/issues/10
2.4.0,Record hyperparameters
2.4.0,Add it on to the information needed for the constructor
2.4.0,Some models autosave
2.4.0,Record performances
2.4.0,Store all results
2.4.0,Store reference to model
2.4.0,GPGO maximize performance by default
2.4.0,set performance to its negative value for minimization
2.4.0,execute GPGO
2.4.0,FIXME: Incompatible types in assignment
2.4.0,Let's fetch the model with the best parameters
2.4.0,Compare best model to default hyperparameters
2.4.0,Record hyperparameters
2.4.0,Return default hyperparameters
2.4.0,Construction dictionary mapping hyperparameter names to values
2.4.0,Some models autosave
2.4.0,arbitrarily return last model
2.4.0,flake8: noqa
2.4.0,Generate dummy dataset
2.4.0,Generate dummy dataset
2.4.0,These are per-example multiplier
2.4.0,Test that 2 parameters were optimized
2.4.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.4.0,Generate dummy dataset
2.4.0,Generate dummy dataset
2.4.0,These are per-example multiplier
2.4.0,Test that 2 parameters were optimized
2.4.0,Recall that the key is a string of the form _batch_size_39_learning_rate_0.01 for example
2.4.0,Have the worker threads generate the rollouts for this iteration.
2.4.0,Perform optimization.
2.4.0,Build the inputs and run the optimizer.
2.4.0,Update the number of steps taken so far and perform checkpointing.
2.4.0,Merge all the rollouts into a single set of arrays.
2.4.0,Iterate slices.
2.4.0,Generate the rollout.
2.4.0,Compute an estimate of the reward for the rest of the episode.
2.4.0,Compute the discounted rewards and advantages.
2.4.0,Convert the actions to one-hot.
2.4.0,Rearrange the states into the proper set of arrays.
2.4.0,Return the processed arrays.
2.4.0,Training loop.
2.4.0,Do checkpointing.
2.4.0,Generate the rollout.
2.4.0,Compute an estimate of the reward for the rest of the episode.
2.4.0,Compute the discounted rewards and advantages.
2.4.0,"Record the actions, converting to one-hot if necessary."
2.4.0,Rearrange the states into the proper set of arrays.
2.4.0,Build the inputs and apply gradients.
2.4.0,Assume all arrays are float32.
2.4.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.4.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.4.0,"game).  The average reward for any bet is slightly negative, so the best"
2.4.0,strategy is to walk away.
2.4.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.4.0,Optimize it.
2.4.0,"It should have learned that the expected value is very close to zero, and that the best"
2.4.0,action is to walk away.
2.4.0,"Verify that we can create a new A2C object, reload the parameters from the first one, and"
2.4.0,get the same result.
2.4.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.4.0,The environment just has a constant state.
2.4.0,The policy includes a single recurrent layer.
2.4.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.4.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.4.0,"On the first call, the initial state should be all zeros."
2.4.0,It should still be zeros since we didn't save it last time.
2.4.0,It should be different now.
2.4.0,This should be the same as the previous one.
2.4.0,"Now we reset it, so we should get the same result as initially."
2.4.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.4.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.4.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.4.0,at all.  Using hindsight makes it much easier.
2.4.0,A simple policy with two hidden layers.
2.4.0,Optimize it.
2.4.0,Try running it a few times and see if it succeeds.
2.4.0,The state consists of two numbers: a current value and a target value.
2.4.0,The policy just needs to learn to output the target value (or at least
2.4.0,move toward it).
2.4.0,A simple policy with no hidden layers.
2.4.0,Optimize it.
2.4.0,Try running it and see if it reaches the target
2.4.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.4.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.4.0,"game).  The average reward for any bet is slightly negative, so the best"
2.4.0,strategy is to walk away.
2.4.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.4.0,Optimize it.
2.4.0,"It should have learned that the expected value is very close to zero, and that the best"
2.4.0,action is to walk away.
2.4.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.4.0,get the same result.
2.4.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.4.0,The environment just has a constant state.
2.4.0,The policy includes a single recurrent layer.
2.4.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.4.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.4.0,"On the first call, the initial state should be all zeros."
2.4.0,It should still be zeros since we didn't save it last time.
2.4.0,It should be different now.
2.4.0,This should be the same as the previous one.
2.4.0,"Now we reset it, so we should get the same result as initially."
2.4.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.4.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.4.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.4.0,at all.  Using hindsight makes it much easier.
2.4.0,A simple policy with two hidden layers.
2.4.0,Optimize it.
2.4.0,Try running it a few times and see if it succeeds.
2.4.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.4.0,Randomize who goes first
2.4.0,Illegal move -- the square is not empty
2.4.0,Move X
2.4.0,Did X Win
2.4.0,Did O Win
2.4.0,"default channels are ""conda-forge"" and ""omnia"""
2.4.0,"default packages are ""rdkit"", ""openmm"" and ""pdbfixer"""
2.3.0,!/usr/bin/env python3
2.3.0,-*- coding: utf-8 -*-
2.3.0,Datasets and models used in the benchmark test
2.3.0,"irv, rf, rf_regression should be assigned manually"
2.3.0,Evaluate performances with different training set fraction
2.3.0,Datasets and models used in the benchmark test
2.3.0,Uncomment the two lines below if hyper_parameters are provided
2.3.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.3.0,hyper_parameters = pickle.load(f)
2.3.0,Will raise a CalledProcessError if fails.
2.3.0,!/usr/bin/env python3
2.3.0,-*- coding: utf-8 -*-
2.3.0,Datasets and models used in the benchmark test
2.3.0,Set numpy seed
2.3.0,##Load data###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Featurize Kinase dataset
2.3.0,##Load data###
2.3.0,num_trials = 5
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,Force matplotlib to not use any Xwindows backend.
2.3.0,##Load data###
2.3.0,the histogram of the data
2.3.0,Set numpy seed
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,num_trials = 5
2.3.0,Set some global variables up top
2.3.0,Fit trained model
2.3.0,Featurize PCBA dataset
2.3.0,Initialize transformers
2.3.0,Fit trained model
2.3.0,Load sider models now
2.3.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.3.0,transformers to predict functions
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,##Load data###
2.3.0,"n_estimators=100, max_features=int(num_features/3),"
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Load tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Featurize FACTORS dataset
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,Force matplotlib to not use any Xwindows backend.
2.3.0,##Load data###
2.3.0,the histogram of the data
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Load QM7 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Batch size of models
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Fit trained model
2.3.0,Batch size of models
2.3.0,Fit models
2.3.0,Load Tox21 dataset
2.3.0,Batch size of models
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load QM8 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Fit trained model
2.3.0,Set numpy seed
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,Load ChEMBL dataset
2.3.0,Fit models
2.3.0,Do setup required for tf/keras models
2.3.0,Number of features on conv-mols
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,DeepCrystal Technologies 2017 - Patrick Hop
2.3.0,MIT License - have fun!!
2.3.0,Set to higher values to get better numbers
2.3.0,======================================================================
2.3.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,Only for debug!
2.3.0,Load Delaney dataset
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Do setup required for tf/keras models
2.3.0,Number of features on conv-mols
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load Delaney dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load MUV dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Evaluate train/test scores
2.3.0,Load MUV data
2.3.0,Build model
2.3.0,Fit trained model
2.3.0,Evaluate train/test scores
2.3.0,Extract active site
2.3.0,Featurize ligand
2.3.0,Default for CircularFingerprint
2.3.0,Featurize pocket
2.3.0,Note broadcast operation
2.3.0,Compute labels for pockets
2.3.0,Some complexes have labels but no PDB files. Filter these manually
2.3.0,Some of the ligand-names are of form (FMN ox). Use regex
2.3.0,to merge into form (FMN-ox)
2.3.0,Filter if missing PDB files
2.3.0,Load PDBBind dataset
2.3.0,Define featurizers
2.3.0,Featurize Dataset
2.3.0,########################################################## DEBUG
2.3.0,########################################################## DEBUG
2.3.0,For stable runs
2.3.0,Fit trained model
2.3.0,4-fold splits
2.3.0,num positive/negative ligands
2.3.0,10 trials on test-set
2.3.0,Define metric
2.3.0,Get supports on test-set
2.3.0,Compute accuracies
2.3.0,Number of features on conv-mols
2.3.0,Batch size of models
2.3.0,graph_model = dc.nn.SequentialGraph(n_feat)
2.3.0,Fit trained model
2.3.0,Test model
2.3.0,Join information for all tasks.
2.3.0,4-fold splits
2.3.0,num positive/negative ligands
2.3.0,Define metric
2.3.0,Get supports on test-set
2.3.0,Compute accuracies
2.3.0,Number of features on conv-mols
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Test model
2.3.0,Join information for all tasks.
2.3.0,4-fold splits
2.3.0,num positive/negative ligands
2.3.0,10 trials on test-set
2.3.0,Define metric
2.3.0,Get supports on test-set
2.3.0,Compute accuracies
2.3.0,Number of features on conv-mols
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Test model
2.3.0,Join information for all tasks.
2.3.0,4-fold splits
2.3.0,num positive/negative ligands
2.3.0,10 trials on test-set
2.3.0,Define metric
2.3.0,Get supports on test-set
2.3.0,Compute accuracies
2.3.0,Train model on support
2.3.0,Test model
2.3.0,Join information for all tasks.
2.3.0,4-fold splits
2.3.0,num positive/negative ligands
2.3.0,10 trials on test-set
2.3.0,Define metric
2.3.0,Get supports on test-set
2.3.0,Compute accuracies
2.3.0,Train model on support
2.3.0,Test model
2.3.0,Join information for all tasks.
2.3.0,Set some global variables up top
2.3.0,Featurize Tox21 dataset
2.3.0,Initialize transformers
2.3.0,Set some global variables up top
2.3.0,Featurize Tox21 dataset
2.3.0,Initialize transformers
2.3.0,Load MUV dataset
2.3.0,Featurize MUV dataset
2.3.0,Initialize transformers
2.3.0,Load MUV dataset
2.3.0,Featurize MUV dataset
2.3.0,Initialize transformers
2.3.0,Featurize SIDER dataset
2.3.0,Initialize transformers
2.3.0,Featurize SIDER dataset
2.3.0,Initialize transformers
2.3.0,Load the data.
2.3.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.3.0,sparse: most tasks do not include data for most molecules.  It also is very
2.3.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.3.0,create a list of alternating positives and negatives so each batch will have
2.3.0,equal numbers of both.
2.3.0,Define a MetaLearner describing the learning problem.
2.3.0,Run meta-learning on 80% of the tasks.
2.3.0,Validate on the remaining tasks.
2.3.0,4-fold splits
2.3.0,10 positive/negative ligands
2.3.0,10 trials on test-set
2.3.0,Sample supports without replacement (all pos/neg should be different)
2.3.0,Define metric
2.3.0,Get supports on test-set
2.3.0,Compute accuracies
2.3.0,Train model on support
2.3.0,Test model
2.3.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.3.0,Join information for all tasks.
2.3.0,4-fold splits
2.3.0,num positive/negative ligands
2.3.0,Define metric
2.3.0,Get supports on test-set
2.3.0,Compute accuracies
2.3.0,Train model on support
2.3.0,Test model
2.3.0,Join information for all tasks.
2.3.0,replace with your own scratch directory
2.3.0,Number of conformations in each file increases exponentially.
2.3.0,Start with a smaller dataset before continuing. Use all of them
2.3.0,for production
2.3.0,"'ani_gdb_s03.h5',"
2.3.0,"'ani_gdb_s04.h5',"
2.3.0,"'ani_gdb_s05.h5',"
2.3.0,"'ani_gdb_s06.h5',"
2.3.0,"'ani_gdb_s07.h5',"
2.3.0,'ani_gdb_s08.h5'
2.3.0,Extract the data
2.3.0,Print the data
2.3.0,self-interaction energies taken from
2.3.0,https://github.com/isayev/ANI1_dataset README
2.3.0,flush once more at the end
2.3.0,"# For production, set nb_epoch to 100+"
2.3.0,"print(""Train scores"")"
2.3.0,print(train_scores)
2.3.0,"print(""Minimization of a single test set structure:"")"
2.3.0,"print(model.minimize_structure(coords, atomic_nums))"
2.3.0,Written by Roman Zubatyuk and Justin S. Smith
2.3.0,Modified by Yutong Zhao to make python2 compatible
2.3.0,opening file
2.3.0,print(store_loc)
2.3.0,print(type(v[0]))
2.3.0,print(k)
2.3.0,print(path)
2.3.0,Number of conformations in each file increases exponentially.
2.3.0,Start with a smaller dataset before continuing. Use all of them
2.3.0,for production
2.3.0,Extract the data
2.3.0,NOTE THE RENAMING:
2.3.0,Note sensitivity = recall
2.3.0,Load nci dataset
2.3.0,Featurize nci dataset
2.3.0,Initialize transformers
2.3.0,Set some global variables up top
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load hiv dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load hiv dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Fit trained model
2.3.0,Load SIDER dataset
2.3.0,Featurize SIDER dataset
2.3.0,Initialize transformers
2.3.0,Featurize permeability dataset
2.3.0,Load Tox21 dataset
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load SAMPL dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load SAMPL(FreeSolv) dataset
2.3.0,Define metric
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load clintox dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load clintox dataset
2.3.0,Fit models
2.3.0,Do setup required for tf/keras models
2.3.0,Number of features on conv-mols
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,-*- coding: utf-8 -*-
2.3.0,#############################################################################
2.3.0,## save dataset
2.3.0,#############################################################################
2.3.0,## load datasets
2.3.0,load sweetfda
2.3.0,load aact
2.3.0,## fixup smiles for matching
2.3.0,return smiles
2.3.0,map original smiles to converted smiles
2.3.0,"## join dataframes, index on smiles"
2.3.0,map original smiles back
2.3.0,## fill all nan with 0
2.3.0,## construct datasets
2.3.0,store in new datasets
2.3.0,## save datasets
2.3.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.3.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.3.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.3.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.3.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.3.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.3.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.3.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.3.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.3.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.3.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.3.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.3.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.3.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.3.0,For stable runs
2.3.0,Fit trained model
2.3.0,For stable runs
2.3.0,Fit trained model
2.3.0,For stable runs
2.3.0,Fit trained model
2.3.0,transformers = [
2.3.0,"dc.trans.LogTransformer(transform_X=True),"
2.3.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.3.0,dataset=train_dataset)]
2.3.0,Featurize UV dataset
2.3.0,##Load data###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Set numpy seed
2.3.0,##Load data###
2.3.0,##Create model###
2.3.0,Use R2 classification metric
2.3.0,Only use for final evaluation
2.3.0,Force matplotlib to not use any Xwindows backend.
2.3.0,##Load data###
2.3.0,the histogram of the data
2.3.0,##Load data###
2.3.0,###################################################### DEBUG
2.3.0,###################################################### DEBUG
2.3.0,Load HOPV dataset
2.3.0,Fit models
2.3.0,Number of features on conv-mols
2.3.0,Batch size of models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load HOPV dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load HOPV dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load HOPV dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Only for debug!
2.3.0,Load HOPV dataset
2.3.0,Fit models
2.3.0,Fit trained model
2.3.0,Load TOXCAST dataset
2.3.0,Featurize TOXCAST dataset
2.3.0,Initialize transformers
2.3.0,Fit trained model
2.3.0,Processing of ToxCast data
2.3.0,Author - Aneesh Pappu
2.3.0,Loading dataframes and editing indices
2.3.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.3.0,get corresponding casn
2.3.0,get corresponding smiles
2.3.0,write to cell
2.3.0,Tidy up and write to csv
2.3.0,TODO(rbharath): Check that this operation is differentiable.
2.3.0,The number of cells which we should theoretically have
2.3.0,The number of cells which we should theoretically have
2.3.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.3.0,The number of cells which we should theoretically have
2.3.0,TODO(rbharath): The test below only checks that shapes work out.
2.3.0,Need to do a correctness implementation vs. a simple CPU impl.
2.3.0,The number of cells which we should theoretically have
2.3.0,TODO(rbharath): The test below only checks that shapes work out.
2.3.0,Need to do a correctness implementation vs. a simple CPU impl.
2.3.0,The number of cells which we should theoretically have
2.3.0,TODO(rbharath): The test below only checks that shapes work out.
2.3.0,Need to do a correctness implementation vs. a simple CPU impl.
2.3.0,TODO(rbharath): Commenting this out due to weird segfaults
2.3.0,def test_vina_generate_conformers(self):
2.3.0,"""""""Test that Vina Model can generate conformers"""""""
2.3.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.3.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.3.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.3.0,max_protein_atoms = 3500
2.3.0,max_ligand_atoms = 100
2.3.0,"print(""Loading protein file"")"
2.3.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.3.0,protein_Z = pad_array(
2.3.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.3.0,max_protein_atoms)
2.3.0,"print(""Loading ligand file"")"
2.3.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.3.0,ligand_Z = pad_array(
2.3.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.3.0,max_ligand_atoms)
2.3.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.3.0,"Shape (n_cells, k)"
2.3.0,"Shape (N, 1)"
2.3.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.3.0,"conditions, so does wrapround. O(constant)"
2.3.0,"Shape (n_cells, 26)"
2.3.0,"Shape (N, 26)"
2.3.0,"coords of shape (N, ndim)"
2.3.0,"Shape (N, 26, k, ndim)"
2.3.0,"Shape (N, 26, k)"
2.3.0,"Shape (N, 26, k)"
2.3.0,"Shape (N, 26, k, ndim)"
2.3.0,"For smaller systems especially, the periodic boundary conditions can"
2.3.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.3.0,make sure duplicate neighbors are ignored?
2.3.0,TODO(rbharath): How does distance need to be modified here to
2.3.0,account for periodic boundary conditions?
2.3.0,"Shape (N, 26, k)"
2.3.0,"Shape (N, 26*k)"
2.3.0,TODO(rbharath): This will cause an issue with duplicates!
2.3.0,"Shape (N, M)"
2.3.0,"N elts of size (M,) each"
2.3.0,"Shape (N, 26*k)"
2.3.0,"N elts of size (26*k,) each"
2.3.0,"N elts of size (M,) each"
2.3.0,"Shape (N, M)"
2.3.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.3.0,"N tensors of shape (n_cells, 1)"
2.3.0,"Shape (N*n_cells, 1) after tile"
2.3.0,"List of N tensors of shape (n_cells, 1)"
2.3.0,Lists of length N
2.3.0,Lists of length n_cells
2.3.0,Get indices of k atoms closest to each cell point
2.3.0,TODO(rbharath): tf.stack for tf 1.0
2.3.0,"Tensor of shape (n_cells, k, ndim)"
2.3.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.3.0,"Tensor of shape (26, k, ndim)"
2.3.0,"Reshape to (26*k, ndim)"
2.3.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.3.0,"Dists of shape (26*k, 1)"
2.3.0,"Of shape (k, ndim)"
2.3.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.3.0,TODO(rbharath): Change this for tf 1.0
2.3.0,"n_cells tensors of shape (N, 1)"
2.3.0,"Shape (N*n_cells, 1) after tile"
2.3.0,"List of n_cells tensors of shape (N, 1)"
2.3.0,Lists of length n_cells
2.3.0,Lists of length n_cells
2.3.0,Get indices of k atoms closest to each cell point
2.3.0,"n_cells tensors of shape (k, ndim)"
2.3.0,"Tensor of shape (n_cells, k)"
2.3.0,TODO(rbharath):
2.3.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.3.0,- Need to group closest atoms amongst cell neighbors
2.3.0,- Need to do another top_k to find indices of closest neighbors.
2.3.0,- Return N lists corresponding to neighbors for every atom.
2.3.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.3.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.3.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.3.0,the cube.
2.3.0,Number of neighbors of central cube in 3-space is
2.3.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.3.0,TODO(rbharath)
2.3.0,n_cells = int(cells.get_shape()[0])
2.3.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.3.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.3.0,"Tile (a, a, a, b, b, b, etc.)"
2.3.0,"Tile (a, b, c, a, b, c, ...)"
2.3.0,"Lists of n_cells tensors of shape (N, 1)"
2.3.0,Lists of length n_cells
2.3.0,Lists of length n_cells
2.3.0,Get indices of k atoms closest to each cell point
2.3.0,"n_cells tensors of shape (26,)"
2.3.0,TODO(rbharath): Make this handle minibatches
2.3.0,"Shape (N_protein+N_ligand, 3)"
2.3.0,"Shape (N_protein+N_ligand,)"
2.3.0,"Shape (N_protein+N_ligand, M)"
2.3.0,"Shape (N_protein+N_ligand,)"
2.3.0,"Shape (N_protein+N_ligand, 3)"
2.3.0,"Shape (N_protein+N_ligand,)"
2.3.0,"Shape (N_protein+N_ligand, M)"
2.3.0,"Shape (N_protein+N_ligand, M, 3)"
2.3.0,"Shape (N_protein+N_ligand, M)"
2.3.0,"Shape (N_protein+N_ligand, M, 3)"
2.3.0,"Shape (N_protein+N_ligand, M)"
2.3.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.3.0,"Shape (N_protein+N_ligand, M)"
2.3.0,"Shape (N_protein+N_ligand, M)"
2.3.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.3.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.3.0,computing free-energy. This implementation currently uses all interaction
2.3.0,terms. Not sure if this makes a difference.
2.3.0,"Shape (N_protein+N_ligand, M)"
2.3.0,Shape () -- scalar
2.3.0,Keep track of the layers
2.3.0,"For graphical layers, add connectivity placeholders"
2.3.0,Add layer to the layer list
2.3.0,Keep track of the layers
2.3.0,Create graph topology and x
2.3.0,Keep track of the layers
2.3.0,Whether or not we have used the GraphGather layer yet
2.3.0,Update new value of x
2.3.0,Update new value of x
2.3.0,Update new value of x
2.3.0,Get train function
2.3.0,Initialize
2.3.0,################################################################### DEBUG
2.3.0,self.test_label_placeholder = Input(
2.3.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.3.0,"name=""label_placeholder""))"
2.3.0,self.test_weight_placeholder = Input(
2.3.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.3.0,"name=""weight_placeholder""))"
2.3.0,TODO(rbharath): Should weights for the support be used?
2.3.0,Support labels
2.3.0,self.support_label_placeholder = Input(
2.3.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.3.0,"name=""support_label_placeholder""))"
2.3.0,################################################################### DEBUG
2.3.0,Generate dictionary elements for support
2.3.0,Get graph information for test
2.3.0,Generate dictionary elements for test
2.3.0,Perform the optimization
2.3.0,Create different support sets
2.3.0,Get batch to try it out on
2.3.0,"Train on support set, batch pair"
2.3.0,Get featurization for test
2.3.0,"Shape (n_test, n_feat)"
2.3.0,Get featurization for support
2.3.0,"Shape (n_support, n_feat)"
2.3.0,Computes the inner part c() of the kernel
2.3.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.3.0,Normalize
2.3.0,TODO(rbharath): euclidean kernel is broken!
2.3.0,elif self.similarity == 'euclidean':
2.3.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.3.0,"Note that gram matrix g has shape (n_test, n_support)"
2.3.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.3.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.3.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.3.0,each test entry) to get attention vector
2.3.0,"Shape (n_test, n_support)"
2.3.0,Weighted sum of support labels
2.3.0,"Shape (n_support, 1)"
2.3.0,pred is yhat in eqn (1) of Matching Networks.
2.3.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.3.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.3.0,"Shape (n_test,)"
2.3.0,Convert to logit space using inverse sigmoid (logit) function
2.3.0,logit function: log(pred) - log(1-pred)
2.3.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.3.0,in Cross Entropy calculation.
2.3.0,"Shape (n_test,)"
2.3.0,Get scores
2.3.0,Remove padded elements
2.3.0,Get scores
2.3.0,pred corresponds to prob(example == 1)
2.3.0,Remove padded elements
2.3.0,Get batches
2.3.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.3.0,multitask case with missing data...
2.3.0,Join information for all tasks.
2.3.0,TODO(rbharath): Find a way to get rid of this import?
2.3.0,Extract model info
2.3.0,Get graph topology for x
2.3.0,Building outputs
2.3.0,Set epsilon
2.3.0,Initialize
2.3.0,"Path to save checkpoint files, which matches the"
2.3.0,replicated supervisor's default path.
2.3.0,Create target inputs
2.3.0,Get train function
2.3.0,TODO(rbharath): I believe this is total amount of data
2.3.0,Get graph information
2.3.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.3.0,the number of labeled data points in target_i. This is to normalize each task
2.3.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.3.0,Get other optimizer information
2.3.0,TODO(rbharath): Figure out how to handle phase appropriately
2.3.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.3.0,"tensors of shape (batch_size,)"
2.3.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.3.0,examples (effect averages out)
2.3.0,Perform the optimization
2.3.0,TODO(rbharath): Disabling saving for now to try to debug.
2.3.0,run eval data through the model
2.3.0,"Shape (n_samples, n_tasks)"
2.3.0,Create target inputs
2.3.0,TODO(rbharath): Find a way to get rid of this import?
2.3.0,Obtain appropriate loss function
2.3.0,Extract model info
2.3.0,Get graph topology for x
2.3.0,Raw logit outputs
2.3.0,Set epsilon
2.3.0,Initialize
2.3.0,"Path to save checkpoint files, which matches the"
2.3.0,replicated supervisor's default path.
2.3.0,Create target inputs
2.3.0,############################################################### DEBUG
2.3.0,"print(""multitask classifier"")"
2.3.0,"print(""feat"")"
2.3.0,print(feat)
2.3.0,############################################################### DEBUG
2.3.0,Get train function
2.3.0,TODO(rbharath): I believe this is total amount of data
2.3.0,Get graph information
2.3.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.3.0,the number of labeled data points in target_i. This is to normalize each task
2.3.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.3.0,Get other optimizer information
2.3.0,TODO(rbharath): Figure out how to handle phase appropriately
2.3.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.3.0,"tensors of shape (batch_size,)"
2.3.0,Convert the labels into one-hot vector encodings.
2.3.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.3.0,un-softmaxed logits rather than softmax outputs.
2.3.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.3.0,examples (effect averages out)
2.3.0,Perform the optimization
2.3.0,TODO(rbharath): Disabling saving for now to try to debug.
2.3.0,run eval data through the model
2.3.0,"Shape (n_samples, n_tasks)"
2.3.0,run eval data through the model
2.3.0,self.n_atoms = n_atoms
2.3.0,Define the list of tensors to be used as topology
2.3.0,Merge mol conv objects
2.3.0,Generate dicts
2.3.0,Define the list of tensors to be used as topology
2.3.0,Extract atom numbers
2.3.0,Generate dicts
2.3.0,molecule * atom(graph) => step => features
2.3.0,molecule * atom(graph) => step
2.3.0,molecule * atom(graph) => step
2.3.0,Define the list of tensors to be used as topology
2.3.0,calculation orders for a batch of molecules
2.3.0,padding atom features vector of each molecule with 0
2.3.0,self.n_atoms = n_atoms
2.3.0,Define the list of tensors to be used as topology
2.3.0,Extract atom numbers
2.3.0,Generate dicts
2.3.0,self.n_atoms = n_atoms
2.3.0,Define the list of tensors to be used as topology
2.3.0,Extract atom numbers
2.3.0,number of atoms in each molecule
2.3.0,index of pair features
2.3.0,number of pairs for each atom
2.3.0,atom features
2.3.0,pair features
2.3.0,Generate dicts
2.3.0,Load Tox21 dataset
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.3.0,Fit trained model
2.3.0,Fit models
2.3.0,Batch size of models
2.3.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.3.0,Fit trained model
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,number positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply an attention lstm layer
2.3.0,Number of folds for split
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,number positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply an attention lstm layer
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,number positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply an attention lstm layer
2.3.0,Number of folds for split
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Number of folds for split
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply a residual lstm layer
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply a residual lstm layer
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply a residual lstm layer
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply a residual lstm layer
2.3.0,Number of folds for split
2.3.0,Depth of attention module
2.3.0,number positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,Apply an attention lstm layer
2.3.0,Number of folds for split
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Number of features on conv-mols
2.3.0,Define metric
2.3.0,Train support model on train
2.3.0,Add layers
2.3.0,# Gather Projection
2.3.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.3.0,There should be 8 layers in graph_model
2.3.0,assert len(graph_model.layers) == 6
2.3.0,Add layers
2.3.0,Need to add batch-norm separately to test/support due to differing
2.3.0,shapes.
2.3.0,Apply an attention lstm layer
2.3.0,Gather Projection
2.3.0,Add layers
2.3.0,Need to add batch-norm separately to test/support due to differing
2.3.0,shapes.
2.3.0,Apply an attention lstm layer
2.3.0,Gather Projection
2.3.0,Degrees from 1 to max_deg inclusive
2.3.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.3.0,"Should have shape (?, deg)"
2.3.0,"Shape of atom_features should be (?, n_feat)"
2.3.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.3.0,-*- coding: utf-8 -*-
2.3.0,Save hyperparameters
2.3.0,-*- coding: utf-8 -*-
2.3.0,Save hyperparameters
2.3.0,setup optimizer
2.3.0,setup optimizer
2.3.0,"print(""tasK: %d"" %task)"
2.3.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.3.0,"print(""scores"")"
2.3.0,print(scores.size())
2.3.0,"print(""task_label"")"
2.3.0,print(task_label.size())
2.3.0,"task_loss =  self.criterion(scores, task_label)"
2.3.0,"print(""task_loss"")"
2.3.0,print(task_loss.size())
2.3.0,-*- coding: utf-8 -*-
2.3.0,Save hyperparameters
2.3.0,weight decay
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,Turns out there are valid cases where we don't want pad-batches
2.3.0,on by default.
2.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.3.0,Run training op.
2.3.0,############################################################# TIMING
2.3.0,The iterbatches does padding with zero-weight examples on the last batch.
2.3.0,Remove padded examples.
2.3.0,Special case to handle singletasks.
2.3.0,The iterbatches does padding with zero-weight examples on the last batch.
2.3.0,Remove padded examples.
2.3.0,References
2.3.0,Arguments
2.3.0,Aliases.
2.3.0,Aliases.
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,TODO(rbharath): This class does not yet have a
2.3.0,"TensorGraph equivalent, but one may not be required."
2.3.0,"Commented out for now, remove if OK."
2.3.0,class AlternateWeaveLayer(WeaveLayer):
2.3.0,""""""" Alternate implementation of weave module"
2.3.0,"same variables, different graph structures"
2.3.0,""""""""
2.3.0,
2.3.0,"def call(self, x, mask=None):"
2.3.0,"""""""Execute this layer on input tensors."
2.3.0,
2.3.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.3.0,
2.3.0,Parameters
2.3.0,----------
2.3.0,x: list
2.3.0,list of Tensors of form described above.
2.3.0,"mask: bool, optional"
2.3.0,Ignored. Present only to shadow superclass call() method.
2.3.0,
2.3.0,Returns
2.3.0,-------
2.3.0,A: Tensor
2.3.0,Tensor of atom_features
2.3.0,P: Tensor
2.3.0,Tensor of pair_features
2.3.0,""""""""
2.3.0,# Add trainable weights
2.3.0,self.build()
2.3.0,
2.3.0,atom_features = x[0]
2.3.0,pair_features = x[1]
2.3.0,
2.3.0,pair_split = x[2]
2.3.0,atom_to_pair = x[4]
2.3.0,
2.3.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.3.0,AA = self.activation(AA)
2.3.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.3.0,PA = self.activation(PA)
2.3.0,"PA = tf.segment_sum(PA, pair_split)"
2.3.0,
2.3.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.3.0,A = self.activation(A)
2.3.0,
2.3.0,if self.update_pair:
2.3.0,AP_ij = tf.matmul(
2.3.0,tf.reshape(
2.3.0,"tf.gather(atom_features, atom_to_pair),"
2.3.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.3.0,AP_ij = self.activation(AP_ij)
2.3.0,AP_ji = tf.matmul(
2.3.0,tf.reshape(
2.3.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.3.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.3.0,AP_ji = self.activation(AP_ji)
2.3.0,
2.3.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.3.0,PP = self.activation(PP)
2.3.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.3.0,P = self.activation(P)
2.3.0,else:
2.3.0,P = pair_features
2.3.0,
2.3.0,"return A, P"
2.3.0,TODO(rbharath): This class does not yet have a
2.3.0,"TensorGraph equivalent, but one may not be required."
2.3.0,"Commented out for now, remove if OK."
2.3.0,class WeaveConcat(Layer):
2.3.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.3.0,""""""""
2.3.0,
2.3.0,"def __init__(self,"
2.3.0,"batch_size,"
2.3.0,"n_atom_input_feat=50,"
2.3.0,"n_output=128,"
2.3.0,"init='glorot_uniform',"
2.3.0,"activation='tanh',"
2.3.0,**kwargs):
2.3.0,""""""""
2.3.0,Parameters
2.3.0,----------
2.3.0,batch_size: int
2.3.0,number of molecules in a batch
2.3.0,"n_atom_input_feat: int, optional"
2.3.0,Number of features for each atom in input.
2.3.0,"n_output: int, optional"
2.3.0,Number of output features for each atom(concatenated)
2.3.0,"init: str, optional"
2.3.0,Weight initialization for filters.
2.3.0,"activation: str, optional"
2.3.0,Activation function applied
2.3.0,
2.3.0,""""""""
2.3.0,self.batch_size = batch_size
2.3.0,self.n_atom_input_feat = n_atom_input_feat
2.3.0,self.n_output = n_output
2.3.0,self.init = initializations.get(init)  # Set weight initialization
2.3.0,self.activation = activations.get(activation)  # Get activations
2.3.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.3.0,
2.3.0,def build(self):
2.3.0,"""""""""Construct internal trainable weights."
2.3.0,""""""""
2.3.0,
2.3.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.3.0,self.b = model_ops.zeros(shape=[
2.3.0,"self.n_output,"
2.3.0,])
2.3.0,
2.3.0,self.trainable_weights = self.W + self.b
2.3.0,
2.3.0,"def call(self, x, mask=None):"
2.3.0,"""""""Execute this layer on input tensors."
2.3.0,
2.3.0,"x = [atom_features, atom_mask]"
2.3.0,
2.3.0,Parameters
2.3.0,----------
2.3.0,x: list
2.3.0,Tensors as listed above
2.3.0,"mask: bool, optional"
2.3.0,Ignored. Present only to shadow superclass call() method.
2.3.0,
2.3.0,Returns
2.3.0,-------
2.3.0,outputs: Tensor
2.3.0,Tensor of concatenated atom features
2.3.0,""""""""
2.3.0,self.build()
2.3.0,atom_features = x[0]
2.3.0,atom_masks = x[1]
2.3.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.3.0,A_mask = tf.split(
2.3.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.3.0,outputs = tf.concat(
2.3.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.3.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.3.0,outputs = self.activation(outputs)
2.3.0,return outputs
2.3.0,TODO(rbharath): This class does not yet have a
2.3.0,"TensorGraph equivalent, but one may not be required."
2.3.0,"Commented out for now, remove if OK."
2.3.0,class AlternateWeaveGather(WeaveGather):
2.3.0,"""""""Alternate implementation of weave gather layer"
2.3.0,corresponding to AlternateWeaveLayer
2.3.0,""""""""
2.3.0,
2.3.0,"def call(self, x, mask=None):"
2.3.0,"""""""Execute this layer on input tensors."
2.3.0,
2.3.0,"x = [atom_features, atom_split]"
2.3.0,
2.3.0,Parameters
2.3.0,----------
2.3.0,x: list
2.3.0,Tensors as listed above
2.3.0,"mask: bool, optional"
2.3.0,Ignored. Present only to shadow superclass call() method.
2.3.0,
2.3.0,Returns
2.3.0,-------
2.3.0,outputs: Tensor
2.3.0,Tensor of molecular features
2.3.0,""""""""
2.3.0,# Add trainable weights
2.3.0,self.build()
2.3.0,outputs = x[0]
2.3.0,atom_split = x[1]
2.3.0,
2.3.0,if self.gaussian_expand:
2.3.0,outputs = self.gaussian_histogram(outputs)
2.3.0,
2.3.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.3.0,
2.3.0,if self.gaussian_expand:
2.3.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.3.0,output_molecules = self.activation(output_molecules)
2.3.0,return output_molecules
2.3.0,Each directory holds a range of assay results
2.3.0,Just write NA
2.3.0,"Now, write out the results csv, going line by line through all molecule results"
2.3.0,printing the mol_id
2.3.0,printing the SMILES
2.3.0,Now gzip it
2.3.0,Now remove the intermediate csv
2.3.0,First download all SDF files. We need these to get smiles
2.3.0,Next download all Bioassays
2.3.0,RDKit consistently hangs when trying to read this file
2.3.0,TODO (LESWING) Lazy Load
2.3.0,TODO (LESWING) Lazy Load
2.3.0,from simdna import simulations
2.3.0,define layer out functions
2.3.0,get layer outputs for a positive simulation example
2.3.0,plot layer outputs
2.3.0,highlight motif sites
2.3.0,get a positive and a negative example from the simulation data
2.3.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.3.0,get motif site locations
2.3.0,organize legends
2.3.0,plot scores and highlight motif site locations
2.3.0,initialize fwd and reverse scores to -infinity
2.3.0,"cross-correlate separately for each base,"
2.3.0,for both the PSSM and its reverse complement
2.3.0,sum over the bases
2.3.0,take max of fwd and reverse scores at each position
2.3.0,return 1D view of sequence characters
2.3.0,class SequenceDNN(Model):
2.3.0,""""""""
2.3.0,Sequence DNN models.
2.3.0,
2.3.0,Parameters
2.3.0,----------
2.3.0,"seq_length : int, optional"
2.3.0,length of input sequence.
2.3.0,"keras_model : instance of keras.models.Sequential, optional"
2.3.0,seq_length or keras_model must be specified.
2.3.0,"num_tasks : int, optional"
2.3.0,number of tasks. Default: 1.
2.3.0,num_filters : list[int] | tuple[int]
2.3.0,"number of convolutional filters in each layer. Default: (15,)."
2.3.0,conv_width : list[int] | tuple[int]
2.3.0,"width of each layer's convolutional filters. Default: (15,)."
2.3.0,pool_width : int
2.3.0,width of max pooling after the last layer. Default: 35.
2.3.0,L1 : float
2.3.0,strength of L1 penalty.
2.3.0,dropout : float
2.3.0,dropout probability in every convolutional layer. Default: 0.
2.3.0,verbose: int
2.3.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.3.0,
2.3.0,Returns
2.3.0,-------
2.3.0,Compiled DNN model.
2.3.0,""""""""
2.3.0,
2.3.0,"def __init__(self,"
2.3.0,"seq_length=None,"
2.3.0,"keras_model=None,"
2.3.0,"use_RNN=False,"
2.3.0,"num_tasks=1,"
2.3.0,"num_filters=(15, 15, 15),"
2.3.0,"conv_width=(15, 15, 15),"
2.3.0,"pool_width=35,"
2.3.0,"GRU_size=35,"
2.3.0,"TDD_size=15,"
2.3.0,"L1=0,"
2.3.0,"dropout=0.0,"
2.3.0,"num_epochs=100,"
2.3.0,verbose=1):
2.3.0,self.num_tasks = num_tasks
2.3.0,self.num_epochs = num_epochs
2.3.0,self.verbose = verbose
2.3.0,self.train_metrics = []
2.3.0,self.valid_metrics = []
2.3.0,if keras_model is not None and seq_length is None:
2.3.0,self.model = keras_model
2.3.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.3.0,elif seq_length is not None and keras_model is None:
2.3.0,self.model = Sequential()
2.3.0,assert len(num_filters) == len(conv_width)
2.3.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.3.0,conv_height = 4 if i == 0 else 1
2.3.0,self.model.add(
2.3.0,Convolution2D(
2.3.0,"nb_filter=nb_filter,"
2.3.0,"nb_row=conv_height,"
2.3.0,"nb_col=nb_col,"
2.3.0,"activation='linear',"
2.3.0,"init='he_normal',"
2.3.0,"input_shape=(1, 4, seq_length),"
2.3.0,"W_regularizer=l1(L1),"
2.3.0,b_regularizer=l1(L1)))
2.3.0,self.model.add(Activation('relu'))
2.3.0,self.model.add(Dropout(dropout))
2.3.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.3.0,if use_RNN:
2.3.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.3.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.3.0,"self.model.add(Permute((2, 1)))"
2.3.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.3.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.3.0,self.model.add(Flatten())
2.3.0,self.model.add(Dense(output_dim=self.num_tasks))
2.3.0,self.model.add(Activation('sigmoid'))
2.3.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.3.0,else:
2.3.0,raise ValueError(
2.3.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.3.0,
2.3.0,"def train(self,"
2.3.0,"X,"
2.3.0,"y,"
2.3.0,"validation_data,"
2.3.0,"early_stopping_metric='Loss',"
2.3.0,"early_stopping_patience=5,"
2.3.0,save_best_model_to_prefix=None):
2.3.0,if y.dtype != bool:
2.3.0,"assert set(np.unique(y)) == {0, 1}"
2.3.0,y = y.astype(bool)
2.3.0,multitask = y.shape[1] > 1
2.3.0,if not multitask:
2.3.0,num_positives = y.sum()
2.3.0,num_sequences = len(y)
2.3.0,num_negatives = num_sequences - num_positives
2.3.0,if self.verbose >= 1:
2.3.0,print('Training model (* indicates new best result)...')
2.3.0,"X_valid, y_valid = validation_data"
2.3.0,early_stopping_wait = 0
2.3.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.3.0,"for epoch in range(1, self.num_epochs + 1):"
2.3.0,self.model.fit(
2.3.0,"X,"
2.3.0,"y,"
2.3.0,"batch_size=128,"
2.3.0,"nb_epoch=1,"
2.3.0,class_weight={
2.3.0,"True: num_sequences / num_positives,"
2.3.0,False: num_sequences / num_negatives
2.3.0,"} if not multitask else None,"
2.3.0,verbose=self.verbose >= 2)
2.3.0,"epoch_train_metrics = self.test(X, y)"
2.3.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.3.0,self.train_metrics.append(epoch_train_metrics)
2.3.0,self.valid_metrics.append(epoch_valid_metrics)
2.3.0,if self.verbose >= 1:
2.3.0,print('Epoch {}:'.format(epoch))
2.3.0,print('Train {}'.format(epoch_train_metrics))
2.3.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.3.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.3.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.3.0,if self.verbose >= 1:
2.3.0,print(' *')
2.3.0,best_metric = current_metric
2.3.0,best_epoch = epoch
2.3.0,early_stopping_wait = 0
2.3.0,if save_best_model_to_prefix is not None:
2.3.0,self.save(save_best_model_to_prefix)
2.3.0,else:
2.3.0,if self.verbose >= 1:
2.3.0,print()
2.3.0,if early_stopping_wait >= early_stopping_patience:
2.3.0,break
2.3.0,early_stopping_wait += 1
2.3.0,if self.verbose >= 1:
2.3.0,print('Finished training after {} epochs.'.format(epoch))
2.3.0,if save_best_model_to_prefix is not None:
2.3.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.3.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.3.0,"best_epoch, save_best_model_to_prefix))"
2.3.0,
2.3.0,"def predict(self, X):"
2.3.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.3.0,
2.3.0,def get_sequence_filters(self):
2.3.0,""""""""
2.3.0,Returns 3D array of 2D sequence filters.
2.3.0,""""""""
2.3.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.3.0,
2.3.0,"def deeplift(self, X, batch_size=200):"
2.3.0,""""""""
2.3.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.3.0,""""""""
2.3.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.3.0,from deeplift.conversion import keras_conversion as kc
2.3.0,
2.3.0,# convert to deeplift model and get scoring function
2.3.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.3.0,score_func = deeplift_model.get_target_contribs_func(
2.3.0,find_scores_layer_idx=0)
2.3.0,# use a 40% GC reference
2.3.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.3.0,# get deeplift scores
2.3.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.3.0,for i in range(self.num_tasks):
2.3.0,deeplift_scores[i] = score_func(
2.3.0,"task_idx=i,"
2.3.0,"input_data_list=[X],"
2.3.0,"batch_size=batch_size,"
2.3.0,"progress_update=None,"
2.3.0,input_references_list=input_references)
2.3.0,return deeplift_scores
2.3.0,
2.3.0,"def in_silico_mutagenesis(self, X):"
2.3.0,""""""""
2.3.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.3.0,""""""""
2.3.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.3.0,wild_type_predictions = self.predict(X)
2.3.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.3.0,np.newaxis]
2.3.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.3.0,"zip(X, wild_type_predictions)):"
2.3.0,mutated_sequences = np.repeat(
2.3.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.3.0,# remove wild-type
2.3.0,arange = np.arange(len(mutated_sequences))
2.3.0,horizontal_cycle = np.tile(
2.3.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.3.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.3.0,# add mutant
2.3.0,vertical_repeat = np.repeat(
2.3.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.3.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.3.0,# make mutant predictions
2.3.0,mutated_predictions = self.predict(mutated_sequences)
2.3.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.3.0,"(self.num_tasks,))"
2.3.0,mutagenesis_scores[
2.3.0,sequence_index] = wild_type_prediction - mutated_predictions
2.3.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.3.0,
2.3.0,@staticmethod
2.3.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.3.0,from dragonn.plot import plot_bases_on_ax
2.3.0,scores = score_func(X).squeeze(
2.3.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.3.0,try:
2.3.0,os.makedirs(output_directory)
2.3.0,except OSError:
2.3.0,pass
2.3.0,num_tasks = len(scores)
2.3.0,"for task_index, task_scores in enumerate(scores):"
2.3.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.3.0,# sequence_scores is num_bases x sequence_length
2.3.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.3.0,plt.clf()
2.3.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.3.0,top_axis.plot(
2.3.0,"range(1,"
2.3.0,"len(basewise_max_sequence_scores) + 1),"
2.3.0,basewise_max_sequence_scores)
2.3.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.3.0,peak_position = basewise_max_sequence_scores.argmax()
2.3.0,top_axis.axvspan(
2.3.0,"peak_position - peak_width,"
2.3.0,"peak_position + peak_width,"
2.3.0,"color='grey',"
2.3.0,alpha=0.1)
2.3.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.3.0,peak_position + peak_width].T
2.3.0,# Set non-max letter_heights to zero
2.3.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.3.0,"letter_heights[np.arange(len(letter_heights)),"
2.3.0,peak_sequence_scores.argmax(axis=1)] = \
2.3.0,basewise_max_sequence_scores[peak_position - peak_width :
2.3.0,peak_position + peak_width]
2.3.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.3.0,bottom_axis.set_xticklabels(
2.3.0,tuple(
2.3.0,"map(str,"
2.3.0,"np.arange(peak_position - peak_width,"
2.3.0,peak_position + peak_width + 1))))
2.3.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.3.0,plt.xlabel('Position')
2.3.0,plt.ylabel('Score')
2.3.0,plt.savefig(
2.3.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.3.0,"sequence_index, '_task_{}'.format(task_index)"
2.3.0,if num_tasks > 1 else '')))
2.3.0,plt.close()
2.3.0,
2.3.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.3.0,self._plot_scores(
2.3.0,"X,"
2.3.0,"output_directory,"
2.3.0,"peak_width,"
2.3.0,"score_func=self.deeplift,"
2.3.0,score_name='DeepLift')
2.3.0,
2.3.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.3.0,self._plot_scores(
2.3.0,"X,"
2.3.0,"output_directory,"
2.3.0,"peak_width,"
2.3.0,"score_func=self.in_silico_mutagenesis,"
2.3.0,score_name='ISM')
2.3.0,
2.3.0,"def plot_architecture(self, output_file):"
2.3.0,from dragonn.visualize_util import plot as plot_keras_model
2.3.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.3.0,
2.3.0,"def save(self, save_best_model_to_prefix):"
2.3.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.3.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.3.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.3.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.3.0,
2.3.0,@staticmethod
2.3.0,"def load(arch_fname, weights_fname=None):"
2.3.0,model_json_string = open(arch_fname).read()
2.3.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.3.0,if weights_fname is not None:
2.3.0,sequence_dnn.model.load_weights(weights_fname)
2.3.0,return sequence_dnn
2.3.0,create temporary fasta files
2.3.0,run command
2.3.0,remove fasta files
2.3.0,write test fasta file
2.3.0,test gkmsvm
2.3.0,get classification results
2.3.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.3.0,"Using canonical smiles for glycine, as in original research paper"
2.3.0,Atom features with padding
2.3.0,A_tilda_k computation
2.3.0,Final feed_dict setup
2.3.0,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.3.0,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.3.0,self.num_node_features)
2.3.0,Fit models
2.3.0,Args
2.3.0,2017 DeepCrystal Technologies - Patrick Hop
2.3.0,
2.3.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.3.0,
2.3.0,MIT License - have fun!!
2.3.0,===========================================================
2.3.0,x = F.selu( fc(x) )
2.3.0,x = F.selu( fc(x) )
2.3.0,2017 DeepCrystal Technologies - Patrick Hop
2.3.0,
2.3.0,Data loading a splitting file
2.3.0,
2.3.0,MIT License - have fun!!
2.3.0,===========================================================
2.3.0,Args
2.3.0,TODO (VIGS25): Account for the reload option
2.3.0,Downloading train files
2.3.0,Parsing training data
2.3.0,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.3.0,Test Files loading
2.3.0,One Hot Featurization
2.3.0,Consistency check
2.3.0,Handle output layer
2.3.0,Iterate over all previous tasks.
2.3.0,prev_layers is a list with elements of size
2.3.0,"(batch_size, layer_sizes[i-1])"
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,Save an initial checkpoint.
2.3.0,Turns out there are valid cases where we don't want pad-batches
2.3.0,on by default.
2.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.3.0,Run training op.
2.3.0,Always save a final checkpoint when complete.
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,Note that we divide by the batch size and not the number of
2.3.0,"non-zero weight examples in the batch.  Also, instead of using"
2.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.3.0,calculate with div/sum so it stays on the GPU.
2.3.0,aggregated costs
2.3.0,weight decay
2.3.0,Dummy placeholders
2.3.0,Dummy placeholders
2.3.0,run eval data through the model
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,Handle edge case when batch-size is 1.
2.3.0,Prune away any padding that was added
2.3.0,allow_soft_placement=True allows ops without a GPU implementation
2.3.0,to run on the CPU instead.
2.3.0,!/usr/bin/python
2.3.0,
2.3.0,Copyright 2015 Google Inc.
2.3.0,
2.3.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.3.0,you may not use this file except in compliance with the License.
2.3.0,You may obtain a copy of the License at
2.3.0,
2.3.0,http://www.apache.org/licenses/LICENSE-2.0
2.3.0,
2.3.0,"Unless required by applicable law or agreed to in writing, software"
2.3.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.3.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.3.0,See the License for the specific language governing permissions and
2.3.0,limitations under the License.
2.3.0,parse CheckpointState proto
2.3.0,parse path to actual checkpoint
2.3.0,the provided mask has to be the same shape as features
2.3.0,test k = 1..4
2.3.0,central moments
2.3.0,standardized moments
2.3.0,central across one axis
2.3.0,standardized across one axis
2.3.0,Fit just on task zero
2.3.0,Notice that we keep the session open
2.3.0,Fit on task one
2.3.0,The predictions for task zero should not change after training
2.3.0,on task one.
2.3.0,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.3.0,!/usr/bin/python
2.3.0,
2.3.0,Copyright 2015 Google Inc.
2.3.0,
2.3.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.3.0,you may not use this file except in compliance with the License.
2.3.0,You may obtain a copy of the License at
2.3.0,
2.3.0,http://www.apache.org/licenses/LICENSE-2.0
2.3.0,
2.3.0,"Unless required by applicable law or agreed to in writing, software"
2.3.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.3.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.3.0,See the License for the specific language governing permissions and
2.3.0,limitations under the License.
2.3.0,get the divisor
2.3.0,compute the requested central moment
2.3.0,"note that mean is a raw moment, not a central moment"
2.3.0,TODO(user): median is not implemented yet in TensorFlow
2.3.0,Add the input features.
2.3.0,"layer has shape [None, layer_sizes[i]]"
2.3.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.3.0,TODO(rbharath): Might want to make it feasible to have multiple
2.3.0,bypass layers.
2.3.0,Construct task bypass layer
2.3.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.3.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.3.0,"layer has shape [None, layer_sizes[i]]"
2.3.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.3.0,TODO(rbharath): Might want to make it feasible to have multiple
2.3.0,bypass layers.
2.3.0,Construct task bypass layer
2.3.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.3.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.3.0,Consistency check
2.3.0,Lazily created by _get_shared_session().
2.3.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.3.0,when subclass-overridden methods use the same scopes.
2.3.0,Setup graph
2.3.0,Create placeholders
2.3.0,Handle output layer
2.3.0,Iterate over all previous tasks.
2.3.0,prev_layers is a list with elements of size
2.3.0,"(batch_size, layer_sizes[i-1])"
2.3.0,Note that we divide by the batch size and not the number of
2.3.0,"non-zero weight examples in the batch.  Also, instead of using"
2.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.3.0,calculate with div/sum so it stays on the GPU.
2.3.0,aggregated costs
2.3.0,weight decay
2.3.0,Dummy placeholders
2.3.0,Dummy placeholders
2.3.0,run eval data through the model
2.3.0,"Shape (n_tasks, n__samples)"
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,Handle edge case when batch-size is 1.
2.3.0,with self._get_shared_session(train=True) as sess:
2.3.0,Save an initial checkpoint.
2.3.0,Always save a final checkpoint when complete.
2.3.0,Note that we divide by the batch size and not the number of
2.3.0,"non-zero weight examples in the batch.  Also, instead of using"
2.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.3.0,calculate with div/sum so it stays on the GPU.
2.3.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.3.0,Dummy placeholders
2.3.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.3.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.3.0,Dummy placeholders
2.3.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.3.0,allow_soft_placement=True allows ops without a GPU implementation
2.3.0,to run on the CPU instead.
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,Turns out there are valid cases where we don't want pad-batches
2.3.0,on by default.
2.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.3.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.3.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,"(n_samples, n_classes)"
2.3.0,"(n_samples, n_tasks, n_classes)"
2.3.0,Save hyperparameters
2.3.0,Guard variable to make sure we don't Restore() this model
2.3.0,from a disk checkpoint more than once.
2.3.0,"Path to save checkpoint files, which matches the"
2.3.0,replicated supervisor's default path.
2.3.0,Lazily created by _get_shared_session().
2.3.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.3.0,when subclass-overridden methods use the same scopes.
2.3.0,Setup graph
2.3.0,Note that we divide by the batch size and not the number of
2.3.0,"non-zero weight examples in the batch.  Also, instead of using"
2.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.3.0,calculate with div/sum so it stays on the GPU.
2.3.0,aggregated costs
2.3.0,weight decay
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,Save an initial checkpoint.
2.3.0,Define the code that runs on a separate thread to feed data into the queue.
2.3.0,Main training loop.
2.3.0,Run training op.
2.3.0,We have reached the end of an epoch.
2.3.0,We have reached the end of the data.
2.3.0,Always save a final checkpoint when complete.
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,allow_soft_placement=True allows ops without a GPU implementation
2.3.0,to run on the CPU instead.
2.3.0,gpu memory growth option
2.3.0,gpu memory growth option
2.3.0,TODO(rbharath): Is setting train=False right here?
2.3.0,Discard any padded predictions
2.3.0,The iterbatches does padding with zero-weight examples on the last batch.
2.3.0,Remove padded examples.
2.3.0,Special case to handle singletasks.
2.3.0,The iterbatches does padding with zero-weight examples on the last batch.
2.3.0,Remove padded examples.
2.3.0,TODO(rbharath): Verify this can be safely removed.
2.3.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.3.0,""""""""
2.3.0,Evaluates the performance of this model on specified dataset.
2.3.0,
2.3.0,Parameters
2.3.0,----------
2.3.0,dataset: dc.data.Dataset
2.3.0,Dataset object.
2.3.0,metric: deepchem.metrics.Metric
2.3.0,Evaluation metric
2.3.0,transformers: list
2.3.0,List of deepchem.transformers.Transformer
2.3.0,Returns
2.3.0,-------
2.3.0,dict
2.3.0,Maps tasks to scores under metric.
2.3.0,""""""""
2.3.0,"evaluator = Evaluator(self, dataset, transformers)"
2.3.0,scores = evaluator.compute_model_performance(metrics)
2.3.0,return scores
2.3.0,checkpoints look like model_dir/model.ckpt-N
2.3.0,"self._save_path is ""model_dir/model.ckpt"""
2.3.0,run eval data through the model
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,run eval data through the model
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,Note that softmax is already applied in construct_grpah
2.3.0,run eval data through the model
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,Handle edge case when batch-size is 1.
2.3.0,Prune away any padding that was added
2.3.0,Handle case of 0-dimensional scalar output
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,inputs placeholder
2.3.0,data preprocessing and augmentation
2.3.0,first conv layer
2.3.0,downsample by max pooling
2.3.0,each module is a residual convolutional block
2.3.0,followed by a convolutional downsample layer
2.3.0,max pooling over the final outcome
2.3.0,fully connected layers
2.3.0,dropout for dense layers
2.3.0,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.3.0,weight decay regularizer
2.3.0,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.3.0,sample cut ratio from a clipped gaussian
2.3.0,train/valid differences
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,Define and build model
2.3.0,model.restore()
2.3.0,Set random seeds
2.3.0,Setup directories
2.3.0,Model constants
2.3.0,Load and transform datasets
2.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.3.0,Atomic convolution variables
2.3.0,at = atomic numbers (atom types)
2.3.0,"radial basis function parameters [cutoff, mean, width]"
2.3.0,Model hyperparameters
2.3.0,Initialize model
2.3.0,Fit model
2.3.0,Evaluate model
2.3.0,Set random seeds
2.3.0,Setup directories
2.3.0,Model constants
2.3.0,Load and transform datasets
2.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.3.0,Atomic convolution variables
2.3.0,at = atomic numbers (atom types)
2.3.0,"radial basis function parameters [cutoff, mean, width]"
2.3.0,Model hyperparameters
2.3.0,Initialize model
2.3.0,Fit model
2.3.0,Evaluate model
2.3.0,Set random seeds
2.3.0,Setup directories
2.3.0,Model constants
2.3.0,Load and transform datasets
2.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.3.0,Atomic convolution variables
2.3.0,at = atomic numbers (atom types)
2.3.0,"radial basis function parameters [cutoff, mean, width]"
2.3.0,Model hyperparameters
2.3.0,Initialize model
2.3.0,Fit model
2.3.0,Evaluate model
2.3.0,Set random seeds
2.3.0,Setup directories
2.3.0,Model constants
2.3.0,Load and transform datasets
2.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.3.0,Atomic convolution variables
2.3.0,at = atomic numbers (atom types)
2.3.0,"radial basis function parameters [cutoff, mean, width]"
2.3.0,Model hyperparameters
2.3.0,Initialize model
2.3.0,Fit model
2.3.0,Evaluate model
2.3.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.3.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.3.0,test_scores = test_evaluator.compute_model_performance(metric)
2.3.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.3.0,param.update(test_scores)
2.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.3.0,for transformer in transformers:
2.3.0,train_dataset = transformer.transform(train_dataset)
2.3.0,test_dataset = transformer.transform(test_dataset)
2.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.3.0,for transformer in transformers:
2.3.0,train_dataset = transformer.transform(train_dataset)
2.3.0,test_dataset = transformer.transform(test_dataset)
2.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.3.0,for transformer in transformers:
2.3.0,train_dataset = transformer.transform(train_dataset)
2.3.0,test_dataset = transformer.transform(test_dataset)
2.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.3.0,for transformer in transformers:
2.3.0,train_dataset = transformer.transform(train_dataset)
2.3.0,test_dataset = transformer.transform(test_dataset)
2.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.3.0,Create some directories for analysis
2.3.0,The base_dir holds the results of all analysis
2.3.0,Make directories to store the raw and featurized datasets.
2.3.0,Load PDBBind dataset
2.3.0,Define featurizers
2.3.0,Currently featurizes with shard_size=1
2.3.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.3.0,This could be done with openbabel in python
2.3.0,Compute cells for this molecule. O(constant)
2.3.0,min == max if molecule is planar in some direction
2.3.0,we should still create a bin
2.3.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.3.0,Note neighbors contains self!
2.3.0,Associate each atom with cell it belongs to. O(N)
2.3.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.3.0,"conditions, so does wrapround. O(constant)"
2.3.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.3.0,Accept as neighbors only those within threshold. This computation should be
2.3.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.3.0,Sort neighbors by distance
2.3.0,Pick up to max_num_neighbors
2.3.0,Type of data created by this featurizer
2.3.0,assumes that every array is of the same dimension
2.3.0,rem_dataset is remaining portion of dataset
2.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.3.0,to k-1.
2.3.0,returns list of per column sum of non zero elements
2.3.0,Compute number of actives needed per task.
2.3.0,loop through each column and obtain index required to splice out for
2.3.0,required fraction of hits
2.3.0,Find the first index where the cumulative number of actives equals
2.3.0,the actives_count
2.3.0,Note that np.where tells us last index required to exceed
2.3.0,"actives_count, so we actually want the following location"
2.3.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.3.0,potentially refactor those to match this.
2.3.0,Handle edge case where frac_split is 1
2.3.0,Create weight matrices fpor two haves.
2.3.0,copy over up to required index for weight first_split
2.3.0,check out if any rows in either w_1 or w_2 are just zeros
2.3.0,"Obtain original x, y, and w arrays and shuffle"
2.3.0,calculate percent split for valid (out of test and valid)
2.3.0,"split test data into valid and test, treating sub test set also as sparse"
2.3.0,rem_dataset is remaining portion of dataset
2.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.3.0,to k-1.
2.3.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.3.0,This can be generalized in the future with some common demoninator determination.
2.3.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.3.0,Append remaining examples to train
2.3.0,Sort by increasing MW
2.3.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.3.0,for m_idx in cluster:
2.3.0,"continue until we find an active in all the tasks, otherwise we can't"
2.3.0,compute a meaningful AUC
2.3.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.3.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.3.0,Sort from largest to smallest scaffold sets
2.3.0,Sort from largest to smallest scaffold sets
2.3.0,"(n_samples, n_classes)"
2.3.0,"(n_samples, n_tasks, n_classes)"
2.3.0,Save hyperparameters
2.3.0,Guard variable to make sure we don't Restore() this model
2.3.0,from a disk checkpoint more than once.
2.3.0,"Path to save checkpoint files, which matches the"
2.3.0,replicated supervisor's default path.
2.3.0,Lazily created by _get_shared_session().
2.3.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.3.0,when subclass-overridden methods use the same scopes.
2.3.0,Setup graph
2.3.0,Note that we divide by the batch size and not the number of
2.3.0,"non-zero weight examples in the batch.  Also, instead of using"
2.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.3.0,calculate with div/sum so it stays on the GPU.
2.3.0,aggregated costs
2.3.0,weight decay
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,Save an initial checkpoint.
2.3.0,Turns out there are valid cases where we don't want pad-batches
2.3.0,on by default.
2.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.3.0,Run training op.
2.3.0,Always save a final checkpoint when complete.
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,allow_soft_placement=True allows ops without a GPU implementation
2.3.0,to run on the CPU instead.
2.3.0,TODO(rbharath): Is setting train=False right here?
2.3.0,Discard any padded predictions
2.3.0,The iterbatches does padding with zero-weight examples on the last batch.
2.3.0,Remove padded examples.
2.3.0,Special case to handle singletasks.
2.3.0,The iterbatches does padding with zero-weight examples on the last batch.
2.3.0,Remove padded examples.
2.3.0,TODO(rbharath): Verify this can be safely removed.
2.3.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.3.0,""""""""
2.3.0,Evaluates the performance of this model on specified dataset.
2.3.0,
2.3.0,Parameters
2.3.0,----------
2.3.0,dataset: dc.data.Dataset
2.3.0,Dataset object.
2.3.0,metric: deepchem.metrics.Metric
2.3.0,Evaluation metric
2.3.0,transformers: list
2.3.0,List of deepchem.transformers.Transformer
2.3.0,Returns
2.3.0,-------
2.3.0,dict
2.3.0,Maps tasks to scores under metric.
2.3.0,""""""""
2.3.0,"evaluator = Evaluator(self, dataset, transformers)"
2.3.0,scores = evaluator.compute_model_performance(metrics)
2.3.0,return scores
2.3.0,checkpoints look like logdir/model.ckpt-N
2.3.0,"self._save_path is ""logdir/model.ckpt"""
2.3.0,run eval data through the model
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,run eval data through the model
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,Note that softmax is already applied in construct_grpah
2.3.0,run eval data through the model
2.3.0,reshape to batch_size x n_tasks x ...
2.3.0,Handle edge case when batch-size is 1.
2.3.0,Prune away any padding that was added
2.3.0,Handle case of 0-dimensional scalar output
2.3.0,Dummy placeholders
2.3.0,Dummy placeholders
2.3.0,## AtomicNet fully-connected layer ops ###
2.3.0,## Atomicnet coordinate transform ops ###
2.3.0,## Atomicnet symmetry function kernel ops ###
2.3.0,## Atomicnet symmetry function ops ###
2.3.0,## Atomcnet symmetry function layer ops ###
2.3.0,We apply the radial pooling filter before atom type conv
2.3.0,to reduce computation
2.3.0,## Misc convenience ops ###
2.3.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.3.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.3.0,"game).  The average reward for any bet is slightly negative, so the best"
2.3.0,strategy is to walk away.
2.3.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.3.0,Optimize it.
2.3.0,"It should have learned that the expected value is very close to zero, and that the best"
2.3.0,action is to walk away.
2.3.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.3.0,get the same result.
2.3.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.3.0,Run the algorithm.
2.3.0,Save a file checkpoint.
2.3.0,Build the tree.
2.3.0,Compute the final probabilities and expected reward.
2.3.0,Mark this node as terminal
2.3.0,Expand this node.
2.3.0,Select the next action to perform.
2.3.0,Recursively build the tree.
2.3.0,Update statistics for this node.
2.3.0,"Copied from the yt_project, commit e8fb57e"
2.3.0,yt/doc/extensions/notebook_sphinxext.py
2.3.0,https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
2.3.0,Almost completely re-written by Matthew Harrigan to use nbconvert v4
2.3.0,1. Uneval notebook
2.3.0,2. Python
2.3.0,3. HTML (execute first)
2.3.0,Set per-cell timeout to 60 seconds
2.3.0,4. Eval'd notebook
2.3.0,Create link to notebook and script files
2.3.0,create notebook node
2.3.0,add dependency
2.3.0,-*- coding: utf-8 -*-
2.3.0,
2.3.0,"deepchem documentation build configuration file, created by"
2.3.0,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
2.3.0,
2.3.0,This file is execfile()d with the current directory set to its
2.3.0,containing dir.
2.3.0,
2.3.0,Note that not all possible configuration values are present in this
2.3.0,autogenerated file.
2.3.0,
2.3.0,All configuration values have a default; values that are commented out
2.3.0,serve to show the default.
2.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.3.0,add these directories to sys.path here. If the directory is relative to the
2.3.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.3.0,"sys.path.insert(0, os.path.abspath('.'))"
2.3.0,-- General configuration ------------------------------------------------
2.3.0,"If your documentation needs a minimal Sphinx version, state it here."
2.3.0,needs_sphinx = '1.0'
2.3.0,"Add any Sphinx extension module names here, as strings. They can be"
2.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.3.0,ones.
2.3.0,"Add any paths that contain templates here, relative to this directory."
2.3.0,The suffix(es) of source filenames.
2.3.0,You can specify multiple suffix as a list of string:
2.3.0,"source_suffix = ['.rst', '.md']"
2.3.0,The encoding of source files.
2.3.0,source_encoding = 'utf-8-sig'
2.3.0,The master toctree document.
2.3.0,General information about the project.
2.3.0,"The version info for the project you're documenting, acts as replacement for"
2.3.0,"|version| and |release|, also used in various other places throughout the"
2.3.0,built documents.
2.3.0,
2.3.0,The short X.Y version.
2.3.0,"The full version, including alpha/beta/rc tags."
2.3.0,The language for content autogenerated by Sphinx. Refer to documentation
2.3.0,for a list of supported languages.
2.3.0,
2.3.0,This is also used if you do content translation via gettext catalogs.
2.3.0,"Usually you set ""language"" from the command line for these cases."
2.3.0,"There are two options for replacing |today|: either, you set today to some"
2.3.0,"non-false value, then it is used:"
2.3.0,today = ''
2.3.0,"Else, today_fmt is used as the format for a strftime call."
2.3.0,"today_fmt = '%B %d, %Y'"
2.3.0,"List of patterns, relative to source directory, that match files and"
2.3.0,directories to ignore when looking for source files.
2.3.0,The reST default role (used for this markup: `text`) to use for all
2.3.0,documents.
2.3.0,default_role = None
2.3.0,"If true, '()' will be appended to :func: etc. cross-reference text."
2.3.0,add_function_parentheses = True
2.3.0,"If true, the current module name will be prepended to all description"
2.3.0,unit titles (such as .. function::).
2.3.0,add_module_names = True
2.3.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
2.3.0,output. They are ignored by default.
2.3.0,show_authors = False
2.3.0,The name of the Pygments (syntax highlighting) style to use.
2.3.0,A list of ignored prefixes for module index sorting.
2.3.0,modindex_common_prefix = []
2.3.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
2.3.0,keep_warnings = False
2.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.3.0,-- Options for HTML output ----------------------------------------------
2.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.3.0,a list of builtin themes.
2.3.0,Theme options are theme-specific and customize the look and feel of a theme
2.3.0,"further.  For a list of options available for each theme, see the"
2.3.0,documentation.
2.3.0,html_theme_options = {}
2.3.0,"Add any paths that contain custom themes here, relative to this directory."
2.3.0,"The name for this set of Sphinx documents.  If None, it defaults to"
2.3.0,"""<project> v<release> documentation""."
2.3.0,html_title = None
2.3.0,A shorter title for the navigation bar.  Default is the same as html_title.
2.3.0,html_short_title = None
2.3.0,The name of an image file (relative to this directory) to place at the top
2.3.0,of the sidebar.
2.3.0,The name of an image file (within the static path) to use as favicon of the
2.3.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
2.3.0,pixels large.
2.3.0,html_favicon = None
2.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.3.0,"relative to this directory. They are copied after the builtin static files,"
2.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.3.0,Add any extra paths that contain custom files (such as robots.txt or
2.3.0,".htaccess) here, relative to this directory. These files are copied"
2.3.0,directly to the root of the documentation.
2.3.0,html_extra_path = []
2.3.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
2.3.0,using the given strftime format.
2.3.0,"html_last_updated_fmt = '%b %d, %Y'"
2.3.0,"If true, SmartyPants will be used to convert quotes and dashes to"
2.3.0,typographically correct entities.
2.3.0,html_use_smartypants = True
2.3.0,"Custom sidebar templates, maps document names to template names."
2.3.0,html_sidebars = {}
2.3.0,"Additional templates that should be rendered to pages, maps page names to"
2.3.0,template names.
2.3.0,html_additional_pages = {}
2.3.0,"If false, no module index is generated."
2.3.0,html_domain_indices = True
2.3.0,"If false, no index is generated."
2.3.0,html_use_index = True
2.3.0,"If true, the index is split into individual pages for each letter."
2.3.0,html_split_index = False
2.3.0,"If true, links to the reST sources are added to the pages."
2.3.0,html_show_sourcelink = True
2.3.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
2.3.0,html_show_sphinx = True
2.3.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
2.3.0,html_show_copyright = True
2.3.0,"If true, an OpenSearch description file will be output, and all pages will"
2.3.0,contain a <link> tag referring to it.  The value of this option must be the
2.3.0,base URL from which the finished HTML is served.
2.3.0,html_use_opensearch = ''
2.3.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
2.3.0,html_file_suffix = None
2.3.0,Language to be used for generating the HTML full-text search index.
2.3.0,Sphinx supports the following languages:
2.3.0,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
2.3.0,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
2.3.0,html_search_language = 'en'
2.3.0,"A dictionary with options for the search language support, empty by default."
2.3.0,Now only 'ja' uses this config value
2.3.0,html_search_options = {'type': 'default'}
2.3.0,The name of a javascript file (relative to the configuration directory) that
2.3.0,"implements a search results scorer. If empty, the default will be used."
2.3.0,html_search_scorer = 'scorer.js'
2.3.0,Output file base name for HTML help builder.
2.3.0,-- Options for LaTeX output ---------------------------------------------
2.3.0,The paper size ('letterpaper' or 'a4paper').
2.3.0,"'papersize': 'letterpaper',"
2.3.0,"The font size ('10pt', '11pt' or '12pt')."
2.3.0,"'pointsize': '10pt',"
2.3.0,Additional stuff for the LaTeX preamble.
2.3.0,"'preamble': '',"
2.3.0,Latex figure (float) alignment
2.3.0,"'figure_align': 'htbp',"
2.3.0,Grouping the document tree into LaTeX files. List of tuples
2.3.0,"(source start file, target name, title,"
2.3.0,"author, documentclass [howto, manual, or own class])."
2.3.0,The name of an image file (relative to this directory) to place at the top of
2.3.0,the title page.
2.3.0,latex_logo = None
2.3.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
2.3.0,not chapters.
2.3.0,latex_use_parts = False
2.3.0,"If true, show page references after internal links."
2.3.0,latex_show_pagerefs = False
2.3.0,"If true, show URL addresses after external links."
2.3.0,latex_show_urls = False
2.3.0,Documents to append as an appendix to all manuals.
2.3.0,latex_appendices = []
2.3.0,"If false, no module index is generated."
2.3.0,latex_domain_indices = True
2.3.0,-- Options for manual page output ---------------------------------------
2.3.0,One entry per manual page. List of tuples
2.3.0,"(source start file, name, description, authors, manual section)."
2.3.0,"If true, show URL addresses after external links."
2.3.0,man_show_urls = False
2.3.0,-- Options for Texinfo output -------------------------------------------
2.3.0,Grouping the document tree into Texinfo files. List of tuples
2.3.0,"(source start file, target name, title, author,"
2.3.0,"dir menu entry, description, category)"
2.3.0,Documents to append as an appendix to all manuals.
2.3.0,texinfo_appendices = []
2.3.0,"If false, no module index is generated."
2.3.0,texinfo_domain_indices = True
2.3.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
2.3.0,texinfo_show_urls = 'footnote'
2.3.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
2.3.0,texinfo_no_detailmenu = False
2.3.0,Example configuration for intersphinx: refer to the Python standard library.
2.3.0,Higher is Better
2.3.0,The secret key is available as a secure environment variable
2.3.0,on travis-ci to push the build documentation to Amazon S3.
2.3.0,Perform recursive modification to set css mime types.
2.3.0,Perform recursive modification to set js mime types.
2.3.0,plt.show()
2.3.0,"run_benchmark(FILE, DEEPCHEM_DIR)"
2.3.0,lines in the label file have format
2.3.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.3.0,"print line[0], line[3]"
2.3.0,Record inputs.
2.3.0,Create the output directory if necessary.
2.3.0,Build the meta-learning model.
2.3.0,"In the final loss, use different placeholders for all inputs so the loss will be"
2.3.0,computed from a different batch.
2.3.0,Create variables for accumulating the gradients.
2.3.0,Create the optimizers for meta-optimization and task optimization.
2.3.0,Create a Checkpoint for saving.
2.3.0,Main optimization loop.
2.3.0,Do checkpointing.
2.3.0,This is a MetaLearner that learns to generate sine functions with variable
2.3.0,amplitude and phase.
2.3.0,Optimize it.
2.3.0,Test it out on some new tasks and see how it works.
2.3.0,Initially the model should do a bad job of fitting the sine function.
2.3.0,After one step of optimization it should do much better.
2.3.0,"If we train on the current task, the loss should go down."
2.3.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.3.0,get the same result.
2.3.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.3.0,Fit model on dataset
2.3.0,Fit model on dataset
2.3.0,"Should be an array of size (n_pocket_atoms, 3)"
2.3.0,"coords[triangle, 0] gives the x-dimension of all triangle points"
2.3.0,Take transpose to make sure rows correspond to atoms.
2.3.0,We voxelize so all grids have integral coordinates (convenience)
2.3.0,"If overlap of box with previously generated output boxes, return"
2.3.0,Carry forward mappings
2.3.0,We know that box has at least one atom not in outputs
2.3.0,Current box has been merged into box further down list.
2.3.0,No need to output current box
2.3.0,"protein_coords is (N, 3) tensor"
2.3.0,Load binding pocket model
2.3.0,TODO(rbharath): Shift refined to full once trained.
2.3.0,Fit model on dataset
2.3.0,Create featurizers
2.3.0,"if not ligand_file.endswith("".sdf""):"
2.3.0,"raise ValueError(""Only .sdf ligand files can be featurized."")"
2.3.0,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
2.3.0,ligand_mol2 = os.path.join(
2.3.0,"self.base_dir, ligand_basename + "".mol2"")"
2.3.0,
2.3.0,# Write mol2 file for ligand
2.3.0,obConversion = ob.OBConversion()
2.3.0,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
2.3.0,ob_mol = ob.OBMol()
2.3.0,"obConversion.ReadFile(ob_mol, str(ligand_file))"
2.3.0,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
2.3.0,
2.3.0,# Featurize ligand
2.3.0,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
2.3.0,if mol is None:
2.3.0,"return None, None"
2.3.0,# Default for CircularFingerprint
2.3.0,n_ligand_features = 1024
2.3.0,ligand_features = self.ligand_featurizer.featurize([mol])
2.3.0,
2.3.0,# Featurize pocket
2.3.0,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
2.3.0,"protein_file, ligand_file)"
2.3.0,n_pockets = len(pockets)
2.3.0,n_pocket_features = BindingPocketFeaturizer.n_features
2.3.0,
2.3.0,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
2.3.0,pocket_features = self.pocket_featurizer.featurize(
2.3.0,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
2.3.0,# Note broadcast operation
2.3.0,"features[:, :n_pocket_features] = pocket_features"
2.3.0,"features[:, n_pocket_features:] = ligand_features"
2.3.0,dataset = NumpyDataset(X=features)
2.3.0,pocket_preds = self.model.predict(dataset)
2.3.0,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
2.3.0,
2.3.0,# Find pockets which are active
2.3.0,active_pockets = []
2.3.0,active_pocket_atoms_map = {}
2.3.0,active_pocket_coords = []
2.3.0,for pocket_ind in range(len(pockets)):
2.3.0,#################################################### DEBUG
2.3.0,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
2.3.0,#if pocket_preds[pocket_ind] == 1:
2.3.0,if pocket_pred_proba[pocket_ind][1] > .15:
2.3.0,#################################################### DEBUG
2.3.0,pocket = pockets[pocket_ind]
2.3.0,active_pockets.append(pocket)
2.3.0,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
2.3.0,active_pocket_coords.append(pocket_coords[pocket_ind])
2.3.0,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
2.3.0,# TODO(LESWING)
2.3.0,TODO: add pi_stack and cation_pi to feature_types (it's not trivial
2.3.0,because they require sanitized molecules)
2.3.0,"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.3.0,"""salt_bridge""],"
2.3.0,TODO(rbharath): May want to move this file to S3 so we can ensure it's
2.3.0,always available.
2.3.0,Prepare receptor
2.3.0,Get protein centroid and range
2.3.0,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
2.3.0,going to be extremely slow!
2.3.0,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
2.3.0,first pocket.
2.3.0,Prepare receptor
2.3.0,TODO(rbharath): Generalize this so can support mol2 files as well.
2.3.0,Write Vina conf file
2.3.0,Define locations of log and output files
2.3.0,TODO(rbharath): Let user specify the number of poses required.
2.3.0,TODO(rbharath): Convert the output pdbqt to a pdb file.
2.3.0,Return docked files
2.3.0,Check returned files exist
2.3.0,Check returned files exist
2.3.0,Check returned files exist
2.3.0,Check returned files exist
2.3.0,Check returned files exist
2.3.0,Note this may download autodock Vina...
2.3.0,Note this may download autodock Vina...
2.3.0,Note this may download autodock Vina...
2.3.0,Check returned files exist
2.3.0,Note this may download autodock Vina...
2.3.0,Check returned files exist
2.3.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.3.0,box1 contained in box2
2.3.0,"box1 in box2, so complete overlap"
2.3.0,"4/5 atoms in box2 in box1, so 80 % overlap"
2.3.0,box2 contains box1
2.3.0,box1 contains box2
2.3.0,"box1 contains box2, box3"
2.3.0,Test that every atom in pocket maps exists
2.3.0,Check that the atoms is actually in protein
2.3.0,Test that every atom in pocket maps exists
2.3.0,Check that the atoms is actually in protein
2.3.0,Add active site to dict
2.3.0,initialize fwd and reverse scores to -infinity
2.3.0,"cross-correlate separately for each base,"
2.3.0,for both the PSSM and its reverse complement
2.3.0,sum over the bases
2.3.0,take max of fwd and reverse scores at each position
2.3.0,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.3.0,"Shape (N_sequences, num_tasks)"
2.3.0,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.3.0,Mutates every position of the sequence to every letter
2.3.0,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.3.0,Breakdown:
2.3.0,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.3.0,remove wild-type
2.3.0,len(arange) = N_letters * sequence_length
2.3.0,len(horizontal cycle) = N_letters * sequence_length
2.3.0,add mutant
2.3.0,make mutant predictions
2.3.0,The convention used is that the first task is the metric.
2.3.0,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
2.3.0,"an option in the Metric class. Instead, this should be possible to move into"
2.3.0,user-space as a custom task_averager function.
2.3.0,"TODO(rbharath, joegomes): What is this magic number?"
2.3.0,"If there are no nonzero examples, metric is ill-defined."
2.3.0,Best score case
2.3.0,Worst score case
2.3.0,Encode motif
2.3.0,"sequences now has shape (3, 4, 5, 1)"
2.3.0,"sequences now has shape (3, 4, 5, 1)"
2.3.0,Construct and train SequenceDNN model
2.3.0,"X = np.random.rand(10, 1, 4, 50)"
2.3.0,"y = np.random.randint(0, 2, size=(10, 1))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y)"
2.3.0,Call in-silico mutagenesis
2.3.0,Construct and train SequenceDNN model
2.3.0,"X = np.random.rand(10, 1, 4, 50)"
2.3.0,"y = np.random.randint(0, 2, size=(10, 1))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y)"
2.3.0,Call in-silico mutagenesis
2.3.0,Check nonzero elements exist
2.3.0,ids = df[id_field].values
2.3.0,Set missing data to have weight zero
2.3.0,TODO (ytz) this is a bandage solution to reorder the atoms so
2.3.0,that they're always in the same canonical order. Presumably this
2.3.0,should be correctly implemented in the future for graph mols.
2.3.0,Featurize task results iff they exist.
2.3.0,Filter out examples where featurization failed.
2.3.0,"For prospective data where results are unknown, it makes"
2.3.0,no sense to have y values or weights.
2.3.0,"(X, y, w, ids)"
2.3.0,Sometimes zip files contain directories within. Traverse directories
2.3.0,TODO(rbharath): Add support for more extensions
2.3.0,Remove support indices
2.3.0,Remove support indices
2.3.0,Remove support indices
2.3.0,Get task specific entries
2.3.0,Now just get weights for this task
2.3.0,Get task specific entries
2.3.0,Now just get weights for this task
2.3.0,Now just get weights for this task
2.3.0,Now just get weights for this task
2.3.0,Split data into pos and neg lists.
2.3.0,No replacement allowed for supports
2.3.0,Handle one-d vs. non one-d feature matrices
2.3.0,Init the iterator
2.3.0,Set initial iterator state
2.3.0,support = self.supports[task][self.trial_num]
2.3.0,Increment and update logic
2.3.0,Init the iterator
2.3.0,Set initial iterator state
2.3.0,support = self.supports[task][self.trial_num]
2.3.0,Increment and update logic
2.3.0,"By invariant of when this is called, can assume num_samples > 0"
2.3.0,and num_samples < batch_size
2.3.0,Fill in batch arrays
2.3.0,"By invariant of when this is called, can assume num_samples > 0"
2.3.0,and num_samples < batch_size
2.3.0,Fill in batch arrays
2.3.0,Only the first set of copy will be counted in training loss
2.3.0,Retrieve the first sample so we can determine the dtypes.
2.3.0,Create a Tensorflow Dataset and have it create an Iterator.
2.3.0,"Set labels to be zero, with zero weights"
2.3.0,Load obsolete format -> save in new format
2.3.0,note that this corresponds to the _construct_metadata column order
2.3.0,if not len(self.metadata_df):
2.3.0,"raise ValueError(""No data in dataset."")"
2.3.0,return next(self.metadata_df.iterrows())[1]['task_names']
2.3.0,Create temp directory to store resharded version
2.3.0,Write data in new shards
2.3.0,Handle spillover from last shard
2.3.0,These columns may be missing is the dataset is unlabelled.
2.3.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.3.0,"than process based pools, since process based pools need to pickle/serialize"
2.3.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.3.0,we're actually protected by the GIL.
2.3.0,(ytz): this skips everything except possibly the last shard
2.3.0,"raw_data = (X, y, w, ids)"
2.3.0,Protect against generator exhaustion
2.3.0,This ensures tasks are consistent for all datasets
2.3.0,Get full dataset in memory
2.3.0,Shuffle in memory
2.3.0,Write shuffled shards out to disk
2.3.0,Shuffle the arrays corresponding to each row in metadata_df
2.3.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.3.0,Handle edge case with empty indices
2.3.0,Find indices which rest in this shard
2.3.0,Need to offset indices to fit within shard_size
2.3.0,Handle the case of datasets with y/w missing
2.3.0,Updating counts
2.3.0,Break when all indices have been used up already
2.3.0,TODO(rbharath): Get rid of * import
2.3.0,Load MUV dataset
2.3.0,Do an approximate comparison since splits are sometimes slightly off from
2.3.0,the exact fraction.
2.3.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.3.0,reloading will cause the transform to be reapplied. This is undesirable in
2.3.0,almost all cases. Need to understand a method to fix this.
2.3.0,def test_shuffle(self):
2.3.0,"""""""Test that datasets can be merged."""""""
2.3.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.3.0,dataset_file = os.path.join(
2.3.0,"current_dir, ""../../models/tests/example.csv"")"
2.3.0,featurizer = dc.feat.CircularFingerprint(size=1024)
2.3.0,"tasks = [""log-solubility""]"
2.3.0,loader = dc.data.CSVLoader(
2.3.0,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
2.3.0,"dataset = loader.featurize(dataset_file, shard_size=2)"
2.3.0,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
2.3.0,dataset.ids)
2.3.0,orig_len = len(dataset)
2.3.0,dataset.shuffle(iterations=5)
2.3.0,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
2.3.0,dataset.ids)
2.3.0,
2.3.0,assert len(dataset) == orig_len
2.3.0,# The shuffling should have switched up the ordering
2.3.0,"assert not np.array_equal(orig_ids, new_ids)"
2.3.0,# But all the same entries should still be present
2.3.0,assert sorted(orig_ids) == sorted(new_ids)
2.3.0,# All the data should have same shape
2.3.0,assert X_orig.shape == X_new.shape
2.3.0,assert y_orig.shape == y_new.shape
2.3.0,assert w_orig.shape == w_new.shape
2.3.0,The shuffling should have switched up the ordering
2.3.0,But all the same entries should still be present
2.3.0,All the data should have same shape
2.3.0,The ids should now store the performed permutation. Check that the
2.3.0,original dataset is recoverable.
2.3.0,The ids should now store the performed permutation. Check that the
2.3.0,original dataset is recoverable.
2.3.0,Set some global variables up top
2.3.0,Featurize emols dataset
2.3.0,example.fasta contains 3 sequences each of length 58.
2.3.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.3.0,"There is one ""image channel""."
2.3.0,Generate dummy dataset
2.3.0,Generate dummy dataset
2.3.0,Generate dummy dataset
2.3.0,Set last n_samples/2 weights to 0
2.3.0,Check that no support elements are sample from zero-weight samples
2.3.0,Generate dummy dataset
2.3.0,Generate dummy dataset
2.3.0,Create support generator
2.3.0,Generate dummy dataset
2.3.0,Create support generator
2.3.0,Generate dummy dataset
2.3.0,Assert all support elements have been removed
2.3.0,Generate dummy dataset
2.3.0,Assert all remove elements have been removed
2.3.0,Generate dummy dataset
2.3.0,Assert all support elements have been removed
2.3.0,Generate dummy dataset
2.3.0,Assert all remove elements have been removed
2.3.0,Generate dummy dataset
2.3.0,Set last n_samples/2 weights to 0
2.3.0,Sample from first n_samples/2 elements for support
2.3.0,Should lie within first n_samples/2 samples only
2.3.0,Generate dummy dataset
2.3.0,Create support generator
2.3.0,Generate dummy dataset
2.3.0,First try using images for X.
2.3.0,Now try using images for y.
2.3.0,Test on identity matrix
2.3.0,Generate random sparse features dataset
2.3.0,Test edge case with array of all zeros
2.3.0,Test cases where n_samples < 2*n_samples < batch_size
2.3.0,Test cases where n_samples < batch_size
2.3.0,Test case where n_samples == batch_size
2.3.0,Test case for object featurization.
2.3.0,Test case for more complicated object featurization
2.3.0,Test case with multidimensional data
2.3.0,Test cases where n_samples < 2*n_samples < batch_size
2.3.0,Test cases where n_samples < batch_size
2.3.0,Test case where n_samples == batch_size
2.3.0,Test case for object featurization.
2.3.0,Test case for more complicated object featurization
2.3.0,Test case with multidimensional data
2.3.0,Test first resharding worked
2.3.0,Test second resharding worked
2.3.0,approx 1/15! chance of equality
2.3.0,Generate data
2.3.0,Generate data
2.3.0,Generate data
2.3.0,Transform it
2.3.0,Transform it
2.3.0,special case to test
2.3.0,deterministic
2.3.0,non-deterministic
2.3.0,we don't know the order in which the shards are iterated in.
2.3.0,Check that we have all the data in
2.3.0,Create image file
2.3.0,Create zip of image file
2.3.0,"self.zip_path = ""/home/rbharath/misc/cells.zip"""
2.3.0,Create zip of multiple image files
2.3.0,"Create zip of multiple image files, multiple_types"
2.3.0,Create image directory
2.3.0,These are the known dimensions of face.png
2.3.0,TODO(rbharath): Where are the color channels?
2.3.0,"Since the different files have different shapes, makes an object array"
2.3.0,Splits featurized samples into train/test
2.3.0,Splits featurized samples into train/test
2.3.0,Splits featurized samples into train/test
2.3.0,"splittype = ""random"""
2.3.0,Splits featurized samples into train/test
2.3.0,Now perform move
2.3.0,Only for debug!
2.3.0,#Make directories to store the raw and featurized datasets.
2.3.0,Load dataset
2.3.0,Featurize tox21 dataset
2.3.0,###### Do featurization
2.3.0,Do train/valid split.
2.3.0,###### Do singletask load
2.3.0,################# Do comparison
2.3.0,Only for debug!
2.3.0,Set some global variables up top
2.3.0,Make directories to store the raw and featurized datasets.
2.3.0,Load dataset
2.3.0,Featurize tox21 dataset
2.3.0,For debugging purposes
2.3.0,###### Do multitask load
2.3.0,Do train/valid split.
2.3.0,###### Do singletask load
2.3.0,################# Do comparison
2.3.0,"task_type = ""regression"""
2.3.0,coding=utf-8
2.3.0,Note that transformers have to be undone in reversed order
2.3.0,Hack to allow for easy unpickling:
2.3.0,http://stefaanlippens.net/pickleproblem
2.3.0,"One, but not both, transform_X or tranform_y is true"
2.3.0,Use fact that bools add as ints in python
2.3.0,Control for pathological case with no variance.
2.3.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.3.0,Find the task dimension of z
2.3.0,Prevent broadcasting on wrong dimension
2.3.0,BalancingTransformer can only transform weights.
2.3.0,Compute weighting factors from dataset.
2.3.0,Ensure dataset is binary
2.3.0,Remove labels with zero weights
2.3.0,self.w = dataset.w
2.3.0,"TODO (flee2): for transform_y, figure out weights"
2.3.0,"print(""y will not be transformed by CDFTransformer, for now."")"
2.3.0,"print(""Cannot undo CDF Transformer, for now."")"
2.3.0,Need this for transform_y
2.3.0,array = np.transpose(array)
2.3.0,"print(""y will not be transformed by PowerTransformer, for now."")"
2.3.0,"print(""Cannot undo Power Transformer, for now."")"
2.3.0,the tf graph here pick up the (K+1) highest similarity values
2.3.0,and their indices
2.3.0,map the indices to labels
2.3.0,generating batch of data by slicing similarity matrix
2.3.0,into 100*reference_dataset_length
2.3.0,concatenate batches of data together
2.3.0,highest similarity is 1: target is in the reference
2.3.0,use the following K points
2.3.0,"highest less than 1: target not in the reference, use top K points"
2.3.0,calculate matrix multiplicatin on slices
2.3.0,concatenate the slices together
2.3.0,list of calculation orders for DAGs
2.3.0,stemming from one specific atom in the molecule
2.3.0,starting from the adjacency list derived by graphconv featurizer
2.3.0,"number of atoms, also number of DAGs"
2.3.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.3.0,each step calculating graph features for one atom.
2.3.0,`max_atoms` is the maximum number of steps
2.3.0,each iteration generates the DAG starting from atom with index `count`
2.3.0,"list of lists, elements represent the calculation orders"
2.3.0,for atoms in the current graph
2.3.0,starting from the target atom with index `count`
2.3.0,flags of whether the atom is already included in the DAG
2.3.0,atom `count` is in the DAG
2.3.0,recording number of radial propagation steps
2.3.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.3.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.3.0,will be added in the second loop(radial=1).
2.3.0,atoms i-bond away will be added in i-th loop
2.3.0,"when molecules have separate parts, starting from one part,"
2.3.0,it is not possible to include all atoms.
2.3.0,this break quit the loop when going into such condition
2.3.0,reinitialize targets for next iteration
2.3.0,atoms connected to current_atom
2.3.0,generate the dependency map of current DAG
2.3.0,atoms connected to `current_atoms`(and not included in the DAG)
2.3.0,"are added, and will be the `current_atoms` for next iteration."
2.3.0,"DAG starts from the target atom, calculation should go in reverse"
2.3.0,`edge[1]` is the parent of `edge[0]`
2.3.0,"after this loop, `parents[i]` includes all parents of atom i"
2.3.0,manually adding the atom index into its parents list
2.3.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.3.0,atoms with less parents(farther from the target atom) come first.
2.3.0,"graph features of atoms without parents will be first calculated,"
2.3.0,then atoms with more parents can be calculated in order
2.3.0,based on previously calculated graph features.
2.3.0,target atom of this DAG will be calculated in the last step
2.3.0,padding with `max_atoms`
2.3.0,padding
2.3.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.3.0,which is a max_atoms * max_atoms numpy array after padding
2.3.0,Calculate pairwise distance
2.3.0,Masking for valid atom index
2.3.0,Cutoff with threshold Rc
2.3.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.3.0,extracting validation set of MNIST for testing the DataTransforms
2.3.0,extract only the images (no need of the labels)
2.3.0,reshaping the vector to image
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is a y transformer
2.3.0,Check w is unchanged since this is a y transformer
2.3.0,Check y is now a logarithmic version of itself
2.3.0,Check that untransform does the right thing.
2.3.0,transforming y should raise an exception
2.3.0,transforming w should raise an exception
2.3.0,transforming X should be okay
2.3.0,Check ids are unchanged.
2.3.0,Check y is unchanged since this is a X transformer
2.3.0,Check w is unchanged since this is a y transformer
2.3.0,Check y is now a logarithmic version of itself
2.3.0,Check that untransform does the right thing.
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is a y transformer
2.3.0,Check w is unchanged since this is a y transformer
2.3.0,Check y is now a logarithmic version of itself
2.3.0,Check that untransform does the right thing.
2.3.0,Tests logarithmic data transformer with selection.
2.3.0,Check ids are unchanged.
2.3.0,Check y is unchanged since this is a X transformer
2.3.0,Check w is unchanged since this is a y transformer
2.3.0,Check y is now a logarithmic version of itself
2.3.0,Check that untransform does the right thing.
2.3.0,Check ids are unchanged before and after transformation
2.3.0,Check X is unchanged since transform_y is true
2.3.0,Check w is unchanged since transform_y is true
2.3.0,Check minimum and maximum values of transformed y are 0 and 1
2.3.0,Check untransform works correctly
2.3.0,Test on random example
2.3.0,Check ids are unchanged before and after transformation
2.3.0,Check X is unchanged since transform_y is true
2.3.0,Check w is unchanged since transform_y is true
2.3.0,Check minimum and maximum values of transformed y are 0 and 1
2.3.0,Test if dimensionality expansion is handled correctly by untransform
2.3.0,Check ids are unchanged before and after transformation
2.3.0,Check X is unchanged since transform_y is true
2.3.0,Check w is unchanged since transform_y is true
2.3.0,Check minimum and maximum values of transformed y are 0 and 1
2.3.0,Check untransform works correctly
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is a y transformer
2.3.0,Check w is unchanged since this is a y transformer
2.3.0,"Check that y_t has zero mean, unit std."
2.3.0,Check that untransform does the right thing.
2.3.0,Check ids are unchanged.
2.3.0,Check y is unchanged since this is a X transformer
2.3.0,Check w is unchanged since this is a y transformer
2.3.0,"Check that X_t has zero mean, unit std."
2.3.0,np.set_printoptions(threshold='nan')
2.3.0,Entries with zero std are not normalized
2.3.0,TODO(rbharath): Untransform doesn't work properly for binary feature
2.3.0,vectors. Need to figure out what's wrong here. (low priority)
2.3.0,# Check that untransform does the right thing.
2.3.0,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
2.3.0,Check ids are unchanged.
2.3.0,Check y is unchanged since this is an X transformer
2.3.0,Check w is unchanged since this is an X transformer
2.3.0,Check X is now holding the proper values when sorted.
2.3.0,Test CDF transformer on Gaussian normal dataset.
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is an y transformer
2.3.0,Check w is unchanged since this is an y transformer
2.3.0,Check y is now holding the proper values when sorted.
2.3.0,Check that untransform does the right thing.
2.3.0,Check ids are unchanged.
2.3.0,Check y is unchanged since this is an X transformer
2.3.0,Check w is unchanged since this is an X transformer
2.3.0,Check X is now holding the proper values when sorted.
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is a y transformer
2.3.0,Check w is unchanged since this is a y transformer
2.3.0,Check y is now holding the proper values when sorted.
2.3.0,Check ids are unchanged.
2.3.0,Check y is unchanged since this is an X transformer
2.3.0,Check w is unchanged since this is an X transformer
2.3.0,Check X is now holding the proper values in each column.
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is an X transformer
2.3.0,Check w is unchanged since this is an X transformer
2.3.0,Check y is now holding the proper values in each column.
2.3.0,Check that untransform does the right thing.
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is a w transformer
2.3.0,Check y is unchanged since this is a w transformer
2.3.0,Assert that entries with zero weight retain zero weight
2.3.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.3.0,Check ids are unchanged.
2.3.0,Check X is unchanged since this is a w transformer
2.3.0,Check y is unchanged since this is a w transformer
2.3.0,Assert that entries with zero weight retain zero weight
2.3.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.3.0,Check Blurring
2.3.0,Check rotation
2.3.0,Some more test cases for flip
2.3.0,Check flip
2.3.0,Check Scales
2.3.0,Check shift
2.3.0,check gaussian noise
2.3.0,check salt and pepper noise
2.3.0,TODO(rbharath): Use standard joblib once old-data has been regenerated.
2.3.0,"If gzipped, need to compute extension again"
2.3.0,Tasks are either in .sdf.csv file or in the .sdf file itself
2.3.0,Structures are stored in .sdf file
2.3.0,First line of user-specified CSV *must* be header.
2.3.0,Try older joblib version for legacy files.
2.3.0,First line of user-specified CSV *must* be header.
2.3.0,First line of user-specified CSV *must* be header.
2.3.0,combine dataframes
2.3.0,TODO: mol should be always sanitized when charges are calculated
2.3.0,can't change it now because it would break a lot of examples
2.3.0,working-with-3d-molecules
2.3.0,initial embedding
2.3.0,minimization and pruning
2.3.0,always keep lowest-energy conformer
2.3.0,discard conformers after max_conformers is reached
2.3.0,get RMSD to selected conformers
2.3.0,discard conformers within the RMSD threshold
2.3.0,create a new molecule to hold the chosen conformers
2.3.0,this ensures proper conformer IDs and energy-based ordering
2.3.0,The label encoder is given characters for ACGTN
2.3.0,Peak at the first sequence to get the length of the sequence.
2.3.0,TODO(rbharath): This is now simple enough that we should probably get rid of
2.3.0,Evaluator object to avoid clutter.
2.3.0,Compute multitask metrics
2.3.0,This is a KerasModel.
2.3.0,This is a TensorGraph.
2.3.0,Compute multitask metrics
2.3.0,Loosening atol to see if tests stop failing sporadically
2.3.0,One sequence has length longer than others. This should throw a
2.3.0,ValueError.
2.3.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,a*x + b*y + c*z = dI think that
2.3.0,"self.x, self.y, self.z = x, y, z"
2.3.0,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
2.3.0,TODO(bramsundar): Should this be __copy__?
2.3.0,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
2.3.0,"return np.array([self.x, self.y, self.z])"
2.3.0,TODO(rbharath): Should this be an atom function?
2.3.0,"This line is necessary for babel to work, though many PDBs in"
2.3.0,the PDB would have this line commented out
2.3.0,now atom type (for pdbqt)
2.3.0,"If atomtype is not specified, but atomname is, set atomtype to the"
2.3.0,"first letter of atomname. This heuristic suffices for proteins,"
2.3.0,since no two-letter elements appear in standard amino acids.
2.3.0,Any number needs to be removed from the element name
2.3.0,"this only uses the rightmost three characters, essentially"
2.3.0,removing unique rotamer identification
2.3.0,"The normal vector to plane is n = [a, b, c]"
2.3.0,We first shift by basepoint (a point on given plane) to make math
2.3.0,simpler. basepoint is given by d/||n||^2 * n
2.3.0,The perpendicular component of diff to plane is
2.3.0,(n^T diff / ||n||^2) * n
2.3.0,Extend shorter strings with padding
2.3.0,Padding before and after
2.3.0,Setup image
2.3.0,Compute bond properties
2.3.0,Compute atom properties
2.3.0,Setup image
2.3.0,Compute bond properties
2.3.0,Compute atom properties
2.3.0,Reshape done for proper broadcast
2.3.0,"Reshapes, and axes manipulations to facilitate vector processing."
2.3.0,Draw a line between the two atoms.
2.3.0,"The coordinates of this line, are indicated in line_coords"
2.3.0,Turn the line coordinates into image positions
2.3.0,Set the bond line coordinates to the bond property used.
2.3.0,Turn atomic coordinates into image positions
2.3.0,Set the atom positions in image to different atomic properties in channels
2.3.0,if ring is aromatic
2.3.0,"save its indices, center, and normal"
2.3.0,remember protein-ligand pairs we already counted
2.3.0,"if this pair is new, count atoms forming a contact"
2.3.0,"if this pair is new, count atoms forming a contact"
2.3.0,if ring from mol1 is aromatic
2.3.0,...and atom from mol2 is a cation
2.3.0,if angle and distance are correct
2.3.0,count atoms forming a contact
2.3.0,find interacting rings from protein and cations from ligand
2.3.0,find interacting cations from protein and rings from ligand
2.3.0,merge counters
2.3.0,TODO(LESWING)
2.3.0,check if user tries to set removed arguments
2.3.0,list of features that require sanitized molecules
2.3.0,not implemented featurization types
2.3.0,default values
2.3.0,update with cutoffs specified by the user
2.3.0,"each entry is a tuple (is_flat, feature_name)"
2.3.0,list of features that cannot be calculated with specified parameters
2.3.0,this list is used to define <flat/voxel/all>_combined subset
2.3.0,parse provided feature types
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,TODO(rbharath): Is this squeeze OK?
2.3.0,Get the degree id list (which corrects for min_deg)
2.3.0,Get the size of each degree block
2.3.0,Get the the start indices for items in each block
2.3.0,Get the node indices when they are reset when the degree changes
2.3.0,Convert to numpy array
2.3.0,Reorder old atom_features
2.3.0,Reorder old deg lists
2.3.0,Sort membership
2.3.0,Create old to new dictionary. not exactly intuitive
2.3.0,Reorder adjacency lists
2.3.0,Get numpy version of degree list for indexing
2.3.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.3.0,Parse as deg separated
2.3.0,Get indices corresponding to the current degree
2.3.0,Extract and save adjacency list for the current degree
2.3.0,Construct the slice information
2.3.0,Get the cumulative indices after the first index
2.3.0,Set indices with zero sized slices to zero to avoid indexing errors
2.3.0,TODO(rbharath): Can this be removed?
2.3.0,Use random insted of zeros to prevent weird issues with summing to zero
2.3.0,"Results should be sorted by (atom_degree, mol_index)"
2.3.0,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.3.0,Sort all atoms by degree.
2.3.0,"Get the size of each atom list separated by molecule id, then by degree"
2.3.0,Get the final size of each degree block
2.3.0,"Get the index at which each degree starts, not resetting after each degree"
2.3.0,And not stopping at any speciic molecule
2.3.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.3.0,first column telling the start indices of each degree block and the
2.3.0,second colum telling the size of each degree block
2.3.0,Input for tensorflow
2.3.0,Determines the membership (atom i belongs to membership[i] molecule)
2.3.0,"Get the index at which each deg starts, resetting after each degree"
2.3.0,(deg x num_mols) matrix describing the start indices when you count up the atoms
2.3.0,"in the final representation, stopping at each molecule,"
2.3.0,resetting every time the degree changes
2.3.0,Gets the degree resetting block indices for the atoms in each molecule
2.3.0,"Here, the indices reset when the molecules change, and reset when the"
2.3.0,degree changes
2.3.0,Get the degree id lookup list. It allows us to search for the degree of a
2.3.0,molecule mol_id with corresponding atom mol_atom_id using
2.3.0,"deg_id_lists[mol_id,mol_atom_id]"
2.3.0,This is used for convience in the following function (explained below)
2.3.0,Get the degree id (corrected for min_deg) of the considered atom
2.3.0,Return the final index of atom mol_atom_id in molecule mol_id.  Using
2.3.0,"the degree of this atom, must find the index in the molecule's original"
2.3.0,"degree block corresponding to degree id deg_id (second term), and then"
2.3.0,calculate which index this degree block ends up in the final
2.3.0,representation (first term). The sum of the two is the final indexn
2.3.0,Initialize the new degree separated adjacency lists
2.3.0,Update the old adjcency lists with the new atom indices and then combine
2.3.0,all together
2.3.0,Iterate through all the molecules
2.3.0,Get the adjacency lists for this molecule and current degree id
2.3.0,"Correct all atom indices to the final indices, and then save the"
2.3.0,results into the new adjacency lists
2.3.0,Increment once row is done
2.3.0,Get the final aggregated molecule
2.3.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.3.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.3.0,consistent with most QM software packages.
2.3.0,Type of data created by this featurizer
2.3.0,TODO(rbharath): Should this return a list?
2.3.0,Type of data created by this featurizer
2.3.0,Currently handles loading failures by returning None
2.3.0,TODO: Is there a better handling procedure?
2.3.0,generate SMILES for fragments
2.3.0,Initalize with 1
2.3.0,Allow 0 index to correspond to null molecule 1
2.3.0,Correct for null
2.3.0,"print(6-k-1, id)"
2.3.0,Correct for last one
2.3.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.3.0,first `bt_len` features are bond features(if applicable)
2.3.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.3.0,graph distance between two atoms
2.3.0,Euclidean distance between atoms
2.3.0,atoms `radial` bonds away from `a1`
2.3.0,atoms less than `radial` bonds away
2.3.0,find atoms `radial`+1 bonds away
2.3.0,Get the node features
2.3.0,Stack nodes into an array
2.3.0,Get bond lists with reverse edges included
2.3.0,Get canonical adjacency list
2.3.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.3.0,only support datasets providing Cartesian coordinates)
2.3.0,Set dtype
2.3.0,If includes explicit hydrogens
2.3.0,If uses use_chirality
2.3.0,Atom features
2.3.0,Stack nodes into an array
2.3.0,Get bond lists
2.3.0,Get canonical adjacency list
2.3.0,Calculate pair features
2.3.0,TODO (VIGS25): Complete the description
2.3.0,Handle loading failures which return None
2.3.0,Fit atomic conv model
2.3.0,Add the Atomic Convolution layers to fetches
2.3.0,Extract the atomic convolution features
2.3.0,Handle loading failures which return None
2.3.0,atom_name is of format RESX-ATOMTYPE
2.3.0,where X is a 1 to 4 digit number
2.3.0,list-of-available-descriptors.
2.3.0,(ytz): This is done to avoid future compatibility issues like inclusion of
2.3.0,the 3D descriptors or changing the feature size.
2.3.0,check for separate count and SMILES entries for each fragment
2.3.0,TODO test more formats for ligand
2.3.0,adding hydrogens and charges is tested in dc.utils
2.3.0,3D vector with unit length
2.3.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.3.0,check if distances do not change
2.3.0,check if it works for molecules with different numbers of atoms
2.3.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.3.0,check if correct distance metric was used
2.3.0,"20 points with coords between -5 and 5, centered at 0"
2.3.0,indices are positive
2.3.0,coordinates were properly translated and scaled
2.3.0,for coordinates outside of the box function should properly transform them
2.3.0,to indices and warn the user
2.3.0,"TODO check if function warns. There is assertWarns method in unittest,"
2.3.0,but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
2.3.0,"20 points with coords between -5 and 5, centered at 0"
2.3.0,3 pairs of indices
2.3.0,simple flat ring
2.3.0,load and sanitize two real molecules
2.3.0,FIXME might break with different version of rdkit
2.3.0,FIXME might break with different version of rdkit
2.3.0,parallel normals
2.3.0,perpendicular normals
2.3.0,too far away
2.3.0,perpendicular normals
2.3.0,parallel normals
2.3.0,too far away
2.3.0,order of the molecules shouldn't matter
2.3.0,with this criteria we should find both types of stacking
2.3.0,parallel normals
2.3.0,perpendicular normals
2.3.0,too far away
2.3.0,"TODO find better example, currently dicts are empty"
2.3.0,"TODO find better example, currently dicts are empty"
2.3.0,TODO test if dict contains smiles
2.3.0,check if results are the same if we provide precomputed distances
2.3.0,...but first check if we actually got two dicts
2.3.0,check if we get less features with smaller distance cutoff
2.3.0,ligands are typically small so all atoms might be present
2.3.0,check if using different ecfp_degree changes anything
2.3.0,TODO upperbound?
2.3.0,test if default parameters work
2.3.0,check if use-case from examples works
2.3.0,test if input is flattened when flat features are used
2.3.0,test voxel features
2.3.0,test flat features
2.3.0,check if aromatic features are ignores if sanitize=False
2.3.0,"protein is too big for the box, some features should be missing"
2.3.0,whole ligand should fit in the box
2.3.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.3.0,of degree 1 (connected only to central nitrogen).
2.3.0,5 atoms in compound
2.3.0,Get the adjacency lists grouped by degree
2.3.0,The 4 outer atoms connected to central nitrogen
2.3.0,Central nitrogen connected to everything else.
2.3.0,Only one carbon
2.3.0,"No bonds, so degree adjacency lists are empty"
2.3.0,3 carbonds in alkane
2.3.0,Outer two carbonds are connected to central carbon
2.3.0,Central carbon connected to outer two
2.3.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.3.0,max num atoms instead of exact.
2.3.0,Cutoff in angstroms
2.3.0,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
2.3.0,this expected behavior? Need to think about API.
2.3.0,Do a manual distance computation and make
2.3.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.3.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.3.0,Do a manual distance computation and ensure that selected neighbor is
2.3.0,closest since we set max_num_neighbors = 1
2.3.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.3.0,max num atoms instead of exact.
2.3.0,Cutoff in angstroms
2.3.0,Splits featurized samples into train/test
2.3.0,Artificial feature array.
2.3.0,0 atoms of degree 0
2.3.0,0 atoms of degree 1
2.3.0,4 atoms of degree 2
2.3.0,0 atoms of degree 3
2.3.0,0 atoms of degree 4
2.3.0,0 atoms of degree 5
2.3.0,0 atoms of degree 6
2.3.0,0 atoms of degree 7
2.3.0,0 atoms of degree 8
2.3.0,0 atoms of degree 9
2.3.0,0 atoms of degree 10
2.3.0,atom 4 has 0 neighbors
2.3.0,atom 0 has 2 neighbors
2.3.0,atom 1 has 2 neighbors
2.3.0,atom 2 has 2 neighbors
2.3.0,atom 3 has 3 neighbors.
2.3.0,Verify that atom features have been sorted by atom degree.
2.3.0,Sorting is done by atom degree as before. So the ordering goes
2.3.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.3.0,from new position to old position is
2.3.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.3.0,list respects this reordering and returns correct adjacency list.
2.3.0,### First example molecule
2.3.0,Artificial feature array.
2.3.0,### Second example molecule
2.3.0,## Third example molecule
2.3.0,Test agglomerate molecule method
2.3.0,No atoms of degree 0
2.3.0,3 atoms of degree 1
2.3.0,8 atoms of degree 2
2.3.0,1 atom of degree 3
2.3.0,0 atoms of degree 4
2.3.0,0 atoms of degree 5
2.3.0,Check that atoms are only connected to themselves.
2.3.0,Check that there's one atom of each degree.
2.3.0,assumes that every array is of the same dimension
2.3.0,rem_dataset is remaining portion of dataset
2.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.3.0,to k-1.
2.3.0,dict is needed in case groups aren't strictly flattened or
2.3.0,hashed by something non-integer like
2.3.0,returns list of per column sum of non zero elements
2.3.0,Compute number of actives needed per task.
2.3.0,loop through each column and obtain index required to splice out for
2.3.0,required fraction of hits
2.3.0,Find the first index where the cumulative number of actives equals
2.3.0,the actives_count
2.3.0,Note that np.where tells us last index required to exceed
2.3.0,"actives_count, so we actually want the following location"
2.3.0,TODO(rbharath): Refactor this split method to match API of other
2.3.0,splits (or potentially refactor those to match this).
2.3.0,Handle edge case where frac_split is 1
2.3.0,Create weight matrices fpor two haves.
2.3.0,copy over up to required index for weight first_split
2.3.0,check out if any rows in either w_1 or w_2 are just zeros
2.3.0,calculate percent split for valid (out of test and valid)
2.3.0,"split remaining data into valid and test, treating sub test set also as sparse"
2.3.0,rem_dataset is remaining portion of dataset
2.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.3.0,to k-1.
2.3.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.3.0,This can be generalized in the future with some common demoninator determination.
2.3.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.3.0,Append remaining examples to train
2.3.0,Sort by increasing MW
2.3.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.3.0,for m_idx in cluster:
2.3.0,"continue until we find an active in all the tasks, otherwise we can't"
2.3.0,compute a meaningful AUC
2.3.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.3.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.3.0,Sort from largest to smallest scaffold sets
2.3.0,Pick the mol closest to everything as the first element of training
2.3.0,Pick the closest mol from what is left
2.3.0,Test is everything else
2.3.0,All datasets share features and identifiers by assumption.
2.3.0,TODO(rbharath): Get rid of * import
2.3.0,Note that the extra task goes to test
2.3.0,Number tasks per fold
2.3.0,Find the tasks that correspond to this test fold
2.3.0,Assert that all arrays look like they should
2.3.0,0 1 2 3 4 5 6 7 8 9
2.3.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.3.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.3.0,reshard() to handle this?
2.3.0,Verify lengths is 10/k == 2
2.3.0,Verify that compounds in this fold are subset of original compounds
2.3.0,Verify that no two folds have overlapping compounds.
2.3.0,Verify lengths is 10/k == 2
2.3.0,Verify that compounds in this fold are subset of original compounds
2.3.0,Verify that no two folds have overlapping compounds.
2.3.0,Verify lengths is 10/k == 2
2.3.0,Verify that compounds in this fold are subset of original compounds
2.3.0,Verify that no two folds have overlapping compounds.
2.3.0,Test singletask case.
2.3.0,The split index should partition dataset in half.
2.3.0,Test singletask case.
2.3.0,Test case where some weights are zero (i.e. masked)
2.3.0,Set half the positives to have zero weight
2.3.0,There are 10 nonzero actives.
2.3.0,"The split index should partition this into half, so expect 5"
2.3.0,The split index should partition dataset in half.
2.3.0,Mask half the examples
2.3.0,The split index should partition dataset in half.
2.3.0,Test singletask case.
2.3.0,Should have split cleanly in half (picked random seed to ensure this)
2.3.0,Check positives are correctly distributed
2.3.0,Test singletask case.
2.3.0,Should have made an 80/10/10 train/valid/test split of actives.
2.3.0,Verify lengths is 100/k == 20
2.3.0,Note: This wouldn't work for multitask str
2.3.0,assert len(fold_dataset) == n_samples/K
2.3.0,Verify that each fold has n_positives/K = 4 positive examples.
2.3.0,Verify that compounds in this fold are subset of original compounds
2.3.0,Verify that no two folds have overlapping compounds.
2.3.0,sparsity is determined by number of w weights that are 0 for a given
2.3.0,task structure of w np array is such that each row corresponds to a
2.3.0,sample. The loaded sparse dataset has many rows with only zeros
2.3.0,verify that there are no rows (samples) in weights matrix w
2.3.0,that have no hits.
2.3.0,Create the inputs.
2.3.0,Create the generators.
2.3.0,Create the discriminators.
2.3.0,Compute the loss functions.
2.3.0,Create learnable weights for the generators and discriminators.
2.3.0,We pass an input to the Variable layer to work around a bug in TF 1.14.
2.3.0,Compute the weighted errors
2.3.0,Add an entropy term to the loss.
2.3.0,Create the Keras model.
2.3.0,"Every call to fit_generator() will increment global_step, but we only"
2.3.0,"want it to get incremented once for the entire batch, so record the"
2.3.0,value and keep resetting it.
2.3.0,Train the discriminator.
2.3.0,Train the generator.
2.3.0,Write checkpoints and report progress.
2.3.0,Write out final results.
2.3.0,-*- coding: utf-8 -*-
2.3.0,"Shape (N_atoms, M_nbrs, ndim)"
2.3.0,"Shape (N_atoms, M_nbrs, ndim)"
2.3.0,"Shape (N_atoms, M_nbrs)"
2.3.0,Generate the nb_affine weights and biases
2.3.0,Extract atom_features
2.3.0,Extract graph topology
2.3.0,Sum all neighbors using adjacency matrix
2.3.0,Get collection of modified atom features
2.3.0,Obtain relevant atoms for this degree
2.3.0,Get self atoms
2.3.0,Apply hidden affine to relevant atoms and append
2.3.0,Determine the min_deg=0 case
2.3.0,Only use the self layer
2.3.0,Combine all atoms back into the list
2.3.0,Tensorflow correctly processes empty lists when using concat
2.3.0,"Sum along neighbors as well as self, and store"
2.3.0,Perform the mol gather
2.3.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.3.0,"self.max_degree, self.min_degree)"
2.3.0,Tensorflow correctly processes empty lists when using concat
2.3.0,Get self atoms
2.3.0,Expand dims
2.3.0,always deg-1 for deg_adj_lists
2.3.0,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
2.3.0,Extract graph topology
2.3.0,No other forget biases supported right now.
2.3.0,Taken from Keras code [citation needed]
2.3.0,"x is test set, xp is support set."
2.3.0,Get initializations
2.3.0,Process using attention
2.3.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.3.0,Generate new attention states
2.3.0,Support set lstm
2.3.0,Test lstm
2.3.0,Get initializations
2.3.0,Rename support
2.3.0,Process support xp using attention
2.3.0,Get linear combination of support set
2.3.0,Process test x using attention
2.3.0,Generate new support attention states
2.3.0,Generate new test attention states
2.3.0,Redefine
2.3.0,Number of rotatable bonds
2.3.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.3.0,a difference.
2.3.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.3.0,neighbors lists an argument instead of a part of this layer.
2.3.0,"Shape (N, M)"
2.3.0,"Shape (N, M)"
2.3.0,"Shape (N, M)"
2.3.0,Number of grid cells
2.3.0,TODO(rbharath): Support batching
2.3.0,"Shape (n_cells, ndim)"
2.3.0,"List of length N_atoms, each element of different length uniques_i"
2.3.0,"List of length N_atoms, each element of different length uniques_i"
2.3.0,"List of length N_atoms, each a tensor of shape"
2.3.0,"(uniques_i, ndim)"
2.3.0,Add phantom atoms that exist far outside the box
2.3.0,"List of length N_atoms, each of shape (1, ndim)"
2.3.0,TODO(rbharath): How does distance need to be modified here to
2.3.0,account for periodic boundary conditions?
2.3.0,List of length N_atoms each of shape (M_nbrs)
2.3.0,"N_atoms elts of size (M_nbrs,) each"
2.3.0,"Shape (N_atoms, 1)"
2.3.0,Find M_nbrs atoms closest to each cell
2.3.0,"Shape (n_cells, M_nbrs)"
2.3.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.3.0,"conditions, so does wrapround. O(constant)"
2.3.0,"Shape (n_cells, n_nbr_cells)"
2.3.0,"Shape (N_atoms, n_nbr_cells)"
2.3.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.3.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.3.0,"List of length N_atoms, each element length uniques_i"
2.3.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.3.0,element removed to remove self from list of neighbors. Need to verify
2.3.0,this holds more broadly or come up with robust alternative.
2.3.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.3.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.3.0,Shape (N_atoms*n_cells)
2.3.0,"Shape (n_cells, N_atoms)"
2.3.0,Find k atoms closest to this cell. Notice negative sign since
2.3.0,tf.nn.top_k returns *largest* not smallest.
2.3.0,"Tensor of shape (n_cells, M_nbrs)"
2.3.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.3.0,"Shape (N_atoms*n_cells, 1) after tile"
2.3.0,9 neighbors in 2-space
2.3.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.3.0,Number of cells for cube in 3-space is
2.3.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.3.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.3.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.3.0,the cube.
2.3.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.3.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.3.0,"Tile (a, a, a, b, b, b, etc.)"
2.3.0,"Tile (a, b, c, a, b, c, ...)"
2.3.0,N: Maximum number of atoms
2.3.0,M: Maximum number of neighbors
2.3.0,d: Number of coordinates/features/filters
2.3.0,B: Batch Size
2.3.0,Compute the distances and radial symmetry functions.
2.3.0,check that there isnt just one or zero inputs
2.3.0,create subspaces
2.3.0,"concatenate subspaces, reshape to size of original input, then stack"
2.3.0,"such that out_tensor has shape (2,?,original_cols)"
2.3.0,creates subspaces the same way it was done in AlphaShare
2.3.0,calculate squared Frobenius norm
2.3.0,"(TODO YTZ:) faster, less memory intensive way"
2.3.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.3.0,"r = tf.expand_dims(r, -1)"
2.3.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.3.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.3.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.3.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.3.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.3.0,Calculate pairwise distance
2.3.0,Cutoff with threshold Rc
2.3.0,return d
2.3.0,tf.stack issues again...
2.3.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.3.0,So the Tensor has known dimensions
2.3.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.3.0,and embeddings of atom j(both gone through a hidden layer)
2.3.0,"for atom i, sum the influence from all other atom j in the molecule"
2.3.0,number of inputs each step
2.3.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.3.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.3.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.3.0,initialize graph features for each graph
2.3.0,initialize graph features for each graph
2.3.0,another row of zeros is generated for padded dummy atoms
2.3.0,`count`-th step
2.3.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.3.0,generating index for graph features used in the inputs
2.3.0,"extracting graph features for parents of the target atoms, then flatten"
2.3.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.3.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.3.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.3.0,of shape: (batch_size*max_atoms) * n_graph_features
2.3.0,representing the graph features of target atoms in each graph
2.3.0,index for targe atoms
2.3.0,update the graph features for target atoms
2.3.0,Extract atom_features
2.3.0,sum all graph outputs
2.3.0,"Default message function: edge network, update function: GRU"
2.3.0,more options to be implemented
2.3.0,Add another value(~-Inf) to prevent error in softmax
2.3.0,Model using this layer must set pad_batches=True
2.3.0,Perform one step of LSTM
2.3.0,task_metadata_rows = {task: [] for task in tasks}
2.3.0,Extract those datapoints which are present for this task
2.3.0,Loading is done on-the-fly
2.3.0,Build the model.
2.3.0,number of atoms in each molecule
2.3.0,index of pair features
2.3.0,number of pairs for each atom
2.3.0,atom features
2.3.0,pair features
2.3.0,Build the model.
2.3.0,Build the model.
2.3.0,calculation orders for a batch of molecules
2.3.0,padding atom features vector of each molecule with 0
2.3.0,Build the model.
2.3.0,Build the model.
2.3.0,number of atoms in each molecule
2.3.0,index of pair features
2.3.0,number of pairs for each atom
2.3.0,atom features
2.3.0,pair features
2.3.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.3.0,Add the input features.
2.3.0,Add the shared dense layers
2.3.0,Add task-specific bypass layers
2.3.0,Add the input features.
2.3.0,Add the shared dense layers
2.3.0,Add task-specific bypass layers
2.3.0,"The model doesn't specify inputs, so guess the input shapes based on the"
2.3.0,example batch.
2.3.0,The loss doesn't depend on any variables.
2.3.0,Main training loop.
2.3.0,"In eager mode we execute the loss function, accumulating the gradients."
2.3.0,In graph mode we execute the training op.
2.3.0,Report progress and write checkpoints.
2.3.0,Report final results.
2.3.0,In eager mode we invoke the model directly.
2.3.0,In graph mode we execute the output tensors.
2.3.0,Apply tranformers and record results.
2.3.0,Concatenate arrays to create the final results.
2.3.0,"If only one output, just return array"
2.3.0,In eager mode we use a GradientTape to compute gradients.
2.3.0,In graph mode we use tf.gradients().
2.3.0,Adapted from https://github.com/tensorflow/tensorflow/issues/675#issuecomment-319891923.
2.3.0,self.session is used because restore was called in the same session
2.3.0,Add the input features.
2.3.0,Add the dense layers
2.3.0,Add the input features.
2.3.0,Add the dense layers
2.3.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.3.0,Similarity values
2.3.0,Labels for all top K similar samples
2.3.0,TODO(rbharath/enf): We need a structured way to deal with potential GPU
2.3.0,memory overflows.
2.3.0,Discard any padded predictions
2.3.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.3.0,Build the model.
2.3.0,Character embedding
2.3.0,Multiple convolutional layers with different filter widths
2.3.0,Max-over-time pooling
2.3.0,Concat features from all filters(one feature per filter)
2.3.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.3.0,SMILES strings
2.3.0,Maximum length is expanded to allow length variation during train and inference
2.3.0,'_' served as delimiter and padding
2.3.0,Initialize common characters as keys
2.3.0,Include space to avoid extra keys
2.3.0,"For 'Cl', 'Br', etc."
2.3.0,"Character not recognized, add to extra_keys"
2.3.0,Add all extra_keys to char_dict
2.3.0,Transform SMILES sequence to integers
2.3.0,Skip all spaces
2.3.0,"For 'Cl', 'Br', etc."
2.3.0,Padding with '_'
2.3.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.3.0,TODO: Turning off queue for now. Safe to re-activate?
2.3.0,Do a simple greedy search.
2.3.0,Do a beam search with length normalization.
2.3.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.3.0,This candidate sequence has already been terminated
2.3.0,Consider all possible tokens we could add to this candidate sequence.
2.3.0,Add the input features.
2.3.0,Handle output layer
2.3.0,Iterate over all previous tasks.
2.3.0,prev_layers is a list with elements of size
2.3.0,"(batch_size, layer_sizes[i-1])"
2.3.0,################### Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0. ####################
2.3.0,Last layer sequences not returned.
2.3.0,This is needed because ImageDataGenerator does infinite looping
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,Calculate pairwise distance
2.3.0,Masking for valid atom index
2.3.0,Cutoff with threshold Rc
2.3.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.3.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.3.0,optimization to allow for tensorcontraction/broadcasted mmul
2.3.0,using a reshape trick. Note that the np and tf matmul behavior
2.3.0,differs when dealing with broadcasts
2.3.0,-*- coding: UTF-8 -*-
2.3.0,Reshape everything to match the input with the most dimensions.
2.3.0,This happens when building an estimator.
2.3.0,Calculate what the new shape will be.
2.3.0,TODO(rbharath): Note sure if this layer can be called with __call__
2.3.0,"meaningfully, so not going to support that functionality for now."
2.3.0,No other forget biases supported right now.
2.3.0,Number of rotatable bonds
2.3.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.3.0,a difference.
2.3.0,Number of grid cells
2.3.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.3.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.3.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.3.0,the cube.
2.3.0,Add in features
2.3.0,Add in labels
2.3.0,Add in all layers
2.3.0,The last layer is the output of the model
2.3.0,TODO(rbharath): Add in support for additional
2.3.0,losses.
2.3.0,TODO(rbharath): The TensorGraph can't be built until
2.3.0,fit is called since the shapes of features/labels
2.3.0,not specified. Need to figure out a good restoration
2.3.0,method for this use case.
2.3.0,ensure that randomness is conditioned by the Numpy RNG
2.3.0,ensure that randomness is conditioned by the Numpy RNG
2.3.0,TODO(rbharath): Should probably swap this over to tf mode.
2.3.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.3.0,"expects logits, Tensorflow expects probabilities."
2.3.0,scale preds so that the class probas of each sample sum to 1
2.3.0,manual computation of crossentropy
2.3.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.3.0,"expects logits, Tensorflow expects probabilities."
2.3.0,if our output includes timesteps we need to reshape
2.3.0,Arguments
2.3.0,Returns
2.3.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.3.0,"expects logits, Tensorflow expects probabilities."
2.3.0,transform back to logits
2.3.0,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
2.3.0,a tensor. Confusing with tf.zeros...
2.3.0,Transpose for mul
2.3.0,exclude bias variables
2.3.0,"tf.scalar_summary('Weight Decay Cost', cost)"
2.3.0,TODO(user): gradient clipping (see Minimize)
2.3.0,Assuming convolution kernels (2D or 3D).
2.3.0,"TF kernel shape: (..., input_depth, depth)"
2.3.0,No specific assumptions.
2.3.0,References
2.3.0,References
2.3.0,References
2.3.0,References
2.3.0,Pick the one with the correct shape.
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,number of inputs each step
2.3.0,Arguments
2.3.0,Aliases.
2.3.0,Layer Management
2.3.0,Singular place to hold Tensor objects which don't serialize
2.3.0,These have to be reconstructed on restoring from pickle
2.3.0,See TensorGraph._get_tf() for more details on lazy construction
2.3.0,In eager mode we want an optimizer and a function to compute the
2.3.0,gradient of the loss.
2.3.0,In graph mode we want a training operation.
2.3.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.3.0,"we try to read more batches than the total number that get queued,"
2.3.0,this thread will hang indefinitely.
2.3.0,"TODO Once we drop Python 2 support, turn outputs into a proper keyword arg"
2.3.0,instead of using the **kwargs hack.
2.3.0,Gather results for each output
2.3.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.3.0,"we try to read more batches than the total number that get queued,"
2.3.0,this thread will hang indefinitely.
2.3.0,"If only one output, just return array"
2.3.0,Adapted from https://github.com/tensorflow/tensorflow/issues/675#issuecomment-319891923.
2.3.0,"The next release of Tensorflow will add a proper jacobian() function, so"
2.3.0,we can remove this then.
2.3.0,"Remove extra dimensions, because I couldn't figure out how to get the"
2.3.0,jacobian() function to not produce them.
2.3.0,"In eager mode, we need to execute every layer once to ensure its variables"
2.3.0,have been created.
2.3.0,"We can't execute Input layers in eager mode, since they would try"
2.3.0,to create placeholders.  Instead create a tensor of the correct
2.3.0,size and type.
2.3.0,Build the layers.
2.3.0,Initialize variables.
2.3.0,In graph mode we need to create the computation graph.
2.3.0,Ensure all training operators have been created.
2.3.0,Initialize variables.
2.3.0,"As a sanity check, make sure all tensors have the correct shape."
2.3.0,Remove out_tensor from the object to be pickled
2.3.0,Pickle itself
2.3.0,add out_tensor back to everyone
2.3.0,The loss doesn't depend on any variables.
2.3.0,Check the inputs.
2.3.0,Define a function that recursively creates tensors from layers.
2.3.0,Define the model function.
2.3.0,Define the inputs.
2.3.0,"Create the correct outputs, based on the mode."
2.3.0,Create the Estimator.
2.3.0,Add or remove dimensions of size 1 to match the shape of the layer.
2.3.0,Should we keep a separate global step count for each submodel?
2.3.0,use central difference since forward difference has a pretty high
2.3.0,approximation error
2.3.0,assert min_coords[1][0] != new_x[3]
2.3.0,assert min_coords[1][1] != new_x[4]
2.3.0,assert min_coords[1][2] != new_x[5]
2.3.0,Prepare Training Data
2.3.0,Train the model
2.3.0,Prepare the Testing data
2.3.0,predict
2.3.0,check output shape
2.3.0,new object of UNet to test if loading the model results in same predictions
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"But if we specify a different starting state, that should produce a"
2.3.0,different result.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"But if we specify a different starting state, that should produce a"
2.3.0,different result.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,TODO What should shape[1] be?  It's not documented.
2.3.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"TODO What should the output shape be?  It's not documented, and there"
2.3.0,are no other test cases for it.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,"Creating a second layer should produce different results, since it has"
2.3.0,different random weights.
2.3.0,But evaluating the first layer again should produce the same result as before.
2.3.0,Set by variable constructor.
2.3.0,Set by set_variable_initial_values().
2.3.0,Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
2.3.0,Optimize the main loss.  This should send both variables toward 1.
2.3.0,Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
2.3.0,"If we don't specify the initial state, it should default to zeros."
2.3.0,Explicitly specifying the zero state should give the same result.
2.3.0,Specifying a different initial state should produce a different result.
2.3.0,We should get the same result with either predict_on_batch() or __call__().
2.3.0,Take a tiny step in the direction of s and see if the output changes by
2.3.0,the expected amount.
2.3.0,Test for correct value return (normal mode)
2.3.0,Test for shapes (normal mode)
2.3.0,Test for correct value return (eager mode)
2.3.0,Test for shape (eager mode)
2.3.0,"This isn't a meaningful loss, but just for test"
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.3.0,"Now an (N, M) shape"
2.3.0,TODO(rbharath): Move this into a model directly
2.3.0,def test_vina(self):
2.3.0,"""""""Test that vina graph can be constructed in TensorGraph."""""""
2.3.0,N_protein = 4
2.3.0,N_ligand = 1
2.3.0,N_atoms = 5
2.3.0,M_nbrs = 2
2.3.0,ndim = 3
2.3.0,start = 0
2.3.0,stop = 4
2.3.0,nbr_cutoff = 1
2.3.0,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
2.3.0,start))
2.3.0,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
2.3.0,start))
2.3.0,y = NumpyDataset(np.random.rand(
2.3.0,"1,))"
2.3.0,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
2.3.0,"# used in the current implementation. This is obviously wrong, but need"
2.3.0,# to dig out why this is happening.
2.3.0,"prot_coords = Feature(shape=(N_protein, ndim))"
2.3.0,"ligand_coords = Feature(shape=(N_ligand, ndim))"
2.3.0,"labels = Label(shape=(1,))"
2.3.0,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
2.3.0,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
2.3.0,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
2.3.0,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
2.3.0,"# Now an (N, M) shape"
2.3.0,nbr_list = NeighborList(
2.3.0,"N_protein + N_ligand,"
2.3.0,"M_nbrs,"
2.3.0,"ndim,"
2.3.0,"nbr_cutoff,"
2.3.0,"start,"
2.3.0,"stop,"
2.3.0,in_layers=[coords])
2.3.0,"# Shape (N, M)"
2.3.0,dists = InteratomicL2Distances(
2.3.0,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
2.3.0,repulsion = VinaRepulsion(in_layers=[dists])
2.3.0,hydrophobic = VinaHydrophobic(in_layers=[dists])
2.3.0,hbond = VinaHydrogenBond(in_layers=[dists])
2.3.0,gauss_1 = VinaGaussianFirst(in_layers=[dists])
2.3.0,gauss_2 = VinaGaussianSecond(in_layers=[dists])
2.3.0,"# Shape (N, M)"
2.3.0,interactions = WeightedLinearCombo(
2.3.0,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
2.3.0,"# Shape (N, M)"
2.3.0,"thresholded = Cutoff(in_layers=[dists, interactions])"
2.3.0,"# Shape (N, M)"
2.3.0,free_energies = VinaNonlinearity(in_layers=[thresholded])
2.3.0,free_energy = ReduceSum(in_layers=[free_energies])
2.3.0,"loss = L2Loss(in_layers=[free_energy, labels])"
2.3.0,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
2.3.0,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
2.3.0,tg.set_loss(loss)
2.3.0,tg.fit_generator(databag.iterbatches(epochs=1))
2.3.0,TODO(rbharath): This test should pass. Fix it!
2.3.0,def test_graph_pool(self):
2.3.0,"""""""Test that GraphPool can be invoked."""""""
2.3.0,out_channels = 2
2.3.0,"n_atoms = 4 # In CCC and C, there are 4 atoms"
2.3.0,"raw_smiles = ['CCC', 'C']"
2.3.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.3.0,featurizer = ConvMolFeaturizer()
2.3.0,mols = featurizer.featurize(mols)
2.3.0,multi_mol = ConvMol.agglomerate_mols(mols)
2.3.0,atom_features = multi_mol.get_atom_features()
2.3.0,degree_slice = multi_mol.deg_slice
2.3.0,membership = multi_mol.membership
2.3.0,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
2.3.0,with self.session() as sess:
2.3.0,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
2.3.0,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
2.3.0,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
2.3.0,deg_adjs_tf = []
2.3.0,for deg_adj in deg_adjs:
2.3.0,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
2.3.0,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
2.3.0,out_tensor = GraphPool(out_channels)(*args)
2.3.0,sess.run(tf.global_variables_initializer())
2.3.0,out_tensor = out_tensor.eval()
2.3.0,"assert out_tensor.shape == (n_atoms, out_channels)"
2.3.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.3.0,"Layer is wrapper around embedding lookup, tested that then"
2.3.0,Expected output
2.3.0,Create a dataset with three tasks.  The first two tasks each depend only
2.3.0,on half the features.  The third task depends on all of them.
2.3.0,Create an OntologyModel.  Two leaf nodes contain half the features.
2.3.0,Train the model on the datase.
2.3.0,It should have learned to predict all of the tasks accurately.
2.3.0,"In addition, it should be able to predict the first task based only on the"
2.3.0,"first leaf node, and the second task based only on the second leaf node."
2.3.0,Create a dataset with three tasks.  The first two tasks each depend only
2.3.0,on half the features.  The third task depends on all of them.
2.3.0,Create an OntologyModel.  Two leaf nodes contain half the features.
2.3.0,Train the model on the datase.
2.3.0,It should have learned to predict all of the tasks accurately.
2.3.0,"In addition, it should be able to predict the first task based only on the"
2.3.0,"first leaf node, and the second task based only on the second leaf node."
2.3.0,Here are mappings for just a few yeast genes.
2.3.0,"Build the ontology, then see if it looks correct."
2.3.0,Should be able to call fit twice without failure.
2.3.0,# TODO(rbharath): Transform these into useful weights.
2.3.0,#class_weight={
2.3.0,"#    True: num_sequences / num_positives,"
2.3.0,#    False: num_sequences / num_negatives
2.3.0,"#} if not multitask else None,"
2.3.0,# TODO(rbharath): Add a test with per-class weighting.
2.3.0,#class_weight={
2.3.0,"#    True: num_sequences / num_positives,"
2.3.0,#    False: num_sequences / num_negatives
2.3.0,"#} if not multitask else None,"
2.3.0,Prepare Training Data
2.3.0,Train the model
2.3.0,Prepare the Testing data
2.3.0,predict
2.3.0,check output shape
2.3.0,new object of ResNet to test if loading the model results in same predictions
2.3.0,Train the model on random sequences.  We aren't training long enough to
2.3.0,"really make it reliable, but I want to keep this test fast, and it should"
2.3.0,still be able to reproduce a reasonable fraction of input sequences.
2.3.0,Test it out.
2.3.0,Check that it got at least a quarter of them correct.
2.3.0,Test it out.
2.3.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.3.0,"few steps of training to make sure nothing crashes, then check that the"
2.3.0,results are at least internally consistent.
2.3.0,def test_multi_task_classifier(self):
2.3.0,"""""""Test creating an Estimator from a MultitaskClassifier."""""""
2.3.0,n_samples = 10
2.3.0,n_features = 3
2.3.0,n_tasks = 2
2.3.0,
2.3.0,# Create a dataset and an input function for processing it.
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"X = np.random.rand(n_samples, n_features)"
2.3.0,"y = np.zeros((n_samples, n_tasks))"
2.3.0,"w = np.ones((n_samples, n_tasks))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y, w)"
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"return {'x': x, 'weights': weights}, y"
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,
2.3.0,"model = dc.models.MultitaskClassifier(n_tasks, n_features, dropouts=0)"
2.3.0,
2.3.0,# Create an estimator from it.
2.3.0,
2.3.0,"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,
2.3.0,"def accuracy(labels, predictions, weights):"
2.3.0,"return tf.metrics.accuracy(labels, tf.round(predictions), weights)"
2.3.0,
2.3.0,metrics = {'accuracy': accuracy}
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
2.3.0,
2.3.0,# Train the model.
2.3.0,
2.3.0,estimator.train(input_fn=lambda: input_fn(100))
2.3.0,
2.3.0,# Evaluate the model.
2.3.0,
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['loss'] < 1e-4
2.3.0,assert results['accuracy'] > 0.9
2.3.0,
2.3.0,def test_multi_task_regressor(self):
2.3.0,"""""""Test creating an Estimator from a MultitaskRegressor."""""""
2.3.0,n_samples = 10
2.3.0,n_features = 3
2.3.0,n_tasks = 2
2.3.0,
2.3.0,# Create a dataset and an input function for processing it.
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"X = np.random.rand(n_samples, n_features)"
2.3.0,"y = np.zeros((n_samples, n_tasks))"
2.3.0,"w = np.ones((n_samples, n_tasks))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y, w)"
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"return {'x': x, 'weights': weights}, y"
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,
2.3.0,"model = dc.models.MultitaskRegressor(n_tasks, n_features, dropouts=0)"
2.3.0,
2.3.0,# Create an estimator from it.
2.3.0,
2.3.0,"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,metrics = {'error': tf.metrics.mean_absolute_error}
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
2.3.0,
2.3.0,# Train the model.
2.3.0,
2.3.0,estimator.train(input_fn=lambda: input_fn(100))
2.3.0,
2.3.0,# Evaluate the model.
2.3.0,
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['loss'] < 1e-3
2.3.0,assert results['error'] < 0.1
2.3.0,
2.3.0,def test_robust_multi_task_classifier(self):
2.3.0,"""""""Test creating an Estimator from a MultitaskClassifier."""""""
2.3.0,n_samples = 10
2.3.0,n_features = 3
2.3.0,n_tasks = 2
2.3.0,
2.3.0,# Create a dataset and an input function for processing it.
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"X = np.random.rand(n_samples, n_features)"
2.3.0,"y = np.zeros((n_samples, n_tasks))"
2.3.0,"w = np.ones((n_samples, n_tasks))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y, w)"
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"return {'x': x, 'weights': weights}, y"
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,
2.3.0,model = dc.models.RobustMultitaskClassifier(
2.3.0,"n_tasks,"
2.3.0,"n_features,"
2.3.0,"layer_sizes=[50],"
2.3.0,"bypass_layer_sizes=[10],"
2.3.0,"dropouts=0,"
2.3.0,"bypass_dropouts=0,"
2.3.0,learning_rate=0.003)
2.3.0,
2.3.0,# Create an estimator from it.
2.3.0,
2.3.0,"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,
2.3.0,"def accuracy(labels, predictions, weights):"
2.3.0,"return tf.metrics.accuracy(labels, tf.round(predictions), weights)"
2.3.0,
2.3.0,metrics = {'accuracy': accuracy}
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
2.3.0,
2.3.0,# Train the model.
2.3.0,
2.3.0,estimator.train(input_fn=lambda: input_fn(500))
2.3.0,
2.3.0,# Evaluate the model.
2.3.0,
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['loss'] < 1e-2
2.3.0,assert results['accuracy'] > 0.9
2.3.0,
2.3.0,def test_robust_multi_task_regressor(self):
2.3.0,"""""""Test creating an Estimator from a MultitaskRegressor."""""""
2.3.0,n_samples = 10
2.3.0,n_features = 3
2.3.0,n_tasks = 2
2.3.0,
2.3.0,# Create a dataset and an input function for processing it.
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"X = np.random.rand(n_samples, n_features)"
2.3.0,"y = np.zeros((n_samples, n_tasks))"
2.3.0,"w = np.ones((n_samples, n_tasks))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y, w)"
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"return {'x': x, 'weights': weights}, y"
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,
2.3.0,model = dc.models.RobustMultitaskRegressor(
2.3.0,"n_tasks,"
2.3.0,"n_features,"
2.3.0,"layer_sizes=[50],"
2.3.0,"bypass_layer_sizes=[10],"
2.3.0,"dropouts=0,"
2.3.0,"bypass_dropouts=0,"
2.3.0,learning_rate=0.003)
2.3.0,
2.3.0,# Create an estimator from it.
2.3.0,
2.3.0,"x_col = tf.feature_column.numeric_column('x', shape=(n_features,))"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,metrics = {'error': tf.metrics.mean_absolute_error}
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
2.3.0,
2.3.0,# Train the model.
2.3.0,
2.3.0,estimator.train(input_fn=lambda: input_fn(500))
2.3.0,
2.3.0,# Evaluate the model.
2.3.0,
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['loss'] < 1e-2
2.3.0,assert results['error'] < 1e-2
2.3.0,Create a dataset and an input function for processing it.
2.3.0,Create the model.
2.3.0,Create an estimator from it.
2.3.0,Train the model.
2.3.0,Evaluate the model.
2.3.0,def test_irv(self):
2.3.0,"""""""Test creating an Estimator from a IRVClassifier."""""""
2.3.0,n_samples = 50
2.3.0,n_features = 3
2.3.0,n_tasks = 2
2.3.0,
2.3.0,# Create a dataset and an input function for processing it.
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"X = np.random.rand(n_samples, n_features)"
2.3.0,"y = np.zeros((n_samples, n_tasks))"
2.3.0,"w = np.ones((n_samples, n_tasks))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y, w)"
2.3.0,"transformers = [dc.trans.IRVTransformer(10, n_tasks, dataset)]"
2.3.0,
2.3.0,for transformer in transformers:
2.3.0,dataset = transformer.transform(dataset)
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"return {'x': x, 'weights': weights}, y"
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,
2.3.0,model = dc.models.TensorflowMultitaskIRVClassifier(
2.3.0,"n_tasks, K=10, learning_rate=0.001, penalty=0.05, batch_size=50)"
2.3.0,model.build()
2.3.0,# Create an estimator from it.
2.3.0,
2.3.0,"x_col = tf.feature_column.numeric_column('x', shape=(2 * 10 * n_tasks,))"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,
2.3.0,"def accuracy(labels, predictions, weights):"
2.3.0,"return tf.metrics.accuracy(labels, tf.round(predictions[:, :, 1]),"
2.3.0,weights)
2.3.0,
2.3.0,metrics = {'accuracy': accuracy}
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
2.3.0,
2.3.0,# Train the model.
2.3.0,
2.3.0,estimator.train(input_fn=lambda: input_fn(100))
2.3.0,
2.3.0,# Evaluate the model.
2.3.0,
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['accuracy'] > 0.9
2.3.0,
2.3.0,def test_textcnn_classification(self):
2.3.0,"""""""Test creating an Estimator from TextCNN for classification."""""""
2.3.0,n_tasks = 2
2.3.0,n_samples = 5
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,seq_length = 20
2.3.0,model = dc.models.TextCNNModel(
2.3.0,"n_tasks=n_tasks,"
2.3.0,"char_dict=default_dict,"
2.3.0,"seq_length=seq_length,"
2.3.0,"kernel_sizes=[5, 5],"
2.3.0,"num_filters=[20, 20])"
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"smile_ids = [""CCCCC"", ""CCC(=O)O"", ""CCC"", ""CC(=O)O"", ""O=C=O""]"
2.3.0,X = smile_ids
2.3.0,"y = np.zeros((n_samples, n_tasks))"
2.3.0,"w = np.ones((n_samples, n_tasks))"
2.3.0,"dataset = NumpyDataset(X, y, w, smile_ids)"
2.3.0,
2.3.0,"def accuracy(labels, predictions, weights):"
2.3.0,"return tf.metrics.accuracy(labels, tf.round(predictions), weights)"
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"smiles_seq = tf.py_func(model.smiles_to_seq_batch, inp=[x], Tout=tf.int32)"
2.3.0,"return {'x': smiles_seq, 'weights': weights}, y"
2.3.0,
2.3.0,# Create an estimator from it.
2.3.0,x_col = tf.feature_column.numeric_column(
2.3.0,"'x', shape=(seq_length,), dtype=tf.int32)"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,metrics = {'accuracy': accuracy}
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
2.3.0,
2.3.0,# Train the model.
2.3.0,estimator.train(input_fn=lambda: input_fn(100))
2.3.0,
2.3.0,# Evaluate results
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['loss'] < 1e-2
2.3.0,assert results['accuracy'] > 0.9
2.3.0,
2.3.0,def test_textcnn_regression(self):
2.3.0,"""""""Test creating an Estimator from TextCNN for regression."""""""
2.3.0,n_tasks = 2
2.3.0,n_samples = 10
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,seq_length = 20
2.3.0,model = dc.models.TextCNNModel(
2.3.0,"n_tasks=n_tasks,"
2.3.0,"char_dict=default_dict,"
2.3.0,"seq_length=seq_length,"
2.3.0,"kernel_sizes=[5, 5],"
2.3.0,"num_filters=[20, 20],"
2.3.0,"mode=""regression"")"
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"smile_ids = [""CCCCC"", ""CCC(=O)O"", ""CCC"", ""CC(=O)O"", ""O=C=O""]"
2.3.0,X = smile_ids
2.3.0,"y = np.zeros((n_samples, n_tasks, 1), dtype=np.float32)"
2.3.0,"w = np.ones((n_samples, n_tasks))"
2.3.0,"dataset = NumpyDataset(X, y, w, smile_ids)"
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"smiles_seq = tf.py_func(model.smiles_to_seq_batch, inp=[x], Tout=tf.int32)"
2.3.0,"return {'x': smiles_seq, 'weights': weights}, y"
2.3.0,
2.3.0,# Create an estimator from it.
2.3.0,x_col = tf.feature_column.numeric_column(
2.3.0,"'x', shape=(seq_length,), dtype=tf.int32)"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,metrics = {'error': tf.metrics.mean_absolute_error}
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col], weight_column=weight_col, metrics=metrics)"
2.3.0,
2.3.0,# Train the model.
2.3.0,estimator.train(input_fn=lambda: input_fn(100))
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['loss'] < 1e-1
2.3.0,assert results['error'] < 0.1
2.3.0,
2.3.0,def test_scscore(self):
2.3.0,"""""""Test creating an Estimator from a ScScoreModel."""""""
2.3.0,n_samples = 10
2.3.0,n_features = 3
2.3.0,n_tasks = 1
2.3.0,
2.3.0,# Create a dataset and an input function for processing it.
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,"X = np.random.rand(n_samples, 2, n_features)"
2.3.0,"y = np.zeros((n_samples, n_tasks))"
2.3.0,"dataset = dc.data.NumpyDataset(X, y)"
2.3.0,
2.3.0,def input_fn(epochs):
2.3.0,"x, y, weights = dataset.make_iterator("
2.3.0,"batch_size=n_samples, epochs=epochs).get_next()"
2.3.0,"x1 = x[:, 0]"
2.3.0,"x2 = x[:, 1]"
2.3.0,"return {'x1': x1, 'x2': x2, 'weights': weights}, y"
2.3.0,
2.3.0,# Create a TensorGraph model.
2.3.0,
2.3.0,"model = dc.models.ScScoreModel(n_features, dropouts=0)"
2.3.0,del model.outputs[:]
2.3.0,model.outputs.append(model.difference)
2.3.0,
2.3.0,"def accuracy(labels, predictions, weights):"
2.3.0,predictions = tf.nn.relu(tf.sign(predictions))
2.3.0,"return tf.metrics.accuracy(labels, predictions, weights)"
2.3.0,
2.3.0,# Create an estimator from it.
2.3.0,
2.3.0,"x_col1 = tf.feature_column.numeric_column('x1', shape=(n_features,))"
2.3.0,"x_col2 = tf.feature_column.numeric_column('x2', shape=(n_features,))"
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(1,))"
2.3.0,
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=[x_col1, x_col2],"
2.3.0,"metrics={'accuracy': accuracy},"
2.3.0,weight_column=weight_col)
2.3.0,
2.3.0,# Train the model.
2.3.0,
2.3.0,estimator.train(input_fn=lambda: input_fn(100))
2.3.0,
2.3.0,# Evaluate the model.
2.3.0,
2.3.0,results = estimator.evaluate(input_fn=lambda: input_fn(1))
2.3.0,assert results['loss'] < 0.5
2.3.0,assert results['accuracy'] > 0.6
2.3.0,Create a dataset and an input function for processing it.
2.3.0,Create a TensorGraph model.
2.3.0,Create an estimator from it.
2.3.0,Train the model.
2.3.0,@flaky
2.3.0,def test_dtnn_regression_model(self):
2.3.0,"""""""Test creating an estimator for DTNNGraphModel for regression"""""""
2.3.0,current_dir = os.path.dirname(os.path.abspath(__file__))
2.3.0,"input_file = os.path.join(current_dir, ""example_DTNN.mat"")"
2.3.0,dataset = loadmat(input_file)
2.3.0,
2.3.0,num_vals_to_use = 20
2.3.0,
2.3.0,np.random.seed(123)
2.3.0,X = dataset['X'][:num_vals_to_use]
2.3.0,y = dataset['T'][:num_vals_to_use].astype(np.float32)
2.3.0,w = np.ones_like(y)
2.3.0,"dataset = dc.data.NumpyDataset(X, y, w, ids=None)"
2.3.0,n_tasks = y.shape[1]
2.3.0,n_samples = y.shape[0]
2.3.0,
2.3.0,"dtypes = [tf.int32, tf.float32, tf.int32, tf.int32, tf.int32]"
2.3.0,
2.3.0,model = dc.models.DTNNModel(
2.3.0,"n_tasks,"
2.3.0,"n_embedding=20,"
2.3.0,"n_distance=100,"
2.3.0,"learning_rate=1.0,"
2.3.0,"mode=""regression"")"
2.3.0,
2.3.0,"def mean_relative_error(labels, predictions, weights):"
2.3.0,"error = tf.abs(1 - tf.math.divide(labels, predictions))"
2.3.0,"error_val, update_op = tf.metrics.mean(error)"
2.3.0,"return error_val, update_op"
2.3.0,
2.3.0,"def input_fn(batch_size, epochs):"
2.3.0,"X, y, weights = dataset.make_iterator("
2.3.0,"batch_size=batch_size, epochs=epochs).get_next()"
2.3.0,features = tf.py_func(
2.3.0,"model.compute_features_on_batch, inp=[X], Tout=dtypes)"
2.3.0,
2.3.0,assert len(features) == 5
2.3.0,feature_dict = dict()
2.3.0,feature_dict['atom_num'] = features[0]
2.3.0,feature_dict['distance'] = features[1]
2.3.0,feature_dict['dist_mem_i'] = features[2]
2.3.0,feature_dict['dist_mem_j'] = features[3]
2.3.0,feature_dict['atom_mem'] = features[4]
2.3.0,feature_dict['weights'] = weights
2.3.0,
2.3.0,"return feature_dict, y"
2.3.0,
2.3.0,atom_number = tf.feature_column.numeric_column(
2.3.0,"'atom_num', shape=[], dtype=dtypes[0])"
2.3.0,distance = tf.feature_column.numeric_column(
2.3.0,"'distance', shape=(model.n_distance,), dtype=dtypes[1])"
2.3.0,atom_mem = tf.feature_column.numeric_column(
2.3.0,"'atom_mem', shape=[], dtype=dtypes[2])"
2.3.0,dist_mem_i = tf.feature_column.numeric_column(
2.3.0,"'dist_mem_i', shape=[], dtype=dtypes[3])"
2.3.0,dist_mem_j = tf.feature_column.numeric_column(
2.3.0,"'dist_mem_j', shape=[], dtype=dtypes[4])"
2.3.0,
2.3.0,"weight_col = tf.feature_column.numeric_column('weights', shape=(n_tasks,))"
2.3.0,metrics = {'error': mean_relative_error}
2.3.0,
2.3.0,"feature_cols = [atom_number, distance, dist_mem_i, dist_mem_j, atom_mem]"
2.3.0,estimator = model.make_estimator(
2.3.0,"feature_columns=feature_cols, weight_column=weight_col, metrics=metrics)"
2.3.0,"estimator.train(input_fn=lambda: input_fn(100, 250))"
2.3.0,
2.3.0,"results = estimator.evaluate(input_fn=lambda: input_fn(n_samples, 1))"
2.3.0,assert results['error'] < 0.1
2.3.0,Construct layers for all nodes.
2.3.0,Create the loss function.
2.3.0,Create inputs for the features.
2.3.0,Create inputs for the children.
2.3.0,Concatenate all inputs together.
2.3.0,Create the output.
2.3.0,"If necessary, download the file defining the ontology."
2.3.0,Parse the ontology definition and create a list of terms.
2.3.0,Create OntologyNode objects for all the terms.
2.3.0,"Assign parent-child relationships between nodes, and identify root nodes."
2.3.0,Create a single root node that combines the three GO roots.
2.3.0,Assign features to nodes.
2.3.0,Count the number of features within each node.  Eliminate nodes with too few
2.3.0,features and set the number of outputs for each one.
2.3.0,import tensorflow as tf
2.3.0,from deepchem.models.tensorgraph.tensor_graph import MultitaskTensorGraph
2.3.0,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
2.3.0,
2.3.0,
2.3.0,class WeightedError(Layer):
2.3.0,
2.3.0,"def __call__(self, *parents):"
2.3.0,"entropy, weights = parents[0], parents[1]"
2.3.0,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
2.3.0,return self.out_tensor
2.3.0,
2.3.0,
2.3.0,"def MultitaskClassifier(n_tasks,"
2.3.0,"n_features,"
2.3.0,"layer_sizes=[500],"
2.3.0,"bypass_layer_sizes=[100],"
2.3.0,model_dir=None):
2.3.0,""""""""
2.3.0,TODO(LESWING) Add Dropout and regularization
2.3.0,
2.3.0,Parameters
2.3.0,----------
2.3.0,n_tasks
2.3.0,n_features
2.3.0,layer_sizes
2.3.0,bypass_layer_sizes
2.3.0,model_dir
2.3.0,
2.3.0,Returns
2.3.0,-------
2.3.0,
2.3.0,""""""""
2.3.0,g = MultitaskTensorGraph(model_dir=model_dir)
2.3.0,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
2.3.0,g.add_layer(in_layer)
2.3.0,g.add_feature(in_layer)
2.3.0,
2.3.0,# Shared Dense Layers
2.3.0,prev_layer = in_layer
2.3.0,dense_layers = []
2.3.0,for i in range(len(layer_sizes)):
2.3.0,dense = Dense(
2.3.0,"out_channels=layer_sizes[i],"
2.3.0,"name=""SDENSE%s"" % i,"
2.3.0,activation_fn=tf.nn.relu)
2.3.0,"g.add_layer(dense, parents=[prev_layer])"
2.3.0,dense_layers.append(dense)
2.3.0,prev_layer = dense
2.3.0,
2.3.0,# Individual Bypass Layers
2.3.0,costs = []
2.3.0,for task in range(n_tasks):
2.3.0,prev_layer = in_layer
2.3.0,for i in range(len(bypass_layer_sizes)):
2.3.0,dense = Dense(
2.3.0,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
2.3.0,"g.add_layer(dense, parents=[prev_layer])"
2.3.0,prev_layer = dense
2.3.0,"joined_layer = Concat(name=""JOIN%s"" % task)"
2.3.0,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
2.3.0,
2.3.0,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
2.3.0,"g.add_layer(classification, parents=[joined_layer])"
2.3.0,
2.3.0,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
2.3.0,"g.add_layer(softmax, parents=[classification])"
2.3.0,g.add_output(softmax)
2.3.0,
2.3.0,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
2.3.0,g.add_layer(label)
2.3.0,g.add_label(label)
2.3.0,
2.3.0,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
2.3.0,"g.add_layer(cost, parents=[label, classification])"
2.3.0,costs.append(cost)
2.3.0,
2.3.0,"entropy = Concat(name=""ENT"")"
2.3.0,"g.add_layer(entropy, parents=costs)"
2.3.0,
2.3.0,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
2.3.0,g.add_layer(task_weights)
2.3.0,g.set_task_weights(task_weights)
2.3.0,
2.3.0,"loss = WeightedError(name=""ERROR"")"
2.3.0,"g.add_layer(loss, parents=[entropy, task_weights])"
2.3.0,g.set_loss(loss)
2.3.0,
2.3.0,return g
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,(ytz): this is really dirty but needed for restoring models
2.3.0,update model with best param
2.3.0,Find optimal n_estimators based on original learning_rate
2.3.0,and early_stopping_rounds
2.3.0,"Since test size is 20%, when retrain model to whole data, expect"
2.3.0,n_estimator increased to 1/0.8 = 1.25 time.
2.3.0,Make sure user specified params are in the grid.
2.3.0,Change params back original params
2.3.0,Predict the output and uncertainty.
2.3.0,Predict the output and uncertainty.
2.3.0,Predict the output and uncertainty.
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Check same predictions are made.
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Load trained model
2.3.0,Eval model on train
2.3.0,"For simplicity, let's assume both molecules have same number of"
2.3.0,atoms.
2.3.0,Creates a set of dummy features that contain the coordinate and
2.3.0,neighbor-list features required by the AtomicConvModel.
2.3.0,"frag2_z = np.random.rand(N_atoms, 3)"
2.3.0,"For simplicity, let's assume both molecules have same number of"
2.3.0,atoms.
2.3.0,Creates a set of dummy features that contain the coordinate and
2.3.0,neighbor-list features required by the AtomicConvModel.
2.3.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.3.0,max num atoms instead of exact.
2.3.0,Cutoff in angstroms
2.3.0,arbitrary label
2.3.0,Run a fitting operation
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Fit trained model
2.3.0,Eval model on train/test
2.3.0,Fit trained model
2.3.0,Eval model on train/test
2.3.0,Test Parameter getting and setting
2.3.0,Fit trained model
2.3.0,Eval model on train/test
2.3.0,See if it has done a plausible job of learning the distribution.
2.3.0,See if it has done a plausible job of learning the distribution.
2.3.0,We have to set the gradient penalty very small because the generator's
2.3.0,"output is only a single number, so the default penalty would constrain"
2.3.0,it far too much.
2.3.0,See if it has done a plausible job of learning the distribution.
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,n_samples = 100
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Load mini log-solubility dataset.
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Load mini log-solubility dataset.
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Load mini log-solubility dataset.
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Load mini log-solubility dataset.
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Load mini log-solubility dataset.
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on train
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Predict the output and uncertainty.
2.3.0,Check that predicting internal layers works.
2.3.0,Create two models using the same model directory.
2.3.0,Check that they produce different results.
2.3.0,"Save a checkpoint from the first model and load it into the second one,"
2.3.0,and make sure they now match.
2.3.0,Train a model to overfit the dataset.
2.3.0,"Create an identical model, do a single step of fitting with restore=True,"
2.3.0,and make sure it got restored correctly.
2.3.0,Build a model that predicts uncertainty.
2.3.0,Fit the model and see if its predictions are correct.
2.3.0,Take a tiny step in the direction of s and see if the output changes by
2.3.0,the expected amount.
2.3.0,def test_singletask_to_multitask_classification(self):
2.3.0,n_features = 10
2.3.0,n_tasks = 17
2.3.0,tasks = range(n_tasks)
2.3.0,# Define train dataset
2.3.0,n_train = 100
2.3.0,"X_train = np.random.rand(n_train, n_features)"
2.3.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.3.0,w_train = np.ones_like(y_train)
2.3.0,"ids_train = [""C""] * n_train"
2.3.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.3.0,"X_train, y_train, w_train, ids_train)"
2.3.0,# Define test dataset
2.3.0,n_test = 10
2.3.0,"X_test = np.random.rand(n_test, n_features)"
2.3.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.3.0,w_test = np.ones_like(y_test)
2.3.0,"ids_test = [""C""] * n_test"
2.3.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.3.0,"X_test, y_test, w_test, ids_test)"
2.3.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.3.0,def model_builder(model_dir):
2.3.0,sklearn_model = LogisticRegression()
2.3.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.3.0,multitask_model = dc.models.SingletaskToMultitask(
2.3.0,"tasks, model_builder)"
2.3.0,# Fit trained model
2.3.0,multitask_model.fit(train_dataset)
2.3.0,multitask_model.save()
2.3.0,# Eval multitask_model on train/test
2.3.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.3.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.3.0,Generate data
2.3.0,Cleanup
2.3.0,Train the model while logging the validation ROC AUC.
2.3.0,Parse the log to pull out the AUC scores.
2.3.0,The last reported score should match the current performance of the model.
2.3.0,Reload the save model and confirm that it matches the best logged score.
2.3.0,Create a dataset and an input function for processing it.
2.3.0,Generate dummy dataset
2.3.0,Fit trained model
2.3.0,Eval model on test
2.3.0,Eval model on train
2.3.0,Fit trained model
2.3.0,Eval model on test
2.3.0,Fit trained model
2.3.0,Eval model on test
2.3.0,def test_sklearn_classification(self):
2.3.0,"""""""Test that sklearn models can learn on simple classification datasets."""""""
2.3.0,np.random.seed(123)
2.3.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.3.0,"X, y = dataset.data, dataset.target"
2.3.0,frac_train = .7
2.3.0,n_samples = len(X)
2.3.0,n_train = int(frac_train*n_samples)
2.3.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.3.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.3.0,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
2.3.0,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
2.3.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.3.0,sklearn_model = LogisticRegression()
2.3.0,model = dc.models.SklearnModel(sklearn_model)
2.3.0,# Fit trained model
2.3.0,model.fit(train_dataset)
2.3.0,model.save()
2.3.0,# Eval model on test
2.3.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.3.0,assert scores[classification_metric.name] > .5
2.3.0,def test_sklearn_multitask_classification(self):
2.3.0,"""""""Test that sklearn models can learn on simple multitask classification."""""""
2.3.0,np.random.seed(123)
2.3.0,n_tasks = 4
2.3.0,tasks = range(n_tasks)
2.3.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.3.0,"X, y = dataset.data, dataset.target"
2.3.0,"y = np.reshape(y, (len(y), 1))"
2.3.0,y = np.hstack([y] * n_tasks)
2.3.0,
2.3.0,frac_train = .7
2.3.0,n_samples = len(X)
2.3.0,n_train = int(frac_train*n_samples)
2.3.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.3.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.3.0,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
2.3.0,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
2.3.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.3.0,def model_builder(model_dir):
2.3.0,sklearn_model = LogisticRegression()
2.3.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.3.0,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
2.3.0,# Fit trained model
2.3.0,model.fit(train_dataset)
2.3.0,model.save()
2.3.0,# Eval model on test
2.3.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.3.0,for score in scores[classification_metric.name]:
2.3.0,assert score > .5
2.3.0,Set early stopping round = n_estimators so that esr won't work
2.3.0,Fit trained model
2.3.0,Eval model on test
2.3.0,Fit trained model
2.3.0,Eval model on test
2.3.0,Fit trained model
2.3.0,Eval model on test
2.3.0,Logistic regression doesn't support weights
2.3.0,-*- coding: utf-8 -*-
2.3.0,Assigning featurizer if not user defined
2.3.0,loading datasets
2.3.0,Assembling train and valid datasets
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,Building tensorflow MultitaskDNN model
2.3.0,Building tensorflow robust MultitaskDNN model
2.3.0,Building scikit logistic regression model
2.3.0,Transform fingerprints to IRV features
2.3.0,Building tensorflow IRV model
2.3.0,Building scikit random forest model
2.3.0,Building scikit learn Kernel SVM model
2.3.0,Building xgboost classification model
2.3.0,Remove token for paddings
2.3.0,Building scikit random forest model
2.3.0,Building scikit learn Kernel Ridge Regression model
2.3.0,Building scikit learn Kernel Ridge Regression model
2.3.0,Building xgboost regression model
2.3.0,Loading hyperparameters
2.3.0,num positive/negative ligands
2.3.0,Set batch sizes for network
2.3.0,Model structure
2.3.0,Traning settings
2.3.0,Fit trained model
2.3.0,Evaluating low data model
2.3.0,-*- coding: utf-8 -*-
2.3.0,Assigning featurizer if not user defined
2.3.0,loading datasets
2.3.0,
2.3.0,Note by @XericZephyr. Reason why I spun off this function:
2.3.0,1. Some model needs dataset information.
2.3.0,2. It offers us possibility to **cache** the dataset
2.3.0,"if the featurizer runs very slow, e.g., GraphConv."
2.3.0,2+. The cache can even happen at Travis CI to accelerate
2.3.0,CI testing.
2.3.0,
2.3.0,loading datasets
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
2.3.0,Featurize qm9 dataset
2.3.0,TODO: Check for this
2.3.0,Download files if they don't exist
2.3.0,Featurize the KINASE dataset
2.3.0,Shuffle the training data
2.3.0,Apply transformations
2.3.0,#### TIMING ######
2.3.0,transformers = [
2.3.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.3.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.3.0,dataset=train_dataset)]
2.3.0,Set shard size low to avoid memory problems.
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,Set some global variables up top
2.3.0,Featurize KAGGLE dataset
2.3.0,############################################################# TIMING
2.3.0,############################################################# TIMING
2.3.0,No tasks since no labels provided.
2.3.0,For now images are loaded directly by ImageLoader
2.3.0,Load Sweetlead dataset
2.3.0,Featurize SWEET dataset
2.3.0,Initialize transformers
2.3.0,Featurize qm7 dataset
2.3.0,Featurize clintox dataset
2.3.0,Transform clintox dataset
2.3.0,Featurize bbb dataset
2.3.0,Initialize transformers
2.3.0,Initialize transformers
2.3.0,Load nci dataset
2.3.0,Featurize nci dataset
2.3.0,Featurize HOPV dataset
2.3.0,Featurize PPB dataset
2.3.0,Load MUV dataset
2.3.0,Featurize MUV dataset
2.3.0,Featurize clearance dataset
2.3.0,Initialize transformers
2.3.0,Featurize BBBC001 dataset
2.3.0,Featurize Images into NumpyArrays
2.3.0,Load text file with labels
2.3.0,Strip the first line which holds field labels
2.3.0,Format is: Image_name count1 count2
2.3.0,This is kludgy way to add y to dataset. Can be done better?
2.3.0,Featurize BBBC002 dataset
2.3.0,Featurize Images into NumpyArrays
2.3.0,Load text file with labels
2.3.0,Strip the first line which holds field labels
2.3.0,Format is: Image_name count1 count2
2.3.0,This is kludgy way to add y to dataset. Can be done better?
2.3.0,Featurize TOXCAST dataset
2.3.0,Download files if they don't exist
2.3.0,Featurizing datasets
2.3.0,Missing entry removal
2.3.0,Shuffle the training data
2.3.0,Apply transformations
2.3.0,#### TIMING ###########
2.3.0,Featurizer thermosol dataset
2.3.0,Featurize bace dataset
2.3.0,Initialize transformers
2.3.0,Featurize bace dataset
2.3.0,Initialize transformers
2.3.0,Featurize Tox21 dataset
2.3.0,Initialize transformers
2.3.0,Featurize ChEMBL dataset
2.3.0,TODO: Check if anything needs to be added
2.3.0,Featurize the FACTORS dataset
2.3.0,Shuffle the training data
2.3.0,Apply transformations
2.3.0,######### TIMING ################
2.3.0,Most reaction dataset ML tasks train the prediction of products from
2.3.0,"ractants. Both of these are contained in the rxn object that is output,"
2.3.0,"so there is no ""tasks"" field."
2.3.0,Download USPTO dataset
2.3.0,Unzip
2.3.0,Unzipped file is a tap seperated values file (despite the .txt)
2.3.0,The first element in the row is the reaction smarts
2.3.0,"Sometimes smarts have extraneous information at end of form """
2.3.0,"|f:0"" that causes parsing to fail. Not sure what this information"
2.3.0,"is, but just ignoring for now."
2.3.0,Make up dummy labels since DiskDataset.from_numpy doesn't allow
2.3.0,creation from just features for now.
2.3.0,TODO: This dataset isn't saved to disk so reload doesn't happen.
2.3.0,Featurizer hppb dataset
2.3.0,Featurize hiv dataset
2.3.0,Extract locations of data
2.3.0,Extract labels
2.3.0,Lines have format
2.3.0,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.3.0,"The base-10 logarithm, -log kd/pk"
2.3.0,Featurize Data
2.3.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.3.0,max num atoms instead of exact.
2.3.0,Cutoff in angstroms
2.3.0,Delete labels and ids for failing elements
2.3.0,No transformations of data
2.3.0,Split dataset
2.3.0,TODO(rbharath): This should be modified to contain a cluster split so
2.3.0,structures of the same protein aren't in both train/test
2.3.0,Featurize SIDER dataset
2.3.0,Initialize transformers
2.3.0,Featurize SAMPL dataset
2.3.0,Featurize Delaney dataset
2.3.0,Featurize PCBA dataset
2.3.0,Featurize Lipophilicity dataset
2.3.0,"Float or int hyper parameters(ex. batch_size, learning_rate)"
2.3.0,List of float or int hyper parameters(ex. layer_sizes)
2.3.0,Number of parameters
2.3.0,Range of optimization
2.3.0,Dummy names
2.3.0,Input hyper parameters
2.3.0,Run benchmark
2.3.0,Record hyperparameters
2.3.0,Record performances
2.3.0,"GPGO maximize performance by default, set performance to its negative value for minimization"
2.3.0,Readout best hyper parameters
2.3.0,Compare best model to default hyperparameters
2.3.0,Record hyperparameters
2.3.0,Record performances
2.3.0,"Optimized model is better, return hyperparameters"
2.3.0,Return default hyperparameters
2.3.0,!/usr/bin/env python2
2.3.0,-*- coding: utf-8 -*-
2.3.0,TODO(rbharath): This function is complicated and monolithic. Is there a nice
2.3.0,way to refactor this?
2.3.0,arbitrarily return last model
2.3.0,Define train dataset
2.3.0,Define validation dataset
2.3.0,Have the worker threads generate the rollouts for this iteration.
2.3.0,Perform optimization.
2.3.0,Build the feed dict and run the optimizer.
2.3.0,Update the number of steps taken so far and perform checkpointing.
2.3.0,Merge all the rollouts into a single set of arrays.
2.3.0,Iterate slices.
2.3.0,Generate the rollout.
2.3.0,Compute an estimate of the reward for the rest of the episode.
2.3.0,Compute the discounted rewards and advantages.
2.3.0,Convert the actions to one-hot.
2.3.0,Rearrange the states into the proper set of arrays.
2.3.0,Return the processed arrays.
2.3.0,Generate the rollout.
2.3.0,Compute an estimate of the reward for the rest of the episode.
2.3.0,Compute the discounted rewards and advantages.
2.3.0,"Record the actions, converting to one-hot if necessary."
2.3.0,Rearrange the states into the proper set of arrays.
2.3.0,Build the feed dict and apply gradients.
2.3.0,Assume all arrays are float32.
2.3.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.3.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.3.0,"game).  The average reward for any bet is slightly negative, so the best"
2.3.0,strategy is to walk away.
2.3.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.3.0,Optimize it.
2.3.0,"It should have learned that the expected value is very close to zero, and that the best"
2.3.0,action is to walk away.
2.3.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.3.0,get the same result.
2.3.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.3.0,The environment just has a constant state.
2.3.0,The policy includes a single recurrent layer.
2.3.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.3.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.3.0,"On the first call, the initial state should be all zeros."
2.3.0,It should still be zeros since we didn't save it last time.
2.3.0,It should be different now.
2.3.0,This should be the same as the previous one.
2.3.0,"Now we reset it, so we should get the same result as initially."
2.3.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.3.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.3.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.3.0,at all.  Using hindsight makes it much easier.
2.3.0,A simple policy with two hidden layers.
2.3.0,Optimize it.
2.3.0,Try running it a few times and see if it succeeds.
2.3.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.3.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.3.0,"game).  The average reward for any bet is slightly negative, so the best"
2.3.0,strategy is to walk away.
2.3.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.3.0,Optimize it.
2.3.0,"It should have learned that the expected value is very close to zero, and that the best"
2.3.0,action is to walk away.
2.3.0,"Verify that we can create a new A3C object, reload the parameters from the first one, and"
2.3.0,get the same result.
2.3.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.3.0,The environment just has a constant state.
2.3.0,The policy includes a single recurrent layer.
2.3.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.3.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.3.0,"On the first call, the initial state should be all zeros."
2.3.0,It should still be zeros since we didn't save it last time.
2.3.0,It should be different now.
2.3.0,This should be the same as the previous one.
2.3.0,"Now we reset it, so we should get the same result as initially."
2.3.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.3.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.3.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.3.0,at all.  Using hindsight makes it much easier.
2.3.0,A simple policy with two hidden layers.
2.3.0,Optimize it.
2.3.0,Try running it a few times and see if it succeeds.
2.3.0,The state consists of two numbers: a current value and a target value.
2.3.0,The policy just needs to learn to output the target value (or at least
2.3.0,move toward it).
2.3.0,A simple policy with no hidden layers.
2.3.0,Optimize it.
2.3.0,Try running it and see if it reaches the target
2.3.0,Randomize who goes first
2.3.0,Illegal move -- the square is not empty
2.3.0,Move X
2.3.0,Did X Win
2.3.0,Did O Win
2.3.0,TODO (Bowen): make this function less memory intensive
2.3.0,set 1st column as the column index of dataframe
2.3.0,merge descriptor and activities dataframe into output dataframe based on
2.3.0,"the molecule name, which is the index for both dataframes (but named"
2.3.0,differently). Default merge is inner merge
2.3.0,need to manually set dataframe indexname after merge based on index
2.3.0,from deepchem.scripts.dock_dude import *
2.3.0,from ipyparallel import Client
2.3.0,rc = Client()
2.3.0,dview = rc[:]
2.3.0,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
2.3.0,
2.3.0,"If mol_id is not set, then use isomeric smiles as unique identifier"
2.3.0,iterator = data_df.iterrows()
2.3.0,TODO(rbharath): BROKEN!
2.3.0,Trim unwanted indexing fields
2.3.0,Connect to running ipython server
2.3.0,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
2.3.0,
2.3.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.3.0,you may not use this file except in compliance with the License.
2.3.0,You may obtain a copy of the License at
2.3.0,
2.3.0,http://www.apache.org/licenses/LICENSE-2.0
2.3.0,
2.3.0,"Unless required by applicable law or agreed to in writing, software"
2.3.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.3.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.3.0,See the License for the specific language governing permissions and
2.3.0,limitations under the License.
2.3.0,==============================================================================
2.3.0,Maps from a function name to a dictionary that describes how to
2.3.0,map from an old argument keyword to the new argument keyword.
2.3.0,Mapping from function to the new name of the function
2.3.0,Functions that were reordered should be changed to the new keyword args
2.3.0,"for safety, if positional arguments are used. If you have reversed the"
2.3.0,"positional arguments yourself, this could do the wrong thing."
2.3.0,Specially handled functions.
2.3.0,TODO(aselle): Could check for a literal list of bools and try to convert
2.3.0,them to indices.
2.3.0,all edits are lists of chars
2.3.0,Iterate of each line
2.3.0,sort by column so that edits are processed in order in order to make
2.3.0,indexing adjustments cumulative for changes that change the string
2.3.0,length
2.3.0,"Extract each line to a list of characters, because mutable lists"
2.3.0,"are editable, unlike immutable strings."
2.3.0,Record a description of the change
2.3.0,Make underscore buffers for underlining where in the line the edit was
2.3.0,Iterate for each edit
2.3.0,"Create effective start, end by accounting for change in length due"
2.3.0,to previous edits
2.3.0,Make sure the edit is changing what it should be changing
2.3.0,Make the edit
2.3.0,Create the underline highlighting of the before and after
2.3.0,Keep track of how to generate effective ranges
2.3.0,Finish the report comment
2.3.0,"Strangely, ast.ListComp returns the col_offset of the first token"
2.3.0,after the '[' token which appears to be a bug. Workaround by
2.3.0,explicitly finding the real start of the list comprehension.
2.3.0,loop over lines
2.3.0,Reverse the text to and regular expression search for whitespace
2.3.0,First find if a [ can be found with only whitespace between it and
2.3.0,col.
2.3.0,TODO(aselle):
2.3.0,"this is poor comment detection, but it is good enough for"
2.3.0,cases where the comment does not contain string literal starting/
2.3.0,ending characters. If ast gave us start and end locations of the
2.3.0,"ast nodes rather than just start, we could use string literal"
2.3.0,node ranges to filter out spurious #'s that appear in string
2.3.0,literals.
2.3.0,"Most other nodes return proper locations (with notably does not), but"
2.3.0,it is not possible to use that in an argument.
2.3.0,"Find a simple attribute name path e.g. ""tf.foo.bar"""
2.3.0,Make sure the func is marked as being part of a call
2.3.0,Call special handlers
2.3.0,Examine any non-keyword argument and make it into a keyword argument
2.3.0,if reordering required.
2.3.0,Examine each keyword argument and convert it to the final renamed form
2.3.0,TODO(aselle): We should scan backward to find the start of the
2.3.0,keyword key. Unfortunately ast does not give you the location of
2.3.0,"keyword keys, so we are forced to infer it from the keyword arg"
2.3.0,value.
2.3.0,"Write to a temporary file, just in case we are doing an implace modify."
2.3.0,Broad exceptions are required here because ast throws whatever it wants.
2.3.0,pylint: disable=broad-except
2.3.0,pylint: enable=broad-except
2.3.0,make sure output directory doesn't exist
2.3.0,make sure output directory does not overlap with root_directory
2.3.0,Collect list of files to process (we do this to correctly handle if the
2.3.0,user puts the output directory in some sub directory of the input dir)
2.3.0,import os
2.3.0,"from deepchem.utils.save import load_from_disk, save_to_disk"
2.3.0,from deepchem.featurizers.fingerprints import CircularFingerprint
2.3.0,from deepchem.featurizers.basic import RDKitDescriptors
2.3.0,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
2.3.0,from deepchem.featurizers.grid_featurizer import GridFeaturizer
2.3.0,from deepchem.featurizers.featurize import DataLoader
2.3.0,
2.3.0,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
2.3.0,"print(""About to load dataset form disk."")"
2.3.0,dataset = load_from_disk(dataset_file)
2.3.0,"print(""Loaded dataset."")"
2.3.0,
2.3.0,grid_featurizer = GridFeaturizer(
2.3.0,"voxel_width=16.0, feature_types=""voxel_combined"","
2.3.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.3.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
2.3.0,"parallel=True, flatten=True)"
2.3.0,featurizers = [CircularFingerprint(size=1024)]
2.3.0,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
2.3.0,
2.3.0,#Make a directory in which to store the featurized complexes.
2.3.0,"base_dir = ""../../../grid_nnscore_circular_features"""
2.3.0,if not os.path.exists(base_dir):
2.3.0,os.makedirs(base_dir)
2.3.0,"data_dir = os.path.join(base_dir, ""data"")"
2.3.0,if not os.path.exists(data_dir):
2.3.0,os.makedirs(data_dir)
2.3.0,
2.3.0,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
2.3.0,
2.3.0,"feature_dir = os.path.join(base_dir, ""features"")"
2.3.0,if not os.path.exists(feature_dir):
2.3.0,os.makedirs(feature_dir)
2.3.0,
2.3.0,"samples_dir = os.path.join(base_dir, ""samples"")"
2.3.0,if not os.path.exists(samples_dir):
2.3.0,os.makedirs(samples_dir)
2.3.0,
2.3.0,
2.3.0,
2.3.0,featurizers = compound_featurizers + complex_featurizers
2.3.0,"featurizer = DataLoader(tasks=[""label""],"
2.3.0,"smiles_field=""smiles"","
2.3.0,"protein_pdb_field=""protein_pdb"","
2.3.0,"ligand_pdb_field=""ligand_pdb"","
2.3.0,"compound_featurizers=compound_featurizers,"
2.3.0,"complex_featurizers=complex_featurizers,"
2.3.0,"id_field=""complex_id"","
2.3.0,verbose=False)
2.3.0,from ipyparallel import Client
2.3.0,c = Client()
2.3.0,"print(""c.ids"")"
2.3.0,print(c.ids)
2.3.0,dview = c[:]
2.3.0,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
2.3.0,"worker_pool=dview, shard_size=1024)"
2.3.0,
2.3.0,"save_to_disk(featurized_samples, featurized_samples_file)"
2.3.0,"print(""Preparing ligand %s"" % mol_name)"
2.2.0,!/usr/bin/env python3
2.2.0,-*- coding: utf-8 -*-
2.2.0,Datasets and models used in the benchmark test
2.2.0,"irv, rf, rf_regression should be assigned manually"
2.2.0,Evaluate performances with different training set fraction
2.2.0,Datasets and models used in the benchmark test
2.2.0,Uncomment the two lines below if hyper_parameters are provided
2.2.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.2.0,hyper_parameters = pickle.load(f)
2.2.0,Will raise a CalledProcessError if fails.
2.2.0,!/usr/bin/env python3
2.2.0,-*- coding: utf-8 -*-
2.2.0,Datasets and models used in the benchmark test
2.2.0,Set numpy seed
2.2.0,##Load data###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Featurize Kinase dataset
2.2.0,##Load data###
2.2.0,num_trials = 5
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,Force matplotlib to not use any Xwindows backend.
2.2.0,##Load data###
2.2.0,the histogram of the data
2.2.0,Set numpy seed
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,num_trials = 5
2.2.0,Set some global variables up top
2.2.0,Fit trained model
2.2.0,Featurize PCBA dataset
2.2.0,Initialize transformers
2.2.0,Fit trained model
2.2.0,Load sider models now
2.2.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.2.0,transformers to predict functions
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,##Load data###
2.2.0,"n_estimators=100, max_features=int(num_features/3),"
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Load tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Featurize FACTORS dataset
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,Force matplotlib to not use any Xwindows backend.
2.2.0,##Load data###
2.2.0,the histogram of the data
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Load QM7 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Batch size of models
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Fit trained model
2.2.0,Batch size of models
2.2.0,Fit models
2.2.0,Load Tox21 dataset
2.2.0,Batch size of models
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load QM8 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Load Tox21 dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Fit trained model
2.2.0,Set numpy seed
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,Load ChEMBL dataset
2.2.0,Fit models
2.2.0,Do setup required for tf/keras models
2.2.0,Number of features on conv-mols
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,DeepCrystal Technologies 2017 - Patrick Hop
2.2.0,MIT License - have fun!!
2.2.0,Set to higher values to get better numbers
2.2.0,======================================================================
2.2.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,Only for debug!
2.2.0,Load Delaney dataset
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Do setup required for tf/keras models
2.2.0,Number of features on conv-mols
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load Delaney dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load MUV dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Evaluate train/test scores
2.2.0,Load MUV data
2.2.0,Build model
2.2.0,Fit trained model
2.2.0,Evaluate train/test scores
2.2.0,Extract active site
2.2.0,Featurize ligand
2.2.0,Default for CircularFingerprint
2.2.0,Featurize pocket
2.2.0,Note broadcast operation
2.2.0,Compute labels for pockets
2.2.0,Some complexes have labels but no PDB files. Filter these manually
2.2.0,Some of the ligand-names are of form (FMN ox). Use regex
2.2.0,to merge into form (FMN-ox)
2.2.0,Filter if missing PDB files
2.2.0,Load PDBBind dataset
2.2.0,Define featurizers
2.2.0,Featurize Dataset
2.2.0,########################################################## DEBUG
2.2.0,########################################################## DEBUG
2.2.0,For stable runs
2.2.0,Fit trained model
2.2.0,4-fold splits
2.2.0,num positive/negative ligands
2.2.0,10 trials on test-set
2.2.0,Define metric
2.2.0,Get supports on test-set
2.2.0,Compute accuracies
2.2.0,Number of features on conv-mols
2.2.0,Batch size of models
2.2.0,graph_model = dc.nn.SequentialGraph(n_feat)
2.2.0,Fit trained model
2.2.0,Test model
2.2.0,Join information for all tasks.
2.2.0,4-fold splits
2.2.0,num positive/negative ligands
2.2.0,Define metric
2.2.0,Get supports on test-set
2.2.0,Compute accuracies
2.2.0,Number of features on conv-mols
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Test model
2.2.0,Join information for all tasks.
2.2.0,4-fold splits
2.2.0,num positive/negative ligands
2.2.0,10 trials on test-set
2.2.0,Define metric
2.2.0,Get supports on test-set
2.2.0,Compute accuracies
2.2.0,Number of features on conv-mols
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Test model
2.2.0,Join information for all tasks.
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,number positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply an attention lstm layer
2.2.0,Number of folds for split
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,4-fold splits
2.2.0,num positive/negative ligands
2.2.0,10 trials on test-set
2.2.0,Define metric
2.2.0,Get supports on test-set
2.2.0,Compute accuracies
2.2.0,Train model on support
2.2.0,Test model
2.2.0,Join information for all tasks.
2.2.0,4-fold splits
2.2.0,num positive/negative ligands
2.2.0,10 trials on test-set
2.2.0,Define metric
2.2.0,Get supports on test-set
2.2.0,Compute accuracies
2.2.0,Train model on support
2.2.0,Test model
2.2.0,Join information for all tasks.
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,number positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply an attention lstm layer
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,number positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply an attention lstm layer
2.2.0,Number of folds for split
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Number of folds for split
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply a residual lstm layer
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply a residual lstm layer
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply a residual lstm layer
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply a residual lstm layer
2.2.0,Set some global variables up top
2.2.0,Featurize Tox21 dataset
2.2.0,Initialize transformers
2.2.0,Set some global variables up top
2.2.0,Featurize Tox21 dataset
2.2.0,Initialize transformers
2.2.0,Load MUV dataset
2.2.0,Featurize MUV dataset
2.2.0,Initialize transformers
2.2.0,Load MUV dataset
2.2.0,Featurize MUV dataset
2.2.0,Initialize transformers
2.2.0,Featurize SIDER dataset
2.2.0,Initialize transformers
2.2.0,Featurize SIDER dataset
2.2.0,Initialize transformers
2.2.0,Load the data.
2.2.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.2.0,sparse: most tasks do not include data for most molecules.  It also is very
2.2.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.2.0,create a list of alternating postives and negatives so each batch will have
2.2.0,equal numbers of both.
2.2.0,Create the model to train.  We use a simple fully connected network with
2.2.0,one hidden layer.
2.2.0,Define a MetaLearner describing the learning problem.
2.2.0,Run meta-learning on 80% of the tasks.
2.2.0,Validate on the remaining tasks.
2.2.0,Number of folds for split
2.2.0,Depth of attention module
2.2.0,number positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,Apply an attention lstm layer
2.2.0,4-fold splits
2.2.0,10 positive/negative ligands
2.2.0,10 trials on test-set
2.2.0,Sample supports without replacement (all pos/neg should be different)
2.2.0,Define metric
2.2.0,Get supports on test-set
2.2.0,Compute accuracies
2.2.0,Train model on support
2.2.0,Test model
2.2.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.2.0,Join information for all tasks.
2.2.0,Number of folds for split
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Number of features on conv-mols
2.2.0,Define metric
2.2.0,Train support model on train
2.2.0,Add layers
2.2.0,4-fold splits
2.2.0,num positive/negative ligands
2.2.0,Define metric
2.2.0,Get supports on test-set
2.2.0,Compute accuracies
2.2.0,Train model on support
2.2.0,Test model
2.2.0,Join information for all tasks.
2.2.0,replace with your own scratch directory
2.2.0,Number of conformations in each file increases exponentially.
2.2.0,Start with a smaller dataset before continuing. Use all of them
2.2.0,for production
2.2.0,"'ani_gdb_s03.h5',"
2.2.0,"'ani_gdb_s04.h5',"
2.2.0,"'ani_gdb_s05.h5',"
2.2.0,"'ani_gdb_s06.h5',"
2.2.0,"'ani_gdb_s07.h5',"
2.2.0,'ani_gdb_s08.h5'
2.2.0,Extract the data
2.2.0,Print the data
2.2.0,self-interaction energies taken from
2.2.0,https://github.com/isayev/ANI1_dataset README
2.2.0,flush once more at the end
2.2.0,"# For production, set nb_epoch to 100+"
2.2.0,"print(""Train scores"")"
2.2.0,print(train_scores)
2.2.0,"print(""Minimization of a single test set structure:"")"
2.2.0,"print(model.minimize_structure(coords, atomic_nums))"
2.2.0,Written by Roman Zubatyuk and Justin S. Smith
2.2.0,Modified by Yutong Zhao to make python2 compatible
2.2.0,opening file
2.2.0,print(store_loc)
2.2.0,print(type(v[0]))
2.2.0,print(k)
2.2.0,print(path)
2.2.0,Number of conformations in each file increases exponentially.
2.2.0,Start with a smaller dataset before continuing. Use all of them
2.2.0,for production
2.2.0,Extract the data
2.2.0,NOTE THE RENAMING:
2.2.0,Note sensitivity = recall
2.2.0,Load nci dataset
2.2.0,Featurize nci dataset
2.2.0,Initialize transformers
2.2.0,Set some global variables up top
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load hiv dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load hiv dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Fit trained model
2.2.0,Fit models
2.2.0,Batch size of models
2.2.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.2.0,Fit trained model
2.2.0,Load SIDER dataset
2.2.0,Featurize SIDER dataset
2.2.0,Initialize transformers
2.2.0,Featurize permeability dataset
2.2.0,Load Tox21 dataset
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load SAMPL dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load SAMPL(FreeSolv) dataset
2.2.0,Define metric
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load clintox dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load clintox dataset
2.2.0,Fit models
2.2.0,Do setup required for tf/keras models
2.2.0,Number of features on conv-mols
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,-*- coding: utf-8 -*-
2.2.0,#############################################################################
2.2.0,## save dataset
2.2.0,#############################################################################
2.2.0,## load datasets
2.2.0,load sweetfda
2.2.0,load aact
2.2.0,## fixup smiles for matching
2.2.0,return smiles
2.2.0,map original smiles to converted smiles
2.2.0,"## join dataframes, index on smiles"
2.2.0,map original smiles back
2.2.0,## fill all nan with 0
2.2.0,## construct datasets
2.2.0,store in new datasets
2.2.0,## save datasets
2.2.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.2.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.2.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.2.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.2.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.2.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.2.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.2.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.2.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.2.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.2.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.2.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.2.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.2.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.2.0,For stable runs
2.2.0,Fit trained model
2.2.0,For stable runs
2.2.0,Fit trained model
2.2.0,For stable runs
2.2.0,Fit trained model
2.2.0,transformers = [
2.2.0,"dc.trans.LogTransformer(transform_X=True),"
2.2.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.2.0,dataset=train_dataset)]
2.2.0,Featurize UV dataset
2.2.0,##Load data###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Set numpy seed
2.2.0,##Load data###
2.2.0,##Create model###
2.2.0,Use R2 classification metric
2.2.0,Only use for final evaluation
2.2.0,Force matplotlib to not use any Xwindows backend.
2.2.0,##Load data###
2.2.0,the histogram of the data
2.2.0,##Load data###
2.2.0,###################################################### DEBUG
2.2.0,###################################################### DEBUG
2.2.0,Load HOPV dataset
2.2.0,Fit models
2.2.0,Number of features on conv-mols
2.2.0,Batch size of models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load HOPV dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load HOPV dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load HOPV dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Only for debug!
2.2.0,Load HOPV dataset
2.2.0,Fit models
2.2.0,Fit trained model
2.2.0,Load TOXCAST dataset
2.2.0,Featurize TOXCAST dataset
2.2.0,Initialize transformers
2.2.0,Fit trained model
2.2.0,Processing of ToxCast data
2.2.0,Author - Aneesh Pappu
2.2.0,Loading dataframes and editing indices
2.2.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.2.0,get corresponding casn
2.2.0,get corresponding smiles
2.2.0,write to cell
2.2.0,Tidy up and write to csv
2.2.0,TODO(rbharath): Check that this operation is differentiable.
2.2.0,The number of cells which we should theoretically have
2.2.0,The number of cells which we should theoretically have
2.2.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.2.0,The number of cells which we should theoretically have
2.2.0,TODO(rbharath): The test below only checks that shapes work out.
2.2.0,Need to do a correctness implementation vs. a simple CPU impl.
2.2.0,The number of cells which we should theoretically have
2.2.0,TODO(rbharath): The test below only checks that shapes work out.
2.2.0,Need to do a correctness implementation vs. a simple CPU impl.
2.2.0,The number of cells which we should theoretically have
2.2.0,TODO(rbharath): The test below only checks that shapes work out.
2.2.0,Need to do a correctness implementation vs. a simple CPU impl.
2.2.0,TODO(rbharath): Commenting this out due to weird segfaults
2.2.0,def test_vina_generate_conformers(self):
2.2.0,"""""""Test that Vina Model can generate conformers"""""""
2.2.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.2.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.2.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.2.0,max_protein_atoms = 3500
2.2.0,max_ligand_atoms = 100
2.2.0,"print(""Loading protein file"")"
2.2.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.2.0,protein_Z = pad_array(
2.2.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.2.0,max_protein_atoms)
2.2.0,"print(""Loading ligand file"")"
2.2.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.2.0,ligand_Z = pad_array(
2.2.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.2.0,max_ligand_atoms)
2.2.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.2.0,"Shape (n_cells, k)"
2.2.0,"Shape (N, 1)"
2.2.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.2.0,"conditions, so does wrapround. O(constant)"
2.2.0,"Shape (n_cells, 26)"
2.2.0,"Shape (N, 26)"
2.2.0,"coords of shape (N, ndim)"
2.2.0,"Shape (N, 26, k, ndim)"
2.2.0,"Shape (N, 26, k)"
2.2.0,"Shape (N, 26, k)"
2.2.0,"Shape (N, 26, k, ndim)"
2.2.0,"For smaller systems especially, the periodic boundary conditions can"
2.2.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.2.0,make sure duplicate neighbors are ignored?
2.2.0,TODO(rbharath): How does distance need to be modified here to
2.2.0,account for periodic boundary conditions?
2.2.0,"Shape (N, 26, k)"
2.2.0,"Shape (N, 26*k)"
2.2.0,TODO(rbharath): This will cause an issue with duplicates!
2.2.0,"Shape (N, M)"
2.2.0,"N elts of size (M,) each"
2.2.0,"Shape (N, 26*k)"
2.2.0,"N elts of size (26*k,) each"
2.2.0,"N elts of size (M,) each"
2.2.0,"Shape (N, M)"
2.2.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.2.0,"N tensors of shape (n_cells, 1)"
2.2.0,"Shape (N*n_cells, 1) after tile"
2.2.0,"List of N tensors of shape (n_cells, 1)"
2.2.0,Lists of length N
2.2.0,Lists of length n_cells
2.2.0,Get indices of k atoms closest to each cell point
2.2.0,TODO(rbharath): tf.stack for tf 1.0
2.2.0,"Tensor of shape (n_cells, k, ndim)"
2.2.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.2.0,"Tensor of shape (26, k, ndim)"
2.2.0,"Reshape to (26*k, ndim)"
2.2.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.2.0,"Dists of shape (26*k, 1)"
2.2.0,"Of shape (k, ndim)"
2.2.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.2.0,TODO(rbharath): Change this for tf 1.0
2.2.0,"n_cells tensors of shape (N, 1)"
2.2.0,"Shape (N*n_cells, 1) after tile"
2.2.0,"List of n_cells tensors of shape (N, 1)"
2.2.0,Lists of length n_cells
2.2.0,Lists of length n_cells
2.2.0,Get indices of k atoms closest to each cell point
2.2.0,"n_cells tensors of shape (k, ndim)"
2.2.0,"Tensor of shape (n_cells, k)"
2.2.0,TODO(rbharath):
2.2.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.2.0,- Need to group closest atoms amongst cell neighbors
2.2.0,- Need to do another top_k to find indices of closest neighbors.
2.2.0,- Return N lists corresponding to neighbors for every atom.
2.2.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.2.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.2.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.2.0,the cube.
2.2.0,Number of neighbors of central cube in 3-space is
2.2.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.2.0,TODO(rbharath)
2.2.0,n_cells = int(cells.get_shape()[0])
2.2.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.2.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.2.0,"Tile (a, a, a, b, b, b, etc.)"
2.2.0,"Tile (a, b, c, a, b, c, ...)"
2.2.0,"Lists of n_cells tensors of shape (N, 1)"
2.2.0,Lists of length n_cells
2.2.0,Lists of length n_cells
2.2.0,Get indices of k atoms closest to each cell point
2.2.0,"n_cells tensors of shape (26,)"
2.2.0,TODO(rbharath): Make this handle minibatches
2.2.0,"Shape (N_protein+N_ligand, 3)"
2.2.0,"Shape (N_protein+N_ligand,)"
2.2.0,"Shape (N_protein+N_ligand, M)"
2.2.0,"Shape (N_protein+N_ligand,)"
2.2.0,"Shape (N_protein+N_ligand, 3)"
2.2.0,"Shape (N_protein+N_ligand,)"
2.2.0,"Shape (N_protein+N_ligand, M)"
2.2.0,"Shape (N_protein+N_ligand, M, 3)"
2.2.0,"Shape (N_protein+N_ligand, M)"
2.2.0,"Shape (N_protein+N_ligand, M, 3)"
2.2.0,"Shape (N_protein+N_ligand, M)"
2.2.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.2.0,"Shape (N_protein+N_ligand, M)"
2.2.0,"Shape (N_protein+N_ligand, M)"
2.2.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.2.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.2.0,computing free-energy. This implementation currently uses all interaction
2.2.0,terms. Not sure if this makes a difference.
2.2.0,"Shape (N_protein+N_ligand, M)"
2.2.0,Shape () -- scalar
2.2.0,Keep track of the layers
2.2.0,"For graphical layers, add connectivity placeholders"
2.2.0,Add layer to the layer list
2.2.0,Keep track of the layers
2.2.0,Create graph topology and x
2.2.0,Keep track of the layers
2.2.0,Whether or not we have used the GraphGather layer yet
2.2.0,Update new value of x
2.2.0,Update new value of x
2.2.0,Update new value of x
2.2.0,Get train function
2.2.0,Initialize
2.2.0,################################################################### DEBUG
2.2.0,self.test_label_placeholder = Input(
2.2.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.2.0,"name=""label_placeholder""))"
2.2.0,self.test_weight_placeholder = Input(
2.2.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.2.0,"name=""weight_placeholder""))"
2.2.0,TODO(rbharath): Should weights for the support be used?
2.2.0,Support labels
2.2.0,self.support_label_placeholder = Input(
2.2.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.2.0,"name=""support_label_placeholder""))"
2.2.0,################################################################### DEBUG
2.2.0,Generate dictionary elements for support
2.2.0,Get graph information for test
2.2.0,Generate dictionary elements for test
2.2.0,Perform the optimization
2.2.0,Create different support sets
2.2.0,Get batch to try it out on
2.2.0,"Train on support set, batch pair"
2.2.0,Get featurization for test
2.2.0,"Shape (n_test, n_feat)"
2.2.0,Get featurization for support
2.2.0,"Shape (n_support, n_feat)"
2.2.0,Computes the inner part c() of the kernel
2.2.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.2.0,Normalize
2.2.0,TODO(rbharath): euclidean kernel is broken!
2.2.0,elif self.similarity == 'euclidean':
2.2.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.2.0,"Note that gram matrix g has shape (n_test, n_support)"
2.2.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.2.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.2.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.2.0,each test entry) to get attention vector
2.2.0,"Shape (n_test, n_support)"
2.2.0,Weighted sum of support labels
2.2.0,"Shape (n_support, 1)"
2.2.0,pred is yhat in eqn (1) of Matching Networks.
2.2.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.2.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.2.0,"Shape (n_test,)"
2.2.0,Convert to logit space using inverse sigmoid (logit) function
2.2.0,logit function: log(pred) - log(1-pred)
2.2.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.2.0,in Cross Entropy calculation.
2.2.0,"Shape (n_test,)"
2.2.0,Get scores
2.2.0,Remove padded elements
2.2.0,Get scores
2.2.0,pred corresponds to prob(example == 1)
2.2.0,Remove padded elements
2.2.0,Get batches
2.2.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.2.0,multitask case with missing data...
2.2.0,Join information for all tasks.
2.2.0,TODO(rbharath): Find a way to get rid of this import?
2.2.0,Extract model info
2.2.0,Get graph topology for x
2.2.0,Building outputs
2.2.0,Set epsilon
2.2.0,Initialize
2.2.0,"Path to save checkpoint files, which matches the"
2.2.0,replicated supervisor's default path.
2.2.0,Create target inputs
2.2.0,Get train function
2.2.0,TODO(rbharath): I believe this is total amount of data
2.2.0,Get graph information
2.2.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.2.0,the number of labeled data points in target_i. This is to normalize each task
2.2.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.2.0,Get other optimizer information
2.2.0,TODO(rbharath): Figure out how to handle phase appropriately
2.2.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.2.0,"tensors of shape (batch_size,)"
2.2.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.2.0,examples (effect averages out)
2.2.0,Perform the optimization
2.2.0,TODO(rbharath): Disabling saving for now to try to debug.
2.2.0,run eval data through the model
2.2.0,"Shape (n_samples, n_tasks)"
2.2.0,Create target inputs
2.2.0,TODO(rbharath): Find a way to get rid of this import?
2.2.0,Obtain appropriate loss function
2.2.0,Extract model info
2.2.0,Get graph topology for x
2.2.0,Raw logit outputs
2.2.0,Set epsilon
2.2.0,Initialize
2.2.0,"Path to save checkpoint files, which matches the"
2.2.0,replicated supervisor's default path.
2.2.0,Create target inputs
2.2.0,############################################################### DEBUG
2.2.0,"print(""multitask classifier"")"
2.2.0,"print(""feat"")"
2.2.0,print(feat)
2.2.0,############################################################### DEBUG
2.2.0,Get train function
2.2.0,TODO(rbharath): I believe this is total amount of data
2.2.0,Get graph information
2.2.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.2.0,the number of labeled data points in target_i. This is to normalize each task
2.2.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.2.0,Get other optimizer information
2.2.0,TODO(rbharath): Figure out how to handle phase appropriately
2.2.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.2.0,"tensors of shape (batch_size,)"
2.2.0,Convert the labels into one-hot vector encodings.
2.2.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.2.0,un-softmaxed logits rather than softmax outputs.
2.2.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.2.0,examples (effect averages out)
2.2.0,Perform the optimization
2.2.0,TODO(rbharath): Disabling saving for now to try to debug.
2.2.0,run eval data through the model
2.2.0,"Shape (n_samples, n_tasks)"
2.2.0,run eval data through the model
2.2.0,self.n_atoms = n_atoms
2.2.0,Define the list of tensors to be used as topology
2.2.0,Merge mol conv objects
2.2.0,Generate dicts
2.2.0,Define the list of tensors to be used as topology
2.2.0,Extract atom numbers
2.2.0,Generate dicts
2.2.0,molecule * atom(graph) => step => features
2.2.0,molecule * atom(graph) => step
2.2.0,molecule * atom(graph) => step
2.2.0,Define the list of tensors to be used as topology
2.2.0,calculation orders for a batch of molecules
2.2.0,padding atom features vector of each molecule with 0
2.2.0,self.n_atoms = n_atoms
2.2.0,Define the list of tensors to be used as topology
2.2.0,Extract atom numbers
2.2.0,Generate dicts
2.2.0,self.n_atoms = n_atoms
2.2.0,Define the list of tensors to be used as topology
2.2.0,Extract atom numbers
2.2.0,number of atoms in each molecule
2.2.0,index of pair features
2.2.0,number of pairs for each atom
2.2.0,atom features
2.2.0,pair features
2.2.0,Generate dicts
2.2.0,# Gather Projection
2.2.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.2.0,There should be 8 layers in graph_model
2.2.0,assert len(graph_model.layers) == 6
2.2.0,Add layers
2.2.0,Need to add batch-norm separately to test/support due to differing
2.2.0,shapes.
2.2.0,Apply an attention lstm layer
2.2.0,Gather Projection
2.2.0,Add layers
2.2.0,Need to add batch-norm separately to test/support due to differing
2.2.0,shapes.
2.2.0,Apply an attention lstm layer
2.2.0,Gather Projection
2.2.0,Degrees from 1 to max_deg inclusive
2.2.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.2.0,"Should have shape (?, deg)"
2.2.0,"Shape of atom_features should be (?, n_feat)"
2.2.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.2.0,-*- coding: utf-8 -*-
2.2.0,Save hyperparameters
2.2.0,-*- coding: utf-8 -*-
2.2.0,Save hyperparameters
2.2.0,setup optimizer
2.2.0,setup optimizer
2.2.0,"print(""tasK: %d"" %task)"
2.2.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.2.0,"print(""scores"")"
2.2.0,print(scores.size())
2.2.0,"print(""task_label"")"
2.2.0,print(task_label.size())
2.2.0,"task_loss =  self.criterion(scores, task_label)"
2.2.0,"print(""task_loss"")"
2.2.0,print(task_loss.size())
2.2.0,-*- coding: utf-8 -*-
2.2.0,Save hyperparameters
2.2.0,weight decay
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,Turns out there are valid cases where we don't want pad-batches
2.2.0,on by default.
2.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.2.0,Run training op.
2.2.0,############################################################# TIMING
2.2.0,The iterbatches does padding with zero-weight examples on the last batch.
2.2.0,Remove padded examples.
2.2.0,Special case to handle singletasks.
2.2.0,The iterbatches does padding with zero-weight examples on the last batch.
2.2.0,Remove padded examples.
2.2.0,References
2.2.0,Arguments
2.2.0,Aliases.
2.2.0,Aliases.
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,TODO(rbharath): This class does not yet have a
2.2.0,"TensorGraph equivalent, but one may not be required."
2.2.0,"Commented out for now, remove if OK."
2.2.0,class AlternateWeaveLayer(WeaveLayer):
2.2.0,""""""" Alternate implementation of weave module"
2.2.0,"same variables, different graph structures"
2.2.0,""""""""
2.2.0,
2.2.0,"def call(self, x, mask=None):"
2.2.0,"""""""Execute this layer on input tensors."
2.2.0,
2.2.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.2.0,
2.2.0,Parameters
2.2.0,----------
2.2.0,x: list
2.2.0,list of Tensors of form described above.
2.2.0,"mask: bool, optional"
2.2.0,Ignored. Present only to shadow superclass call() method.
2.2.0,
2.2.0,Returns
2.2.0,-------
2.2.0,A: Tensor
2.2.0,Tensor of atom_features
2.2.0,P: Tensor
2.2.0,Tensor of pair_features
2.2.0,""""""""
2.2.0,# Add trainable weights
2.2.0,self.build()
2.2.0,
2.2.0,atom_features = x[0]
2.2.0,pair_features = x[1]
2.2.0,
2.2.0,pair_split = x[2]
2.2.0,atom_to_pair = x[4]
2.2.0,
2.2.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.2.0,AA = self.activation(AA)
2.2.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.2.0,PA = self.activation(PA)
2.2.0,"PA = tf.segment_sum(PA, pair_split)"
2.2.0,
2.2.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.2.0,A = self.activation(A)
2.2.0,
2.2.0,if self.update_pair:
2.2.0,AP_ij = tf.matmul(
2.2.0,tf.reshape(
2.2.0,"tf.gather(atom_features, atom_to_pair),"
2.2.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.2.0,AP_ij = self.activation(AP_ij)
2.2.0,AP_ji = tf.matmul(
2.2.0,tf.reshape(
2.2.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.2.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.2.0,AP_ji = self.activation(AP_ji)
2.2.0,
2.2.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.2.0,PP = self.activation(PP)
2.2.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.2.0,P = self.activation(P)
2.2.0,else:
2.2.0,P = pair_features
2.2.0,
2.2.0,"return A, P"
2.2.0,TODO(rbharath): This class does not yet have a
2.2.0,"TensorGraph equivalent, but one may not be required."
2.2.0,"Commented out for now, remove if OK."
2.2.0,class WeaveConcat(Layer):
2.2.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.2.0,""""""""
2.2.0,
2.2.0,"def __init__(self,"
2.2.0,"batch_size,"
2.2.0,"n_atom_input_feat=50,"
2.2.0,"n_output=128,"
2.2.0,"init='glorot_uniform',"
2.2.0,"activation='tanh',"
2.2.0,**kwargs):
2.2.0,""""""""
2.2.0,Parameters
2.2.0,----------
2.2.0,batch_size: int
2.2.0,number of molecules in a batch
2.2.0,"n_atom_input_feat: int, optional"
2.2.0,Number of features for each atom in input.
2.2.0,"n_output: int, optional"
2.2.0,Number of output features for each atom(concatenated)
2.2.0,"init: str, optional"
2.2.0,Weight initialization for filters.
2.2.0,"activation: str, optional"
2.2.0,Activation function applied
2.2.0,
2.2.0,""""""""
2.2.0,self.batch_size = batch_size
2.2.0,self.n_atom_input_feat = n_atom_input_feat
2.2.0,self.n_output = n_output
2.2.0,self.init = initializations.get(init)  # Set weight initialization
2.2.0,self.activation = activations.get(activation)  # Get activations
2.2.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.2.0,
2.2.0,def build(self):
2.2.0,"""""""""Construct internal trainable weights."
2.2.0,""""""""
2.2.0,
2.2.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.2.0,self.b = model_ops.zeros(shape=[
2.2.0,"self.n_output,"
2.2.0,])
2.2.0,
2.2.0,self.trainable_weights = self.W + self.b
2.2.0,
2.2.0,"def call(self, x, mask=None):"
2.2.0,"""""""Execute this layer on input tensors."
2.2.0,
2.2.0,"x = [atom_features, atom_mask]"
2.2.0,
2.2.0,Parameters
2.2.0,----------
2.2.0,x: list
2.2.0,Tensors as listed above
2.2.0,"mask: bool, optional"
2.2.0,Ignored. Present only to shadow superclass call() method.
2.2.0,
2.2.0,Returns
2.2.0,-------
2.2.0,outputs: Tensor
2.2.0,Tensor of concatenated atom features
2.2.0,""""""""
2.2.0,self.build()
2.2.0,atom_features = x[0]
2.2.0,atom_masks = x[1]
2.2.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.2.0,A_mask = tf.split(
2.2.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.2.0,outputs = tf.concat(
2.2.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.2.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.2.0,outputs = self.activation(outputs)
2.2.0,return outputs
2.2.0,TODO(rbharath): This class does not yet have a
2.2.0,"TensorGraph equivalent, but one may not be required."
2.2.0,"Commented out for now, remove if OK."
2.2.0,class AlternateWeaveGather(WeaveGather):
2.2.0,"""""""Alternate implementation of weave gather layer"
2.2.0,corresponding to AlternateWeaveLayer
2.2.0,""""""""
2.2.0,
2.2.0,"def call(self, x, mask=None):"
2.2.0,"""""""Execute this layer on input tensors."
2.2.0,
2.2.0,"x = [atom_features, atom_split]"
2.2.0,
2.2.0,Parameters
2.2.0,----------
2.2.0,x: list
2.2.0,Tensors as listed above
2.2.0,"mask: bool, optional"
2.2.0,Ignored. Present only to shadow superclass call() method.
2.2.0,
2.2.0,Returns
2.2.0,-------
2.2.0,outputs: Tensor
2.2.0,Tensor of molecular features
2.2.0,""""""""
2.2.0,# Add trainable weights
2.2.0,self.build()
2.2.0,outputs = x[0]
2.2.0,atom_split = x[1]
2.2.0,
2.2.0,if self.gaussian_expand:
2.2.0,outputs = self.gaussian_histogram(outputs)
2.2.0,
2.2.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.2.0,
2.2.0,if self.gaussian_expand:
2.2.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.2.0,output_molecules = self.activation(output_molecules)
2.2.0,return output_molecules
2.2.0,Each directory holds a range of assay results
2.2.0,Just write NA
2.2.0,"Now, write out the results csv, going line by line through all molecule results"
2.2.0,printing the mol_id
2.2.0,printing the SMILES
2.2.0,Now gzip it
2.2.0,Now remove the intermediate csv
2.2.0,First download all SDF files. We need these to get smiles
2.2.0,Next download all Bioassays
2.2.0,RDKit consistently hangs when trying to read this file
2.2.0,TODO (LESWING) Lazy Load
2.2.0,TODO (LESWING) Lazy Load
2.2.0,from simdna import simulations
2.2.0,define layer out functions
2.2.0,get layer outputs for a positive simulation example
2.2.0,plot layer outputs
2.2.0,highlight motif sites
2.2.0,get a positive and a negative example from the simulation data
2.2.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.2.0,get motif site locations
2.2.0,organize legends
2.2.0,plot scores and highlight motif site locations
2.2.0,initialize fwd and reverse scores to -infinity
2.2.0,"cross-correlate separately for each base,"
2.2.0,for both the PSSM and its reverse complement
2.2.0,sum over the bases
2.2.0,take max of fwd and reverse scores at each position
2.2.0,return 1D view of sequence characters
2.2.0,class SequenceDNN(Model):
2.2.0,""""""""
2.2.0,Sequence DNN models.
2.2.0,
2.2.0,Parameters
2.2.0,----------
2.2.0,"seq_length : int, optional"
2.2.0,length of input sequence.
2.2.0,"keras_model : instance of keras.models.Sequential, optional"
2.2.0,seq_length or keras_model must be specified.
2.2.0,"num_tasks : int, optional"
2.2.0,number of tasks. Default: 1.
2.2.0,num_filters : list[int] | tuple[int]
2.2.0,"number of convolutional filters in each layer. Default: (15,)."
2.2.0,conv_width : list[int] | tuple[int]
2.2.0,"width of each layer's convolutional filters. Default: (15,)."
2.2.0,pool_width : int
2.2.0,width of max pooling after the last layer. Default: 35.
2.2.0,L1 : float
2.2.0,strength of L1 penalty.
2.2.0,dropout : float
2.2.0,dropout probability in every convolutional layer. Default: 0.
2.2.0,verbose: int
2.2.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.2.0,
2.2.0,Returns
2.2.0,-------
2.2.0,Compiled DNN model.
2.2.0,""""""""
2.2.0,
2.2.0,"def __init__(self,"
2.2.0,"seq_length=None,"
2.2.0,"keras_model=None,"
2.2.0,"use_RNN=False,"
2.2.0,"num_tasks=1,"
2.2.0,"num_filters=(15, 15, 15),"
2.2.0,"conv_width=(15, 15, 15),"
2.2.0,"pool_width=35,"
2.2.0,"GRU_size=35,"
2.2.0,"TDD_size=15,"
2.2.0,"L1=0,"
2.2.0,"dropout=0.0,"
2.2.0,"num_epochs=100,"
2.2.0,verbose=1):
2.2.0,self.num_tasks = num_tasks
2.2.0,self.num_epochs = num_epochs
2.2.0,self.verbose = verbose
2.2.0,self.train_metrics = []
2.2.0,self.valid_metrics = []
2.2.0,if keras_model is not None and seq_length is None:
2.2.0,self.model = keras_model
2.2.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.2.0,elif seq_length is not None and keras_model is None:
2.2.0,self.model = Sequential()
2.2.0,assert len(num_filters) == len(conv_width)
2.2.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.2.0,conv_height = 4 if i == 0 else 1
2.2.0,self.model.add(
2.2.0,Convolution2D(
2.2.0,"nb_filter=nb_filter,"
2.2.0,"nb_row=conv_height,"
2.2.0,"nb_col=nb_col,"
2.2.0,"activation='linear',"
2.2.0,"init='he_normal',"
2.2.0,"input_shape=(1, 4, seq_length),"
2.2.0,"W_regularizer=l1(L1),"
2.2.0,b_regularizer=l1(L1)))
2.2.0,self.model.add(Activation('relu'))
2.2.0,self.model.add(Dropout(dropout))
2.2.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.2.0,if use_RNN:
2.2.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.2.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.2.0,"self.model.add(Permute((2, 1)))"
2.2.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.2.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.2.0,self.model.add(Flatten())
2.2.0,self.model.add(Dense(output_dim=self.num_tasks))
2.2.0,self.model.add(Activation('sigmoid'))
2.2.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.2.0,else:
2.2.0,raise ValueError(
2.2.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.2.0,
2.2.0,"def train(self,"
2.2.0,"X,"
2.2.0,"y,"
2.2.0,"validation_data,"
2.2.0,"early_stopping_metric='Loss',"
2.2.0,"early_stopping_patience=5,"
2.2.0,save_best_model_to_prefix=None):
2.2.0,if y.dtype != bool:
2.2.0,"assert set(np.unique(y)) == {0, 1}"
2.2.0,y = y.astype(bool)
2.2.0,multitask = y.shape[1] > 1
2.2.0,if not multitask:
2.2.0,num_positives = y.sum()
2.2.0,num_sequences = len(y)
2.2.0,num_negatives = num_sequences - num_positives
2.2.0,if self.verbose >= 1:
2.2.0,print('Training model (* indicates new best result)...')
2.2.0,"X_valid, y_valid = validation_data"
2.2.0,early_stopping_wait = 0
2.2.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.2.0,"for epoch in range(1, self.num_epochs + 1):"
2.2.0,self.model.fit(
2.2.0,"X,"
2.2.0,"y,"
2.2.0,"batch_size=128,"
2.2.0,"nb_epoch=1,"
2.2.0,class_weight={
2.2.0,"True: num_sequences / num_positives,"
2.2.0,False: num_sequences / num_negatives
2.2.0,"} if not multitask else None,"
2.2.0,verbose=self.verbose >= 2)
2.2.0,"epoch_train_metrics = self.test(X, y)"
2.2.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.2.0,self.train_metrics.append(epoch_train_metrics)
2.2.0,self.valid_metrics.append(epoch_valid_metrics)
2.2.0,if self.verbose >= 1:
2.2.0,print('Epoch {}:'.format(epoch))
2.2.0,print('Train {}'.format(epoch_train_metrics))
2.2.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.2.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.2.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.2.0,if self.verbose >= 1:
2.2.0,print(' *')
2.2.0,best_metric = current_metric
2.2.0,best_epoch = epoch
2.2.0,early_stopping_wait = 0
2.2.0,if save_best_model_to_prefix is not None:
2.2.0,self.save(save_best_model_to_prefix)
2.2.0,else:
2.2.0,if self.verbose >= 1:
2.2.0,print()
2.2.0,if early_stopping_wait >= early_stopping_patience:
2.2.0,break
2.2.0,early_stopping_wait += 1
2.2.0,if self.verbose >= 1:
2.2.0,print('Finished training after {} epochs.'.format(epoch))
2.2.0,if save_best_model_to_prefix is not None:
2.2.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.2.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.2.0,"best_epoch, save_best_model_to_prefix))"
2.2.0,
2.2.0,"def predict(self, X):"
2.2.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.2.0,
2.2.0,def get_sequence_filters(self):
2.2.0,""""""""
2.2.0,Returns 3D array of 2D sequence filters.
2.2.0,""""""""
2.2.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.2.0,
2.2.0,"def deeplift(self, X, batch_size=200):"
2.2.0,""""""""
2.2.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.2.0,""""""""
2.2.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.2.0,from deeplift.conversion import keras_conversion as kc
2.2.0,
2.2.0,# convert to deeplift model and get scoring function
2.2.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.2.0,score_func = deeplift_model.get_target_contribs_func(
2.2.0,find_scores_layer_idx=0)
2.2.0,# use a 40% GC reference
2.2.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.2.0,# get deeplift scores
2.2.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.2.0,for i in range(self.num_tasks):
2.2.0,deeplift_scores[i] = score_func(
2.2.0,"task_idx=i,"
2.2.0,"input_data_list=[X],"
2.2.0,"batch_size=batch_size,"
2.2.0,"progress_update=None,"
2.2.0,input_references_list=input_references)
2.2.0,return deeplift_scores
2.2.0,
2.2.0,"def in_silico_mutagenesis(self, X):"
2.2.0,""""""""
2.2.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.2.0,""""""""
2.2.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.2.0,wild_type_predictions = self.predict(X)
2.2.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.2.0,np.newaxis]
2.2.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.2.0,"zip(X, wild_type_predictions)):"
2.2.0,mutated_sequences = np.repeat(
2.2.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.2.0,# remove wild-type
2.2.0,arange = np.arange(len(mutated_sequences))
2.2.0,horizontal_cycle = np.tile(
2.2.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.2.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.2.0,# add mutant
2.2.0,vertical_repeat = np.repeat(
2.2.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.2.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.2.0,# make mutant predictions
2.2.0,mutated_predictions = self.predict(mutated_sequences)
2.2.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.2.0,"(self.num_tasks,))"
2.2.0,mutagenesis_scores[
2.2.0,sequence_index] = wild_type_prediction - mutated_predictions
2.2.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.2.0,
2.2.0,@staticmethod
2.2.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.2.0,from dragonn.plot import plot_bases_on_ax
2.2.0,scores = score_func(X).squeeze(
2.2.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.2.0,try:
2.2.0,os.makedirs(output_directory)
2.2.0,except OSError:
2.2.0,pass
2.2.0,num_tasks = len(scores)
2.2.0,"for task_index, task_scores in enumerate(scores):"
2.2.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.2.0,# sequence_scores is num_bases x sequence_length
2.2.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.2.0,plt.clf()
2.2.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.2.0,top_axis.plot(
2.2.0,"range(1,"
2.2.0,"len(basewise_max_sequence_scores) + 1),"
2.2.0,basewise_max_sequence_scores)
2.2.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.2.0,peak_position = basewise_max_sequence_scores.argmax()
2.2.0,top_axis.axvspan(
2.2.0,"peak_position - peak_width,"
2.2.0,"peak_position + peak_width,"
2.2.0,"color='grey',"
2.2.0,alpha=0.1)
2.2.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.2.0,peak_position + peak_width].T
2.2.0,# Set non-max letter_heights to zero
2.2.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.2.0,"letter_heights[np.arange(len(letter_heights)),"
2.2.0,peak_sequence_scores.argmax(axis=1)] = \
2.2.0,basewise_max_sequence_scores[peak_position - peak_width :
2.2.0,peak_position + peak_width]
2.2.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.2.0,bottom_axis.set_xticklabels(
2.2.0,tuple(
2.2.0,"map(str,"
2.2.0,"np.arange(peak_position - peak_width,"
2.2.0,peak_position + peak_width + 1))))
2.2.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.2.0,plt.xlabel('Position')
2.2.0,plt.ylabel('Score')
2.2.0,plt.savefig(
2.2.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.2.0,"sequence_index, '_task_{}'.format(task_index)"
2.2.0,if num_tasks > 1 else '')))
2.2.0,plt.close()
2.2.0,
2.2.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.2.0,self._plot_scores(
2.2.0,"X,"
2.2.0,"output_directory,"
2.2.0,"peak_width,"
2.2.0,"score_func=self.deeplift,"
2.2.0,score_name='DeepLift')
2.2.0,
2.2.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.2.0,self._plot_scores(
2.2.0,"X,"
2.2.0,"output_directory,"
2.2.0,"peak_width,"
2.2.0,"score_func=self.in_silico_mutagenesis,"
2.2.0,score_name='ISM')
2.2.0,
2.2.0,"def plot_architecture(self, output_file):"
2.2.0,from dragonn.visualize_util import plot as plot_keras_model
2.2.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.2.0,
2.2.0,"def save(self, save_best_model_to_prefix):"
2.2.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.2.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.2.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.2.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.2.0,
2.2.0,@staticmethod
2.2.0,"def load(arch_fname, weights_fname=None):"
2.2.0,model_json_string = open(arch_fname).read()
2.2.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.2.0,if weights_fname is not None:
2.2.0,sequence_dnn.model.load_weights(weights_fname)
2.2.0,return sequence_dnn
2.2.0,create temporary fasta files
2.2.0,run command
2.2.0,remove fasta files
2.2.0,write test fasta file
2.2.0,test gkmsvm
2.2.0,get classification results
2.2.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.2.0,"Using canonical smiles for glycine, as in original research paper"
2.2.0,Atom features with padding
2.2.0,A_tilda_k computation
2.2.0,Final feed_dict setup
2.2.0,"assert val.shape == (self.batch_size, self.max_nodes, self.max_nodes)"
2.2.0,"assert atom_features.shape == (self.batch_size, self.max_nodes,"
2.2.0,self.num_node_features)
2.2.0,Fit models
2.2.0,Args
2.2.0,2017 DeepCrystal Technologies - Patrick Hop
2.2.0,
2.2.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.2.0,
2.2.0,MIT License - have fun!!
2.2.0,===========================================================
2.2.0,x = F.selu( fc(x) )
2.2.0,x = F.selu( fc(x) )
2.2.0,2017 DeepCrystal Technologies - Patrick Hop
2.2.0,
2.2.0,Data loading a splitting file
2.2.0,
2.2.0,MIT License - have fun!!
2.2.0,===========================================================
2.2.0,Args
2.2.0,TODO (VIGS25): Account for the reload option
2.2.0,Downloading train files
2.2.0,Parsing training data
2.2.0,"Pick only sequences from humans, belong to specific MHC allele and having given seq_len"
2.2.0,Test Files loading
2.2.0,One Hot Featurization
2.2.0,Consistency check
2.2.0,Handle output layer
2.2.0,Iterate over all previous tasks.
2.2.0,prev_layers is a list with elements of size
2.2.0,"(batch_size, layer_sizes[i-1])"
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,Save an initial checkpoint.
2.2.0,Turns out there are valid cases where we don't want pad-batches
2.2.0,on by default.
2.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.2.0,Run training op.
2.2.0,Always save a final checkpoint when complete.
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,Note that we divide by the batch size and not the number of
2.2.0,"non-zero weight examples in the batch.  Also, instead of using"
2.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.2.0,calculate with div/sum so it stays on the GPU.
2.2.0,aggregated costs
2.2.0,weight decay
2.2.0,Dummy placeholders
2.2.0,Dummy placeholders
2.2.0,run eval data through the model
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,Handle edge case when batch-size is 1.
2.2.0,Prune away any padding that was added
2.2.0,allow_soft_placement=True allows ops without a GPU implementation
2.2.0,to run on the CPU instead.
2.2.0,!/usr/bin/python
2.2.0,
2.2.0,Copyright 2015 Google Inc.
2.2.0,
2.2.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.2.0,you may not use this file except in compliance with the License.
2.2.0,You may obtain a copy of the License at
2.2.0,
2.2.0,http://www.apache.org/licenses/LICENSE-2.0
2.2.0,
2.2.0,"Unless required by applicable law or agreed to in writing, software"
2.2.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.2.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.2.0,See the License for the specific language governing permissions and
2.2.0,limitations under the License.
2.2.0,parse CheckpointState proto
2.2.0,parse path to actual checkpoint
2.2.0,the provided mask has to be the same shape as features
2.2.0,test k = 1..4
2.2.0,central moments
2.2.0,standardized moments
2.2.0,central across one axis
2.2.0,standardized across one axis
2.2.0,Fit just on task zero
2.2.0,Notice that we keep the session open
2.2.0,Fit on task one
2.2.0,The predictions for task zero should not change after training
2.2.0,on task one.
2.2.0,following lines added to run train_and_evaluate function of deepchem which is compatible for distributed training
2.2.0,!/usr/bin/python
2.2.0,
2.2.0,Copyright 2015 Google Inc.
2.2.0,
2.2.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.2.0,you may not use this file except in compliance with the License.
2.2.0,You may obtain a copy of the License at
2.2.0,
2.2.0,http://www.apache.org/licenses/LICENSE-2.0
2.2.0,
2.2.0,"Unless required by applicable law or agreed to in writing, software"
2.2.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.2.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.2.0,See the License for the specific language governing permissions and
2.2.0,limitations under the License.
2.2.0,get the divisor
2.2.0,compute the requested central moment
2.2.0,"note that mean is a raw moment, not a central moment"
2.2.0,TODO(user): median is not implemented yet in TensorFlow
2.2.0,Add the input features.
2.2.0,"layer has shape [None, layer_sizes[i]]"
2.2.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.2.0,TODO(rbharath): Might want to make it feasible to have multiple
2.2.0,bypass layers.
2.2.0,Construct task bypass layer
2.2.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.2.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.2.0,"layer has shape [None, layer_sizes[i]]"
2.2.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.2.0,TODO(rbharath): Might want to make it feasible to have multiple
2.2.0,bypass layers.
2.2.0,Construct task bypass layer
2.2.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.2.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.2.0,Consistency check
2.2.0,Lazily created by _get_shared_session().
2.2.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.2.0,when subclass-overridden methods use the same scopes.
2.2.0,Setup graph
2.2.0,Create placeholders
2.2.0,Handle output layer
2.2.0,Iterate over all previous tasks.
2.2.0,prev_layers is a list with elements of size
2.2.0,"(batch_size, layer_sizes[i-1])"
2.2.0,Note that we divide by the batch size and not the number of
2.2.0,"non-zero weight examples in the batch.  Also, instead of using"
2.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.2.0,calculate with div/sum so it stays on the GPU.
2.2.0,aggregated costs
2.2.0,weight decay
2.2.0,Dummy placeholders
2.2.0,Dummy placeholders
2.2.0,run eval data through the model
2.2.0,"Shape (n_tasks, n__samples)"
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,Handle edge case when batch-size is 1.
2.2.0,with self._get_shared_session(train=True) as sess:
2.2.0,Save an initial checkpoint.
2.2.0,Always save a final checkpoint when complete.
2.2.0,Note that we divide by the batch size and not the number of
2.2.0,"non-zero weight examples in the batch.  Also, instead of using"
2.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.2.0,calculate with div/sum so it stays on the GPU.
2.2.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.2.0,Dummy placeholders
2.2.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.2.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.2.0,Dummy placeholders
2.2.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.2.0,allow_soft_placement=True allows ops without a GPU implementation
2.2.0,to run on the CPU instead.
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,Turns out there are valid cases where we don't want pad-batches
2.2.0,on by default.
2.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.2.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.2.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,"(n_samples, n_classes)"
2.2.0,"(n_samples, n_tasks, n_classes)"
2.2.0,Save hyperparameters
2.2.0,Guard variable to make sure we don't Restore() this model
2.2.0,from a disk checkpoint more than once.
2.2.0,"Path to save checkpoint files, which matches the"
2.2.0,replicated supervisor's default path.
2.2.0,Lazily created by _get_shared_session().
2.2.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.2.0,when subclass-overridden methods use the same scopes.
2.2.0,Setup graph
2.2.0,Note that we divide by the batch size and not the number of
2.2.0,"non-zero weight examples in the batch.  Also, instead of using"
2.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.2.0,calculate with div/sum so it stays on the GPU.
2.2.0,aggregated costs
2.2.0,weight decay
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,Save an initial checkpoint.
2.2.0,Define the code that runs on a separate thread to feed data into the queue.
2.2.0,Main training loop.
2.2.0,Run training op.
2.2.0,We have reached the end of an epoch.
2.2.0,We have reached the end of the data.
2.2.0,Always save a final checkpoint when complete.
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,allow_soft_placement=True allows ops without a GPU implementation
2.2.0,to run on the CPU instead.
2.2.0,gpu memory growth option
2.2.0,gpu memory growth option
2.2.0,TODO(rbharath): Is setting train=False right here?
2.2.0,Discard any padded predictions
2.2.0,The iterbatches does padding with zero-weight examples on the last batch.
2.2.0,Remove padded examples.
2.2.0,Special case to handle singletasks.
2.2.0,The iterbatches does padding with zero-weight examples on the last batch.
2.2.0,Remove padded examples.
2.2.0,TODO(rbharath): Verify this can be safely removed.
2.2.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.2.0,""""""""
2.2.0,Evaluates the performance of this model on specified dataset.
2.2.0,
2.2.0,Parameters
2.2.0,----------
2.2.0,dataset: dc.data.Dataset
2.2.0,Dataset object.
2.2.0,metric: deepchem.metrics.Metric
2.2.0,Evaluation metric
2.2.0,transformers: list
2.2.0,List of deepchem.transformers.Transformer
2.2.0,Returns
2.2.0,-------
2.2.0,dict
2.2.0,Maps tasks to scores under metric.
2.2.0,""""""""
2.2.0,"evaluator = Evaluator(self, dataset, transformers)"
2.2.0,scores = evaluator.compute_model_performance(metrics)
2.2.0,return scores
2.2.0,checkpoints look like model_dir/model.ckpt-N
2.2.0,"self._save_path is ""model_dir/model.ckpt"""
2.2.0,run eval data through the model
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,run eval data through the model
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,Note that softmax is already applied in construct_grpah
2.2.0,run eval data through the model
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,Handle edge case when batch-size is 1.
2.2.0,Prune away any padding that was added
2.2.0,Handle case of 0-dimensional scalar output
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,inputs placeholder
2.2.0,data preprocessing and augmentation
2.2.0,first conv layer
2.2.0,downsample by max pooling
2.2.0,each module is a residual convolutional block
2.2.0,followed by a convolutional downsample layer
2.2.0,max pooling over the final outcome
2.2.0,fully connected layers
2.2.0,dropout for dense layers
2.2.0,"in_layer = Dropout(0.25, in_layers=[in_layer])"
2.2.0,weight decay regularizer
2.2.0,"weighted_loss = WeightDecay(0.1, 'l2', in_layers=[weighted_loss])"
2.2.0,sample cut ratio from a clipped gaussian
2.2.0,train/valid differences
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,Define and build model
2.2.0,model.restore()
2.2.0,Set random seeds
2.2.0,Setup directories
2.2.0,Model constants
2.2.0,Load and transform datasets
2.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.2.0,Atomic convolution variables
2.2.0,at = atomic numbers (atom types)
2.2.0,"radial basis function parameters [cutoff, mean, width]"
2.2.0,Model hyperparameters
2.2.0,Initialize model
2.2.0,Fit model
2.2.0,Evaluate model
2.2.0,Set random seeds
2.2.0,Setup directories
2.2.0,Model constants
2.2.0,Load and transform datasets
2.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.2.0,Atomic convolution variables
2.2.0,at = atomic numbers (atom types)
2.2.0,"radial basis function parameters [cutoff, mean, width]"
2.2.0,Model hyperparameters
2.2.0,Initialize model
2.2.0,Fit model
2.2.0,Evaluate model
2.2.0,Set random seeds
2.2.0,Setup directories
2.2.0,Model constants
2.2.0,Load and transform datasets
2.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.2.0,Atomic convolution variables
2.2.0,at = atomic numbers (atom types)
2.2.0,"radial basis function parameters [cutoff, mean, width]"
2.2.0,Model hyperparameters
2.2.0,Initialize model
2.2.0,Fit model
2.2.0,Evaluate model
2.2.0,Set random seeds
2.2.0,Setup directories
2.2.0,Model constants
2.2.0,Load and transform datasets
2.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.2.0,Atomic convolution variables
2.2.0,at = atomic numbers (atom types)
2.2.0,"radial basis function parameters [cutoff, mean, width]"
2.2.0,Model hyperparameters
2.2.0,Initialize model
2.2.0,Fit model
2.2.0,Evaluate model
2.2.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.2.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.2.0,test_scores = test_evaluator.compute_model_performance(metric)
2.2.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.2.0,param.update(test_scores)
2.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.2.0,for transformer in transformers:
2.2.0,train_dataset = transformer.transform(train_dataset)
2.2.0,test_dataset = transformer.transform(test_dataset)
2.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.2.0,for transformer in transformers:
2.2.0,train_dataset = transformer.transform(train_dataset)
2.2.0,test_dataset = transformer.transform(test_dataset)
2.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.2.0,for transformer in transformers:
2.2.0,train_dataset = transformer.transform(train_dataset)
2.2.0,test_dataset = transformer.transform(test_dataset)
2.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.2.0,for transformer in transformers:
2.2.0,train_dataset = transformer.transform(train_dataset)
2.2.0,test_dataset = transformer.transform(test_dataset)
2.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.2.0,Create some directories for analysis
2.2.0,The base_dir holds the results of all analysis
2.2.0,Make directories to store the raw and featurized datasets.
2.2.0,Load PDBBind dataset
2.2.0,Define featurizers
2.2.0,Currently featurizes with shard_size=1
2.2.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.2.0,This could be done with openbabel in python
2.2.0,Compute cells for this molecule. O(constant)
2.2.0,min == max if molecule is planar in some direction
2.2.0,we should still create a bin
2.2.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.2.0,Note neighbors contains self!
2.2.0,Associate each atom with cell it belongs to. O(N)
2.2.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.2.0,"conditions, so does wrapround. O(constant)"
2.2.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.2.0,Accept as neighbors only those within threshold. This computation should be
2.2.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.2.0,Sort neighbors by distance
2.2.0,Pick up to max_num_neighbors
2.2.0,Type of data created by this featurizer
2.2.0,assumes that every array is of the same dimension
2.2.0,rem_dataset is remaining portion of dataset
2.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.2.0,to k-1.
2.2.0,returns list of per column sum of non zero elements
2.2.0,Compute number of actives needed per task.
2.2.0,loop through each column and obtain index required to splice out for
2.2.0,required fraction of hits
2.2.0,Find the first index where the cumulative number of actives equals
2.2.0,the actives_count
2.2.0,Note that np.where tells us last index required to exceed
2.2.0,"actives_count, so we actually want the following location"
2.2.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.2.0,potentially refactor those to match this.
2.2.0,Handle edge case where frac_split is 1
2.2.0,Create weight matrices fpor two haves.
2.2.0,copy over up to required index for weight first_split
2.2.0,check out if any rows in either w_1 or w_2 are just zeros
2.2.0,"Obtain original x, y, and w arrays and shuffle"
2.2.0,calculate percent split for valid (out of test and valid)
2.2.0,"split test data into valid and test, treating sub test set also as sparse"
2.2.0,rem_dataset is remaining portion of dataset
2.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.2.0,to k-1.
2.2.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.2.0,This can be generalized in the future with some common demoninator determination.
2.2.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.2.0,Append remaining examples to train
2.2.0,Sort by increasing MW
2.2.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.2.0,for m_idx in cluster:
2.2.0,"continue until we find an active in all the tasks, otherwise we can't"
2.2.0,compute a meaningful AUC
2.2.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.2.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.2.0,Sort from largest to smallest scaffold sets
2.2.0,Sort from largest to smallest scaffold sets
2.2.0,"(n_samples, n_classes)"
2.2.0,"(n_samples, n_tasks, n_classes)"
2.2.0,Save hyperparameters
2.2.0,Guard variable to make sure we don't Restore() this model
2.2.0,from a disk checkpoint more than once.
2.2.0,"Path to save checkpoint files, which matches the"
2.2.0,replicated supervisor's default path.
2.2.0,Lazily created by _get_shared_session().
2.2.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.2.0,when subclass-overridden methods use the same scopes.
2.2.0,Setup graph
2.2.0,Note that we divide by the batch size and not the number of
2.2.0,"non-zero weight examples in the batch.  Also, instead of using"
2.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.2.0,calculate with div/sum so it stays on the GPU.
2.2.0,aggregated costs
2.2.0,weight decay
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,Save an initial checkpoint.
2.2.0,Turns out there are valid cases where we don't want pad-batches
2.2.0,on by default.
2.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.2.0,Run training op.
2.2.0,Always save a final checkpoint when complete.
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,allow_soft_placement=True allows ops without a GPU implementation
2.2.0,to run on the CPU instead.
2.2.0,TODO(rbharath): Is setting train=False right here?
2.2.0,Discard any padded predictions
2.2.0,The iterbatches does padding with zero-weight examples on the last batch.
2.2.0,Remove padded examples.
2.2.0,Special case to handle singletasks.
2.2.0,The iterbatches does padding with zero-weight examples on the last batch.
2.2.0,Remove padded examples.
2.2.0,TODO(rbharath): Verify this can be safely removed.
2.2.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.2.0,""""""""
2.2.0,Evaluates the performance of this model on specified dataset.
2.2.0,
2.2.0,Parameters
2.2.0,----------
2.2.0,dataset: dc.data.Dataset
2.2.0,Dataset object.
2.2.0,metric: deepchem.metrics.Metric
2.2.0,Evaluation metric
2.2.0,transformers: list
2.2.0,List of deepchem.transformers.Transformer
2.2.0,Returns
2.2.0,-------
2.2.0,dict
2.2.0,Maps tasks to scores under metric.
2.2.0,""""""""
2.2.0,"evaluator = Evaluator(self, dataset, transformers)"
2.2.0,scores = evaluator.compute_model_performance(metrics)
2.2.0,return scores
2.2.0,checkpoints look like logdir/model.ckpt-N
2.2.0,"self._save_path is ""logdir/model.ckpt"""
2.2.0,run eval data through the model
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,run eval data through the model
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,Note that softmax is already applied in construct_grpah
2.2.0,run eval data through the model
2.2.0,reshape to batch_size x n_tasks x ...
2.2.0,Handle edge case when batch-size is 1.
2.2.0,Prune away any padding that was added
2.2.0,Handle case of 0-dimensional scalar output
2.2.0,Dummy placeholders
2.2.0,Dummy placeholders
2.2.0,## AtomicNet fully-connected layer ops ###
2.2.0,## Atomicnet coordinate transform ops ###
2.2.0,## Atomicnet symmetry function kernel ops ###
2.2.0,## Atomicnet symmetry function ops ###
2.2.0,## Atomcnet symmetry function layer ops ###
2.2.0,We apply the radial pooling filter before atom type conv
2.2.0,to reduce computation
2.2.0,## Misc convenience ops ###
2.2.0,"Copied from the yt_project, commit e8fb57e"
2.2.0,yt/doc/extensions/notebook_sphinxext.py
2.2.0,https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
2.2.0,Almost completely re-written by Matthew Harrigan to use nbconvert v4
2.2.0,1. Uneval notebook
2.2.0,2. Python
2.2.0,3. HTML (execute first)
2.2.0,Set per-cell timeout to 60 seconds
2.2.0,4. Eval'd notebook
2.2.0,Create link to notebook and script files
2.2.0,create notebook node
2.2.0,add dependency
2.2.0,-*- coding: utf-8 -*-
2.2.0,
2.2.0,"deepchem documentation build configuration file, created by"
2.2.0,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
2.2.0,
2.2.0,This file is execfile()d with the current directory set to its
2.2.0,containing dir.
2.2.0,
2.2.0,Note that not all possible configuration values are present in this
2.2.0,autogenerated file.
2.2.0,
2.2.0,All configuration values have a default; values that are commented out
2.2.0,serve to show the default.
2.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.2.0,add these directories to sys.path here. If the directory is relative to the
2.2.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.2.0,"sys.path.insert(0, os.path.abspath('.'))"
2.2.0,-- General configuration ------------------------------------------------
2.2.0,"If your documentation needs a minimal Sphinx version, state it here."
2.2.0,needs_sphinx = '1.0'
2.2.0,"Add any Sphinx extension module names here, as strings. They can be"
2.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.2.0,ones.
2.2.0,"Add any paths that contain templates here, relative to this directory."
2.2.0,The suffix(es) of source filenames.
2.2.0,You can specify multiple suffix as a list of string:
2.2.0,"source_suffix = ['.rst', '.md']"
2.2.0,The encoding of source files.
2.2.0,source_encoding = 'utf-8-sig'
2.2.0,The master toctree document.
2.2.0,General information about the project.
2.2.0,"The version info for the project you're documenting, acts as replacement for"
2.2.0,"|version| and |release|, also used in various other places throughout the"
2.2.0,built documents.
2.2.0,
2.2.0,The short X.Y version.
2.2.0,"The full version, including alpha/beta/rc tags."
2.2.0,The language for content autogenerated by Sphinx. Refer to documentation
2.2.0,for a list of supported languages.
2.2.0,
2.2.0,This is also used if you do content translation via gettext catalogs.
2.2.0,"Usually you set ""language"" from the command line for these cases."
2.2.0,"There are two options for replacing |today|: either, you set today to some"
2.2.0,"non-false value, then it is used:"
2.2.0,today = ''
2.2.0,"Else, today_fmt is used as the format for a strftime call."
2.2.0,"today_fmt = '%B %d, %Y'"
2.2.0,"List of patterns, relative to source directory, that match files and"
2.2.0,directories to ignore when looking for source files.
2.2.0,The reST default role (used for this markup: `text`) to use for all
2.2.0,documents.
2.2.0,default_role = None
2.2.0,"If true, '()' will be appended to :func: etc. cross-reference text."
2.2.0,add_function_parentheses = True
2.2.0,"If true, the current module name will be prepended to all description"
2.2.0,unit titles (such as .. function::).
2.2.0,add_module_names = True
2.2.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
2.2.0,output. They are ignored by default.
2.2.0,show_authors = False
2.2.0,The name of the Pygments (syntax highlighting) style to use.
2.2.0,A list of ignored prefixes for module index sorting.
2.2.0,modindex_common_prefix = []
2.2.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
2.2.0,keep_warnings = False
2.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.2.0,-- Options for HTML output ----------------------------------------------
2.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.2.0,a list of builtin themes.
2.2.0,Theme options are theme-specific and customize the look and feel of a theme
2.2.0,"further.  For a list of options available for each theme, see the"
2.2.0,documentation.
2.2.0,html_theme_options = {}
2.2.0,"Add any paths that contain custom themes here, relative to this directory."
2.2.0,"The name for this set of Sphinx documents.  If None, it defaults to"
2.2.0,"""<project> v<release> documentation""."
2.2.0,html_title = None
2.2.0,A shorter title for the navigation bar.  Default is the same as html_title.
2.2.0,html_short_title = None
2.2.0,The name of an image file (relative to this directory) to place at the top
2.2.0,of the sidebar.
2.2.0,The name of an image file (within the static path) to use as favicon of the
2.2.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
2.2.0,pixels large.
2.2.0,html_favicon = None
2.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.2.0,"relative to this directory. They are copied after the builtin static files,"
2.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.2.0,Add any extra paths that contain custom files (such as robots.txt or
2.2.0,".htaccess) here, relative to this directory. These files are copied"
2.2.0,directly to the root of the documentation.
2.2.0,html_extra_path = []
2.2.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
2.2.0,using the given strftime format.
2.2.0,"html_last_updated_fmt = '%b %d, %Y'"
2.2.0,"If true, SmartyPants will be used to convert quotes and dashes to"
2.2.0,typographically correct entities.
2.2.0,html_use_smartypants = True
2.2.0,"Custom sidebar templates, maps document names to template names."
2.2.0,html_sidebars = {}
2.2.0,"Additional templates that should be rendered to pages, maps page names to"
2.2.0,template names.
2.2.0,html_additional_pages = {}
2.2.0,"If false, no module index is generated."
2.2.0,html_domain_indices = True
2.2.0,"If false, no index is generated."
2.2.0,html_use_index = True
2.2.0,"If true, the index is split into individual pages for each letter."
2.2.0,html_split_index = False
2.2.0,"If true, links to the reST sources are added to the pages."
2.2.0,html_show_sourcelink = True
2.2.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
2.2.0,html_show_sphinx = True
2.2.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
2.2.0,html_show_copyright = True
2.2.0,"If true, an OpenSearch description file will be output, and all pages will"
2.2.0,contain a <link> tag referring to it.  The value of this option must be the
2.2.0,base URL from which the finished HTML is served.
2.2.0,html_use_opensearch = ''
2.2.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
2.2.0,html_file_suffix = None
2.2.0,Language to be used for generating the HTML full-text search index.
2.2.0,Sphinx supports the following languages:
2.2.0,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
2.2.0,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
2.2.0,html_search_language = 'en'
2.2.0,"A dictionary with options for the search language support, empty by default."
2.2.0,Now only 'ja' uses this config value
2.2.0,html_search_options = {'type': 'default'}
2.2.0,The name of a javascript file (relative to the configuration directory) that
2.2.0,"implements a search results scorer. If empty, the default will be used."
2.2.0,html_search_scorer = 'scorer.js'
2.2.0,Output file base name for HTML help builder.
2.2.0,-- Options for LaTeX output ---------------------------------------------
2.2.0,The paper size ('letterpaper' or 'a4paper').
2.2.0,"'papersize': 'letterpaper',"
2.2.0,"The font size ('10pt', '11pt' or '12pt')."
2.2.0,"'pointsize': '10pt',"
2.2.0,Additional stuff for the LaTeX preamble.
2.2.0,"'preamble': '',"
2.2.0,Latex figure (float) alignment
2.2.0,"'figure_align': 'htbp',"
2.2.0,Grouping the document tree into LaTeX files. List of tuples
2.2.0,"(source start file, target name, title,"
2.2.0,"author, documentclass [howto, manual, or own class])."
2.2.0,The name of an image file (relative to this directory) to place at the top of
2.2.0,the title page.
2.2.0,latex_logo = None
2.2.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
2.2.0,not chapters.
2.2.0,latex_use_parts = False
2.2.0,"If true, show page references after internal links."
2.2.0,latex_show_pagerefs = False
2.2.0,"If true, show URL addresses after external links."
2.2.0,latex_show_urls = False
2.2.0,Documents to append as an appendix to all manuals.
2.2.0,latex_appendices = []
2.2.0,"If false, no module index is generated."
2.2.0,latex_domain_indices = True
2.2.0,-- Options for manual page output ---------------------------------------
2.2.0,One entry per manual page. List of tuples
2.2.0,"(source start file, name, description, authors, manual section)."
2.2.0,"If true, show URL addresses after external links."
2.2.0,man_show_urls = False
2.2.0,-- Options for Texinfo output -------------------------------------------
2.2.0,Grouping the document tree into Texinfo files. List of tuples
2.2.0,"(source start file, target name, title, author,"
2.2.0,"dir menu entry, description, category)"
2.2.0,Documents to append as an appendix to all manuals.
2.2.0,texinfo_appendices = []
2.2.0,"If false, no module index is generated."
2.2.0,texinfo_domain_indices = True
2.2.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
2.2.0,texinfo_show_urls = 'footnote'
2.2.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
2.2.0,texinfo_no_detailmenu = False
2.2.0,Example configuration for intersphinx: refer to the Python standard library.
2.2.0,Higher is Better
2.2.0,The secret key is available as a secure environment variable
2.2.0,on travis-ci to push the build documentation to Amazon S3.
2.2.0,Perform recursive modification to set css mime types.
2.2.0,Perform recursive modification to set js mime types.
2.2.0,plt.show()
2.2.0,"run_benchmark(FILE, DEEPCHEM_DIR)"
2.2.0,lines in the label file have format
2.2.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.2.0,"print line[0], line[3]"
2.2.0,Record inputs.
2.2.0,Create the output directory if necessary.
2.2.0,Create duplicate placeholders for meta-optimization.
2.2.0,Create the loss function for meta-optimization.
2.2.0,"In the final loss, use different placeholders for all inputs so the loss will be"
2.2.0,computed from a different batch.
2.2.0,Create variables for accumulating the gradients.
2.2.0,Create the optimizers for meta-optimization and task optimization.
2.2.0,Main optimization loop.
2.2.0,Do checkpointing.
2.2.0,This is a MetaLearner that learns to generate sine functions with variable
2.2.0,amplitude and phase.
2.2.0,Optimize it.
2.2.0,Test it out on some new tasks and see how it works.
2.2.0,Initially the model should do a bad job of fitting the sine function.
2.2.0,After one step of optimization it should do much better.
2.2.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.2.0,get the same result.
2.2.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.2.0,Fit model on dataset
2.2.0,Fit model on dataset
2.2.0,"Should be an array of size (n_pocket_atoms, 3)"
2.2.0,"coords[triangle, 0] gives the x-dimension of all triangle points"
2.2.0,Take transpose to make sure rows correspond to atoms.
2.2.0,We voxelize so all grids have integral coordinates (convenience)
2.2.0,"If overlap of box with previously generated output boxes, return"
2.2.0,Carry forward mappings
2.2.0,We know that box has at least one atom not in outputs
2.2.0,Current box has been merged into box further down list.
2.2.0,No need to output current box
2.2.0,"protein_coords is (N, 3) tensor"
2.2.0,Load binding pocket model
2.2.0,TODO(rbharath): Shift refined to full once trained.
2.2.0,Fit model on dataset
2.2.0,Create featurizers
2.2.0,"if not ligand_file.endswith("".sdf""):"
2.2.0,"raise ValueError(""Only .sdf ligand files can be featurized."")"
2.2.0,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
2.2.0,ligand_mol2 = os.path.join(
2.2.0,"self.base_dir, ligand_basename + "".mol2"")"
2.2.0,
2.2.0,# Write mol2 file for ligand
2.2.0,obConversion = ob.OBConversion()
2.2.0,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
2.2.0,ob_mol = ob.OBMol()
2.2.0,"obConversion.ReadFile(ob_mol, str(ligand_file))"
2.2.0,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
2.2.0,
2.2.0,# Featurize ligand
2.2.0,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
2.2.0,if mol is None:
2.2.0,"return None, None"
2.2.0,# Default for CircularFingerprint
2.2.0,n_ligand_features = 1024
2.2.0,ligand_features = self.ligand_featurizer.featurize([mol])
2.2.0,
2.2.0,# Featurize pocket
2.2.0,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
2.2.0,"protein_file, ligand_file)"
2.2.0,n_pockets = len(pockets)
2.2.0,n_pocket_features = BindingPocketFeaturizer.n_features
2.2.0,
2.2.0,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
2.2.0,pocket_features = self.pocket_featurizer.featurize(
2.2.0,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
2.2.0,# Note broadcast operation
2.2.0,"features[:, :n_pocket_features] = pocket_features"
2.2.0,"features[:, n_pocket_features:] = ligand_features"
2.2.0,dataset = NumpyDataset(X=features)
2.2.0,pocket_preds = self.model.predict(dataset)
2.2.0,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
2.2.0,
2.2.0,# Find pockets which are active
2.2.0,active_pockets = []
2.2.0,active_pocket_atoms_map = {}
2.2.0,active_pocket_coords = []
2.2.0,for pocket_ind in range(len(pockets)):
2.2.0,#################################################### DEBUG
2.2.0,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
2.2.0,#if pocket_preds[pocket_ind] == 1:
2.2.0,if pocket_pred_proba[pocket_ind][1] > .15:
2.2.0,#################################################### DEBUG
2.2.0,pocket = pockets[pocket_ind]
2.2.0,active_pockets.append(pocket)
2.2.0,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
2.2.0,active_pocket_coords.append(pocket_coords[pocket_ind])
2.2.0,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
2.2.0,# TODO(LESWING)
2.2.0,TODO: add pi_stack and cation_pi to feature_types (it's not trivial
2.2.0,because they require sanitized molecules)
2.2.0,"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.2.0,"""salt_bridge""],"
2.2.0,TODO(rbharath): May want to move this file to S3 so we can ensure it's
2.2.0,always available.
2.2.0,Prepare receptor
2.2.0,Get protein centroid and range
2.2.0,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
2.2.0,going to be extremely slow!
2.2.0,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
2.2.0,first pocket.
2.2.0,Prepare receptor
2.2.0,TODO(rbharath): Generalize this so can support mol2 files as well.
2.2.0,Write Vina conf file
2.2.0,Define locations of log and output files
2.2.0,TODO(rbharath): Let user specify the number of poses required.
2.2.0,TODO(rbharath): Convert the output pdbqt to a pdb file.
2.2.0,Return docked files
2.2.0,Check returned files exist
2.2.0,Check returned files exist
2.2.0,Check returned files exist
2.2.0,Check returned files exist
2.2.0,Check returned files exist
2.2.0,Note this may download autodock Vina...
2.2.0,Note this may download autodock Vina...
2.2.0,Note this may download autodock Vina...
2.2.0,Check returned files exist
2.2.0,Note this may download autodock Vina...
2.2.0,Check returned files exist
2.2.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.2.0,box1 contained in box2
2.2.0,"box1 in box2, so complete overlap"
2.2.0,"4/5 atoms in box2 in box1, so 80 % overlap"
2.2.0,box2 contains box1
2.2.0,box1 contains box2
2.2.0,"box1 contains box2, box3"
2.2.0,Test that every atom in pocket maps exists
2.2.0,Check that the atoms is actually in protein
2.2.0,Test that every atom in pocket maps exists
2.2.0,Check that the atoms is actually in protein
2.2.0,Add active site to dict
2.2.0,initialize fwd and reverse scores to -infinity
2.2.0,"cross-correlate separately for each base,"
2.2.0,for both the PSSM and its reverse complement
2.2.0,sum over the bases
2.2.0,take max of fwd and reverse scores at each position
2.2.0,"Shape (N_sequences, N_letters, sequence_length, 1, num_tasks)"
2.2.0,"Shape (N_sequences, num_tasks)"
2.2.0,"Shape (N_sequences, num_tasks, 1, 1, 1)"
2.2.0,Mutates every position of the sequence to every letter
2.2.0,"Shape (N_letters * sequence_length, N_letters, sequence_length, 1)"
2.2.0,Breakdown:
2.2.0,"Shape of sequence[np.newaxis] (1, N_letters, sequence_length, 1)"
2.2.0,remove wild-type
2.2.0,len(arange) = N_letters * sequence_length
2.2.0,len(horizontal cycle) = N_letters * sequence_length
2.2.0,add mutant
2.2.0,make mutant predictions
2.2.0,The convention used is that the first task is the metric.
2.2.0,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
2.2.0,"an option in the Metric class. Instead, this should be possible to move into"
2.2.0,user-space as a custom task_averager function.
2.2.0,"TODO(rbharath, joegomes): What is this magic number?"
2.2.0,"If there are no nonzero examples, metric is ill-defined."
2.2.0,Best score case
2.2.0,Worst score case
2.2.0,Encode motif
2.2.0,"sequences now has shape (3, 4, 5, 1)"
2.2.0,"sequences now has shape (3, 4, 5, 1)"
2.2.0,Construct and train SequenceDNN model
2.2.0,"X = np.random.rand(10, 1, 4, 50)"
2.2.0,"y = np.random.randint(0, 2, size=(10, 1))"
2.2.0,"dataset = dc.data.NumpyDataset(X, y)"
2.2.0,Call in-silico mutagenesis
2.2.0,Construct and train SequenceDNN model
2.2.0,"X = np.random.rand(10, 1, 4, 50)"
2.2.0,"y = np.random.randint(0, 2, size=(10, 1))"
2.2.0,"dataset = dc.data.NumpyDataset(X, y)"
2.2.0,Call in-silico mutagenesis
2.2.0,Check nonzero elements exist
2.2.0,ids = df[id_field].values
2.2.0,Set missing data to have weight zero
2.2.0,TODO (ytz) this is a bandage solution to reorder the atoms so
2.2.0,that they're always in the same canonical order. Presumably this
2.2.0,should be correctly implemented in the future for graph mols.
2.2.0,Featurize task results iff they exist.
2.2.0,Filter out examples where featurization failed.
2.2.0,"For prospective data where results are unknown, it makes"
2.2.0,no sense to have y values or weights.
2.2.0,"(X, y, w, ids)"
2.2.0,Sometimes zip files contain directories within. Traverse directories
2.2.0,TODO(rbharath): Add support for more extensions
2.2.0,Remove support indices
2.2.0,Remove support indices
2.2.0,Remove support indices
2.2.0,Get task specific entries
2.2.0,Now just get weights for this task
2.2.0,Get task specific entries
2.2.0,Now just get weights for this task
2.2.0,Now just get weights for this task
2.2.0,Now just get weights for this task
2.2.0,Split data into pos and neg lists.
2.2.0,No replacement allowed for supports
2.2.0,Handle one-d vs. non one-d feature matrices
2.2.0,Init the iterator
2.2.0,Set initial iterator state
2.2.0,support = self.supports[task][self.trial_num]
2.2.0,Increment and update logic
2.2.0,Init the iterator
2.2.0,Set initial iterator state
2.2.0,support = self.supports[task][self.trial_num]
2.2.0,Increment and update logic
2.2.0,"By invariant of when this is called, can assume num_samples > 0"
2.2.0,and num_samples < batch_size
2.2.0,Fill in batch arrays
2.2.0,"By invariant of when this is called, can assume num_samples > 0"
2.2.0,and num_samples < batch_size
2.2.0,Fill in batch arrays
2.2.0,Only the first set of copy will be counted in training loss
2.2.0,Retrieve the first sample so we can determine the dtypes.
2.2.0,Create a Tensorflow Dataset and have it create an Iterator.
2.2.0,"Set labels to be zero, with zero weights"
2.2.0,Load obsolete format -> save in new format
2.2.0,note that this corresponds to the _construct_metadata column order
2.2.0,if not len(self.metadata_df):
2.2.0,"raise ValueError(""No data in dataset."")"
2.2.0,return next(self.metadata_df.iterrows())[1]['task_names']
2.2.0,Create temp directory to store resharded version
2.2.0,Write data in new shards
2.2.0,Handle spillover from last shard
2.2.0,These columns may be missing is the dataset is unlabelled.
2.2.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.2.0,"than process based pools, since process based pools need to pickle/serialize"
2.2.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.2.0,we're actually protected by the GIL.
2.2.0,(ytz): this skips everything except possibly the last shard
2.2.0,"raw_data = (X, y, w, ids)"
2.2.0,Protect against generator exhaustion
2.2.0,This ensures tasks are consistent for all datasets
2.2.0,Get full dataset in memory
2.2.0,Shuffle in memory
2.2.0,Write shuffled shards out to disk
2.2.0,Shuffle the arrays corresponding to each row in metadata_df
2.2.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.2.0,Handle edge case with empty indices
2.2.0,Find indices which rest in this shard
2.2.0,Need to offset indices to fit within shard_size
2.2.0,Handle the case of datasets with y/w missing
2.2.0,Updating counts
2.2.0,Break when all indices have been used up already
2.2.0,TODO(rbharath): Get rid of * import
2.2.0,Load MUV dataset
2.2.0,Do an approximate comparison since splits are sometimes slightly off from
2.2.0,the exact fraction.
2.2.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.2.0,reloading will cause the transform to be reapplied. This is undesirable in
2.2.0,almost all cases. Need to understand a method to fix this.
2.2.0,def test_shuffle(self):
2.2.0,"""""""Test that datasets can be merged."""""""
2.2.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.2.0,dataset_file = os.path.join(
2.2.0,"current_dir, ""../../models/tests/example.csv"")"
2.2.0,featurizer = dc.feat.CircularFingerprint(size=1024)
2.2.0,"tasks = [""log-solubility""]"
2.2.0,loader = dc.data.CSVLoader(
2.2.0,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
2.2.0,"dataset = loader.featurize(dataset_file, shard_size=2)"
2.2.0,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
2.2.0,dataset.ids)
2.2.0,orig_len = len(dataset)
2.2.0,dataset.shuffle(iterations=5)
2.2.0,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
2.2.0,dataset.ids)
2.2.0,
2.2.0,assert len(dataset) == orig_len
2.2.0,# The shuffling should have switched up the ordering
2.2.0,"assert not np.array_equal(orig_ids, new_ids)"
2.2.0,# But all the same entries should still be present
2.2.0,assert sorted(orig_ids) == sorted(new_ids)
2.2.0,# All the data should have same shape
2.2.0,assert X_orig.shape == X_new.shape
2.2.0,assert y_orig.shape == y_new.shape
2.2.0,assert w_orig.shape == w_new.shape
2.2.0,The shuffling should have switched up the ordering
2.2.0,But all the same entries should still be present
2.2.0,All the data should have same shape
2.2.0,The ids should now store the performed permutation. Check that the
2.2.0,original dataset is recoverable.
2.2.0,The ids should now store the performed permutation. Check that the
2.2.0,original dataset is recoverable.
2.2.0,Set some global variables up top
2.2.0,Featurize emols dataset
2.2.0,example.fasta contains 3 sequences each of length 58.
2.2.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.2.0,"There is one ""image channel""."
2.2.0,Generate dummy dataset
2.2.0,Generate dummy dataset
2.2.0,Generate dummy dataset
2.2.0,Set last n_samples/2 weights to 0
2.2.0,Check that no support elements are sample from zero-weight samples
2.2.0,Generate dummy dataset
2.2.0,Generate dummy dataset
2.2.0,Create support generator
2.2.0,Generate dummy dataset
2.2.0,Create support generator
2.2.0,Generate dummy dataset
2.2.0,Assert all support elements have been removed
2.2.0,Generate dummy dataset
2.2.0,Assert all remove elements have been removed
2.2.0,Generate dummy dataset
2.2.0,Assert all support elements have been removed
2.2.0,Generate dummy dataset
2.2.0,Assert all remove elements have been removed
2.2.0,Generate dummy dataset
2.2.0,Set last n_samples/2 weights to 0
2.2.0,Sample from first n_samples/2 elements for support
2.2.0,Should lie within first n_samples/2 samples only
2.2.0,Generate dummy dataset
2.2.0,Create support generator
2.2.0,Generate dummy dataset
2.2.0,First try using images for X.
2.2.0,Now try using images for y.
2.2.0,Test on identity matrix
2.2.0,Generate random sparse features dataset
2.2.0,Test edge case with array of all zeros
2.2.0,Test cases where n_samples < 2*n_samples < batch_size
2.2.0,Test cases where n_samples < batch_size
2.2.0,Test case where n_samples == batch_size
2.2.0,Test case for object featurization.
2.2.0,Test case for more complicated object featurization
2.2.0,Test case with multidimensional data
2.2.0,Test cases where n_samples < 2*n_samples < batch_size
2.2.0,Test cases where n_samples < batch_size
2.2.0,Test case where n_samples == batch_size
2.2.0,Test case for object featurization.
2.2.0,Test case for more complicated object featurization
2.2.0,Test case with multidimensional data
2.2.0,Test first resharding worked
2.2.0,Test second resharding worked
2.2.0,approx 1/15! chance of equality
2.2.0,Generate data
2.2.0,Generate data
2.2.0,Generate data
2.2.0,Transform it
2.2.0,Transform it
2.2.0,special case to test
2.2.0,deterministic
2.2.0,non-deterministic
2.2.0,we don't know the order in which the shards are iterated in.
2.2.0,Check that we have all the data in
2.2.0,Create image file
2.2.0,Create zip of image file
2.2.0,"self.zip_path = ""/home/rbharath/misc/cells.zip"""
2.2.0,Create zip of multiple image files
2.2.0,"Create zip of multiple image files, multiple_types"
2.2.0,Create image directory
2.2.0,These are the known dimensions of face.png
2.2.0,TODO(rbharath): Where are the color channels?
2.2.0,"Since the different files have different shapes, makes an object array"
2.2.0,Splits featurized samples into train/test
2.2.0,Splits featurized samples into train/test
2.2.0,Splits featurized samples into train/test
2.2.0,"splittype = ""random"""
2.2.0,Splits featurized samples into train/test
2.2.0,Now perform move
2.2.0,Only for debug!
2.2.0,#Make directories to store the raw and featurized datasets.
2.2.0,Load dataset
2.2.0,Featurize tox21 dataset
2.2.0,###### Do featurization
2.2.0,Do train/valid split.
2.2.0,###### Do singletask load
2.2.0,################# Do comparison
2.2.0,Only for debug!
2.2.0,Set some global variables up top
2.2.0,Make directories to store the raw and featurized datasets.
2.2.0,Load dataset
2.2.0,Featurize tox21 dataset
2.2.0,For debugging purposes
2.2.0,###### Do multitask load
2.2.0,Do train/valid split.
2.2.0,###### Do singletask load
2.2.0,################# Do comparison
2.2.0,"task_type = ""regression"""
2.2.0,coding=utf-8
2.2.0,Note that transformers have to be undone in reversed order
2.2.0,Hack to allow for easy unpickling:
2.2.0,http://stefaanlippens.net/pickleproblem
2.2.0,"One, but not both, transform_X or tranform_y is true"
2.2.0,Use fact that bools add as ints in python
2.2.0,Control for pathological case with no variance.
2.2.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.2.0,Find the task dimension of z
2.2.0,Prevent broadcasting on wrong dimension
2.2.0,BalancingTransformer can only transform weights.
2.2.0,Compute weighting factors from dataset.
2.2.0,Ensure dataset is binary
2.2.0,Remove labels with zero weights
2.2.0,self.w = dataset.w
2.2.0,"TODO (flee2): for transform_y, figure out weights"
2.2.0,"print(""y will not be transformed by CDFTransformer, for now."")"
2.2.0,"print(""Cannot undo CDF Transformer, for now."")"
2.2.0,Need this for transform_y
2.2.0,array = np.transpose(array)
2.2.0,"print(""y will not be transformed by PowerTransformer, for now."")"
2.2.0,"print(""Cannot undo Power Transformer, for now."")"
2.2.0,the tf graph here pick up the (K+1) highest similarity values
2.2.0,and their indices
2.2.0,map the indices to labels
2.2.0,generating batch of data by slicing similarity matrix
2.2.0,into 100*reference_dataset_length
2.2.0,concatenate batches of data together
2.2.0,highest similarity is 1: target is in the reference
2.2.0,use the following K points
2.2.0,"highest less than 1: target not in the reference, use top K points"
2.2.0,calculate matrix multiplicatin on slices
2.2.0,concatenate the slices together
2.2.0,list of calculation orders for DAGs
2.2.0,stemming from one specific atom in the molecule
2.2.0,starting from the adjacency list derived by graphconv featurizer
2.2.0,"number of atoms, also number of DAGs"
2.2.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.2.0,each step calculating graph features for one atom.
2.2.0,`max_atoms` is the maximum number of steps
2.2.0,each iteration generates the DAG starting from atom with index `count`
2.2.0,"list of lists, elements represent the calculation orders"
2.2.0,for atoms in the current graph
2.2.0,starting from the target atom with index `count`
2.2.0,flags of whether the atom is already included in the DAG
2.2.0,atom `count` is in the DAG
2.2.0,recording number of radial propagation steps
2.2.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.2.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.2.0,will be added in the second loop(radial=1).
2.2.0,atoms i-bond away will be added in i-th loop
2.2.0,"when molecules have separate parts, starting from one part,"
2.2.0,it is not possible to include all atoms.
2.2.0,this break quit the loop when going into such condition
2.2.0,reinitialize targets for next iteration
2.2.0,atoms connected to current_atom
2.2.0,generate the dependency map of current DAG
2.2.0,atoms connected to `current_atoms`(and not included in the DAG)
2.2.0,"are added, and will be the `current_atoms` for next iteration."
2.2.0,"DAG starts from the target atom, calculation should go in reverse"
2.2.0,`edge[1]` is the parent of `edge[0]`
2.2.0,"after this loop, `parents[i]` includes all parents of atom i"
2.2.0,manually adding the atom index into its parents list
2.2.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.2.0,atoms with less parents(farther from the target atom) come first.
2.2.0,"graph features of atoms without parents will be first calculated,"
2.2.0,then atoms with more parents can be calculated in order
2.2.0,based on previously calculated graph features.
2.2.0,target atom of this DAG will be calculated in the last step
2.2.0,padding with `max_atoms`
2.2.0,padding
2.2.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.2.0,which is a max_atoms * max_atoms numpy array after padding
2.2.0,Calculate pairwise distance
2.2.0,Masking for valid atom index
2.2.0,Cutoff with threshold Rc
2.2.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.2.0,extracting validation set of MNIST for testing the DataTransforms
2.2.0,extract only the images (no need of the labels)
2.2.0,reshaping the vector to image
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is a y transformer
2.2.0,Check w is unchanged since this is a y transformer
2.2.0,Check y is now a logarithmic version of itself
2.2.0,Check that untransform does the right thing.
2.2.0,transforming y should raise an exception
2.2.0,transforming w should raise an exception
2.2.0,transforming X should be okay
2.2.0,Check ids are unchanged.
2.2.0,Check y is unchanged since this is a X transformer
2.2.0,Check w is unchanged since this is a y transformer
2.2.0,Check y is now a logarithmic version of itself
2.2.0,Check that untransform does the right thing.
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is a y transformer
2.2.0,Check w is unchanged since this is a y transformer
2.2.0,Check y is now a logarithmic version of itself
2.2.0,Check that untransform does the right thing.
2.2.0,Tests logarithmic data transformer with selection.
2.2.0,Check ids are unchanged.
2.2.0,Check y is unchanged since this is a X transformer
2.2.0,Check w is unchanged since this is a y transformer
2.2.0,Check y is now a logarithmic version of itself
2.2.0,Check that untransform does the right thing.
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is a y transformer
2.2.0,Check w is unchanged since this is a y transformer
2.2.0,"Check that y_t has zero mean, unit std."
2.2.0,Check that untransform does the right thing.
2.2.0,Check ids are unchanged.
2.2.0,Check y is unchanged since this is a X transformer
2.2.0,Check w is unchanged since this is a y transformer
2.2.0,"Check that X_t has zero mean, unit std."
2.2.0,np.set_printoptions(threshold='nan')
2.2.0,Entries with zero std are not normalized
2.2.0,TODO(rbharath): Untransform doesn't work properly for binary feature
2.2.0,vectors. Need to figure out what's wrong here. (low priority)
2.2.0,# Check that untransform does the right thing.
2.2.0,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
2.2.0,Check ids are unchanged.
2.2.0,Check y is unchanged since this is an X transformer
2.2.0,Check w is unchanged since this is an X transformer
2.2.0,Check X is now holding the proper values when sorted.
2.2.0,Test CDF transformer on Gaussian normal dataset.
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is an y transformer
2.2.0,Check w is unchanged since this is an y transformer
2.2.0,Check y is now holding the proper values when sorted.
2.2.0,Check that untransform does the right thing.
2.2.0,Check ids are unchanged.
2.2.0,Check y is unchanged since this is an X transformer
2.2.0,Check w is unchanged since this is an X transformer
2.2.0,Check X is now holding the proper values when sorted.
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is a y transformer
2.2.0,Check w is unchanged since this is a y transformer
2.2.0,Check y is now holding the proper values when sorted.
2.2.0,Check ids are unchanged.
2.2.0,Check y is unchanged since this is an X transformer
2.2.0,Check w is unchanged since this is an X transformer
2.2.0,Check X is now holding the proper values in each column.
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is an X transformer
2.2.0,Check w is unchanged since this is an X transformer
2.2.0,Check y is now holding the proper values in each column.
2.2.0,Check that untransform does the right thing.
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is a w transformer
2.2.0,Check y is unchanged since this is a w transformer
2.2.0,Assert that entries with zero weight retain zero weight
2.2.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.2.0,Check ids are unchanged.
2.2.0,Check X is unchanged since this is a w transformer
2.2.0,Check y is unchanged since this is a w transformer
2.2.0,Assert that entries with zero weight retain zero weight
2.2.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.2.0,Check Blurring
2.2.0,Check rotation
2.2.0,Some more test cases for flip
2.2.0,Check flip
2.2.0,Check Scales
2.2.0,Check shift
2.2.0,check gaussian noise
2.2.0,check salt and pepper noise
2.2.0,TODO(rbharath): Use standard joblib once old-data has been regenerated.
2.2.0,"If gzipped, need to compute extension again"
2.2.0,Tasks are either in .sdf.csv file or in the .sdf file itself
2.2.0,Structures are stored in .sdf file
2.2.0,First line of user-specified CSV *must* be header.
2.2.0,Try older joblib version for legacy files.
2.2.0,First line of user-specified CSV *must* be header.
2.2.0,First line of user-specified CSV *must* be header.
2.2.0,combine dataframes
2.2.0,TODO: mol should be always sanitized when charges are calculated
2.2.0,can't change it now because it would break a lot of examples
2.2.0,working-with-3d-molecules
2.2.0,initial embedding
2.2.0,minimization and pruning
2.2.0,always keep lowest-energy conformer
2.2.0,discard conformers after max_conformers is reached
2.2.0,get RMSD to selected conformers
2.2.0,discard conformers within the RMSD threshold
2.2.0,create a new molecule to hold the chosen conformers
2.2.0,this ensures proper conformer IDs and energy-based ordering
2.2.0,The label encoder is given characters for ACGTN
2.2.0,Peak at the first sequence to get the length of the sequence.
2.2.0,TODO(rbharath): This is now simple enough that we should probably get rid of
2.2.0,Evaluator object to avoid clutter.
2.2.0,Compute multitask metrics
2.2.0,Compute multitask metrics
2.2.0,Loosening atol to see if tests stop failing sporadically
2.2.0,One sequence has length longer than others. This should throw a
2.2.0,ValueError.
2.2.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,a*x + b*y + c*z = dI think that
2.2.0,"self.x, self.y, self.z = x, y, z"
2.2.0,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
2.2.0,TODO(bramsundar): Should this be __copy__?
2.2.0,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
2.2.0,"return np.array([self.x, self.y, self.z])"
2.2.0,TODO(rbharath): Should this be an atom function?
2.2.0,"This line is necessary for babel to work, though many PDBs in"
2.2.0,the PDB would have this line commented out
2.2.0,now atom type (for pdbqt)
2.2.0,"If atomtype is not specified, but atomname is, set atomtype to the"
2.2.0,"first letter of atomname. This heuristic suffices for proteins,"
2.2.0,since no two-letter elements appear in standard amino acids.
2.2.0,Any number needs to be removed from the element name
2.2.0,"this only uses the rightmost three characters, essentially"
2.2.0,removing unique rotamer identification
2.2.0,"The normal vector to plane is n = [a, b, c]"
2.2.0,We first shift by basepoint (a point on given plane) to make math
2.2.0,simpler. basepoint is given by d/||n||^2 * n
2.2.0,The perpendicular component of diff to plane is
2.2.0,(n^T diff / ||n||^2) * n
2.2.0,if ring is aromatic
2.2.0,"save its indices, center, and normal"
2.2.0,remember protein-ligand pairs we already counted
2.2.0,"if this pair is new, count atoms forming a contact"
2.2.0,"if this pair is new, count atoms forming a contact"
2.2.0,if ring from mol1 is aromatic
2.2.0,...and atom from mol2 is a cation
2.2.0,if angle and distance are correct
2.2.0,count atoms forming a contact
2.2.0,find interacting rings from protein and cations from ligand
2.2.0,find interacting cations from protein and rings from ligand
2.2.0,merge counters
2.2.0,TODO(LESWING)
2.2.0,check if user tries to set removed arguments
2.2.0,list of features that require sanitized molecules
2.2.0,not implemented featurization types
2.2.0,default values
2.2.0,update with cutoffs specified by the user
2.2.0,"each entry is a tuple (is_flat, feature_name)"
2.2.0,list of features that cannot be calculated with specified parameters
2.2.0,this list is used to define <flat/voxel/all>_combined subset
2.2.0,parse provided feature types
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,TODO(rbharath): Is this squeeze OK?
2.2.0,Get the degree id list (which corrects for min_deg)
2.2.0,Get the size of each degree block
2.2.0,Get the the start indices for items in each block
2.2.0,Get the node indices when they are reset when the degree changes
2.2.0,Convert to numpy array
2.2.0,Reorder old atom_features
2.2.0,Reorder old deg lists
2.2.0,Sort membership
2.2.0,Create old to new dictionary. not exactly intuitive
2.2.0,Reorder adjacency lists
2.2.0,Get numpy version of degree list for indexing
2.2.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.2.0,Parse as deg separated
2.2.0,Get indices corresponding to the current degree
2.2.0,Extract and save adjacency list for the current degree
2.2.0,Construct the slice information
2.2.0,Get the cumulative indices after the first index
2.2.0,Set indices with zero sized slices to zero to avoid indexing errors
2.2.0,TODO(rbharath): Can this be removed?
2.2.0,Use random insted of zeros to prevent weird issues with summing to zero
2.2.0,"Results should be sorted by (atom_degree, mol_index)"
2.2.0,"Mergesort is a ""stable"" sort, so the array maintains it's secondary sort of mol_index"
2.2.0,Sort all atoms by degree.
2.2.0,"Get the size of each atom list separated by molecule id, then by degree"
2.2.0,Get the final size of each degree block
2.2.0,"Get the index at which each degree starts, not resetting after each degree"
2.2.0,And not stopping at any speciic molecule
2.2.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.2.0,first column telling the start indices of each degree block and the
2.2.0,second colum telling the size of each degree block
2.2.0,Input for tensorflow
2.2.0,Determines the membership (atom i belongs to membership[i] molecule)
2.2.0,"Get the index at which each deg starts, resetting after each degree"
2.2.0,(deg x num_mols) matrix describing the start indices when you count up the atoms
2.2.0,"in the final representation, stopping at each molecule,"
2.2.0,resetting every time the degree changes
2.2.0,Gets the degree resetting block indices for the atoms in each molecule
2.2.0,"Here, the indices reset when the molecules change, and reset when the"
2.2.0,degree changes
2.2.0,Get the degree id lookup list. It allows us to search for the degree of a
2.2.0,molecule mol_id with corresponding atom mol_atom_id using
2.2.0,"deg_id_lists[mol_id,mol_atom_id]"
2.2.0,This is used for convience in the following function (explained below)
2.2.0,Get the degree id (corrected for min_deg) of the considered atom
2.2.0,Return the final index of atom mol_atom_id in molecule mol_id.  Using
2.2.0,"the degree of this atom, must find the index in the molecule's original"
2.2.0,"degree block corresponding to degree id deg_id (second term), and then"
2.2.0,calculate which index this degree block ends up in the final
2.2.0,representation (first term). The sum of the two is the final indexn
2.2.0,Initialize the new degree separated adjacency lists
2.2.0,Update the old adjcency lists with the new atom indices and then combine
2.2.0,all together
2.2.0,Iterate through all the molecules
2.2.0,Get the adjacency lists for this molecule and current degree id
2.2.0,"Correct all atom indices to the final indices, and then save the"
2.2.0,results into the new adjacency lists
2.2.0,Increment once row is done
2.2.0,Get the final aggregated molecule
2.2.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.2.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.2.0,consistent with most QM software packages.
2.2.0,Type of data created by this featurizer
2.2.0,TODO(rbharath): Should this return a list?
2.2.0,Type of data created by this featurizer
2.2.0,Currently handles loading failures by returning None
2.2.0,TODO: Is there a better handling procedure?
2.2.0,generate SMILES for fragments
2.2.0,Initalize with 1
2.2.0,Allow 0 index to correspond to null molecule 1
2.2.0,Correct for null
2.2.0,"print(6-k-1, id)"
2.2.0,Correct for last one
2.2.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.2.0,first `bt_len` features are bond features(if applicable)
2.2.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.2.0,graph distance between two atoms
2.2.0,Euclidean distance between atoms
2.2.0,atoms `radial` bonds away from `a1`
2.2.0,atoms less than `radial` bonds away
2.2.0,find atoms `radial`+1 bonds away
2.2.0,Get the node features
2.2.0,Stack nodes into an array
2.2.0,Get bond lists with reverse edges included
2.2.0,Get canonical adjacency list
2.2.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.2.0,only support datasets providing Cartesian coordinates)
2.2.0,Set dtype
2.2.0,If includes explicit hydrogens
2.2.0,If uses use_chirality
2.2.0,Atom features
2.2.0,Stack nodes into an array
2.2.0,Get bond lists
2.2.0,Get canonical adjacency list
2.2.0,Calculate pair features
2.2.0,TODO (VIGS25): Complete the description
2.2.0,Handle loading failures which return None
2.2.0,Fit atomic conv model
2.2.0,Add the Atomic Convolution layers to fetches
2.2.0,Extract the atomic convolution features
2.2.0,Handle loading failures which return None
2.2.0,atom_name is of format RESX-ATOMTYPE
2.2.0,where X is a 1 to 4 digit number
2.2.0,list-of-available-descriptors.
2.2.0,(ytz): This is done to avoid future compatibility issues like inclusion of
2.2.0,the 3D descriptors or changing the feature size.
2.2.0,check for separate count and SMILES entries for each fragment
2.2.0,TODO test more formats for ligand
2.2.0,adding hydrogens and charges is tested in dc.utils
2.2.0,3D vector with unit length
2.2.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.2.0,check if distances do not change
2.2.0,check if it works for molecules with different numbers of atoms
2.2.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.2.0,check if correct distance metric was used
2.2.0,"20 points with coords between -5 and 5, centered at 0"
2.2.0,indices are positive
2.2.0,coordinates were properly translated and scaled
2.2.0,for coordinates outside of the box function should properly transform them
2.2.0,to indices and warn the user
2.2.0,"TODO check if function warns. There is assertWarns method in unittest,"
2.2.0,but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
2.2.0,"20 points with coords between -5 and 5, centered at 0"
2.2.0,3 pairs of indices
2.2.0,simple flat ring
2.2.0,load and sanitize two real molecules
2.2.0,FIXME might break with different version of rdkit
2.2.0,FIXME might break with different version of rdkit
2.2.0,parallel normals
2.2.0,perpendicular normals
2.2.0,too far away
2.2.0,perpendicular normals
2.2.0,parallel normals
2.2.0,too far away
2.2.0,order of the molecules shouldn't matter
2.2.0,with this criteria we should find both types of stacking
2.2.0,parallel normals
2.2.0,perpendicular normals
2.2.0,too far away
2.2.0,"TODO find better example, currently dicts are empty"
2.2.0,"TODO find better example, currently dicts are empty"
2.2.0,TODO test if dict contains smiles
2.2.0,check if results are the same if we provide precomputed distances
2.2.0,...but first check if we actually got two dicts
2.2.0,check if we get less features with smaller distance cutoff
2.2.0,ligands are typically small so all atoms might be present
2.2.0,check if using different ecfp_degree changes anything
2.2.0,TODO upperbound?
2.2.0,test if default parameters work
2.2.0,check if use-case from examples works
2.2.0,test if input is flattened when flat features are used
2.2.0,test voxel features
2.2.0,test flat features
2.2.0,check if aromatic features are ignores if sanitize=False
2.2.0,"protein is too big for the box, some features should be missing"
2.2.0,whole ligand should fit in the box
2.2.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.2.0,of degree 1 (connected only to central nitrogen).
2.2.0,5 atoms in compound
2.2.0,Get the adjacency lists grouped by degree
2.2.0,The 4 outer atoms connected to central nitrogen
2.2.0,Central nitrogen connected to everything else.
2.2.0,Only one carbon
2.2.0,"No bonds, so degree adjacency lists are empty"
2.2.0,3 carbonds in alkane
2.2.0,Outer two carbonds are connected to central carbon
2.2.0,Central carbon connected to outer two
2.2.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.2.0,max num atoms instead of exact.
2.2.0,Cutoff in angstroms
2.2.0,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
2.2.0,this expected behavior? Need to think about API.
2.2.0,Do a manual distance computation and make
2.2.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.2.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.2.0,Do a manual distance computation and ensure that selected neighbor is
2.2.0,closest since we set max_num_neighbors = 1
2.2.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.2.0,max num atoms instead of exact.
2.2.0,Cutoff in angstroms
2.2.0,Splits featurized samples into train/test
2.2.0,Artificial feature array.
2.2.0,0 atoms of degree 0
2.2.0,0 atoms of degree 1
2.2.0,4 atoms of degree 2
2.2.0,0 atoms of degree 3
2.2.0,0 atoms of degree 4
2.2.0,0 atoms of degree 5
2.2.0,0 atoms of degree 6
2.2.0,0 atoms of degree 7
2.2.0,0 atoms of degree 8
2.2.0,0 atoms of degree 9
2.2.0,0 atoms of degree 10
2.2.0,atom 4 has 0 neighbors
2.2.0,atom 0 has 2 neighbors
2.2.0,atom 1 has 2 neighbors
2.2.0,atom 2 has 2 neighbors
2.2.0,atom 3 has 3 neighbors.
2.2.0,Verify that atom features have been sorted by atom degree.
2.2.0,Sorting is done by atom degree as before. So the ordering goes
2.2.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.2.0,from new position to old position is
2.2.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.2.0,list respects this reordering and returns correct adjacency list.
2.2.0,### First example molecule
2.2.0,Artificial feature array.
2.2.0,### Second example molecule
2.2.0,## Third example molecule
2.2.0,Test agglomerate molecule method
2.2.0,No atoms of degree 0
2.2.0,3 atoms of degree 1
2.2.0,8 atoms of degree 2
2.2.0,1 atom of degree 3
2.2.0,0 atoms of degree 4
2.2.0,0 atoms of degree 5
2.2.0,Check that atoms are only connected to themselves.
2.2.0,Check that there's one atom of each degree.
2.2.0,assumes that every array is of the same dimension
2.2.0,rem_dataset is remaining portion of dataset
2.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.2.0,to k-1.
2.2.0,dict is needed in case groups aren't strictly flattened or
2.2.0,hashed by something non-integer like
2.2.0,returns list of per column sum of non zero elements
2.2.0,Compute number of actives needed per task.
2.2.0,loop through each column and obtain index required to splice out for
2.2.0,required fraction of hits
2.2.0,Find the first index where the cumulative number of actives equals
2.2.0,the actives_count
2.2.0,Note that np.where tells us last index required to exceed
2.2.0,"actives_count, so we actually want the following location"
2.2.0,TODO(rbharath): Refactor this split method to match API of other
2.2.0,splits (or potentially refactor those to match this).
2.2.0,Handle edge case where frac_split is 1
2.2.0,Create weight matrices fpor two haves.
2.2.0,copy over up to required index for weight first_split
2.2.0,check out if any rows in either w_1 or w_2 are just zeros
2.2.0,calculate percent split for valid (out of test and valid)
2.2.0,"split remaining data into valid and test, treating sub test set also as sparse"
2.2.0,rem_dataset is remaining portion of dataset
2.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.2.0,to k-1.
2.2.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.2.0,This can be generalized in the future with some common demoninator determination.
2.2.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.2.0,Append remaining examples to train
2.2.0,Sort by increasing MW
2.2.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.2.0,for m_idx in cluster:
2.2.0,"continue until we find an active in all the tasks, otherwise we can't"
2.2.0,compute a meaningful AUC
2.2.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.2.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.2.0,Sort from largest to smallest scaffold sets
2.2.0,Pick the mol closest to everything as the first element of training
2.2.0,Pick the closest mol from what is left
2.2.0,Test is everything else
2.2.0,All datasets share features and identifiers by assumption.
2.2.0,TODO(rbharath): Get rid of * import
2.2.0,Note that the extra task goes to test
2.2.0,Number tasks per fold
2.2.0,Find the tasks that correspond to this test fold
2.2.0,Assert that all arrays look like they should
2.2.0,0 1 2 3 4 5 6 7 8 9
2.2.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.2.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.2.0,reshard() to handle this?
2.2.0,Verify lengths is 10/k == 2
2.2.0,Verify that compounds in this fold are subset of original compounds
2.2.0,Verify that no two folds have overlapping compounds.
2.2.0,Verify lengths is 10/k == 2
2.2.0,Verify that compounds in this fold are subset of original compounds
2.2.0,Verify that no two folds have overlapping compounds.
2.2.0,Verify lengths is 10/k == 2
2.2.0,Verify that compounds in this fold are subset of original compounds
2.2.0,Verify that no two folds have overlapping compounds.
2.2.0,Test singletask case.
2.2.0,The split index should partition dataset in half.
2.2.0,Test singletask case.
2.2.0,Test case where some weights are zero (i.e. masked)
2.2.0,Set half the positives to have zero weight
2.2.0,There are 10 nonzero actives.
2.2.0,"The split index should partition this into half, so expect 5"
2.2.0,The split index should partition dataset in half.
2.2.0,Mask half the examples
2.2.0,The split index should partition dataset in half.
2.2.0,Test singletask case.
2.2.0,Should have split cleanly in half (picked random seed to ensure this)
2.2.0,Check positives are correctly distributed
2.2.0,Test singletask case.
2.2.0,Should have made an 80/10/10 train/valid/test split of actives.
2.2.0,Verify lengths is 100/k == 20
2.2.0,Note: This wouldn't work for multitask str
2.2.0,assert len(fold_dataset) == n_samples/K
2.2.0,Verify that each fold has n_positives/K = 4 positive examples.
2.2.0,Verify that compounds in this fold are subset of original compounds
2.2.0,Verify that no two folds have overlapping compounds.
2.2.0,sparsity is determined by number of w weights that are 0 for a given
2.2.0,task structure of w np array is such that each row corresponds to a
2.2.0,sample. The loaded sparse dataset has many rows with only zeros
2.2.0,verify that there are no rows (samples) in weights matrix w
2.2.0,that have no hits.
2.2.0,task_metadata_rows = {task: [] for task in tasks}
2.2.0,Extract those datapoints which are present for this task
2.2.0,Loading is done on-the-fly
2.2.0,TODO(rbharath/enf): We need a structured way to deal with potential GPU
2.2.0,memory overflows.
2.2.0,Discard any padded predictions
2.2.0,################### Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0. ####################
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,Calculate pairwise distance
2.2.0,Masking for valid atom index
2.2.0,Cutoff with threshold Rc
2.2.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.2.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.2.0,optimization to allow for tensorcontraction/broadcasted mmul
2.2.0,using a reshape trick. Note that the np and tf matmul behavior
2.2.0,differs when dealing with broadcasts
2.2.0,-*- coding: UTF-8 -*-
2.2.0,Reshape everything to match the input with the most dimensions.
2.2.0,"This probably means the variable hasn't been created yet, so try again"
2.2.0,with reuse set to false.
2.2.0,Calculate what the new shape will be.
2.2.0,"Shape (N_atoms, M_nbrs, ndim)"
2.2.0,"Shape (N_atoms, M_nbrs, ndim)"
2.2.0,"Shape (N_atoms, M_nbrs)"
2.2.0,"This probably means the variable hasn't been created yet, so try again"
2.2.0,with reuse set to false.
2.2.0,"This probably means the variable hasn't been created yet, so try again"
2.2.0,with reuse set to false.
2.2.0,"This probably means the variable hasn't been created yet, so try again"
2.2.0,with reuse set to false.
2.2.0,"This probably means the variable hasn't been created yet, so try again"
2.2.0,with reuse set to false.
2.2.0,TODO(rbharath): Note sure if this layer can be called with __call__
2.2.0,"meaningfully, so not going to support that functionality for now."
2.2.0,Generate the nb_affine weights and biases
2.2.0,"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
2.2.0,Extract atom_features
2.2.0,Extract graph topology
2.2.0,Perform the mol conv
2.2.0,"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
2.2.0,"self.max_deg, self.min_deg, W_list,"
2.2.0,b_list)
2.2.0,Sum all neighbors using adjacency matrix
2.2.0,Get collection of modified atom features
2.2.0,Obtain relevant atoms for this degree
2.2.0,Get self atoms
2.2.0,Apply hidden affine to relevant atoms and append
2.2.0,Determine the min_deg=0 case
2.2.0,Only use the self layer
2.2.0,Combine all atoms back into the list
2.2.0,Tensorflow correctly processes empty lists when using concat
2.2.0,"Sum along neighbors as well as self, and store"
2.2.0,Perform the mol gather
2.2.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.2.0,"self.max_degree, self.min_degree)"
2.2.0,Tensorflow correctly processes empty lists when using concat
2.2.0,Get self atoms
2.2.0,Expand dims
2.2.0,always deg-1 for deg_adj_lists
2.2.0,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
2.2.0,Extract graph topology
2.2.0,No other forget biases supported right now.
2.2.0,Taken from Keras code [citation needed]
2.2.0,"x is test set, xp is support set."
2.2.0,## Performs computations
2.2.0,Get initializations
2.2.0,Process using attention
2.2.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.2.0,Generate new attention states
2.2.0,Support set lstm
2.2.0,Test lstm
2.2.0,self.build()
2.2.0,Get initializations
2.2.0,Rename support
2.2.0,Process support xp using attention
2.2.0,Get linear combination of support set
2.2.0,Process test x using attention
2.2.0,Generate new support attention states
2.2.0,Generate new test attention states
2.2.0,Redefine
2.2.0,Number of rotatable bonds
2.2.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.2.0,a difference.
2.2.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.2.0,neighbors lists an argument instead of a part of this layer.
2.2.0,"Shape (N, M)"
2.2.0,"Shape (N, M)"
2.2.0,"Shape (N, M)"
2.2.0,Number of grid cells
2.2.0,TODO(rbharath): Support batching
2.2.0,"Shape (n_cells, ndim)"
2.2.0,"List of length N_atoms, each element of different length uniques_i"
2.2.0,"List of length N_atoms, each element of different length uniques_i"
2.2.0,"List of length N_atoms, each a tensor of shape"
2.2.0,"(uniques_i, ndim)"
2.2.0,Add phantom atoms that exist far outside the box
2.2.0,"List of length N_atoms, each of shape (1, ndim)"
2.2.0,TODO(rbharath): How does distance need to be modified here to
2.2.0,account for periodic boundary conditions?
2.2.0,List of length N_atoms each of shape (M_nbrs)
2.2.0,"N_atoms elts of size (M_nbrs,) each"
2.2.0,"Shape (N_atoms, 1)"
2.2.0,Find M_nbrs atoms closest to each cell
2.2.0,"Shape (n_cells, M_nbrs)"
2.2.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.2.0,"conditions, so does wrapround. O(constant)"
2.2.0,"Shape (n_cells, n_nbr_cells)"
2.2.0,"Shape (N_atoms, n_nbr_cells)"
2.2.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.2.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.2.0,"List of length N_atoms, each element length uniques_i"
2.2.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.2.0,element removed to remove self from list of neighbors. Need to verify
2.2.0,this holds more broadly or come up with robust alternative.
2.2.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.2.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.2.0,Shape (N_atoms*n_cells)
2.2.0,"Shape (n_cells, N_atoms)"
2.2.0,Find k atoms closest to this cell. Notice negative sign since
2.2.0,tf.nn.top_k returns *largest* not smallest.
2.2.0,"Tensor of shape (n_cells, M_nbrs)"
2.2.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.2.0,"Shape (N_atoms*n_cells, 1) after tile"
2.2.0,9 neighbors in 2-space
2.2.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.2.0,Number of cells for cube in 3-space is
2.2.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.2.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.2.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.2.0,the cube.
2.2.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.2.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.2.0,"Tile (a, a, a, b, b, b, etc.)"
2.2.0,"Tile (a, b, c, a, b, c, ...)"
2.2.0,N: Maximum number of atoms
2.2.0,M: Maximum number of neighbors
2.2.0,d: Number of coordinates/features/filters
2.2.0,B: Batch Size
2.2.0,Create the variables.
2.2.0,Compute the distances and radial symmetry functions.
2.2.0,check that there isnt just one or zero inputs
2.2.0,create subspaces
2.2.0,create the alpha learnable parameters
2.2.0,"concatenate subspaces, reshape to size of original input, then stack"
2.2.0,"such that out_tensor has shape (2,?,original_cols)"
2.2.0,creates subspaces the same way it was done in AlphaShare
2.2.0,calculate squared Frobenius norm
2.2.0,"(TODO YTZ:) faster, less memory intensive way"
2.2.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.2.0,"r = tf.expand_dims(r, -1)"
2.2.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.2.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.2.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.2.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.2.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.2.0,Calculate pairwise distance
2.2.0,Cutoff with threshold Rc
2.2.0,return d
2.2.0,tf.stack issues again...
2.2.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.2.0,We do not need the mask because every graph has self.num_vertices vertices now
2.2.0,So the Tensor has known dimensions
2.2.0,Add in features
2.2.0,Add in labels
2.2.0,Add in all layers
2.2.0,The last layer is the output of the model
2.2.0,TODO(rbharath): Add in support for additional
2.2.0,losses.
2.2.0,TODO(rbharath): The TensorGraph can't be built until
2.2.0,fit is called since the shapes of features/labels
2.2.0,not specified. Need to figure out a good restoration
2.2.0,method for this use case.
2.2.0,ensure that randomness is conditioned by the Numpy RNG
2.2.0,ensure that randomness is conditioned by the Numpy RNG
2.2.0,TODO(rbharath): Should probably swap this over to tf mode.
2.2.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.2.0,"expects logits, Tensorflow expects probabilities."
2.2.0,scale preds so that the class probas of each sample sum to 1
2.2.0,manual computation of crossentropy
2.2.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.2.0,"expects logits, Tensorflow expects probabilities."
2.2.0,if our output includes timesteps we need to reshape
2.2.0,Arguments
2.2.0,Returns
2.2.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.2.0,"expects logits, Tensorflow expects probabilities."
2.2.0,transform back to logits
2.2.0,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
2.2.0,a tensor. Confusing with tf.zeros...
2.2.0,Transpose for mul
2.2.0,exclude bias variables
2.2.0,"tf.scalar_summary('Weight Decay Cost', cost)"
2.2.0,TODO(user): gradient clipping (see Minimize)
2.2.0,Assuming convolution kernels (2D or 3D).
2.2.0,"TF kernel shape: (..., input_depth, depth)"
2.2.0,No specific assumptions.
2.2.0,References
2.2.0,References
2.2.0,References
2.2.0,References
2.2.0,Pick the one with the correct shape.
2.2.0,Add the input features.
2.2.0,Add the shared dense layers
2.2.0,Add task-specific bypass layers
2.2.0,Add the input features.
2.2.0,Add the shared dense layers
2.2.0,Add task-specific bypass layers
2.2.0,Add the input features.
2.2.0,Add the dense layers
2.2.0,Compute the loss function for each label.
2.2.0,Add the input features.
2.2.0,Add the dense layers
2.2.0,Compute the loss function for each label.
2.2.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.2.0,Similarity values
2.2.0,Labels for all top K similar samples
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.2.0,and embeddings of atom j(both gone through a hidden layer)
2.2.0,"for atom i, sum the influence from all other atom j in the molecule"
2.2.0,number of inputs each step
2.2.0,Add trainable weights
2.2.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.2.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.2.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.2.0,initialize graph features for each graph
2.2.0,initialize graph features for each graph
2.2.0,another row of zeros is generated for padded dummy atoms
2.2.0,`count`-th step
2.2.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.2.0,generating index for graph features used in the inputs
2.2.0,"extracting graph features for parents of the target atoms, then flatten"
2.2.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.2.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.2.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.2.0,of shape: (batch_size*max_atoms) * n_graph_features
2.2.0,representing the graph features of target atoms in each graph
2.2.0,index for targe atoms
2.2.0,update the graph features for target atoms
2.2.0,Add trainable weights
2.2.0,Extract atom_features
2.2.0,Extract atom_features
2.2.0,sum all graph outputs
2.2.0,"Default message function: edge network, update function: GRU"
2.2.0,more options to be implemented
2.2.0,Extract atom_features
2.2.0,Add trainable weights
2.2.0,Extract atom_features
2.2.0,Add another value(~-Inf) to prevent error in softmax
2.2.0,Model using this layer must set pad_batches=True
2.2.0,Perform one step of LSTM
2.2.0,Arguments
2.2.0,Aliases.
2.2.0,Layer Management
2.2.0,Singular place to hold Tensor objects which don't serialize
2.2.0,These have to be reconstructed on restoring from pickle
2.2.0,See TensorGraph._get_tf() for more details on lazy construction
2.2.0,In eager mode we want an optimizer and a function to compute the
2.2.0,gradient of the loss.
2.2.0,In graph mode we want a training operation.
2.2.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.2.0,"we try to read more batches than the total number that get queued,"
2.2.0,this thread will hang indefinitely.
2.2.0,"TODO Once we drop Python 2 support, turn outputs into a proper keyword arg"
2.2.0,instead of using the **kwargs hack.
2.2.0,Gather results for each output
2.2.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.2.0,"we try to read more batches than the total number that get queued,"
2.2.0,this thread will hang indefinitely.
2.2.0,"If only one output, just return array"
2.2.0,Adapted from https://github.com/tensorflow/tensorflow/issues/675#issuecomment-319891923.
2.2.0,"The next release of Tensorflow will add a proper jacobian() function, so"
2.2.0,we can remove this then.
2.2.0,"Remove extra dimensions, because I couldn't figure out how to get the"
2.2.0,jacobian() function to not produce them.
2.2.0,"In eager mode, we need to execute every layer once to ensure its variables"
2.2.0,have been created.
2.2.0,"We can't execute Input layers in eager mode, since they would try"
2.2.0,to create placeholders.  Instead create a tensor of the correct
2.2.0,size and type.
2.2.0,Build the layers.
2.2.0,Initialize variables.
2.2.0,In graph mode we need to create the computation graph.
2.2.0,Ensure all training operators have been created.
2.2.0,Initialize variables.
2.2.0,"As a sanity check, make sure all tensors have the correct shape."
2.2.0,Remove out_tensor from the object to be pickled
2.2.0,Pickle itself
2.2.0,add out_tensor back to everyone
2.2.0,The loss doesn't depend on any variables.
2.2.0,Check the inputs.
2.2.0,Define a function that recursively creates tensors from layers.
2.2.0,Define the model function.
2.2.0,Define the inputs.
2.2.0,"Create the correct outputs, based on the mode."
2.2.0,Create the Estimator.
2.2.0,Add or remove dimensions of size 1 to match the shape of the layer.
2.2.0,Should we keep a separate global step count for each submodel?
2.2.0,Add the input features.
2.2.0,Weight decay not activated
2.2.0,Handle output layer
2.2.0,Iterate over all previous tasks.
2.2.0,prev_layers is a list with elements of size
2.2.0,"(batch_size, layer_sizes[i-1])"
2.2.0,use central difference since forward difference has a pretty high
2.2.0,approximation error
2.2.0,assert min_coords[1][0] != new_x[3]
2.2.0,assert min_coords[1][1] != new_x[4]
2.2.0,assert min_coords[1][2] != new_x[5]
2.2.0,Predict the output and uncertainty.
2.2.0,Predict the output and uncertainty.
2.2.0,Predict the output and uncertainty.
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Prepare Training Data
2.2.0,Train the model
2.2.0,Prepare the Testing data
2.2.0,predict
2.2.0,check output shape
2.2.0,new object of UNet to test if loading the model results in same predictions
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"But if we specify a different starting state, that should produce a"
2.2.0,different result.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"But if we specify a different starting state, that should produce a"
2.2.0,different result.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,TODO What should shape[1] be?  It's not documented.
2.2.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"TODO What should the output shape be?  It's not documented, and there"
2.2.0,are no other test cases for it.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,"Creating a second layer should produce different results, since it has"
2.2.0,different random weights.
2.2.0,But evaluating the first layer again should produce the same result as before.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,"For simplicity, let's assume both molecules have same number of"
2.2.0,atoms.
2.2.0,Creates a set of dummy features that contain the coordinate and
2.2.0,neighbor-list features required by the AtomicConvModel.
2.2.0,"frag2_z = np.random.rand(N_atoms, 3)"
2.2.0,"For simplicity, let's assume both molecules have same number of"
2.2.0,atoms.
2.2.0,Creates a set of dummy features that contain the coordinate and
2.2.0,neighbor-list features required by the AtomicConvModel.
2.2.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.2.0,max num atoms instead of exact.
2.2.0,Cutoff in angstroms
2.2.0,arbitrary label
2.2.0,Run a fitting operation
2.2.0,Set by variable constructor.
2.2.0,Set by set_variable_initial_values().
2.2.0,Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
2.2.0,Optimize the main loss.  This should send both variables toward 1.
2.2.0,Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
2.2.0,"If we don't specify the initial state, it should default to zeros."
2.2.0,Explicitly specifying the zero state should give the same result.
2.2.0,Specifying a different initial state should produce a different result.
2.2.0,We should get the same result with either predict_on_batch() or __call__().
2.2.0,Take a tiny step in the direction of s and see if the output changes by
2.2.0,the expected amount.
2.2.0,Test for correct value return (normal mode)
2.2.0,Test for shapes (normal mode)
2.2.0,Test for correct value return (eager mode)
2.2.0,Test for shape (eager mode)
2.2.0,See if it has done a plausible job of learning the distribution.
2.2.0,See if it has done a plausible job of learning the distribution.
2.2.0,We have to set the gradient penalty very small because the generator's
2.2.0,"output is only a single number, so the default penalty would constrain"
2.2.0,it far too much.
2.2.0,See if it has done a plausible job of learning the distribution.
2.2.0,"This isn't a meaningful loss, but just for test"
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.2.0,"Now an (N, M) shape"
2.2.0,TODO(rbharath): Move this into a model directly
2.2.0,def test_vina(self):
2.2.0,"""""""Test that vina graph can be constructed in TensorGraph."""""""
2.2.0,N_protein = 4
2.2.0,N_ligand = 1
2.2.0,N_atoms = 5
2.2.0,M_nbrs = 2
2.2.0,ndim = 3
2.2.0,start = 0
2.2.0,stop = 4
2.2.0,nbr_cutoff = 1
2.2.0,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
2.2.0,start))
2.2.0,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
2.2.0,start))
2.2.0,y = NumpyDataset(np.random.rand(
2.2.0,"1,))"
2.2.0,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
2.2.0,"# used in the current implementation. This is obviously wrong, but need"
2.2.0,# to dig out why this is happening.
2.2.0,"prot_coords = Feature(shape=(N_protein, ndim))"
2.2.0,"ligand_coords = Feature(shape=(N_ligand, ndim))"
2.2.0,"labels = Label(shape=(1,))"
2.2.0,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
2.2.0,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
2.2.0,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
2.2.0,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
2.2.0,"# Now an (N, M) shape"
2.2.0,nbr_list = NeighborList(
2.2.0,"N_protein + N_ligand,"
2.2.0,"M_nbrs,"
2.2.0,"ndim,"
2.2.0,"nbr_cutoff,"
2.2.0,"start,"
2.2.0,"stop,"
2.2.0,in_layers=[coords])
2.2.0,"# Shape (N, M)"
2.2.0,dists = InteratomicL2Distances(
2.2.0,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
2.2.0,repulsion = VinaRepulsion(in_layers=[dists])
2.2.0,hydrophobic = VinaHydrophobic(in_layers=[dists])
2.2.0,hbond = VinaHydrogenBond(in_layers=[dists])
2.2.0,gauss_1 = VinaGaussianFirst(in_layers=[dists])
2.2.0,gauss_2 = VinaGaussianSecond(in_layers=[dists])
2.2.0,"# Shape (N, M)"
2.2.0,interactions = WeightedLinearCombo(
2.2.0,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
2.2.0,"# Shape (N, M)"
2.2.0,"thresholded = Cutoff(in_layers=[dists, interactions])"
2.2.0,"# Shape (N, M)"
2.2.0,free_energies = VinaNonlinearity(in_layers=[thresholded])
2.2.0,free_energy = ReduceSum(in_layers=[free_energies])
2.2.0,"loss = L2Loss(in_layers=[free_energy, labels])"
2.2.0,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
2.2.0,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
2.2.0,tg.set_loss(loss)
2.2.0,tg.fit_generator(databag.iterbatches(epochs=1))
2.2.0,TODO(rbharath): This test should pass. Fix it!
2.2.0,def test_graph_pool(self):
2.2.0,"""""""Test that GraphPool can be invoked."""""""
2.2.0,out_channels = 2
2.2.0,"n_atoms = 4 # In CCC and C, there are 4 atoms"
2.2.0,"raw_smiles = ['CCC', 'C']"
2.2.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.2.0,featurizer = ConvMolFeaturizer()
2.2.0,mols = featurizer.featurize(mols)
2.2.0,multi_mol = ConvMol.agglomerate_mols(mols)
2.2.0,atom_features = multi_mol.get_atom_features()
2.2.0,degree_slice = multi_mol.deg_slice
2.2.0,membership = multi_mol.membership
2.2.0,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
2.2.0,with self.test_session() as sess:
2.2.0,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
2.2.0,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
2.2.0,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
2.2.0,deg_adjs_tf = []
2.2.0,for deg_adj in deg_adjs:
2.2.0,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
2.2.0,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
2.2.0,out_tensor = GraphPool(out_channels)(*args)
2.2.0,sess.run(tf.global_variables_initializer())
2.2.0,out_tensor = out_tensor.eval()
2.2.0,"assert out_tensor.shape == (n_atoms, out_channels)"
2.2.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.2.0,"Layer is wrapper around embedding lookup, tested that then"
2.2.0,Expected output
2.2.0,Create a dataset with three tasks.  The first two tasks each depend only
2.2.0,on half the features.  The third task depends on all of them.
2.2.0,Create an OntologyModel.  Two leaf nodes contain half the features.
2.2.0,Train the model on the datase.
2.2.0,It should have learned to predict all of the tasks accurately.
2.2.0,"In addition, it should be able to predict the first task based only on the"
2.2.0,"first leaf node, and the second task based only on the second leaf node."
2.2.0,Create a dataset with three tasks.  The first two tasks each depend only
2.2.0,on half the features.  The third task depends on all of them.
2.2.0,Create an OntologyModel.  Two leaf nodes contain half the features.
2.2.0,Train the model on the datase.
2.2.0,It should have learned to predict all of the tasks accurately.
2.2.0,"In addition, it should be able to predict the first task based only on the"
2.2.0,"first leaf node, and the second task based only on the second leaf node."
2.2.0,Here are mappings for just a few yeast genes.
2.2.0,"Build the ontology, then see if it looks correct."
2.2.0,Should be able to call fit twice without failure.
2.2.0,# TODO(rbharath): Transform these into useful weights.
2.2.0,#class_weight={
2.2.0,"#    True: num_sequences / num_positives,"
2.2.0,#    False: num_sequences / num_negatives
2.2.0,"#} if not multitask else None,"
2.2.0,# TODO(rbharath): Add a test with per-class weighting.
2.2.0,#class_weight={
2.2.0,"#    True: num_sequences / num_positives,"
2.2.0,#    False: num_sequences / num_negatives
2.2.0,"#} if not multitask else None,"
2.2.0,Prepare Training Data
2.2.0,Train the model
2.2.0,Prepare the Testing data
2.2.0,predict
2.2.0,check output shape
2.2.0,new object of ResNet to test if loading the model results in same predictions
2.2.0,Train the model on random sequences.  We aren't training long enough to
2.2.0,"really make it reliable, but I want to keep this test fast, and it should"
2.2.0,still be able to reproduce a reasonable fraction of input sequences.
2.2.0,Test it out.
2.2.0,Check that it got at least a quarter of them correct.
2.2.0,Test it out.
2.2.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.2.0,"few steps of training to make sure nothing crashes, then check that the"
2.2.0,results are at least internally consistent.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate the model.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate the model.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate the model.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate the model.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create the model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate the model.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate the model.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate results
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Evaluate the model.
2.2.0,Create a dataset and an input function for processing it.
2.2.0,Create a TensorGraph model.
2.2.0,Create an estimator from it.
2.2.0,Train the model.
2.2.0,Construct layers for all nodes.
2.2.0,Create the loss function.
2.2.0,Create inputs for the features.
2.2.0,Create inputs for the children.
2.2.0,Concatenate all inputs together.
2.2.0,Create the output.
2.2.0,"If necessary, download the file defining the ontology."
2.2.0,Parse the ontology definition and create a list of terms.
2.2.0,Create OntologyNode objects for all the terms.
2.2.0,"Assign parent-child relationships between nodes, and identify root nodes."
2.2.0,Create a single root node that combines the three GO roots.
2.2.0,Assign features to nodes.
2.2.0,Count the number of features within each node.  Eliminate nodes with too few
2.2.0,features and set the number of outputs for each one.
2.2.0,Create the inputs.
2.2.0,Create the generators.
2.2.0,Create the discriminators.
2.2.0,Make a copy of the discriminator that takes each generator's output as
2.2.0,its input.
2.2.0,Make a list of all layers in the generators and discriminators.
2.2.0,Compute the loss functions.
2.2.0,Create learnable weights for the generators and discriminators.
2.2.0,Compute the weighted errors
2.2.0,Add an entropy term to the loss.
2.2.0,Create submodels for training the generators and discriminators.
2.2.0,"Every call to fit_generator() will increment global_step, but we only"
2.2.0,"want it to get incremented once for the entire batch, so record the"
2.2.0,value and keep resetting it.
2.2.0,Train the discriminator.
2.2.0,Train the generator.
2.2.0,Write checkpoints and report progress.
2.2.0,Write out final results.
2.2.0,number of atoms in each molecule
2.2.0,index of pair features
2.2.0,number of pairs for each atom
2.2.0,atom features
2.2.0,pair features
2.2.0,calculation orders for a batch of molecules
2.2.0,padding atom features vector of each molecule with 0
2.2.0,Returns:
2.2.0,Build placeholders
2.2.0,number of atoms in each molecule
2.2.0,index of pair features
2.2.0,number of pairs for each atom
2.2.0,atom features
2.2.0,pair features
2.2.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.2.0,import tensorflow as tf
2.2.0,from deepchem.models.tensorgraph.tensor_graph import MultitaskTensorGraph
2.2.0,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
2.2.0,
2.2.0,
2.2.0,class WeightedError(Layer):
2.2.0,
2.2.0,"def __call__(self, *parents):"
2.2.0,"entropy, weights = parents[0], parents[1]"
2.2.0,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
2.2.0,return self.out_tensor
2.2.0,
2.2.0,
2.2.0,"def MultitaskClassifier(n_tasks,"
2.2.0,"n_features,"
2.2.0,"layer_sizes=[500],"
2.2.0,"bypass_layer_sizes=[100],"
2.2.0,model_dir=None):
2.2.0,""""""""
2.2.0,TODO(LESWING) Add Dropout and regularization
2.2.0,
2.2.0,Parameters
2.2.0,----------
2.2.0,n_tasks
2.2.0,n_features
2.2.0,layer_sizes
2.2.0,bypass_layer_sizes
2.2.0,model_dir
2.2.0,
2.2.0,Returns
2.2.0,-------
2.2.0,
2.2.0,""""""""
2.2.0,g = MultitaskTensorGraph(model_dir=model_dir)
2.2.0,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
2.2.0,g.add_layer(in_layer)
2.2.0,g.add_feature(in_layer)
2.2.0,
2.2.0,# Shared Dense Layers
2.2.0,prev_layer = in_layer
2.2.0,dense_layers = []
2.2.0,for i in range(len(layer_sizes)):
2.2.0,dense = Dense(
2.2.0,"out_channels=layer_sizes[i],"
2.2.0,"name=""SDENSE%s"" % i,"
2.2.0,activation_fn=tf.nn.relu)
2.2.0,"g.add_layer(dense, parents=[prev_layer])"
2.2.0,dense_layers.append(dense)
2.2.0,prev_layer = dense
2.2.0,
2.2.0,# Individual Bypass Layers
2.2.0,costs = []
2.2.0,for task in range(n_tasks):
2.2.0,prev_layer = in_layer
2.2.0,for i in range(len(bypass_layer_sizes)):
2.2.0,dense = Dense(
2.2.0,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
2.2.0,"g.add_layer(dense, parents=[prev_layer])"
2.2.0,prev_layer = dense
2.2.0,"joined_layer = Concat(name=""JOIN%s"" % task)"
2.2.0,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
2.2.0,
2.2.0,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
2.2.0,"g.add_layer(classification, parents=[joined_layer])"
2.2.0,
2.2.0,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
2.2.0,"g.add_layer(softmax, parents=[classification])"
2.2.0,g.add_output(softmax)
2.2.0,
2.2.0,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
2.2.0,g.add_layer(label)
2.2.0,g.add_label(label)
2.2.0,
2.2.0,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
2.2.0,"g.add_layer(cost, parents=[label, classification])"
2.2.0,costs.append(cost)
2.2.0,
2.2.0,"entropy = Concat(name=""ENT"")"
2.2.0,"g.add_layer(entropy, parents=costs)"
2.2.0,
2.2.0,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
2.2.0,g.add_layer(task_weights)
2.2.0,g.set_task_weights(task_weights)
2.2.0,
2.2.0,"loss = WeightedError(name=""ERROR"")"
2.2.0,"g.add_layer(loss, parents=[entropy, task_weights])"
2.2.0,g.set_loss(loss)
2.2.0,
2.2.0,return g
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,(ytz): this is really dirty but needed for restoring models
2.2.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.2.0,SMILES strings
2.2.0,Maximum length is expanded to allow length variation during train and inference
2.2.0,'_' served as delimiter and padding
2.2.0,Initialize common characters as keys
2.2.0,Include space to avoid extra keys
2.2.0,"For 'Cl', 'Br', etc."
2.2.0,"Character not recognized, add to extra_keys"
2.2.0,Add all extra_keys to char_dict
2.2.0,Character embedding
2.2.0,Multiple convolutional layers with different filter widths
2.2.0,Max-over-time pooling
2.2.0,Concat features from all filters(one feature per filter)
2.2.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.2.0,Transform SMILES sequence to integers
2.2.0,Skip all spaces
2.2.0,"For 'Cl', 'Br', etc."
2.2.0,Padding with '_'
2.2.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.2.0,TODO: Turning off queue for now. Safe to re-activate?
2.2.0,Do a simple greedy search.
2.2.0,Do a beam search with length normalization.
2.2.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.2.0,This candidate sequence has already been terminated
2.2.0,Consider all possible tokens we could add to this candidate sequence.
2.2.0,update model with best param
2.2.0,Find optimal n_estimators based on original learning_rate
2.2.0,and early_stopping_rounds
2.2.0,"Since test size is 20%, when retrain model to whole data, expect"
2.2.0,n_estimator increased to 1/0.8 = 1.25 time.
2.2.0,Make sure user specified params are in the grid.
2.2.0,Change params back original params
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Check same predictions are made.
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Load trained model
2.2.0,Eval model on train
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Fit trained model
2.2.0,Eval model on train/test
2.2.0,Fit trained model
2.2.0,Eval model on train/test
2.2.0,Test Parameter getting and setting
2.2.0,Fit trained model
2.2.0,Eval model on train/test
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,n_samples = 100
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Load mini log-solubility dataset.
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Load mini log-solubility dataset.
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Load mini log-solubility dataset.
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Load mini log-solubility dataset.
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Load mini log-solubility dataset.
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on train
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Predict the output and uncertainty.
2.2.0,def test_singletask_to_multitask_classification(self):
2.2.0,n_features = 10
2.2.0,n_tasks = 17
2.2.0,tasks = range(n_tasks)
2.2.0,# Define train dataset
2.2.0,n_train = 100
2.2.0,"X_train = np.random.rand(n_train, n_features)"
2.2.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.2.0,w_train = np.ones_like(y_train)
2.2.0,"ids_train = [""C""] * n_train"
2.2.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.2.0,"X_train, y_train, w_train, ids_train)"
2.2.0,# Define test dataset
2.2.0,n_test = 10
2.2.0,"X_test = np.random.rand(n_test, n_features)"
2.2.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.2.0,w_test = np.ones_like(y_test)
2.2.0,"ids_test = [""C""] * n_test"
2.2.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.2.0,"X_test, y_test, w_test, ids_test)"
2.2.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.2.0,def model_builder(model_dir):
2.2.0,sklearn_model = LogisticRegression()
2.2.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.2.0,multitask_model = dc.models.SingletaskToMultitask(
2.2.0,"tasks, model_builder)"
2.2.0,# Fit trained model
2.2.0,multitask_model.fit(train_dataset)
2.2.0,multitask_model.save()
2.2.0,# Eval multitask_model on train/test
2.2.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.2.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.2.0,Generate data
2.2.0,Cleanup
2.2.0,Generate dummy dataset
2.2.0,Fit trained model
2.2.0,Eval model on test
2.2.0,Eval model on train
2.2.0,Fit trained model
2.2.0,Eval model on test
2.2.0,Fit trained model
2.2.0,Eval model on test
2.2.0,def test_sklearn_classification(self):
2.2.0,"""""""Test that sklearn models can learn on simple classification datasets."""""""
2.2.0,np.random.seed(123)
2.2.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.2.0,"X, y = dataset.data, dataset.target"
2.2.0,frac_train = .7
2.2.0,n_samples = len(X)
2.2.0,n_train = int(frac_train*n_samples)
2.2.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.2.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.2.0,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
2.2.0,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
2.2.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.2.0,sklearn_model = LogisticRegression()
2.2.0,model = dc.models.SklearnModel(sklearn_model)
2.2.0,# Fit trained model
2.2.0,model.fit(train_dataset)
2.2.0,model.save()
2.2.0,# Eval model on test
2.2.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.2.0,assert scores[classification_metric.name] > .5
2.2.0,def test_sklearn_multitask_classification(self):
2.2.0,"""""""Test that sklearn models can learn on simple multitask classification."""""""
2.2.0,np.random.seed(123)
2.2.0,n_tasks = 4
2.2.0,tasks = range(n_tasks)
2.2.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.2.0,"X, y = dataset.data, dataset.target"
2.2.0,"y = np.reshape(y, (len(y), 1))"
2.2.0,y = np.hstack([y] * n_tasks)
2.2.0,
2.2.0,frac_train = .7
2.2.0,n_samples = len(X)
2.2.0,n_train = int(frac_train*n_samples)
2.2.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.2.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.2.0,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
2.2.0,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
2.2.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.2.0,def model_builder(model_dir):
2.2.0,sklearn_model = LogisticRegression()
2.2.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.2.0,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
2.2.0,# Fit trained model
2.2.0,model.fit(train_dataset)
2.2.0,model.save()
2.2.0,# Eval model on test
2.2.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.2.0,for score in scores[classification_metric.name]:
2.2.0,assert score > .5
2.2.0,Set early stopping round = n_estimators so that esr won't work
2.2.0,Fit trained model
2.2.0,Eval model on test
2.2.0,Fit trained model
2.2.0,Eval model on test
2.2.0,Fit trained model
2.2.0,Eval model on test
2.2.0,Logistic regression doesn't support weights
2.2.0,-*- coding: utf-8 -*-
2.2.0,Assigning featurizer if not user defined
2.2.0,loading datasets
2.2.0,Assembling train and valid datasets
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,Building tensorflow MultitaskDNN model
2.2.0,Building tensorflow robust MultitaskDNN model
2.2.0,Building scikit logistic regression model
2.2.0,Transform fingerprints to IRV features
2.2.0,Building tensorflow IRV model
2.2.0,Building scikit random forest model
2.2.0,Building scikit learn Kernel SVM model
2.2.0,Building xgboost classification model
2.2.0,Remove token for paddings
2.2.0,Building scikit random forest model
2.2.0,Building scikit learn Kernel Ridge Regression model
2.2.0,Building scikit learn Kernel Ridge Regression model
2.2.0,Building xgboost regression model
2.2.0,Loading hyperparameters
2.2.0,num positive/negative ligands
2.2.0,Set batch sizes for network
2.2.0,Model structure
2.2.0,Traning settings
2.2.0,Fit trained model
2.2.0,Evaluating low data model
2.2.0,-*- coding: utf-8 -*-
2.2.0,Assigning featurizer if not user defined
2.2.0,loading datasets
2.2.0,
2.2.0,Note by @XericZephyr. Reason why I spun off this function:
2.2.0,1. Some model needs dataset information.
2.2.0,2. It offers us possibility to **cache** the dataset
2.2.0,"if the featurizer runs very slow, e.g., GraphConv."
2.2.0,2+. The cache can even happen at Travis CI to accelerate
2.2.0,CI testing.
2.2.0,
2.2.0,loading datasets
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
2.2.0,Featurize qm9 dataset
2.2.0,TODO: Check for this
2.2.0,Download files if they don't exist
2.2.0,Featurize the KINASE dataset
2.2.0,Shuffle the training data
2.2.0,Apply transformations
2.2.0,#### TIMING ######
2.2.0,transformers = [
2.2.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.2.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.2.0,dataset=train_dataset)]
2.2.0,Set shard size low to avoid memory problems.
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,Set some global variables up top
2.2.0,Featurize KAGGLE dataset
2.2.0,############################################################# TIMING
2.2.0,############################################################# TIMING
2.2.0,No tasks since no labels provided.
2.2.0,For now images are loaded directly by ImageLoader
2.2.0,Load Sweetlead dataset
2.2.0,Featurize SWEET dataset
2.2.0,Initialize transformers
2.2.0,Featurize qm7 dataset
2.2.0,Featurize clintox dataset
2.2.0,Transform clintox dataset
2.2.0,Split clintox dataset
2.2.0,Featurize bbb dataset
2.2.0,Initialize transformers
2.2.0,Load nci dataset
2.2.0,Featurize nci dataset
2.2.0,Initialize transformers
2.2.0,Featurize HOPV dataset
2.2.0,Initialize transformers
2.2.0,Featurize PPB dataset
2.2.0,Initialize transformers
2.2.0,Load MUV dataset
2.2.0,Featurize MUV dataset
2.2.0,Initialize transformers
2.2.0,Featurize clearance dataset
2.2.0,Initialize transformers
2.2.0,Featurize BBBC001 dataset
2.2.0,Featurize Images into NumpyArrays
2.2.0,Load text file with labels
2.2.0,Strip the first line which holds field labels
2.2.0,Format is: Image_name count1 count2
2.2.0,This is kludgy way to add y to dataset. Can be done better?
2.2.0,Featurize BBBC002 dataset
2.2.0,Featurize Images into NumpyArrays
2.2.0,Load text file with labels
2.2.0,Strip the first line which holds field labels
2.2.0,Format is: Image_name count1 count2
2.2.0,This is kludgy way to add y to dataset. Can be done better?
2.2.0,Featurize TOXCAST dataset
2.2.0,Initialize transformers
2.2.0,Download files if they don't exist
2.2.0,Featurizing datasets
2.2.0,Missing entry removal
2.2.0,Shuffle the training data
2.2.0,Apply transformations
2.2.0,#### TIMING ###########
2.2.0,Featurize bace dataset
2.2.0,Initialize transformers
2.2.0,Featurize bace dataset
2.2.0,Initialize transformers
2.2.0,Featurize Tox21 dataset
2.2.0,Initialize transformers
2.2.0,Featurize ChEMBL dataset
2.2.0,Initialize transformers
2.2.0,TODO: Check if anything needs to be added
2.2.0,Featurize the FACTORS dataset
2.2.0,Shuffle the training data
2.2.0,Apply transformations
2.2.0,######### TIMING ################
2.2.0,Most reaction dataset ML tasks train the prediction of products from
2.2.0,"ractants. Both of these are contained in the rxn object that is output,"
2.2.0,"so there is no ""tasks"" field."
2.2.0,DeepChem currently has no transformers for reaction data
2.2.0,Download USPTO dataset
2.2.0,Unzip
2.2.0,Unzipped file is a tap seperated values file (despite the .txt)
2.2.0,The first element in the row is the reaction smarts
2.2.0,"Sometimes smarts have extraneous information at end of form """
2.2.0,"|f:0"" that causes parsing to fail. Not sure what this information"
2.2.0,"is, but just ignoring for now."
2.2.0,Make up dummy labels since DiskDataset.from_numpy doesn't allow
2.2.0,creation from just features for now.
2.2.0,TODO: This dataset isn't saved to disk so reload doesn't happen.
2.2.0,Featurize hiv dataset
2.2.0,Initialize transformers
2.2.0,Extract locations of data
2.2.0,Extract labels
2.2.0,Skip comment lines
2.2.0,Lines have format
2.2.0,"PDB code, resolution, release year, -logKd/Ki, Kd/Ki, reference, ligand name"
2.2.0,"The base-10 logarithm, -log kd/pk"
2.2.0,Featurize Data
2.2.0,"Pulled from PDB files. For larger datasets with more PDBs, would use"
2.2.0,max num atoms instead of exact.
2.2.0,Cutoff in angstroms
2.2.0,Cutoff in angstroms
2.2.0,Delete labels for failing elements
2.2.0,No transformations of data
2.2.0,TODO(rbharath): This should be modified to contain a cluster split so
2.2.0,structures of the same protein aren't in both train/test
2.2.0,Featurize SIDER dataset
2.2.0,Initialize transformers
2.2.0,Featurize SAMPL dataset
2.2.0,Initialize transformers
2.2.0,Featurize Delaney dataset
2.2.0,Initialize transformers
2.2.0,Featurize PCBA dataset
2.2.0,Initialize transformers
2.2.0,Featurize Lipophilicity dataset
2.2.0,Initialize transformers
2.2.0,"Float or int hyper parameters(ex. batch_size, learning_rate)"
2.2.0,List of float or int hyper parameters(ex. layer_sizes)
2.2.0,Number of parameters
2.2.0,Range of optimization
2.2.0,Dummy names
2.2.0,Input hyper parameters
2.2.0,Run benchmark
2.2.0,Record hyperparameters
2.2.0,Record performances
2.2.0,"GPGO maximize performance by default, set performance to its negative value for minimization"
2.2.0,Readout best hyper parameters
2.2.0,Compare best model to default hyperparameters
2.2.0,Record hyperparameters
2.2.0,Record performances
2.2.0,"Optimized model is better, return hyperparameters"
2.2.0,Return default hyperparameters
2.2.0,!/usr/bin/env python2
2.2.0,-*- coding: utf-8 -*-
2.2.0,TODO(rbharath): This function is complicated and monolithic. Is there a nice
2.2.0,way to refactor this?
2.2.0,arbitrarily return last model
2.2.0,Define train dataset
2.2.0,Define validation dataset
2.2.0,Have the worker threads generate the rollouts for this iteration.
2.2.0,Perform optimization.
2.2.0,Build the feed dict and run the optimizer.
2.2.0,Update the number of steps taken so far and perform checkpointing.
2.2.0,Merge all the rollouts into a single set of arrays.
2.2.0,Iterate slices.
2.2.0,Generate the rollout.
2.2.0,Compute an estimate of the reward for the rest of the episode.
2.2.0,Compute the discounted rewards and advantages.
2.2.0,Convert the actions to one-hot.
2.2.0,Rearrange the states into the proper set of arrays.
2.2.0,Return the processed arrays.
2.2.0,Generate the rollout.
2.2.0,Compute an estimate of the reward for the rest of the episode.
2.2.0,Compute the discounted rewards and advantages.
2.2.0,"Record the actions, converting to one-hot if necessary."
2.2.0,Rearrange the states into the proper set of arrays.
2.2.0,Build the feed dict and apply gradients.
2.2.0,Run the algorithm.
2.2.0,Save a file checkpoint.
2.2.0,Build the tree.
2.2.0,Compute the final probabilities and expected reward.
2.2.0,Mark this node as terminal
2.2.0,Expand this node.
2.2.0,Select the next action to perform.
2.2.0,Recursively build the tree.
2.2.0,Update statistics for this node.
2.2.0,Assume all arrays are float32.
2.2.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.2.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.2.0,"game).  The average reward for any bet is slightly negative, so the best"
2.2.0,strategy is to walk away.
2.2.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.2.0,Optimize it.
2.2.0,"It should have learned that the expected value is very close to zero, and that the best"
2.2.0,action is to walk away.
2.2.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.2.0,get the same result.
2.2.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.2.0,The environment just has a constant state.
2.2.0,The policy includes a single recurrent layer.
2.2.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.2.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.2.0,"On the first call, the initial state should be all zeros."
2.2.0,It should still be zeros since we didn't save it last time.
2.2.0,It should be different now.
2.2.0,This should be the same as the previous one.
2.2.0,"Now we reset it, so we should get the same result as initially."
2.2.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.2.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.2.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.2.0,at all.  Using hindsight makes it much easier.
2.2.0,A simple policy with two hidden layers.
2.2.0,Optimize it.
2.2.0,Try running it a few times and see if it succeeds.
2.2.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.2.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.2.0,"game).  The average reward for any bet is slightly negative, so the best"
2.2.0,strategy is to walk away.
2.2.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.2.0,Optimize it.
2.2.0,"It should have learned that the expected value is very close to zero, and that the best"
2.2.0,action is to walk away.
2.2.0,"Verify that we can create a new A3C object, reload the parameters from the first one, and"
2.2.0,get the same result.
2.2.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.2.0,The environment just has a constant state.
2.2.0,The policy includes a single recurrent layer.
2.2.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.2.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.2.0,"On the first call, the initial state should be all zeros."
2.2.0,It should still be zeros since we didn't save it last time.
2.2.0,It should be different now.
2.2.0,This should be the same as the previous one.
2.2.0,"Now we reset it, so we should get the same result as initially."
2.2.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.2.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.2.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.2.0,at all.  Using hindsight makes it much easier.
2.2.0,A simple policy with two hidden layers.
2.2.0,Optimize it.
2.2.0,Try running it a few times and see if it succeeds.
2.2.0,The state consists of two numbers: a current value and a target value.
2.2.0,The policy just needs to learn to output the target value (or at least
2.2.0,move toward it).
2.2.0,A simple policy with no hidden layers.
2.2.0,Optimize it.
2.2.0,Try running it and see if it reaches the target
2.2.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.2.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.2.0,"game).  The average reward for any bet is slightly negative, so the best"
2.2.0,strategy is to walk away.
2.2.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.2.0,Optimize it.
2.2.0,"It should have learned that the expected value is very close to zero, and that the best"
2.2.0,action is to walk away.
2.2.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.2.0,get the same result.
2.2.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.2.0,Randomize who goes first
2.2.0,Illegal move -- the square is not empty
2.2.0,Move X
2.2.0,Did X Win
2.2.0,Did O Win
2.2.0,TODO (Bowen): make this function less memory intensive
2.2.0,set 1st column as the column index of dataframe
2.2.0,merge descriptor and activities dataframe into output dataframe based on
2.2.0,"the molecule name, which is the index for both dataframes (but named"
2.2.0,differently). Default merge is inner merge
2.2.0,need to manually set dataframe indexname after merge based on index
2.2.0,from deepchem.scripts.dock_dude import *
2.2.0,from ipyparallel import Client
2.2.0,rc = Client()
2.2.0,dview = rc[:]
2.2.0,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
2.2.0,
2.2.0,"If mol_id is not set, then use isomeric smiles as unique identifier"
2.2.0,iterator = data_df.iterrows()
2.2.0,TODO(rbharath): BROKEN!
2.2.0,Trim unwanted indexing fields
2.2.0,Connect to running ipython server
2.2.0,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
2.2.0,
2.2.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.2.0,you may not use this file except in compliance with the License.
2.2.0,You may obtain a copy of the License at
2.2.0,
2.2.0,http://www.apache.org/licenses/LICENSE-2.0
2.2.0,
2.2.0,"Unless required by applicable law or agreed to in writing, software"
2.2.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.2.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.2.0,See the License for the specific language governing permissions and
2.2.0,limitations under the License.
2.2.0,==============================================================================
2.2.0,Maps from a function name to a dictionary that describes how to
2.2.0,map from an old argument keyword to the new argument keyword.
2.2.0,Mapping from function to the new name of the function
2.2.0,Functions that were reordered should be changed to the new keyword args
2.2.0,"for safety, if positional arguments are used. If you have reversed the"
2.2.0,"positional arguments yourself, this could do the wrong thing."
2.2.0,Specially handled functions.
2.2.0,TODO(aselle): Could check for a literal list of bools and try to convert
2.2.0,them to indices.
2.2.0,all edits are lists of chars
2.2.0,Iterate of each line
2.2.0,sort by column so that edits are processed in order in order to make
2.2.0,indexing adjustments cumulative for changes that change the string
2.2.0,length
2.2.0,"Extract each line to a list of characters, because mutable lists"
2.2.0,"are editable, unlike immutable strings."
2.2.0,Record a description of the change
2.2.0,Make underscore buffers for underlining where in the line the edit was
2.2.0,Iterate for each edit
2.2.0,"Create effective start, end by accounting for change in length due"
2.2.0,to previous edits
2.2.0,Make sure the edit is changing what it should be changing
2.2.0,Make the edit
2.2.0,Create the underline highlighting of the before and after
2.2.0,Keep track of how to generate effective ranges
2.2.0,Finish the report comment
2.2.0,"Strangely, ast.ListComp returns the col_offset of the first token"
2.2.0,after the '[' token which appears to be a bug. Workaround by
2.2.0,explicitly finding the real start of the list comprehension.
2.2.0,loop over lines
2.2.0,Reverse the text to and regular expression search for whitespace
2.2.0,First find if a [ can be found with only whitespace between it and
2.2.0,col.
2.2.0,TODO(aselle):
2.2.0,"this is poor comment detection, but it is good enough for"
2.2.0,cases where the comment does not contain string literal starting/
2.2.0,ending characters. If ast gave us start and end locations of the
2.2.0,"ast nodes rather than just start, we could use string literal"
2.2.0,node ranges to filter out spurious #'s that appear in string
2.2.0,literals.
2.2.0,"Most other nodes return proper locations (with notably does not), but"
2.2.0,it is not possible to use that in an argument.
2.2.0,"Find a simple attribute name path e.g. ""tf.foo.bar"""
2.2.0,Make sure the func is marked as being part of a call
2.2.0,Call special handlers
2.2.0,Examine any non-keyword argument and make it into a keyword argument
2.2.0,if reordering required.
2.2.0,Examine each keyword argument and convert it to the final renamed form
2.2.0,TODO(aselle): We should scan backward to find the start of the
2.2.0,keyword key. Unfortunately ast does not give you the location of
2.2.0,"keyword keys, so we are forced to infer it from the keyword arg"
2.2.0,value.
2.2.0,"Write to a temporary file, just in case we are doing an implace modify."
2.2.0,Broad exceptions are required here because ast throws whatever it wants.
2.2.0,pylint: disable=broad-except
2.2.0,pylint: enable=broad-except
2.2.0,make sure output directory doesn't exist
2.2.0,make sure output directory does not overlap with root_directory
2.2.0,Collect list of files to process (we do this to correctly handle if the
2.2.0,user puts the output directory in some sub directory of the input dir)
2.2.0,import os
2.2.0,"from deepchem.utils.save import load_from_disk, save_to_disk"
2.2.0,from deepchem.featurizers.fingerprints import CircularFingerprint
2.2.0,from deepchem.featurizers.basic import RDKitDescriptors
2.2.0,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
2.2.0,from deepchem.featurizers.grid_featurizer import GridFeaturizer
2.2.0,from deepchem.featurizers.featurize import DataLoader
2.2.0,
2.2.0,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
2.2.0,"print(""About to load dataset form disk."")"
2.2.0,dataset = load_from_disk(dataset_file)
2.2.0,"print(""Loaded dataset."")"
2.2.0,
2.2.0,grid_featurizer = GridFeaturizer(
2.2.0,"voxel_width=16.0, feature_types=""voxel_combined"","
2.2.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.2.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
2.2.0,"parallel=True, flatten=True)"
2.2.0,featurizers = [CircularFingerprint(size=1024)]
2.2.0,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
2.2.0,
2.2.0,#Make a directory in which to store the featurized complexes.
2.2.0,"base_dir = ""../../../grid_nnscore_circular_features"""
2.2.0,if not os.path.exists(base_dir):
2.2.0,os.makedirs(base_dir)
2.2.0,"data_dir = os.path.join(base_dir, ""data"")"
2.2.0,if not os.path.exists(data_dir):
2.2.0,os.makedirs(data_dir)
2.2.0,
2.2.0,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
2.2.0,
2.2.0,"feature_dir = os.path.join(base_dir, ""features"")"
2.2.0,if not os.path.exists(feature_dir):
2.2.0,os.makedirs(feature_dir)
2.2.0,
2.2.0,"samples_dir = os.path.join(base_dir, ""samples"")"
2.2.0,if not os.path.exists(samples_dir):
2.2.0,os.makedirs(samples_dir)
2.2.0,
2.2.0,
2.2.0,
2.2.0,featurizers = compound_featurizers + complex_featurizers
2.2.0,"featurizer = DataLoader(tasks=[""label""],"
2.2.0,"smiles_field=""smiles"","
2.2.0,"protein_pdb_field=""protein_pdb"","
2.2.0,"ligand_pdb_field=""ligand_pdb"","
2.2.0,"compound_featurizers=compound_featurizers,"
2.2.0,"complex_featurizers=complex_featurizers,"
2.2.0,"id_field=""complex_id"","
2.2.0,verbose=False)
2.2.0,from ipyparallel import Client
2.2.0,c = Client()
2.2.0,"print(""c.ids"")"
2.2.0,print(c.ids)
2.2.0,dview = c[:]
2.2.0,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
2.2.0,"worker_pool=dview, shard_size=1024)"
2.2.0,
2.2.0,"save_to_disk(featurized_samples, featurized_samples_file)"
2.2.0,"print(""Preparing ligand %s"" % mol_name)"
2.1.0,!/usr/bin/env python3
2.1.0,-*- coding: utf-8 -*-
2.1.0,Datasets and models used in the benchmark test
2.1.0,"irv, rf, rf_regression should be assigned manually"
2.1.0,Evaluate performances with different training set fraction
2.1.0,Datasets and models used in the benchmark test
2.1.0,Uncomment the two lines below if hyper_parameters are provided
2.1.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.1.0,hyper_parameters = pickle.load(f)
2.1.0,Will raise a CalledProcessError if fails.
2.1.0,!/usr/bin/env python3
2.1.0,-*- coding: utf-8 -*-
2.1.0,Datasets and models used in the benchmark test
2.1.0,Set numpy seed
2.1.0,##Load data###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Featurize Kinase dataset
2.1.0,##Load data###
2.1.0,num_trials = 5
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,Force matplotlib to not use any Xwindows backend.
2.1.0,##Load data###
2.1.0,the histogram of the data
2.1.0,Set numpy seed
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,num_trials = 5
2.1.0,Set some global variables up top
2.1.0,Fit trained model
2.1.0,Featurize PCBA dataset
2.1.0,Initialize transformers
2.1.0,Fit trained model
2.1.0,Load sider models now
2.1.0,Load sweetlead dataset now. Pass in dataset object and appropriate
2.1.0,transformers to predict functions
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,##Load data###
2.1.0,"n_estimators=100, max_features=int(num_features/3),"
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Load tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Featurize FACTORS dataset
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,Force matplotlib to not use any Xwindows backend.
2.1.0,##Load data###
2.1.0,the histogram of the data
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Load QM7 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Batch size of models
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Fit trained model
2.1.0,Batch size of models
2.1.0,Fit models
2.1.0,Load Tox21 dataset
2.1.0,Batch size of models
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load QM8 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Load Tox21 dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Fit trained model
2.1.0,Set numpy seed
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,Load ChEMBL dataset
2.1.0,Fit models
2.1.0,Do setup required for tf/keras models
2.1.0,Number of features on conv-mols
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,DeepCrystal Technologies 2017 - Patrick Hop
2.1.0,MIT License - have fun!!
2.1.0,Set to higher values to get better numbers
2.1.0,======================================================================
2.1.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,Only for debug!
2.1.0,Load Delaney dataset
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Do setup required for tf/keras models
2.1.0,Number of features on conv-mols
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load Delaney dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load MUV dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Evaluate train/test scores
2.1.0,Load MUV data
2.1.0,Build model
2.1.0,Fit trained model
2.1.0,Evaluate train/test scores
2.1.0,Extract active site
2.1.0,Featurize ligand
2.1.0,Default for CircularFingerprint
2.1.0,Featurize pocket
2.1.0,Note broadcast operation
2.1.0,Compute labels for pockets
2.1.0,Some complexes have labels but no PDB files. Filter these manually
2.1.0,Some of the ligand-names are of form (FMN ox). Use regex
2.1.0,to merge into form (FMN-ox)
2.1.0,Filter if missing PDB files
2.1.0,Load PDBBind dataset
2.1.0,Define featurizers
2.1.0,Featurize Dataset
2.1.0,########################################################## DEBUG
2.1.0,########################################################## DEBUG
2.1.0,For stable runs
2.1.0,Fit trained model
2.1.0,4-fold splits
2.1.0,num positive/negative ligands
2.1.0,10 trials on test-set
2.1.0,Define metric
2.1.0,Get supports on test-set
2.1.0,Compute accuracies
2.1.0,Number of features on conv-mols
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Test model
2.1.0,Join information for all tasks.
2.1.0,4-fold splits
2.1.0,num positive/negative ligands
2.1.0,Define metric
2.1.0,Get supports on test-set
2.1.0,Compute accuracies
2.1.0,Number of features on conv-mols
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Test model
2.1.0,Join information for all tasks.
2.1.0,4-fold splits
2.1.0,num positive/negative ligands
2.1.0,10 trials on test-set
2.1.0,Define metric
2.1.0,Get supports on test-set
2.1.0,Compute accuracies
2.1.0,Number of features on conv-mols
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Test model
2.1.0,Join information for all tasks.
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,number positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply an attention lstm layer
2.1.0,Number of folds for split
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,4-fold splits
2.1.0,num positive/negative ligands
2.1.0,10 trials on test-set
2.1.0,Define metric
2.1.0,Get supports on test-set
2.1.0,Compute accuracies
2.1.0,Train model on support
2.1.0,Test model
2.1.0,Join information for all tasks.
2.1.0,4-fold splits
2.1.0,num positive/negative ligands
2.1.0,10 trials on test-set
2.1.0,Define metric
2.1.0,Get supports on test-set
2.1.0,Compute accuracies
2.1.0,Train model on support
2.1.0,Test model
2.1.0,Join information for all tasks.
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,number positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply an attention lstm layer
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,number positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply an attention lstm layer
2.1.0,Number of folds for split
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Number of folds for split
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply a residual lstm layer
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply a residual lstm layer
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply a residual lstm layer
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply a residual lstm layer
2.1.0,Set some global variables up top
2.1.0,Featurize Tox21 dataset
2.1.0,Initialize transformers
2.1.0,Set some global variables up top
2.1.0,Featurize Tox21 dataset
2.1.0,Initialize transformers
2.1.0,Load MUV dataset
2.1.0,Featurize MUV dataset
2.1.0,Initialize transformers
2.1.0,Load MUV dataset
2.1.0,Featurize MUV dataset
2.1.0,Initialize transformers
2.1.0,Featurize SIDER dataset
2.1.0,Initialize transformers
2.1.0,Featurize SIDER dataset
2.1.0,Initialize transformers
2.1.0,Load the data.
2.1.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.1.0,sparse: most tasks do not include data for most molecules.  It also is very
2.1.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.1.0,create a list of alternating postives and negatives so each batch will have
2.1.0,equal numbers of both.
2.1.0,Create the model to train.  We use a simple fully connected network with
2.1.0,one hidden layer.
2.1.0,Define a MetaLearner describing the learning problem.
2.1.0,Run meta-learning on 80% of the tasks.
2.1.0,Validate on the remaining tasks.
2.1.0,Number of folds for split
2.1.0,Depth of attention module
2.1.0,number positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,Apply an attention lstm layer
2.1.0,4-fold splits
2.1.0,10 positive/negative ligands
2.1.0,10 trials on test-set
2.1.0,Sample supports without replacement (all pos/neg should be different)
2.1.0,Define metric
2.1.0,Get supports on test-set
2.1.0,Compute accuracies
2.1.0,Train model on support
2.1.0,Test model
2.1.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.1.0,Join information for all tasks.
2.1.0,Number of folds for split
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Number of features on conv-mols
2.1.0,Define metric
2.1.0,Train support model on train
2.1.0,Add layers
2.1.0,4-fold splits
2.1.0,num positive/negative ligands
2.1.0,Define metric
2.1.0,Get supports on test-set
2.1.0,Compute accuracies
2.1.0,Train model on support
2.1.0,Test model
2.1.0,Join information for all tasks.
2.1.0,replace with your own scratch directory
2.1.0,Number of conformations in each file increases exponentially.
2.1.0,Start with a smaller dataset before continuing. Use all of them
2.1.0,for production
2.1.0,"'ani_gdb_s03.h5',"
2.1.0,"'ani_gdb_s04.h5',"
2.1.0,"'ani_gdb_s05.h5',"
2.1.0,"'ani_gdb_s06.h5',"
2.1.0,"'ani_gdb_s07.h5',"
2.1.0,'ani_gdb_s08.h5'
2.1.0,Extract the data
2.1.0,Print the data
2.1.0,self-interaction energies taken from
2.1.0,https://github.com/isayev/ANI1_dataset README
2.1.0,flush once more at the end
2.1.0,"# For production, set nb_epoch to 100+"
2.1.0,"print(""Train scores"")"
2.1.0,print(train_scores)
2.1.0,"print(""Minimization of a single test set structure:"")"
2.1.0,"print(model.minimize_structure(coords, atomic_nums))"
2.1.0,Written by Roman Zubatyuk and Justin S. Smith
2.1.0,Modified by Yutong Zhao to make python2 compatible
2.1.0,opening file
2.1.0,print(store_loc)
2.1.0,print(type(v[0]))
2.1.0,print(k)
2.1.0,print(path)
2.1.0,Number of conformations in each file increases exponentially.
2.1.0,Start with a smaller dataset before continuing. Use all of them
2.1.0,for production
2.1.0,Extract the data
2.1.0,NOTE THE RENAMING:
2.1.0,Note sensitivity = recall
2.1.0,Load nci dataset
2.1.0,Featurize nci dataset
2.1.0,Initialize transformers
2.1.0,Set some global variables up top
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load hiv dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load hiv dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Fit trained model
2.1.0,Fit models
2.1.0,Batch size of models
2.1.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.1.0,Fit trained model
2.1.0,Load SIDER dataset
2.1.0,Featurize SIDER dataset
2.1.0,Initialize transformers
2.1.0,Featurize permeability dataset
2.1.0,Load Tox21 dataset
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load SAMPL dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load SAMPL(FreeSolv) dataset
2.1.0,Define metric
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load clintox dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load clintox dataset
2.1.0,Fit models
2.1.0,Do setup required for tf/keras models
2.1.0,Number of features on conv-mols
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,-*- coding: utf-8 -*-
2.1.0,#############################################################################
2.1.0,## save dataset
2.1.0,#############################################################################
2.1.0,## load datasets
2.1.0,load sweetfda
2.1.0,load aact
2.1.0,## fixup smiles for matching
2.1.0,return smiles
2.1.0,map original smiles to converted smiles
2.1.0,"## join dataframes, index on smiles"
2.1.0,map original smiles back
2.1.0,## fill all nan with 0
2.1.0,## construct datasets
2.1.0,store in new datasets
2.1.0,## save datasets
2.1.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.1.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.1.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.1.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.1.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.1.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.1.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.1.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.1.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.1.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.1.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.1.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.1.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.1.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.1.0,For stable runs
2.1.0,Fit trained model
2.1.0,For stable runs
2.1.0,Fit trained model
2.1.0,transformers = [
2.1.0,"dc.trans.LogTransformer(transform_X=True),"
2.1.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.1.0,dataset=train_dataset)]
2.1.0,Featurize UV dataset
2.1.0,##Load data###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Set numpy seed
2.1.0,##Load data###
2.1.0,##Create model###
2.1.0,Use R2 classification metric
2.1.0,Only use for final evaluation
2.1.0,Force matplotlib to not use any Xwindows backend.
2.1.0,##Load data###
2.1.0,the histogram of the data
2.1.0,##Load data###
2.1.0,###################################################### DEBUG
2.1.0,###################################################### DEBUG
2.1.0,Load HOPV dataset
2.1.0,Fit models
2.1.0,Number of features on conv-mols
2.1.0,Batch size of models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load HOPV dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load HOPV dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load HOPV dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Only for debug!
2.1.0,Load HOPV dataset
2.1.0,Fit models
2.1.0,Fit trained model
2.1.0,Load TOXCAST dataset
2.1.0,Featurize TOXCAST dataset
2.1.0,Initialize transformers
2.1.0,Fit trained model
2.1.0,Processing of ToxCast data
2.1.0,Author - Aneesh Pappu
2.1.0,Loading dataframes and editing indices
2.1.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.1.0,get corresponding casn
2.1.0,get corresponding smiles
2.1.0,write to cell
2.1.0,Tidy up and write to csv
2.1.0,TODO(rbharath): Check that this operation is differentiable.
2.1.0,The number of cells which we should theoretically have
2.1.0,The number of cells which we should theoretically have
2.1.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.1.0,The number of cells which we should theoretically have
2.1.0,TODO(rbharath): The test below only checks that shapes work out.
2.1.0,Need to do a correctness implementation vs. a simple CPU impl.
2.1.0,The number of cells which we should theoretically have
2.1.0,TODO(rbharath): The test below only checks that shapes work out.
2.1.0,Need to do a correctness implementation vs. a simple CPU impl.
2.1.0,The number of cells which we should theoretically have
2.1.0,TODO(rbharath): The test below only checks that shapes work out.
2.1.0,Need to do a correctness implementation vs. a simple CPU impl.
2.1.0,TODO(rbharath): Commenting this out due to weird segfaults
2.1.0,def test_vina_generate_conformers(self):
2.1.0,"""""""Test that Vina Model can generate conformers"""""""
2.1.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.1.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.1.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.1.0,max_protein_atoms = 3500
2.1.0,max_ligand_atoms = 100
2.1.0,"print(""Loading protein file"")"
2.1.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.1.0,protein_Z = pad_array(
2.1.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.1.0,max_protein_atoms)
2.1.0,"print(""Loading ligand file"")"
2.1.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.1.0,ligand_Z = pad_array(
2.1.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.1.0,max_ligand_atoms)
2.1.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.1.0,"Shape (n_cells, k)"
2.1.0,"Shape (N, 1)"
2.1.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.1.0,"conditions, so does wrapround. O(constant)"
2.1.0,"Shape (n_cells, 26)"
2.1.0,"Shape (N, 26)"
2.1.0,"coords of shape (N, ndim)"
2.1.0,"Shape (N, 26, k, ndim)"
2.1.0,"Shape (N, 26, k)"
2.1.0,"Shape (N, 26, k)"
2.1.0,"Shape (N, 26, k, ndim)"
2.1.0,"For smaller systems especially, the periodic boundary conditions can"
2.1.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.1.0,make sure duplicate neighbors are ignored?
2.1.0,TODO(rbharath): How does distance need to be modified here to
2.1.0,account for periodic boundary conditions?
2.1.0,"Shape (N, 26, k)"
2.1.0,"Shape (N, 26*k)"
2.1.0,TODO(rbharath): This will cause an issue with duplicates!
2.1.0,"Shape (N, M)"
2.1.0,"N elts of size (M,) each"
2.1.0,"Shape (N, 26*k)"
2.1.0,"N elts of size (26*k,) each"
2.1.0,"N elts of size (M,) each"
2.1.0,"Shape (N, M)"
2.1.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.1.0,"N tensors of shape (n_cells, 1)"
2.1.0,"Shape (N*n_cells, 1) after tile"
2.1.0,"List of N tensors of shape (n_cells, 1)"
2.1.0,Lists of length N
2.1.0,Lists of length n_cells
2.1.0,Get indices of k atoms closest to each cell point
2.1.0,TODO(rbharath): tf.stack for tf 1.0
2.1.0,"Tensor of shape (n_cells, k, ndim)"
2.1.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.1.0,"Tensor of shape (26, k, ndim)"
2.1.0,"Reshape to (26*k, ndim)"
2.1.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.1.0,"Dists of shape (26*k, 1)"
2.1.0,"Of shape (k, ndim)"
2.1.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.1.0,TODO(rbharath): Change this for tf 1.0
2.1.0,"n_cells tensors of shape (N, 1)"
2.1.0,"Shape (N*n_cells, 1) after tile"
2.1.0,"List of n_cells tensors of shape (N, 1)"
2.1.0,Lists of length n_cells
2.1.0,Lists of length n_cells
2.1.0,Get indices of k atoms closest to each cell point
2.1.0,"n_cells tensors of shape (k, ndim)"
2.1.0,"Tensor of shape (n_cells, k)"
2.1.0,TODO(rbharath):
2.1.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.1.0,- Need to group closest atoms amongst cell neighbors
2.1.0,- Need to do another top_k to find indices of closest neighbors.
2.1.0,- Return N lists corresponding to neighbors for every atom.
2.1.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.1.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.1.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.1.0,the cube.
2.1.0,Number of neighbors of central cube in 3-space is
2.1.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.1.0,TODO(rbharath)
2.1.0,n_cells = int(cells.get_shape()[0])
2.1.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.1.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.1.0,"Tile (a, a, a, b, b, b, etc.)"
2.1.0,"Tile (a, b, c, a, b, c, ...)"
2.1.0,"Lists of n_cells tensors of shape (N, 1)"
2.1.0,Lists of length n_cells
2.1.0,Lists of length n_cells
2.1.0,Get indices of k atoms closest to each cell point
2.1.0,"n_cells tensors of shape (26,)"
2.1.0,TODO(rbharath): Make this handle minibatches
2.1.0,"Shape (N_protein+N_ligand, 3)"
2.1.0,"Shape (N_protein+N_ligand,)"
2.1.0,"Shape (N_protein+N_ligand, M)"
2.1.0,"Shape (N_protein+N_ligand,)"
2.1.0,"Shape (N_protein+N_ligand, 3)"
2.1.0,"Shape (N_protein+N_ligand,)"
2.1.0,"Shape (N_protein+N_ligand, M)"
2.1.0,"Shape (N_protein+N_ligand, M, 3)"
2.1.0,"Shape (N_protein+N_ligand, M)"
2.1.0,"Shape (N_protein+N_ligand, M, 3)"
2.1.0,"Shape (N_protein+N_ligand, M)"
2.1.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.1.0,"Shape (N_protein+N_ligand, M)"
2.1.0,"Shape (N_protein+N_ligand, M)"
2.1.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.1.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.1.0,computing free-energy. This implementation currently uses all interaction
2.1.0,terms. Not sure if this makes a difference.
2.1.0,"Shape (N_protein+N_ligand, M)"
2.1.0,Shape () -- scalar
2.1.0,Keep track of the layers
2.1.0,"For graphical layers, add connectivity placeholders"
2.1.0,Add layer to the layer list
2.1.0,Keep track of the layers
2.1.0,Create graph topology and x
2.1.0,Keep track of the layers
2.1.0,Whether or not we have used the GraphGather layer yet
2.1.0,Update new value of x
2.1.0,Update new value of x
2.1.0,Update new value of x
2.1.0,Get train function
2.1.0,Initialize
2.1.0,################################################################### DEBUG
2.1.0,self.test_label_placeholder = Input(
2.1.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.1.0,"name=""label_placeholder""))"
2.1.0,self.test_weight_placeholder = Input(
2.1.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.1.0,"name=""weight_placeholder""))"
2.1.0,TODO(rbharath): Should weights for the support be used?
2.1.0,Support labels
2.1.0,self.support_label_placeholder = Input(
2.1.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.1.0,"name=""support_label_placeholder""))"
2.1.0,################################################################### DEBUG
2.1.0,Generate dictionary elements for support
2.1.0,Get graph information for test
2.1.0,Generate dictionary elements for test
2.1.0,Perform the optimization
2.1.0,Create different support sets
2.1.0,Get batch to try it out on
2.1.0,"Train on support set, batch pair"
2.1.0,Get featurization for test
2.1.0,"Shape (n_test, n_feat)"
2.1.0,Get featurization for support
2.1.0,"Shape (n_support, n_feat)"
2.1.0,Computes the inner part c() of the kernel
2.1.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.1.0,Normalize
2.1.0,TODO(rbharath): euclidean kernel is broken!
2.1.0,elif self.similarity == 'euclidean':
2.1.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.1.0,"Note that gram matrix g has shape (n_test, n_support)"
2.1.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.1.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.1.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.1.0,each test entry) to get attention vector
2.1.0,"Shape (n_test, n_support)"
2.1.0,Weighted sum of support labels
2.1.0,"Shape (n_support, 1)"
2.1.0,pred is yhat in eqn (1) of Matching Networks.
2.1.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.1.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.1.0,"Shape (n_test,)"
2.1.0,Convert to logit space using inverse sigmoid (logit) function
2.1.0,logit function: log(pred) - log(1-pred)
2.1.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.1.0,in Cross Entropy calculation.
2.1.0,"Shape (n_test,)"
2.1.0,Get scores
2.1.0,Remove padded elements
2.1.0,Get scores
2.1.0,pred corresponds to prob(example == 1)
2.1.0,Remove padded elements
2.1.0,Get batches
2.1.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.1.0,multitask case with missing data...
2.1.0,Join information for all tasks.
2.1.0,TODO(rbharath): Find a way to get rid of this import?
2.1.0,Extract model info
2.1.0,Get graph topology for x
2.1.0,Building outputs
2.1.0,Set epsilon
2.1.0,Initialize
2.1.0,"Path to save checkpoint files, which matches the"
2.1.0,replicated supervisor's default path.
2.1.0,Create target inputs
2.1.0,Get train function
2.1.0,TODO(rbharath): I believe this is total amount of data
2.1.0,Get graph information
2.1.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.1.0,the number of labeled data points in target_i. This is to normalize each task
2.1.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.1.0,Get other optimizer information
2.1.0,TODO(rbharath): Figure out how to handle phase appropriately
2.1.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.1.0,"tensors of shape (batch_size,)"
2.1.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.1.0,examples (effect averages out)
2.1.0,Perform the optimization
2.1.0,TODO(rbharath): Disabling saving for now to try to debug.
2.1.0,run eval data through the model
2.1.0,"Shape (n_samples, n_tasks)"
2.1.0,Create target inputs
2.1.0,TODO(rbharath): Find a way to get rid of this import?
2.1.0,Obtain appropriate loss function
2.1.0,Extract model info
2.1.0,Get graph topology for x
2.1.0,Raw logit outputs
2.1.0,Set epsilon
2.1.0,Initialize
2.1.0,"Path to save checkpoint files, which matches the"
2.1.0,replicated supervisor's default path.
2.1.0,Create target inputs
2.1.0,############################################################### DEBUG
2.1.0,"print(""multitask classifier"")"
2.1.0,"print(""feat"")"
2.1.0,print(feat)
2.1.0,############################################################### DEBUG
2.1.0,Get train function
2.1.0,TODO(rbharath): I believe this is total amount of data
2.1.0,Get graph information
2.1.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.1.0,the number of labeled data points in target_i. This is to normalize each task
2.1.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.1.0,Get other optimizer information
2.1.0,TODO(rbharath): Figure out how to handle phase appropriately
2.1.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.1.0,"tensors of shape (batch_size,)"
2.1.0,Convert the labels into one-hot vector encodings.
2.1.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.1.0,un-softmaxed logits rather than softmax outputs.
2.1.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.1.0,examples (effect averages out)
2.1.0,Perform the optimization
2.1.0,TODO(rbharath): Disabling saving for now to try to debug.
2.1.0,run eval data through the model
2.1.0,"Shape (n_samples, n_tasks)"
2.1.0,run eval data through the model
2.1.0,self.n_atoms = n_atoms
2.1.0,Define the list of tensors to be used as topology
2.1.0,Merge mol conv objects
2.1.0,Generate dicts
2.1.0,Define the list of tensors to be used as topology
2.1.0,Extract atom numbers
2.1.0,Generate dicts
2.1.0,molecule * atom(graph) => step => features
2.1.0,molecule * atom(graph) => step
2.1.0,molecule * atom(graph) => step
2.1.0,Define the list of tensors to be used as topology
2.1.0,calculation orders for a batch of molecules
2.1.0,padding atom features vector of each molecule with 0
2.1.0,self.n_atoms = n_atoms
2.1.0,Define the list of tensors to be used as topology
2.1.0,Extract atom numbers
2.1.0,Generate dicts
2.1.0,self.n_atoms = n_atoms
2.1.0,Define the list of tensors to be used as topology
2.1.0,Extract atom numbers
2.1.0,number of atoms in each molecule
2.1.0,index of pair features
2.1.0,number of pairs for each atom
2.1.0,atom features
2.1.0,pair features
2.1.0,Generate dicts
2.1.0,# Gather Projection
2.1.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.1.0,There should be 8 layers in graph_model
2.1.0,assert len(graph_model.layers) == 6
2.1.0,Add layers
2.1.0,Need to add batch-norm separately to test/support due to differing
2.1.0,shapes.
2.1.0,Apply an attention lstm layer
2.1.0,Gather Projection
2.1.0,Add layers
2.1.0,Need to add batch-norm separately to test/support due to differing
2.1.0,shapes.
2.1.0,Apply an attention lstm layer
2.1.0,Gather Projection
2.1.0,Degrees from 1 to max_deg inclusive
2.1.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.1.0,"Should have shape (?, deg)"
2.1.0,"Shape of atom_features should be (?, n_feat)"
2.1.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.1.0,-*- coding: utf-8 -*-
2.1.0,Save hyperparameters
2.1.0,-*- coding: utf-8 -*-
2.1.0,Save hyperparameters
2.1.0,setup optimizer
2.1.0,setup optimizer
2.1.0,"print(""tasK: %d"" %task)"
2.1.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.1.0,"print(""scores"")"
2.1.0,print(scores.size())
2.1.0,"print(""task_label"")"
2.1.0,print(task_label.size())
2.1.0,"task_loss =  self.criterion(scores, task_label)"
2.1.0,"print(""task_loss"")"
2.1.0,print(task_loss.size())
2.1.0,-*- coding: utf-8 -*-
2.1.0,Save hyperparameters
2.1.0,weight decay
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Turns out there are valid cases where we don't want pad-batches
2.1.0,on by default.
2.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.1.0,Run training op.
2.1.0,############################################################# TIMING
2.1.0,The iterbatches does padding with zero-weight examples on the last batch.
2.1.0,Remove padded examples.
2.1.0,Special case to handle singletasks.
2.1.0,The iterbatches does padding with zero-weight examples on the last batch.
2.1.0,Remove padded examples.
2.1.0,References
2.1.0,Arguments
2.1.0,Aliases.
2.1.0,Aliases.
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,TODO(rbharath): This class does not yet have a
2.1.0,"TensorGraph equivalent, but one may not be required."
2.1.0,"Commented out for now, remove if OK."
2.1.0,class AlternateWeaveLayer(WeaveLayer):
2.1.0,""""""" Alternate implementation of weave module"
2.1.0,"same variables, different graph structures"
2.1.0,""""""""
2.1.0,
2.1.0,"def call(self, x, mask=None):"
2.1.0,"""""""Execute this layer on input tensors."
2.1.0,
2.1.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.1.0,
2.1.0,Parameters
2.1.0,----------
2.1.0,x: list
2.1.0,list of Tensors of form described above.
2.1.0,"mask: bool, optional"
2.1.0,Ignored. Present only to shadow superclass call() method.
2.1.0,
2.1.0,Returns
2.1.0,-------
2.1.0,A: Tensor
2.1.0,Tensor of atom_features
2.1.0,P: Tensor
2.1.0,Tensor of pair_features
2.1.0,""""""""
2.1.0,# Add trainable weights
2.1.0,self.build()
2.1.0,
2.1.0,atom_features = x[0]
2.1.0,pair_features = x[1]
2.1.0,
2.1.0,pair_split = x[2]
2.1.0,atom_to_pair = x[4]
2.1.0,
2.1.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.1.0,AA = self.activation(AA)
2.1.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.1.0,PA = self.activation(PA)
2.1.0,"PA = tf.segment_sum(PA, pair_split)"
2.1.0,
2.1.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.1.0,A = self.activation(A)
2.1.0,
2.1.0,if self.update_pair:
2.1.0,AP_ij = tf.matmul(
2.1.0,tf.reshape(
2.1.0,"tf.gather(atom_features, atom_to_pair),"
2.1.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.1.0,AP_ij = self.activation(AP_ij)
2.1.0,AP_ji = tf.matmul(
2.1.0,tf.reshape(
2.1.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.1.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.1.0,AP_ji = self.activation(AP_ji)
2.1.0,
2.1.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.1.0,PP = self.activation(PP)
2.1.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.1.0,P = self.activation(P)
2.1.0,else:
2.1.0,P = pair_features
2.1.0,
2.1.0,"return A, P"
2.1.0,TODO(rbharath): This class does not yet have a
2.1.0,"TensorGraph equivalent, but one may not be required."
2.1.0,"Commented out for now, remove if OK."
2.1.0,class WeaveConcat(Layer):
2.1.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.1.0,""""""""
2.1.0,
2.1.0,"def __init__(self,"
2.1.0,"batch_size,"
2.1.0,"n_atom_input_feat=50,"
2.1.0,"n_output=128,"
2.1.0,"init='glorot_uniform',"
2.1.0,"activation='tanh',"
2.1.0,**kwargs):
2.1.0,""""""""
2.1.0,Parameters
2.1.0,----------
2.1.0,batch_size: int
2.1.0,number of molecules in a batch
2.1.0,"n_atom_input_feat: int, optional"
2.1.0,Number of features for each atom in input.
2.1.0,"n_output: int, optional"
2.1.0,Number of output features for each atom(concatenated)
2.1.0,"init: str, optional"
2.1.0,Weight initialization for filters.
2.1.0,"activation: str, optional"
2.1.0,Activation function applied
2.1.0,
2.1.0,""""""""
2.1.0,self.batch_size = batch_size
2.1.0,self.n_atom_input_feat = n_atom_input_feat
2.1.0,self.n_output = n_output
2.1.0,self.init = initializations.get(init)  # Set weight initialization
2.1.0,self.activation = activations.get(activation)  # Get activations
2.1.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.1.0,
2.1.0,def build(self):
2.1.0,"""""""""Construct internal trainable weights."
2.1.0,""""""""
2.1.0,
2.1.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.1.0,self.b = model_ops.zeros(shape=[
2.1.0,"self.n_output,"
2.1.0,])
2.1.0,
2.1.0,self.trainable_weights = self.W + self.b
2.1.0,
2.1.0,"def call(self, x, mask=None):"
2.1.0,"""""""Execute this layer on input tensors."
2.1.0,
2.1.0,"x = [atom_features, atom_mask]"
2.1.0,
2.1.0,Parameters
2.1.0,----------
2.1.0,x: list
2.1.0,Tensors as listed above
2.1.0,"mask: bool, optional"
2.1.0,Ignored. Present only to shadow superclass call() method.
2.1.0,
2.1.0,Returns
2.1.0,-------
2.1.0,outputs: Tensor
2.1.0,Tensor of concatenated atom features
2.1.0,""""""""
2.1.0,self.build()
2.1.0,atom_features = x[0]
2.1.0,atom_masks = x[1]
2.1.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.1.0,A_mask = tf.split(
2.1.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.1.0,outputs = tf.concat(
2.1.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.1.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.1.0,outputs = self.activation(outputs)
2.1.0,return outputs
2.1.0,TODO(rbharath): This class does not yet have a
2.1.0,"TensorGraph equivalent, but one may not be required."
2.1.0,"Commented out for now, remove if OK."
2.1.0,class AlternateWeaveGather(WeaveGather):
2.1.0,"""""""Alternate implementation of weave gather layer"
2.1.0,corresponding to AlternateWeaveLayer
2.1.0,""""""""
2.1.0,
2.1.0,"def call(self, x, mask=None):"
2.1.0,"""""""Execute this layer on input tensors."
2.1.0,
2.1.0,"x = [atom_features, atom_split]"
2.1.0,
2.1.0,Parameters
2.1.0,----------
2.1.0,x: list
2.1.0,Tensors as listed above
2.1.0,"mask: bool, optional"
2.1.0,Ignored. Present only to shadow superclass call() method.
2.1.0,
2.1.0,Returns
2.1.0,-------
2.1.0,outputs: Tensor
2.1.0,Tensor of molecular features
2.1.0,""""""""
2.1.0,# Add trainable weights
2.1.0,self.build()
2.1.0,outputs = x[0]
2.1.0,atom_split = x[1]
2.1.0,
2.1.0,if self.gaussian_expand:
2.1.0,outputs = self.gaussian_histogram(outputs)
2.1.0,
2.1.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.1.0,
2.1.0,if self.gaussian_expand:
2.1.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.1.0,output_molecules = self.activation(output_molecules)
2.1.0,return output_molecules
2.1.0,Each directory holds a range of assay results
2.1.0,Just write NA
2.1.0,"Now, write out the results csv, going line by line through all molecule results"
2.1.0,printing the mol_id
2.1.0,printing the SMILES
2.1.0,Now gzip it
2.1.0,Now remove the intermediate csv
2.1.0,First download all SDF files. We need these to get smiles
2.1.0,Next download all Bioassays
2.1.0,RDKit consistently hangs when trying to read this file
2.1.0,TODO (LESWING) Lazy Load
2.1.0,TODO (LESWING) Lazy Load
2.1.0,from simdna import simulations
2.1.0,define layer out functions
2.1.0,get layer outputs for a positive simulation example
2.1.0,plot layer outputs
2.1.0,highlight motif sites
2.1.0,get a positive and a negative example from the simulation data
2.1.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.1.0,get motif site locations
2.1.0,organize legends
2.1.0,plot scores and highlight motif site locations
2.1.0,initialize fwd and reverse scores to -infinity
2.1.0,"cross-correlate separately for each base,"
2.1.0,for both the PSSM and its reverse complement
2.1.0,sum over the bases
2.1.0,take max of fwd and reverse scores at each position
2.1.0,return 1D view of sequence characters
2.1.0,class SequenceDNN(Model):
2.1.0,""""""""
2.1.0,Sequence DNN models.
2.1.0,
2.1.0,Parameters
2.1.0,----------
2.1.0,"seq_length : int, optional"
2.1.0,length of input sequence.
2.1.0,"keras_model : instance of keras.models.Sequential, optional"
2.1.0,seq_length or keras_model must be specified.
2.1.0,"num_tasks : int, optional"
2.1.0,number of tasks. Default: 1.
2.1.0,num_filters : list[int] | tuple[int]
2.1.0,"number of convolutional filters in each layer. Default: (15,)."
2.1.0,conv_width : list[int] | tuple[int]
2.1.0,"width of each layer's convolutional filters. Default: (15,)."
2.1.0,pool_width : int
2.1.0,width of max pooling after the last layer. Default: 35.
2.1.0,L1 : float
2.1.0,strength of L1 penalty.
2.1.0,dropout : float
2.1.0,dropout probability in every convolutional layer. Default: 0.
2.1.0,verbose: int
2.1.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.1.0,
2.1.0,Returns
2.1.0,-------
2.1.0,Compiled DNN model.
2.1.0,""""""""
2.1.0,
2.1.0,"def __init__(self,"
2.1.0,"seq_length=None,"
2.1.0,"keras_model=None,"
2.1.0,"use_RNN=False,"
2.1.0,"num_tasks=1,"
2.1.0,"num_filters=(15, 15, 15),"
2.1.0,"conv_width=(15, 15, 15),"
2.1.0,"pool_width=35,"
2.1.0,"GRU_size=35,"
2.1.0,"TDD_size=15,"
2.1.0,"L1=0,"
2.1.0,"dropout=0.0,"
2.1.0,"num_epochs=100,"
2.1.0,verbose=1):
2.1.0,self.num_tasks = num_tasks
2.1.0,self.num_epochs = num_epochs
2.1.0,self.verbose = verbose
2.1.0,self.train_metrics = []
2.1.0,self.valid_metrics = []
2.1.0,if keras_model is not None and seq_length is None:
2.1.0,self.model = keras_model
2.1.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.1.0,elif seq_length is not None and keras_model is None:
2.1.0,self.model = Sequential()
2.1.0,assert len(num_filters) == len(conv_width)
2.1.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.1.0,conv_height = 4 if i == 0 else 1
2.1.0,self.model.add(
2.1.0,Convolution2D(
2.1.0,"nb_filter=nb_filter,"
2.1.0,"nb_row=conv_height,"
2.1.0,"nb_col=nb_col,"
2.1.0,"activation='linear',"
2.1.0,"init='he_normal',"
2.1.0,"input_shape=(1, 4, seq_length),"
2.1.0,"W_regularizer=l1(L1),"
2.1.0,b_regularizer=l1(L1)))
2.1.0,self.model.add(Activation('relu'))
2.1.0,self.model.add(Dropout(dropout))
2.1.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.1.0,if use_RNN:
2.1.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.1.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.1.0,"self.model.add(Permute((2, 1)))"
2.1.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.1.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.1.0,self.model.add(Flatten())
2.1.0,self.model.add(Dense(output_dim=self.num_tasks))
2.1.0,self.model.add(Activation('sigmoid'))
2.1.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.1.0,else:
2.1.0,raise ValueError(
2.1.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.1.0,
2.1.0,"def train(self,"
2.1.0,"X,"
2.1.0,"y,"
2.1.0,"validation_data,"
2.1.0,"early_stopping_metric='Loss',"
2.1.0,"early_stopping_patience=5,"
2.1.0,save_best_model_to_prefix=None):
2.1.0,if y.dtype != bool:
2.1.0,"assert set(np.unique(y)) == {0, 1}"
2.1.0,y = y.astype(bool)
2.1.0,multitask = y.shape[1] > 1
2.1.0,if not multitask:
2.1.0,num_positives = y.sum()
2.1.0,num_sequences = len(y)
2.1.0,num_negatives = num_sequences - num_positives
2.1.0,if self.verbose >= 1:
2.1.0,print('Training model (* indicates new best result)...')
2.1.0,"X_valid, y_valid = validation_data"
2.1.0,early_stopping_wait = 0
2.1.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.1.0,"for epoch in range(1, self.num_epochs + 1):"
2.1.0,self.model.fit(
2.1.0,"X,"
2.1.0,"y,"
2.1.0,"batch_size=128,"
2.1.0,"nb_epoch=1,"
2.1.0,class_weight={
2.1.0,"True: num_sequences / num_positives,"
2.1.0,False: num_sequences / num_negatives
2.1.0,"} if not multitask else None,"
2.1.0,verbose=self.verbose >= 2)
2.1.0,"epoch_train_metrics = self.test(X, y)"
2.1.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.1.0,self.train_metrics.append(epoch_train_metrics)
2.1.0,self.valid_metrics.append(epoch_valid_metrics)
2.1.0,if self.verbose >= 1:
2.1.0,print('Epoch {}:'.format(epoch))
2.1.0,print('Train {}'.format(epoch_train_metrics))
2.1.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.1.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.1.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.1.0,if self.verbose >= 1:
2.1.0,print(' *')
2.1.0,best_metric = current_metric
2.1.0,best_epoch = epoch
2.1.0,early_stopping_wait = 0
2.1.0,if save_best_model_to_prefix is not None:
2.1.0,self.save(save_best_model_to_prefix)
2.1.0,else:
2.1.0,if self.verbose >= 1:
2.1.0,print()
2.1.0,if early_stopping_wait >= early_stopping_patience:
2.1.0,break
2.1.0,early_stopping_wait += 1
2.1.0,if self.verbose >= 1:
2.1.0,print('Finished training after {} epochs.'.format(epoch))
2.1.0,if save_best_model_to_prefix is not None:
2.1.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.1.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.1.0,"best_epoch, save_best_model_to_prefix))"
2.1.0,
2.1.0,"def predict(self, X):"
2.1.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.1.0,
2.1.0,def get_sequence_filters(self):
2.1.0,""""""""
2.1.0,Returns 3D array of 2D sequence filters.
2.1.0,""""""""
2.1.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.1.0,
2.1.0,"def deeplift(self, X, batch_size=200):"
2.1.0,""""""""
2.1.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.1.0,""""""""
2.1.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.1.0,from deeplift.conversion import keras_conversion as kc
2.1.0,
2.1.0,# convert to deeplift model and get scoring function
2.1.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.1.0,score_func = deeplift_model.get_target_contribs_func(
2.1.0,find_scores_layer_idx=0)
2.1.0,# use a 40% GC reference
2.1.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.1.0,# get deeplift scores
2.1.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.1.0,for i in range(self.num_tasks):
2.1.0,deeplift_scores[i] = score_func(
2.1.0,"task_idx=i,"
2.1.0,"input_data_list=[X],"
2.1.0,"batch_size=batch_size,"
2.1.0,"progress_update=None,"
2.1.0,input_references_list=input_references)
2.1.0,return deeplift_scores
2.1.0,
2.1.0,"def in_silico_mutagenesis(self, X):"
2.1.0,""""""""
2.1.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.1.0,""""""""
2.1.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.1.0,wild_type_predictions = self.predict(X)
2.1.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.1.0,np.newaxis]
2.1.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.1.0,"zip(X, wild_type_predictions)):"
2.1.0,mutated_sequences = np.repeat(
2.1.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.1.0,# remove wild-type
2.1.0,arange = np.arange(len(mutated_sequences))
2.1.0,horizontal_cycle = np.tile(
2.1.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.1.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.1.0,# add mutant
2.1.0,vertical_repeat = np.repeat(
2.1.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.1.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.1.0,# make mutant predictions
2.1.0,mutated_predictions = self.predict(mutated_sequences)
2.1.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.1.0,"(self.num_tasks,))"
2.1.0,mutagenesis_scores[
2.1.0,sequence_index] = wild_type_prediction - mutated_predictions
2.1.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.1.0,
2.1.0,@staticmethod
2.1.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.1.0,from dragonn.plot import plot_bases_on_ax
2.1.0,scores = score_func(X).squeeze(
2.1.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.1.0,try:
2.1.0,os.makedirs(output_directory)
2.1.0,except OSError:
2.1.0,pass
2.1.0,num_tasks = len(scores)
2.1.0,"for task_index, task_scores in enumerate(scores):"
2.1.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.1.0,# sequence_scores is num_bases x sequence_length
2.1.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.1.0,plt.clf()
2.1.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.1.0,top_axis.plot(
2.1.0,"range(1,"
2.1.0,"len(basewise_max_sequence_scores) + 1),"
2.1.0,basewise_max_sequence_scores)
2.1.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.1.0,peak_position = basewise_max_sequence_scores.argmax()
2.1.0,top_axis.axvspan(
2.1.0,"peak_position - peak_width,"
2.1.0,"peak_position + peak_width,"
2.1.0,"color='grey',"
2.1.0,alpha=0.1)
2.1.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.1.0,peak_position + peak_width].T
2.1.0,# Set non-max letter_heights to zero
2.1.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.1.0,"letter_heights[np.arange(len(letter_heights)),"
2.1.0,peak_sequence_scores.argmax(axis=1)] = \
2.1.0,basewise_max_sequence_scores[peak_position - peak_width :
2.1.0,peak_position + peak_width]
2.1.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.1.0,bottom_axis.set_xticklabels(
2.1.0,tuple(
2.1.0,"map(str,"
2.1.0,"np.arange(peak_position - peak_width,"
2.1.0,peak_position + peak_width + 1))))
2.1.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.1.0,plt.xlabel('Position')
2.1.0,plt.ylabel('Score')
2.1.0,plt.savefig(
2.1.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.1.0,"sequence_index, '_task_{}'.format(task_index)"
2.1.0,if num_tasks > 1 else '')))
2.1.0,plt.close()
2.1.0,
2.1.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.1.0,self._plot_scores(
2.1.0,"X,"
2.1.0,"output_directory,"
2.1.0,"peak_width,"
2.1.0,"score_func=self.deeplift,"
2.1.0,score_name='DeepLift')
2.1.0,
2.1.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.1.0,self._plot_scores(
2.1.0,"X,"
2.1.0,"output_directory,"
2.1.0,"peak_width,"
2.1.0,"score_func=self.in_silico_mutagenesis,"
2.1.0,score_name='ISM')
2.1.0,
2.1.0,"def plot_architecture(self, output_file):"
2.1.0,from dragonn.visualize_util import plot as plot_keras_model
2.1.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.1.0,
2.1.0,"def save(self, save_best_model_to_prefix):"
2.1.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.1.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.1.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.1.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.1.0,
2.1.0,@staticmethod
2.1.0,"def load(arch_fname, weights_fname=None):"
2.1.0,model_json_string = open(arch_fname).read()
2.1.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.1.0,if weights_fname is not None:
2.1.0,sequence_dnn.model.load_weights(weights_fname)
2.1.0,return sequence_dnn
2.1.0,create temporary fasta files
2.1.0,run command
2.1.0,remove fasta files
2.1.0,write test fasta file
2.1.0,test gkmsvm
2.1.0,get classification results
2.1.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.1.0,"Using canonical smiles for glycine, as in original research paper"
2.1.0,2017 DeepCrystal Technologies - Patrick Hop
2.1.0,
2.1.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.1.0,
2.1.0,MIT License - have fun!!
2.1.0,===========================================================
2.1.0,x = F.selu( fc(x) )
2.1.0,x = F.selu( fc(x) )
2.1.0,2017 DeepCrystal Technologies - Patrick Hop
2.1.0,
2.1.0,Data loading a splitting file
2.1.0,
2.1.0,MIT License - have fun!!
2.1.0,===========================================================
2.1.0,Consistency check
2.1.0,Handle output layer
2.1.0,Iterate over all previous tasks.
2.1.0,prev_layers is a list with elements of size
2.1.0,"(batch_size, layer_sizes[i-1])"
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Save an initial checkpoint.
2.1.0,Turns out there are valid cases where we don't want pad-batches
2.1.0,on by default.
2.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.1.0,Run training op.
2.1.0,Always save a final checkpoint when complete.
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Note that we divide by the batch size and not the number of
2.1.0,"non-zero weight examples in the batch.  Also, instead of using"
2.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.1.0,calculate with div/sum so it stays on the GPU.
2.1.0,aggregated costs
2.1.0,weight decay
2.1.0,Dummy placeholders
2.1.0,Dummy placeholders
2.1.0,run eval data through the model
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,Handle edge case when batch-size is 1.
2.1.0,Prune away any padding that was added
2.1.0,allow_soft_placement=True allows ops without a GPU implementation
2.1.0,to run on the CPU instead.
2.1.0,!/usr/bin/python
2.1.0,
2.1.0,Copyright 2015 Google Inc.
2.1.0,
2.1.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.1.0,you may not use this file except in compliance with the License.
2.1.0,You may obtain a copy of the License at
2.1.0,
2.1.0,http://www.apache.org/licenses/LICENSE-2.0
2.1.0,
2.1.0,"Unless required by applicable law or agreed to in writing, software"
2.1.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.1.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.1.0,See the License for the specific language governing permissions and
2.1.0,limitations under the License.
2.1.0,parse CheckpointState proto
2.1.0,parse path to actual checkpoint
2.1.0,the provided mask has to be the same shape as features
2.1.0,test k = 1..4
2.1.0,central moments
2.1.0,standardized moments
2.1.0,central across one axis
2.1.0,standardized across one axis
2.1.0,Fit just on task zero
2.1.0,Notice that we keep the session open
2.1.0,Fit on task one
2.1.0,The predictions for task zero should not change after training
2.1.0,on task one.
2.1.0,!/usr/bin/python
2.1.0,
2.1.0,Copyright 2015 Google Inc.
2.1.0,
2.1.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.1.0,you may not use this file except in compliance with the License.
2.1.0,You may obtain a copy of the License at
2.1.0,
2.1.0,http://www.apache.org/licenses/LICENSE-2.0
2.1.0,
2.1.0,"Unless required by applicable law or agreed to in writing, software"
2.1.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.1.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.1.0,See the License for the specific language governing permissions and
2.1.0,limitations under the License.
2.1.0,get the divisor
2.1.0,compute the requested central moment
2.1.0,"note that mean is a raw moment, not a central moment"
2.1.0,TODO(user): median is not implemented yet in TensorFlow
2.1.0,Add the input features.
2.1.0,"layer has shape [None, layer_sizes[i]]"
2.1.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.1.0,TODO(rbharath): Might want to make it feasible to have multiple
2.1.0,bypass layers.
2.1.0,Construct task bypass layer
2.1.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.1.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.1.0,"layer has shape [None, layer_sizes[i]]"
2.1.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.1.0,TODO(rbharath): Might want to make it feasible to have multiple
2.1.0,bypass layers.
2.1.0,Construct task bypass layer
2.1.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.1.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.1.0,Consistency check
2.1.0,Lazily created by _get_shared_session().
2.1.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.1.0,when subclass-overridden methods use the same scopes.
2.1.0,Setup graph
2.1.0,Create placeholders
2.1.0,Handle output layer
2.1.0,Iterate over all previous tasks.
2.1.0,prev_layers is a list with elements of size
2.1.0,"(batch_size, layer_sizes[i-1])"
2.1.0,Note that we divide by the batch size and not the number of
2.1.0,"non-zero weight examples in the batch.  Also, instead of using"
2.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.1.0,calculate with div/sum so it stays on the GPU.
2.1.0,aggregated costs
2.1.0,weight decay
2.1.0,Dummy placeholders
2.1.0,Dummy placeholders
2.1.0,run eval data through the model
2.1.0,"Shape (n_tasks, n__samples)"
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,Handle edge case when batch-size is 1.
2.1.0,with self._get_shared_session(train=True) as sess:
2.1.0,Save an initial checkpoint.
2.1.0,Always save a final checkpoint when complete.
2.1.0,Note that we divide by the batch size and not the number of
2.1.0,"non-zero weight examples in the batch.  Also, instead of using"
2.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.1.0,calculate with div/sum so it stays on the GPU.
2.1.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.1.0,Dummy placeholders
2.1.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.1.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.1.0,Dummy placeholders
2.1.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.1.0,allow_soft_placement=True allows ops without a GPU implementation
2.1.0,to run on the CPU instead.
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Turns out there are valid cases where we don't want pad-batches
2.1.0,on by default.
2.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.1.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.1.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,"(n_samples, n_classes)"
2.1.0,"(n_samples, n_tasks, n_classes)"
2.1.0,Save hyperparameters
2.1.0,Guard variable to make sure we don't Restore() this model
2.1.0,from a disk checkpoint more than once.
2.1.0,"Path to save checkpoint files, which matches the"
2.1.0,replicated supervisor's default path.
2.1.0,Lazily created by _get_shared_session().
2.1.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.1.0,when subclass-overridden methods use the same scopes.
2.1.0,Setup graph
2.1.0,Note that we divide by the batch size and not the number of
2.1.0,"non-zero weight examples in the batch.  Also, instead of using"
2.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.1.0,calculate with div/sum so it stays on the GPU.
2.1.0,aggregated costs
2.1.0,weight decay
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Save an initial checkpoint.
2.1.0,Define the code that runs on a separate thread to feed data into the queue.
2.1.0,Main training loop.
2.1.0,Run training op.
2.1.0,We have reached the end of an epoch.
2.1.0,We have reached the end of the data.
2.1.0,Always save a final checkpoint when complete.
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,allow_soft_placement=True allows ops without a GPU implementation
2.1.0,to run on the CPU instead.
2.1.0,gpu memory growth option
2.1.0,gpu memory growth option
2.1.0,TODO(rbharath): Is setting train=False right here?
2.1.0,Discard any padded predictions
2.1.0,The iterbatches does padding with zero-weight examples on the last batch.
2.1.0,Remove padded examples.
2.1.0,Special case to handle singletasks.
2.1.0,The iterbatches does padding with zero-weight examples on the last batch.
2.1.0,Remove padded examples.
2.1.0,TODO(rbharath): Verify this can be safely removed.
2.1.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.1.0,""""""""
2.1.0,Evaluates the performance of this model on specified dataset.
2.1.0,
2.1.0,Parameters
2.1.0,----------
2.1.0,dataset: dc.data.Dataset
2.1.0,Dataset object.
2.1.0,metric: deepchem.metrics.Metric
2.1.0,Evaluation metric
2.1.0,transformers: list
2.1.0,List of deepchem.transformers.Transformer
2.1.0,Returns
2.1.0,-------
2.1.0,dict
2.1.0,Maps tasks to scores under metric.
2.1.0,""""""""
2.1.0,"evaluator = Evaluator(self, dataset, transformers)"
2.1.0,scores = evaluator.compute_model_performance(metrics)
2.1.0,return scores
2.1.0,checkpoints look like model_dir/model.ckpt-N
2.1.0,"self._save_path is ""model_dir/model.ckpt"""
2.1.0,run eval data through the model
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,run eval data through the model
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,Note that softmax is already applied in construct_grpah
2.1.0,run eval data through the model
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,Handle edge case when batch-size is 1.
2.1.0,Prune away any padding that was added
2.1.0,Handle case of 0-dimensional scalar output
2.1.0,Set random seeds
2.1.0,Setup directories
2.1.0,Model constants
2.1.0,Load and transform datasets
2.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.1.0,Atomic convolution variables
2.1.0,at = atomic numbers (atom types)
2.1.0,"radial basis function parameters [cutoff, mean, width]"
2.1.0,Model hyperparameters
2.1.0,Initialize model
2.1.0,Fit model
2.1.0,Evaluate model
2.1.0,Set random seeds
2.1.0,Setup directories
2.1.0,Model constants
2.1.0,Load and transform datasets
2.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.1.0,Atomic convolution variables
2.1.0,at = atomic numbers (atom types)
2.1.0,"radial basis function parameters [cutoff, mean, width]"
2.1.0,Model hyperparameters
2.1.0,Initialize model
2.1.0,Fit model
2.1.0,Evaluate model
2.1.0,Set random seeds
2.1.0,Setup directories
2.1.0,Model constants
2.1.0,Load and transform datasets
2.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.1.0,Atomic convolution variables
2.1.0,at = atomic numbers (atom types)
2.1.0,"radial basis function parameters [cutoff, mean, width]"
2.1.0,Model hyperparameters
2.1.0,Initialize model
2.1.0,Fit model
2.1.0,Evaluate model
2.1.0,Set random seeds
2.1.0,Setup directories
2.1.0,Model constants
2.1.0,Load and transform datasets
2.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.1.0,Atomic convolution variables
2.1.0,at = atomic numbers (atom types)
2.1.0,"radial basis function parameters [cutoff, mean, width]"
2.1.0,Model hyperparameters
2.1.0,Initialize model
2.1.0,Fit model
2.1.0,Evaluate model
2.1.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.1.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.1.0,test_scores = test_evaluator.compute_model_performance(metric)
2.1.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.1.0,param.update(test_scores)
2.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.1.0,for transformer in transformers:
2.1.0,train_dataset = transformer.transform(train_dataset)
2.1.0,test_dataset = transformer.transform(test_dataset)
2.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.1.0,for transformer in transformers:
2.1.0,train_dataset = transformer.transform(train_dataset)
2.1.0,test_dataset = transformer.transform(test_dataset)
2.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.1.0,for transformer in transformers:
2.1.0,train_dataset = transformer.transform(train_dataset)
2.1.0,test_dataset = transformer.transform(test_dataset)
2.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.1.0,for transformer in transformers:
2.1.0,train_dataset = transformer.transform(train_dataset)
2.1.0,test_dataset = transformer.transform(test_dataset)
2.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.1.0,Create some directories for analysis
2.1.0,The base_dir holds the results of all analysis
2.1.0,Make directories to store the raw and featurized datasets.
2.1.0,Load PDBBind dataset
2.1.0,Define featurizers
2.1.0,Currently featurizes with shard_size=1
2.1.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.1.0,This could be done with openbabel in python
2.1.0,Compute cells for this molecule. O(constant)
2.1.0,min == max if molecule is planar in some direction
2.1.0,we should still create a bin
2.1.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.1.0,Note neighbors contains self!
2.1.0,Associate each atom with cell it belongs to. O(N)
2.1.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.1.0,"conditions, so does wrapround. O(constant)"
2.1.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.1.0,Accept as neighbors only those within threshold. This computation should be
2.1.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.1.0,Sort neighbors by distance
2.1.0,Pick up to max_num_neighbors
2.1.0,Type of data created by this featurizer
2.1.0,assumes that every array is of the same dimension
2.1.0,rem_dataset is remaining portion of dataset
2.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.1.0,to k-1.
2.1.0,returns list of per column sum of non zero elements
2.1.0,Compute number of actives needed per task.
2.1.0,loop through each column and obtain index required to splice out for
2.1.0,required fraction of hits
2.1.0,Find the first index where the cumulative number of actives equals
2.1.0,the actives_count
2.1.0,Note that np.where tells us last index required to exceed
2.1.0,"actives_count, so we actually want the following location"
2.1.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.1.0,potentially refactor those to match this.
2.1.0,Handle edge case where frac_split is 1
2.1.0,Create weight matrices fpor two haves.
2.1.0,copy over up to required index for weight first_split
2.1.0,check out if any rows in either w_1 or w_2 are just zeros
2.1.0,"Obtain original x, y, and w arrays and shuffle"
2.1.0,calculate percent split for valid (out of test and valid)
2.1.0,"split test data into valid and test, treating sub test set also as sparse"
2.1.0,rem_dataset is remaining portion of dataset
2.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.1.0,to k-1.
2.1.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.1.0,This can be generalized in the future with some common demoninator determination.
2.1.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.1.0,Append remaining examples to train
2.1.0,Sort by increasing MW
2.1.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.1.0,for m_idx in cluster:
2.1.0,"continue until we find an active in all the tasks, otherwise we can't"
2.1.0,compute a meaningful AUC
2.1.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.1.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.1.0,Sort from largest to smallest scaffold sets
2.1.0,Sort from largest to smallest scaffold sets
2.1.0,"(n_samples, n_classes)"
2.1.0,"(n_samples, n_tasks, n_classes)"
2.1.0,Save hyperparameters
2.1.0,Guard variable to make sure we don't Restore() this model
2.1.0,from a disk checkpoint more than once.
2.1.0,"Path to save checkpoint files, which matches the"
2.1.0,replicated supervisor's default path.
2.1.0,Lazily created by _get_shared_session().
2.1.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.1.0,when subclass-overridden methods use the same scopes.
2.1.0,Setup graph
2.1.0,Note that we divide by the batch size and not the number of
2.1.0,"non-zero weight examples in the batch.  Also, instead of using"
2.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.1.0,calculate with div/sum so it stays on the GPU.
2.1.0,aggregated costs
2.1.0,weight decay
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Save an initial checkpoint.
2.1.0,Turns out there are valid cases where we don't want pad-batches
2.1.0,on by default.
2.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.1.0,Run training op.
2.1.0,Always save a final checkpoint when complete.
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,allow_soft_placement=True allows ops without a GPU implementation
2.1.0,to run on the CPU instead.
2.1.0,TODO(rbharath): Is setting train=False right here?
2.1.0,Discard any padded predictions
2.1.0,The iterbatches does padding with zero-weight examples on the last batch.
2.1.0,Remove padded examples.
2.1.0,Special case to handle singletasks.
2.1.0,The iterbatches does padding with zero-weight examples on the last batch.
2.1.0,Remove padded examples.
2.1.0,TODO(rbharath): Verify this can be safely removed.
2.1.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.1.0,""""""""
2.1.0,Evaluates the performance of this model on specified dataset.
2.1.0,
2.1.0,Parameters
2.1.0,----------
2.1.0,dataset: dc.data.Dataset
2.1.0,Dataset object.
2.1.0,metric: deepchem.metrics.Metric
2.1.0,Evaluation metric
2.1.0,transformers: list
2.1.0,List of deepchem.transformers.Transformer
2.1.0,Returns
2.1.0,-------
2.1.0,dict
2.1.0,Maps tasks to scores under metric.
2.1.0,""""""""
2.1.0,"evaluator = Evaluator(self, dataset, transformers)"
2.1.0,scores = evaluator.compute_model_performance(metrics)
2.1.0,return scores
2.1.0,checkpoints look like logdir/model.ckpt-N
2.1.0,"self._save_path is ""logdir/model.ckpt"""
2.1.0,run eval data through the model
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,run eval data through the model
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,Note that softmax is already applied in construct_grpah
2.1.0,run eval data through the model
2.1.0,reshape to batch_size x n_tasks x ...
2.1.0,Handle edge case when batch-size is 1.
2.1.0,Prune away any padding that was added
2.1.0,Handle case of 0-dimensional scalar output
2.1.0,Dummy placeholders
2.1.0,Dummy placeholders
2.1.0,## AtomicNet fully-connected layer ops ###
2.1.0,## Atomicnet coordinate transform ops ###
2.1.0,## Atomicnet symmetry function kernel ops ###
2.1.0,## Atomicnet symmetry function ops ###
2.1.0,## Atomcnet symmetry function layer ops ###
2.1.0,We apply the radial pooling filter before atom type conv
2.1.0,to reduce computation
2.1.0,## Misc convenience ops ###
2.1.0,"Copied from the yt_project, commit e8fb57e"
2.1.0,yt/doc/extensions/notebook_sphinxext.py
2.1.0,https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
2.1.0,Almost completely re-written by Matthew Harrigan to use nbconvert v4
2.1.0,1. Uneval notebook
2.1.0,2. Python
2.1.0,3. HTML (execute first)
2.1.0,Set per-cell timeout to 60 seconds
2.1.0,4. Eval'd notebook
2.1.0,Create link to notebook and script files
2.1.0,create notebook node
2.1.0,add dependency
2.1.0,-*- coding: utf-8 -*-
2.1.0,
2.1.0,"deepchem documentation build configuration file, created by"
2.1.0,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
2.1.0,
2.1.0,This file is execfile()d with the current directory set to its
2.1.0,containing dir.
2.1.0,
2.1.0,Note that not all possible configuration values are present in this
2.1.0,autogenerated file.
2.1.0,
2.1.0,All configuration values have a default; values that are commented out
2.1.0,serve to show the default.
2.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.1.0,add these directories to sys.path here. If the directory is relative to the
2.1.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.1.0,"sys.path.insert(0, os.path.abspath('.'))"
2.1.0,-- General configuration ------------------------------------------------
2.1.0,"If your documentation needs a minimal Sphinx version, state it here."
2.1.0,needs_sphinx = '1.0'
2.1.0,"Add any Sphinx extension module names here, as strings. They can be"
2.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.1.0,ones.
2.1.0,"Add any paths that contain templates here, relative to this directory."
2.1.0,The suffix(es) of source filenames.
2.1.0,You can specify multiple suffix as a list of string:
2.1.0,"source_suffix = ['.rst', '.md']"
2.1.0,The encoding of source files.
2.1.0,source_encoding = 'utf-8-sig'
2.1.0,The master toctree document.
2.1.0,General information about the project.
2.1.0,"The version info for the project you're documenting, acts as replacement for"
2.1.0,"|version| and |release|, also used in various other places throughout the"
2.1.0,built documents.
2.1.0,
2.1.0,The short X.Y version.
2.1.0,"The full version, including alpha/beta/rc tags."
2.1.0,The language for content autogenerated by Sphinx. Refer to documentation
2.1.0,for a list of supported languages.
2.1.0,
2.1.0,This is also used if you do content translation via gettext catalogs.
2.1.0,"Usually you set ""language"" from the command line for these cases."
2.1.0,"There are two options for replacing |today|: either, you set today to some"
2.1.0,"non-false value, then it is used:"
2.1.0,today = ''
2.1.0,"Else, today_fmt is used as the format for a strftime call."
2.1.0,"today_fmt = '%B %d, %Y'"
2.1.0,"List of patterns, relative to source directory, that match files and"
2.1.0,directories to ignore when looking for source files.
2.1.0,The reST default role (used for this markup: `text`) to use for all
2.1.0,documents.
2.1.0,default_role = None
2.1.0,"If true, '()' will be appended to :func: etc. cross-reference text."
2.1.0,add_function_parentheses = True
2.1.0,"If true, the current module name will be prepended to all description"
2.1.0,unit titles (such as .. function::).
2.1.0,add_module_names = True
2.1.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
2.1.0,output. They are ignored by default.
2.1.0,show_authors = False
2.1.0,The name of the Pygments (syntax highlighting) style to use.
2.1.0,A list of ignored prefixes for module index sorting.
2.1.0,modindex_common_prefix = []
2.1.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
2.1.0,keep_warnings = False
2.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.1.0,-- Options for HTML output ----------------------------------------------
2.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.1.0,a list of builtin themes.
2.1.0,Theme options are theme-specific and customize the look and feel of a theme
2.1.0,"further.  For a list of options available for each theme, see the"
2.1.0,documentation.
2.1.0,html_theme_options = {}
2.1.0,"Add any paths that contain custom themes here, relative to this directory."
2.1.0,"The name for this set of Sphinx documents.  If None, it defaults to"
2.1.0,"""<project> v<release> documentation""."
2.1.0,html_title = None
2.1.0,A shorter title for the navigation bar.  Default is the same as html_title.
2.1.0,html_short_title = None
2.1.0,The name of an image file (relative to this directory) to place at the top
2.1.0,of the sidebar.
2.1.0,The name of an image file (within the static path) to use as favicon of the
2.1.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
2.1.0,pixels large.
2.1.0,html_favicon = None
2.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.1.0,"relative to this directory. They are copied after the builtin static files,"
2.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.1.0,Add any extra paths that contain custom files (such as robots.txt or
2.1.0,".htaccess) here, relative to this directory. These files are copied"
2.1.0,directly to the root of the documentation.
2.1.0,html_extra_path = []
2.1.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
2.1.0,using the given strftime format.
2.1.0,"html_last_updated_fmt = '%b %d, %Y'"
2.1.0,"If true, SmartyPants will be used to convert quotes and dashes to"
2.1.0,typographically correct entities.
2.1.0,html_use_smartypants = True
2.1.0,"Custom sidebar templates, maps document names to template names."
2.1.0,html_sidebars = {}
2.1.0,"Additional templates that should be rendered to pages, maps page names to"
2.1.0,template names.
2.1.0,html_additional_pages = {}
2.1.0,"If false, no module index is generated."
2.1.0,html_domain_indices = True
2.1.0,"If false, no index is generated."
2.1.0,html_use_index = True
2.1.0,"If true, the index is split into individual pages for each letter."
2.1.0,html_split_index = False
2.1.0,"If true, links to the reST sources are added to the pages."
2.1.0,html_show_sourcelink = True
2.1.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
2.1.0,html_show_sphinx = True
2.1.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
2.1.0,html_show_copyright = True
2.1.0,"If true, an OpenSearch description file will be output, and all pages will"
2.1.0,contain a <link> tag referring to it.  The value of this option must be the
2.1.0,base URL from which the finished HTML is served.
2.1.0,html_use_opensearch = ''
2.1.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
2.1.0,html_file_suffix = None
2.1.0,Language to be used for generating the HTML full-text search index.
2.1.0,Sphinx supports the following languages:
2.1.0,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
2.1.0,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
2.1.0,html_search_language = 'en'
2.1.0,"A dictionary with options for the search language support, empty by default."
2.1.0,Now only 'ja' uses this config value
2.1.0,html_search_options = {'type': 'default'}
2.1.0,The name of a javascript file (relative to the configuration directory) that
2.1.0,"implements a search results scorer. If empty, the default will be used."
2.1.0,html_search_scorer = 'scorer.js'
2.1.0,Output file base name for HTML help builder.
2.1.0,-- Options for LaTeX output ---------------------------------------------
2.1.0,The paper size ('letterpaper' or 'a4paper').
2.1.0,"'papersize': 'letterpaper',"
2.1.0,"The font size ('10pt', '11pt' or '12pt')."
2.1.0,"'pointsize': '10pt',"
2.1.0,Additional stuff for the LaTeX preamble.
2.1.0,"'preamble': '',"
2.1.0,Latex figure (float) alignment
2.1.0,"'figure_align': 'htbp',"
2.1.0,Grouping the document tree into LaTeX files. List of tuples
2.1.0,"(source start file, target name, title,"
2.1.0,"author, documentclass [howto, manual, or own class])."
2.1.0,The name of an image file (relative to this directory) to place at the top of
2.1.0,the title page.
2.1.0,latex_logo = None
2.1.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
2.1.0,not chapters.
2.1.0,latex_use_parts = False
2.1.0,"If true, show page references after internal links."
2.1.0,latex_show_pagerefs = False
2.1.0,"If true, show URL addresses after external links."
2.1.0,latex_show_urls = False
2.1.0,Documents to append as an appendix to all manuals.
2.1.0,latex_appendices = []
2.1.0,"If false, no module index is generated."
2.1.0,latex_domain_indices = True
2.1.0,-- Options for manual page output ---------------------------------------
2.1.0,One entry per manual page. List of tuples
2.1.0,"(source start file, name, description, authors, manual section)."
2.1.0,"If true, show URL addresses after external links."
2.1.0,man_show_urls = False
2.1.0,-- Options for Texinfo output -------------------------------------------
2.1.0,Grouping the document tree into Texinfo files. List of tuples
2.1.0,"(source start file, target name, title, author,"
2.1.0,"dir menu entry, description, category)"
2.1.0,Documents to append as an appendix to all manuals.
2.1.0,texinfo_appendices = []
2.1.0,"If false, no module index is generated."
2.1.0,texinfo_domain_indices = True
2.1.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
2.1.0,texinfo_show_urls = 'footnote'
2.1.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
2.1.0,texinfo_no_detailmenu = False
2.1.0,Example configuration for intersphinx: refer to the Python standard library.
2.1.0,Higher is Better
2.1.0,The secret key is available as a secure environment variable
2.1.0,on travis-ci to push the build documentation to Amazon S3.
2.1.0,Perform recursive modification to set css mime types.
2.1.0,Perform recursive modification to set js mime types.
2.1.0,plt.show()
2.1.0,"run_benchmark(FILE, DEEPCHEM_DIR)"
2.1.0,lines in the label file have format
2.1.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.1.0,"print line[0], line[3]"
2.1.0,Record inputs.
2.1.0,Create the output directory if necessary.
2.1.0,Create duplicate placeholders for meta-optimization.
2.1.0,Create the loss function for meta-optimization.
2.1.0,"In the final loss, use different placeholders for all inputs so the loss will be"
2.1.0,computed from a different batch.
2.1.0,Create variables for accumulating the gradients.
2.1.0,Create the optimizers for meta-optimization and task optimization.
2.1.0,Main optimization loop.
2.1.0,Do checkpointing.
2.1.0,This is a MetaLearner that learns to generate sine functions with variable
2.1.0,amplitude and phase.
2.1.0,Optimize it.
2.1.0,Test it out on some new tasks and see how it works.
2.1.0,Initially the model should do a bad job of fitting the sine function.
2.1.0,After one step of optimization it should do much better.
2.1.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.1.0,get the same result.
2.1.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.1.0,Fit model on dataset
2.1.0,Fit model on dataset
2.1.0,"Should be an array of size (n_pocket_atoms, 3)"
2.1.0,"coords[triangle, 0] gives the x-dimension of all triangle points"
2.1.0,Take transpose to make sure rows correspond to atoms.
2.1.0,We voxelize so all grids have integral coordinates (convenience)
2.1.0,"If overlap of box with previously generated output boxes, return"
2.1.0,Carry forward mappings
2.1.0,We know that box has at least one atom not in outputs
2.1.0,Current box has been merged into box further down list.
2.1.0,No need to output current box
2.1.0,"protein_coords is (N, 3) tensor"
2.1.0,Load binding pocket model
2.1.0,TODO(rbharath): Shift refined to full once trained.
2.1.0,Fit model on dataset
2.1.0,Create featurizers
2.1.0,"if not ligand_file.endswith("".sdf""):"
2.1.0,"raise ValueError(""Only .sdf ligand files can be featurized."")"
2.1.0,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
2.1.0,ligand_mol2 = os.path.join(
2.1.0,"self.base_dir, ligand_basename + "".mol2"")"
2.1.0,
2.1.0,# Write mol2 file for ligand
2.1.0,obConversion = ob.OBConversion()
2.1.0,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
2.1.0,ob_mol = ob.OBMol()
2.1.0,"obConversion.ReadFile(ob_mol, str(ligand_file))"
2.1.0,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
2.1.0,
2.1.0,# Featurize ligand
2.1.0,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
2.1.0,if mol is None:
2.1.0,"return None, None"
2.1.0,# Default for CircularFingerprint
2.1.0,n_ligand_features = 1024
2.1.0,ligand_features = self.ligand_featurizer.featurize([mol])
2.1.0,
2.1.0,# Featurize pocket
2.1.0,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
2.1.0,"protein_file, ligand_file)"
2.1.0,n_pockets = len(pockets)
2.1.0,n_pocket_features = BindingPocketFeaturizer.n_features
2.1.0,
2.1.0,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
2.1.0,pocket_features = self.pocket_featurizer.featurize(
2.1.0,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
2.1.0,# Note broadcast operation
2.1.0,"features[:, :n_pocket_features] = pocket_features"
2.1.0,"features[:, n_pocket_features:] = ligand_features"
2.1.0,dataset = NumpyDataset(X=features)
2.1.0,pocket_preds = self.model.predict(dataset)
2.1.0,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
2.1.0,
2.1.0,# Find pockets which are active
2.1.0,active_pockets = []
2.1.0,active_pocket_atoms_map = {}
2.1.0,active_pocket_coords = []
2.1.0,for pocket_ind in range(len(pockets)):
2.1.0,#################################################### DEBUG
2.1.0,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
2.1.0,#if pocket_preds[pocket_ind] == 1:
2.1.0,if pocket_pred_proba[pocket_ind][1] > .15:
2.1.0,#################################################### DEBUG
2.1.0,pocket = pockets[pocket_ind]
2.1.0,active_pockets.append(pocket)
2.1.0,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
2.1.0,active_pocket_coords.append(pocket_coords[pocket_ind])
2.1.0,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
2.1.0,# TODO(LESWING)
2.1.0,TODO: add pi_stack and cation_pi to feature_types (it's not trivial
2.1.0,because they require sanitized molecules)
2.1.0,"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.1.0,"""salt_bridge""],"
2.1.0,TODO(rbharath): May want to move this file to S3 so we can ensure it's
2.1.0,always available.
2.1.0,Prepare receptor
2.1.0,Get protein centroid and range
2.1.0,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
2.1.0,going to be extremely slow!
2.1.0,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
2.1.0,first pocket.
2.1.0,Prepare receptor
2.1.0,TODO(rbharath): Generalize this so can support mol2 files as well.
2.1.0,Write Vina conf file
2.1.0,Define locations of log and output files
2.1.0,TODO(rbharath): Let user specify the number of poses required.
2.1.0,TODO(rbharath): Convert the output pdbqt to a pdb file.
2.1.0,Return docked files
2.1.0,Check returned files exist
2.1.0,Check returned files exist
2.1.0,Check returned files exist
2.1.0,Check returned files exist
2.1.0,Check returned files exist
2.1.0,Note this may download autodock Vina...
2.1.0,Note this may download autodock Vina...
2.1.0,Note this may download autodock Vina...
2.1.0,Check returned files exist
2.1.0,Note this may download autodock Vina...
2.1.0,Check returned files exist
2.1.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.1.0,box1 contained in box2
2.1.0,"box1 in box2, so complete overlap"
2.1.0,"4/5 atoms in box2 in box1, so 80 % overlap"
2.1.0,box2 contains box1
2.1.0,box1 contains box2
2.1.0,"box1 contains box2, box3"
2.1.0,Test that every atom in pocket maps exists
2.1.0,Check that the atoms is actually in protein
2.1.0,Test that every atom in pocket maps exists
2.1.0,Check that the atoms is actually in protein
2.1.0,Add active site to dict
2.1.0,The convention used is that the first task is the metric.
2.1.0,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
2.1.0,"an option in the Metric class. Instead, this should be possible to move into"
2.1.0,user-space as a custom task_averager function.
2.1.0,"TODO(rbharath, joegomes): What is this magic number?"
2.1.0,"If there are no nonzero examples, metric is ill-defined."
2.1.0,ids = df[id_field].values
2.1.0,Set missing data to have weight zero
2.1.0,TODO (ytz) this is a bandage solution to reorder the atoms so
2.1.0,that they're always in the same canonical order. Presumably this
2.1.0,should be correctly implemented in the future for graph mols.
2.1.0,Featurize task results iff they exist.
2.1.0,Filter out examples where featurization failed.
2.1.0,"For prospective data where results are unknown, it makes"
2.1.0,no sense to have y values or weights.
2.1.0,"(X, y, w, ids)"
2.1.0,Remove support indices
2.1.0,Remove support indices
2.1.0,Remove support indices
2.1.0,Get task specific entries
2.1.0,Now just get weights for this task
2.1.0,Get task specific entries
2.1.0,Now just get weights for this task
2.1.0,Now just get weights for this task
2.1.0,Now just get weights for this task
2.1.0,Split data into pos and neg lists.
2.1.0,No replacement allowed for supports
2.1.0,Handle one-d vs. non one-d feature matrices
2.1.0,Init the iterator
2.1.0,Set initial iterator state
2.1.0,support = self.supports[task][self.trial_num]
2.1.0,Increment and update logic
2.1.0,Init the iterator
2.1.0,Set initial iterator state
2.1.0,support = self.supports[task][self.trial_num]
2.1.0,Increment and update logic
2.1.0,"By invariant of when this is called, can assume num_samples > 0"
2.1.0,and num_samples < batch_size
2.1.0,Fill in batch arrays
2.1.0,"By invariant of when this is called, can assume num_samples > 0"
2.1.0,and num_samples < batch_size
2.1.0,Fill in batch arrays
2.1.0,Only the first set of copy will be counted in training loss
2.1.0,Retrieve the first sample so we can determine the dtypes.
2.1.0,Create a Tensorflow Dataset and have it create an Iterator.
2.1.0,"Set labels to be zero, with zero weights"
2.1.0,Load obsolete format -> save in new format
2.1.0,note that this corresponds to the _construct_metadata column order
2.1.0,if not len(self.metadata_df):
2.1.0,"raise ValueError(""No data in dataset."")"
2.1.0,return next(self.metadata_df.iterrows())[1]['task_names']
2.1.0,Create temp directory to store resharded version
2.1.0,Write data in new shards
2.1.0,Handle spillover from last shard
2.1.0,These columns may be missing is the dataset is unlabelled.
2.1.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.1.0,"than process based pools, since process based pools need to pickle/serialize"
2.1.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.1.0,we're actually protected by the GIL.
2.1.0,(ytz): this skips everything except possibly the last shard
2.1.0,if data_dir is None:
2.1.0,data_dir = tempfile.mkdtemp()
2.1.0,The -1 indicates that y will be reshaped to have length -1
2.1.0,"raw_data = (X, y, w, ids)"
2.1.0,Protect against generator exhaustion
2.1.0,This ensures tasks are consistent for all datasets
2.1.0,Get full dataset in memory
2.1.0,Shuffle in memory
2.1.0,Write shuffled shards out to disk
2.1.0,Shuffle the arrays corresponding to each row in metadata_df
2.1.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.1.0,Handle edge case with empty indices
2.1.0,Find indices which rest in this shard
2.1.0,Need to offset indices to fit within shard_size
2.1.0,Handle the case of datasets with y/w missing
2.1.0,Updating counts
2.1.0,Break when all indices have been used up already
2.1.0,TODO(rbharath): Get rid of * import
2.1.0,Load MUV dataset
2.1.0,Do an approximate comparison since splits are sometimes slightly off from
2.1.0,the exact fraction.
2.1.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.1.0,reloading will cause the transform to be reapplied. This is undesirable in
2.1.0,almost all cases. Need to understand a method to fix this.
2.1.0,def test_shuffle(self):
2.1.0,"""""""Test that datasets can be merged."""""""
2.1.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.1.0,dataset_file = os.path.join(
2.1.0,"current_dir, ""../../models/tests/example.csv"")"
2.1.0,featurizer = dc.feat.CircularFingerprint(size=1024)
2.1.0,"tasks = [""log-solubility""]"
2.1.0,loader = dc.data.CSVLoader(
2.1.0,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
2.1.0,"dataset = loader.featurize(dataset_file, shard_size=2)"
2.1.0,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
2.1.0,dataset.ids)
2.1.0,orig_len = len(dataset)
2.1.0,dataset.shuffle(iterations=5)
2.1.0,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
2.1.0,dataset.ids)
2.1.0,
2.1.0,assert len(dataset) == orig_len
2.1.0,# The shuffling should have switched up the ordering
2.1.0,"assert not np.array_equal(orig_ids, new_ids)"
2.1.0,# But all the same entries should still be present
2.1.0,assert sorted(orig_ids) == sorted(new_ids)
2.1.0,# All the data should have same shape
2.1.0,assert X_orig.shape == X_new.shape
2.1.0,assert y_orig.shape == y_new.shape
2.1.0,assert w_orig.shape == w_new.shape
2.1.0,The shuffling should have switched up the ordering
2.1.0,But all the same entries should still be present
2.1.0,All the data should have same shape
2.1.0,The ids should now store the performed permutation. Check that the
2.1.0,original dataset is recoverable.
2.1.0,The ids should now store the performed permutation. Check that the
2.1.0,original dataset is recoverable.
2.1.0,Set some global variables up top
2.1.0,Featurize emols dataset
2.1.0,example.fasta contains 3 sequences each of length 58.
2.1.0,The one-hot encoding turns base-pairs into vectors of length 5 (ATCGN).
2.1.0,"There is one ""image channel""."
2.1.0,Generate dummy dataset
2.1.0,Generate dummy dataset
2.1.0,Generate dummy dataset
2.1.0,Set last n_samples/2 weights to 0
2.1.0,Check that no support elements are sample from zero-weight samples
2.1.0,Generate dummy dataset
2.1.0,Generate dummy dataset
2.1.0,Create support generator
2.1.0,Generate dummy dataset
2.1.0,Create support generator
2.1.0,Generate dummy dataset
2.1.0,Assert all support elements have been removed
2.1.0,Generate dummy dataset
2.1.0,Assert all remove elements have been removed
2.1.0,Generate dummy dataset
2.1.0,Assert all support elements have been removed
2.1.0,Generate dummy dataset
2.1.0,Assert all remove elements have been removed
2.1.0,Generate dummy dataset
2.1.0,Set last n_samples/2 weights to 0
2.1.0,Sample from first n_samples/2 elements for support
2.1.0,Should lie within first n_samples/2 samples only
2.1.0,Generate dummy dataset
2.1.0,Create support generator
2.1.0,Generate dummy dataset
2.1.0,Test on identity matrix
2.1.0,Generate random sparse features dataset
2.1.0,Test edge case with array of all zeros
2.1.0,Test cases where n_samples < 2*n_samples < batch_size
2.1.0,Test cases where n_samples < batch_size
2.1.0,Test case where n_samples == batch_size
2.1.0,Test case for object featurization.
2.1.0,Test case for more complicated object featurization
2.1.0,Test case with multidimensional data
2.1.0,Test cases where n_samples < 2*n_samples < batch_size
2.1.0,Test cases where n_samples < batch_size
2.1.0,Test case where n_samples == batch_size
2.1.0,Test case for object featurization.
2.1.0,Test case for more complicated object featurization
2.1.0,Test case with multidimensional data
2.1.0,Test first resharding worked
2.1.0,Test second resharding worked
2.1.0,approx 1/15! chance of equality
2.1.0,Generate data
2.1.0,Generate data
2.1.0,Generate data
2.1.0,Transform it
2.1.0,Transform it
2.1.0,special case to test
2.1.0,deterministic
2.1.0,non-deterministic
2.1.0,we don't know the order in which the shards are iterated in.
2.1.0,Check that we have all the data in
2.1.0,Splits featurized samples into train/test
2.1.0,Splits featurized samples into train/test
2.1.0,Splits featurized samples into train/test
2.1.0,"splittype = ""random"""
2.1.0,Splits featurized samples into train/test
2.1.0,Now perform move
2.1.0,Only for debug!
2.1.0,#Make directories to store the raw and featurized datasets.
2.1.0,Load dataset
2.1.0,Featurize tox21 dataset
2.1.0,###### Do featurization
2.1.0,Do train/valid split.
2.1.0,###### Do singletask load
2.1.0,################# Do comparison
2.1.0,Only for debug!
2.1.0,Set some global variables up top
2.1.0,Make directories to store the raw and featurized datasets.
2.1.0,Load dataset
2.1.0,Featurize tox21 dataset
2.1.0,For debugging purposes
2.1.0,###### Do multitask load
2.1.0,Do train/valid split.
2.1.0,###### Do singletask load
2.1.0,################# Do comparison
2.1.0,"task_type = ""regression"""
2.1.0,coding=utf-8
2.1.0,Note that transformers have to be undone in reversed order
2.1.0,Hack to allow for easy unpickling:
2.1.0,http://stefaanlippens.net/pickleproblem
2.1.0,"One, but not both, transform_X or tranform_y is true"
2.1.0,Use fact that bools add as ints in python
2.1.0,Control for pathological case with no variance.
2.1.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.1.0,Find the task dimension of z
2.1.0,Prevent broadcasting on wrong dimension
2.1.0,BalancingTransformer can only transform weights.
2.1.0,Compute weighting factors from dataset.
2.1.0,Ensure dataset is binary
2.1.0,Remove labels with zero weights
2.1.0,self.w = dataset.w
2.1.0,"TODO (flee2): for transform_y, figure out weights"
2.1.0,"print(""y will not be transformed by CDFTransformer, for now."")"
2.1.0,"print(""Cannot undo CDF Transformer, for now."")"
2.1.0,Need this for transform_y
2.1.0,array = np.transpose(array)
2.1.0,"print(""y will not be transformed by PowerTransformer, for now."")"
2.1.0,"print(""Cannot undo Power Transformer, for now."")"
2.1.0,the tf graph here pick up the (K+1) highest similarity values
2.1.0,and their indices
2.1.0,map the indices to labels
2.1.0,generating batch of data by slicing similarity matrix
2.1.0,into 100*reference_dataset_length
2.1.0,concatenate batches of data together
2.1.0,highest similarity is 1: target is in the reference
2.1.0,use the following K points
2.1.0,"highest less than 1: target not in the reference, use top K points"
2.1.0,calculate matrix multiplicatin on slices
2.1.0,concatenate the slices together
2.1.0,list of calculation orders for DAGs
2.1.0,stemming from one specific atom in the molecule
2.1.0,starting from the adjacency list derived by graphconv featurizer
2.1.0,"number of atoms, also number of DAGs"
2.1.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.1.0,each step calculating graph features for one atom.
2.1.0,`max_atoms` is the maximum number of steps
2.1.0,each iteration generates the DAG starting from atom with index `count`
2.1.0,"list of lists, elements represent the calculation orders"
2.1.0,for atoms in the current graph
2.1.0,starting from the target atom with index `count`
2.1.0,flags of whether the atom is already included in the DAG
2.1.0,atom `count` is in the DAG
2.1.0,recording number of radial propagation steps
2.1.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.1.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.1.0,will be added in the second loop(radial=1).
2.1.0,atoms i-bond away will be added in i-th loop
2.1.0,"when molecules have separate parts, starting from one part,"
2.1.0,it is not possible to include all atoms.
2.1.0,this break quit the loop when going into such condition
2.1.0,reinitialize targets for next iteration
2.1.0,atoms connected to current_atom
2.1.0,generate the dependency map of current DAG
2.1.0,atoms connected to `current_atoms`(and not included in the DAG)
2.1.0,"are added, and will be the `current_atoms` for next iteration."
2.1.0,"DAG starts from the target atom, calculation should go in reverse"
2.1.0,`edge[1]` is the parent of `edge[0]`
2.1.0,"after this loop, `parents[i]` includes all parents of atom i"
2.1.0,manually adding the atom index into its parents list
2.1.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.1.0,atoms with less parents(farther from the target atom) come first.
2.1.0,"graph features of atoms without parents will be first calculated,"
2.1.0,then atoms with more parents can be calculated in order
2.1.0,based on previously calculated graph features.
2.1.0,target atom of this DAG will be calculated in the last step
2.1.0,padding with `max_atoms`
2.1.0,padding
2.1.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.1.0,which is a max_atoms * max_atoms numpy array after padding
2.1.0,Calculate pairwise distance
2.1.0,Masking for valid atom index
2.1.0,Cutoff with threshold Rc
2.1.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.1.0,extracting validation set of MNIST for testing the DataTransforms
2.1.0,extract only the images (no need of the labels)
2.1.0,reshaping the vector to image
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is a y transformer
2.1.0,Check w is unchanged since this is a y transformer
2.1.0,Check y is now a logarithmic version of itself
2.1.0,Check that untransform does the right thing.
2.1.0,transforming y should raise an exception
2.1.0,transforming w should raise an exception
2.1.0,transforming X should be okay
2.1.0,Check ids are unchanged.
2.1.0,Check y is unchanged since this is a X transformer
2.1.0,Check w is unchanged since this is a y transformer
2.1.0,Check y is now a logarithmic version of itself
2.1.0,Check that untransform does the right thing.
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is a y transformer
2.1.0,Check w is unchanged since this is a y transformer
2.1.0,Check y is now a logarithmic version of itself
2.1.0,Check that untransform does the right thing.
2.1.0,Tests logarithmic data transformer with selection.
2.1.0,Check ids are unchanged.
2.1.0,Check y is unchanged since this is a X transformer
2.1.0,Check w is unchanged since this is a y transformer
2.1.0,Check y is now a logarithmic version of itself
2.1.0,Check that untransform does the right thing.
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is a y transformer
2.1.0,Check w is unchanged since this is a y transformer
2.1.0,"Check that y_t has zero mean, unit std."
2.1.0,Check that untransform does the right thing.
2.1.0,Check ids are unchanged.
2.1.0,Check y is unchanged since this is a X transformer
2.1.0,Check w is unchanged since this is a y transformer
2.1.0,"Check that X_t has zero mean, unit std."
2.1.0,np.set_printoptions(threshold='nan')
2.1.0,Entries with zero std are not normalized
2.1.0,TODO(rbharath): Untransform doesn't work properly for binary feature
2.1.0,vectors. Need to figure out what's wrong here. (low priority)
2.1.0,# Check that untransform does the right thing.
2.1.0,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
2.1.0,Check ids are unchanged.
2.1.0,Check y is unchanged since this is an X transformer
2.1.0,Check w is unchanged since this is an X transformer
2.1.0,Check X is now holding the proper values when sorted.
2.1.0,Test CDF transformer on Gaussian normal dataset.
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is an y transformer
2.1.0,Check w is unchanged since this is an y transformer
2.1.0,Check y is now holding the proper values when sorted.
2.1.0,Check that untransform does the right thing.
2.1.0,Check ids are unchanged.
2.1.0,Check y is unchanged since this is an X transformer
2.1.0,Check w is unchanged since this is an X transformer
2.1.0,Check X is now holding the proper values when sorted.
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is a y transformer
2.1.0,Check w is unchanged since this is a y transformer
2.1.0,Check y is now holding the proper values when sorted.
2.1.0,Check ids are unchanged.
2.1.0,Check y is unchanged since this is an X transformer
2.1.0,Check w is unchanged since this is an X transformer
2.1.0,Check X is now holding the proper values in each column.
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is an X transformer
2.1.0,Check w is unchanged since this is an X transformer
2.1.0,Check y is now holding the proper values in each column.
2.1.0,Check that untransform does the right thing.
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is a w transformer
2.1.0,Check y is unchanged since this is a w transformer
2.1.0,Assert that entries with zero weight retain zero weight
2.1.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.1.0,Check ids are unchanged.
2.1.0,Check X is unchanged since this is a w transformer
2.1.0,Check y is unchanged since this is a w transformer
2.1.0,Assert that entries with zero weight retain zero weight
2.1.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.1.0,Check Blurring
2.1.0,Check rotation
2.1.0,Some more test cases for flip
2.1.0,Check flip
2.1.0,Check Scales
2.1.0,TODO(rbharath): Use standard joblib once old-data has been regenerated.
2.1.0,"If gzipped, need to compute extension again"
2.1.0,Tasks are stored in .sdf.csv file
2.1.0,Structures are stored in .sdf file
2.1.0,First line of user-specified CSV *must* be header.
2.1.0,The label encoder is given characters for ACGTN
2.1.0,Peak at the first sequence to get the length of the sequence.
2.1.0,Try older joblib version for legacy files.
2.1.0,First line of user-specified CSV *must* be header.
2.1.0,First line of user-specified CSV *must* be header.
2.1.0,combine dataframes
2.1.0,TODO: mol should be always sanitized when charges are calculated
2.1.0,can't change it now because it would break a lot of examples
2.1.0,working-with-3d-molecules
2.1.0,initial embedding
2.1.0,minimization and pruning
2.1.0,always keep lowest-energy conformer
2.1.0,discard conformers after max_conformers is reached
2.1.0,get RMSD to selected conformers
2.1.0,discard conformers within the RMSD threshold
2.1.0,create a new molecule to hold the chosen conformers
2.1.0,this ensures proper conformer IDs and energy-based ordering
2.1.0,TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
2.1.0,import nglview
2.1.0,import tempfile
2.1.0,import os
2.1.0,import mdtraj as md
2.1.0,import numpy as np
2.1.0,import tempfile
2.1.0,from rdkit import Chem
2.1.0,from rdkit.Chem import Draw
2.1.0,from itertools import islice
2.1.0,"from IPython.display import Image, HTML, display"
2.1.0,
2.1.0,"def combine_mdtraj(protein, ligand):"
2.1.0,chain = protein.topology.add_chain()
2.1.0,"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
2.1.0,for atom in ligand.topology.atoms:
2.1.0,"protein.topology.add_atom(atom.name, atom.element, residue)"
2.1.0,"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
2.1.0,protein.topology.create_standard_bonds()
2.1.0,return protein
2.1.0,
2.1.0,def visualize_complex(complex_mdtraj):
2.1.0,"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
2.1.0,"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
2.1.0,binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
2.1.0,binding_pocket_residues = [str(r) for r in binding_pocket_residues]
2.1.0,"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
2.1.0,
2.1.0,traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
2.1.0,ngltraj = nglview.NGLWidget( traj )
2.1.0,ngltraj.representations = [
2.1.0,"{ ""type"": ""cartoon"", ""params"": {"
2.1.0,"""sele"": ""protein"", ""color"": ""residueindex"""
2.1.0,"} },"
2.1.0,"{ ""type"": ""licorice"", ""params"": {"
2.1.0,"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
2.1.0,"} },"
2.1.0,"{ ""type"": ""ball+stick"", ""params"": {"
2.1.0,"""sele"": ""LIG"""
2.1.0,} }
2.1.0,]
2.1.0,return ngltraj
2.1.0,
2.1.0,def visualize_ligand(ligand_mdtraj):
2.1.0,traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
2.1.0,ngltraj = nglview.NGLWidget( traj )
2.1.0,ngltraj.representations = [
2.1.0,"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
2.1.0,return ngltraj
2.1.0,
2.1.0,def convert_lines_to_mdtraj(molecule_lines):
2.1.0,tempdir = tempfile.mkdtemp()
2.1.0,"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
2.1.0,"with open(molecule_file, ""wb"") as f:"
2.1.0,f.writelines(molecule_lines)
2.1.0,molecule_mdtraj = md.load(molecule_file)
2.1.0,return molecule_mdtraj
2.1.0,
2.1.0,def display_images(filenames):
2.1.0,"""""""Helper to pretty-print images."""""""
2.1.0,imagesList=''.join(
2.1.0,"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
2.1.0,% str(s) for s in sorted(filenames)])
2.1.0,display(HTML(imagesList))
2.1.0,
2.1.0,"def mols_to_pngs(mols, basename=""test""):"
2.1.0,"""""""Helper to write RDKit mols to png files."""""""
2.1.0,filenames = []
2.1.0,"for i, mol in enumerate(mols):"
2.1.0,"filename = ""%s%d.png"" % (basename, i)"
2.1.0,"Draw.MolToFile(mol, filename)"
2.1.0,filenames.append(filename)
2.1.0,return filenames
2.1.0,TODO(rbharath): This is now simple enough that we should probably get rid of
2.1.0,Evaluator object to avoid clutter.
2.1.0,Compute multitask metrics
2.1.0,Compute multitask metrics
2.1.0,Loosening atol to see if tests stop failing sporadically
2.1.0,One sequence has length longer than others. This should throw a
2.1.0,ValueError.
2.1.0,Test it's possible to load a sequence with an aribrary alphabet from a fasta file.
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,a*x + b*y + c*z = dI think that
2.1.0,"self.x, self.y, self.z = x, y, z"
2.1.0,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
2.1.0,TODO(bramsundar): Should this be __copy__?
2.1.0,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
2.1.0,"return np.array([self.x, self.y, self.z])"
2.1.0,TODO(rbharath): Should this be an atom function?
2.1.0,"This line is necessary for babel to work, though many PDBs in"
2.1.0,the PDB would have this line commented out
2.1.0,now atom type (for pdbqt)
2.1.0,"If atomtype is not specified, but atomname is, set atomtype to the"
2.1.0,"first letter of atomname. This heuristic suffices for proteins,"
2.1.0,since no two-letter elements appear in standard amino acids.
2.1.0,Any number needs to be removed from the element name
2.1.0,"this only uses the rightmost three characters, essentially"
2.1.0,removing unique rotamer identification
2.1.0,"The normal vector to plane is n = [a, b, c]"
2.1.0,We first shift by basepoint (a point on given plane) to make math
2.1.0,simpler. basepoint is given by d/||n||^2 * n
2.1.0,The perpendicular component of diff to plane is
2.1.0,(n^T diff / ||n||^2) * n
2.1.0,if ring is aromatic
2.1.0,"save its indices, center, and normal"
2.1.0,remember protein-ligand pairs we already counted
2.1.0,"if this pair is new, count atoms forming a contact"
2.1.0,"if this pair is new, count atoms forming a contact"
2.1.0,if ring from mol1 is aromatic
2.1.0,...and atom from mol2 is a cation
2.1.0,if angle and distance are correct
2.1.0,count atoms forming a contact
2.1.0,find interacting rings from protein and cations from ligand
2.1.0,find interacting cations from protein and rings from ligand
2.1.0,merge counters
2.1.0,TODO(LESWING)
2.1.0,check if user tries to set removed arguments
2.1.0,list of features that require sanitized molecules
2.1.0,not implemented featurization types
2.1.0,default values
2.1.0,update with cutoffs specified by the user
2.1.0,define methods to calculate available flat features
2.1.0,all methods (flat and voxel) must have the same API:
2.1.0,"f(prot_xyz, prot_rdk, lig_xyz, lig_rdk, distances) -> list of np.ndarrays"
2.1.0,define methods to calculate available voxel features
2.1.0,"each entry is a tuple (is_flat, feature_name)"
2.1.0,list of features that cannot be calculated with specified parameters
2.1.0,this list is used to define <flat/voxel/all>_combined subset
2.1.0,parse provided feature types
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Get the degree id list (which corrects for min_deg)
2.1.0,Get the size of each degree block
2.1.0,Get the the start indices for items in each block
2.1.0,Get the node indices when they are reset when the degree changes
2.1.0,Convert to numpy array
2.1.0,Reorder old atom_features
2.1.0,Reorder old deg lists
2.1.0,Sort membership
2.1.0,Create old to new dictionary. not exactly intuitive
2.1.0,Reorder adjacency lists
2.1.0,Get numpy version of degree list for indexing
2.1.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.1.0,Parse as deg separated
2.1.0,Get indices corresponding to the current degree
2.1.0,Extract and save adjacency list for the current degree
2.1.0,Construct the slice information
2.1.0,Get the cumulative indices after the first index
2.1.0,Set indices with zero sized slices to zero to avoid indexing errors
2.1.0,TODO(rbharath): Can this be removed?
2.1.0,Use random insted of zeros to prevent weird issues with summing to zero
2.1.0,Get atoms by degree
2.1.0,stack the atoms
2.1.0,Sort all atoms by degree.
2.1.0,"Get the size of each atom list separated by molecule id, then by degree"
2.1.0,Get the final size of each degree block
2.1.0,"Get the index at which each degree starts, not resetting after each degree"
2.1.0,And not stopping at any speciic molecule
2.1.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.1.0,first column telling the start indices of each degree block and the
2.1.0,second colum telling the size of each degree block
2.1.0,Input for tensorflow
2.1.0,Determines the membership (atom i belongs to membership[i] molecule)
2.1.0,"Get the index at which each deg starts, resetting after each degree"
2.1.0,(deg x num_mols) matrix describing the start indices when you count up the atoms
2.1.0,"in the final representation, stopping at each molecule,"
2.1.0,resetting every time the degree changes
2.1.0,Gets the degree resetting block indices for the atoms in each molecule
2.1.0,"Here, the indices reset when the molecules change, and reset when the"
2.1.0,degree changes
2.1.0,Get the degree id lookup list. It allows us to search for the degree of a
2.1.0,molecule mol_id with corresponding atom mol_atom_id using
2.1.0,"deg_id_lists[mol_id,mol_atom_id]"
2.1.0,This is used for convience in the following function (explained below)
2.1.0,Get the degree id (corrected for min_deg) of the considered atom
2.1.0,Return the final index of atom mol_atom_id in molecule mol_id.  Using
2.1.0,"the degree of this atom, must find the index in the molecule's original"
2.1.0,"degree block corresponding to degree id deg_id (second term), and then"
2.1.0,calculate which index this degree block ends up in the final
2.1.0,representation (first term). The sum of the two is the final indexn
2.1.0,Initialize the new degree separated adjacency lists
2.1.0,Update the old adjcency lists with the new atom indices and then combine
2.1.0,all together
2.1.0,Iterate through all the molecules
2.1.0,Get the adjacency lists for this molecule and current degree id
2.1.0,"Correct all atom indices to the final indices, and then save the"
2.1.0,results into the new adjacency lists
2.1.0,Increment once row is done
2.1.0,Get the final aggregated molecule
2.1.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.1.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.1.0,consistent with most QM software packages.
2.1.0,Type of data created by this featurizer
2.1.0,TODO(rbharath): Should this return a list?
2.1.0,Type of data created by this featurizer
2.1.0,generate SMILES for fragments
2.1.0,Initalize with 1
2.1.0,Allow 0 index to correspond to null molecule 1
2.1.0,Correct for null
2.1.0,"print(6-k-1, id)"
2.1.0,Correct for last one
2.1.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.1.0,first `bt_len` features are bond features(if applicable)
2.1.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.1.0,graph distance between two atoms
2.1.0,Euclidean distance between atoms
2.1.0,atoms `radial` bonds away from `a1`
2.1.0,atoms less than `radial` bonds away
2.1.0,find atoms `radial`+1 bonds away
2.1.0,Get the node features
2.1.0,Stack nodes into an array
2.1.0,Get bond lists with reverse edges included
2.1.0,Get canonical adjacency list
2.1.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.1.0,only support datasets providing Cartesian coordinates)
2.1.0,Set dtype
2.1.0,If includes explicit hydrogens
2.1.0,If uses use_chirality
2.1.0,Atom features
2.1.0,Stack nodes into an array
2.1.0,Get bond lists
2.1.0,Get canonical adjacency list
2.1.0,Calculate pair features
2.1.0,atom_name is of format RESX-ATOMTYPE
2.1.0,where X is a 1 to 4 digit number
2.1.0,list-of-available-descriptors.
2.1.0,(ytz): This is done to avoid future compatibility issues like inclusion of
2.1.0,the 3D descriptors or changing the feature size.
2.1.0,check for separate count and SMILES entries for each fragment
2.1.0,TODO test more formats for ligand
2.1.0,some users might try to read smiles with this function
2.1.0,adding hydrogens and charges is tested in dc.utils
2.1.0,3D vector with unit length
2.1.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.1.0,check if distances do not change
2.1.0,check if it works for molecules with different numbers of atoms
2.1.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.1.0,check if correct distance metric was used
2.1.0,"20 points with coords between -5 and 5, centered at 0"
2.1.0,indices are positive
2.1.0,coordinates were properly translated and scaled
2.1.0,for coordinates outside of the box function should properly transform them
2.1.0,to indices and warn the user
2.1.0,"TODO check if function warns. There is assertWarns method in unittest,"
2.1.0,but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
2.1.0,"20 points with coords between -5 and 5, centered at 0"
2.1.0,3 pairs of indices
2.1.0,simple flat ring
2.1.0,load and sanitize two real molecules
2.1.0,FIXME might break with different version of rdkit
2.1.0,FIXME might break with different version of rdkit
2.1.0,parallel normals
2.1.0,perpendicular normals
2.1.0,too far away
2.1.0,perpendicular normals
2.1.0,parallel normals
2.1.0,too far away
2.1.0,order of the molecules shouldn't matter
2.1.0,with this criteria we should find both types of stacking
2.1.0,parallel normals
2.1.0,perpendicular normals
2.1.0,too far away
2.1.0,"TODO find better example, currently dicts are empty"
2.1.0,"TODO find better example, currently dicts are empty"
2.1.0,TODO test if dict contains smiles
2.1.0,check if results are the same if we provide precomputed distances
2.1.0,...but first check if we actually got two dicts
2.1.0,check if we get less features with smaller distance cutoff
2.1.0,ligands are typically small so all atoms might be present
2.1.0,check if using different ecfp_degree changes anything
2.1.0,TODO upperbound?
2.1.0,test if default parameters work
2.1.0,check if use-case from examples works
2.1.0,test if input is flattened when flat features are used
2.1.0,test voxel features
2.1.0,test flat features
2.1.0,check if aromatic features are ignores if sanitize=False
2.1.0,"protein is too big for the box, some features should be missing"
2.1.0,whole ligand should fit in the box
2.1.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.1.0,of degree 1 (connected only to central nitrogen).
2.1.0,5 atoms in compound
2.1.0,Get the adjacency lists grouped by degree
2.1.0,The 4 outer atoms connected to central nitrogen
2.1.0,Central nitrogen connected to everything else.
2.1.0,Only one carbon
2.1.0,"No bonds, so degree adjacency lists are empty"
2.1.0,3 carbonds in alkane
2.1.0,Outer two carbonds are connected to central carbon
2.1.0,Central carbon connected to outer two
2.1.0,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
2.1.0,this expected behavior? Need to think about API.
2.1.0,Do a manual distance computation and make
2.1.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.1.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.1.0,Do a manual distance computation and ensure that selected neighbor is
2.1.0,closest since we set max_num_neighbors = 1
2.1.0,Splits featurized samples into train/test
2.1.0,Artificial feature array.
2.1.0,0 atoms of degree 0
2.1.0,0 atoms of degree 1
2.1.0,4 atoms of degree 2
2.1.0,0 atoms of degree 3
2.1.0,0 atoms of degree 4
2.1.0,0 atoms of degree 5
2.1.0,0 atoms of degree 6
2.1.0,0 atoms of degree 7
2.1.0,0 atoms of degree 8
2.1.0,0 atoms of degree 9
2.1.0,0 atoms of degree 10
2.1.0,atom 4 has 0 neighbors
2.1.0,atom 0 has 2 neighbors
2.1.0,atom 1 has 2 neighbors
2.1.0,atom 2 has 2 neighbors
2.1.0,atom 3 has 3 neighbors.
2.1.0,Verify that atom features have been sorted by atom degree.
2.1.0,Sorting is done by atom degree as before. So the ordering goes
2.1.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.1.0,from new position to old position is
2.1.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.1.0,list respects this reordering and returns correct adjacency list.
2.1.0,### First example molecule
2.1.0,Artificial feature array.
2.1.0,### Second example molecule
2.1.0,## Third example molecule
2.1.0,Test agglomerate molecule method
2.1.0,No atoms of degree 0
2.1.0,3 atoms of degree 1
2.1.0,8 atoms of degree 2
2.1.0,1 atom of degree 3
2.1.0,0 atoms of degree 4
2.1.0,0 atoms of degree 5
2.1.0,Check that atoms are only connected to themselves.
2.1.0,Check that there's one atom of each degree.
2.1.0,assumes that every array is of the same dimension
2.1.0,rem_dataset is remaining portion of dataset
2.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.1.0,to k-1.
2.1.0,dict is needed in case groups aren't strictly flattened or
2.1.0,hashed by something non-integer like
2.1.0,returns list of per column sum of non zero elements
2.1.0,Compute number of actives needed per task.
2.1.0,loop through each column and obtain index required to splice out for
2.1.0,required fraction of hits
2.1.0,Find the first index where the cumulative number of actives equals
2.1.0,the actives_count
2.1.0,Note that np.where tells us last index required to exceed
2.1.0,"actives_count, so we actually want the following location"
2.1.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.1.0,potentially refactor those to match this.
2.1.0,Handle edge case where frac_split is 1
2.1.0,Create weight matrices fpor two haves.
2.1.0,copy over up to required index for weight first_split
2.1.0,check out if any rows in either w_1 or w_2 are just zeros
2.1.0,"Obtain original x, y, and w arrays and shuffle"
2.1.0,calculate percent split for valid (out of test and valid)
2.1.0,"split test data into valid and test, treating sub test set also as sparse"
2.1.0,rem_dataset is remaining portion of dataset
2.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.1.0,to k-1.
2.1.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.1.0,This can be generalized in the future with some common demoninator determination.
2.1.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.1.0,Append remaining examples to train
2.1.0,Sort by increasing MW
2.1.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.1.0,for m_idx in cluster:
2.1.0,"continue until we find an active in all the tasks, otherwise we can't"
2.1.0,compute a meaningful AUC
2.1.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.1.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.1.0,Sort from largest to smallest scaffold sets
2.1.0,Pick the mol closest to everything as the first element of training
2.1.0,Pick the closest mol from what is left
2.1.0,Test is everything else
2.1.0,All datasets share features and identifiers by assumption.
2.1.0,TODO(rbharath): Get rid of * import
2.1.0,Note that the extra task goes to test
2.1.0,Number tasks per fold
2.1.0,Find the tasks that correspond to this test fold
2.1.0,Assert that all arrays look like they should
2.1.0,0 1 2 3 4 5 6 7 8 9
2.1.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.1.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.1.0,reshard() to handle this?
2.1.0,Verify lengths is 10/k == 2
2.1.0,Verify that compounds in this fold are subset of original compounds
2.1.0,Verify that no two folds have overlapping compounds.
2.1.0,Verify lengths is 10/k == 2
2.1.0,Verify that compounds in this fold are subset of original compounds
2.1.0,Verify that no two folds have overlapping compounds.
2.1.0,Verify lengths is 10/k == 2
2.1.0,Verify that compounds in this fold are subset of original compounds
2.1.0,Verify that no two folds have overlapping compounds.
2.1.0,Test singletask case.
2.1.0,The split index should partition dataset in half.
2.1.0,Test singletask case.
2.1.0,Test case where some weights are zero (i.e. masked)
2.1.0,Set half the positives to have zero weight
2.1.0,There are 10 nonzero actives.
2.1.0,"The split index should partition this into half, so expect 5"
2.1.0,The split index should partition dataset in half.
2.1.0,Mask half the examples
2.1.0,The split index should partition dataset in half.
2.1.0,Test singletask case.
2.1.0,Should have split cleanly in half (picked random seed to ensure this)
2.1.0,Check positives are correctly distributed
2.1.0,Verify lengths is 100/k == 20
2.1.0,Note: This wouldn't work for multitask str
2.1.0,assert len(fold_dataset) == n_samples/K
2.1.0,Verify that each fold has n_positives/K = 4 positive examples.
2.1.0,Verify that compounds in this fold are subset of original compounds
2.1.0,Verify that no two folds have overlapping compounds.
2.1.0,sparsity is determined by number of w weights that are 0 for a given
2.1.0,task structure of w np array is such that each row corresponds to a
2.1.0,sample. The loaded sparse dataset has many rows with only zeros
2.1.0,verify that there are no rows (samples) in weights matrix w
2.1.0,that have no hits.
2.1.0,task_metadata_rows = {task: [] for task in tasks}
2.1.0,Extract those datapoints which are present for this task
2.1.0,Loading is done on-the-fly
2.1.0,TODO(rbharath/enf): We need a structured way to deal with potential GPU
2.1.0,memory overflows.
2.1.0,Discard any padded predictions
2.1.0,################### Compatibility imports for renamed TensorGraph models. Remove below with DeepChem 3.0. ####################
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,Calculate pairwise distance
2.1.0,Masking for valid atom index
2.1.0,Cutoff with threshold Rc
2.1.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.1.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.1.0,optimization to allow for tensorcontraction/broadcasted mmul
2.1.0,using a reshape trick. Note that the np and tf matmul behavior
2.1.0,differs when dealing with broadcasts
2.1.0,-*- coding: UTF-8 -*-
2.1.0,Reshape everything to match the input with the most dimensions.
2.1.0,"This probably means the variable hasn't been created yet, so try again"
2.1.0,with reuse set to false.
2.1.0,Calculate what the new shape will be.
2.1.0,"Shape (N_atoms, M_nbrs, ndim)"
2.1.0,"Shape (N_atoms, M_nbrs, ndim)"
2.1.0,"Shape (N_atoms, M_nbrs)"
2.1.0,"This probably means the variable hasn't been created yet, so try again"
2.1.0,with reuse set to false.
2.1.0,"This probably means the variable hasn't been created yet, so try again"
2.1.0,with reuse set to false.
2.1.0,"This probably means the variable hasn't been created yet, so try again"
2.1.0,with reuse set to false.
2.1.0,"This probably means the variable hasn't been created yet, so try again"
2.1.0,with reuse set to false.
2.1.0,TODO(rbharath): Note sure if this layer can be called with __call__
2.1.0,"meaningfully, so not going to support that functionality for now."
2.1.0,Generate the nb_affine weights and biases
2.1.0,"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
2.1.0,Extract atom_features
2.1.0,Extract graph topology
2.1.0,Perform the mol conv
2.1.0,"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
2.1.0,"self.max_deg, self.min_deg, W_list,"
2.1.0,b_list)
2.1.0,Sum all neighbors using adjacency matrix
2.1.0,Get collection of modified atom features
2.1.0,Obtain relevant atoms for this degree
2.1.0,Get self atoms
2.1.0,Apply hidden affine to relevant atoms and append
2.1.0,Determine the min_deg=0 case
2.1.0,Only use the self layer
2.1.0,Combine all atoms back into the list
2.1.0,Tensorflow correctly processes empty lists when using concat
2.1.0,"Sum along neighbors as well as self, and store"
2.1.0,Perform the mol gather
2.1.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.1.0,"self.max_degree, self.min_degree)"
2.1.0,Tensorflow correctly processes empty lists when using concat
2.1.0,Get self atoms
2.1.0,Expand dims
2.1.0,always deg-1 for deg_adj_lists
2.1.0,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
2.1.0,Extract graph topology
2.1.0,Perform the mol gather
2.1.0,Obtain the partitions for each of the molecules
2.1.0,Sum over atoms for each molecule
2.1.0,Get the final sparse representations
2.1.0,No other forget biases supported right now.
2.1.0,Taken from Keras code [citation needed]
2.1.0,"x is test set, xp is support set."
2.1.0,## Performs computations
2.1.0,Get initializations
2.1.0,Process using attention
2.1.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.1.0,Generate new attention states
2.1.0,Support set lstm
2.1.0,Test lstm
2.1.0,self.build()
2.1.0,Get initializations
2.1.0,Rename support
2.1.0,Process support xp using attention
2.1.0,Get linear combination of support set
2.1.0,Process test x using attention
2.1.0,Generate new support attention states
2.1.0,Generate new test attention states
2.1.0,Redefine
2.1.0,Number of rotatable bonds
2.1.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.1.0,a difference.
2.1.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.1.0,neighbors lists an argument instead of a part of this layer.
2.1.0,"Shape (N, M)"
2.1.0,"Shape (N, M)"
2.1.0,"Shape (N, M)"
2.1.0,Number of grid cells
2.1.0,TODO(rbharath): Support batching
2.1.0,"Shape (n_cells, ndim)"
2.1.0,"List of length N_atoms, each element of different length uniques_i"
2.1.0,"List of length N_atoms, each element of different length uniques_i"
2.1.0,"List of length N_atoms, each a tensor of shape"
2.1.0,"(uniques_i, ndim)"
2.1.0,Add phantom atoms that exist far outside the box
2.1.0,"List of length N_atoms, each of shape (1, ndim)"
2.1.0,TODO(rbharath): How does distance need to be modified here to
2.1.0,account for periodic boundary conditions?
2.1.0,List of length N_atoms each of shape (M_nbrs)
2.1.0,"N_atoms elts of size (M_nbrs,) each"
2.1.0,"Shape (N_atoms, 1)"
2.1.0,Find M_nbrs atoms closest to each cell
2.1.0,"Shape (n_cells, M_nbrs)"
2.1.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.1.0,"conditions, so does wrapround. O(constant)"
2.1.0,"Shape (n_cells, n_nbr_cells)"
2.1.0,"Shape (N_atoms, n_nbr_cells)"
2.1.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.1.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.1.0,"List of length N_atoms, each element length uniques_i"
2.1.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.1.0,element removed to remove self from list of neighbors. Need to verify
2.1.0,this holds more broadly or come up with robust alternative.
2.1.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.1.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.1.0,Shape (N_atoms*n_cells)
2.1.0,"Shape (n_cells, N_atoms)"
2.1.0,Find k atoms closest to this cell. Notice negative sign since
2.1.0,tf.nn.top_k returns *largest* not smallest.
2.1.0,"Tensor of shape (n_cells, M_nbrs)"
2.1.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.1.0,"Shape (N_atoms*n_cells, 1) after tile"
2.1.0,9 neighbors in 2-space
2.1.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.1.0,Number of cells for cube in 3-space is
2.1.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.1.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.1.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.1.0,the cube.
2.1.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.1.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.1.0,"Tile (a, a, a, b, b, b, etc.)"
2.1.0,"Tile (a, b, c, a, b, c, ...)"
2.1.0,N: Maximum number of atoms
2.1.0,M: Maximum number of neighbors
2.1.0,d: Number of coordinates/features/filters
2.1.0,B: Batch Size
2.1.0,We apply the radial pooling filter before atom type conv
2.1.0,to reduce computation
2.1.0,check that there isnt just one or zero inputs
2.1.0,create subspaces
2.1.0,create the alpha learnable parameters
2.1.0,"concatenate subspaces, reshape to size of original input, then stack"
2.1.0,"such that out_tensor has shape (2,?,original_cols)"
2.1.0,creates subspaces the same way it was done in AlphaShare
2.1.0,calculate squared Frobenius norm
2.1.0,"(TODO YTZ:) faster, less memory intensive way"
2.1.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.1.0,"r = tf.expand_dims(r, -1)"
2.1.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.1.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.1.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.1.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.1.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.1.0,Calculate pairwise distance
2.1.0,Cutoff with threshold Rc
2.1.0,return d
2.1.0,tf.stack issues again...
2.1.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.1.0,We do not need the mask because every graph has self.num_vertices vertices now
2.1.0,So the Tensor has known dimensions
2.1.0,Add in features
2.1.0,Add in labels
2.1.0,Add in all layers
2.1.0,The last layer is the output of the model
2.1.0,TODO(rbharath): Add in support for additional
2.1.0,losses.
2.1.0,TODO(rbharath): The TensorGraph can't be built until
2.1.0,fit is called since the shapes of features/labels
2.1.0,not specified. Need to figure out a good restoration
2.1.0,method for this use case.
2.1.0,ensure that randomness is conditioned by the Numpy RNG
2.1.0,ensure that randomness is conditioned by the Numpy RNG
2.1.0,TODO(rbharath): Should probably swap this over to tf mode.
2.1.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.1.0,"expects logits, Tensorflow expects probabilities."
2.1.0,scale preds so that the class probas of each sample sum to 1
2.1.0,manual computation of crossentropy
2.1.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.1.0,"expects logits, Tensorflow expects probabilities."
2.1.0,if our output includes timesteps we need to reshape
2.1.0,Arguments
2.1.0,Returns
2.1.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.1.0,"expects logits, Tensorflow expects probabilities."
2.1.0,transform back to logits
2.1.0,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
2.1.0,a tensor. Confusing with tf.zeros...
2.1.0,Transpose for mul
2.1.0,exclude bias variables
2.1.0,"tf.scalar_summary('Weight Decay Cost', cost)"
2.1.0,TODO(user): gradient clipping (see Minimize)
2.1.0,Assuming convolution kernels (2D or 3D).
2.1.0,"TF kernel shape: (..., input_depth, depth)"
2.1.0,No specific assumptions.
2.1.0,References
2.1.0,References
2.1.0,References
2.1.0,References
2.1.0,Pick the one with the correct shape.
2.1.0,Add the input features.
2.1.0,Add the shared dense layers
2.1.0,Add task-specific bypass layers
2.1.0,Add the input features.
2.1.0,Add the shared dense layers
2.1.0,Add task-specific bypass layers
2.1.0,Add the input features.
2.1.0,Add the dense layers
2.1.0,Compute the loss function for each label.
2.1.0,Add the input features.
2.1.0,Add the dense layers
2.1.0,Compute the loss function for each label.
2.1.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.1.0,Similarity values
2.1.0,Labels for all top K similar samples
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.1.0,and embeddings of atom j(both gone through a hidden layer)
2.1.0,"for atom i, sum the influence from all other atom j in the molecule"
2.1.0,number of inputs each step
2.1.0,Add trainable weights
2.1.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.1.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.1.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.1.0,initialize graph features for each graph
2.1.0,initialize graph features for each graph
2.1.0,another row of zeros is generated for padded dummy atoms
2.1.0,`count`-th step
2.1.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.1.0,generating index for graph features used in the inputs
2.1.0,"extracting graph features for parents of the target atoms, then flatten"
2.1.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.1.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.1.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.1.0,of shape: (batch_size*max_atoms) * n_graph_features
2.1.0,representing the graph features of target atoms in each graph
2.1.0,index for targe atoms
2.1.0,update the graph features for target atoms
2.1.0,Add trainable weights
2.1.0,Extract atom_features
2.1.0,Extract atom_features
2.1.0,sum all graph outputs
2.1.0,"Default message function: edge network, update function: GRU"
2.1.0,more options to be implemented
2.1.0,Extract atom_features
2.1.0,Add trainable weights
2.1.0,Extract atom_features
2.1.0,Add another value(~-Inf) to prevent error in softmax
2.1.0,Model using this layer must set pad_batches=True
2.1.0,Perform one step of LSTM
2.1.0,Arguments
2.1.0,Aliases.
2.1.0,Layer Management
2.1.0,Singular place to hold Tensor objects which don't serialize
2.1.0,These have to be reconstructed on restoring from pickle
2.1.0,See TensorGraph._get_tf() for more details on lazy construction
2.1.0,In eager mode we want an optimizer and a function to compute the
2.1.0,gradient of the loss.
2.1.0,In graph mode we want a training operation.
2.1.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.1.0,"we try to read more batches than the total number that get queued,"
2.1.0,this thread will hang indefinitely.
2.1.0,"TODO Once we drop Python 2 support, turn outputs into a proper keyword arg"
2.1.0,instead of using the **kwargs hack.
2.1.0,Gather results for each output
2.1.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.1.0,"we try to read more batches than the total number that get queued,"
2.1.0,this thread will hang indefinitely.
2.1.0,"If only one output, just return array"
2.1.0,"In eager mode, we need to execute every layer once to ensure its variables"
2.1.0,have been created.
2.1.0,"We can't execute Input layers in eager mode, since they would try"
2.1.0,to create placeholders.  Instead create a tensor of the correct
2.1.0,size and type.
2.1.0,Build the layers.
2.1.0,Initialize variables.
2.1.0,In graph mode we need to create the computation graph.
2.1.0,Ensure all training operators have been created.
2.1.0,Initialize variables.
2.1.0,"As a sanity check, make sure all tensors have the correct shape."
2.1.0,Remove out_tensor from the object to be pickled
2.1.0,Pickle itself
2.1.0,add out_tensor back to everyone
2.1.0,The loss doesn't depend on any variables.
2.1.0,Check the inputs.
2.1.0,Define a function that recursively creates tensors from layers.
2.1.0,Define the model function.
2.1.0,Define the inputs.
2.1.0,"Create the correct outputs, based on the mode."
2.1.0,Create the Estimator.
2.1.0,Add or remove dimensions of size 1 to match the shape of the layer.
2.1.0,Should we keep a separate global step count for each submodel?
2.1.0,Add the input features.
2.1.0,Weight decay not activated
2.1.0,Handle output layer
2.1.0,Iterate over all previous tasks.
2.1.0,prev_layers is a list with elements of size
2.1.0,"(batch_size, layer_sizes[i-1])"
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"But if we specify a different starting state, that should produce a"
2.1.0,different result.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"But if we specify a different starting state, that should produce a"
2.1.0,different result.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,TODO What should shape[1] be?  It's not documented.
2.1.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"TODO What should the output shape be?  It's not documented, and there"
2.1.0,are no other test cases for it.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,"Creating a second layer should produce different results, since it has"
2.1.0,different random weights.
2.1.0,But evaluating the first layer again should produce the same result as before.
2.1.0,Set by variable constructor.
2.1.0,Set by set_variable_initial_values().
2.1.0,Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
2.1.0,Optimize the main loss.  This should send both variables toward 1.
2.1.0,Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
2.1.0,"If we don't specify the initial state, it should default to zeros."
2.1.0,Explicitly specifying the zero state should give the same result.
2.1.0,Specifying a different initial state should produce a different result.
2.1.0,We should get the same result with either predict_on_batch() or __call__().
2.1.0,See if it has done a plausible job of learning the distribution.
2.1.0,See if it has done a plausible job of learning the distribution.
2.1.0,We have to set the gradient penalty very small because the generator's
2.1.0,"output is only a single number, so the default penalty would constrain"
2.1.0,it far too much.
2.1.0,See if it has done a plausible job of learning the distribution.
2.1.0,"This isn't a meaningful loss, but just for test"
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.1.0,"Now an (N, M) shape"
2.1.0,TODO(rbharath): Move this into a model directly
2.1.0,def test_vina(self):
2.1.0,"""""""Test that vina graph can be constructed in TensorGraph."""""""
2.1.0,N_protein = 4
2.1.0,N_ligand = 1
2.1.0,N_atoms = 5
2.1.0,M_nbrs = 2
2.1.0,ndim = 3
2.1.0,start = 0
2.1.0,stop = 4
2.1.0,nbr_cutoff = 1
2.1.0,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
2.1.0,start))
2.1.0,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
2.1.0,start))
2.1.0,y = NumpyDataset(np.random.rand(
2.1.0,"1,))"
2.1.0,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
2.1.0,"# used in the current implementation. This is obviously wrong, but need"
2.1.0,# to dig out why this is happening.
2.1.0,"prot_coords = Feature(shape=(N_protein, ndim))"
2.1.0,"ligand_coords = Feature(shape=(N_ligand, ndim))"
2.1.0,"labels = Label(shape=(1,))"
2.1.0,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
2.1.0,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
2.1.0,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
2.1.0,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
2.1.0,"# Now an (N, M) shape"
2.1.0,nbr_list = NeighborList(
2.1.0,"N_protein + N_ligand,"
2.1.0,"M_nbrs,"
2.1.0,"ndim,"
2.1.0,"nbr_cutoff,"
2.1.0,"start,"
2.1.0,"stop,"
2.1.0,in_layers=[coords])
2.1.0,"# Shape (N, M)"
2.1.0,dists = InteratomicL2Distances(
2.1.0,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
2.1.0,repulsion = VinaRepulsion(in_layers=[dists])
2.1.0,hydrophobic = VinaHydrophobic(in_layers=[dists])
2.1.0,hbond = VinaHydrogenBond(in_layers=[dists])
2.1.0,gauss_1 = VinaGaussianFirst(in_layers=[dists])
2.1.0,gauss_2 = VinaGaussianSecond(in_layers=[dists])
2.1.0,"# Shape (N, M)"
2.1.0,interactions = WeightedLinearCombo(
2.1.0,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
2.1.0,"# Shape (N, M)"
2.1.0,"thresholded = Cutoff(in_layers=[dists, interactions])"
2.1.0,"# Shape (N, M)"
2.1.0,free_energies = VinaNonlinearity(in_layers=[thresholded])
2.1.0,free_energy = ReduceSum(in_layers=[free_energies])
2.1.0,"loss = L2Loss(in_layers=[free_energy, labels])"
2.1.0,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
2.1.0,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
2.1.0,tg.set_loss(loss)
2.1.0,tg.fit_generator(databag.iterbatches(epochs=1))
2.1.0,TODO(rbharath): This test should pass. Fix it!
2.1.0,def test_graph_pool(self):
2.1.0,"""""""Test that GraphPool can be invoked."""""""
2.1.0,out_channels = 2
2.1.0,"n_atoms = 4 # In CCC and C, there are 4 atoms"
2.1.0,"raw_smiles = ['CCC', 'C']"
2.1.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.1.0,featurizer = ConvMolFeaturizer()
2.1.0,mols = featurizer.featurize(mols)
2.1.0,multi_mol = ConvMol.agglomerate_mols(mols)
2.1.0,atom_features = multi_mol.get_atom_features()
2.1.0,degree_slice = multi_mol.deg_slice
2.1.0,membership = multi_mol.membership
2.1.0,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
2.1.0,with self.test_session() as sess:
2.1.0,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
2.1.0,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
2.1.0,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
2.1.0,deg_adjs_tf = []
2.1.0,for deg_adj in deg_adjs:
2.1.0,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
2.1.0,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
2.1.0,out_tensor = GraphPool(out_channels)(*args)
2.1.0,sess.run(tf.global_variables_initializer())
2.1.0,out_tensor = out_tensor.eval()
2.1.0,"assert out_tensor.shape == (n_atoms, out_channels)"
2.1.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.1.0,Should be able to call fit twice without failure.
2.1.0,# TODO(rbharath): Transform these into useful weights.
2.1.0,#class_weight={
2.1.0,"#    True: num_sequences / num_positives,"
2.1.0,#    False: num_sequences / num_negatives
2.1.0,"#} if not multitask else None,"
2.1.0,# TODO(rbharath): Add a test with per-class weighting.
2.1.0,#class_weight={
2.1.0,"#    True: num_sequences / num_positives,"
2.1.0,#    False: num_sequences / num_negatives
2.1.0,"#} if not multitask else None,"
2.1.0,Train the model on random sequences.  We aren't training long enough to
2.1.0,"really make it reliable, but I want to keep this test fast, and it should"
2.1.0,still be able to reproduce a reasonable fraction of input sequences.
2.1.0,Test it out.
2.1.0,Check that it got at least a quarter of them correct.
2.1.0,Test it out.
2.1.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.1.0,"few steps of training to make sure nothing crashes, then check that the"
2.1.0,results are at least internally consistent.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create a TensorGraph model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,Evaluate the model.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create a TensorGraph model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,Evaluate the model.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create a TensorGraph model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,Evaluate the model.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create a TensorGraph model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,Evaluate the model.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create the model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,Evaluate the model.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create a TensorGraph model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,Evaluate the model.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create a TensorGraph model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,Evaluate the model.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,Create a TensorGraph model.
2.1.0,Create an estimator from it.
2.1.0,Train the model.
2.1.0,use central difference since forward difference has a pretty high
2.1.0,approximation error
2.1.0,assert min_coords[1][0] != new_x[3]
2.1.0,assert min_coords[1][1] != new_x[4]
2.1.0,assert min_coords[1][2] != new_x[5]
2.1.0,Predict the output and uncertainty.
2.1.0,Predict the output and uncertainty.
2.1.0,Predict the output and uncertainty.
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Create the inputs.
2.1.0,Create the generators.
2.1.0,Create the discriminators.
2.1.0,Make a copy of the discriminator that takes each generator's output as
2.1.0,its input.
2.1.0,Make a list of all layers in the generators and discriminators.
2.1.0,Compute the loss functions.
2.1.0,Create learnable weights for the generators and discriminators.
2.1.0,Compute the weighted errors
2.1.0,Add an entropy term to the loss.
2.1.0,Create submodels for training the generators and discriminators.
2.1.0,"Every call to fit_generator() will increment global_step, but we only"
2.1.0,"want it to get incremented once for the entire batch, so record the"
2.1.0,value and keep resetting it.
2.1.0,Train the discriminator.
2.1.0,Train the generator.
2.1.0,Write checkpoints and report progress.
2.1.0,Write out final results.
2.1.0,Create a dataset and an input function for processing it.
2.1.0,number of atoms in each molecule
2.1.0,index of pair features
2.1.0,number of pairs for each atom
2.1.0,atom features
2.1.0,pair features
2.1.0,calculation orders for a batch of molecules
2.1.0,padding atom features vector of each molecule with 0
2.1.0,Returns:
2.1.0,Build placeholders
2.1.0,number of atoms in each molecule
2.1.0,index of pair features
2.1.0,number of pairs for each atom
2.1.0,atom features
2.1.0,pair features
2.1.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.1.0,import tensorflow as tf
2.1.0,from deepchem.models.tensorgraph.tensor_graph import MultitaskTensorGraph
2.1.0,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
2.1.0,
2.1.0,
2.1.0,class WeightedError(Layer):
2.1.0,
2.1.0,"def __call__(self, *parents):"
2.1.0,"entropy, weights = parents[0], parents[1]"
2.1.0,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
2.1.0,return self.out_tensor
2.1.0,
2.1.0,
2.1.0,"def MultitaskClassifier(n_tasks,"
2.1.0,"n_features,"
2.1.0,"layer_sizes=[500],"
2.1.0,"bypass_layer_sizes=[100],"
2.1.0,model_dir=None):
2.1.0,""""""""
2.1.0,TODO(LESWING) Add Dropout and regularization
2.1.0,
2.1.0,Parameters
2.1.0,----------
2.1.0,n_tasks
2.1.0,n_features
2.1.0,layer_sizes
2.1.0,bypass_layer_sizes
2.1.0,model_dir
2.1.0,
2.1.0,Returns
2.1.0,-------
2.1.0,
2.1.0,""""""""
2.1.0,g = MultitaskTensorGraph(model_dir=model_dir)
2.1.0,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
2.1.0,g.add_layer(in_layer)
2.1.0,g.add_feature(in_layer)
2.1.0,
2.1.0,# Shared Dense Layers
2.1.0,prev_layer = in_layer
2.1.0,dense_layers = []
2.1.0,for i in range(len(layer_sizes)):
2.1.0,dense = Dense(
2.1.0,"out_channels=layer_sizes[i],"
2.1.0,"name=""SDENSE%s"" % i,"
2.1.0,activation_fn=tf.nn.relu)
2.1.0,"g.add_layer(dense, parents=[prev_layer])"
2.1.0,dense_layers.append(dense)
2.1.0,prev_layer = dense
2.1.0,
2.1.0,# Individual Bypass Layers
2.1.0,costs = []
2.1.0,for task in range(n_tasks):
2.1.0,prev_layer = in_layer
2.1.0,for i in range(len(bypass_layer_sizes)):
2.1.0,dense = Dense(
2.1.0,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
2.1.0,"g.add_layer(dense, parents=[prev_layer])"
2.1.0,prev_layer = dense
2.1.0,"joined_layer = Concat(name=""JOIN%s"" % task)"
2.1.0,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
2.1.0,
2.1.0,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
2.1.0,"g.add_layer(classification, parents=[joined_layer])"
2.1.0,
2.1.0,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
2.1.0,"g.add_layer(softmax, parents=[classification])"
2.1.0,g.add_output(softmax)
2.1.0,
2.1.0,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
2.1.0,g.add_layer(label)
2.1.0,g.add_label(label)
2.1.0,
2.1.0,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
2.1.0,"g.add_layer(cost, parents=[label, classification])"
2.1.0,costs.append(cost)
2.1.0,
2.1.0,"entropy = Concat(name=""ENT"")"
2.1.0,"g.add_layer(entropy, parents=costs)"
2.1.0,
2.1.0,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
2.1.0,g.add_layer(task_weights)
2.1.0,g.set_task_weights(task_weights)
2.1.0,
2.1.0,"loss = WeightedError(name=""ERROR"")"
2.1.0,"g.add_layer(loss, parents=[entropy, task_weights])"
2.1.0,g.set_loss(loss)
2.1.0,
2.1.0,return g
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,(ytz): this is really dirty but needed for restoring models
2.1.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.1.0,SMILES strings
2.1.0,Maximum length is expanded to allow length variation during train and inference
2.1.0,'_' served as delimiter and padding
2.1.0,Initialize common characters as keys
2.1.0,Include space to avoid extra keys
2.1.0,"For 'Cl', 'Br', etc."
2.1.0,"Character not recognized, add to extra_keys"
2.1.0,Add all extra_keys to char_dict
2.1.0,Character embedding
2.1.0,Multiple convolutional layers with different filter widths
2.1.0,Max-over-time pooling
2.1.0,Concat features from all filters(one feature per filter)
2.1.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.1.0,Transform SMILES string to integer vectors
2.1.0,Skip all spaces
2.1.0,"For 'Cl', 'Br', etc."
2.1.0,Padding with '_'
2.1.0,################### Deprecation warnings for renamed TensorGraph models ####################
2.1.0,Do a simple greedy search.
2.1.0,Do a beam search with length normalization.
2.1.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.1.0,This candidate sequence has already been terminated
2.1.0,Consider all possible tokens we could add to this candidate sequence.
2.1.0,update model with best param
2.1.0,Find optimal n_estimators based on original learning_rate
2.1.0,and early_stopping_rounds
2.1.0,"Since test size is 20%, when retrain model to whole data, expect"
2.1.0,n_estimator increased to 1/0.8 = 1.25 time.
2.1.0,Make sure user specified params are in the grid.
2.1.0,Change params back original params
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Check same predictions are made.
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Load trained model
2.1.0,Eval model on train
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Fit trained model
2.1.0,Eval model on train/test
2.1.0,Fit trained model
2.1.0,Eval model on train/test
2.1.0,Test Parameter getting and setting
2.1.0,Fit trained model
2.1.0,Eval model on train/test
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,n_samples = 100
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Load mini log-solubility dataset.
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Load mini log-solubility dataset.
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Load mini log-solubility dataset.
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Load mini log-solubility dataset.
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Load mini log-solubility dataset.
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on train
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Predict the output and uncertainty.
2.1.0,def test_singletask_to_multitask_classification(self):
2.1.0,n_features = 10
2.1.0,n_tasks = 17
2.1.0,tasks = range(n_tasks)
2.1.0,# Define train dataset
2.1.0,n_train = 100
2.1.0,"X_train = np.random.rand(n_train, n_features)"
2.1.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.1.0,w_train = np.ones_like(y_train)
2.1.0,"ids_train = [""C""] * n_train"
2.1.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.1.0,"X_train, y_train, w_train, ids_train)"
2.1.0,# Define test dataset
2.1.0,n_test = 10
2.1.0,"X_test = np.random.rand(n_test, n_features)"
2.1.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.1.0,w_test = np.ones_like(y_test)
2.1.0,"ids_test = [""C""] * n_test"
2.1.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.1.0,"X_test, y_test, w_test, ids_test)"
2.1.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.1.0,def model_builder(model_dir):
2.1.0,sklearn_model = LogisticRegression()
2.1.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.1.0,multitask_model = dc.models.SingletaskToMultitask(
2.1.0,"tasks, model_builder)"
2.1.0,# Fit trained model
2.1.0,multitask_model.fit(train_dataset)
2.1.0,multitask_model.save()
2.1.0,# Eval multitask_model on train/test
2.1.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.1.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.1.0,Generate data
2.1.0,Cleanup
2.1.0,Generate dummy dataset
2.1.0,Fit trained model
2.1.0,Eval model on test
2.1.0,Eval model on train
2.1.0,Fit trained model
2.1.0,Eval model on test
2.1.0,Fit trained model
2.1.0,Eval model on test
2.1.0,def test_sklearn_classification(self):
2.1.0,"""""""Test that sklearn models can learn on simple classification datasets."""""""
2.1.0,np.random.seed(123)
2.1.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.1.0,"X, y = dataset.data, dataset.target"
2.1.0,frac_train = .7
2.1.0,n_samples = len(X)
2.1.0,n_train = int(frac_train*n_samples)
2.1.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.1.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.1.0,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
2.1.0,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
2.1.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.1.0,sklearn_model = LogisticRegression()
2.1.0,model = dc.models.SklearnModel(sklearn_model)
2.1.0,# Fit trained model
2.1.0,model.fit(train_dataset)
2.1.0,model.save()
2.1.0,# Eval model on test
2.1.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.1.0,assert scores[classification_metric.name] > .5
2.1.0,def test_sklearn_multitask_classification(self):
2.1.0,"""""""Test that sklearn models can learn on simple multitask classification."""""""
2.1.0,np.random.seed(123)
2.1.0,n_tasks = 4
2.1.0,tasks = range(n_tasks)
2.1.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.1.0,"X, y = dataset.data, dataset.target"
2.1.0,"y = np.reshape(y, (len(y), 1))"
2.1.0,y = np.hstack([y] * n_tasks)
2.1.0,
2.1.0,frac_train = .7
2.1.0,n_samples = len(X)
2.1.0,n_train = int(frac_train*n_samples)
2.1.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.1.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.1.0,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
2.1.0,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
2.1.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.1.0,def model_builder(model_dir):
2.1.0,sklearn_model = LogisticRegression()
2.1.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.1.0,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
2.1.0,# Fit trained model
2.1.0,model.fit(train_dataset)
2.1.0,model.save()
2.1.0,# Eval model on test
2.1.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.1.0,for score in scores[classification_metric.name]:
2.1.0,assert score > .5
2.1.0,Set early stopping round = n_estimators so that esr won't work
2.1.0,Fit trained model
2.1.0,Eval model on test
2.1.0,Fit trained model
2.1.0,Eval model on test
2.1.0,Fit trained model
2.1.0,Eval model on test
2.1.0,Logistic regression doesn't support weights
2.1.0,-*- coding: utf-8 -*-
2.1.0,Assigning featurizer if not user defined
2.1.0,loading datasets
2.1.0,Assembling train and valid datasets
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,Building tensorflow MultitaskDNN model
2.1.0,Building tensorflow robust MultitaskDNN model
2.1.0,Building scikit logistic regression model
2.1.0,Transform fingerprints to IRV features
2.1.0,Building tensorflow IRV model
2.1.0,Building scikit random forest model
2.1.0,Building scikit learn Kernel SVM model
2.1.0,Building xgboost classification model
2.1.0,Remove token for paddings
2.1.0,Building scikit random forest model
2.1.0,Building scikit learn Kernel Ridge Regression model
2.1.0,Building scikit learn Kernel Ridge Regression model
2.1.0,Building xgboost regression model
2.1.0,Loading hyperparameters
2.1.0,num positive/negative ligands
2.1.0,Set batch sizes for network
2.1.0,Model structure
2.1.0,Traning settings
2.1.0,Fit trained model
2.1.0,Evaluating low data model
2.1.0,-*- coding: utf-8 -*-
2.1.0,Assigning featurizer if not user defined
2.1.0,loading datasets
2.1.0,
2.1.0,Note by @XericZephyr. Reason why I spun off this function:
2.1.0,1. Some model needs dataset information.
2.1.0,2. It offers us possibility to **cache** the dataset
2.1.0,"if the featurizer runs very slow, e.g., GraphConv."
2.1.0,2+. The cache can even happen at Travis CI to accelerate
2.1.0,CI testing.
2.1.0,
2.1.0,loading datasets
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
2.1.0,Featurize qm9 dataset
2.1.0,transformers = [
2.1.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.1.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.1.0,dataset=train_dataset)]
2.1.0,Set shard size low to avoid memory problems.
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Set some global variables up top
2.1.0,Featurize KAGGLE dataset
2.1.0,############################################################# TIMING
2.1.0,############################################################# TIMING
2.1.0,Load Sweetlead dataset
2.1.0,Featurize SWEET dataset
2.1.0,Initialize transformers
2.1.0,Featurize qm7 dataset
2.1.0,Featurize clintox dataset
2.1.0,Transform clintox dataset
2.1.0,Split clintox dataset
2.1.0,Featurize bbb dataset
2.1.0,Initialize transformers
2.1.0,Load nci dataset
2.1.0,Featurize nci dataset
2.1.0,Initialize transformers
2.1.0,Featurize HOPV dataset
2.1.0,Initialize transformers
2.1.0,Featurize PPB dataset
2.1.0,Initialize transformers
2.1.0,Load MUV dataset
2.1.0,Featurize MUV dataset
2.1.0,Initialize transformers
2.1.0,Featurize clearance dataset
2.1.0,Initialize transformers
2.1.0,Featurize TOXCAST dataset
2.1.0,Initialize transformers
2.1.0,Featurize bace dataset
2.1.0,Initialize transformers
2.1.0,Featurize bace dataset
2.1.0,Initialize transformers
2.1.0,Featurize Tox21 dataset
2.1.0,Initialize transformers
2.1.0,Featurize ChEMBL dataset
2.1.0,Initialize transformers
2.1.0,Featurize hiv dataset
2.1.0,Initialize transformers
2.1.0,Featurize SIDER dataset
2.1.0,Initialize transformers
2.1.0,Featurize SAMPL dataset
2.1.0,Initialize transformers
2.1.0,Featurize Delaney dataset
2.1.0,Initialize transformers
2.1.0,Featurize PCBA dataset
2.1.0,Initialize transformers
2.1.0,Featurize Lipophilicity dataset
2.1.0,Initialize transformers
2.1.0,"Float or int hyper parameters(ex. batch_size, learning_rate)"
2.1.0,List of float or int hyper parameters(ex. layer_sizes)
2.1.0,Number of parameters
2.1.0,Range of optimization
2.1.0,Dummy names
2.1.0,Input hyper parameters
2.1.0,Run benchmark
2.1.0,Record hyperparameters
2.1.0,Record performances
2.1.0,"GPGO maximize performance by default, set performance to its negative value for minimization"
2.1.0,Readout best hyper parameters
2.1.0,Compare best model to default hyperparameters
2.1.0,Record hyperparameters
2.1.0,Record performances
2.1.0,"Optimized model is better, return hyperparameters"
2.1.0,Return default hyperparameters
2.1.0,!/usr/bin/env python2
2.1.0,-*- coding: utf-8 -*-
2.1.0,TODO(rbharath): This function is complicated and monolithic. Is there a nice
2.1.0,way to refactor this?
2.1.0,arbitrarily return last model
2.1.0,Define train dataset
2.1.0,Define validation dataset
2.1.0,Have the worker threads generate the rollouts for this iteration.
2.1.0,Perform optimization.
2.1.0,Build the feed dict and run the optimizer.
2.1.0,Update the number of steps taken so far and perform checkpointing.
2.1.0,Merge all the rollouts into a single set of arrays.
2.1.0,Iterate slices.
2.1.0,Generate the rollout.
2.1.0,Compute an estimate of the reward for the rest of the episode.
2.1.0,Compute the discounted rewards and advantages.
2.1.0,Convert the actions to one-hot.
2.1.0,Rearrange the states into the proper set of arrays.
2.1.0,Return the processed arrays.
2.1.0,Generate the rollout.
2.1.0,Compute an estimate of the reward for the rest of the episode.
2.1.0,Compute the discounted rewards and advantages.
2.1.0,"Record the actions, converting to one-hot if necessary."
2.1.0,Rearrange the states into the proper set of arrays.
2.1.0,Build the feed dict and apply gradients.
2.1.0,Run the algorithm.
2.1.0,Save a file checkpoint.
2.1.0,Build the tree.
2.1.0,Compute the final probabilities and expected reward.
2.1.0,Mark this node as terminal
2.1.0,Expand this node.
2.1.0,Select the next action to perform.
2.1.0,Recursively build the tree.
2.1.0,Update statistics for this node.
2.1.0,Assume all arrays are float32.
2.1.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.1.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.1.0,"game).  The average reward for any bet is slightly negative, so the best"
2.1.0,strategy is to walk away.
2.1.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.1.0,Optimize it.
2.1.0,"It should have learned that the expected value is very close to zero, and that the best"
2.1.0,action is to walk away.
2.1.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.1.0,get the same result.
2.1.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.1.0,The environment just has a constant state.
2.1.0,The policy includes a single recurrent layer.
2.1.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.1.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.1.0,"On the first call, the initial state should be all zeros."
2.1.0,It should still be zeros since we didn't save it last time.
2.1.0,It should be different now.
2.1.0,This should be the same as the previous one.
2.1.0,"Now we reset it, so we should get the same result as initially."
2.1.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.1.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.1.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.1.0,at all.  Using hindsight makes it much easier.
2.1.0,A simple policy with two hidden layers.
2.1.0,Optimize it.
2.1.0,Try running it a few times and see if it succeeds.
2.1.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.1.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.1.0,"game).  The average reward for any bet is slightly negative, so the best"
2.1.0,strategy is to walk away.
2.1.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.1.0,Optimize it.
2.1.0,"It should have learned that the expected value is very close to zero, and that the best"
2.1.0,action is to walk away.
2.1.0,"Verify that we can create a new A3C object, reload the parameters from the first one, and"
2.1.0,get the same result.
2.1.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.1.0,The environment just has a constant state.
2.1.0,The policy includes a single recurrent layer.
2.1.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.1.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.1.0,"On the first call, the initial state should be all zeros."
2.1.0,It should still be zeros since we didn't save it last time.
2.1.0,It should be different now.
2.1.0,This should be the same as the previous one.
2.1.0,"Now we reset it, so we should get the same result as initially."
2.1.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.1.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.1.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.1.0,at all.  Using hindsight makes it much easier.
2.1.0,A simple policy with two hidden layers.
2.1.0,Optimize it.
2.1.0,Try running it a few times and see if it succeeds.
2.1.0,The state consists of two numbers: a current value and a target value.
2.1.0,The policy just needs to learn to output the target value (or at least
2.1.0,move toward it).
2.1.0,A simple policy with no hidden layers.
2.1.0,Optimize it.
2.1.0,Try running it and see if it reaches the target
2.1.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.1.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.1.0,"game).  The average reward for any bet is slightly negative, so the best"
2.1.0,strategy is to walk away.
2.1.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.1.0,Optimize it.
2.1.0,"It should have learned that the expected value is very close to zero, and that the best"
2.1.0,action is to walk away.
2.1.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.1.0,get the same result.
2.1.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.1.0,Randomize who goes first
2.1.0,Illegal move -- the square is not empty
2.1.0,Move X
2.1.0,Did X Win
2.1.0,Did O Win
2.1.0,TODO (Bowen): make this function less memory intensive
2.1.0,set 1st column as the column index of dataframe
2.1.0,merge descriptor and activities dataframe into output dataframe based on
2.1.0,"the molecule name, which is the index for both dataframes (but named"
2.1.0,differently). Default merge is inner merge
2.1.0,need to manually set dataframe indexname after merge based on index
2.1.0,from deepchem.scripts.dock_dude import *
2.1.0,from ipyparallel import Client
2.1.0,rc = Client()
2.1.0,dview = rc[:]
2.1.0,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
2.1.0,
2.1.0,"If mol_id is not set, then use isomeric smiles as unique identifier"
2.1.0,iterator = data_df.iterrows()
2.1.0,TODO(rbharath): BROKEN!
2.1.0,Trim unwanted indexing fields
2.1.0,Connect to running ipython server
2.1.0,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
2.1.0,
2.1.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.1.0,you may not use this file except in compliance with the License.
2.1.0,You may obtain a copy of the License at
2.1.0,
2.1.0,http://www.apache.org/licenses/LICENSE-2.0
2.1.0,
2.1.0,"Unless required by applicable law or agreed to in writing, software"
2.1.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.1.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.1.0,See the License for the specific language governing permissions and
2.1.0,limitations under the License.
2.1.0,==============================================================================
2.1.0,Maps from a function name to a dictionary that describes how to
2.1.0,map from an old argument keyword to the new argument keyword.
2.1.0,Mapping from function to the new name of the function
2.1.0,Functions that were reordered should be changed to the new keyword args
2.1.0,"for safety, if positional arguments are used. If you have reversed the"
2.1.0,"positional arguments yourself, this could do the wrong thing."
2.1.0,Specially handled functions.
2.1.0,TODO(aselle): Could check for a literal list of bools and try to convert
2.1.0,them to indices.
2.1.0,all edits are lists of chars
2.1.0,Iterate of each line
2.1.0,sort by column so that edits are processed in order in order to make
2.1.0,indexing adjustments cumulative for changes that change the string
2.1.0,length
2.1.0,"Extract each line to a list of characters, because mutable lists"
2.1.0,"are editable, unlike immutable strings."
2.1.0,Record a description of the change
2.1.0,Make underscore buffers for underlining where in the line the edit was
2.1.0,Iterate for each edit
2.1.0,"Create effective start, end by accounting for change in length due"
2.1.0,to previous edits
2.1.0,Make sure the edit is changing what it should be changing
2.1.0,Make the edit
2.1.0,Create the underline highlighting of the before and after
2.1.0,Keep track of how to generate effective ranges
2.1.0,Finish the report comment
2.1.0,"Strangely, ast.ListComp returns the col_offset of the first token"
2.1.0,after the '[' token which appears to be a bug. Workaround by
2.1.0,explicitly finding the real start of the list comprehension.
2.1.0,loop over lines
2.1.0,Reverse the text to and regular expression search for whitespace
2.1.0,First find if a [ can be found with only whitespace between it and
2.1.0,col.
2.1.0,TODO(aselle):
2.1.0,"this is poor comment detection, but it is good enough for"
2.1.0,cases where the comment does not contain string literal starting/
2.1.0,ending characters. If ast gave us start and end locations of the
2.1.0,"ast nodes rather than just start, we could use string literal"
2.1.0,node ranges to filter out spurious #'s that appear in string
2.1.0,literals.
2.1.0,"Most other nodes return proper locations (with notably does not), but"
2.1.0,it is not possible to use that in an argument.
2.1.0,"Find a simple attribute name path e.g. ""tf.foo.bar"""
2.1.0,Make sure the func is marked as being part of a call
2.1.0,Call special handlers
2.1.0,Examine any non-keyword argument and make it into a keyword argument
2.1.0,if reordering required.
2.1.0,Examine each keyword argument and convert it to the final renamed form
2.1.0,TODO(aselle): We should scan backward to find the start of the
2.1.0,keyword key. Unfortunately ast does not give you the location of
2.1.0,"keyword keys, so we are forced to infer it from the keyword arg"
2.1.0,value.
2.1.0,"Write to a temporary file, just in case we are doing an implace modify."
2.1.0,Broad exceptions are required here because ast throws whatever it wants.
2.1.0,pylint: disable=broad-except
2.1.0,pylint: enable=broad-except
2.1.0,make sure output directory doesn't exist
2.1.0,make sure output directory does not overlap with root_directory
2.1.0,Collect list of files to process (we do this to correctly handle if the
2.1.0,user puts the output directory in some sub directory of the input dir)
2.1.0,import os
2.1.0,"from deepchem.utils.save import load_from_disk, save_to_disk"
2.1.0,from deepchem.featurizers.fingerprints import CircularFingerprint
2.1.0,from deepchem.featurizers.basic import RDKitDescriptors
2.1.0,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
2.1.0,from deepchem.featurizers.grid_featurizer import GridFeaturizer
2.1.0,from deepchem.featurizers.featurize import DataLoader
2.1.0,
2.1.0,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
2.1.0,"print(""About to load dataset form disk."")"
2.1.0,dataset = load_from_disk(dataset_file)
2.1.0,"print(""Loaded dataset."")"
2.1.0,
2.1.0,grid_featurizer = GridFeaturizer(
2.1.0,"voxel_width=16.0, feature_types=""voxel_combined"","
2.1.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.1.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
2.1.0,"parallel=True, flatten=True)"
2.1.0,featurizers = [CircularFingerprint(size=1024)]
2.1.0,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
2.1.0,
2.1.0,#Make a directory in which to store the featurized complexes.
2.1.0,"base_dir = ""../../../grid_nnscore_circular_features"""
2.1.0,if not os.path.exists(base_dir):
2.1.0,os.makedirs(base_dir)
2.1.0,"data_dir = os.path.join(base_dir, ""data"")"
2.1.0,if not os.path.exists(data_dir):
2.1.0,os.makedirs(data_dir)
2.1.0,
2.1.0,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
2.1.0,
2.1.0,"feature_dir = os.path.join(base_dir, ""features"")"
2.1.0,if not os.path.exists(feature_dir):
2.1.0,os.makedirs(feature_dir)
2.1.0,
2.1.0,"samples_dir = os.path.join(base_dir, ""samples"")"
2.1.0,if not os.path.exists(samples_dir):
2.1.0,os.makedirs(samples_dir)
2.1.0,
2.1.0,
2.1.0,
2.1.0,featurizers = compound_featurizers + complex_featurizers
2.1.0,"featurizer = DataLoader(tasks=[""label""],"
2.1.0,"smiles_field=""smiles"","
2.1.0,"protein_pdb_field=""protein_pdb"","
2.1.0,"ligand_pdb_field=""ligand_pdb"","
2.1.0,"compound_featurizers=compound_featurizers,"
2.1.0,"complex_featurizers=complex_featurizers,"
2.1.0,"id_field=""complex_id"","
2.1.0,verbose=False)
2.1.0,from ipyparallel import Client
2.1.0,c = Client()
2.1.0,"print(""c.ids"")"
2.1.0,print(c.ids)
2.1.0,dview = c[:]
2.1.0,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
2.1.0,"worker_pool=dview, shard_size=1024)"
2.1.0,
2.1.0,"save_to_disk(featurized_samples, featurized_samples_file)"
2.1.0,"print(""Preparing ligand %s"" % mol_name)"
2.0.0,!/usr/bin/env python3
2.0.0,-*- coding: utf-8 -*-
2.0.0,Datasets and models used in the benchmark test
2.0.0,"irv, rf, rf_regression should be assigned manually"
2.0.0,Evaluate performances with different training set fraction
2.0.0,Datasets and models used in the benchmark test
2.0.0,Uncomment the two lines below if hyper_parameters are provided
2.0.0,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
2.0.0,hyper_parameters = pickle.load(f)
2.0.0,Will raise a CalledProcessError if fails.
2.0.0,!/usr/bin/env python3
2.0.0,-*- coding: utf-8 -*-
2.0.0,Datasets and models used in the benchmark test
2.0.0,Set numpy seed
2.0.0,##Load data###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Featurize Kinase dataset
2.0.0,##Load data###
2.0.0,num_trials = 5
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,Force matplotlib to not use any Xwindows backend.
2.0.0,##Load data###
2.0.0,the histogram of the data
2.0.0,Set numpy seed
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,num_trials = 5
2.0.0,Set some global variables up top
2.0.0,Fit trained model
2.0.0,Featurize PCBA dataset
2.0.0,Initialize transformers
2.0.0,Fit trained model
2.0.0,Load SWEET dataset
2.0.0,Featurize SWEET dataset
2.0.0,Initialize transformers
2.0.0,Set some global variables up top
2.0.0,removes directory if present -- warning
2.0.0,default split is 80-10-10 train-valid-test split
2.0.0,Fit Logistic Regression models
2.0.0,Fit Logistic Regression models
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,##Load data###
2.0.0,"n_estimators=100, max_features=int(num_features/3),"
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Load tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Featurize FACTORS dataset
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,Force matplotlib to not use any Xwindows backend.
2.0.0,##Load data###
2.0.0,the histogram of the data
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Batch size of models
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Batch size of models
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Batch size of models
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load QM8 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Load Tox21 dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Fit trained model
2.0.0,Set numpy seed
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,Load ChEMBL dataset
2.0.0,Fit models
2.0.0,Do setup required for tf/keras models
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,DeepCrystal Technologies 2017 - Patrick Hop
2.0.0,MIT License - have fun!!
2.0.0,Set to higher values to get better numbers
2.0.0,======================================================================
2.0.0,"Run Benchmarks {GC-DNN, SVR, RF}"
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,Only for debug!
2.0.0,Load Delaney dataset
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Do setup required for tf/keras models
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load Delaney dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load MUV dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Evaluate train/test scores
2.0.0,Load MUV data
2.0.0,Build model
2.0.0,Fit trained model
2.0.0,Evaluate train/test scores
2.0.0,Extract active site
2.0.0,Featurize ligand
2.0.0,Default for CircularFingerprint
2.0.0,Featurize pocket
2.0.0,Note broadcast operation
2.0.0,Compute labels for pockets
2.0.0,Some complexes have labels but no PDB files. Filter these manually
2.0.0,Some of the ligand-names are of form (FMN ox). Use regex
2.0.0,to merge into form (FMN-ox)
2.0.0,Filter if missing PDB files
2.0.0,Load PDBBind dataset
2.0.0,Define featurizers
2.0.0,Featurize Dataset
2.0.0,########################################################## DEBUG
2.0.0,########################################################## DEBUG
2.0.0,For stable runs
2.0.0,Fit trained model
2.0.0,4-fold splits
2.0.0,num positive/negative ligands
2.0.0,10 trials on test-set
2.0.0,Define metric
2.0.0,Get supports on test-set
2.0.0,Compute accuracies
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Test model
2.0.0,Join information for all tasks.
2.0.0,4-fold splits
2.0.0,num positive/negative ligands
2.0.0,Define metric
2.0.0,Get supports on test-set
2.0.0,Compute accuracies
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Test model
2.0.0,Join information for all tasks.
2.0.0,4-fold splits
2.0.0,num positive/negative ligands
2.0.0,10 trials on test-set
2.0.0,Define metric
2.0.0,Get supports on test-set
2.0.0,Compute accuracies
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Test model
2.0.0,Join information for all tasks.
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,number positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply an attention lstm layer
2.0.0,Number of folds for split
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,4-fold splits
2.0.0,num positive/negative ligands
2.0.0,10 trials on test-set
2.0.0,Define metric
2.0.0,Get supports on test-set
2.0.0,Compute accuracies
2.0.0,Train model on support
2.0.0,Test model
2.0.0,Join information for all tasks.
2.0.0,4-fold splits
2.0.0,num positive/negative ligands
2.0.0,10 trials on test-set
2.0.0,Define metric
2.0.0,Get supports on test-set
2.0.0,Compute accuracies
2.0.0,Train model on support
2.0.0,Test model
2.0.0,Join information for all tasks.
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,number positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply an attention lstm layer
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,number positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply an attention lstm layer
2.0.0,Number of folds for split
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Number of folds for split
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply a residual lstm layer
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply a residual lstm layer
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply a residual lstm layer
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply a residual lstm layer
2.0.0,Set some global variables up top
2.0.0,Featurize Tox21 dataset
2.0.0,Initialize transformers
2.0.0,Set some global variables up top
2.0.0,Featurize Tox21 dataset
2.0.0,Initialize transformers
2.0.0,Load MUV dataset
2.0.0,Featurize MUV dataset
2.0.0,Initialize transformers
2.0.0,Load MUV dataset
2.0.0,Featurize MUV dataset
2.0.0,Initialize transformers
2.0.0,Featurize SIDER dataset
2.0.0,Initialize transformers
2.0.0,Featurize SIDER dataset
2.0.0,Initialize transformers
2.0.0,Load the data.
2.0.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
2.0.0,sparse: most tasks do not include data for most molecules.  It also is very
2.0.0,"unbalanced: there are many more negatives than positives.  For each task,"
2.0.0,create a list of alternating postives and negatives so each batch will have
2.0.0,equal numbers of both.
2.0.0,Create the model to train.  We use a simple fully connected network with
2.0.0,one hidden layer.
2.0.0,Define a MetaLearner describing the learning problem.
2.0.0,Run meta-learning on 80% of the tasks.
2.0.0,Validate on the remaining tasks.
2.0.0,Number of folds for split
2.0.0,Depth of attention module
2.0.0,number positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,Apply an attention lstm layer
2.0.0,4-fold splits
2.0.0,10 positive/negative ligands
2.0.0,10 trials on test-set
2.0.0,Sample supports without replacement (all pos/neg should be different)
2.0.0,Define metric
2.0.0,Get supports on test-set
2.0.0,Compute accuracies
2.0.0,Train model on support
2.0.0,Test model
2.0.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
2.0.0,Join information for all tasks.
2.0.0,Number of folds for split
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Number of features on conv-mols
2.0.0,Define metric
2.0.0,Train support model on train
2.0.0,Add layers
2.0.0,4-fold splits
2.0.0,num positive/negative ligands
2.0.0,Define metric
2.0.0,Get supports on test-set
2.0.0,Compute accuracies
2.0.0,Train model on support
2.0.0,Test model
2.0.0,Join information for all tasks.
2.0.0,replace with your own scratch directory
2.0.0,Number of conformations in each file increases exponentially.
2.0.0,Start with a smaller dataset before continuing. Use all of them
2.0.0,for production
2.0.0,"'ani_gdb_s03.h5',"
2.0.0,"'ani_gdb_s04.h5',"
2.0.0,"'ani_gdb_s05.h5',"
2.0.0,"'ani_gdb_s06.h5',"
2.0.0,"'ani_gdb_s07.h5',"
2.0.0,'ani_gdb_s08.h5'
2.0.0,Extract the data
2.0.0,Print the data
2.0.0,self-interaction energies taken from
2.0.0,https://github.com/isayev/ANI1_dataset README
2.0.0,flush once more at the end
2.0.0,"# For production, set nb_epoch to 100+"
2.0.0,"print(""Train scores"")"
2.0.0,print(train_scores)
2.0.0,"print(""Minimization of a single test set structure:"")"
2.0.0,"print(model.minimize_structure(coords, atomic_nums))"
2.0.0,Written by Roman Zubatyuk and Justin S. Smith
2.0.0,Modified by Yutong Zhao to make python2 compatible
2.0.0,opening file
2.0.0,print(store_loc)
2.0.0,print(type(v[0]))
2.0.0,print(k)
2.0.0,print(path)
2.0.0,Number of conformations in each file increases exponentially.
2.0.0,Start with a smaller dataset before continuing. Use all of them
2.0.0,for production
2.0.0,Extract the data
2.0.0,NOTE THE RENAMING:
2.0.0,Note sensitivity = recall
2.0.0,Load nci dataset
2.0.0,Featurize nci dataset
2.0.0,Initialize transformers
2.0.0,Set some global variables up top
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load hiv dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load hiv dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Fit trained model
2.0.0,Fit models
2.0.0,Batch size of models
2.0.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
2.0.0,Fit trained model
2.0.0,Load SIDER dataset
2.0.0,Featurize SIDER dataset
2.0.0,Initialize transformers
2.0.0,Featurize permeability dataset
2.0.0,Load Tox21 dataset
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load SAMPL dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load SAMPL(FreeSolv) dataset
2.0.0,Fit models
2.0.0,Do setup required for tf/keras models
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Gather Projection
2.0.0,Dense post-processing layer
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load clintox dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load clintox dataset
2.0.0,Fit models
2.0.0,Do setup required for tf/keras models
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,-*- coding: utf-8 -*-
2.0.0,#############################################################################
2.0.0,## save dataset
2.0.0,#############################################################################
2.0.0,## load datasets
2.0.0,load sweetfda
2.0.0,load aact
2.0.0,## fixup smiles for matching
2.0.0,return smiles
2.0.0,map original smiles to converted smiles
2.0.0,"## join dataframes, index on smiles"
2.0.0,map original smiles back
2.0.0,## fill all nan with 0
2.0.0,## construct datasets
2.0.0,store in new datasets
2.0.0,## save datasets
2.0.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
2.0.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
2.0.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
2.0.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
2.0.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
2.0.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
2.0.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
2.0.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
2.0.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
2.0.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
2.0.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
2.0.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
2.0.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
2.0.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
2.0.0,For stable runs
2.0.0,Fit trained model
2.0.0,For stable runs
2.0.0,Fit trained model
2.0.0,transformers = [
2.0.0,"dc.trans.LogTransformer(transform_X=True),"
2.0.0,"dc.trans.NormalizationTransformer(transform_y=True,"
2.0.0,dataset=train_dataset)]
2.0.0,Featurize UV dataset
2.0.0,##Load data###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Set numpy seed
2.0.0,##Load data###
2.0.0,##Create model###
2.0.0,Use R2 classification metric
2.0.0,Only use for final evaluation
2.0.0,Force matplotlib to not use any Xwindows backend.
2.0.0,##Load data###
2.0.0,the histogram of the data
2.0.0,##Load data###
2.0.0,###################################################### DEBUG
2.0.0,###################################################### DEBUG
2.0.0,Load HOPV dataset
2.0.0,Fit models
2.0.0,Number of features on conv-mols
2.0.0,Batch size of models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load HOPV dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load HOPV dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load HOPV dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Only for debug!
2.0.0,Load HOPV dataset
2.0.0,Fit models
2.0.0,Fit trained model
2.0.0,Load TOXCAST dataset
2.0.0,Featurize TOXCAST dataset
2.0.0,Initialize transformers
2.0.0,Fit trained model
2.0.0,Processing of ToxCast data
2.0.0,Author - Aneesh Pappu
2.0.0,Loading dataframes and editing indices
2.0.0,Loop through rows of hitc matrix and replace codes with smiles strings
2.0.0,get corresponding casn
2.0.0,get corresponding smiles
2.0.0,write to cell
2.0.0,Tidy up and write to csv
2.0.0,TODO(rbharath): Check that this operation is differentiable.
2.0.0,The number of cells which we should theoretically have
2.0.0,The number of cells which we should theoretically have
2.0.0,"Each atom neighbors tensor should be (k, ndim) shaped."
2.0.0,The number of cells which we should theoretically have
2.0.0,TODO(rbharath): The test below only checks that shapes work out.
2.0.0,Need to do a correctness implementation vs. a simple CPU impl.
2.0.0,The number of cells which we should theoretically have
2.0.0,TODO(rbharath): The test below only checks that shapes work out.
2.0.0,Need to do a correctness implementation vs. a simple CPU impl.
2.0.0,The number of cells which we should theoretically have
2.0.0,TODO(rbharath): The test below only checks that shapes work out.
2.0.0,Need to do a correctness implementation vs. a simple CPU impl.
2.0.0,TODO(rbharath): Commenting this out due to weird segfaults
2.0.0,def test_vina_generate_conformers(self):
2.0.0,"""""""Test that Vina Model can generate conformers"""""""
2.0.0,data_dir = os.path.dirname(os.path.realpath(__file__))
2.0.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
2.0.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
2.0.0,max_protein_atoms = 3500
2.0.0,max_ligand_atoms = 100
2.0.0,"print(""Loading protein file"")"
2.0.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
2.0.0,protein_Z = pad_array(
2.0.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
2.0.0,max_protein_atoms)
2.0.0,"print(""Loading ligand file"")"
2.0.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
2.0.0,ligand_Z = pad_array(
2.0.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
2.0.0,max_ligand_atoms)
2.0.0,Associate each atom with cell it belongs to. O(N*n_cells)
2.0.0,"Shape (n_cells, k)"
2.0.0,"Shape (N, 1)"
2.0.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.0.0,"conditions, so does wrapround. O(constant)"
2.0.0,"Shape (n_cells, 26)"
2.0.0,"Shape (N, 26)"
2.0.0,"coords of shape (N, ndim)"
2.0.0,"Shape (N, 26, k, ndim)"
2.0.0,"Shape (N, 26, k)"
2.0.0,"Shape (N, 26, k)"
2.0.0,"Shape (N, 26, k, ndim)"
2.0.0,"For smaller systems especially, the periodic boundary conditions can"
2.0.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
2.0.0,make sure duplicate neighbors are ignored?
2.0.0,TODO(rbharath): How does distance need to be modified here to
2.0.0,account for periodic boundary conditions?
2.0.0,"Shape (N, 26, k)"
2.0.0,"Shape (N, 26*k)"
2.0.0,TODO(rbharath): This will cause an issue with duplicates!
2.0.0,"Shape (N, M)"
2.0.0,"N elts of size (M,) each"
2.0.0,"Shape (N, 26*k)"
2.0.0,"N elts of size (26*k,) each"
2.0.0,"N elts of size (M,) each"
2.0.0,"Shape (N, M)"
2.0.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.0.0,"N tensors of shape (n_cells, 1)"
2.0.0,"Shape (N*n_cells, 1) after tile"
2.0.0,"List of N tensors of shape (n_cells, 1)"
2.0.0,Lists of length N
2.0.0,Lists of length n_cells
2.0.0,Get indices of k atoms closest to each cell point
2.0.0,TODO(rbharath): tf.stack for tf 1.0
2.0.0,"Tensor of shape (n_cells, k, ndim)"
2.0.0,atoms_in_cells = tf.stack(atoms_in_cells)
2.0.0,"Tensor of shape (26, k, ndim)"
2.0.0,"Reshape to (26*k, ndim)"
2.0.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
2.0.0,"Dists of shape (26*k, 1)"
2.0.0,"Of shape (k, ndim)"
2.0.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
2.0.0,TODO(rbharath): Change this for tf 1.0
2.0.0,"n_cells tensors of shape (N, 1)"
2.0.0,"Shape (N*n_cells, 1) after tile"
2.0.0,"List of n_cells tensors of shape (N, 1)"
2.0.0,Lists of length n_cells
2.0.0,Lists of length n_cells
2.0.0,Get indices of k atoms closest to each cell point
2.0.0,"n_cells tensors of shape (k, ndim)"
2.0.0,"Tensor of shape (n_cells, k)"
2.0.0,TODO(rbharath):
2.0.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
2.0.0,- Need to group closest atoms amongst cell neighbors
2.0.0,- Need to do another top_k to find indices of closest neighbors.
2.0.0,- Return N lists corresponding to neighbors for every atom.
2.0.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.0.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.0.0,"looking for 26 neighbors, which isn't right for boundary cells in"
2.0.0,the cube.
2.0.0,Number of neighbors of central cube in 3-space is
2.0.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
2.0.0,TODO(rbharath)
2.0.0,n_cells = int(cells.get_shape()[0])
2.0.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.0.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.0.0,"Tile (a, a, a, b, b, b, etc.)"
2.0.0,"Tile (a, b, c, a, b, c, ...)"
2.0.0,"Lists of n_cells tensors of shape (N, 1)"
2.0.0,Lists of length n_cells
2.0.0,Lists of length n_cells
2.0.0,Get indices of k atoms closest to each cell point
2.0.0,"n_cells tensors of shape (26,)"
2.0.0,TODO(rbharath): Make this handle minibatches
2.0.0,"Shape (N_protein+N_ligand, 3)"
2.0.0,"Shape (N_protein+N_ligand,)"
2.0.0,"Shape (N_protein+N_ligand, M)"
2.0.0,"Shape (N_protein+N_ligand,)"
2.0.0,"Shape (N_protein+N_ligand, 3)"
2.0.0,"Shape (N_protein+N_ligand,)"
2.0.0,"Shape (N_protein+N_ligand, M)"
2.0.0,"Shape (N_protein+N_ligand, M, 3)"
2.0.0,"Shape (N_protein+N_ligand, M)"
2.0.0,"Shape (N_protein+N_ligand, M, 3)"
2.0.0,"Shape (N_protein+N_ligand, M)"
2.0.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
2.0.0,"Shape (N_protein+N_ligand, M)"
2.0.0,"Shape (N_protein+N_ligand, M)"
2.0.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
2.0.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
2.0.0,computing free-energy. This implementation currently uses all interaction
2.0.0,terms. Not sure if this makes a difference.
2.0.0,"Shape (N_protein+N_ligand, M)"
2.0.0,Shape () -- scalar
2.0.0,Keep track of the layers
2.0.0,"For graphical layers, add connectivity placeholders"
2.0.0,Add layer to the layer list
2.0.0,Keep track of the layers
2.0.0,Create graph topology and x
2.0.0,Keep track of the layers
2.0.0,Whether or not we have used the GraphGather layer yet
2.0.0,Update new value of x
2.0.0,Update new value of x
2.0.0,Update new value of x
2.0.0,Get train function
2.0.0,Initialize
2.0.0,################################################################### DEBUG
2.0.0,self.test_label_placeholder = Input(
2.0.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.0.0,"name=""label_placeholder""))"
2.0.0,self.test_weight_placeholder = Input(
2.0.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
2.0.0,"name=""weight_placeholder""))"
2.0.0,TODO(rbharath): Should weights for the support be used?
2.0.0,Support labels
2.0.0,self.support_label_placeholder = Input(
2.0.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
2.0.0,"name=""support_label_placeholder""))"
2.0.0,################################################################### DEBUG
2.0.0,Generate dictionary elements for support
2.0.0,Get graph information for test
2.0.0,Generate dictionary elements for test
2.0.0,Perform the optimization
2.0.0,Create different support sets
2.0.0,Get batch to try it out on
2.0.0,"Train on support set, batch pair"
2.0.0,Get featurization for test
2.0.0,"Shape (n_test, n_feat)"
2.0.0,Get featurization for support
2.0.0,"Shape (n_support, n_feat)"
2.0.0,Computes the inner part c() of the kernel
2.0.0,(the inset equation in section 2.1.1 of Matching networks paper).
2.0.0,Normalize
2.0.0,TODO(rbharath): euclidean kernel is broken!
2.0.0,elif self.similarity == 'euclidean':
2.0.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
2.0.0,"Note that gram matrix g has shape (n_test, n_support)"
2.0.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
2.0.0,https://arxiv.org/pdf/1606.04080v1.pdf
2.0.0,"Computes softmax across axis 1, (so sums distances to support set for"
2.0.0,each test entry) to get attention vector
2.0.0,"Shape (n_test, n_support)"
2.0.0,Weighted sum of support labels
2.0.0,"Shape (n_support, 1)"
2.0.0,pred is yhat in eqn (1) of Matching Networks.
2.0.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
2.0.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
2.0.0,"Shape (n_test,)"
2.0.0,Convert to logit space using inverse sigmoid (logit) function
2.0.0,logit function: log(pred) - log(1-pred)
2.0.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
2.0.0,in Cross Entropy calculation.
2.0.0,"Shape (n_test,)"
2.0.0,Get scores
2.0.0,Remove padded elements
2.0.0,Get scores
2.0.0,pred corresponds to prob(example == 1)
2.0.0,Remove padded elements
2.0.0,Get batches
2.0.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
2.0.0,multitask case with missing data...
2.0.0,Join information for all tasks.
2.0.0,TODO(rbharath): Find a way to get rid of this import?
2.0.0,Extract model info
2.0.0,Get graph topology for x
2.0.0,Building outputs
2.0.0,Set epsilon
2.0.0,Initialize
2.0.0,"Path to save checkpoint files, which matches the"
2.0.0,replicated supervisor's default path.
2.0.0,Create target inputs
2.0.0,Get train function
2.0.0,TODO(rbharath): I believe this is total amount of data
2.0.0,Get graph information
2.0.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.0.0,the number of labeled data points in target_i. This is to normalize each task
2.0.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.0.0,Get other optimizer information
2.0.0,TODO(rbharath): Figure out how to handle phase appropriately
2.0.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.0.0,"tensors of shape (batch_size,)"
2.0.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.0.0,examples (effect averages out)
2.0.0,Perform the optimization
2.0.0,TODO(rbharath): Disabling saving for now to try to debug.
2.0.0,run eval data through the model
2.0.0,"Shape (n_samples, n_tasks)"
2.0.0,Create target inputs
2.0.0,TODO(rbharath): Find a way to get rid of this import?
2.0.0,Obtain appropriate loss function
2.0.0,Extract model info
2.0.0,Get graph topology for x
2.0.0,Raw logit outputs
2.0.0,Set epsilon
2.0.0,Initialize
2.0.0,"Path to save checkpoint files, which matches the"
2.0.0,replicated supervisor's default path.
2.0.0,Create target inputs
2.0.0,############################################################### DEBUG
2.0.0,"print(""multitask classifier"")"
2.0.0,"print(""feat"")"
2.0.0,print(feat)
2.0.0,############################################################### DEBUG
2.0.0,Get train function
2.0.0,TODO(rbharath): I believe this is total amount of data
2.0.0,Get graph information
2.0.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
2.0.0,the number of labeled data points in target_i. This is to normalize each task
2.0.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
2.0.0,Get other optimizer information
2.0.0,TODO(rbharath): Figure out how to handle phase appropriately
2.0.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
2.0.0,"tensors of shape (batch_size,)"
2.0.0,Convert the labels into one-hot vector encodings.
2.0.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
2.0.0,un-softmaxed logits rather than softmax outputs.
2.0.0,It's ok to divide by just the batch_size rather than the number of nonzero
2.0.0,examples (effect averages out)
2.0.0,Perform the optimization
2.0.0,TODO(rbharath): Disabling saving for now to try to debug.
2.0.0,run eval data through the model
2.0.0,"Shape (n_samples, n_tasks)"
2.0.0,run eval data through the model
2.0.0,self.n_atoms = n_atoms
2.0.0,Define the list of tensors to be used as topology
2.0.0,Merge mol conv objects
2.0.0,Generate dicts
2.0.0,Define the list of tensors to be used as topology
2.0.0,Extract atom numbers
2.0.0,Generate dicts
2.0.0,molecule * atom(graph) => step => features
2.0.0,molecule * atom(graph) => step
2.0.0,molecule * atom(graph) => step
2.0.0,Define the list of tensors to be used as topology
2.0.0,calculation orders for a batch of molecules
2.0.0,padding atom features vector of each molecule with 0
2.0.0,self.n_atoms = n_atoms
2.0.0,Define the list of tensors to be used as topology
2.0.0,Extract atom numbers
2.0.0,Generate dicts
2.0.0,self.n_atoms = n_atoms
2.0.0,Define the list of tensors to be used as topology
2.0.0,Extract atom numbers
2.0.0,number of atoms in each molecule
2.0.0,index of pair features
2.0.0,number of pairs for each atom
2.0.0,atom features
2.0.0,pair features
2.0.0,Generate dicts
2.0.0,# Gather Projection
2.0.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
2.0.0,There should be 8 layers in graph_model
2.0.0,assert len(graph_model.layers) == 6
2.0.0,Add layers
2.0.0,Need to add batch-norm separately to test/support due to differing
2.0.0,shapes.
2.0.0,Apply an attention lstm layer
2.0.0,Gather Projection
2.0.0,Add layers
2.0.0,Need to add batch-norm separately to test/support due to differing
2.0.0,shapes.
2.0.0,Apply an attention lstm layer
2.0.0,Gather Projection
2.0.0,Degrees from 1 to max_deg inclusive
2.0.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
2.0.0,"Should have shape (?, deg)"
2.0.0,"Shape of atom_features should be (?, n_feat)"
2.0.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
2.0.0,-*- coding: utf-8 -*-
2.0.0,Save hyperparameters
2.0.0,-*- coding: utf-8 -*-
2.0.0,Save hyperparameters
2.0.0,setup optimizer
2.0.0,setup optimizer
2.0.0,"print(""tasK: %d"" %task)"
2.0.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
2.0.0,"print(""scores"")"
2.0.0,print(scores.size())
2.0.0,"print(""task_label"")"
2.0.0,print(task_label.size())
2.0.0,"task_loss =  self.criterion(scores, task_label)"
2.0.0,"print(""task_loss"")"
2.0.0,print(task_loss.size())
2.0.0,-*- coding: utf-8 -*-
2.0.0,Save hyperparameters
2.0.0,weight decay
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Turns out there are valid cases where we don't want pad-batches
2.0.0,on by default.
2.0.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.0.0,Run training op.
2.0.0,############################################################# TIMING
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,Special case to handle singletasks.
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,References
2.0.0,Arguments
2.0.0,Aliases.
2.0.0,Aliases.
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,TODO(rbharath): This class does not yet have a
2.0.0,"TensorGraph equivalent, but one may not be required."
2.0.0,"Commented out for now, remove if OK."
2.0.0,class AlternateWeaveLayer(WeaveLayer):
2.0.0,""""""" Alternate implementation of weave module"
2.0.0,"same variables, different graph structures"
2.0.0,""""""""
2.0.0,
2.0.0,"def call(self, x, mask=None):"
2.0.0,"""""""Execute this layer on input tensors."
2.0.0,
2.0.0,"x = [atom_features, pair_features, pair_split, atom_split, atom_to_pair]"
2.0.0,
2.0.0,Parameters
2.0.0,----------
2.0.0,x: list
2.0.0,list of Tensors of form described above.
2.0.0,"mask: bool, optional"
2.0.0,Ignored. Present only to shadow superclass call() method.
2.0.0,
2.0.0,Returns
2.0.0,-------
2.0.0,A: Tensor
2.0.0,Tensor of atom_features
2.0.0,P: Tensor
2.0.0,Tensor of pair_features
2.0.0,""""""""
2.0.0,# Add trainable weights
2.0.0,self.build()
2.0.0,
2.0.0,atom_features = x[0]
2.0.0,pair_features = x[1]
2.0.0,
2.0.0,pair_split = x[2]
2.0.0,atom_to_pair = x[4]
2.0.0,
2.0.0,"AA = tf.matmul(atom_features, self.W_AA) + self.b_AA"
2.0.0,AA = self.activation(AA)
2.0.0,"PA = tf.matmul(pair_features, self.W_PA) + self.b_PA"
2.0.0,PA = self.activation(PA)
2.0.0,"PA = tf.segment_sum(PA, pair_split)"
2.0.0,
2.0.0,"A = tf.matmul(tf.concat([AA, PA], 1), self.W_A) + self.b_A"
2.0.0,A = self.activation(A)
2.0.0,
2.0.0,if self.update_pair:
2.0.0,AP_ij = tf.matmul(
2.0.0,tf.reshape(
2.0.0,"tf.gather(atom_features, atom_to_pair),"
2.0.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.0.0,AP_ij = self.activation(AP_ij)
2.0.0,AP_ji = tf.matmul(
2.0.0,tf.reshape(
2.0.0,"tf.gather(atom_features, tf.reverse(atom_to_pair, [1])),"
2.0.0,"[-1, 2 * self.n_atom_input_feat]), self.W_AP) + self.b_AP"
2.0.0,AP_ji = self.activation(AP_ji)
2.0.0,
2.0.0,"PP = tf.matmul(pair_features, self.W_PP) + self.b_PP"
2.0.0,PP = self.activation(PP)
2.0.0,"P = tf.matmul(tf.concat([AP_ij + AP_ji, PP], 1), self.W_P) + self.b_P"
2.0.0,P = self.activation(P)
2.0.0,else:
2.0.0,P = pair_features
2.0.0,
2.0.0,"return A, P"
2.0.0,TODO(rbharath): This class does not yet have a
2.0.0,"TensorGraph equivalent, but one may not be required."
2.0.0,"Commented out for now, remove if OK."
2.0.0,class WeaveConcat(Layer):
2.0.0,""""""""" Concat a batch of molecules into a batch of atoms"
2.0.0,""""""""
2.0.0,
2.0.0,"def __init__(self,"
2.0.0,"batch_size,"
2.0.0,"n_atom_input_feat=50,"
2.0.0,"n_output=128,"
2.0.0,"init='glorot_uniform',"
2.0.0,"activation='tanh',"
2.0.0,**kwargs):
2.0.0,""""""""
2.0.0,Parameters
2.0.0,----------
2.0.0,batch_size: int
2.0.0,number of molecules in a batch
2.0.0,"n_atom_input_feat: int, optional"
2.0.0,Number of features for each atom in input.
2.0.0,"n_output: int, optional"
2.0.0,Number of output features for each atom(concatenated)
2.0.0,"init: str, optional"
2.0.0,Weight initialization for filters.
2.0.0,"activation: str, optional"
2.0.0,Activation function applied
2.0.0,
2.0.0,""""""""
2.0.0,self.batch_size = batch_size
2.0.0,self.n_atom_input_feat = n_atom_input_feat
2.0.0,self.n_output = n_output
2.0.0,self.init = initializations.get(init)  # Set weight initialization
2.0.0,self.activation = activations.get(activation)  # Get activations
2.0.0,"super(WeaveConcat, self).__init__(**kwargs)"
2.0.0,
2.0.0,def build(self):
2.0.0,"""""""""Construct internal trainable weights."
2.0.0,""""""""
2.0.0,
2.0.0,"self.W = self.init([self.n_atom_input_feat, self.n_output])"
2.0.0,self.b = model_ops.zeros(shape=[
2.0.0,"self.n_output,"
2.0.0,])
2.0.0,
2.0.0,self.trainable_weights = self.W + self.b
2.0.0,
2.0.0,"def call(self, x, mask=None):"
2.0.0,"""""""Execute this layer on input tensors."
2.0.0,
2.0.0,"x = [atom_features, atom_mask]"
2.0.0,
2.0.0,Parameters
2.0.0,----------
2.0.0,x: list
2.0.0,Tensors as listed above
2.0.0,"mask: bool, optional"
2.0.0,Ignored. Present only to shadow superclass call() method.
2.0.0,
2.0.0,Returns
2.0.0,-------
2.0.0,outputs: Tensor
2.0.0,Tensor of concatenated atom features
2.0.0,""""""""
2.0.0,self.build()
2.0.0,atom_features = x[0]
2.0.0,atom_masks = x[1]
2.0.0,"A = tf.split(atom_features, self.batch_size, axis=0)"
2.0.0,A_mask = tf.split(
2.0.0,"tf.cast(atom_masks, dtype=tf.bool), self.batch_size, axis=0)"
2.0.0,outputs = tf.concat(
2.0.0,"[tf.boolean_mask(A[i], A_mask[i]) for i in range(len(A))], axis=0)"
2.0.0,"outputs = tf.matmul(outputs, self.W) + self.b"
2.0.0,outputs = self.activation(outputs)
2.0.0,return outputs
2.0.0,TODO(rbharath): This class does not yet have a
2.0.0,"TensorGraph equivalent, but one may not be required."
2.0.0,"Commented out for now, remove if OK."
2.0.0,class AlternateWeaveGather(WeaveGather):
2.0.0,"""""""Alternate implementation of weave gather layer"
2.0.0,corresponding to AlternateWeaveLayer
2.0.0,""""""""
2.0.0,
2.0.0,"def call(self, x, mask=None):"
2.0.0,"""""""Execute this layer on input tensors."
2.0.0,
2.0.0,"x = [atom_features, atom_split]"
2.0.0,
2.0.0,Parameters
2.0.0,----------
2.0.0,x: list
2.0.0,Tensors as listed above
2.0.0,"mask: bool, optional"
2.0.0,Ignored. Present only to shadow superclass call() method.
2.0.0,
2.0.0,Returns
2.0.0,-------
2.0.0,outputs: Tensor
2.0.0,Tensor of molecular features
2.0.0,""""""""
2.0.0,# Add trainable weights
2.0.0,self.build()
2.0.0,outputs = x[0]
2.0.0,atom_split = x[1]
2.0.0,
2.0.0,if self.gaussian_expand:
2.0.0,outputs = self.gaussian_histogram(outputs)
2.0.0,
2.0.0,"output_molecules = tf.segment_sum(outputs, atom_split)"
2.0.0,
2.0.0,if self.gaussian_expand:
2.0.0,"output_molecules = tf.matmul(output_molecules, self.W) + self.b"
2.0.0,output_molecules = self.activation(output_molecules)
2.0.0,return output_molecules
2.0.0,Each directory holds a range of assay results
2.0.0,Just write NA
2.0.0,"Now, write out the results csv, going line by line through all molecule results"
2.0.0,printing the mol_id
2.0.0,printing the SMILES
2.0.0,Now gzip it
2.0.0,Now remove the intermediate csv
2.0.0,First download all SDF files. We need these to get smiles
2.0.0,Next download all Bioassays
2.0.0,RDKit consistently hangs when trying to read this file
2.0.0,TODO (LESWING) Lazy Load
2.0.0,TODO (LESWING) Lazy Load
2.0.0,from simdna import simulations
2.0.0,define layer out functions
2.0.0,get layer outputs for a positive simulation example
2.0.0,plot layer outputs
2.0.0,highlight motif sites
2.0.0,get a positive and a negative example from the simulation data
2.0.0,"get motif scores, ISM scores, and DeepLIFT scores"
2.0.0,get motif site locations
2.0.0,organize legends
2.0.0,plot scores and highlight motif site locations
2.0.0,initialize fwd and reverse scores to -infinity
2.0.0,"cross-correlate separately for each base,"
2.0.0,for both the PSSM and its reverse complement
2.0.0,sum over the bases
2.0.0,take max of fwd and reverse scores at each position
2.0.0,return 1D view of sequence characters
2.0.0,class SequenceDNN(Model):
2.0.0,""""""""
2.0.0,Sequence DNN models.
2.0.0,
2.0.0,Parameters
2.0.0,----------
2.0.0,"seq_length : int, optional"
2.0.0,length of input sequence.
2.0.0,"keras_model : instance of keras.models.Sequential, optional"
2.0.0,seq_length or keras_model must be specified.
2.0.0,"num_tasks : int, optional"
2.0.0,number of tasks. Default: 1.
2.0.0,num_filters : list[int] | tuple[int]
2.0.0,"number of convolutional filters in each layer. Default: (15,)."
2.0.0,conv_width : list[int] | tuple[int]
2.0.0,"width of each layer's convolutional filters. Default: (15,)."
2.0.0,pool_width : int
2.0.0,width of max pooling after the last layer. Default: 35.
2.0.0,L1 : float
2.0.0,strength of L1 penalty.
2.0.0,dropout : float
2.0.0,dropout probability in every convolutional layer. Default: 0.
2.0.0,verbose: int
2.0.0,"Verbosity level during training. Valida values: 0, 1, 2."
2.0.0,
2.0.0,Returns
2.0.0,-------
2.0.0,Compiled DNN model.
2.0.0,""""""""
2.0.0,
2.0.0,"def __init__(self,"
2.0.0,"seq_length=None,"
2.0.0,"keras_model=None,"
2.0.0,"use_RNN=False,"
2.0.0,"num_tasks=1,"
2.0.0,"num_filters=(15, 15, 15),"
2.0.0,"conv_width=(15, 15, 15),"
2.0.0,"pool_width=35,"
2.0.0,"GRU_size=35,"
2.0.0,"TDD_size=15,"
2.0.0,"L1=0,"
2.0.0,"dropout=0.0,"
2.0.0,"num_epochs=100,"
2.0.0,verbose=1):
2.0.0,self.num_tasks = num_tasks
2.0.0,self.num_epochs = num_epochs
2.0.0,self.verbose = verbose
2.0.0,self.train_metrics = []
2.0.0,self.valid_metrics = []
2.0.0,if keras_model is not None and seq_length is None:
2.0.0,self.model = keras_model
2.0.0,self.num_tasks = keras_model.layers[-1].output_shape[-1]
2.0.0,elif seq_length is not None and keras_model is None:
2.0.0,self.model = Sequential()
2.0.0,assert len(num_filters) == len(conv_width)
2.0.0,"for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width)):"
2.0.0,conv_height = 4 if i == 0 else 1
2.0.0,self.model.add(
2.0.0,Convolution2D(
2.0.0,"nb_filter=nb_filter,"
2.0.0,"nb_row=conv_height,"
2.0.0,"nb_col=nb_col,"
2.0.0,"activation='linear',"
2.0.0,"init='he_normal',"
2.0.0,"input_shape=(1, 4, seq_length),"
2.0.0,"W_regularizer=l1(L1),"
2.0.0,b_regularizer=l1(L1)))
2.0.0,self.model.add(Activation('relu'))
2.0.0,self.model.add(Dropout(dropout))
2.0.0,"self.model.add(MaxPooling2D(pool_size=(1, pool_width)))"
2.0.0,if use_RNN:
2.0.0,num_max_pool_outputs = self.model.layers[-1].output_shape[-1]
2.0.0,"self.model.add(Reshape((num_filters[-1], num_max_pool_outputs)))"
2.0.0,"self.model.add(Permute((2, 1)))"
2.0.0,"self.model.add(GRU(GRU_size, return_sequences=True))"
2.0.0,"self.model.add(TimeDistributedDense(TDD_size, activation='relu'))"
2.0.0,self.model.add(Flatten())
2.0.0,self.model.add(Dense(output_dim=self.num_tasks))
2.0.0,self.model.add(Activation('sigmoid'))
2.0.0,"self.model.compile(optimizer='adam', loss='binary_crossentropy')"
2.0.0,else:
2.0.0,raise ValueError(
2.0.0,"""Exactly one of seq_length or keras_model must be specified!"")"
2.0.0,
2.0.0,"def train(self,"
2.0.0,"X,"
2.0.0,"y,"
2.0.0,"validation_data,"
2.0.0,"early_stopping_metric='Loss',"
2.0.0,"early_stopping_patience=5,"
2.0.0,save_best_model_to_prefix=None):
2.0.0,if y.dtype != bool:
2.0.0,"assert set(np.unique(y)) == {0, 1}"
2.0.0,y = y.astype(bool)
2.0.0,multitask = y.shape[1] > 1
2.0.0,if not multitask:
2.0.0,num_positives = y.sum()
2.0.0,num_sequences = len(y)
2.0.0,num_negatives = num_sequences - num_positives
2.0.0,if self.verbose >= 1:
2.0.0,print('Training model (* indicates new best result)...')
2.0.0,"X_valid, y_valid = validation_data"
2.0.0,early_stopping_wait = 0
2.0.0,best_metric = np.inf if early_stopping_metric == 'Loss' else -np.inf
2.0.0,"for epoch in range(1, self.num_epochs + 1):"
2.0.0,self.model.fit(
2.0.0,"X,"
2.0.0,"y,"
2.0.0,"batch_size=128,"
2.0.0,"nb_epoch=1,"
2.0.0,class_weight={
2.0.0,"True: num_sequences / num_positives,"
2.0.0,False: num_sequences / num_negatives
2.0.0,"} if not multitask else None,"
2.0.0,verbose=self.verbose >= 2)
2.0.0,"epoch_train_metrics = self.test(X, y)"
2.0.0,"epoch_valid_metrics = self.test(X_valid, y_valid)"
2.0.0,self.train_metrics.append(epoch_train_metrics)
2.0.0,self.valid_metrics.append(epoch_valid_metrics)
2.0.0,if self.verbose >= 1:
2.0.0,print('Epoch {}:'.format(epoch))
2.0.0,print('Train {}'.format(epoch_train_metrics))
2.0.0,"print('Valid {}'.format(epoch_valid_metrics), end='')"
2.0.0,current_metric = epoch_valid_metrics[early_stopping_metric].mean()
2.0.0,if (early_stopping_metric == 'Loss') == (current_metric <= best_metric):
2.0.0,if self.verbose >= 1:
2.0.0,print(' *')
2.0.0,best_metric = current_metric
2.0.0,best_epoch = epoch
2.0.0,early_stopping_wait = 0
2.0.0,if save_best_model_to_prefix is not None:
2.0.0,self.save(save_best_model_to_prefix)
2.0.0,else:
2.0.0,if self.verbose >= 1:
2.0.0,print()
2.0.0,if early_stopping_wait >= early_stopping_patience:
2.0.0,break
2.0.0,early_stopping_wait += 1
2.0.0,if self.verbose >= 1:
2.0.0,print('Finished training after {} epochs.'.format(epoch))
2.0.0,if save_best_model_to_prefix is not None:
2.0.0,"print(""The best model's architecture and weights (from epoch {0}) """
2.0.0,'were saved to {1}.arch.json and {1}.weights.h5'.format(
2.0.0,"best_epoch, save_best_model_to_prefix))"
2.0.0,
2.0.0,"def predict(self, X):"
2.0.0,"return self.model.predict(X, batch_size=128, verbose=False)"
2.0.0,
2.0.0,def get_sequence_filters(self):
2.0.0,""""""""
2.0.0,Returns 3D array of 2D sequence filters.
2.0.0,""""""""
2.0.0,return self.model.layers[0].get_weights()[0].squeeze(axis=1)
2.0.0,
2.0.0,"def deeplift(self, X, batch_size=200):"
2.0.0,""""""""
2.0.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) deeplift score array."
2.0.0,""""""""
2.0.0,assert len(np.shape(X)) == 4 and np.shape(X)[1] == 1
2.0.0,from deeplift.conversion import keras_conversion as kc
2.0.0,
2.0.0,# convert to deeplift model and get scoring function
2.0.0,"deeplift_model = kc.convert_sequential_model(self.model, verbose=False)"
2.0.0,score_func = deeplift_model.get_target_contribs_func(
2.0.0,find_scores_layer_idx=0)
2.0.0,# use a 40% GC reference
2.0.0,"input_references = [np.array([0.3, 0.2, 0.2, 0.3])[None, None, :, None]]"
2.0.0,# get deeplift scores
2.0.0,"deeplift_scores = np.zeros((self.num_tasks,) + X.shape)"
2.0.0,for i in range(self.num_tasks):
2.0.0,deeplift_scores[i] = score_func(
2.0.0,"task_idx=i,"
2.0.0,"input_data_list=[X],"
2.0.0,"batch_size=batch_size,"
2.0.0,"progress_update=None,"
2.0.0,input_references_list=input_references)
2.0.0,return deeplift_scores
2.0.0,
2.0.0,"def in_silico_mutagenesis(self, X):"
2.0.0,""""""""
2.0.0,"Returns (num_task, num_samples, 1, num_bases, sequence_length) ISM score array."
2.0.0,""""""""
2.0.0,"mutagenesis_scores = np.empty(X.shape + (self.num_tasks,), dtype=np.float32)"
2.0.0,wild_type_predictions = self.predict(X)
2.0.0,"wild_type_predictions = wild_type_predictions[:, np.newaxis, np.newaxis,"
2.0.0,np.newaxis]
2.0.0,"for sequence_index, (sequence, wild_type_prediction) in enumerate("
2.0.0,"zip(X, wild_type_predictions)):"
2.0.0,mutated_sequences = np.repeat(
2.0.0,"sequence[np.newaxis], np.prod(sequence.shape), axis=0)"
2.0.0,# remove wild-type
2.0.0,arange = np.arange(len(mutated_sequences))
2.0.0,horizontal_cycle = np.tile(
2.0.0,"np.arange(sequence.shape[-1]), sequence.shape[-2])"
2.0.0,"mutated_sequences[arange, :, :, horizontal_cycle] = 0"
2.0.0,# add mutant
2.0.0,vertical_repeat = np.repeat(
2.0.0,"np.arange(sequence.shape[-2]), sequence.shape[-1])"
2.0.0,"mutated_sequences[arange, :, vertical_repeat, horizontal_cycle] = 1"
2.0.0,# make mutant predictions
2.0.0,mutated_predictions = self.predict(mutated_sequences)
2.0.0,mutated_predictions = mutated_predictions.reshape(sequence.shape +
2.0.0,"(self.num_tasks,))"
2.0.0,mutagenesis_scores[
2.0.0,sequence_index] = wild_type_prediction - mutated_predictions
2.0.0,"return np.rollaxis(mutagenesis_scores, -1)"
2.0.0,
2.0.0,@staticmethod
2.0.0,"def _plot_scores(X, output_directory, peak_width, score_func, score_name):"
2.0.0,from dragonn.plot import plot_bases_on_ax
2.0.0,scores = score_func(X).squeeze(
2.0.0,"axis=2)  # (num_task, num_samples, num_bases, sequence_length)"
2.0.0,try:
2.0.0,os.makedirs(output_directory)
2.0.0,except OSError:
2.0.0,pass
2.0.0,num_tasks = len(scores)
2.0.0,"for task_index, task_scores in enumerate(scores):"
2.0.0,"for sequence_index, sequence_scores in enumerate(task_scores):"
2.0.0,# sequence_scores is num_bases x sequence_length
2.0.0,basewise_max_sequence_scores = sequence_scores.max(axis=0)
2.0.0,plt.clf()
2.0.0,"figure, (top_axis, bottom_axis) = plt.subplots(2)"
2.0.0,top_axis.plot(
2.0.0,"range(1,"
2.0.0,"len(basewise_max_sequence_scores) + 1),"
2.0.0,basewise_max_sequence_scores)
2.0.0,top_axis.set_title('{} scores (motif highlighted)'.format(score_name))
2.0.0,peak_position = basewise_max_sequence_scores.argmax()
2.0.0,top_axis.axvspan(
2.0.0,"peak_position - peak_width,"
2.0.0,"peak_position + peak_width,"
2.0.0,"color='grey',"
2.0.0,alpha=0.1)
2.0.0,"peak_sequence_scores = sequence_scores[:, peak_position - peak_width:"
2.0.0,peak_position + peak_width].T
2.0.0,# Set non-max letter_heights to zero
2.0.0,letter_heights = np.zeros_like(peak_sequence_scores)
2.0.0,"letter_heights[np.arange(len(letter_heights)),"
2.0.0,peak_sequence_scores.argmax(axis=1)] = \
2.0.0,basewise_max_sequence_scores[peak_position - peak_width :
2.0.0,peak_position + peak_width]
2.0.0,"plot_bases_on_ax(letter_heights, bottom_axis)"
2.0.0,bottom_axis.set_xticklabels(
2.0.0,tuple(
2.0.0,"map(str,"
2.0.0,"np.arange(peak_position - peak_width,"
2.0.0,peak_position + peak_width + 1))))
2.0.0,"bottom_axis.tick_params(axis='x', labelsize='small')"
2.0.0,plt.xlabel('Position')
2.0.0,plt.ylabel('Score')
2.0.0,plt.savefig(
2.0.0,"os.path.join(output_directory, 'sequence_{}{}'.format("
2.0.0,"sequence_index, '_task_{}'.format(task_index)"
2.0.0,if num_tasks > 1 else '')))
2.0.0,plt.close()
2.0.0,
2.0.0,"def plot_deeplift(self, X, output_directory, peak_width=10):"
2.0.0,self._plot_scores(
2.0.0,"X,"
2.0.0,"output_directory,"
2.0.0,"peak_width,"
2.0.0,"score_func=self.deeplift,"
2.0.0,score_name='DeepLift')
2.0.0,
2.0.0,"def plot_in_silico_mutagenesis(self, X, output_directory, peak_width=10):"
2.0.0,self._plot_scores(
2.0.0,"X,"
2.0.0,"output_directory,"
2.0.0,"peak_width,"
2.0.0,"score_func=self.in_silico_mutagenesis,"
2.0.0,score_name='ISM')
2.0.0,
2.0.0,"def plot_architecture(self, output_file):"
2.0.0,from dragonn.visualize_util import plot as plot_keras_model
2.0.0,"plot_keras_model(self.model, output_file, show_shape=True)"
2.0.0,
2.0.0,"def save(self, save_best_model_to_prefix):"
2.0.0,arch_fname = save_best_model_to_prefix + '.arch.json'
2.0.0,weights_fname = save_best_model_to_prefix + '.weights.h5'
2.0.0,"open(arch_fname, 'w').write(self.model.to_json())"
2.0.0,"self.model.save_weights(weights_fname, overwrite=True)"
2.0.0,
2.0.0,@staticmethod
2.0.0,"def load(arch_fname, weights_fname=None):"
2.0.0,model_json_string = open(arch_fname).read()
2.0.0,sequence_dnn = SequenceDNN(keras_model=model_from_json(model_json_string))
2.0.0,if weights_fname is not None:
2.0.0,sequence_dnn.model.load_weights(weights_fname)
2.0.0,return sequence_dnn
2.0.0,create temporary fasta files
2.0.0,run command
2.0.0,remove fasta files
2.0.0,write test fasta file
2.0.0,test gkmsvm
2.0.0,get classification results
2.0.0,This SDF file fails to parse with RDKit on Ubuntu 16.04
2.0.0,"Using canonical smiles for glycine, as in original research paper"
2.0.0,2017 DeepCrystal Technologies - Patrick Hop
2.0.0,
2.0.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
2.0.0,
2.0.0,MIT License - have fun!!
2.0.0,===========================================================
2.0.0,x = F.selu( fc(x) )
2.0.0,x = F.selu( fc(x) )
2.0.0,2017 DeepCrystal Technologies - Patrick Hop
2.0.0,
2.0.0,Data loading a splitting file
2.0.0,
2.0.0,MIT License - have fun!!
2.0.0,===========================================================
2.0.0,Consistency check
2.0.0,Handle output layer
2.0.0,Iterate over all previous tasks.
2.0.0,prev_layers is a list with elements of size
2.0.0,"(batch_size, layer_sizes[i-1])"
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Save an initial checkpoint.
2.0.0,Turns out there are valid cases where we don't want pad-batches
2.0.0,on by default.
2.0.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.0.0,Run training op.
2.0.0,Always save a final checkpoint when complete.
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Note that we divide by the batch size and not the number of
2.0.0,"non-zero weight examples in the batch.  Also, instead of using"
2.0.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.0.0,calculate with div/sum so it stays on the GPU.
2.0.0,aggregated costs
2.0.0,weight decay
2.0.0,Dummy placeholders
2.0.0,Dummy placeholders
2.0.0,run eval data through the model
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,Handle edge case when batch-size is 1.
2.0.0,Prune away any padding that was added
2.0.0,allow_soft_placement=True allows ops without a GPU implementation
2.0.0,to run on the CPU instead.
2.0.0,!/usr/bin/python
2.0.0,
2.0.0,Copyright 2015 Google Inc.
2.0.0,
2.0.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.0.0,you may not use this file except in compliance with the License.
2.0.0,You may obtain a copy of the License at
2.0.0,
2.0.0,http://www.apache.org/licenses/LICENSE-2.0
2.0.0,
2.0.0,"Unless required by applicable law or agreed to in writing, software"
2.0.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.0.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.0.0,See the License for the specific language governing permissions and
2.0.0,limitations under the License.
2.0.0,parse CheckpointState proto
2.0.0,parse path to actual checkpoint
2.0.0,the provided mask has to be the same shape as features
2.0.0,test k = 1..4
2.0.0,central moments
2.0.0,standardized moments
2.0.0,central across one axis
2.0.0,standardized across one axis
2.0.0,Fit just on task zero
2.0.0,Notice that we keep the session open
2.0.0,Fit on task one
2.0.0,The predictions for task zero should not change after training
2.0.0,on task one.
2.0.0,!/usr/bin/python
2.0.0,
2.0.0,Copyright 2015 Google Inc.
2.0.0,
2.0.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.0.0,you may not use this file except in compliance with the License.
2.0.0,You may obtain a copy of the License at
2.0.0,
2.0.0,http://www.apache.org/licenses/LICENSE-2.0
2.0.0,
2.0.0,"Unless required by applicable law or agreed to in writing, software"
2.0.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.0.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.0.0,See the License for the specific language governing permissions and
2.0.0,limitations under the License.
2.0.0,get the divisor
2.0.0,compute the requested central moment
2.0.0,"note that mean is a raw moment, not a central moment"
2.0.0,TODO(user): median is not implemented yet in TensorFlow
2.0.0,Add the input features.
2.0.0,"layer has shape [None, layer_sizes[i]]"
2.0.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.0.0,TODO(rbharath): Might want to make it feasible to have multiple
2.0.0,bypass layers.
2.0.0,Construct task bypass layer
2.0.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.0.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.0.0,"layer has shape [None, layer_sizes[i]]"
2.0.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
2.0.0,TODO(rbharath): Might want to make it feasible to have multiple
2.0.0,bypass layers.
2.0.0,Construct task bypass layer
2.0.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
2.0.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
2.0.0,Consistency check
2.0.0,Lazily created by _get_shared_session().
2.0.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.0.0,when subclass-overridden methods use the same scopes.
2.0.0,Setup graph
2.0.0,Create placeholders
2.0.0,Handle output layer
2.0.0,Iterate over all previous tasks.
2.0.0,prev_layers is a list with elements of size
2.0.0,"(batch_size, layer_sizes[i-1])"
2.0.0,Note that we divide by the batch size and not the number of
2.0.0,"non-zero weight examples in the batch.  Also, instead of using"
2.0.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.0.0,calculate with div/sum so it stays on the GPU.
2.0.0,aggregated costs
2.0.0,weight decay
2.0.0,Dummy placeholders
2.0.0,Dummy placeholders
2.0.0,run eval data through the model
2.0.0,"Shape (n_tasks, n__samples)"
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,Handle edge case when batch-size is 1.
2.0.0,with self._get_shared_session(train=True) as sess:
2.0.0,Save an initial checkpoint.
2.0.0,Always save a final checkpoint when complete.
2.0.0,Note that we divide by the batch size and not the number of
2.0.0,"non-zero weight examples in the batch.  Also, instead of using"
2.0.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.0.0,calculate with div/sum so it stays on the GPU.
2.0.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
2.0.0,Dummy placeholders
2.0.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
2.0.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
2.0.0,Dummy placeholders
2.0.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
2.0.0,allow_soft_placement=True allows ops without a GPU implementation
2.0.0,to run on the CPU instead.
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Turns out there are valid cases where we don't want pad-batches
2.0.0,on by default.
2.0.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.0.0,if epoch%checkpoint_interval == checkpoint_interval-1:
2.0.0,"saver.save(sess, self._save_path, global_step=epoch)"
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,"(n_samples, n_classes)"
2.0.0,"(n_samples, n_tasks, n_classes)"
2.0.0,Save hyperparameters
2.0.0,Guard variable to make sure we don't Restore() this model
2.0.0,from a disk checkpoint more than once.
2.0.0,"Path to save checkpoint files, which matches the"
2.0.0,replicated supervisor's default path.
2.0.0,Lazily created by _get_shared_session().
2.0.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.0.0,when subclass-overridden methods use the same scopes.
2.0.0,Setup graph
2.0.0,Note that we divide by the batch size and not the number of
2.0.0,"non-zero weight examples in the batch.  Also, instead of using"
2.0.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.0.0,calculate with div/sum so it stays on the GPU.
2.0.0,aggregated costs
2.0.0,weight decay
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Save an initial checkpoint.
2.0.0,Define the code that runs on a separate thread to feed data into the queue.
2.0.0,Main training loop.
2.0.0,Run training op.
2.0.0,We have reached the end of an epoch.
2.0.0,We have reached the end of the data.
2.0.0,Always save a final checkpoint when complete.
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,allow_soft_placement=True allows ops without a GPU implementation
2.0.0,to run on the CPU instead.
2.0.0,gpu memory growth option
2.0.0,gpu memory growth option
2.0.0,TODO(rbharath): Is setting train=False right here?
2.0.0,Discard any padded predictions
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,Special case to handle singletasks.
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,TODO(rbharath): Verify this can be safely removed.
2.0.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.0.0,""""""""
2.0.0,Evaluates the performance of this model on specified dataset.
2.0.0,
2.0.0,Parameters
2.0.0,----------
2.0.0,dataset: dc.data.Dataset
2.0.0,Dataset object.
2.0.0,metric: deepchem.metrics.Metric
2.0.0,Evaluation metric
2.0.0,transformers: list
2.0.0,List of deepchem.transformers.Transformer
2.0.0,Returns
2.0.0,-------
2.0.0,dict
2.0.0,Maps tasks to scores under metric.
2.0.0,""""""""
2.0.0,"evaluator = Evaluator(self, dataset, transformers)"
2.0.0,scores = evaluator.compute_model_performance(metrics)
2.0.0,return scores
2.0.0,checkpoints look like model_dir/model.ckpt-N
2.0.0,"self._save_path is ""model_dir/model.ckpt"""
2.0.0,run eval data through the model
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,run eval data through the model
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,Note that softmax is already applied in construct_grpah
2.0.0,run eval data through the model
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,Handle edge case when batch-size is 1.
2.0.0,Prune away any padding that was added
2.0.0,Handle case of 0-dimensional scalar output
2.0.0,Set random seeds
2.0.0,Setup directories
2.0.0,Model constants
2.0.0,Load and transform datasets
2.0.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.0.0,Atomic convolution variables
2.0.0,at = atomic numbers (atom types)
2.0.0,"radial basis function parameters [cutoff, mean, width]"
2.0.0,Model hyperparameters
2.0.0,Initialize model
2.0.0,Fit model
2.0.0,Evaluate model
2.0.0,Set random seeds
2.0.0,Setup directories
2.0.0,Model constants
2.0.0,Load and transform datasets
2.0.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.0.0,Atomic convolution variables
2.0.0,at = atomic numbers (atom types)
2.0.0,"radial basis function parameters [cutoff, mean, width]"
2.0.0,Model hyperparameters
2.0.0,Initialize model
2.0.0,Fit model
2.0.0,Evaluate model
2.0.0,Set random seeds
2.0.0,Setup directories
2.0.0,Model constants
2.0.0,Load and transform datasets
2.0.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.0.0,Atomic convolution variables
2.0.0,at = atomic numbers (atom types)
2.0.0,"radial basis function parameters [cutoff, mean, width]"
2.0.0,Model hyperparameters
2.0.0,Initialize model
2.0.0,Fit model
2.0.0,Evaluate model
2.0.0,Set random seeds
2.0.0,Setup directories
2.0.0,Model constants
2.0.0,Load and transform datasets
2.0.0,convert -logKi to dG = +RTlogKi [kJ/mol]
2.0.0,Atomic convolution variables
2.0.0,at = atomic numbers (atom types)
2.0.0,"radial basis function parameters [cutoff, mean, width]"
2.0.0,Model hyperparameters
2.0.0,Initialize model
2.0.0,Fit model
2.0.0,Evaluate model
2.0.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
2.0.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
2.0.0,test_scores = test_evaluator.compute_model_performance(metric)
2.0.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
2.0.0,param.update(test_scores)
2.0.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.0.0,for transformer in transformers:
2.0.0,train_dataset = transformer.transform(train_dataset)
2.0.0,test_dataset = transformer.transform(test_dataset)
2.0.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.0.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.0.0,for transformer in transformers:
2.0.0,train_dataset = transformer.transform(train_dataset)
2.0.0,test_dataset = transformer.transform(test_dataset)
2.0.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.0.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.0.0,for transformer in transformers:
2.0.0,train_dataset = transformer.transform(train_dataset)
2.0.0,test_dataset = transformer.transform(test_dataset)
2.0.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.0.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
2.0.0,for transformer in transformers:
2.0.0,train_dataset = transformer.transform(train_dataset)
2.0.0,test_dataset = transformer.transform(test_dataset)
2.0.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
2.0.0,Create some directories for analysis
2.0.0,The base_dir holds the results of all analysis
2.0.0,Make directories to store the raw and featurized datasets.
2.0.0,Load PDBBind dataset
2.0.0,Define featurizers
2.0.0,Currently featurizes with shard_size=1
2.0.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
2.0.0,This could be done with openbabel in python
2.0.0,Compute cells for this molecule. O(constant)
2.0.0,min == max if molecule is planar in some direction
2.0.0,we should still create a bin
2.0.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
2.0.0,Note neighbors contains self!
2.0.0,Associate each atom with cell it belongs to. O(N)
2.0.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.0.0,"conditions, so does wrapround. O(constant)"
2.0.0,"For each atom, loop through all atoms in its cell and neighboring cells."
2.0.0,Accept as neighbors only those within threshold. This computation should be
2.0.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
2.0.0,Sort neighbors by distance
2.0.0,Pick up to max_num_neighbors
2.0.0,Type of data created by this featurizer
2.0.0,assumes that every array is of the same dimension
2.0.0,rem_dataset is remaining portion of dataset
2.0.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.0.0,to k-1.
2.0.0,returns list of per column sum of non zero elements
2.0.0,Compute number of actives needed per task.
2.0.0,loop through each column and obtain index required to splice out for
2.0.0,required fraction of hits
2.0.0,Find the first index where the cumulative number of actives equals
2.0.0,the actives_count
2.0.0,Note that np.where tells us last index required to exceed
2.0.0,"actives_count, so we actually want the following location"
2.0.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.0.0,potentially refactor those to match this.
2.0.0,Handle edge case where frac_split is 1
2.0.0,Create weight matrices fpor two haves.
2.0.0,copy over up to required index for weight first_split
2.0.0,check out if any rows in either w_1 or w_2 are just zeros
2.0.0,"Obtain original x, y, and w arrays and shuffle"
2.0.0,calculate percent split for valid (out of test and valid)
2.0.0,"split test data into valid and test, treating sub test set also as sparse"
2.0.0,rem_dataset is remaining portion of dataset
2.0.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.0.0,to k-1.
2.0.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.0.0,This can be generalized in the future with some common demoninator determination.
2.0.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.0.0,Append remaining examples to train
2.0.0,Sort by increasing MW
2.0.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.0.0,for m_idx in cluster:
2.0.0,"continue until we find an active in all the tasks, otherwise we can't"
2.0.0,compute a meaningful AUC
2.0.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.0.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.0.0,Sort from largest to smallest scaffold sets
2.0.0,Sort from largest to smallest scaffold sets
2.0.0,"(n_samples, n_classes)"
2.0.0,"(n_samples, n_tasks, n_classes)"
2.0.0,Save hyperparameters
2.0.0,Guard variable to make sure we don't Restore() this model
2.0.0,from a disk checkpoint more than once.
2.0.0,"Path to save checkpoint files, which matches the"
2.0.0,replicated supervisor's default path.
2.0.0,Lazily created by _get_shared_session().
2.0.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
2.0.0,when subclass-overridden methods use the same scopes.
2.0.0,Setup graph
2.0.0,Note that we divide by the batch size and not the number of
2.0.0,"non-zero weight examples in the batch.  Also, instead of using"
2.0.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
2.0.0,calculate with div/sum so it stays on the GPU.
2.0.0,aggregated costs
2.0.0,weight decay
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Save an initial checkpoint.
2.0.0,Turns out there are valid cases where we don't want pad-batches
2.0.0,on by default.
2.0.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
2.0.0,Run training op.
2.0.0,Always save a final checkpoint when complete.
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,allow_soft_placement=True allows ops without a GPU implementation
2.0.0,to run on the CPU instead.
2.0.0,TODO(rbharath): Is setting train=False right here?
2.0.0,Discard any padded predictions
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,Special case to handle singletasks.
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,TODO(rbharath): Verify this can be safely removed.
2.0.0,"def evaluate(self, dataset, metrics, transformers=[]):"
2.0.0,""""""""
2.0.0,Evaluates the performance of this model on specified dataset.
2.0.0,
2.0.0,Parameters
2.0.0,----------
2.0.0,dataset: dc.data.Dataset
2.0.0,Dataset object.
2.0.0,metric: deepchem.metrics.Metric
2.0.0,Evaluation metric
2.0.0,transformers: list
2.0.0,List of deepchem.transformers.Transformer
2.0.0,Returns
2.0.0,-------
2.0.0,dict
2.0.0,Maps tasks to scores under metric.
2.0.0,""""""""
2.0.0,"evaluator = Evaluator(self, dataset, transformers)"
2.0.0,scores = evaluator.compute_model_performance(metrics)
2.0.0,return scores
2.0.0,checkpoints look like logdir/model.ckpt-N
2.0.0,"self._save_path is ""logdir/model.ckpt"""
2.0.0,run eval data through the model
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,run eval data through the model
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,Note that softmax is already applied in construct_grpah
2.0.0,run eval data through the model
2.0.0,reshape to batch_size x n_tasks x ...
2.0.0,Handle edge case when batch-size is 1.
2.0.0,Prune away any padding that was added
2.0.0,Handle case of 0-dimensional scalar output
2.0.0,Dummy placeholders
2.0.0,Dummy placeholders
2.0.0,## AtomicNet fully-connected layer ops ###
2.0.0,## Atomicnet coordinate transform ops ###
2.0.0,## Atomicnet symmetry function kernel ops ###
2.0.0,## Atomicnet symmetry function ops ###
2.0.0,## Atomcnet symmetry function layer ops ###
2.0.0,We apply the radial pooling filter before atom type conv
2.0.0,to reduce computation
2.0.0,## Misc convenience ops ###
2.0.0,"Copied from the yt_project, commit e8fb57e"
2.0.0,yt/doc/extensions/notebook_sphinxext.py
2.0.0,https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
2.0.0,Almost completely re-written by Matthew Harrigan to use nbconvert v4
2.0.0,1. Uneval notebook
2.0.0,2. Python
2.0.0,3. HTML (execute first)
2.0.0,Set per-cell timeout to 60 seconds
2.0.0,4. Eval'd notebook
2.0.0,Create link to notebook and script files
2.0.0,create notebook node
2.0.0,add dependency
2.0.0,-*- coding: utf-8 -*-
2.0.0,
2.0.0,"deepchem documentation build configuration file, created by"
2.0.0,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
2.0.0,
2.0.0,This file is execfile()d with the current directory set to its
2.0.0,containing dir.
2.0.0,
2.0.0,Note that not all possible configuration values are present in this
2.0.0,autogenerated file.
2.0.0,
2.0.0,All configuration values have a default; values that are commented out
2.0.0,serve to show the default.
2.0.0,"If extensions (or modules to document with autodoc) are in another directory,"
2.0.0,add these directories to sys.path here. If the directory is relative to the
2.0.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
2.0.0,"sys.path.insert(0, os.path.abspath('.'))"
2.0.0,-- General configuration ------------------------------------------------
2.0.0,"If your documentation needs a minimal Sphinx version, state it here."
2.0.0,needs_sphinx = '1.0'
2.0.0,"Add any Sphinx extension module names here, as strings. They can be"
2.0.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
2.0.0,ones.
2.0.0,"Add any paths that contain templates here, relative to this directory."
2.0.0,The suffix(es) of source filenames.
2.0.0,You can specify multiple suffix as a list of string:
2.0.0,"source_suffix = ['.rst', '.md']"
2.0.0,The encoding of source files.
2.0.0,source_encoding = 'utf-8-sig'
2.0.0,The master toctree document.
2.0.0,General information about the project.
2.0.0,"The version info for the project you're documenting, acts as replacement for"
2.0.0,"|version| and |release|, also used in various other places throughout the"
2.0.0,built documents.
2.0.0,
2.0.0,The short X.Y version.
2.0.0,"The full version, including alpha/beta/rc tags."
2.0.0,The language for content autogenerated by Sphinx. Refer to documentation
2.0.0,for a list of supported languages.
2.0.0,
2.0.0,This is also used if you do content translation via gettext catalogs.
2.0.0,"Usually you set ""language"" from the command line for these cases."
2.0.0,"There are two options for replacing |today|: either, you set today to some"
2.0.0,"non-false value, then it is used:"
2.0.0,today = ''
2.0.0,"Else, today_fmt is used as the format for a strftime call."
2.0.0,"today_fmt = '%B %d, %Y'"
2.0.0,"List of patterns, relative to source directory, that match files and"
2.0.0,directories to ignore when looking for source files.
2.0.0,The reST default role (used for this markup: `text`) to use for all
2.0.0,documents.
2.0.0,default_role = None
2.0.0,"If true, '()' will be appended to :func: etc. cross-reference text."
2.0.0,add_function_parentheses = True
2.0.0,"If true, the current module name will be prepended to all description"
2.0.0,unit titles (such as .. function::).
2.0.0,add_module_names = True
2.0.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
2.0.0,output. They are ignored by default.
2.0.0,show_authors = False
2.0.0,The name of the Pygments (syntax highlighting) style to use.
2.0.0,A list of ignored prefixes for module index sorting.
2.0.0,modindex_common_prefix = []
2.0.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
2.0.0,keep_warnings = False
2.0.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
2.0.0,-- Options for HTML output ----------------------------------------------
2.0.0,The theme to use for HTML and HTML Help pages.  See the documentation for
2.0.0,a list of builtin themes.
2.0.0,Theme options are theme-specific and customize the look and feel of a theme
2.0.0,"further.  For a list of options available for each theme, see the"
2.0.0,documentation.
2.0.0,html_theme_options = {}
2.0.0,"Add any paths that contain custom themes here, relative to this directory."
2.0.0,"The name for this set of Sphinx documents.  If None, it defaults to"
2.0.0,"""<project> v<release> documentation""."
2.0.0,html_title = None
2.0.0,A shorter title for the navigation bar.  Default is the same as html_title.
2.0.0,html_short_title = None
2.0.0,The name of an image file (relative to this directory) to place at the top
2.0.0,of the sidebar.
2.0.0,The name of an image file (within the static path) to use as favicon of the
2.0.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
2.0.0,pixels large.
2.0.0,html_favicon = None
2.0.0,"Add any paths that contain custom static files (such as style sheets) here,"
2.0.0,"relative to this directory. They are copied after the builtin static files,"
2.0.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
2.0.0,Add any extra paths that contain custom files (such as robots.txt or
2.0.0,".htaccess) here, relative to this directory. These files are copied"
2.0.0,directly to the root of the documentation.
2.0.0,html_extra_path = []
2.0.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
2.0.0,using the given strftime format.
2.0.0,"html_last_updated_fmt = '%b %d, %Y'"
2.0.0,"If true, SmartyPants will be used to convert quotes and dashes to"
2.0.0,typographically correct entities.
2.0.0,html_use_smartypants = True
2.0.0,"Custom sidebar templates, maps document names to template names."
2.0.0,html_sidebars = {}
2.0.0,"Additional templates that should be rendered to pages, maps page names to"
2.0.0,template names.
2.0.0,html_additional_pages = {}
2.0.0,"If false, no module index is generated."
2.0.0,html_domain_indices = True
2.0.0,"If false, no index is generated."
2.0.0,html_use_index = True
2.0.0,"If true, the index is split into individual pages for each letter."
2.0.0,html_split_index = False
2.0.0,"If true, links to the reST sources are added to the pages."
2.0.0,html_show_sourcelink = True
2.0.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
2.0.0,html_show_sphinx = True
2.0.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
2.0.0,html_show_copyright = True
2.0.0,"If true, an OpenSearch description file will be output, and all pages will"
2.0.0,contain a <link> tag referring to it.  The value of this option must be the
2.0.0,base URL from which the finished HTML is served.
2.0.0,html_use_opensearch = ''
2.0.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
2.0.0,html_file_suffix = None
2.0.0,Language to be used for generating the HTML full-text search index.
2.0.0,Sphinx supports the following languages:
2.0.0,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
2.0.0,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
2.0.0,html_search_language = 'en'
2.0.0,"A dictionary with options for the search language support, empty by default."
2.0.0,Now only 'ja' uses this config value
2.0.0,html_search_options = {'type': 'default'}
2.0.0,The name of a javascript file (relative to the configuration directory) that
2.0.0,"implements a search results scorer. If empty, the default will be used."
2.0.0,html_search_scorer = 'scorer.js'
2.0.0,Output file base name for HTML help builder.
2.0.0,-- Options for LaTeX output ---------------------------------------------
2.0.0,The paper size ('letterpaper' or 'a4paper').
2.0.0,"'papersize': 'letterpaper',"
2.0.0,"The font size ('10pt', '11pt' or '12pt')."
2.0.0,"'pointsize': '10pt',"
2.0.0,Additional stuff for the LaTeX preamble.
2.0.0,"'preamble': '',"
2.0.0,Latex figure (float) alignment
2.0.0,"'figure_align': 'htbp',"
2.0.0,Grouping the document tree into LaTeX files. List of tuples
2.0.0,"(source start file, target name, title,"
2.0.0,"author, documentclass [howto, manual, or own class])."
2.0.0,The name of an image file (relative to this directory) to place at the top of
2.0.0,the title page.
2.0.0,latex_logo = None
2.0.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
2.0.0,not chapters.
2.0.0,latex_use_parts = False
2.0.0,"If true, show page references after internal links."
2.0.0,latex_show_pagerefs = False
2.0.0,"If true, show URL addresses after external links."
2.0.0,latex_show_urls = False
2.0.0,Documents to append as an appendix to all manuals.
2.0.0,latex_appendices = []
2.0.0,"If false, no module index is generated."
2.0.0,latex_domain_indices = True
2.0.0,-- Options for manual page output ---------------------------------------
2.0.0,One entry per manual page. List of tuples
2.0.0,"(source start file, name, description, authors, manual section)."
2.0.0,"If true, show URL addresses after external links."
2.0.0,man_show_urls = False
2.0.0,-- Options for Texinfo output -------------------------------------------
2.0.0,Grouping the document tree into Texinfo files. List of tuples
2.0.0,"(source start file, target name, title, author,"
2.0.0,"dir menu entry, description, category)"
2.0.0,Documents to append as an appendix to all manuals.
2.0.0,texinfo_appendices = []
2.0.0,"If false, no module index is generated."
2.0.0,texinfo_domain_indices = True
2.0.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
2.0.0,texinfo_show_urls = 'footnote'
2.0.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
2.0.0,texinfo_no_detailmenu = False
2.0.0,Example configuration for intersphinx: refer to the Python standard library.
2.0.0,Higher is Better
2.0.0,The secret key is available as a secure environment variable
2.0.0,on travis-ci to push the build documentation to Amazon S3.
2.0.0,Perform recursive modification to set css mime types.
2.0.0,Perform recursive modification to set js mime types.
2.0.0,plt.show()
2.0.0,"run_benchmark(FILE, DEEPCHEM_DIR)"
2.0.0,lines in the label file have format
2.0.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
2.0.0,"print line[0], line[3]"
2.0.0,Record inputs.
2.0.0,Create the output directory if necessary.
2.0.0,Create duplicate placeholders for meta-optimization.
2.0.0,Create the loss function for meta-optimization.
2.0.0,"In the final loss, use different placeholders for all inputs so the loss will be"
2.0.0,computed from a different batch.
2.0.0,Create variables for accumulating the gradients.
2.0.0,Create the optimizers for meta-optimization and task optimization.
2.0.0,Main optimization loop.
2.0.0,Do checkpointing.
2.0.0,This is a MetaLearner that learns to generate sine functions with variable
2.0.0,amplitude and phase.
2.0.0,Optimize it.
2.0.0,Test it out on some new tasks and see how it works.
2.0.0,Initially the model should do a bad job of fitting the sine function.
2.0.0,After one step of optimization it should do much better.
2.0.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
2.0.0,get the same result.
2.0.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.0.0,Fit model on dataset
2.0.0,Fit model on dataset
2.0.0,"Should be an array of size (n_pocket_atoms, 3)"
2.0.0,"coords[triangle, 0] gives the x-dimension of all triangle points"
2.0.0,Take transpose to make sure rows correspond to atoms.
2.0.0,We voxelize so all grids have integral coordinates (convenience)
2.0.0,"If overlap of box with previously generated output boxes, return"
2.0.0,Carry forward mappings
2.0.0,We know that box has at least one atom not in outputs
2.0.0,Current box has been merged into box further down list.
2.0.0,No need to output current box
2.0.0,"protein_coords is (N, 3) tensor"
2.0.0,Load binding pocket model
2.0.0,TODO(rbharath): Shift refined to full once trained.
2.0.0,Fit model on dataset
2.0.0,Create featurizers
2.0.0,"if not ligand_file.endswith("".sdf""):"
2.0.0,"raise ValueError(""Only .sdf ligand files can be featurized."")"
2.0.0,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
2.0.0,ligand_mol2 = os.path.join(
2.0.0,"self.base_dir, ligand_basename + "".mol2"")"
2.0.0,
2.0.0,# Write mol2 file for ligand
2.0.0,obConversion = ob.OBConversion()
2.0.0,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
2.0.0,ob_mol = ob.OBMol()
2.0.0,"obConversion.ReadFile(ob_mol, str(ligand_file))"
2.0.0,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
2.0.0,
2.0.0,# Featurize ligand
2.0.0,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
2.0.0,if mol is None:
2.0.0,"return None, None"
2.0.0,# Default for CircularFingerprint
2.0.0,n_ligand_features = 1024
2.0.0,ligand_features = self.ligand_featurizer.featurize([mol])
2.0.0,
2.0.0,# Featurize pocket
2.0.0,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
2.0.0,"protein_file, ligand_file)"
2.0.0,n_pockets = len(pockets)
2.0.0,n_pocket_features = BindingPocketFeaturizer.n_features
2.0.0,
2.0.0,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
2.0.0,pocket_features = self.pocket_featurizer.featurize(
2.0.0,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
2.0.0,# Note broadcast operation
2.0.0,"features[:, :n_pocket_features] = pocket_features"
2.0.0,"features[:, n_pocket_features:] = ligand_features"
2.0.0,dataset = NumpyDataset(X=features)
2.0.0,pocket_preds = self.model.predict(dataset)
2.0.0,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
2.0.0,
2.0.0,# Find pockets which are active
2.0.0,active_pockets = []
2.0.0,active_pocket_atoms_map = {}
2.0.0,active_pocket_coords = []
2.0.0,for pocket_ind in range(len(pockets)):
2.0.0,#################################################### DEBUG
2.0.0,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
2.0.0,#if pocket_preds[pocket_ind] == 1:
2.0.0,if pocket_pred_proba[pocket_ind][1] > .15:
2.0.0,#################################################### DEBUG
2.0.0,pocket = pockets[pocket_ind]
2.0.0,active_pockets.append(pocket)
2.0.0,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
2.0.0,active_pocket_coords.append(pocket_coords[pocket_ind])
2.0.0,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
2.0.0,# TODO(LESWING)
2.0.0,TODO: add pi_stack and cation_pi to feature_types (it's not trivial
2.0.0,because they require sanitized molecules)
2.0.0,"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.0.0,"""salt_bridge""],"
2.0.0,TODO(rbharath): May want to move this file to S3 so we can ensure it's
2.0.0,always available.
2.0.0,Prepare receptor
2.0.0,Get protein centroid and range
2.0.0,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
2.0.0,going to be extremely slow!
2.0.0,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
2.0.0,first pocket.
2.0.0,Prepare receptor
2.0.0,TODO(rbharath): Generalize this so can support mol2 files as well.
2.0.0,Write Vina conf file
2.0.0,Define locations of log and output files
2.0.0,TODO(rbharath): Let user specify the number of poses required.
2.0.0,TODO(rbharath): Convert the output pdbqt to a pdb file.
2.0.0,Return docked files
2.0.0,Check returned files exist
2.0.0,Check returned files exist
2.0.0,Check returned files exist
2.0.0,Check returned files exist
2.0.0,Check returned files exist
2.0.0,Note this may download autodock Vina...
2.0.0,Note this may download autodock Vina...
2.0.0,Note this may download autodock Vina...
2.0.0,Check returned files exist
2.0.0,Note this may download autodock Vina...
2.0.0,Check returned files exist
2.0.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
2.0.0,box1 contained in box2
2.0.0,"box1 in box2, so complete overlap"
2.0.0,"4/5 atoms in box2 in box1, so 80 % overlap"
2.0.0,box2 contains box1
2.0.0,box1 contains box2
2.0.0,"box1 contains box2, box3"
2.0.0,Test that every atom in pocket maps exists
2.0.0,Check that the atoms is actually in protein
2.0.0,Test that every atom in pocket maps exists
2.0.0,Check that the atoms is actually in protein
2.0.0,Add active site to dict
2.0.0,The convention used is that the first task is the metric.
2.0.0,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
2.0.0,"an option in the Metric class. Instead, this should be possible to move into"
2.0.0,user-space as a custom task_averager function.
2.0.0,"TODO(rbharath, joegomes): What is this magic number?"
2.0.0,"If there are no nonzero examples, metric is ill-defined."
2.0.0,TODO(rbharath): This has been a major source of bugs. Is there a more
2.0.0,robust characterization of which metrics require class-probs and which
2.0.0,don't?
2.0.0,Reshape to handle 1-d edge cases
2.0.0,ids = df[id_field].values
2.0.0,Set missing data to have weight zero
2.0.0,TODO (ytz) this is a bandage solution to reorder the atoms so
2.0.0,that they're always in the same canonical order. Presumably this
2.0.0,should be correctly implemented in the future for graph mols.
2.0.0,Featurize task results iff they exist.
2.0.0,Filter out examples where featurization failed.
2.0.0,"For prospective data where results are unknown, it makes"
2.0.0,no sense to have y values or weights.
2.0.0,"(X, y, w, ids)"
2.0.0,Remove support indices
2.0.0,Remove support indices
2.0.0,Remove support indices
2.0.0,Get task specific entries
2.0.0,Now just get weights for this task
2.0.0,Get task specific entries
2.0.0,Now just get weights for this task
2.0.0,Now just get weights for this task
2.0.0,Now just get weights for this task
2.0.0,Split data into pos and neg lists.
2.0.0,No replacement allowed for supports
2.0.0,Handle one-d vs. non one-d feature matrices
2.0.0,Init the iterator
2.0.0,Set initial iterator state
2.0.0,support = self.supports[task][self.trial_num]
2.0.0,Increment and update logic
2.0.0,Init the iterator
2.0.0,Set initial iterator state
2.0.0,support = self.supports[task][self.trial_num]
2.0.0,Increment and update logic
2.0.0,"By invariant of when this is called, can assume num_samples > 0"
2.0.0,and num_samples < batch_size
2.0.0,Fill in batch arrays
2.0.0,"By invariant of when this is called, can assume num_samples > 0"
2.0.0,and num_samples < batch_size
2.0.0,Fill in batch arrays
2.0.0,Only the first set of copy will be counted in training loss
2.0.0,Retrieve the first sample so we can determine the dtypes.
2.0.0,Create a Tensorflow Dataset and have it create an Iterator.
2.0.0,The -1 indicates that y will be reshaped to have length -1
2.0.0,"Set labels to be zero, with zero weights"
2.0.0,Load obsolete format -> save in new format
2.0.0,note that this corresponds to the _construct_metadata column order
2.0.0,if not len(self.metadata_df):
2.0.0,"raise ValueError(""No data in dataset."")"
2.0.0,return next(self.metadata_df.iterrows())[1]['task_names']
2.0.0,Create temp directory to store resharded version
2.0.0,Write data in new shards
2.0.0,Handle spillover from last shard
2.0.0,These columns may be missing is the dataset is unlabelled.
2.0.0,"(ytz): Depending on the application, thread-based pools may be faster"
2.0.0,"than process based pools, since process based pools need to pickle/serialize"
2.0.0,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
2.0.0,we're actually protected by the GIL.
2.0.0,(ytz): this skips everything except possibly the last shard
2.0.0,if data_dir is None:
2.0.0,data_dir = tempfile.mkdtemp()
2.0.0,The -1 indicates that y will be reshaped to have length -1
2.0.0,"raw_data = (X, y, w, ids)"
2.0.0,Protect against generator exhaustion
2.0.0,This ensures tasks are consistent for all datasets
2.0.0,Get full dataset in memory
2.0.0,Shuffle in memory
2.0.0,Write shuffled shards out to disk
2.0.0,Shuffle the arrays corresponding to each row in metadata_df
2.0.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
2.0.0,Handle edge case with empty indices
2.0.0,Find indices which rest in this shard
2.0.0,Need to offset indices to fit within shard_size
2.0.0,Handle the case of datasets with y/w missing
2.0.0,Updating counts
2.0.0,Break when all indices have been used up already
2.0.0,TODO(rbharath): Get rid of * import
2.0.0,Load MUV dataset
2.0.0,Do an approximate comparison since splits are sometimes slightly off from
2.0.0,the exact fraction.
2.0.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
2.0.0,reloading will cause the transform to be reapplied. This is undesirable in
2.0.0,almost all cases. Need to understand a method to fix this.
2.0.0,def test_shuffle(self):
2.0.0,"""""""Test that datasets can be merged."""""""
2.0.0,current_dir = os.path.dirname(os.path.realpath(__file__))
2.0.0,dataset_file = os.path.join(
2.0.0,"current_dir, ""../../models/tests/example.csv"")"
2.0.0,featurizer = dc.feat.CircularFingerprint(size=1024)
2.0.0,"tasks = [""log-solubility""]"
2.0.0,loader = dc.data.CSVLoader(
2.0.0,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
2.0.0,"dataset = loader.featurize(dataset_file, shard_size=2)"
2.0.0,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
2.0.0,dataset.ids)
2.0.0,orig_len = len(dataset)
2.0.0,dataset.shuffle(iterations=5)
2.0.0,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
2.0.0,dataset.ids)
2.0.0,
2.0.0,assert len(dataset) == orig_len
2.0.0,# The shuffling should have switched up the ordering
2.0.0,"assert not np.array_equal(orig_ids, new_ids)"
2.0.0,# But all the same entries should still be present
2.0.0,assert sorted(orig_ids) == sorted(new_ids)
2.0.0,# All the data should have same shape
2.0.0,assert X_orig.shape == X_new.shape
2.0.0,assert y_orig.shape == y_new.shape
2.0.0,assert w_orig.shape == w_new.shape
2.0.0,The shuffling should have switched up the ordering
2.0.0,But all the same entries should still be present
2.0.0,All the data should have same shape
2.0.0,The ids should now store the performed permutation. Check that the
2.0.0,original dataset is recoverable.
2.0.0,The ids should now store the performed permutation. Check that the
2.0.0,original dataset is recoverable.
2.0.0,Set some global variables up top
2.0.0,Featurize emols dataset
2.0.0,example.fasta contains 3 sequences each of length 58.
2.0.0,The one-hot encoding turns base-pairs into vectors of length 4.
2.0.0,"There is one ""image channel"")"
2.0.0,Generate dummy dataset
2.0.0,Generate dummy dataset
2.0.0,Generate dummy dataset
2.0.0,Set last n_samples/2 weights to 0
2.0.0,Check that no support elements are sample from zero-weight samples
2.0.0,Generate dummy dataset
2.0.0,Generate dummy dataset
2.0.0,Create support generator
2.0.0,Generate dummy dataset
2.0.0,Create support generator
2.0.0,Generate dummy dataset
2.0.0,Assert all support elements have been removed
2.0.0,Generate dummy dataset
2.0.0,Assert all remove elements have been removed
2.0.0,Generate dummy dataset
2.0.0,Assert all support elements have been removed
2.0.0,Generate dummy dataset
2.0.0,Assert all remove elements have been removed
2.0.0,Generate dummy dataset
2.0.0,Set last n_samples/2 weights to 0
2.0.0,Sample from first n_samples/2 elements for support
2.0.0,Should lie within first n_samples/2 samples only
2.0.0,Generate dummy dataset
2.0.0,Create support generator
2.0.0,Generate dummy dataset
2.0.0,Test on identity matrix
2.0.0,Generate random sparse features dataset
2.0.0,Test edge case with array of all zeros
2.0.0,Test cases where n_samples < 2*n_samples < batch_size
2.0.0,Test cases where n_samples < batch_size
2.0.0,Test case where n_samples == batch_size
2.0.0,Test case for object featurization.
2.0.0,Test case for more complicated object featurization
2.0.0,Test case with multidimensional data
2.0.0,Test cases where n_samples < 2*n_samples < batch_size
2.0.0,Test cases where n_samples < batch_size
2.0.0,Test case where n_samples == batch_size
2.0.0,Test case for object featurization.
2.0.0,Test case for more complicated object featurization
2.0.0,Test case with multidimensional data
2.0.0,Test first resharding worked
2.0.0,Test second resharding worked
2.0.0,approx 1/15! chance of equality
2.0.0,Generate data
2.0.0,Generate data
2.0.0,Generate data
2.0.0,Transform it
2.0.0,Transform it
2.0.0,special case to test
2.0.0,deterministic
2.0.0,non-deterministic
2.0.0,we don't know the order in which the shards are iterated in.
2.0.0,Check that we have all the data in
2.0.0,Splits featurized samples into train/test
2.0.0,Splits featurized samples into train/test
2.0.0,Splits featurized samples into train/test
2.0.0,"splittype = ""random"""
2.0.0,Splits featurized samples into train/test
2.0.0,Now perform move
2.0.0,Only for debug!
2.0.0,#Make directories to store the raw and featurized datasets.
2.0.0,Load dataset
2.0.0,Featurize tox21 dataset
2.0.0,###### Do featurization
2.0.0,Do train/valid split.
2.0.0,###### Do singletask load
2.0.0,################# Do comparison
2.0.0,Only for debug!
2.0.0,Set some global variables up top
2.0.0,Make directories to store the raw and featurized datasets.
2.0.0,Load dataset
2.0.0,Featurize tox21 dataset
2.0.0,For debugging purposes
2.0.0,###### Do multitask load
2.0.0,Do train/valid split.
2.0.0,###### Do singletask load
2.0.0,################# Do comparison
2.0.0,"task_type = ""regression"""
2.0.0,coding=utf-8
2.0.0,Note that transformers have to be undone in reversed order
2.0.0,Hack to allow for easy unpickling:
2.0.0,http://stefaanlippens.net/pickleproblem
2.0.0,"One, but not both, transform_X or tranform_y is true"
2.0.0,Use fact that bools add as ints in python
2.0.0,Control for pathological case with no variance.
2.0.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
2.0.0,Find the task dimension of z
2.0.0,Prevent broadcasting on wrong dimension
2.0.0,BalancingTransformer can only transform weights.
2.0.0,Compute weighting factors from dataset.
2.0.0,Ensure dataset is binary
2.0.0,Remove labels with zero weights
2.0.0,self.w = dataset.w
2.0.0,"TODO (flee2): for transform_y, figure out weights"
2.0.0,"print(""y will not be transformed by CDFTransformer, for now."")"
2.0.0,"print(""Cannot undo CDF Transformer, for now."")"
2.0.0,Need this for transform_y
2.0.0,array = np.transpose(array)
2.0.0,"print(""y will not be transformed by PowerTransformer, for now."")"
2.0.0,"print(""Cannot undo Power Transformer, for now."")"
2.0.0,the tf graph here pick up the (K+1) highest similarity values
2.0.0,and their indices
2.0.0,map the indices to labels
2.0.0,generating batch of data by slicing similarity matrix
2.0.0,into 100*reference_dataset_length
2.0.0,concatenate batches of data together
2.0.0,highest similarity is 1: target is in the reference
2.0.0,use the following K points
2.0.0,"highest less than 1: target not in the reference, use top K points"
2.0.0,calculate matrix multiplicatin on slices
2.0.0,concatenate the slices together
2.0.0,list of calculation orders for DAGs
2.0.0,stemming from one specific atom in the molecule
2.0.0,starting from the adjacency list derived by graphconv featurizer
2.0.0,"number of atoms, also number of DAGs"
2.0.0,"DAG on a molecule with k atoms includes k steps of calculation,"
2.0.0,each step calculating graph features for one atom.
2.0.0,`max_atoms` is the maximum number of steps
2.0.0,each iteration generates the DAG starting from atom with index `count`
2.0.0,"list of lists, elements represent the calculation orders"
2.0.0,for atoms in the current graph
2.0.0,starting from the target atom with index `count`
2.0.0,flags of whether the atom is already included in the DAG
2.0.0,atom `count` is in the DAG
2.0.0,recording number of radial propagation steps
2.0.0,"in the fisrt loop, atoms directly connected to `count` will be added"
2.0.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
2.0.0,will be added in the second loop(radial=1).
2.0.0,atoms i-bond away will be added in i-th loop
2.0.0,"when molecules have separate parts, starting from one part,"
2.0.0,it is not possible to include all atoms.
2.0.0,this break quit the loop when going into such condition
2.0.0,reinitialize targets for next iteration
2.0.0,atoms connected to current_atom
2.0.0,generate the dependency map of current DAG
2.0.0,atoms connected to `current_atoms`(and not included in the DAG)
2.0.0,"are added, and will be the `current_atoms` for next iteration."
2.0.0,"DAG starts from the target atom, calculation should go in reverse"
2.0.0,`edge[1]` is the parent of `edge[0]`
2.0.0,"after this loop, `parents[i]` includes all parents of atom i"
2.0.0,manually adding the atom index into its parents list
2.0.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
2.0.0,atoms with less parents(farther from the target atom) come first.
2.0.0,"graph features of atoms without parents will be first calculated,"
2.0.0,then atoms with more parents can be calculated in order
2.0.0,based on previously calculated graph features.
2.0.0,target atom of this DAG will be calculated in the last step
2.0.0,padding with `max_atoms`
2.0.0,padding
2.0.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
2.0.0,which is a max_atoms * max_atoms numpy array after padding
2.0.0,Calculate pairwise distance
2.0.0,Masking for valid atom index
2.0.0,Cutoff with threshold Rc
2.0.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is a y transformer
2.0.0,Check w is unchanged since this is a y transformer
2.0.0,Check y is now a logarithmic version of itself
2.0.0,Check that untransform does the right thing.
2.0.0,transforming y should raise an exception
2.0.0,transforming w should raise an exception
2.0.0,transforming X should be okay
2.0.0,Check ids are unchanged.
2.0.0,Check y is unchanged since this is a X transformer
2.0.0,Check w is unchanged since this is a y transformer
2.0.0,Check y is now a logarithmic version of itself
2.0.0,Check that untransform does the right thing.
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is a y transformer
2.0.0,Check w is unchanged since this is a y transformer
2.0.0,Check y is now a logarithmic version of itself
2.0.0,Check that untransform does the right thing.
2.0.0,Tests logarithmic data transformer with selection.
2.0.0,Check ids are unchanged.
2.0.0,Check y is unchanged since this is a X transformer
2.0.0,Check w is unchanged since this is a y transformer
2.0.0,Check y is now a logarithmic version of itself
2.0.0,Check that untransform does the right thing.
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is a y transformer
2.0.0,Check w is unchanged since this is a y transformer
2.0.0,"Check that y_t has zero mean, unit std."
2.0.0,Check that untransform does the right thing.
2.0.0,Check ids are unchanged.
2.0.0,Check y is unchanged since this is a X transformer
2.0.0,Check w is unchanged since this is a y transformer
2.0.0,"Check that X_t has zero mean, unit std."
2.0.0,np.set_printoptions(threshold='nan')
2.0.0,Entries with zero std are not normalized
2.0.0,TODO(rbharath): Untransform doesn't work properly for binary feature
2.0.0,vectors. Need to figure out what's wrong here. (low priority)
2.0.0,# Check that untransform does the right thing.
2.0.0,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
2.0.0,Check ids are unchanged.
2.0.0,Check y is unchanged since this is an X transformer
2.0.0,Check w is unchanged since this is an X transformer
2.0.0,Check X is now holding the proper values when sorted.
2.0.0,Test CDF transformer on Gaussian normal dataset.
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is an y transformer
2.0.0,Check w is unchanged since this is an y transformer
2.0.0,Check y is now holding the proper values when sorted.
2.0.0,Check that untransform does the right thing.
2.0.0,Check ids are unchanged.
2.0.0,Check y is unchanged since this is an X transformer
2.0.0,Check w is unchanged since this is an X transformer
2.0.0,Check X is now holding the proper values when sorted.
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is a y transformer
2.0.0,Check w is unchanged since this is a y transformer
2.0.0,Check y is now holding the proper values when sorted.
2.0.0,Check ids are unchanged.
2.0.0,Check y is unchanged since this is an X transformer
2.0.0,Check w is unchanged since this is an X transformer
2.0.0,Check X is now holding the proper values in each column.
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is an X transformer
2.0.0,Check w is unchanged since this is an X transformer
2.0.0,Check y is now holding the proper values in each column.
2.0.0,Check that untransform does the right thing.
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is a w transformer
2.0.0,Check y is unchanged since this is a w transformer
2.0.0,Assert that entries with zero weight retain zero weight
2.0.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.0.0,Check ids are unchanged.
2.0.0,Check X is unchanged since this is a w transformer
2.0.0,Check y is unchanged since this is a w transformer
2.0.0,Assert that entries with zero weight retain zero weight
2.0.0,Check that sum of 0s equals sum of 1s in transformed for each task
2.0.0,TODO(rbharath): Use standard joblib once old-data has been regenerated.
2.0.0,"If gzipped, need to compute extension again"
2.0.0,Tasks are stored in .sdf.csv file
2.0.0,Structures are stored in .sdf file
2.0.0,First line of user-specified CSV *must* be header.
2.0.0,depends on Python version
2.0.0,The label encoder is given characters for ACGTN
2.0.0,"These are transformed in 0, 1, 2, 3, 4 in input sequence"
2.0.0,TODO(rbharath): Unlike the DRAGONN implementation from which this
2.0.0,"was ported, I couldn't transform the ""ACGT..."" strings into"
2.0.0,integers all at once. Had to do one at a time. Might be worth
2.0.0,figuring out what's going on under the hood.
2.0.0,Try older joblib version for legacy files.
2.0.0,First line of user-specified CSV *must* be header.
2.0.0,First line of user-specified CSV *must* be header.
2.0.0,combine dataframes
2.0.0,TODO: mol should be always sanitized when charges are calculated
2.0.0,can't change it now because it would break a lot of examples
2.0.0,working-with-3d-molecules
2.0.0,initial embedding
2.0.0,minimization and pruning
2.0.0,always keep lowest-energy conformer
2.0.0,discard conformers after max_conformers is reached
2.0.0,get RMSD to selected conformers
2.0.0,discard conformers within the RMSD threshold
2.0.0,create a new molecule to hold the chosen conformers
2.0.0,this ensures proper conformer IDs and energy-based ordering
2.0.0,TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
2.0.0,import nglview
2.0.0,import tempfile
2.0.0,import os
2.0.0,import mdtraj as md
2.0.0,import numpy as np
2.0.0,import tempfile
2.0.0,from rdkit import Chem
2.0.0,from rdkit.Chem import Draw
2.0.0,from itertools import islice
2.0.0,"from IPython.display import Image, HTML, display"
2.0.0,
2.0.0,"def combine_mdtraj(protein, ligand):"
2.0.0,chain = protein.topology.add_chain()
2.0.0,"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
2.0.0,for atom in ligand.topology.atoms:
2.0.0,"protein.topology.add_atom(atom.name, atom.element, residue)"
2.0.0,"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
2.0.0,protein.topology.create_standard_bonds()
2.0.0,return protein
2.0.0,
2.0.0,def visualize_complex(complex_mdtraj):
2.0.0,"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
2.0.0,"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
2.0.0,binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
2.0.0,binding_pocket_residues = [str(r) for r in binding_pocket_residues]
2.0.0,"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
2.0.0,
2.0.0,traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
2.0.0,ngltraj = nglview.NGLWidget( traj )
2.0.0,ngltraj.representations = [
2.0.0,"{ ""type"": ""cartoon"", ""params"": {"
2.0.0,"""sele"": ""protein"", ""color"": ""residueindex"""
2.0.0,"} },"
2.0.0,"{ ""type"": ""licorice"", ""params"": {"
2.0.0,"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
2.0.0,"} },"
2.0.0,"{ ""type"": ""ball+stick"", ""params"": {"
2.0.0,"""sele"": ""LIG"""
2.0.0,} }
2.0.0,]
2.0.0,return ngltraj
2.0.0,
2.0.0,def visualize_ligand(ligand_mdtraj):
2.0.0,traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
2.0.0,ngltraj = nglview.NGLWidget( traj )
2.0.0,ngltraj.representations = [
2.0.0,"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
2.0.0,return ngltraj
2.0.0,
2.0.0,def convert_lines_to_mdtraj(molecule_lines):
2.0.0,tempdir = tempfile.mkdtemp()
2.0.0,"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
2.0.0,"with open(molecule_file, ""wb"") as f:"
2.0.0,f.writelines(molecule_lines)
2.0.0,molecule_mdtraj = md.load(molecule_file)
2.0.0,return molecule_mdtraj
2.0.0,
2.0.0,def display_images(filenames):
2.0.0,"""""""Helper to pretty-print images."""""""
2.0.0,imagesList=''.join(
2.0.0,"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
2.0.0,% str(s) for s in sorted(filenames)])
2.0.0,display(HTML(imagesList))
2.0.0,
2.0.0,"def mols_to_pngs(mols, basename=""test""):"
2.0.0,"""""""Helper to write RDKit mols to png files."""""""
2.0.0,filenames = []
2.0.0,"for i, mol in enumerate(mols):"
2.0.0,"filename = ""%s%d.png"" % (basename, i)"
2.0.0,"Draw.MolToFile(mol, filename)"
2.0.0,filenames.append(filename)
2.0.0,return filenames
2.0.0,TODO(rbharath): This is now simple enough that we should probably get rid of
2.0.0,Evaluator object to avoid clutter.
2.0.0,Compute multitask metrics
2.0.0,Compute multitask metrics
2.0.0,Loosening atol to see if tests stop failing sporadically
2.0.0,One sequence has length longer than others. This should throw a
2.0.0,value error.
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,a*x + b*y + c*z = dI think that
2.0.0,"self.x, self.y, self.z = x, y, z"
2.0.0,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
2.0.0,TODO(bramsundar): Should this be __copy__?
2.0.0,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
2.0.0,"return np.array([self.x, self.y, self.z])"
2.0.0,TODO(rbharath): Should this be an atom function?
2.0.0,"This line is necessary for babel to work, though many PDBs in"
2.0.0,the PDB would have this line commented out
2.0.0,now atom type (for pdbqt)
2.0.0,"If atomtype is not specified, but atomname is, set atomtype to the"
2.0.0,"first letter of atomname. This heuristic suffices for proteins,"
2.0.0,since no two-letter elements appear in standard amino acids.
2.0.0,Any number needs to be removed from the element name
2.0.0,"this only uses the rightmost three characters, essentially"
2.0.0,removing unique rotamer identification
2.0.0,"The normal vector to plane is n = [a, b, c]"
2.0.0,We first shift by basepoint (a point on given plane) to make math
2.0.0,simpler. basepoint is given by d/||n||^2 * n
2.0.0,The perpendicular component of diff to plane is
2.0.0,(n^T diff / ||n||^2) * n
2.0.0,if ring is aromatic
2.0.0,"save its indices, center, and normal"
2.0.0,remember protein-ligand pairs we already counted
2.0.0,"if this pair is new, count atoms forming a contact"
2.0.0,"if this pair is new, count atoms forming a contact"
2.0.0,if ring from mol1 is aromatic
2.0.0,...and atom from mol2 is a cation
2.0.0,if angle and distance are correct
2.0.0,count atoms forming a contact
2.0.0,find interacting rings from protein and cations from ligand
2.0.0,find interacting cations from protein and rings from ligand
2.0.0,merge counters
2.0.0,TODO(LESWING)
2.0.0,check if user tries to set removed arguments
2.0.0,list of features that require sanitized molecules
2.0.0,not implemented featurization types
2.0.0,default values
2.0.0,update with cutoffs specified by the user
2.0.0,define methods to calculate available flat features
2.0.0,all methods (flat and voxel) must have the same API:
2.0.0,"f(prot_xyz, prot_rdk, lig_xyz, lig_rdk, distances) -> list of np.ndarrays"
2.0.0,define methods to calculate available voxel features
2.0.0,"each entry is a tuple (is_flat, feature_name)"
2.0.0,list of features that cannot be calculated with specified parameters
2.0.0,this list is used to define <flat/voxel/all>_combined subset
2.0.0,parse provided feature types
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Get the degree id list (which corrects for min_deg)
2.0.0,Get the size of each degree block
2.0.0,Get the the start indices for items in each block
2.0.0,Get the node indices when they are reset when the degree changes
2.0.0,Convert to numpy array
2.0.0,Reorder old atom_features
2.0.0,Reorder old deg lists
2.0.0,Sort membership
2.0.0,Create old to new dictionary. not exactly intuitive
2.0.0,Reorder adjacency lists
2.0.0,Get numpy version of degree list for indexing
2.0.0,"Initialize adj_lists, which supports min_deg = 1 only"
2.0.0,Parse as deg separated
2.0.0,Get indices corresponding to the current degree
2.0.0,Extract and save adjacency list for the current degree
2.0.0,Construct the slice information
2.0.0,Get the cumulative indices after the first index
2.0.0,Set indices with zero sized slices to zero to avoid indexing errors
2.0.0,TODO(rbharath): Can this be removed?
2.0.0,Use random insted of zeros to prevent weird issues with summing to zero
2.0.0,Get atoms by degree
2.0.0,stack the atoms
2.0.0,Sort all atoms by degree.
2.0.0,"Get the size of each atom list separated by molecule id, then by degree"
2.0.0,Get the final size of each degree block
2.0.0,"Get the index at which each degree starts, not resetting after each degree"
2.0.0,And not stopping at any speciic molecule
2.0.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
2.0.0,first column telling the start indices of each degree block and the
2.0.0,second colum telling the size of each degree block
2.0.0,Input for tensorflow
2.0.0,Determines the membership (atom i belongs to membership[i] molecule)
2.0.0,"Get the index at which each deg starts, resetting after each degree"
2.0.0,(deg x num_mols) matrix describing the start indices when you count up the atoms
2.0.0,"in the final representation, stopping at each molecule,"
2.0.0,resetting every time the degree changes
2.0.0,Gets the degree resetting block indices for the atoms in each molecule
2.0.0,"Here, the indices reset when the molecules change, and reset when the"
2.0.0,degree changes
2.0.0,Get the degree id lookup list. It allows us to search for the degree of a
2.0.0,molecule mol_id with corresponding atom mol_atom_id using
2.0.0,"deg_id_lists[mol_id,mol_atom_id]"
2.0.0,This is used for convience in the following function (explained below)
2.0.0,Get the degree id (corrected for min_deg) of the considered atom
2.0.0,Return the final index of atom mol_atom_id in molecule mol_id.  Using
2.0.0,"the degree of this atom, must find the index in the molecule's original"
2.0.0,"degree block corresponding to degree id deg_id (second term), and then"
2.0.0,calculate which index this degree block ends up in the final
2.0.0,representation (first term). The sum of the two is the final indexn
2.0.0,Initialize the new degree separated adjacency lists
2.0.0,Update the old adjcency lists with the new atom indices and then combine
2.0.0,all together
2.0.0,Iterate through all the molecules
2.0.0,Get the adjacency lists for this molecule and current degree id
2.0.0,"Correct all atom indices to the final indices, and then save the"
2.0.0,results into the new adjacency lists
2.0.0,Increment once row is done
2.0.0,Get the final aggregated molecule
2.0.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
2.0.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
2.0.0,consistent with most QM software packages.
2.0.0,Type of data created by this featurizer
2.0.0,TODO(rbharath): Should this return a list?
2.0.0,Type of data created by this featurizer
2.0.0,generate SMILES for fragments
2.0.0,Initalize with 1
2.0.0,Allow 0 index to correspond to null molecule 1
2.0.0,Correct for null
2.0.0,"print(6-k-1, id)"
2.0.0,Correct for last one
2.0.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
2.0.0,first `bt_len` features are bond features(if applicable)
2.0.0,`bt_len`-th feature is if the pair of atoms are in the same ring
2.0.0,graph distance between two atoms
2.0.0,Euclidean distance between atoms
2.0.0,atoms `radial` bonds away from `a1`
2.0.0,atoms less than `radial` bonds away
2.0.0,find atoms `radial`+1 bonds away
2.0.0,Get the node features
2.0.0,Stack nodes into an array
2.0.0,Get bond lists with reverse edges included
2.0.0,Get canonical adjacency list
2.0.0,"Distance is either graph distance(True) or Euclidean distance(False,"
2.0.0,only support datasets providing Cartesian coordinates)
2.0.0,Set dtype
2.0.0,If includes explicit hydrogens
2.0.0,If uses use_chirality
2.0.0,Atom features
2.0.0,Stack nodes into an array
2.0.0,Get bond lists
2.0.0,Get canonical adjacency list
2.0.0,Calculate pair features
2.0.0,atom_name is of format RESX-ATOMTYPE
2.0.0,where X is a 1 to 4 digit number
2.0.0,list-of-available-descriptors.
2.0.0,(ytz): This is done to avoid future compatibility issues like inclusion of
2.0.0,the 3D descriptors or changing the feature size.
2.0.0,check for separate count and SMILES entries for each fragment
2.0.0,TODO test more formats for ligand
2.0.0,some users might try to read smiles with this function
2.0.0,adding hydrogens and charges is tested in dc.utils
2.0.0,3D vector with unit length
2.0.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
2.0.0,check if distances do not change
2.0.0,check if it works for molecules with different numbers of atoms
2.0.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
2.0.0,check if correct distance metric was used
2.0.0,"20 points with coords between -5 and 5, centered at 0"
2.0.0,indices are positive
2.0.0,coordinates were properly translated and scaled
2.0.0,for coordinates outside of the box function should properly transform them
2.0.0,to indices and warn the user
2.0.0,"TODO check if function warns. There is assertWarns method in unittest,"
2.0.0,but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
2.0.0,"20 points with coords between -5 and 5, centered at 0"
2.0.0,3 pairs of indices
2.0.0,simple flat ring
2.0.0,load and sanitize two real molecules
2.0.0,FIXME might break with different version of rdkit
2.0.0,FIXME might break with different version of rdkit
2.0.0,parallel normals
2.0.0,perpendicular normals
2.0.0,too far away
2.0.0,perpendicular normals
2.0.0,parallel normals
2.0.0,too far away
2.0.0,order of the molecules shouldn't matter
2.0.0,with this criteria we should find both types of stacking
2.0.0,parallel normals
2.0.0,perpendicular normals
2.0.0,too far away
2.0.0,"TODO find better example, currently dicts are empty"
2.0.0,"TODO find better example, currently dicts are empty"
2.0.0,TODO test if dict contains smiles
2.0.0,check if results are the same if we provide precomputed distances
2.0.0,...but first check if we actually got two dicts
2.0.0,check if we get less features with smaller distance cutoff
2.0.0,ligands are typically small so all atoms might be present
2.0.0,check if using different ecfp_degree changes anything
2.0.0,TODO upperbound?
2.0.0,test if default parameters work
2.0.0,check if use-case from examples works
2.0.0,test if input is flattened when flat features are used
2.0.0,test voxel features
2.0.0,test flat features
2.0.0,check if aromatic features are ignores if sanitize=False
2.0.0,"protein is too big for the box, some features should be missing"
2.0.0,whole ligand should fit in the box
2.0.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
2.0.0,of degree 1 (connected only to central nitrogen).
2.0.0,5 atoms in compound
2.0.0,Get the adjacency lists grouped by degree
2.0.0,The 4 outer atoms connected to central nitrogen
2.0.0,Central nitrogen connected to everything else.
2.0.0,Only one carbon
2.0.0,"No bonds, so degree adjacency lists are empty"
2.0.0,3 carbonds in alkane
2.0.0,Outer two carbonds are connected to central carbon
2.0.0,Central carbon connected to outer two
2.0.0,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
2.0.0,this expected behavior? Need to think about API.
2.0.0,Do a manual distance computation and make
2.0.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
2.0.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
2.0.0,Do a manual distance computation and ensure that selected neighbor is
2.0.0,closest since we set max_num_neighbors = 1
2.0.0,Splits featurized samples into train/test
2.0.0,Artificial feature array.
2.0.0,0 atoms of degree 0
2.0.0,0 atoms of degree 1
2.0.0,4 atoms of degree 2
2.0.0,0 atoms of degree 3
2.0.0,0 atoms of degree 4
2.0.0,0 atoms of degree 5
2.0.0,0 atoms of degree 6
2.0.0,0 atoms of degree 7
2.0.0,0 atoms of degree 8
2.0.0,0 atoms of degree 9
2.0.0,0 atoms of degree 10
2.0.0,atom 4 has 0 neighbors
2.0.0,atom 0 has 2 neighbors
2.0.0,atom 1 has 2 neighbors
2.0.0,atom 2 has 2 neighbors
2.0.0,atom 3 has 3 neighbors.
2.0.0,Verify that atom features have been sorted by atom degree.
2.0.0,Sorting is done by atom degree as before. So the ordering goes
2.0.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
2.0.0,from new position to old position is
2.0.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
2.0.0,list respects this reordering and returns correct adjacency list.
2.0.0,### First example molecule
2.0.0,Artificial feature array.
2.0.0,### Second example molecule
2.0.0,## Third example molecule
2.0.0,Test agglomerate molecule method
2.0.0,No atoms of degree 0
2.0.0,3 atoms of degree 1
2.0.0,8 atoms of degree 2
2.0.0,1 atom of degree 3
2.0.0,0 atoms of degree 4
2.0.0,0 atoms of degree 5
2.0.0,Check that atoms are only connected to themselves.
2.0.0,Check that there's one atom of each degree.
2.0.0,assumes that every array is of the same dimension
2.0.0,rem_dataset is remaining portion of dataset
2.0.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.0.0,to k-1.
2.0.0,dict is needed in case groups aren't strictly flattened or
2.0.0,hashed by something non-integer like
2.0.0,returns list of per column sum of non zero elements
2.0.0,Compute number of actives needed per task.
2.0.0,loop through each column and obtain index required to splice out for
2.0.0,required fraction of hits
2.0.0,Find the first index where the cumulative number of actives equals
2.0.0,the actives_count
2.0.0,Note that np.where tells us last index required to exceed
2.0.0,"actives_count, so we actually want the following location"
2.0.0,TODO(rbharath): Refactor this split method to match API of other splits (or
2.0.0,potentially refactor those to match this.
2.0.0,Handle edge case where frac_split is 1
2.0.0,Create weight matrices fpor two haves.
2.0.0,copy over up to required index for weight first_split
2.0.0,check out if any rows in either w_1 or w_2 are just zeros
2.0.0,"Obtain original x, y, and w arrays and shuffle"
2.0.0,calculate percent split for valid (out of test and valid)
2.0.0,"split test data into valid and test, treating sub test set also as sparse"
2.0.0,rem_dataset is remaining portion of dataset
2.0.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
2.0.0,to k-1.
2.0.0,JSG Assert that split fractions can be written as proper fractions over 10.
2.0.0,This can be generalized in the future with some common demoninator determination.
2.0.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
2.0.0,Append remaining examples to train
2.0.0,Sort by increasing MW
2.0.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
2.0.0,for m_idx in cluster:
2.0.0,"continue until we find an active in all the tasks, otherwise we can't"
2.0.0,compute a meaningful AUC
2.0.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
2.0.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
2.0.0,Sort from largest to smallest scaffold sets
2.0.0,Pick the mol closest to everything as the first element of training
2.0.0,Pick the closest mol from what is left
2.0.0,Test is everything else
2.0.0,All datasets share features and identifiers by assumption.
2.0.0,TODO(rbharath): Get rid of * import
2.0.0,Note that the extra task goes to test
2.0.0,Number tasks per fold
2.0.0,Find the tasks that correspond to this test fold
2.0.0,Assert that all arrays look like they should
2.0.0,0 1 2 3 4 5 6 7 8 9
2.0.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
2.0.0,data. Make a test for properly splitting of sharded data. Perhaps using
2.0.0,reshard() to handle this?
2.0.0,Verify lengths is 10/k == 2
2.0.0,Verify that compounds in this fold are subset of original compounds
2.0.0,Verify that no two folds have overlapping compounds.
2.0.0,Verify lengths is 10/k == 2
2.0.0,Verify that compounds in this fold are subset of original compounds
2.0.0,Verify that no two folds have overlapping compounds.
2.0.0,Verify lengths is 10/k == 2
2.0.0,Verify that compounds in this fold are subset of original compounds
2.0.0,Verify that no two folds have overlapping compounds.
2.0.0,Test singletask case.
2.0.0,The split index should partition dataset in half.
2.0.0,Test singletask case.
2.0.0,Test case where some weights are zero (i.e. masked)
2.0.0,Set half the positives to have zero weight
2.0.0,There are 10 nonzero actives.
2.0.0,"The split index should partition this into half, so expect 5"
2.0.0,The split index should partition dataset in half.
2.0.0,Mask half the examples
2.0.0,The split index should partition dataset in half.
2.0.0,Test singletask case.
2.0.0,Should have split cleanly in half (picked random seed to ensure this)
2.0.0,Check positives are correctly distributed
2.0.0,Verify lengths is 100/k == 20
2.0.0,Note: This wouldn't work for multitask str
2.0.0,assert len(fold_dataset) == n_samples/K
2.0.0,Verify that each fold has n_positives/K = 4 positive examples.
2.0.0,Verify that compounds in this fold are subset of original compounds
2.0.0,Verify that no two folds have overlapping compounds.
2.0.0,sparsity is determined by number of w weights that are 0 for a given
2.0.0,task structure of w np array is such that each row corresponds to a
2.0.0,sample. The loaded sparse dataset has many rows with only zeros
2.0.0,verify that there are no rows (samples) in weights matrix w
2.0.0,that have no hits.
2.0.0,task_metadata_rows = {task: [] for task in tasks}
2.0.0,Extract those datapoints which are present for this task
2.0.0,Loading is done on-the-fly
2.0.0,TODO(rbharath/enf): We need a structured way to deal with potential GPU
2.0.0,memory overflows.
2.0.0,Discard any padded predictions
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,Special case to handle singletasks.
2.0.0,The iterbatches does padding with zero-weight examples on the last batch.
2.0.0,Remove padded examples.
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,Calculate pairwise distance
2.0.0,Masking for valid atom index
2.0.0,Cutoff with threshold Rc
2.0.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.0.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
2.0.0,optimization to allow for tensorcontraction/broadcasted mmul
2.0.0,using a reshape trick. Note that the np and tf matmul behavior
2.0.0,differs when dealing with broadcasts
2.0.0,-*- coding: UTF-8 -*-
2.0.0,Reshape everything to match the input with the most dimensions.
2.0.0,"This probably means the variable hasn't been created yet, so try again"
2.0.0,with reuse set to false.
2.0.0,"H(x), with same number of input and output channels"
2.0.0,"T(x), with same number of input and output channels"
2.0.0,Calculate what the new shape will be.
2.0.0,"Shape (N_atoms, M_nbrs, ndim)"
2.0.0,"Shape (N_atoms, M_nbrs, ndim)"
2.0.0,"Shape (N_atoms, M_nbrs)"
2.0.0,"This probably means the variable hasn't been created yet, so try again"
2.0.0,with reuse set to false.
2.0.0,"This probably means the variable hasn't been created yet, so try again"
2.0.0,with reuse set to false.
2.0.0,"This probably means the variable hasn't been created yet, so try again"
2.0.0,with reuse set to false.
2.0.0,"This probably means the variable hasn't been created yet, so try again"
2.0.0,with reuse set to false.
2.0.0,TODO(rbharath): Note sure if this layer can be called with __call__
2.0.0,"meaningfully, so not going to support that functionality for now."
2.0.0,"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
2.0.0,Generate the nb_affine weights and biases
2.0.0,Extract atom_features
2.0.0,Extract graph topology
2.0.0,Perform the mol conv
2.0.0,"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
2.0.0,"self.max_deg, self.min_deg, self.W_list,"
2.0.0,self.b_list)
2.0.0,Sum all neighbors using adjacency matrix
2.0.0,Get collection of modified atom features
2.0.0,Obtain relevant atoms for this degree
2.0.0,Get self atoms
2.0.0,Apply hidden affine to relevant atoms and append
2.0.0,Determine the min_deg=0 case
2.0.0,Only use the self layer
2.0.0,Combine all atoms back into the list
2.0.0,Tensorflow correctly processes empty lists when using concat
2.0.0,"Sum along neighbors as well as self, and store"
2.0.0,Perform the mol gather
2.0.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
2.0.0,"self.max_degree, self.min_degree)"
2.0.0,Tensorflow correctly processes empty lists when using concat
2.0.0,Get self atoms
2.0.0,Expand dims
2.0.0,always deg-1 for deg_adj_lists
2.0.0,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
2.0.0,Extract graph topology
2.0.0,Perform the mol gather
2.0.0,Obtain the partitions for each of the molecules
2.0.0,Sum over atoms for each molecule
2.0.0,Get the final sparse representations
2.0.0,No other forget biases supported right now.
2.0.0,Taken from Keras code [citation needed]
2.0.0,"x is test set, xp is support set."
2.0.0,# Initializes trainable weights.
2.0.0,## Performs computations
2.0.0,Get initializations
2.0.0,Process using attention
2.0.0,"Eqn (4), appendix A.1 of Matching Networks paper"
2.0.0,Generate new attention states
2.0.0,Support set lstm
2.0.0,Test lstm
2.0.0,self.build()
2.0.0,Get initializations
2.0.0,Rename support
2.0.0,Process support xp using attention
2.0.0,Get linear combination of support set
2.0.0,Process test x using attention
2.0.0,Generate new support attention states
2.0.0,Generate new test attention states
2.0.0,Redefine
2.0.0,Number of rotatable bonds
2.0.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
2.0.0,a difference.
2.0.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
2.0.0,neighbors lists an argument instead of a part of this layer.
2.0.0,"Shape (N, M)"
2.0.0,"Shape (N, M)"
2.0.0,"Shape (N, M)"
2.0.0,Number of grid cells
2.0.0,TODO(rbharath): Support batching
2.0.0,"Shape (n_cells, ndim)"
2.0.0,"List of length N_atoms, each element of different length uniques_i"
2.0.0,"List of length N_atoms, each element of different length uniques_i"
2.0.0,"List of length N_atoms, each a tensor of shape"
2.0.0,"(uniques_i, ndim)"
2.0.0,Add phantom atoms that exist far outside the box
2.0.0,"List of length N_atoms, each of shape (1, ndim)"
2.0.0,TODO(rbharath): How does distance need to be modified here to
2.0.0,account for periodic boundary conditions?
2.0.0,List of length N_atoms each of shape (M_nbrs)
2.0.0,"N_atoms elts of size (M_nbrs,) each"
2.0.0,"Shape (N_atoms, 1)"
2.0.0,Find M_nbrs atoms closest to each cell
2.0.0,"Shape (n_cells, M_nbrs)"
2.0.0,Associate each cell with its neighbor cells. Assumes periodic boundary
2.0.0,"conditions, so does wrapround. O(constant)"
2.0.0,"Shape (n_cells, n_nbr_cells)"
2.0.0,"Shape (N_atoms, n_nbr_cells)"
2.0.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
2.0.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
2.0.0,"List of length N_atoms, each element length uniques_i"
2.0.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
2.0.0,element removed to remove self from list of neighbors. Need to verify
2.0.0,this holds more broadly or come up with robust alternative.
2.0.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.0.0,"Shape (N_atoms*n_cells, ndim) after tile"
2.0.0,Shape (N_atoms*n_cells)
2.0.0,"Shape (n_cells, N_atoms)"
2.0.0,Find k atoms closest to this cell. Notice negative sign since
2.0.0,tf.nn.top_k returns *largest* not smallest.
2.0.0,"Tensor of shape (n_cells, M_nbrs)"
2.0.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
2.0.0,"Shape (N_atoms*n_cells, 1) after tile"
2.0.0,9 neighbors in 2-space
2.0.0,TODO(rbharath): Shoddy handling of higher dimensions...
2.0.0,Number of cells for cube in 3-space is
2.0.0,TODO(rbharath): Do we need to handle periodic boundary conditions
2.0.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
2.0.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
2.0.0,the cube.
2.0.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
2.0.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
2.0.0,"Tile (a, a, a, b, b, b, etc.)"
2.0.0,"Tile (a, b, c, a, b, c, ...)"
2.0.0,N: Maximum number of atoms
2.0.0,M: Maximum number of neighbors
2.0.0,d: Number of coordinates/features/filters
2.0.0,B: Batch Size
2.0.0,We apply the radial pooling filter before atom type conv
2.0.0,to reduce computation
2.0.0,check that there isnt just one or zero inputs
2.0.0,create subspaces
2.0.0,create the alpha learnable parameters
2.0.0,"concatenate subspaces, reshape to size of original input, then stack"
2.0.0,"such that out_tensor has shape (2,?,original_cols)"
2.0.0,creates subspaces the same way it was done in AlphaShare
2.0.0,calculate squared Frobenius norm
2.0.0,"(TODO YTZ:) faster, less memory intensive way"
2.0.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
2.0.0,"r = tf.expand_dims(r, -1)"
2.0.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
2.0.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
2.0.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
2.0.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
2.0.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
2.0.0,Calculate pairwise distance
2.0.0,Cutoff with threshold Rc
2.0.0,return d
2.0.0,tf.stack issues again...
2.0.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
2.0.0,We do not need the mask because every graph has self.num_vertices vertices now
2.0.0,So the Tensor has known dimensions
2.0.0,Calling fit() for first time
2.0.0,Add in features
2.0.0,Add in labels
2.0.0,Add in all layers
2.0.0,The last layer is the output of the model
2.0.0,TODO(rbharath): Add in support for additional
2.0.0,losses.
2.0.0,TODO(rbharath): The TensorGraph can't be built until
2.0.0,fit is called since the shapes of features/labels
2.0.0,not specified. Need to figure out a good restoration
2.0.0,method for this use case.
2.0.0,ensure that randomness is conditioned by the Numpy RNG
2.0.0,ensure that randomness is conditioned by the Numpy RNG
2.0.0,TODO(rbharath): Should probably swap this over to tf mode.
2.0.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.0.0,"expects logits, Keras expects probabilities."
2.0.0,scale preds so that the class probas of each sample sum to 1
2.0.0,manual computation of crossentropy
2.0.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.0.0,"expects logits, Keras expects probabilities."
2.0.0,if our output includes timesteps we need to reshape
2.0.0,Arguments
2.0.0,Returns
2.0.0,Note: tf.nn.softmax_cross_entropy_with_logits
2.0.0,"expects logits, Keras expects probabilities."
2.0.0,transform back to logits
2.0.0,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
2.0.0,a tensor. Confusing with tf.zeros...
2.0.0,Transpose for mul
2.0.0,exclude bias variables
2.0.0,"tf.scalar_summary('Weight Decay Cost', cost)"
2.0.0,TODO(user): gradient clipping (see Minimize)
2.0.0,Assuming convolution kernels (2D or 3D).
2.0.0,"TF kernel shape: (..., input_depth, depth)"
2.0.0,No specific assumptions.
2.0.0,References
2.0.0,References
2.0.0,References
2.0.0,References
2.0.0,Pick the one with the correct shape.
2.0.0,Add the input features.
2.0.0,Add the shared dense layers
2.0.0,Add task-specific bypass layers
2.0.0,"Results is of shape (n_samples, n_tasks, n_classes)"
2.0.0,"Results is of shape (n_samples, n_tasks)"
2.0.0,Add the input features.
2.0.0,Add the shared dense layers
2.0.0,Add task-specific bypass layers
2.0.0,Add the input features.
2.0.0,Add the dense layers
2.0.0,Compute the loss function for each label.
2.0.0,"Results is of shape (n_samples, n_tasks, n_classes)"
2.0.0,"retval is of shape (n_samples, n_tasks)"
2.0.0,Add the input features.
2.0.0,Add the dense layers
2.0.0,Compute the loss function for each label.
2.0.0,Run fit transformers on dummy dataset to determine n_features after transformation
2.0.0,Similarity values
2.0.0,Labels for all top K similar samples
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
2.0.0,and embeddings of atom j(both gone through a hidden layer)
2.0.0,"for atom i, sum the influence from all other atom j in the molecule"
2.0.0,number of inputs each step
2.0.0,Add trainable weights
2.0.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
2.0.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
2.0.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
2.0.0,initialize graph features for each graph
2.0.0,initialize graph features for each graph
2.0.0,another row of zeros is generated for padded dummy atoms
2.0.0,`count`-th step
2.0.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
2.0.0,generating index for graph features used in the inputs
2.0.0,"extracting graph features for parents of the target atoms, then flatten"
2.0.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
2.0.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
2.0.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
2.0.0,of shape: (batch_size*max_atoms) * n_graph_features
2.0.0,representing the graph features of target atoms in each graph
2.0.0,index for targe atoms
2.0.0,update the graph features for target atoms
2.0.0,Add trainable weights
2.0.0,Extract atom_features
2.0.0,Extract atom_features
2.0.0,sum all graph outputs
2.0.0,"Default message function: edge network, update function: GRU"
2.0.0,more options to be implemented
2.0.0,Extract atom_features
2.0.0,Add trainable weights
2.0.0,Extract atom_features
2.0.0,Add another value(~-Inf) to prevent error in softmax
2.0.0,Model using this layer must set pad_batches=True
2.0.0,Perform one step of LSTM
2.0.0,Arguments
2.0.0,Aliases.
2.0.0,Layer Management
2.0.0,Singular place to hold Tensor objects which don't serialize
2.0.0,These have to be reconstructed on restoring from pickle
2.0.0,See TensorGraph._get_tf() for more details on lazy construction
2.0.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.0.0,"we try to read more batches than the total number that get queued,"
2.0.0,this thread will hang indefinitely.
2.0.0,Gather results for each output
2.0.0,"Don't let this thread get ahead of the enqueue thread, since if"
2.0.0,"we try to read more batches than the total number that get queued,"
2.0.0,this thread will hang indefinitely.
2.0.0,"If only one output, just return array"
2.0.0,Ensure all training operators have been created.
2.0.0,Initialize variables.
2.0.0,"As a sanity check, make sure all tensors have the correct shape."
2.0.0,Remove out_tensor from the object to be pickled
2.0.0,Pickle itself
2.0.0,add out_tensor back to everyone
2.0.0,The loss doesn't depend on any variables.
2.0.0,Should we keep a separate global step count for each submodel?
2.0.0,Add the input features.
2.0.0,Weight decay not activated
2.0.0,Handle output layer
2.0.0,Iterate over all previous tasks.
2.0.0,prev_layers is a list with elements of size
2.0.0,"(batch_size, layer_sizes[i-1])"
2.0.0,"Results is of shape (n_samples, n_tasks, 1)"
2.0.0,Set by variable constructor.
2.0.0,Set by set_variable_initial_values().
2.0.0,Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
2.0.0,Optimize the main loss.  This should send both variables toward 1.
2.0.0,Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
2.0.0,See if it has done a plausible job of learning the distribution.
2.0.0,See if it has done a plausible job of learning the distribution.
2.0.0,We have to set the gradient penalty very small because the generator's
2.0.0,"output is only a single number, so the default penalty would constrain"
2.0.0,it far too much.
2.0.0,See if it has done a plausible job of learning the distribution.
2.0.0,"This isn't a meaningful loss, but just for test"
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,1 and 2 are nbrs. 8 and 9 are nbrs
2.0.0,"Now an (N, M) shape"
2.0.0,TODO(rbharath): Move this into a model directly
2.0.0,def test_vina(self):
2.0.0,"""""""Test that vina graph can be constructed in TensorGraph."""""""
2.0.0,N_protein = 4
2.0.0,N_ligand = 1
2.0.0,N_atoms = 5
2.0.0,M_nbrs = 2
2.0.0,ndim = 3
2.0.0,start = 0
2.0.0,stop = 4
2.0.0,nbr_cutoff = 1
2.0.0,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
2.0.0,start))
2.0.0,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
2.0.0,start))
2.0.0,y = NumpyDataset(np.random.rand(
2.0.0,"1,))"
2.0.0,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
2.0.0,"# used in the current implementation. This is obviously wrong, but need"
2.0.0,# to dig out why this is happening.
2.0.0,"prot_coords = Feature(shape=(N_protein, ndim))"
2.0.0,"ligand_coords = Feature(shape=(N_ligand, ndim))"
2.0.0,"labels = Label(shape=(1,))"
2.0.0,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
2.0.0,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
2.0.0,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
2.0.0,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
2.0.0,"# Now an (N, M) shape"
2.0.0,nbr_list = NeighborList(
2.0.0,"N_protein + N_ligand,"
2.0.0,"M_nbrs,"
2.0.0,"ndim,"
2.0.0,"nbr_cutoff,"
2.0.0,"start,"
2.0.0,"stop,"
2.0.0,in_layers=[coords])
2.0.0,"# Shape (N, M)"
2.0.0,dists = InteratomicL2Distances(
2.0.0,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
2.0.0,repulsion = VinaRepulsion(in_layers=[dists])
2.0.0,hydrophobic = VinaHydrophobic(in_layers=[dists])
2.0.0,hbond = VinaHydrogenBond(in_layers=[dists])
2.0.0,gauss_1 = VinaGaussianFirst(in_layers=[dists])
2.0.0,gauss_2 = VinaGaussianSecond(in_layers=[dists])
2.0.0,"# Shape (N, M)"
2.0.0,interactions = WeightedLinearCombo(
2.0.0,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
2.0.0,"# Shape (N, M)"
2.0.0,"thresholded = Cutoff(in_layers=[dists, interactions])"
2.0.0,"# Shape (N, M)"
2.0.0,free_energies = VinaNonlinearity(in_layers=[thresholded])
2.0.0,free_energy = ReduceSum(in_layers=[free_energies])
2.0.0,"loss = L2Loss(in_layers=[free_energy, labels])"
2.0.0,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
2.0.0,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
2.0.0,tg.set_loss(loss)
2.0.0,tg.fit_generator(databag.iterbatches(epochs=1))
2.0.0,TODO(rbharath): This test should pass. Fix it!
2.0.0,def test_graph_pool(self):
2.0.0,"""""""Test that GraphPool can be invoked."""""""
2.0.0,out_channels = 2
2.0.0,"n_atoms = 4 # In CCC and C, there are 4 atoms"
2.0.0,"raw_smiles = ['CCC', 'C']"
2.0.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
2.0.0,featurizer = ConvMolFeaturizer()
2.0.0,mols = featurizer.featurize(mols)
2.0.0,multi_mol = ConvMol.agglomerate_mols(mols)
2.0.0,atom_features = multi_mol.get_atom_features()
2.0.0,degree_slice = multi_mol.deg_slice
2.0.0,membership = multi_mol.membership
2.0.0,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
2.0.0,with self.test_session() as sess:
2.0.0,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
2.0.0,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
2.0.0,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
2.0.0,deg_adjs_tf = []
2.0.0,for deg_adj in deg_adjs:
2.0.0,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
2.0.0,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
2.0.0,out_tensor = GraphPool(out_channels)(*args)
2.0.0,sess.run(tf.global_variables_initializer())
2.0.0,out_tensor = out_tensor.eval()
2.0.0,"assert out_tensor.shape == (n_atoms, out_channels)"
2.0.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
2.0.0,Should be able to call fit twice without failure.
2.0.0,# TODO(rbharath): Transform these into useful weights.
2.0.0,#class_weight={
2.0.0,"#    True: num_sequences / num_positives,"
2.0.0,#    False: num_sequences / num_negatives
2.0.0,"#} if not multitask else None,"
2.0.0,# TODO(rbharath): Add a test with per-class weighting.
2.0.0,#class_weight={
2.0.0,"#    True: num_sequences / num_positives,"
2.0.0,#    False: num_sequences / num_negatives
2.0.0,"#} if not multitask else None,"
2.0.0,Train the model on random sequences.  We aren't training long enough to
2.0.0,"really make it reliable, but I want to keep this test fast, and it should"
2.0.0,still be able to reproduce a reasonable fraction of input sequences.
2.0.0,Test it out.
2.0.0,Check that it got at least a quarter of them correct.
2.0.0,Test it out.
2.0.0,Actually training a VAE takes far too long for a unit test.  Just run a
2.0.0,"few steps of training to make sure nothing crashes, then check that the"
2.0.0,results are at least internally consistent.
2.0.0,use central difference since forward difference has a pretty high
2.0.0,approximation error
2.0.0,assert min_coords[1][0] != new_x[3]
2.0.0,assert min_coords[1][1] != new_x[4]
2.0.0,assert min_coords[1][2] != new_x[5]
2.0.0,Create the inputs.
2.0.0,Create the generators.
2.0.0,Create the discriminators.
2.0.0,Make a copy of the discriminator that takes each generator's output as
2.0.0,its input.
2.0.0,Make a list of all layers in the generators and discriminators.
2.0.0,Compute the loss functions.
2.0.0,Create learnable weights for the generators and discriminators.
2.0.0,Compute the weighted errors
2.0.0,Add an entropy term to the loss.
2.0.0,Create submodels for training the generators and discriminators.
2.0.0,"Every call to fit_generator() will increment global_step, but we only"
2.0.0,"want it to get incremented once for the entire batch, so record the"
2.0.0,value and keep resetting it.
2.0.0,Train the discriminator.
2.0.0,Train the generator.
2.0.0,Write checkpoints and report progress.
2.0.0,Write out final results.
2.0.0,number of atoms in each molecule
2.0.0,index of pair features
2.0.0,number of pairs for each atom
2.0.0,atom features
2.0.0,pair features
2.0.0,calculation orders for a batch of molecules
2.0.0,padding atom features vector of each molecule with 0
2.0.0,Gather results for each output
2.0.0,Recording the number of samples in the input batch
2.0.0,GraphConvTensorGraph constantly outputs batch_size number of
2.0.0,"results, only valid samples should be appended to final results"
2.0.0,"If only one output, just return array"
2.0.0,Returns:
2.0.0,Concatenates along 0-th dimension
2.0.0,Returns:
2.0.0,Build placeholders
2.0.0,w_b act as the indicator of unique samples in the batch
2.0.0,number of atoms in each molecule
2.0.0,index of pair features
2.0.0,number of pairs for each atom
2.0.0,atom features
2.0.0,pair features
2.0.0,MPNN only accept padded input
2.0.0,MPNN only accept padded input
2.0.0,Extract number of unique samples in the batch from w_b
2.0.0,Only fetch the first set of unique samples
2.0.0,import tensorflow as tf
2.0.0,from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
2.0.0,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
2.0.0,
2.0.0,
2.0.0,class WeightedError(Layer):
2.0.0,
2.0.0,"def __call__(self, *parents):"
2.0.0,"entropy, weights = parents[0], parents[1]"
2.0.0,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
2.0.0,return self.out_tensor
2.0.0,
2.0.0,
2.0.0,"def MultiTaskClassifier(n_tasks,"
2.0.0,"n_features,"
2.0.0,"layer_sizes=[500],"
2.0.0,"bypass_layer_sizes=[100],"
2.0.0,model_dir=None):
2.0.0,""""""""
2.0.0,TODO(LESWING) Add Dropout and regularization
2.0.0,
2.0.0,Parameters
2.0.0,----------
2.0.0,n_tasks
2.0.0,n_features
2.0.0,layer_sizes
2.0.0,bypass_layer_sizes
2.0.0,model_dir
2.0.0,
2.0.0,Returns
2.0.0,-------
2.0.0,
2.0.0,""""""""
2.0.0,g = MultiTaskTensorGraph(model_dir=model_dir)
2.0.0,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
2.0.0,g.add_layer(in_layer)
2.0.0,g.add_feature(in_layer)
2.0.0,
2.0.0,# Shared Dense Layers
2.0.0,prev_layer = in_layer
2.0.0,dense_layers = []
2.0.0,for i in range(len(layer_sizes)):
2.0.0,dense = Dense(
2.0.0,"out_channels=layer_sizes[i],"
2.0.0,"name=""SDENSE%s"" % i,"
2.0.0,activation_fn=tf.nn.relu)
2.0.0,"g.add_layer(dense, parents=[prev_layer])"
2.0.0,dense_layers.append(dense)
2.0.0,prev_layer = dense
2.0.0,
2.0.0,# Individual Bypass Layers
2.0.0,costs = []
2.0.0,for task in range(n_tasks):
2.0.0,prev_layer = in_layer
2.0.0,for i in range(len(bypass_layer_sizes)):
2.0.0,dense = Dense(
2.0.0,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
2.0.0,"g.add_layer(dense, parents=[prev_layer])"
2.0.0,prev_layer = dense
2.0.0,"joined_layer = Concat(name=""JOIN%s"" % task)"
2.0.0,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
2.0.0,
2.0.0,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
2.0.0,"g.add_layer(classification, parents=[joined_layer])"
2.0.0,
2.0.0,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
2.0.0,"g.add_layer(softmax, parents=[classification])"
2.0.0,g.add_output(softmax)
2.0.0,
2.0.0,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
2.0.0,g.add_layer(label)
2.0.0,g.add_label(label)
2.0.0,
2.0.0,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
2.0.0,"g.add_layer(cost, parents=[label, classification])"
2.0.0,costs.append(cost)
2.0.0,
2.0.0,"entropy = Concat(name=""ENT"")"
2.0.0,"g.add_layer(entropy, parents=costs)"
2.0.0,
2.0.0,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
2.0.0,g.add_layer(task_weights)
2.0.0,g.set_task_weights(task_weights)
2.0.0,
2.0.0,"loss = WeightedError(name=""ERROR"")"
2.0.0,"g.add_layer(loss, parents=[entropy, task_weights])"
2.0.0,g.set_loss(loss)
2.0.0,
2.0.0,return g
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,(ytz): this is really dirty but needed for restoring models
2.0.0,TODO(rbharath): This model only supports one-conv layer. Extend
2.0.0,so that conv layers of greater depth can be implemented.
2.0.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
2.0.0,SMILES strings
2.0.0,Maximum length is expanded to allow length variation during train and inference
2.0.0,'_' served as delimiter and padding
2.0.0,Initialize common characters as keys
2.0.0,Include space to avoid extra keys
2.0.0,"For 'Cl', 'Br', etc."
2.0.0,"Character not recognized, add to extra_keys"
2.0.0,Add all extra_keys to char_dict
2.0.0,Character embedding
2.0.0,Multiple convolutional layers with different filter widths
2.0.0,Max-over-time pooling
2.0.0,Concat features from all filters(one feature per filter)
2.0.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
2.0.0,Transform SMILES string to integer vectors
2.0.0,Skip all spaces
2.0.0,"For 'Cl', 'Br', etc."
2.0.0,Padding with '_'
2.0.0,Do a simple greedy search.
2.0.0,Do a beam search with length normalization.
2.0.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
2.0.0,This candidate sequence has already been terminated
2.0.0,Consider all possible tokens we could add to this candidate sequence.
2.0.0,update model with best param
2.0.0,Find optimal n_estimators based on original learning_rate
2.0.0,and early_stopping_rounds
2.0.0,"Since test size is 20%, when retrain model to whole data, expect"
2.0.0,n_estimator increased to 1/0.8 = 1.25 time.
2.0.0,Make sure user specified params are in the grid.
2.0.0,Change params back original params
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Check same predictions are made.
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Load trained model
2.0.0,Eval model on train
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Fit trained model
2.0.0,Eval model on train/test
2.0.0,Fit trained model
2.0.0,Eval model on train/test
2.0.0,Test Parameter getting and setting
2.0.0,Fit trained model
2.0.0,Eval model on train/test
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,n_samples = 100
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Load mini log-solubility dataset.
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Load mini log-solubility dataset.
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Load mini log-solubility dataset.
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Load mini log-solubility dataset.
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Load mini log-solubility dataset.
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on train
2.0.0,def test_singletask_to_multitask_classification(self):
2.0.0,n_features = 10
2.0.0,n_tasks = 17
2.0.0,tasks = range(n_tasks)
2.0.0,# Define train dataset
2.0.0,n_train = 100
2.0.0,"X_train = np.random.rand(n_train, n_features)"
2.0.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
2.0.0,w_train = np.ones_like(y_train)
2.0.0,"ids_train = [""C""] * n_train"
2.0.0,train_dataset = dc.data.DiskDataset.from_numpy(
2.0.0,"X_train, y_train, w_train, ids_train)"
2.0.0,# Define test dataset
2.0.0,n_test = 10
2.0.0,"X_test = np.random.rand(n_test, n_features)"
2.0.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
2.0.0,w_test = np.ones_like(y_test)
2.0.0,"ids_test = [""C""] * n_test"
2.0.0,test_dataset = dc.data.DiskDataset.from_numpy(
2.0.0,"X_test, y_test, w_test, ids_test)"
2.0.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
2.0.0,def model_builder(model_dir):
2.0.0,sklearn_model = LogisticRegression()
2.0.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.0.0,multitask_model = dc.models.SingletaskToMultitask(
2.0.0,"tasks, model_builder)"
2.0.0,# Fit trained model
2.0.0,multitask_model.fit(train_dataset)
2.0.0,multitask_model.save()
2.0.0,# Eval multitask_model on train/test
2.0.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
2.0.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
2.0.0,Generate data
2.0.0,Cleanup
2.0.0,Generate dummy dataset
2.0.0,Fit trained model
2.0.0,Eval model on test
2.0.0,Eval model on train
2.0.0,Fit trained model
2.0.0,Eval model on test
2.0.0,Fit trained model
2.0.0,Eval model on test
2.0.0,def test_sklearn_classification(self):
2.0.0,"""""""Test that sklearn models can learn on simple classification datasets."""""""
2.0.0,np.random.seed(123)
2.0.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.0.0,"X, y = dataset.data, dataset.target"
2.0.0,frac_train = .7
2.0.0,n_samples = len(X)
2.0.0,n_train = int(frac_train*n_samples)
2.0.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.0.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.0.0,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
2.0.0,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
2.0.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.0.0,sklearn_model = LogisticRegression()
2.0.0,model = dc.models.SklearnModel(sklearn_model)
2.0.0,# Fit trained model
2.0.0,model.fit(train_dataset)
2.0.0,model.save()
2.0.0,# Eval model on test
2.0.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.0.0,assert scores[classification_metric.name] > .5
2.0.0,def test_sklearn_multitask_classification(self):
2.0.0,"""""""Test that sklearn models can learn on simple multitask classification."""""""
2.0.0,np.random.seed(123)
2.0.0,n_tasks = 4
2.0.0,tasks = range(n_tasks)
2.0.0,dataset = sklearn.datasets.load_digits(n_class=2)
2.0.0,"X, y = dataset.data, dataset.target"
2.0.0,"y = np.reshape(y, (len(y), 1))"
2.0.0,y = np.hstack([y] * n_tasks)
2.0.0,
2.0.0,frac_train = .7
2.0.0,n_samples = len(X)
2.0.0,n_train = int(frac_train*n_samples)
2.0.0,"X_train, y_train = X[:n_train], y[:n_train]"
2.0.0,"X_test, y_test = X[n_train:], y[n_train:]"
2.0.0,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
2.0.0,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
2.0.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
2.0.0,def model_builder(model_dir):
2.0.0,sklearn_model = LogisticRegression()
2.0.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
2.0.0,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
2.0.0,# Fit trained model
2.0.0,model.fit(train_dataset)
2.0.0,model.save()
2.0.0,# Eval model on test
2.0.0,"scores = model.evaluate(test_dataset, [classification_metric])"
2.0.0,for score in scores[classification_metric.name]:
2.0.0,assert score > .5
2.0.0,Set early stopping round = n_estimators so that esr won't work
2.0.0,Fit trained model
2.0.0,Eval model on test
2.0.0,Fit trained model
2.0.0,Eval model on test
2.0.0,Fit trained model
2.0.0,Eval model on test
2.0.0,Logistic regression doesn't support weights
2.0.0,-*- coding: utf-8 -*-
2.0.0,Assigning featurizer if not user defined
2.0.0,loading datasets
2.0.0,Assembling train and valid datasets
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,Building tensorflow MultiTaskDNN model
2.0.0,Building tensorflow robust MultiTaskDNN model
2.0.0,Building scikit logistic regression model
2.0.0,Transform fingerprints to IRV features
2.0.0,Building tensorflow IRV model
2.0.0,Building scikit random forest model
2.0.0,Building scikit learn Kernel SVM model
2.0.0,Building xgboost classification model
2.0.0,Remove token for paddings
2.0.0,Building scikit random forest model
2.0.0,Building scikit learn Kernel Ridge Regression model
2.0.0,Building scikit learn Kernel Ridge Regression model
2.0.0,Building xgboost regression model
2.0.0,Loading hyperparameters
2.0.0,num positive/negative ligands
2.0.0,Set batch sizes for network
2.0.0,Model structure
2.0.0,Traning settings
2.0.0,Fit trained model
2.0.0,Evaluating low data model
2.0.0,-*- coding: utf-8 -*-
2.0.0,Assigning featurizer if not user defined
2.0.0,loading datasets
2.0.0,
2.0.0,Note by @XericZephyr. Reason why I spun off this function:
2.0.0,1. Some model needs dataset information.
2.0.0,2. It offers us possibility to **cache** the dataset
2.0.0,"if the featurizer runs very slow, e.g., GraphConv."
2.0.0,2+. The cache can even happen at Travis CI to accelerate
2.0.0,CI testing.
2.0.0,
2.0.0,loading datasets
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,from deepchem.molnet.run_benchmark_low_data import run_benchmark_low_data
2.0.0,Featurize qm9 dataset
2.0.0,transformers = [
2.0.0,"deepchem.trans.LogTransformer(transform_X=True),"
2.0.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
2.0.0,dataset=train_dataset)]
2.0.0,Set shard size low to avoid memory problems.
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Set some global variables up top
2.0.0,Featurize KAGGLE dataset
2.0.0,############################################################# TIMING
2.0.0,############################################################# TIMING
2.0.0,Featurize qm7 dataset
2.0.0,Featurize clintox dataset
2.0.0,Transform clintox dataset
2.0.0,Split clintox dataset
2.0.0,Featurize bbb dataset
2.0.0,Initialize transformers
2.0.0,Load nci dataset
2.0.0,Featurize nci dataset
2.0.0,Initialize transformers
2.0.0,Featurize HOPV dataset
2.0.0,Initialize transformers
2.0.0,Featurize PPB dataset
2.0.0,Initialize transformers
2.0.0,Load MUV dataset
2.0.0,Featurize MUV dataset
2.0.0,Initialize transformers
2.0.0,Featurize clearance dataset
2.0.0,Initialize transformers
2.0.0,Featurize TOXCAST dataset
2.0.0,Initialize transformers
2.0.0,Featurize bace dataset
2.0.0,Initialize transformers
2.0.0,Featurize bace dataset
2.0.0,Initialize transformers
2.0.0,Featurize Tox21 dataset
2.0.0,Initialize transformers
2.0.0,Featurize ChEMBL dataset
2.0.0,Initialize transformers
2.0.0,Featurize hiv dataset
2.0.0,Initialize transformers
2.0.0,Featurize SIDER dataset
2.0.0,Initialize transformers
2.0.0,Featurize SAMPL dataset
2.0.0,Initialize transformers
2.0.0,Featurize Delaney dataset
2.0.0,Initialize transformers
2.0.0,Featurize PCBA dataset
2.0.0,Initialize transformers
2.0.0,Featurize Lipophilicity dataset
2.0.0,Initialize transformers
2.0.0,"Float or int hyper parameters(ex. batch_size, learning_rate)"
2.0.0,List of float or int hyper parameters(ex. layer_sizes)
2.0.0,Number of parameters
2.0.0,Range of optimization
2.0.0,Dummy names
2.0.0,Input hyper parameters
2.0.0,Run benchmark
2.0.0,Record hyperparameters
2.0.0,Record performances
2.0.0,"GPGO maximize performance by default, set performance to its negative value for minimization"
2.0.0,Readout best hyper parameters
2.0.0,Compare best model to default hyperparameters
2.0.0,Record hyperparameters
2.0.0,Record performances
2.0.0,"Optimized model is better, return hyperparameters"
2.0.0,Return default hyperparameters
2.0.0,!/usr/bin/env python2
2.0.0,-*- coding: utf-8 -*-
2.0.0,TODO(rbharath): This function is complicated and monolithic. Is there a nice
2.0.0,way to refactor this?
2.0.0,arbitrarily return last model
2.0.0,Define train dataset
2.0.0,Define validation dataset
2.0.0,Have the worker threads generate the rollouts for this iteration.
2.0.0,Perform optimization.
2.0.0,Build the feed dict and run the optimizer.
2.0.0,Update the number of steps taken so far and perform checkpointing.
2.0.0,Merge all the rollouts into a single set of arrays.
2.0.0,Iterate slices.
2.0.0,Generate the rollout.
2.0.0,Compute an estimate of the reward for the rest of the episode.
2.0.0,Compute the discounted rewards and advantages.
2.0.0,Convert the actions to one-hot.
2.0.0,Rearrange the states into the proper set of arrays.
2.0.0,Return the processed arrays.
2.0.0,Generate the rollout.
2.0.0,Compute an estimate of the reward for the rest of the episode.
2.0.0,Compute the discounted rewards and advantages.
2.0.0,"Record the actions, converting to one-hot if necessary."
2.0.0,Rearrange the states into the proper set of arrays.
2.0.0,Build the feed dict and apply gradients.
2.0.0,Run the algorithm.
2.0.0,Save a file checkpoint.
2.0.0,Build the tree.
2.0.0,Compute the final probabilities and expected reward.
2.0.0,Mark this node as terminal
2.0.0,Expand this node.
2.0.0,Select the next action to perform.
2.0.0,Recursively build the tree.
2.0.0,Update statistics for this node.
2.0.0,Assume all arrays are float32.
2.0.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.0.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.0.0,"game).  The average reward for any bet is slightly negative, so the best"
2.0.0,strategy is to walk away.
2.0.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.0.0,Optimize it.
2.0.0,"It should have learned that the expected value is very close to zero, and that the best"
2.0.0,action is to walk away.
2.0.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
2.0.0,get the same result.
2.0.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.0.0,The environment just has a constant state.
2.0.0,The policy includes a single recurrent layer.
2.0.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.0.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.0.0,"On the first call, the initial state should be all zeros."
2.0.0,It should still be zeros since we didn't save it last time.
2.0.0,It should be different now.
2.0.0,This should be the same as the previous one.
2.0.0,"Now we reset it, so we should get the same result as initially."
2.0.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.0.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.0.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.0.0,at all.  Using hindsight makes it much easier.
2.0.0,A simple policy with two hidden layers.
2.0.0,Optimize it.
2.0.0,Try running it a few times and see if it succeeds.
2.0.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.0.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.0.0,"game).  The average reward for any bet is slightly negative, so the best"
2.0.0,strategy is to walk away.
2.0.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.0.0,Optimize it.
2.0.0,"It should have learned that the expected value is very close to zero, and that the best"
2.0.0,action is to walk away.
2.0.0,"Verify that we can create a new A3C object, reload the parameters from the first one, and"
2.0.0,get the same result.
2.0.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.0.0,The environment just has a constant state.
2.0.0,The policy includes a single recurrent layer.
2.0.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
2.0.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
2.0.0,"On the first call, the initial state should be all zeros."
2.0.0,It should still be zeros since we didn't save it last time.
2.0.0,It should be different now.
2.0.0,This should be the same as the previous one.
2.0.0,"Now we reset it, so we should get the same result as initially."
2.0.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
2.0.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
2.0.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
2.0.0,at all.  Using hindsight makes it much easier.
2.0.0,A simple policy with two hidden layers.
2.0.0,Optimize it.
2.0.0,Try running it a few times and see if it succeeds.
2.0.0,The state consists of two numbers: a current value and a target value.
2.0.0,The policy just needs to learn to output the target value (or at least
2.0.0,move toward it).
2.0.0,A simple policy with no hidden layers.
2.0.0,Optimize it.
2.0.0,Try running it and see if it reaches the target
2.0.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
2.0.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
2.0.0,"game).  The average reward for any bet is slightly negative, so the best"
2.0.0,strategy is to walk away.
2.0.0,"This policy just learns a constant probability for each action, and a constant for the value."
2.0.0,Optimize it.
2.0.0,"It should have learned that the expected value is very close to zero, and that the best"
2.0.0,action is to walk away.
2.0.0,"Verify that we can create a new MCTS object, reload the parameters from the first one, and"
2.0.0,get the same result.
2.0.0,"Do the same thing, only using the ""restore"" argument to fit()."
2.0.0,Randomize who goes first
2.0.0,Illegal move -- the square is not empty
2.0.0,Move X
2.0.0,Did X Win
2.0.0,Did O Win
2.0.0,TODO (Bowen): make this function less memory intensive
2.0.0,set 1st column as the column index of dataframe
2.0.0,merge descriptor and activities dataframe into output dataframe based on
2.0.0,"the molecule name, which is the index for both dataframes (but named"
2.0.0,differently). Default merge is inner merge
2.0.0,need to manually set dataframe indexname after merge based on index
2.0.0,from deepchem.scripts.dock_dude import *
2.0.0,from ipyparallel import Client
2.0.0,rc = Client()
2.0.0,dview = rc[:]
2.0.0,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
2.0.0,
2.0.0,"If mol_id is not set, then use isomeric smiles as unique identifier"
2.0.0,iterator = data_df.iterrows()
2.0.0,TODO(rbharath): BROKEN!
2.0.0,Trim unwanted indexing fields
2.0.0,Connect to running ipython server
2.0.0,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
2.0.0,
2.0.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
2.0.0,you may not use this file except in compliance with the License.
2.0.0,You may obtain a copy of the License at
2.0.0,
2.0.0,http://www.apache.org/licenses/LICENSE-2.0
2.0.0,
2.0.0,"Unless required by applicable law or agreed to in writing, software"
2.0.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
2.0.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
2.0.0,See the License for the specific language governing permissions and
2.0.0,limitations under the License.
2.0.0,==============================================================================
2.0.0,Maps from a function name to a dictionary that describes how to
2.0.0,map from an old argument keyword to the new argument keyword.
2.0.0,Mapping from function to the new name of the function
2.0.0,Functions that were reordered should be changed to the new keyword args
2.0.0,"for safety, if positional arguments are used. If you have reversed the"
2.0.0,"positional arguments yourself, this could do the wrong thing."
2.0.0,Specially handled functions.
2.0.0,TODO(aselle): Could check for a literal list of bools and try to convert
2.0.0,them to indices.
2.0.0,all edits are lists of chars
2.0.0,Iterate of each line
2.0.0,sort by column so that edits are processed in order in order to make
2.0.0,indexing adjustments cumulative for changes that change the string
2.0.0,length
2.0.0,"Extract each line to a list of characters, because mutable lists"
2.0.0,"are editable, unlike immutable strings."
2.0.0,Record a description of the change
2.0.0,Make underscore buffers for underlining where in the line the edit was
2.0.0,Iterate for each edit
2.0.0,"Create effective start, end by accounting for change in length due"
2.0.0,to previous edits
2.0.0,Make sure the edit is changing what it should be changing
2.0.0,Make the edit
2.0.0,Create the underline highlighting of the before and after
2.0.0,Keep track of how to generate effective ranges
2.0.0,Finish the report comment
2.0.0,"Strangely, ast.ListComp returns the col_offset of the first token"
2.0.0,after the '[' token which appears to be a bug. Workaround by
2.0.0,explicitly finding the real start of the list comprehension.
2.0.0,loop over lines
2.0.0,Reverse the text to and regular expression search for whitespace
2.0.0,First find if a [ can be found with only whitespace between it and
2.0.0,col.
2.0.0,TODO(aselle):
2.0.0,"this is poor comment detection, but it is good enough for"
2.0.0,cases where the comment does not contain string literal starting/
2.0.0,ending characters. If ast gave us start and end locations of the
2.0.0,"ast nodes rather than just start, we could use string literal"
2.0.0,node ranges to filter out spurious #'s that appear in string
2.0.0,literals.
2.0.0,"Most other nodes return proper locations (with notably does not), but"
2.0.0,it is not possible to use that in an argument.
2.0.0,"Find a simple attribute name path e.g. ""tf.foo.bar"""
2.0.0,Make sure the func is marked as being part of a call
2.0.0,Call special handlers
2.0.0,Examine any non-keyword argument and make it into a keyword argument
2.0.0,if reordering required.
2.0.0,Examine each keyword argument and convert it to the final renamed form
2.0.0,TODO(aselle): We should scan backward to find the start of the
2.0.0,keyword key. Unfortunately ast does not give you the location of
2.0.0,"keyword keys, so we are forced to infer it from the keyword arg"
2.0.0,value.
2.0.0,"Write to a temporary file, just in case we are doing an implace modify."
2.0.0,Broad exceptions are required here because ast throws whatever it wants.
2.0.0,pylint: disable=broad-except
2.0.0,pylint: enable=broad-except
2.0.0,make sure output directory doesn't exist
2.0.0,make sure output directory does not overlap with root_directory
2.0.0,Collect list of files to process (we do this to correctly handle if the
2.0.0,user puts the output directory in some sub directory of the input dir)
2.0.0,import os
2.0.0,"from deepchem.utils.save import load_from_disk, save_to_disk"
2.0.0,from deepchem.featurizers.fingerprints import CircularFingerprint
2.0.0,from deepchem.featurizers.basic import RDKitDescriptors
2.0.0,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
2.0.0,from deepchem.featurizers.grid_featurizer import GridFeaturizer
2.0.0,from deepchem.featurizers.featurize import DataLoader
2.0.0,
2.0.0,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
2.0.0,"print(""About to load dataset form disk."")"
2.0.0,dataset = load_from_disk(dataset_file)
2.0.0,"print(""Loaded dataset."")"
2.0.0,
2.0.0,grid_featurizer = GridFeaturizer(
2.0.0,"voxel_width=16.0, feature_types=""voxel_combined"","
2.0.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
2.0.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
2.0.0,"parallel=True, flatten=True)"
2.0.0,featurizers = [CircularFingerprint(size=1024)]
2.0.0,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
2.0.0,
2.0.0,#Make a directory in which to store the featurized complexes.
2.0.0,"base_dir = ""../../../grid_nnscore_circular_features"""
2.0.0,if not os.path.exists(base_dir):
2.0.0,os.makedirs(base_dir)
2.0.0,"data_dir = os.path.join(base_dir, ""data"")"
2.0.0,if not os.path.exists(data_dir):
2.0.0,os.makedirs(data_dir)
2.0.0,
2.0.0,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
2.0.0,
2.0.0,"feature_dir = os.path.join(base_dir, ""features"")"
2.0.0,if not os.path.exists(feature_dir):
2.0.0,os.makedirs(feature_dir)
2.0.0,
2.0.0,"samples_dir = os.path.join(base_dir, ""samples"")"
2.0.0,if not os.path.exists(samples_dir):
2.0.0,os.makedirs(samples_dir)
2.0.0,
2.0.0,
2.0.0,
2.0.0,featurizers = compound_featurizers + complex_featurizers
2.0.0,"featurizer = DataLoader(tasks=[""label""],"
2.0.0,"smiles_field=""smiles"","
2.0.0,"protein_pdb_field=""protein_pdb"","
2.0.0,"ligand_pdb_field=""ligand_pdb"","
2.0.0,"compound_featurizers=compound_featurizers,"
2.0.0,"complex_featurizers=complex_featurizers,"
2.0.0,"id_field=""complex_id"","
2.0.0,verbose=False)
2.0.0,from ipyparallel import Client
2.0.0,c = Client()
2.0.0,"print(""c.ids"")"
2.0.0,print(c.ids)
2.0.0,dview = c[:]
2.0.0,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
2.0.0,"worker_pool=dview, shard_size=1024)"
2.0.0,
2.0.0,"save_to_disk(featurized_samples, featurized_samples_file)"
2.0.0,"print(""Preparing ligand %s"" % mol_name)"
1.3.1,!/usr/bin/env python3
1.3.1,-*- coding: utf-8 -*-
1.3.1,Datasets and models used in the benchmark test
1.3.1,"irv, rf, rf_regression should be assigned manually"
1.3.1,Evaluate performances with different training set fraction
1.3.1,Datasets and models used in the benchmark test
1.3.1,Uncomment the two lines below if hyper_parameters are provided
1.3.1,"with open(os.path.join(out_path, dataset + model + '.pkl'), 'r') as f:"
1.3.1,hyper_parameters = pickle.load(f)
1.3.1,Will raise a CalledProcessError if fails.
1.3.1,!/usr/bin/env python3
1.3.1,-*- coding: utf-8 -*-
1.3.1,Datasets and models used in the benchmark test
1.3.1,Set numpy seed
1.3.1,##Load data###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Featurize Kinase dataset
1.3.1,##Load data###
1.3.1,num_trials = 5
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,Force matplotlib to not use any Xwindows backend.
1.3.1,##Load data###
1.3.1,the histogram of the data
1.3.1,Set numpy seed
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,num_trials = 5
1.3.1,Set some global variables up top
1.3.1,Fit trained model
1.3.1,Featurize PCBA dataset
1.3.1,Initialize transformers
1.3.1,Fit trained model
1.3.1,Load SWEET dataset
1.3.1,Featurize SWEET dataset
1.3.1,Initialize transformers
1.3.1,Set some global variables up top
1.3.1,removes directory if present -- warning
1.3.1,default split is 80-10-10 train-valid-test split
1.3.1,Fit Logistic Regression models
1.3.1,Fit Logistic Regression models
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,transformers = [
1.3.1,"dc.trans.LogTransformer(transform_X=True),"
1.3.1,"dc.trans.NormalizationTransformer(transform_y=True,"
1.3.1,dataset=train_dataset)]
1.3.1,Set shard size low to avoid memory problems.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Set some global variables up top
1.3.1,Featurize KAGGLE dataset
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,##Load data###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,##Load data###
1.3.1,"n_estimators=100, max_features=int(num_features/3),"
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,Featurize qm9 dataset
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Gather Projection
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Load tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Featurize Tox21 dataset
1.3.1,Initialize transformers
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Load tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Featurize FACTORS dataset
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,Force matplotlib to not use any Xwindows backend.
1.3.1,##Load data###
1.3.1,the histogram of the data
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Featurize qm7 dataset
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Batch size of models
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Batch size of models
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Batch size of models
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load QM8 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Featurize qm8 dataset
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Fit trained model
1.3.1,Set numpy seed
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,Set shard size low to avoid memory problems.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Set some global variables up top
1.3.1,Load dataset
1.3.1,Featurize ChEMBL dataset
1.3.1,Initialize transformers
1.3.1,Load ChEMBL dataset
1.3.1,Fit models
1.3.1,Do setup required for tf/keras models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Gather Projection
1.3.1,Fit trained model
1.3.1,DeepCrystal Technologies 2017 - Patrick Hop
1.3.1,MIT License - have fun!!
1.3.1,Set to higher values to get better numbers
1.3.1,======================================================================
1.3.1,"Run Benchmarks {GC-DNN, SVR, RF}"
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,Only for debug!
1.3.1,Load Delaney dataset
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Do setup required for tf/keras models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
1.3.1,Fit trained model
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Do setup required for tf/keras models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Gather Projection
1.3.1,Dense post-processing layer
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Featurize Delaney dataset
1.3.1,Initialize transformers
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load Delaney dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load MUV dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Evaluate train/test scores
1.3.1,Load MUV dataset
1.3.1,Featurize MUV dataset
1.3.1,Initialize transformers
1.3.1,Load MUV data
1.3.1,Build model
1.3.1,Fit trained model
1.3.1,Evaluate train/test scores
1.3.1,Extract active site
1.3.1,Featurize ligand
1.3.1,Default for CircularFingerprint
1.3.1,Featurize pocket
1.3.1,Note broadcast operation
1.3.1,Compute labels for pockets
1.3.1,Some complexes have labels but no PDB files. Filter these manually
1.3.1,Some of the ligand-names are of form (FMN ox). Use regex
1.3.1,to merge into form (FMN-ox)
1.3.1,Filter if missing PDB files
1.3.1,Load PDBBind dataset
1.3.1,Define featurizers
1.3.1,Featurize Dataset
1.3.1,########################################################## DEBUG
1.3.1,########################################################## DEBUG
1.3.1,For stable runs
1.3.1,Fit trained model
1.3.1,4-fold splits
1.3.1,num positive/negative ligands
1.3.1,10 trials on test-set
1.3.1,Define metric
1.3.1,Get supports on test-set
1.3.1,Compute accuracies
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Test model
1.3.1,Join information for all tasks.
1.3.1,4-fold splits
1.3.1,num positive/negative ligands
1.3.1,Define metric
1.3.1,Get supports on test-set
1.3.1,Compute accuracies
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Test model
1.3.1,Join information for all tasks.
1.3.1,4-fold splits
1.3.1,num positive/negative ligands
1.3.1,10 trials on test-set
1.3.1,Define metric
1.3.1,Get supports on test-set
1.3.1,Compute accuracies
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Fit trained model
1.3.1,Test model
1.3.1,Join information for all tasks.
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,number positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply an attention lstm layer
1.3.1,Number of folds for split
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,4-fold splits
1.3.1,num positive/negative ligands
1.3.1,10 trials on test-set
1.3.1,Define metric
1.3.1,Get supports on test-set
1.3.1,Compute accuracies
1.3.1,Train model on support
1.3.1,Test model
1.3.1,Join information for all tasks.
1.3.1,4-fold splits
1.3.1,num positive/negative ligands
1.3.1,10 trials on test-set
1.3.1,Define metric
1.3.1,Get supports on test-set
1.3.1,Compute accuracies
1.3.1,Train model on support
1.3.1,Test model
1.3.1,Join information for all tasks.
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,number positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply an attention lstm layer
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,number positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply an attention lstm layer
1.3.1,Number of folds for split
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Number of folds for split
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply a residual lstm layer
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply a residual lstm layer
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply a residual lstm layer
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply a residual lstm layer
1.3.1,Set some global variables up top
1.3.1,Featurize Tox21 dataset
1.3.1,Initialize transformers
1.3.1,Set some global variables up top
1.3.1,Featurize Tox21 dataset
1.3.1,Initialize transformers
1.3.1,Load MUV dataset
1.3.1,Featurize MUV dataset
1.3.1,Initialize transformers
1.3.1,Load MUV dataset
1.3.1,Featurize MUV dataset
1.3.1,Initialize transformers
1.3.1,Featurize SIDER dataset
1.3.1,Initialize transformers
1.3.1,Featurize SIDER dataset
1.3.1,Initialize transformers
1.3.1,Load the data.
1.3.1,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
1.3.1,sparse: most tasks do not include data for most molecules.  It also is very
1.3.1,"unbalanced: there are many more negatives than positives.  For each task,"
1.3.1,create a list of alternating postives and negatives so each batch will have
1.3.1,equal numbers of both.
1.3.1,Create the model to train.  We use a simple fully connected network with
1.3.1,one hidden layer.
1.3.1,Define a MetaLearner describing the learning problem.
1.3.1,Run meta-learning on 80% of the tasks.
1.3.1,Validate on the remaining tasks.
1.3.1,Number of folds for split
1.3.1,Depth of attention module
1.3.1,number positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,Apply an attention lstm layer
1.3.1,4-fold splits
1.3.1,10 positive/negative ligands
1.3.1,10 trials on test-set
1.3.1,Sample supports without replacement (all pos/neg should be different)
1.3.1,Define metric
1.3.1,Get supports on test-set
1.3.1,Compute accuracies
1.3.1,Train model on support
1.3.1,Test model
1.3.1,"print(""Score on task %s is %s"" % (str(task), str(score)))"
1.3.1,Join information for all tasks.
1.3.1,Number of folds for split
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Number of features on conv-mols
1.3.1,Define metric
1.3.1,Train support model on train
1.3.1,Add layers
1.3.1,4-fold splits
1.3.1,num positive/negative ligands
1.3.1,Define metric
1.3.1,Get supports on test-set
1.3.1,Compute accuracies
1.3.1,Train model on support
1.3.1,Test model
1.3.1,Join information for all tasks.
1.3.1,replace with your own scratch directory
1.3.1,Number of conformations in each file increases exponentially.
1.3.1,Start with a smaller dataset before continuing. Use all of them
1.3.1,for production
1.3.1,"'ani_gdb_s03.h5',"
1.3.1,"'ani_gdb_s04.h5',"
1.3.1,"'ani_gdb_s05.h5',"
1.3.1,"'ani_gdb_s06.h5',"
1.3.1,"'ani_gdb_s07.h5',"
1.3.1,'ani_gdb_s08.h5'
1.3.1,Extract the data
1.3.1,Print the data
1.3.1,self-interaction energies taken from
1.3.1,https://github.com/isayev/ANI1_dataset README
1.3.1,flush once more at the end
1.3.1,"# For production, set nb_epoch to 100+"
1.3.1,"print(""Train scores"")"
1.3.1,print(train_scores)
1.3.1,"print(""Minimization of a single test set structure:"")"
1.3.1,"print(model.minimize_structure(coords, atomic_nums))"
1.3.1,Written by Roman Zubatyuk and Justin S. Smith
1.3.1,Modified by Yutong Zhao to make python2 compatible
1.3.1,opening file
1.3.1,print(store_loc)
1.3.1,print(type(v[0]))
1.3.1,print(k)
1.3.1,print(path)
1.3.1,Number of conformations in each file increases exponentially.
1.3.1,Start with a smaller dataset before continuing. Use all of them
1.3.1,for production
1.3.1,Extract the data
1.3.1,Note sensitivity = recall
1.3.1,NOTE THE RENAMING:
1.3.1,Note sensitivity = recall
1.3.1,Load nci dataset
1.3.1,Featurize nci dataset
1.3.1,Initialize transformers
1.3.1,Set some global variables up top
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load hiv dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Featurize hiv dataset
1.3.1,Initialize transformers
1.3.1,Only for debug!
1.3.1,Load hiv dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Fit trained model
1.3.1,Fit models
1.3.1,Batch size of models
1.3.1,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.3.1,Fit trained model
1.3.1,Load SIDER dataset
1.3.1,Featurize SIDER dataset
1.3.1,Initialize transformers
1.3.1,Featurize permeability dataset
1.3.1,Load Tox21 dataset
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load SAMPL dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load Tox21 dataset
1.3.1,Fit models
1.3.1,Do setup required for tf/keras models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Gather Projection
1.3.1,Dense post-processing layer
1.3.1,Fit trained model
1.3.1,Featurize SAMPL dataset
1.3.1,Initialize transformers
1.3.1,Load clintox dataset
1.3.1,Featurize clintox dataset
1.3.1,Transform clintox dataset
1.3.1,Split clintox dataset
1.3.1,Only for debug!
1.3.1,Load clintox dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load clintox dataset
1.3.1,Fit models
1.3.1,Do setup required for tf/keras models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Gather Projection
1.3.1,Fit trained model
1.3.1,-*- coding: utf-8 -*-
1.3.1,#############################################################################
1.3.1,## save dataset
1.3.1,#############################################################################
1.3.1,## load datasets
1.3.1,load sweetfda
1.3.1,load aact
1.3.1,## fixup smiles for matching
1.3.1,return smiles
1.3.1,map original smiles to converted smiles
1.3.1,"## join dataframes, index on smiles"
1.3.1,map original smiles back
1.3.1,## fill all nan with 0
1.3.1,## construct datasets
1.3.1,store in new datasets
1.3.1,## save datasets
1.3.1,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
1.3.1,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
1.3.1,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
1.3.1,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
1.3.1,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
1.3.1,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
1.3.1,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
1.3.1,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
1.3.1,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
1.3.1,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
1.3.1,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
1.3.1,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
1.3.1,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
1.3.1,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
1.3.1,For stable runs
1.3.1,Fit trained model
1.3.1,For stable runs
1.3.1,Fit trained model
1.3.1,Some complexes have labels but no PDB files. Filter these manually
1.3.1,Some of the ligand-names are of form (FMN ox). Use regex
1.3.1,to merge into form (FMN-ox)
1.3.1,Load PDBBind dataset
1.3.1,Define featurizers
1.3.1,TODO: add pi_stack and cation_pi to feature_types (it's not trivial
1.3.1,because they require sanitized molecules)
1.3.1,"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.3.1,"""salt_bridge""],"
1.3.1,Featurize Dataset
1.3.1,Currently featurizes with shard_size=1
1.3.1,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.3.1,transformers = [
1.3.1,"dc.trans.LogTransformer(transform_X=True),"
1.3.1,"dc.trans.NormalizationTransformer(transform_y=True,"
1.3.1,dataset=train_dataset)]
1.3.1,Featurize UV dataset
1.3.1,##Load data###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Set numpy seed
1.3.1,##Load data###
1.3.1,##Create model###
1.3.1,Use R2 classification metric
1.3.1,"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
1.3.1,Only use for final evaluation
1.3.1,Force matplotlib to not use any Xwindows backend.
1.3.1,##Load data###
1.3.1,the histogram of the data
1.3.1,##Load data###
1.3.1,###################################################### DEBUG
1.3.1,###################################################### DEBUG
1.3.1,Load HOPV dataset
1.3.1,Fit models
1.3.1,Number of features on conv-mols
1.3.1,Batch size of models
1.3.1,Gather Projection
1.3.1,Fit trained model
1.3.1,Featurize HOPV dataset
1.3.1,Initialize transformers
1.3.1,Only for debug!
1.3.1,Load HOPV dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load HOPV dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load HOPV dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Only for debug!
1.3.1,Load HOPV dataset
1.3.1,Fit models
1.3.1,Fit trained model
1.3.1,Load TOXCAST dataset
1.3.1,Featurize TOXCAST dataset
1.3.1,Initialize transformers
1.3.1,Fit trained model
1.3.1,Processing of ToxCast data
1.3.1,Author - Aneesh Pappu
1.3.1,Loading dataframes and editing indices
1.3.1,Loop through rows of hitc matrix and replace codes with smiles strings
1.3.1,get corresponding casn
1.3.1,get corresponding smiles
1.3.1,write to cell
1.3.1,Tidy up and write to csv
1.3.1,-*- coding: utf-8 -*-
1.3.1,Save hyperparameters
1.3.1,-*- coding: utf-8 -*-
1.3.1,Save hyperparameters
1.3.1,setup optimizer
1.3.1,setup optimizer
1.3.1,"print(""tasK: %d"" %task)"
1.3.1,"cores = torch.cat([scores, 1.-scores], dim=1)"
1.3.1,"print(""scores"")"
1.3.1,print(scores.size())
1.3.1,"print(""task_label"")"
1.3.1,print(task_label.size())
1.3.1,"task_loss =  self.criterion(scores, task_label)"
1.3.1,"print(""task_loss"")"
1.3.1,print(task_loss.size())
1.3.1,-*- coding: utf-8 -*-
1.3.1,Save hyperparameters
1.3.1,weight decay
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Turns out there are valid cases where we don't want pad-batches
1.3.1,on by default.
1.3.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.1,Run training op.
1.3.1,############################################################# TIMING
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,Special case to handle singletasks.
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,2017 DeepCrystal Technologies - Patrick Hop
1.3.1,
1.3.1,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
1.3.1,
1.3.1,MIT License - have fun!!
1.3.1,===========================================================
1.3.1,x = F.selu( fc(x) )
1.3.1,x = F.selu( fc(x) )
1.3.1,2017 DeepCrystal Technologies - Patrick Hop
1.3.1,
1.3.1,Data loading a splitting file
1.3.1,
1.3.1,MIT License - have fun!!
1.3.1,===========================================================
1.3.1,Set random seeds
1.3.1,Setup directories
1.3.1,Model constants
1.3.1,Load and transform datasets
1.3.1,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.1,Atomic convolution variables
1.3.1,at = atomic numbers (atom types)
1.3.1,"radial basis function parameters [cutoff, mean, width]"
1.3.1,Model hyperparameters
1.3.1,Initialize model
1.3.1,Fit model
1.3.1,Evaluate model
1.3.1,Set random seeds
1.3.1,Setup directories
1.3.1,Model constants
1.3.1,Load and transform datasets
1.3.1,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.1,Atomic convolution variables
1.3.1,at = atomic numbers (atom types)
1.3.1,"radial basis function parameters [cutoff, mean, width]"
1.3.1,Model hyperparameters
1.3.1,Initialize model
1.3.1,Fit model
1.3.1,Evaluate model
1.3.1,Set random seeds
1.3.1,Setup directories
1.3.1,Model constants
1.3.1,Load and transform datasets
1.3.1,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.1,Atomic convolution variables
1.3.1,at = atomic numbers (atom types)
1.3.1,"radial basis function parameters [cutoff, mean, width]"
1.3.1,Model hyperparameters
1.3.1,Initialize model
1.3.1,Fit model
1.3.1,Evaluate model
1.3.1,Set random seeds
1.3.1,Setup directories
1.3.1,Model constants
1.3.1,Load and transform datasets
1.3.1,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.1,Atomic convolution variables
1.3.1,at = atomic numbers (atom types)
1.3.1,"radial basis function parameters [cutoff, mean, width]"
1.3.1,Model hyperparameters
1.3.1,Initialize model
1.3.1,Fit model
1.3.1,Evaluate model
1.3.1,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
1.3.1,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
1.3.1,test_scores = test_evaluator.compute_model_performance(metric)
1.3.1,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
1.3.1,param.update(test_scores)
1.3.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.1,for transformer in transformers:
1.3.1,train_dataset = transformer.transform(train_dataset)
1.3.1,test_dataset = transformer.transform(test_dataset)
1.3.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.1,for transformer in transformers:
1.3.1,train_dataset = transformer.transform(train_dataset)
1.3.1,test_dataset = transformer.transform(test_dataset)
1.3.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.1,for transformer in transformers:
1.3.1,train_dataset = transformer.transform(train_dataset)
1.3.1,test_dataset = transformer.transform(test_dataset)
1.3.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.1,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.1,for transformer in transformers:
1.3.1,train_dataset = transformer.transform(train_dataset)
1.3.1,test_dataset = transformer.transform(test_dataset)
1.3.1,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.1,Create some directories for analysis
1.3.1,The base_dir holds the results of all analysis
1.3.1,Make directories to store the raw and featurized datasets.
1.3.1,Load PDBBind dataset
1.3.1,Define featurizers
1.3.1,Currently featurizes with shard_size=1
1.3.1,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.3.1,This could be done with openbabel in python
1.3.1,Compute cells for this molecule. O(constant)
1.3.1,min == max if molecule is planar in some direction
1.3.1,we should still create a bin
1.3.1,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
1.3.1,Note neighbors contains self!
1.3.1,Associate each atom with cell it belongs to. O(N)
1.3.1,Associate each cell with its neighbor cells. Assumes periodic boundary
1.3.1,"conditions, so does wrapround. O(constant)"
1.3.1,"For each atom, loop through all atoms in its cell and neighboring cells."
1.3.1,Accept as neighbors only those within threshold. This computation should be
1.3.1,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
1.3.1,Sort neighbors by distance
1.3.1,Pick up to max_num_neighbors
1.3.1,Type of data created by this featurizer
1.3.1,assumes that every array is of the same dimension
1.3.1,rem_dataset is remaining portion of dataset
1.3.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.1,to k-1.
1.3.1,returns list of per column sum of non zero elements
1.3.1,Compute number of actives needed per task.
1.3.1,loop through each column and obtain index required to splice out for
1.3.1,required fraction of hits
1.3.1,Find the first index where the cumulative number of actives equals
1.3.1,the actives_count
1.3.1,Note that np.where tells us last index required to exceed
1.3.1,"actives_count, so we actually want the following location"
1.3.1,TODO(rbharath): Refactor this split method to match API of other splits (or
1.3.1,potentially refactor those to match this.
1.3.1,Handle edge case where frac_split is 1
1.3.1,Create weight matrices fpor two haves.
1.3.1,copy over up to required index for weight first_split
1.3.1,check out if any rows in either w_1 or w_2 are just zeros
1.3.1,"Obtain original x, y, and w arrays and shuffle"
1.3.1,calculate percent split for valid (out of test and valid)
1.3.1,"split test data into valid and test, treating sub test set also as sparse"
1.3.1,rem_dataset is remaining portion of dataset
1.3.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.1,to k-1.
1.3.1,JSG Assert that split fractions can be written as proper fractions over 10.
1.3.1,This can be generalized in the future with some common demoninator determination.
1.3.1,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.3.1,Append remaining examples to train
1.3.1,Sort by increasing MW
1.3.1,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.3.1,for m_idx in cluster:
1.3.1,"continue until we find an active in all the tasks, otherwise we can't"
1.3.1,compute a meaningful AUC
1.3.1,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.3.1,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.3.1,Sort from largest to smallest scaffold sets
1.3.1,Sort from largest to smallest scaffold sets
1.3.1,"(n_samples, n_classes)"
1.3.1,"(n_samples, n_tasks, n_classes)"
1.3.1,Save hyperparameters
1.3.1,Guard variable to make sure we don't Restore() this model
1.3.1,from a disk checkpoint more than once.
1.3.1,"Path to save checkpoint files, which matches the"
1.3.1,replicated supervisor's default path.
1.3.1,Lazily created by _get_shared_session().
1.3.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.3.1,when subclass-overridden methods use the same scopes.
1.3.1,Setup graph
1.3.1,Note that we divide by the batch size and not the number of
1.3.1,"non-zero weight examples in the batch.  Also, instead of using"
1.3.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.1,calculate with div/sum so it stays on the GPU.
1.3.1,aggregated costs
1.3.1,weight decay
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Save an initial checkpoint.
1.3.1,Turns out there are valid cases where we don't want pad-batches
1.3.1,on by default.
1.3.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.1,Run training op.
1.3.1,Always save a final checkpoint when complete.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,allow_soft_placement=True allows ops without a GPU implementation
1.3.1,to run on the CPU instead.
1.3.1,TODO(rbharath): Is setting train=False right here?
1.3.1,Discard any padded predictions
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,Special case to handle singletasks.
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,TODO(rbharath): Verify this can be safely removed.
1.3.1,"def evaluate(self, dataset, metrics, transformers=[]):"
1.3.1,""""""""
1.3.1,Evaluates the performance of this model on specified dataset.
1.3.1,
1.3.1,Parameters
1.3.1,----------
1.3.1,dataset: dc.data.Dataset
1.3.1,Dataset object.
1.3.1,metric: deepchem.metrics.Metric
1.3.1,Evaluation metric
1.3.1,transformers: list
1.3.1,List of deepchem.transformers.Transformer
1.3.1,Returns
1.3.1,-------
1.3.1,dict
1.3.1,Maps tasks to scores under metric.
1.3.1,""""""""
1.3.1,"evaluator = Evaluator(self, dataset, transformers)"
1.3.1,scores = evaluator.compute_model_performance(metrics)
1.3.1,return scores
1.3.1,checkpoints look like logdir/model.ckpt-N
1.3.1,"self._save_path is ""logdir/model.ckpt"""
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,Note that softmax is already applied in construct_grpah
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,Handle edge case when batch-size is 1.
1.3.1,Prune away any padding that was added
1.3.1,Handle case of 0-dimensional scalar output
1.3.1,Dummy placeholders
1.3.1,Dummy placeholders
1.3.1,## AtomicNet fully-connected layer ops ###
1.3.1,## Atomicnet coordinate transform ops ###
1.3.1,## Atomicnet symmetry function kernel ops ###
1.3.1,## Atomicnet symmetry function ops ###
1.3.1,## Atomcnet symmetry function layer ops ###
1.3.1,We apply the radial pooling filter before atom type conv
1.3.1,to reduce computation
1.3.1,## Misc convenience ops ###
1.3.1,"Copied from the yt_project, commit e8fb57e"
1.3.1,yt/doc/extensions/notebook_sphinxext.py
1.3.1,https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
1.3.1,Almost completely re-written by Matthew Harrigan to use nbconvert v4
1.3.1,1. Uneval notebook
1.3.1,2. Python
1.3.1,3. HTML (execute first)
1.3.1,Set per-cell timeout to 60 seconds
1.3.1,4. Eval'd notebook
1.3.1,Create link to notebook and script files
1.3.1,create notebook node
1.3.1,add dependency
1.3.1,-*- coding: utf-8 -*-
1.3.1,
1.3.1,"deepchem documentation build configuration file, created by"
1.3.1,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
1.3.1,
1.3.1,This file is execfile()d with the current directory set to its
1.3.1,containing dir.
1.3.1,
1.3.1,Note that not all possible configuration values are present in this
1.3.1,autogenerated file.
1.3.1,
1.3.1,All configuration values have a default; values that are commented out
1.3.1,serve to show the default.
1.3.1,"If extensions (or modules to document with autodoc) are in another directory,"
1.3.1,add these directories to sys.path here. If the directory is relative to the
1.3.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.3.1,"sys.path.insert(0, os.path.abspath('.'))"
1.3.1,-- General configuration ------------------------------------------------
1.3.1,"If your documentation needs a minimal Sphinx version, state it here."
1.3.1,needs_sphinx = '1.0'
1.3.1,"Add any Sphinx extension module names here, as strings. They can be"
1.3.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.3.1,ones.
1.3.1,"Add any paths that contain templates here, relative to this directory."
1.3.1,The suffix(es) of source filenames.
1.3.1,You can specify multiple suffix as a list of string:
1.3.1,"source_suffix = ['.rst', '.md']"
1.3.1,The encoding of source files.
1.3.1,source_encoding = 'utf-8-sig'
1.3.1,The master toctree document.
1.3.1,General information about the project.
1.3.1,"The version info for the project you're documenting, acts as replacement for"
1.3.1,"|version| and |release|, also used in various other places throughout the"
1.3.1,built documents.
1.3.1,
1.3.1,The short X.Y version.
1.3.1,"The full version, including alpha/beta/rc tags."
1.3.1,The language for content autogenerated by Sphinx. Refer to documentation
1.3.1,for a list of supported languages.
1.3.1,
1.3.1,This is also used if you do content translation via gettext catalogs.
1.3.1,"Usually you set ""language"" from the command line for these cases."
1.3.1,"There are two options for replacing |today|: either, you set today to some"
1.3.1,"non-false value, then it is used:"
1.3.1,today = ''
1.3.1,"Else, today_fmt is used as the format for a strftime call."
1.3.1,"today_fmt = '%B %d, %Y'"
1.3.1,"List of patterns, relative to source directory, that match files and"
1.3.1,directories to ignore when looking for source files.
1.3.1,The reST default role (used for this markup: `text`) to use for all
1.3.1,documents.
1.3.1,default_role = None
1.3.1,"If true, '()' will be appended to :func: etc. cross-reference text."
1.3.1,add_function_parentheses = True
1.3.1,"If true, the current module name will be prepended to all description"
1.3.1,unit titles (such as .. function::).
1.3.1,add_module_names = True
1.3.1,"If true, sectionauthor and moduleauthor directives will be shown in the"
1.3.1,output. They are ignored by default.
1.3.1,show_authors = False
1.3.1,The name of the Pygments (syntax highlighting) style to use.
1.3.1,A list of ignored prefixes for module index sorting.
1.3.1,modindex_common_prefix = []
1.3.1,"If true, keep warnings as ""system message"" paragraphs in the built documents."
1.3.1,keep_warnings = False
1.3.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.3.1,-- Options for HTML output ----------------------------------------------
1.3.1,The theme to use for HTML and HTML Help pages.  See the documentation for
1.3.1,a list of builtin themes.
1.3.1,Theme options are theme-specific and customize the look and feel of a theme
1.3.1,"further.  For a list of options available for each theme, see the"
1.3.1,documentation.
1.3.1,html_theme_options = {}
1.3.1,"Add any paths that contain custom themes here, relative to this directory."
1.3.1,"The name for this set of Sphinx documents.  If None, it defaults to"
1.3.1,"""<project> v<release> documentation""."
1.3.1,html_title = None
1.3.1,A shorter title for the navigation bar.  Default is the same as html_title.
1.3.1,html_short_title = None
1.3.1,The name of an image file (relative to this directory) to place at the top
1.3.1,of the sidebar.
1.3.1,The name of an image file (within the static path) to use as favicon of the
1.3.1,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
1.3.1,pixels large.
1.3.1,html_favicon = None
1.3.1,"Add any paths that contain custom static files (such as style sheets) here,"
1.3.1,"relative to this directory. They are copied after the builtin static files,"
1.3.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.3.1,Add any extra paths that contain custom files (such as robots.txt or
1.3.1,".htaccess) here, relative to this directory. These files are copied"
1.3.1,directly to the root of the documentation.
1.3.1,html_extra_path = []
1.3.1,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
1.3.1,using the given strftime format.
1.3.1,"html_last_updated_fmt = '%b %d, %Y'"
1.3.1,"If true, SmartyPants will be used to convert quotes and dashes to"
1.3.1,typographically correct entities.
1.3.1,html_use_smartypants = True
1.3.1,"Custom sidebar templates, maps document names to template names."
1.3.1,html_sidebars = {}
1.3.1,"Additional templates that should be rendered to pages, maps page names to"
1.3.1,template names.
1.3.1,html_additional_pages = {}
1.3.1,"If false, no module index is generated."
1.3.1,html_domain_indices = True
1.3.1,"If false, no index is generated."
1.3.1,html_use_index = True
1.3.1,"If true, the index is split into individual pages for each letter."
1.3.1,html_split_index = False
1.3.1,"If true, links to the reST sources are added to the pages."
1.3.1,html_show_sourcelink = True
1.3.1,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
1.3.1,html_show_sphinx = True
1.3.1,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
1.3.1,html_show_copyright = True
1.3.1,"If true, an OpenSearch description file will be output, and all pages will"
1.3.1,contain a <link> tag referring to it.  The value of this option must be the
1.3.1,base URL from which the finished HTML is served.
1.3.1,html_use_opensearch = ''
1.3.1,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
1.3.1,html_file_suffix = None
1.3.1,Language to be used for generating the HTML full-text search index.
1.3.1,Sphinx supports the following languages:
1.3.1,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
1.3.1,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
1.3.1,html_search_language = 'en'
1.3.1,"A dictionary with options for the search language support, empty by default."
1.3.1,Now only 'ja' uses this config value
1.3.1,html_search_options = {'type': 'default'}
1.3.1,The name of a javascript file (relative to the configuration directory) that
1.3.1,"implements a search results scorer. If empty, the default will be used."
1.3.1,html_search_scorer = 'scorer.js'
1.3.1,Output file base name for HTML help builder.
1.3.1,-- Options for LaTeX output ---------------------------------------------
1.3.1,The paper size ('letterpaper' or 'a4paper').
1.3.1,"'papersize': 'letterpaper',"
1.3.1,"The font size ('10pt', '11pt' or '12pt')."
1.3.1,"'pointsize': '10pt',"
1.3.1,Additional stuff for the LaTeX preamble.
1.3.1,"'preamble': '',"
1.3.1,Latex figure (float) alignment
1.3.1,"'figure_align': 'htbp',"
1.3.1,Grouping the document tree into LaTeX files. List of tuples
1.3.1,"(source start file, target name, title,"
1.3.1,"author, documentclass [howto, manual, or own class])."
1.3.1,The name of an image file (relative to this directory) to place at the top of
1.3.1,the title page.
1.3.1,latex_logo = None
1.3.1,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
1.3.1,not chapters.
1.3.1,latex_use_parts = False
1.3.1,"If true, show page references after internal links."
1.3.1,latex_show_pagerefs = False
1.3.1,"If true, show URL addresses after external links."
1.3.1,latex_show_urls = False
1.3.1,Documents to append as an appendix to all manuals.
1.3.1,latex_appendices = []
1.3.1,"If false, no module index is generated."
1.3.1,latex_domain_indices = True
1.3.1,-- Options for manual page output ---------------------------------------
1.3.1,One entry per manual page. List of tuples
1.3.1,"(source start file, name, description, authors, manual section)."
1.3.1,"If true, show URL addresses after external links."
1.3.1,man_show_urls = False
1.3.1,-- Options for Texinfo output -------------------------------------------
1.3.1,Grouping the document tree into Texinfo files. List of tuples
1.3.1,"(source start file, target name, title, author,"
1.3.1,"dir menu entry, description, category)"
1.3.1,Documents to append as an appendix to all manuals.
1.3.1,texinfo_appendices = []
1.3.1,"If false, no module index is generated."
1.3.1,texinfo_domain_indices = True
1.3.1,"How to display URL addresses: 'footnote', 'no', or 'inline'."
1.3.1,texinfo_show_urls = 'footnote'
1.3.1,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
1.3.1,texinfo_no_detailmenu = False
1.3.1,Example configuration for intersphinx: refer to the Python standard library.
1.3.1,Higher is Better
1.3.1,The secret key is available as a secure environment variable
1.3.1,on travis-ci to push the build documentation to Amazon S3.
1.3.1,Perform recursive modification to set css mime types.
1.3.1,Perform recursive modification to set js mime types.
1.3.1,lines in the label file have format
1.3.1,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
1.3.1,"print line[0], line[3]"
1.3.1,Record inputs.
1.3.1,Create the output directory if necessary.
1.3.1,Create duplicate placeholders for meta-optimization.
1.3.1,Create the loss function for meta-optimization.
1.3.1,"In the final loss, use different placeholders for all inputs so the loss will be"
1.3.1,computed from a different batch.
1.3.1,Create variables for accumulating the gradients.
1.3.1,Create the optimizers for meta-optimization and task optimization.
1.3.1,Main optimization loop.
1.3.1,Do checkpointing.
1.3.1,This is a MetaLearner that learns to generate sine functions with variable
1.3.1,amplitude and phase.
1.3.1,Optimize it.
1.3.1,Test it out on some new tasks and see how it works.
1.3.1,Initially the model should do a bad job of fitting the sine function.
1.3.1,After one step of optimization it should do much better.
1.3.1,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
1.3.1,get the same result.
1.3.1,"Do the same thing, only using the ""restore"" argument to fit()."
1.3.1,References
1.3.1,Arguments
1.3.1,Aliases.
1.3.1,Tensorflow correctly processes empty lists when using concat
1.3.1,"Sum along neighbors as well as self, and store"
1.3.1,Sum all neighbors using adjacency matrix
1.3.1,Get collection of modified atom features
1.3.1,Obtain relevant atoms for this degree
1.3.1,Get self atoms
1.3.1,Apply hidden affine to relevant atoms and append
1.3.1,Determine the min_deg=0 case
1.3.1,Only use the self layer
1.3.1,Combine all atoms back into the list
1.3.1,"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
1.3.1,Obtain the partitions for each of the molecules
1.3.1,Sum over atoms for each molecule
1.3.1,Get the final sparse representations
1.3.1,Store the summed atoms by degree
1.3.1,Tensorflow correctly processes empty lists when using concat
1.3.1,Get self atoms
1.3.1,Expand dims
1.3.1,always deg-1 for deg_adj_lists
1.3.1,TODO(rbharath): It's not clear where nb_affine comes from.
1.3.1,Is there a solid explanation here?
1.3.1,Generate the nb_affine weights and biases
1.3.1,Add trainable weights
1.3.1,Extract atom_features
1.3.1,Extract graph topology
1.3.1,Perform the mol conv
1.3.1,Extract nodes and membership
1.3.1,Extract atom_features
1.3.1,Extract graph topology
1.3.1,Perform the mol gather
1.3.1,Extract nodes
1.3.1,Extract atom_features
1.3.1,Extract graph topology
1.3.1,Perform the mol gather
1.3.1,"x is test set, xp is support set."
1.3.1,# Initializes trainable weights.
1.3.1,## Performs computations
1.3.1,Get initializations
1.3.1,r = self.r_init
1.3.1,Process using attention
1.3.1,"Eqn (4), appendix A.1 of Matching Networks paper"
1.3.1,Generate new aattention states
1.3.1,"def build(self, input_shape):"
1.3.1,"_, support_input_shape = input_shape  #Unpack"
1.3.1,n_feat = support_input_shape[1]
1.3.1,Support set lstm
1.3.1,Test lstm
1.3.1,Get initializations
1.3.1,Rename support
1.3.1,Process support xp using attention
1.3.1,Get linear combination of support set
1.3.1,Not sure if it helps to place the update here or later yet.  Will
1.3.1,decide
1.3.1,z = r
1.3.1,Process test x using attention
1.3.1,Generate new support attention states
1.3.1,Generate new test attention states
1.3.1,Redefine
1.3.1,"return [x+p, z+q]"
1.3.1,No other forget biases supported right now.
1.3.1,"def build(self, input_shape):"
1.3.1,Taken from Keras code [citation needed]
1.3.1,###################################################### DEBUG
1.3.1,"return o, [h, c]"
1.3.1,###################################################### DEBUG
1.3.1,"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
1.3.1,distance_hidden = self.activation(distance_hidden)
1.3.1,atom_features_hidden = self.activation(atom_features_hidden)
1.3.1,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.3.1,and embeddings of atom j(both gone through a hidden layer)
1.3.1,"for atom i, sum the influence from all other atom j in the molecule"
1.3.1,number of inputs each step
1.3.1,Add trainable weights
1.3.1,Extract atom_features
1.3.1,Basic features of every atom: (batch_size*max_atoms) * n_atom_features
1.3.1,calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
1.3.1,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.3.1,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.3.1,"step i calculates the graph features for atoms of index `parents[:,i,0]`"
1.3.1,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.3.1,"represent the same atoms of `parents[:, :, 0]`,"
1.3.1,different in that these index are positions in `atom_features`
1.3.1,"number of atoms in total, should equal `batch_size*max_atoms`"
1.3.1,initialize graph features for each graph
1.3.1,another row of zeros is generated for padded dummy atoms
1.3.1,`count`-th step
1.3.1,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.3.1,generating index for graph features used in the inputs
1.3.1,"extracting graph features for parents of the target atoms, then flatten"
1.3.1,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.3.1,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.3.1,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.3.1,of shape: (batch_size*max_atoms) * n_graph_features
1.3.1,representing the graph features of target atoms in each graph
1.3.1,index for targe atoms
1.3.1,update the graph features for target atoms
1.3.1,last step generates graph features for all target atom
1.3.1,Add trainable weights
1.3.1,Extract atom_features
1.3.1,sum all graph outputs
1.3.1,Aliases.
1.3.1,TODO(rbharath): What does this line do?
1.3.1,TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
1.3.1,This dictionary holds a mapping {graph: learning_phase}.
1.3.1,A learning phase is a bool tensor used to run Keras models in
1.3.1,either train mode (learning_phase == 1) or test mode (learning_phase == 0).
1.3.1,else: assume learning phase is a placeholder tensor.
1.3.1,need broadcasting
1.3.1,ensure that randomness is conditioned by the Numpy RNG
1.3.1,ensure that randomness is conditioned by the Numpy RNG
1.3.1,TODO(rbharath): Should probably swap this over to tf mode.
1.3.1,Note: tf.nn.softmax_cross_entropy_with_logits
1.3.1,"expects logits, Keras expects probabilities."
1.3.1,scale preds so that the class probas of each sample sum to 1
1.3.1,manual computation of crossentropy
1.3.1,Note: tf.nn.softmax_cross_entropy_with_logits
1.3.1,"expects logits, Keras expects probabilities."
1.3.1,if our output includes timesteps we need to reshape
1.3.1,Arguments
1.3.1,Returns
1.3.1,Note: tf.nn.softmax_cross_entropy_with_logits
1.3.1,"expects logits, Keras expects probabilities."
1.3.1,transform back to logits
1.3.1,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
1.3.1,a tensor. Confusing with tf.zeros...
1.3.1,Transpose for mul
1.3.1,exclude bias variables
1.3.1,"tf.scalar_summary('Weight Decay Cost', cost)"
1.3.1,TODO(user): gradient clipping (see Minimize)
1.3.1,These properties should have been set
1.3.1,"by the child class, as appropriate."
1.3.1,These properties should be set by the user via keyword arguments.
1.3.1,"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
1.3.1,are only applicable to input layers: do not pass these keywords
1.3.1,to non-input layers.
1.3.1,In this case we will create an input layer
1.3.1,to insert before the current layer
1.3.1,Update self.losses
1.3.1,In case self.losses isn't settable
1.3.1,(i.e. it's a getter method).
1.3.1,In that case the `losses` property is
1.3.1,auto-computed and shouldn't be set.
1.3.1,Update self._per_input_updates
1.3.1,Updates indexed by None are unconditional
1.3.1,rather than input-dependent
1.3.1,outputs = to_list(self.call(x))
1.3.1,return outputs
1.3.1,TODO(rbharath): Keras uses a global var here to maintain
1.3.1,unique counts. This seems dangerous. How does tensorflow handle?
1.3.1,TODO(rbharath): Support this type of functional API.
1.3.1,If batch size not specified
1.3.1,Input shape
1.3.1,Output shape
1.3.1,References
1.3.1,Not Trainable
1.3.1,Not Trainable
1.3.1,need broadcasting
1.3.1,pick the normalized form of x corresponding to the training phase
1.3.1,sample-wise normalization
1.3.1,from deepchem.nn.model_ops import variable
1.3.1,Assuming convolution kernels (2D or 3D).
1.3.1,"TF kernel shape: (..., input_depth, depth)"
1.3.1,No specific assumptions.
1.3.1,References
1.3.1,References
1.3.1,References
1.3.1,References
1.3.1,Pick the one with the correct shape.
1.3.1,Arguments
1.3.1,Aliases.
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,Add trainable weights
1.3.1,Add trainable weights
1.3.1,Add trainable weights
1.3.1,Add trainable weights
1.3.1,"Output should be of shape (?, nb_filter)"
1.3.1,"Output should be of shape (batch_size, n_feat)"
1.3.1,Try concatenating the two lists of placeholders
1.3.1,Try concatenating the two lists of placeholders
1.3.1,Fit model on dataset
1.3.1,Fit model on dataset
1.3.1,"Should be an array of size (n_pocket_atoms, 3)"
1.3.1,"coords[triangle, 0] gives the x-dimension of all triangle points"
1.3.1,Take transpose to make sure rows correspond to atoms.
1.3.1,We voxelize so all grids have integral coordinates (convenience)
1.3.1,"If overlap of box with previously generated output boxes, return"
1.3.1,Carry forward mappings
1.3.1,We know that box has at least one atom not in outputs
1.3.1,Current box has been merged into box further down list.
1.3.1,No need to output current box
1.3.1,"protein_coords is (N, 3) tensor"
1.3.1,Load binding pocket model
1.3.1,TODO(rbharath): Shift refined to full once trained.
1.3.1,Fit model on dataset
1.3.1,Create featurizers
1.3.1,"if not ligand_file.endswith("".sdf""):"
1.3.1,"raise ValueError(""Only .sdf ligand files can be featurized."")"
1.3.1,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
1.3.1,ligand_mol2 = os.path.join(
1.3.1,"self.base_dir, ligand_basename + "".mol2"")"
1.3.1,
1.3.1,# Write mol2 file for ligand
1.3.1,obConversion = ob.OBConversion()
1.3.1,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
1.3.1,ob_mol = ob.OBMol()
1.3.1,"obConversion.ReadFile(ob_mol, str(ligand_file))"
1.3.1,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
1.3.1,
1.3.1,# Featurize ligand
1.3.1,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
1.3.1,if mol is None:
1.3.1,"return None, None"
1.3.1,# Default for CircularFingerprint
1.3.1,n_ligand_features = 1024
1.3.1,ligand_features = self.ligand_featurizer.featurize([mol])
1.3.1,
1.3.1,# Featurize pocket
1.3.1,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
1.3.1,"protein_file, ligand_file)"
1.3.1,n_pockets = len(pockets)
1.3.1,n_pocket_features = BindingPocketFeaturizer.n_features
1.3.1,
1.3.1,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
1.3.1,pocket_features = self.pocket_featurizer.featurize(
1.3.1,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
1.3.1,# Note broadcast operation
1.3.1,"features[:, :n_pocket_features] = pocket_features"
1.3.1,"features[:, n_pocket_features:] = ligand_features"
1.3.1,dataset = NumpyDataset(X=features)
1.3.1,pocket_preds = self.model.predict(dataset)
1.3.1,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
1.3.1,
1.3.1,# Find pockets which are active
1.3.1,active_pockets = []
1.3.1,active_pocket_atoms_map = {}
1.3.1,active_pocket_coords = []
1.3.1,for pocket_ind in range(len(pockets)):
1.3.1,#################################################### DEBUG
1.3.1,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
1.3.1,#if pocket_preds[pocket_ind] == 1:
1.3.1,if pocket_pred_proba[pocket_ind][1] > .15:
1.3.1,#################################################### DEBUG
1.3.1,pocket = pockets[pocket_ind]
1.3.1,active_pockets.append(pocket)
1.3.1,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
1.3.1,active_pocket_coords.append(pocket_coords[pocket_ind])
1.3.1,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
1.3.1,# TODO(LESWING)
1.3.1,TODO: add pi_stack and cation_pi to feature_types (it's not trivial
1.3.1,because they require sanitized molecules)
1.3.1,"feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.3.1,"""salt_bridge""],"
1.3.1,TODO(rbharath): May want to move this file to S3 so we can ensure it's
1.3.1,always available.
1.3.1,Prepare receptor
1.3.1,Get protein centroid and range
1.3.1,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
1.3.1,going to be extremely slow!
1.3.1,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
1.3.1,first pocket.
1.3.1,Prepare receptor
1.3.1,TODO(rbharath): Generalize this so can support mol2 files as well.
1.3.1,Write Vina conf file
1.3.1,Define locations of log and output files
1.3.1,TODO(rbharath): Let user specify the number of poses required.
1.3.1,TODO(rbharath): Convert the output pdbqt to a pdb file.
1.3.1,Return docked files
1.3.1,Check returned files exist
1.3.1,Check returned files exist
1.3.1,Check returned files exist
1.3.1,Check returned files exist
1.3.1,Check returned files exist
1.3.1,Note this may download autodock Vina...
1.3.1,Note this may download autodock Vina...
1.3.1,Note this may download autodock Vina...
1.3.1,Check returned files exist
1.3.1,Note this may download autodock Vina...
1.3.1,Check returned files exist
1.3.1,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
1.3.1,box1 contained in box2
1.3.1,"box1 in box2, so complete overlap"
1.3.1,"4/5 atoms in box2 in box1, so 80 % overlap"
1.3.1,box2 contains box1
1.3.1,box1 contains box2
1.3.1,"box1 contains box2, box3"
1.3.1,Test that every atom in pocket maps exists
1.3.1,Check that the atoms is actually in protein
1.3.1,Test that every atom in pocket maps exists
1.3.1,Check that the atoms is actually in protein
1.3.1,Add active site to dict
1.3.1,The convention used is that the first task is the metric.
1.3.1,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
1.3.1,"an option in the Metric class. Instead, this should be possible to move into"
1.3.1,user-space as a custom task_averager function.
1.3.1,"TODO(rbharath, joegomes): What is this magic number?"
1.3.1,"If there are no nonzero examples, metric is ill-defined."
1.3.1,TODO(rbharath): This has been a major source of bugs. Is there a more
1.3.1,robust characterization of which metrics require class-probs and which
1.3.1,don't?
1.3.1,Reshape to handle 1-d edge cases
1.3.1,ids = df[id_field].values
1.3.1,Set missing data to have weight zero
1.3.1,TODO (ytz) this is a bandage solution to reorder the atoms so
1.3.1,that they're always in the same canonical order. Presumably this
1.3.1,should be correctly implemented in the future for graph mols.
1.3.1,Featurize task results iff they exist.
1.3.1,Filter out examples where featurization failed.
1.3.1,"For prospective data where results are unknown, it makes"
1.3.1,no sense to have y values or weights.
1.3.1,Remove support indices
1.3.1,Remove support indices
1.3.1,Remove support indices
1.3.1,Get task specific entries
1.3.1,Now just get weights for this task
1.3.1,Get task specific entries
1.3.1,Now just get weights for this task
1.3.1,Now just get weights for this task
1.3.1,Now just get weights for this task
1.3.1,Split data into pos and neg lists.
1.3.1,No replacement allowed for supports
1.3.1,Handle one-d vs. non one-d feature matrices
1.3.1,Init the iterator
1.3.1,Set initial iterator state
1.3.1,support = self.supports[task][self.trial_num]
1.3.1,Increment and update logic
1.3.1,Init the iterator
1.3.1,Set initial iterator state
1.3.1,support = self.supports[task][self.trial_num]
1.3.1,Increment and update logic
1.3.1,"By invariant of when this is called, can assume num_samples > 0"
1.3.1,and num_samples < batch_size
1.3.1,Fill in batch arrays
1.3.1,"By invariant of when this is called, can assume num_samples > 0"
1.3.1,and num_samples < batch_size
1.3.1,Fill in batch arrays
1.3.1,Only the first set of copy will be counted in training loss
1.3.1,The -1 indicates that y will be reshaped to have length -1
1.3.1,"Set labels to be zero, with zero weights"
1.3.1,Load obsolete format -> save in new format
1.3.1,note that this corresponds to the _construct_metadata column order
1.3.1,if not len(self.metadata_df):
1.3.1,"raise ValueError(""No data in dataset."")"
1.3.1,return next(self.metadata_df.iterrows())[1]['task_names']
1.3.1,Create temp directory to store resharded version
1.3.1,Write data in new shards
1.3.1,Handle spillover from last shard
1.3.1,These columns may be missing is the dataset is unlabelled.
1.3.1,"(ytz): Depending on the application, thread-based pools may be faster"
1.3.1,"than process based pools, since process based pools need to pickle/serialize"
1.3.1,"objects as an extra overhead. Also, as hideously as un-thread safe this looks,"
1.3.1,we're actually protected by the GIL.
1.3.1,(ytz): this skips everything except possibly the last shard
1.3.1,if data_dir is None:
1.3.1,data_dir = tempfile.mkdtemp()
1.3.1,The -1 indicates that y will be reshaped to have length -1
1.3.1,"raw_data = (X, y, w, ids)"
1.3.1,Get full dataset in memory
1.3.1,Shuffle in memory
1.3.1,Write shuffled shards out to disk
1.3.1,Shuffle the arrays corresponding to each row in metadata_df
1.3.1,TODO (ytz): Under what condition does this exist but the file itself doesn't?
1.3.1,Handle edge case with empty indices
1.3.1,Find indices which rest in this shard
1.3.1,Need to offset indices to fit within shard_size
1.3.1,Handle the case of datasets with y/w missing
1.3.1,Updating counts
1.3.1,Break when all indices have been used up already
1.3.1,TODO(rbharath): Get rid of * import
1.3.1,Load MUV dataset
1.3.1,Do an approximate comparison since splits are sometimes slightly off from
1.3.1,the exact fraction.
1.3.1,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
1.3.1,reloading will cause the transform to be reapplied. This is undesirable in
1.3.1,almost all cases. Need to understand a method to fix this.
1.3.1,def test_shuffle(self):
1.3.1,"""""""Test that datasets can be merged."""""""
1.3.1,current_dir = os.path.dirname(os.path.realpath(__file__))
1.3.1,dataset_file = os.path.join(
1.3.1,"current_dir, ""../../models/tests/example.csv"")"
1.3.1,featurizer = dc.feat.CircularFingerprint(size=1024)
1.3.1,"tasks = [""log-solubility""]"
1.3.1,loader = dc.data.CSVLoader(
1.3.1,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
1.3.1,"dataset = loader.featurize(dataset_file, shard_size=2)"
1.3.1,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
1.3.1,dataset.ids)
1.3.1,orig_len = len(dataset)
1.3.1,dataset.shuffle(iterations=5)
1.3.1,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
1.3.1,dataset.ids)
1.3.1,
1.3.1,assert len(dataset) == orig_len
1.3.1,# The shuffling should have switched up the ordering
1.3.1,"assert not np.array_equal(orig_ids, new_ids)"
1.3.1,# But all the same entries should still be present
1.3.1,assert sorted(orig_ids) == sorted(new_ids)
1.3.1,# All the data should have same shape
1.3.1,assert X_orig.shape == X_new.shape
1.3.1,assert y_orig.shape == y_new.shape
1.3.1,assert w_orig.shape == w_new.shape
1.3.1,The shuffling should have switched up the ordering
1.3.1,But all the same entries should still be present
1.3.1,All the data should have same shape
1.3.1,The ids should now store the performed permutation. Check that the
1.3.1,original dataset is recoverable.
1.3.1,The ids should now store the performed permutation. Check that the
1.3.1,original dataset is recoverable.
1.3.1,Set some global variables up top
1.3.1,Featurize emols dataset
1.3.1,Generate dummy dataset
1.3.1,Generate dummy dataset
1.3.1,Generate dummy dataset
1.3.1,Set last n_samples/2 weights to 0
1.3.1,Check that no support elements are sample from zero-weight samples
1.3.1,Generate dummy dataset
1.3.1,Generate dummy dataset
1.3.1,Create support generator
1.3.1,Generate dummy dataset
1.3.1,Create support generator
1.3.1,Generate dummy dataset
1.3.1,Assert all support elements have been removed
1.3.1,Generate dummy dataset
1.3.1,Assert all remove elements have been removed
1.3.1,Generate dummy dataset
1.3.1,Assert all support elements have been removed
1.3.1,Generate dummy dataset
1.3.1,Assert all remove elements have been removed
1.3.1,Generate dummy dataset
1.3.1,Set last n_samples/2 weights to 0
1.3.1,Sample from first n_samples/2 elements for support
1.3.1,Should lie within first n_samples/2 samples only
1.3.1,Generate dummy dataset
1.3.1,Create support generator
1.3.1,Generate dummy dataset
1.3.1,Test on identity matrix
1.3.1,Generate random sparse features dataset
1.3.1,Test edge case with array of all zeros
1.3.1,Test cases where n_samples < 2*n_samples < batch_size
1.3.1,Test cases where n_samples < batch_size
1.3.1,Test case where n_samples == batch_size
1.3.1,Test case for object featurization.
1.3.1,Test case for more complicated object featurization
1.3.1,Test case with multidimensional data
1.3.1,Test cases where n_samples < 2*n_samples < batch_size
1.3.1,Test cases where n_samples < batch_size
1.3.1,Test case where n_samples == batch_size
1.3.1,Test case for object featurization.
1.3.1,Test case for more complicated object featurization
1.3.1,Test case with multidimensional data
1.3.1,Test first resharding worked
1.3.1,Test second resharding worked
1.3.1,Generate data
1.3.1,Generate data
1.3.1,Generate data
1.3.1,Transform it
1.3.1,Transform it
1.3.1,special case to test
1.3.1,deterministic
1.3.1,non-deterministic
1.3.1,we don't know the order in which the shards are iterated in.
1.3.1,Splits featurized samples into train/test
1.3.1,Splits featurized samples into train/test
1.3.1,Splits featurized samples into train/test
1.3.1,"splittype = ""random"""
1.3.1,Splits featurized samples into train/test
1.3.1,Now perform move
1.3.1,Only for debug!
1.3.1,#Make directories to store the raw and featurized datasets.
1.3.1,Load dataset
1.3.1,Featurize tox21 dataset
1.3.1,###### Do featurization
1.3.1,Do train/valid split.
1.3.1,###### Do singletask load
1.3.1,################# Do comparison
1.3.1,Only for debug!
1.3.1,Set some global variables up top
1.3.1,Make directories to store the raw and featurized datasets.
1.3.1,Load dataset
1.3.1,Featurize tox21 dataset
1.3.1,For debugging purposes
1.3.1,###### Do multitask load
1.3.1,Do train/valid split.
1.3.1,###### Do singletask load
1.3.1,################# Do comparison
1.3.1,"task_type = ""regression"""
1.3.1,coding=utf-8
1.3.1,Note that transformers have to be undone in reversed order
1.3.1,Hack to allow for easy unpickling:
1.3.1,http://stefaanlippens.net/pickleproblem
1.3.1,"One, but not both, transform_X or tranform_y is true"
1.3.1,Use fact that bools add as ints in python
1.3.1,Control for pathological case with no variance.
1.3.1,"Get the reversed shape of z: (..., n_tasks, batch_size)"
1.3.1,Find the task dimension of z
1.3.1,Prevent broadcasting on wrong dimension
1.3.1,BalancingTransformer can only transform weights.
1.3.1,Compute weighting factors from dataset.
1.3.1,Ensure dataset is binary
1.3.1,Remove labels with zero weights
1.3.1,self.w = dataset.w
1.3.1,"TODO (flee2): for transform_y, figure out weights"
1.3.1,"print(""y will not be transformed by CDFTransformer, for now."")"
1.3.1,"print(""Cannot undo CDF Transformer, for now."")"
1.3.1,Need this for transform_y
1.3.1,array = np.transpose(array)
1.3.1,"print(""y will not be transformed by PowerTransformer, for now."")"
1.3.1,"print(""Cannot undo Power Transformer, for now."")"
1.3.1,the tf graph here pick up the (K+1) highest similarity values
1.3.1,and their indices
1.3.1,map the indices to labels
1.3.1,generating batch of data by slicing similarity matrix
1.3.1,into 100*reference_dataset_length
1.3.1,concatenate batches of data together
1.3.1,highest similarity is 1: target is in the reference
1.3.1,use the following K points
1.3.1,"highest less than 1: target not in the reference, use top K points"
1.3.1,calculate matrix multiplicatin on slices
1.3.1,concatenate the slices together
1.3.1,list of calculation orders for DAGs
1.3.1,stemming from one specific atom in the molecule
1.3.1,starting from the adjacency list derived by graphconv featurizer
1.3.1,"number of atoms, also number of DAGs"
1.3.1,"DAG on a molecule with k atoms includes k steps of calculation,"
1.3.1,each step calculating graph features for one atom.
1.3.1,`max_atoms` is the maximum number of steps
1.3.1,each iteration generates the DAG starting from atom with index `count`
1.3.1,"list of lists, elements represent the calculation orders"
1.3.1,for atoms in the current graph
1.3.1,starting from the target atom with index `count`
1.3.1,flags of whether the atom is already included in the DAG
1.3.1,atom `count` is in the DAG
1.3.1,recording number of radial propagation steps
1.3.1,"in the fisrt loop, atoms directly connected to `count` will be added"
1.3.1,"into the DAG(radial=0), then atoms two-bond away from `count`"
1.3.1,will be added in the second loop(radial=1).
1.3.1,atoms i-bond away will be added in i-th loop
1.3.1,"when molecules have separate parts, starting from one part,"
1.3.1,it is not possible to include all atoms.
1.3.1,this break quit the loop when going into such condition
1.3.1,reinitialize targets for next iteration
1.3.1,atoms connected to current_atom
1.3.1,generate the dependency map of current DAG
1.3.1,atoms connected to `current_atoms`(and not included in the DAG)
1.3.1,"are added, and will be the `current_atoms` for next iteration."
1.3.1,"DAG starts from the target atom, calculation should go in reverse"
1.3.1,`edge[1]` is the parent of `edge[0]`
1.3.1,"after this loop, `parents[i]` includes all parents of atom i"
1.3.1,manually adding the atom index into its parents list
1.3.1,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
1.3.1,atoms with less parents(farther from the target atom) come first.
1.3.1,"graph features of atoms without parents will be first calculated,"
1.3.1,then atoms with more parents can be calculated in order
1.3.1,based on previously calculated graph features.
1.3.1,target atom of this DAG will be calculated in the last step
1.3.1,padding with `max_atoms`
1.3.1,padding
1.3.1,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
1.3.1,which is a max_atoms * max_atoms numpy array after padding
1.3.1,Calculate pairwise distance
1.3.1,Masking for valid atom index
1.3.1,Cutoff with threshold Rc
1.3.1,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is a y transformer
1.3.1,Check w is unchanged since this is a y transformer
1.3.1,Check y is now a logarithmic version of itself
1.3.1,Check that untransform does the right thing.
1.3.1,transforming y should raise an exception
1.3.1,transforming w should raise an exception
1.3.1,transforming X should be okay
1.3.1,Check ids are unchanged.
1.3.1,Check y is unchanged since this is a X transformer
1.3.1,Check w is unchanged since this is a y transformer
1.3.1,Check y is now a logarithmic version of itself
1.3.1,Check that untransform does the right thing.
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is a y transformer
1.3.1,Check w is unchanged since this is a y transformer
1.3.1,Check y is now a logarithmic version of itself
1.3.1,Check that untransform does the right thing.
1.3.1,Tests logarithmic data transformer with selection.
1.3.1,Check ids are unchanged.
1.3.1,Check y is unchanged since this is a X transformer
1.3.1,Check w is unchanged since this is a y transformer
1.3.1,Check y is now a logarithmic version of itself
1.3.1,Check that untransform does the right thing.
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is a y transformer
1.3.1,Check w is unchanged since this is a y transformer
1.3.1,"Check that y_t has zero mean, unit std."
1.3.1,Check that untransform does the right thing.
1.3.1,Check ids are unchanged.
1.3.1,Check y is unchanged since this is a X transformer
1.3.1,Check w is unchanged since this is a y transformer
1.3.1,"Check that X_t has zero mean, unit std."
1.3.1,np.set_printoptions(threshold='nan')
1.3.1,Entries with zero std are not normalized
1.3.1,TODO(rbharath): Untransform doesn't work properly for binary feature
1.3.1,vectors. Need to figure out what's wrong here. (low priority)
1.3.1,# Check that untransform does the right thing.
1.3.1,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
1.3.1,Check ids are unchanged.
1.3.1,Check y is unchanged since this is an X transformer
1.3.1,Check w is unchanged since this is an X transformer
1.3.1,Check X is now holding the proper values when sorted.
1.3.1,Test CDF transformer on Gaussian normal dataset.
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is an y transformer
1.3.1,Check w is unchanged since this is an y transformer
1.3.1,Check y is now holding the proper values when sorted.
1.3.1,Check that untransform does the right thing.
1.3.1,Check ids are unchanged.
1.3.1,Check y is unchanged since this is an X transformer
1.3.1,Check w is unchanged since this is an X transformer
1.3.1,Check X is now holding the proper values when sorted.
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is a y transformer
1.3.1,Check w is unchanged since this is a y transformer
1.3.1,Check y is now holding the proper values when sorted.
1.3.1,Check ids are unchanged.
1.3.1,Check y is unchanged since this is an X transformer
1.3.1,Check w is unchanged since this is an X transformer
1.3.1,Check X is now holding the proper values in each column.
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is an X transformer
1.3.1,Check w is unchanged since this is an X transformer
1.3.1,Check y is now holding the proper values in each column.
1.3.1,Check that untransform does the right thing.
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is a w transformer
1.3.1,Check y is unchanged since this is a w transformer
1.3.1,Assert that entries with zero weight retain zero weight
1.3.1,Check that sum of 0s equals sum of 1s in transformed for each task
1.3.1,Check ids are unchanged.
1.3.1,Check X is unchanged since this is a w transformer
1.3.1,Check y is unchanged since this is a w transformer
1.3.1,Assert that entries with zero weight retain zero weight
1.3.1,Check that sum of 0s equals sum of 1s in transformed for each task
1.3.1,TODO(rbharath): Use standard joblib once old-data has been regenerated.
1.3.1,"If gzipped, need to compute extension again"
1.3.1,Tasks are stored in .sdf.csv file
1.3.1,Structures are stored in .sdf file
1.3.1,First line of user-specified CSV *must* be header.
1.3.1,Try older joblib version for legacy files.
1.3.1,First line of user-specified CSV *must* be header.
1.3.1,First line of user-specified CSV *must* be header.
1.3.1,combine dataframes
1.3.1,TODO: mol should be always sanitized when charges are calculated
1.3.1,can't change it now because it would break a lot of examples
1.3.1,working-with-3d-molecules
1.3.1,initial embedding
1.3.1,minimization and pruning
1.3.1,always keep lowest-energy conformer
1.3.1,discard conformers after max_conformers is reached
1.3.1,get RMSD to selected conformers
1.3.1,discard conformers within the RMSD threshold
1.3.1,create a new molecule to hold the chosen conformers
1.3.1,this ensures proper conformer IDs and energy-based ordering
1.3.1,TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
1.3.1,import nglview
1.3.1,import tempfile
1.3.1,import os
1.3.1,import mdtraj as md
1.3.1,import numpy as np
1.3.1,import tempfile
1.3.1,from rdkit import Chem
1.3.1,from rdkit.Chem import Draw
1.3.1,from itertools import islice
1.3.1,"from IPython.display import Image, HTML, display"
1.3.1,
1.3.1,"def combine_mdtraj(protein, ligand):"
1.3.1,chain = protein.topology.add_chain()
1.3.1,"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
1.3.1,for atom in ligand.topology.atoms:
1.3.1,"protein.topology.add_atom(atom.name, atom.element, residue)"
1.3.1,"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
1.3.1,protein.topology.create_standard_bonds()
1.3.1,return protein
1.3.1,
1.3.1,def visualize_complex(complex_mdtraj):
1.3.1,"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
1.3.1,"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
1.3.1,binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
1.3.1,binding_pocket_residues = [str(r) for r in binding_pocket_residues]
1.3.1,"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
1.3.1,
1.3.1,traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
1.3.1,ngltraj = nglview.NGLWidget( traj )
1.3.1,ngltraj.representations = [
1.3.1,"{ ""type"": ""cartoon"", ""params"": {"
1.3.1,"""sele"": ""protein"", ""color"": ""residueindex"""
1.3.1,"} },"
1.3.1,"{ ""type"": ""licorice"", ""params"": {"
1.3.1,"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
1.3.1,"} },"
1.3.1,"{ ""type"": ""ball+stick"", ""params"": {"
1.3.1,"""sele"": ""LIG"""
1.3.1,} }
1.3.1,]
1.3.1,return ngltraj
1.3.1,
1.3.1,def visualize_ligand(ligand_mdtraj):
1.3.1,traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
1.3.1,ngltraj = nglview.NGLWidget( traj )
1.3.1,ngltraj.representations = [
1.3.1,"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
1.3.1,return ngltraj
1.3.1,
1.3.1,def convert_lines_to_mdtraj(molecule_lines):
1.3.1,tempdir = tempfile.mkdtemp()
1.3.1,"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
1.3.1,"with open(molecule_file, ""wb"") as f:"
1.3.1,f.writelines(molecule_lines)
1.3.1,molecule_mdtraj = md.load(molecule_file)
1.3.1,return molecule_mdtraj
1.3.1,
1.3.1,def display_images(filenames):
1.3.1,"""""""Helper to pretty-print images."""""""
1.3.1,imagesList=''.join(
1.3.1,"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
1.3.1,% str(s) for s in sorted(filenames)])
1.3.1,display(HTML(imagesList))
1.3.1,
1.3.1,"def mols_to_pngs(mols, basename=""test""):"
1.3.1,"""""""Helper to write RDKit mols to png files."""""""
1.3.1,filenames = []
1.3.1,"for i, mol in enumerate(mols):"
1.3.1,"filename = ""%s%d.png"" % (basename, i)"
1.3.1,"Draw.MolToFile(mol, filename)"
1.3.1,filenames.append(filename)
1.3.1,return filenames
1.3.1,TODO(rbharath): This is now simple enough that we should probably get rid of
1.3.1,Evaluator object to avoid clutter.
1.3.1,Compute multitask metrics
1.3.1,Compute multitask metrics
1.3.1,Loosening atol to see if tests stop failing sporadically
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,a*x + b*y + c*z = dI think that
1.3.1,"self.x, self.y, self.z = x, y, z"
1.3.1,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
1.3.1,TODO(bramsundar): Should this be __copy__?
1.3.1,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
1.3.1,"return np.array([self.x, self.y, self.z])"
1.3.1,TODO(rbharath): Should this be an atom function?
1.3.1,"This line is necessary for babel to work, though many PDBs in"
1.3.1,the PDB would have this line commented out
1.3.1,now atom type (for pdbqt)
1.3.1,"If atomtype is not specified, but atomname is, set atomtype to the"
1.3.1,"first letter of atomname. This heuristic suffices for proteins,"
1.3.1,since no two-letter elements appear in standard amino acids.
1.3.1,Any number needs to be removed from the element name
1.3.1,"this only uses the rightmost three characters, essentially"
1.3.1,removing unique rotamer identification
1.3.1,"The normal vector to plane is n = [a, b, c]"
1.3.1,We first shift by basepoint (a point on given plane) to make math
1.3.1,simpler. basepoint is given by d/||n||^2 * n
1.3.1,The perpendicular component of diff to plane is
1.3.1,(n^T diff / ||n||^2) * n
1.3.1,if ring is aromatic
1.3.1,"save its indices, center, and normal"
1.3.1,remember protein-ligand pairs we already counted
1.3.1,"if this pair is new, count atoms forming a contact"
1.3.1,"if this pair is new, count atoms forming a contact"
1.3.1,if ring from mol1 is aromatic
1.3.1,...and atom from mol2 is a cation
1.3.1,if angle and distance are correct
1.3.1,count atoms forming a contact
1.3.1,find interacting rings from protein and cations from ligand
1.3.1,find interacting cations from protein and rings from ligand
1.3.1,merge counters
1.3.1,TODO(LESWING)
1.3.1,check if user tries to set removed arguments
1.3.1,list of features that require sanitized molecules
1.3.1,not implemented featurization types
1.3.1,default values
1.3.1,update with cutoffs specified by the user
1.3.1,define methods to calculate available flat features
1.3.1,all methods (flat and voxel) must have the same API:
1.3.1,"f(prot_xyz, prot_rdk, lig_xyz, lig_rdk, distances) -> list of np.ndarrays"
1.3.1,define methods to calculate available voxel features
1.3.1,"each entry is a tuple (is_flat, feature_name)"
1.3.1,list of features that cannot be calculated with specified parameters
1.3.1,this list is used to define <flat/voxel/all>_combined subset
1.3.1,parse provided feature types
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Get the degree id list (which corrects for min_deg)
1.3.1,Get the size of each degree block
1.3.1,Get the the start indices for items in each block
1.3.1,Get the node indices when they are reset when the degree changes
1.3.1,Convert to numpy array
1.3.1,Reorder old atom_features
1.3.1,Reorder old deg lists
1.3.1,Sort membership
1.3.1,Create old to new dictionary. not exactly intuitive
1.3.1,Reorder adjacency lists
1.3.1,Get numpy version of degree list for indexing
1.3.1,"Initialize adj_lists, which supports min_deg = 1 only"
1.3.1,Parse as deg separated
1.3.1,Get indices corresponding to the current degree
1.3.1,Extract and save adjacency list for the current degree
1.3.1,Construct the slice information
1.3.1,Get the cumulative indices after the first index
1.3.1,Set indices with zero sized slices to zero to avoid indexing errors
1.3.1,TODO(rbharath): Can this be removed?
1.3.1,Use random insted of zeros to prevent weird issues with summing to zero
1.3.1,Get atoms by degree
1.3.1,stack the atoms
1.3.1,Sort all atoms by degree.
1.3.1,"Get the size of each atom list separated by molecule id, then by degree"
1.3.1,Get the final size of each degree block
1.3.1,"Get the index at which each degree starts, not resetting after each degree"
1.3.1,And not stopping at any speciic molecule
1.3.1,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
1.3.1,first column telling the start indices of each degree block and the
1.3.1,second colum telling the size of each degree block
1.3.1,Input for tensorflow
1.3.1,Determines the membership (atom i belongs to membership[i] molecule)
1.3.1,"Get the index at which each deg starts, resetting after each degree"
1.3.1,(deg x num_mols) matrix describing the start indices when you count up the atoms
1.3.1,"in the final representation, stopping at each molecule,"
1.3.1,resetting every time the degree changes
1.3.1,Gets the degree resetting block indices for the atoms in each molecule
1.3.1,"Here, the indices reset when the molecules change, and reset when the"
1.3.1,degree changes
1.3.1,Get the degree id lookup list. It allows us to search for the degree of a
1.3.1,molecule mol_id with corresponding atom mol_atom_id using
1.3.1,"deg_id_lists[mol_id,mol_atom_id]"
1.3.1,This is used for convience in the following function (explained below)
1.3.1,Get the degree id (corrected for min_deg) of the considered atom
1.3.1,Return the final index of atom mol_atom_id in molecule mol_id.  Using
1.3.1,"the degree of this atom, must find the index in the molecule's original"
1.3.1,"degree block corresponding to degree id deg_id (second term), and then"
1.3.1,calculate which index this degree block ends up in the final
1.3.1,representation (first term). The sum of the two is the final indexn
1.3.1,Initialize the new degree separated adjacency lists
1.3.1,Update the old adjcency lists with the new atom indices and then combine
1.3.1,all together
1.3.1,Iterate through all the molecules
1.3.1,Get the adjacency lists for this molecule and current degree id
1.3.1,"Correct all atom indices to the final indices, and then save the"
1.3.1,results into the new adjacency lists
1.3.1,Increment once row is done
1.3.1,Get the final aggregated molecule
1.3.1,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
1.3.1,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
1.3.1,consistent with most QM software packages.
1.3.1,Type of data created by this featurizer
1.3.1,TODO(rbharath): Should this return a list?
1.3.1,Type of data created by this featurizer
1.3.1,generate SMILES for fragments
1.3.1,Initalize with 1
1.3.1,Allow 0 index to correspond to null molecule 1
1.3.1,Correct for null
1.3.1,"print(6-k-1, id)"
1.3.1,Correct for last one
1.3.1,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
1.3.1,first `bt_len` features are bond features(if applicable)
1.3.1,`bt_len`-th feature is if the pair of atoms are in the same ring
1.3.1,graph distance between two atoms
1.3.1,Euclidean distance between atoms
1.3.1,atoms `radial` bonds away from `a1`
1.3.1,atoms less than `radial` bonds away
1.3.1,find atoms `radial`+1 bonds away
1.3.1,"Since ConvMol is an object and not a numpy array, need to set dtype to"
1.3.1,object.
1.3.1,Get the node features
1.3.1,Stack nodes into an array
1.3.1,Get bond lists with reverse edges included
1.3.1,Get canonical adjacency list
1.3.1,"Distance is either graph distance(True) or Euclidean distance(False,"
1.3.1,only support datasets providing Cartesian coordinates)
1.3.1,Set dtype
1.3.1,If includes explicit hydrogens
1.3.1,Atom features
1.3.1,Stack nodes into an array
1.3.1,Get bond lists
1.3.1,Get canonical adjacency list
1.3.1,Calculate pair features
1.3.1,atom_name is of format RESX-ATOMTYPE
1.3.1,where X is a 1 to 4 digit number
1.3.1,list-of-available-descriptors.
1.3.1,(ytz): This is done to avoid future compatibility issues like inclusion of
1.3.1,the 3D descriptors or changing the feature size.
1.3.1,check for separate count and SMILES entries for each fragment
1.3.1,TODO test more formats for ligand
1.3.1,some users might try to read smiles with this function
1.3.1,adding hydrogens and charges is tested in dc.utils
1.3.1,3D vector with unit length
1.3.1,"very basic test, we check if rotations actually work in test_rotate_molecules"
1.3.1,check if distances do not change
1.3.1,check if it works for molecules with different numbers of atoms
1.3.1,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
1.3.1,check if correct distance metric was used
1.3.1,"20 points with coords between -5 and 5, centered at 0"
1.3.1,indices are positive
1.3.1,coordinates were properly translated and scaled
1.3.1,for coordinates outside of the box function should properly transform them
1.3.1,to indices and warn the user
1.3.1,"TODO check if function warns. There is assertWarns method in unittest,"
1.3.1,but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
1.3.1,"20 points with coords between -5 and 5, centered at 0"
1.3.1,3 pairs of indices
1.3.1,simple flat ring
1.3.1,load and sanitize two real molecules
1.3.1,FIXME might break with different version of rdkit
1.3.1,FIXME might break with different version of rdkit
1.3.1,parallel normals
1.3.1,perpendicular normals
1.3.1,too far away
1.3.1,perpendicular normals
1.3.1,parallel normals
1.3.1,too far away
1.3.1,order of the molecules shouldn't matter
1.3.1,with this criteria we should find both types of stacking
1.3.1,parallel normals
1.3.1,perpendicular normals
1.3.1,too far away
1.3.1,"TODO find better example, currently dicts are empty"
1.3.1,"TODO find better example, currently dicts are empty"
1.3.1,TODO test if dict contains smiles
1.3.1,check if results are the same if we provide precomputed distances
1.3.1,...but first check if we actually got two dicts
1.3.1,check if we get less features with smaller distance cutoff
1.3.1,ligands are typically small so all atoms might be present
1.3.1,check if using different ecfp_degree changes anything
1.3.1,TODO upperbound?
1.3.1,test if default parameters work
1.3.1,check if use-case from examples works
1.3.1,test if input is flattened when flat features are used
1.3.1,test voxel features
1.3.1,test flat features
1.3.1,check if aromatic features are ignores if sanitize=False
1.3.1,"protein is too big for the box, some features should be missing"
1.3.1,whole ligand should fit in the box
1.3.1,"Note there is a central nitrogen of degree 4, with 4 carbons"
1.3.1,of degree 1 (connected only to central nitrogen).
1.3.1,5 atoms in compound
1.3.1,Get the adjacency lists grouped by degree
1.3.1,The 4 outer atoms connected to central nitrogen
1.3.1,Central nitrogen connected to everything else.
1.3.1,Only one carbon
1.3.1,"No bonds, so degree adjacency lists are empty"
1.3.1,3 carbonds in alkane
1.3.1,Outer two carbonds are connected to central carbon
1.3.1,Central carbon connected to outer two
1.3.1,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
1.3.1,this expected behavior? Need to think about API.
1.3.1,Do a manual distance computation and make
1.3.1,Test with cutoff 0 angstroms. There should be no neighbors in this case.
1.3.1,Test with cutoff 100 angstroms. Everything should be neighbors now.
1.3.1,Do a manual distance computation and ensure that selected neighbor is
1.3.1,closest since we set max_num_neighbors = 1
1.3.1,Splits featurized samples into train/test
1.3.1,Artificial feature array.
1.3.1,0 atoms of degree 0
1.3.1,0 atoms of degree 1
1.3.1,4 atoms of degree 2
1.3.1,0 atoms of degree 3
1.3.1,0 atoms of degree 4
1.3.1,0 atoms of degree 5
1.3.1,0 atoms of degree 6
1.3.1,0 atoms of degree 7
1.3.1,0 atoms of degree 8
1.3.1,0 atoms of degree 9
1.3.1,0 atoms of degree 10
1.3.1,atom 4 has 0 neighbors
1.3.1,atom 0 has 2 neighbors
1.3.1,atom 1 has 2 neighbors
1.3.1,atom 2 has 2 neighbors
1.3.1,atom 3 has 3 neighbors.
1.3.1,Verify that atom features have been sorted by atom degree.
1.3.1,Sorting is done by atom degree as before. So the ordering goes
1.3.1,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
1.3.1,from new position to old position is
1.3.1,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
1.3.1,list respects this reordering and returns correct adjacency list.
1.3.1,### First example molecule
1.3.1,Artificial feature array.
1.3.1,### Second example molecule
1.3.1,## Third example molecule
1.3.1,Test agglomerate molecule method
1.3.1,No atoms of degree 0
1.3.1,3 atoms of degree 1
1.3.1,8 atoms of degree 2
1.3.1,1 atom of degree 3
1.3.1,0 atoms of degree 4
1.3.1,0 atoms of degree 5
1.3.1,Check that atoms are only connected to themselves.
1.3.1,Check that there's one atom of each degree.
1.3.1,assumes that every array is of the same dimension
1.3.1,rem_dataset is remaining portion of dataset
1.3.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.1,to k-1.
1.3.1,dict is needed in case groups aren't strictly flattened or
1.3.1,hashed by something non-integer like
1.3.1,returns list of per column sum of non zero elements
1.3.1,Compute number of actives needed per task.
1.3.1,loop through each column and obtain index required to splice out for
1.3.1,required fraction of hits
1.3.1,Find the first index where the cumulative number of actives equals
1.3.1,the actives_count
1.3.1,Note that np.where tells us last index required to exceed
1.3.1,"actives_count, so we actually want the following location"
1.3.1,TODO(rbharath): Refactor this split method to match API of other splits (or
1.3.1,potentially refactor those to match this.
1.3.1,Handle edge case where frac_split is 1
1.3.1,Create weight matrices fpor two haves.
1.3.1,copy over up to required index for weight first_split
1.3.1,check out if any rows in either w_1 or w_2 are just zeros
1.3.1,"Obtain original x, y, and w arrays and shuffle"
1.3.1,calculate percent split for valid (out of test and valid)
1.3.1,"split test data into valid and test, treating sub test set also as sparse"
1.3.1,rem_dataset is remaining portion of dataset
1.3.1,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.1,to k-1.
1.3.1,JSG Assert that split fractions can be written as proper fractions over 10.
1.3.1,This can be generalized in the future with some common demoninator determination.
1.3.1,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.3.1,Append remaining examples to train
1.3.1,Sort by increasing MW
1.3.1,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.3.1,for m_idx in cluster:
1.3.1,"continue until we find an active in all the tasks, otherwise we can't"
1.3.1,compute a meaningful AUC
1.3.1,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.3.1,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.3.1,Sort from largest to smallest scaffold sets
1.3.1,Pick the mol closest to everything as the first element of training
1.3.1,Pick the closest mol from what is left
1.3.1,Test is everything else
1.3.1,All datasets share features and identifiers by assumption.
1.3.1,TODO(rbharath): Get rid of * import
1.3.1,Note that the extra task goes to test
1.3.1,Number tasks per fold
1.3.1,Find the tasks that correspond to this test fold
1.3.1,Assert that all arrays look like they should
1.3.1,0 1 2 3 4 5 6 7 8 9
1.3.1,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
1.3.1,data. Make a test for properly splitting of sharded data. Perhaps using
1.3.1,reshard() to handle this?
1.3.1,Verify lengths is 10/k == 2
1.3.1,Verify that compounds in this fold are subset of original compounds
1.3.1,Verify that no two folds have overlapping compounds.
1.3.1,Verify lengths is 10/k == 2
1.3.1,Verify that compounds in this fold are subset of original compounds
1.3.1,Verify that no two folds have overlapping compounds.
1.3.1,Verify lengths is 10/k == 2
1.3.1,Verify that compounds in this fold are subset of original compounds
1.3.1,Verify that no two folds have overlapping compounds.
1.3.1,Test singletask case.
1.3.1,The split index should partition dataset in half.
1.3.1,Test singletask case.
1.3.1,Test case where some weights are zero (i.e. masked)
1.3.1,Set half the positives to have zero weight
1.3.1,There are 10 nonzero actives.
1.3.1,"The split index should partition this into half, so expect 5"
1.3.1,The split index should partition dataset in half.
1.3.1,Mask half the examples
1.3.1,The split index should partition dataset in half.
1.3.1,Test singletask case.
1.3.1,Should have split cleanly in half (picked random seed to ensure this)
1.3.1,Check positives are correctly distributed
1.3.1,Verify lengths is 100/k == 20
1.3.1,Note: This wouldn't work for multitask str
1.3.1,assert len(fold_dataset) == n_samples/K
1.3.1,Verify that each fold has n_positives/K = 4 positive examples.
1.3.1,Verify that compounds in this fold are subset of original compounds
1.3.1,Verify that no two folds have overlapping compounds.
1.3.1,sparsity is determined by number of w weights that are 0 for a given
1.3.1,task structure of w np array is such that each row corresponds to a
1.3.1,sample. The loaded sparse dataset has many rows with only zeros
1.3.1,verify that there are no rows (samples) in weights matrix w
1.3.1,that have no hits.
1.3.1,Path to save checkpoint files
1.3.1,first layer in model: check that it is an input layer
1.3.1,Add losses to graph
1.3.1,Loss for each batch element
1.3.1,Loss should be a float
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Save an initial checkpoint.
1.3.1,TODO(rbharath): Don't support example weighting yet.
1.3.1,Always save a final checkpoint when complete.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Arguments
1.3.1,Returns
1.3.1,Arguments
1.3.1,Returns
1.3.1,Arguments
1.3.1,Returns
1.3.1,Arguments
1.3.1,Returns
1.3.1,task_metadata_rows = {task: [] for task in tasks}
1.3.1,Extract those datapoints which are present for this task
1.3.1,Loading is done on-the-fly
1.3.1,TODO(rbharath/enf): We need a structured way to deal with potential GPU
1.3.1,memory overflows.
1.3.1,Discard any padded predictions
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,Special case to handle singletasks.
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,Calculate pairwise distance
1.3.1,Masking for valid atom index
1.3.1,Cutoff with threshold Rc
1.3.1,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
1.3.1,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
1.3.1,optimization to allow for tensorcontraction/broadcasted mmul
1.3.1,using a reshape trick. Note that the np and tf matmul behavior
1.3.1,differs when dealing with broadcasts
1.3.1,-*- coding: UTF-8 -*-
1.3.1,Reshape everything to match the input with the most dimensions.
1.3.1,"This probably means the variable hasn't been created yet, so try again"
1.3.1,with reuse set to false.
1.3.1,"H(x), with same number of input and output channels"
1.3.1,"T(x), with same number of input and output channels"
1.3.1,Calculate what the new shape will be.
1.3.1,"Shape (N_atoms, M_nbrs, ndim)"
1.3.1,"Shape (N_atoms, M_nbrs, ndim)"
1.3.1,"Shape (N_atoms, M_nbrs)"
1.3.1,"This probably means the variable hasn't been created yet, so try again"
1.3.1,with reuse set to false.
1.3.1,"This probably means the variable hasn't been created yet, so try again"
1.3.1,with reuse set to false.
1.3.1,"This probably means the variable hasn't been created yet, so try again"
1.3.1,with reuse set to false.
1.3.1,"This probably means the variable hasn't been created yet, so try again"
1.3.1,with reuse set to false.
1.3.1,TODO(rbharath): Note sure if this layer can be called with __call__
1.3.1,"meaningfully, so not going to support that functionality for now."
1.3.1,"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.3.1,Generate the nb_affine weights and biases
1.3.1,Extract atom_features
1.3.1,Extract graph topology
1.3.1,Perform the mol conv
1.3.1,"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
1.3.1,"self.max_deg, self.min_deg, self.W_list,"
1.3.1,self.b_list)
1.3.1,Sum all neighbors using adjacency matrix
1.3.1,Get collection of modified atom features
1.3.1,Obtain relevant atoms for this degree
1.3.1,Get self atoms
1.3.1,Apply hidden affine to relevant atoms and append
1.3.1,Determine the min_deg=0 case
1.3.1,Only use the self layer
1.3.1,Combine all atoms back into the list
1.3.1,Tensorflow correctly processes empty lists when using concat
1.3.1,"Sum along neighbors as well as self, and store"
1.3.1,Perform the mol gather
1.3.1,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
1.3.1,"self.max_degree, self.min_degree)"
1.3.1,Tensorflow correctly processes empty lists when using concat
1.3.1,Get self atoms
1.3.1,Expand dims
1.3.1,always deg-1 for deg_adj_lists
1.3.1,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.3.1,Extract graph topology
1.3.1,Perform the mol gather
1.3.1,Obtain the partitions for each of the molecules
1.3.1,Sum over atoms for each molecule
1.3.1,Get the final sparse representations
1.3.1,No other forget biases supported right now.
1.3.1,Taken from Keras code [citation needed]
1.3.1,"x is test set, xp is support set."
1.3.1,# Initializes trainable weights.
1.3.1,## Performs computations
1.3.1,Get initializations
1.3.1,Process using attention
1.3.1,"Eqn (4), appendix A.1 of Matching Networks paper"
1.3.1,Generate new attention states
1.3.1,Support set lstm
1.3.1,Test lstm
1.3.1,self.build()
1.3.1,Get initializations
1.3.1,Rename support
1.3.1,Process support xp using attention
1.3.1,Get linear combination of support set
1.3.1,Process test x using attention
1.3.1,Generate new support attention states
1.3.1,Generate new test attention states
1.3.1,Redefine
1.3.1,Number of rotatable bonds
1.3.1,TODO(rbharath): Vina actually sets this per-molecule. See if makes
1.3.1,a difference.
1.3.1,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
1.3.1,neighbors lists an argument instead of a part of this layer.
1.3.1,"Shape (N, M)"
1.3.1,"Shape (N, M)"
1.3.1,"Shape (N, M)"
1.3.1,Number of grid cells
1.3.1,TODO(rbharath): Support batching
1.3.1,"Shape (n_cells, ndim)"
1.3.1,"List of length N_atoms, each element of different length uniques_i"
1.3.1,"List of length N_atoms, each element of different length uniques_i"
1.3.1,"List of length N_atoms, each a tensor of shape"
1.3.1,"(uniques_i, ndim)"
1.3.1,Add phantom atoms that exist far outside the box
1.3.1,"List of length N_atoms, each of shape (1, ndim)"
1.3.1,TODO(rbharath): How does distance need to be modified here to
1.3.1,account for periodic boundary conditions?
1.3.1,List of length N_atoms each of shape (M_nbrs)
1.3.1,"N_atoms elts of size (M_nbrs,) each"
1.3.1,"Shape (N_atoms, 1)"
1.3.1,Find M_nbrs atoms closest to each cell
1.3.1,"Shape (n_cells, M_nbrs)"
1.3.1,Associate each cell with its neighbor cells. Assumes periodic boundary
1.3.1,"conditions, so does wrapround. O(constant)"
1.3.1,"Shape (n_cells, n_nbr_cells)"
1.3.1,"Shape (N_atoms, n_nbr_cells)"
1.3.1,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
1.3.1,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
1.3.1,"List of length N_atoms, each element length uniques_i"
1.3.1,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
1.3.1,element removed to remove self from list of neighbors. Need to verify
1.3.1,this holds more broadly or come up with robust alternative.
1.3.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.3.1,"Shape (N_atoms*n_cells, ndim) after tile"
1.3.1,Shape (N_atoms*n_cells)
1.3.1,"Shape (n_cells, N_atoms)"
1.3.1,Find k atoms closest to this cell. Notice negative sign since
1.3.1,tf.nn.top_k returns *largest* not smallest.
1.3.1,"Tensor of shape (n_cells, M_nbrs)"
1.3.1,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.3.1,"Shape (N_atoms*n_cells, 1) after tile"
1.3.1,9 neighbors in 2-space
1.3.1,TODO(rbharath): Shoddy handling of higher dimensions...
1.3.1,Number of cells for cube in 3-space is
1.3.1,TODO(rbharath): Do we need to handle periodic boundary conditions
1.3.1,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.3.1,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
1.3.1,the cube.
1.3.1,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.3.1,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.3.1,"Tile (a, a, a, b, b, b, etc.)"
1.3.1,"Tile (a, b, c, a, b, c, ...)"
1.3.1,N: Maximum number of atoms
1.3.1,M: Maximum number of neighbors
1.3.1,d: Number of coordinates/features/filters
1.3.1,B: Batch Size
1.3.1,We apply the radial pooling filter before atom type conv
1.3.1,to reduce computation
1.3.1,check that there isnt just one or zero inputs
1.3.1,create subspaces
1.3.1,create the alpha learnable parameters
1.3.1,"concatenate subspaces, reshape to size of original input, then stack"
1.3.1,"such that out_tensor has shape (2,?,original_cols)"
1.3.1,creates subspaces the same way it was done in AlphaShare
1.3.1,calculate squared Frobenius norm
1.3.1,"(TODO YTZ:) faster, less memory intensive way"
1.3.1,"r = tf.reduce_sum(tf.square(coordinates), 2)"
1.3.1,"r = tf.expand_dims(r, -1)"
1.3.1,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
1.3.1,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
1.3.1,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
1.3.1,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
1.3.1,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
1.3.1,Calculate pairwise distance
1.3.1,Cutoff with threshold Rc
1.3.1,return d
1.3.1,tf.stack issues again...
1.3.1,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
1.3.1,We do not need the mask because every graph has self.num_vertices vertices now
1.3.1,So the Tensor has known dimensions
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.3.1,and embeddings of atom j(both gone through a hidden layer)
1.3.1,"for atom i, sum the influence from all other atom j in the molecule"
1.3.1,number of inputs each step
1.3.1,Add trainable weights
1.3.1,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.3.1,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.3.1,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.3.1,initialize graph features for each graph
1.3.1,initialize graph features for each graph
1.3.1,another row of zeros is generated for padded dummy atoms
1.3.1,`count`-th step
1.3.1,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.3.1,generating index for graph features used in the inputs
1.3.1,"extracting graph features for parents of the target atoms, then flatten"
1.3.1,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.3.1,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.3.1,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.3.1,of shape: (batch_size*max_atoms) * n_graph_features
1.3.1,representing the graph features of target atoms in each graph
1.3.1,index for targe atoms
1.3.1,update the graph features for target atoms
1.3.1,Add trainable weights
1.3.1,Extract atom_features
1.3.1,Extract atom_features
1.3.1,sum all graph outputs
1.3.1,"Default message function: edge network, update function: GRU"
1.3.1,more options to be implemented
1.3.1,Extract atom_features
1.3.1,Add trainable weights
1.3.1,Extract atom_features
1.3.1,Add another value(~-Inf) to prevent error in softmax
1.3.1,Model using this layer must set pad_batches=True
1.3.1,Perform one step of LSTM
1.3.1,Layer Management
1.3.1,Singular place to hold Tensor objects which don't serialize
1.3.1,These have to be reconstructed on restoring from pickle
1.3.1,See TensorGraph._get_tf() for more details on lazy construction
1.3.1,"Don't let this thread get ahead of the enqueue thread, since if"
1.3.1,"we try to read more batches than the total number that get queued,"
1.3.1,this thread will hang indefinitely.
1.3.1,Gather results for each output
1.3.1,"If only one output, just return array"
1.3.1,Ensure all training operators have been created.
1.3.1,Initialize variables.
1.3.1,"As a sanity check, make sure all tensors have the correct shape."
1.3.1,Remove out_tensor from the object to be pickled
1.3.1,Pickle itself
1.3.1,add out_tensor back to everyone
1.3.1,The loss doesn't depend on any variables.
1.3.1,Set by variable constructor.
1.3.1,Set by set_variable_initial_values().
1.3.1,Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
1.3.1,Optimize the main loss.  This should send both variables toward 1.
1.3.1,Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
1.3.1,See if it has done a plausible job of learning the distribution.
1.3.1,We have to set the gradient penalty very small because the generator's
1.3.1,"output is only a single number, so the default penalty would constrain"
1.3.1,it far too much.
1.3.1,See if it has done a plausible job of learning the distribution.
1.3.1,"This isn't a meaningful loss, but just for test"
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.1,"Now an (N, M) shape"
1.3.1,TODO(rbharath): Move this into a model directly
1.3.1,def test_vina(self):
1.3.1,"""""""Test that vina graph can be constructed in TensorGraph."""""""
1.3.1,N_protein = 4
1.3.1,N_ligand = 1
1.3.1,N_atoms = 5
1.3.1,M_nbrs = 2
1.3.1,ndim = 3
1.3.1,start = 0
1.3.1,stop = 4
1.3.1,nbr_cutoff = 1
1.3.1,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
1.3.1,start))
1.3.1,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
1.3.1,start))
1.3.1,y = NumpyDataset(np.random.rand(
1.3.1,"1,))"
1.3.1,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
1.3.1,"# used in the current implementation. This is obviously wrong, but need"
1.3.1,# to dig out why this is happening.
1.3.1,"prot_coords = Feature(shape=(N_protein, ndim))"
1.3.1,"ligand_coords = Feature(shape=(N_ligand, ndim))"
1.3.1,"labels = Label(shape=(1,))"
1.3.1,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
1.3.1,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
1.3.1,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
1.3.1,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
1.3.1,"# Now an (N, M) shape"
1.3.1,nbr_list = NeighborList(
1.3.1,"N_protein + N_ligand,"
1.3.1,"M_nbrs,"
1.3.1,"ndim,"
1.3.1,"nbr_cutoff,"
1.3.1,"start,"
1.3.1,"stop,"
1.3.1,in_layers=[coords])
1.3.1,"# Shape (N, M)"
1.3.1,dists = InteratomicL2Distances(
1.3.1,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
1.3.1,repulsion = VinaRepulsion(in_layers=[dists])
1.3.1,hydrophobic = VinaHydrophobic(in_layers=[dists])
1.3.1,hbond = VinaHydrogenBond(in_layers=[dists])
1.3.1,gauss_1 = VinaGaussianFirst(in_layers=[dists])
1.3.1,gauss_2 = VinaGaussianSecond(in_layers=[dists])
1.3.1,"# Shape (N, M)"
1.3.1,interactions = WeightedLinearCombo(
1.3.1,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
1.3.1,"# Shape (N, M)"
1.3.1,"thresholded = Cutoff(in_layers=[dists, interactions])"
1.3.1,"# Shape (N, M)"
1.3.1,free_energies = VinaNonlinearity(in_layers=[thresholded])
1.3.1,free_energy = ReduceSum(in_layers=[free_energies])
1.3.1,"loss = L2Loss(in_layers=[free_energy, labels])"
1.3.1,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
1.3.1,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
1.3.1,tg.set_loss(loss)
1.3.1,tg.fit_generator(databag.iterbatches(epochs=1))
1.3.1,TODO(rbharath): This test should pass. Fix it!
1.3.1,def test_graph_pool(self):
1.3.1,"""""""Test that GraphPool can be invoked."""""""
1.3.1,out_channels = 2
1.3.1,"n_atoms = 4 # In CCC and C, there are 4 atoms"
1.3.1,"raw_smiles = ['CCC', 'C']"
1.3.1,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
1.3.1,featurizer = ConvMolFeaturizer()
1.3.1,mols = featurizer.featurize(mols)
1.3.1,multi_mol = ConvMol.agglomerate_mols(mols)
1.3.1,atom_features = multi_mol.get_atom_features()
1.3.1,degree_slice = multi_mol.deg_slice
1.3.1,membership = multi_mol.membership
1.3.1,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
1.3.1,with self.test_session() as sess:
1.3.1,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
1.3.1,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
1.3.1,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
1.3.1,deg_adjs_tf = []
1.3.1,for deg_adj in deg_adjs:
1.3.1,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
1.3.1,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
1.3.1,out_tensor = GraphPool(out_channels)(*args)
1.3.1,sess.run(tf.global_variables_initializer())
1.3.1,out_tensor = out_tensor.eval()
1.3.1,"assert out_tensor.shape == (n_atoms, out_channels)"
1.3.1,TODO(rbharath): Why is it 2*n_features instead of n_features?
1.3.1,Train the model on random sequences.  We aren't training long enough to
1.3.1,"really make it reliable, but I want to keep this test fast, and it should"
1.3.1,still be able to reproduce a reasonable fraction of input sequences.
1.3.1,Test it out.
1.3.1,Check that it got at least a quarter of them correct.
1.3.1,Actually training a VAE takes far too long for a unit test.  Just run a
1.3.1,"few steps of training to make sure nothing crashes, then check that the"
1.3.1,results are at least internally consistent.
1.3.1,use central difference since forward difference has a pretty high
1.3.1,approximation error
1.3.1,assert min_coords[1][0] != new_x[3]
1.3.1,assert min_coords[1][1] != new_x[4]
1.3.1,assert min_coords[1][2] != new_x[5]
1.3.1,Create the inputs.
1.3.1,Create the generator.
1.3.1,Create the discriminator.
1.3.1,Make a copy of the discriminator that takes the generator's output as
1.3.1,its input.
1.3.1,Make a list of all layers in the generator and discriminator.
1.3.1,Create submodels for training the generator and discriminator.
1.3.1,"Every call to fit_generator() will increment global_step, but we only"
1.3.1,"want it to get incremented once for the entire batch, so record the"
1.3.1,value and keep resetting it.
1.3.1,Train the discriminator.
1.3.1,Train the generator.
1.3.1,Write checkpoints and report progress.
1.3.1,Write out final results.
1.3.1,number of atoms in each molecule
1.3.1,index of pair features
1.3.1,number of pairs for each atom
1.3.1,atom features
1.3.1,pair features
1.3.1,calculation orders for a batch of molecules
1.3.1,padding atom features vector of each molecule with 0
1.3.1,Gather results for each output
1.3.1,Recording the number of samples in the input batch
1.3.1,GraphConvTensorGraph constantly outputs batch_size number of
1.3.1,"results, only valid samples should be appended to final results"
1.3.1,"If only one output, just return array"
1.3.1,Returns:
1.3.1,Concatenates along 0-th dimension
1.3.1,Returns:
1.3.1,Build placeholders
1.3.1,w_b act as the indicator of unique samples in the batch
1.3.1,number of atoms in each molecule
1.3.1,index of pair features
1.3.1,number of pairs for each atom
1.3.1,atom features
1.3.1,pair features
1.3.1,MPNN only accept padded input
1.3.1,MPNN only accept padded input
1.3.1,Extract number of unique samples in the batch from w_b
1.3.1,Only fetch the first set of unique samples
1.3.1,import tensorflow as tf
1.3.1,from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
1.3.1,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
1.3.1,
1.3.1,
1.3.1,class WeightedError(Layer):
1.3.1,
1.3.1,"def __call__(self, *parents):"
1.3.1,"entropy, weights = parents[0], parents[1]"
1.3.1,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
1.3.1,return self.out_tensor
1.3.1,
1.3.1,
1.3.1,"def tensorGraphMultitaskClassifier(n_tasks,"
1.3.1,"n_features,"
1.3.1,"layer_sizes=[500],"
1.3.1,"bypass_layer_sizes=[100],"
1.3.1,model_dir=None):
1.3.1,""""""""
1.3.1,TODO(LESWING) Add Dropout and regularization
1.3.1,
1.3.1,Parameters
1.3.1,----------
1.3.1,n_tasks
1.3.1,n_features
1.3.1,layer_sizes
1.3.1,bypass_layer_sizes
1.3.1,model_dir
1.3.1,
1.3.1,Returns
1.3.1,-------
1.3.1,
1.3.1,""""""""
1.3.1,g = MultiTaskTensorGraph(model_dir=model_dir)
1.3.1,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
1.3.1,g.add_layer(in_layer)
1.3.1,g.add_feature(in_layer)
1.3.1,
1.3.1,# Shared Dense Layers
1.3.1,prev_layer = in_layer
1.3.1,dense_layers = []
1.3.1,for i in range(len(layer_sizes)):
1.3.1,dense = Dense(
1.3.1,"out_channels=layer_sizes[i],"
1.3.1,"name=""SDENSE%s"" % i,"
1.3.1,activation_fn=tf.nn.relu)
1.3.1,"g.add_layer(dense, parents=[prev_layer])"
1.3.1,dense_layers.append(dense)
1.3.1,prev_layer = dense
1.3.1,
1.3.1,# Individual Bypass Layers
1.3.1,costs = []
1.3.1,for task in range(n_tasks):
1.3.1,prev_layer = in_layer
1.3.1,for i in range(len(bypass_layer_sizes)):
1.3.1,dense = Dense(
1.3.1,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
1.3.1,"g.add_layer(dense, parents=[prev_layer])"
1.3.1,prev_layer = dense
1.3.1,"joined_layer = Concat(name=""JOIN%s"" % task)"
1.3.1,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
1.3.1,
1.3.1,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
1.3.1,"g.add_layer(classification, parents=[joined_layer])"
1.3.1,
1.3.1,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
1.3.1,"g.add_layer(softmax, parents=[classification])"
1.3.1,g.add_output(softmax)
1.3.1,
1.3.1,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
1.3.1,g.add_layer(label)
1.3.1,g.add_label(label)
1.3.1,
1.3.1,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
1.3.1,"g.add_layer(cost, parents=[label, classification])"
1.3.1,costs.append(cost)
1.3.1,
1.3.1,"entropy = Concat(name=""ENT"")"
1.3.1,"g.add_layer(entropy, parents=costs)"
1.3.1,
1.3.1,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
1.3.1,g.add_layer(task_weights)
1.3.1,g.set_task_weights(task_weights)
1.3.1,
1.3.1,"loss = WeightedError(name=""ERROR"")"
1.3.1,"g.add_layer(loss, parents=[entropy, task_weights])"
1.3.1,g.set_loss(loss)
1.3.1,
1.3.1,return g
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,(ytz): this is really dirty but needed for restoring models
1.3.1,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
1.3.1,SMILES strings
1.3.1,Maximum length is expanded to allow length variation during train and inference
1.3.1,'_' served as delimiter and padding
1.3.1,Initialize common characters as keys
1.3.1,Include space to avoid extra keys
1.3.1,"For 'Cl', 'Br', etc."
1.3.1,"Character not recognized, add to extra_keys"
1.3.1,Add all extra_keys to char_dict
1.3.1,Character embedding
1.3.1,Multiple convolutional layers with different filter widths
1.3.1,Max-over-time pooling
1.3.1,Concat features from all filters(one feature per filter)
1.3.1,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
1.3.1,Transform SMILES string to integer vectors
1.3.1,Skip all spaces
1.3.1,"For 'Cl', 'Br', etc."
1.3.1,Padding with '_'
1.3.1,Do a simple greedy search.
1.3.1,Do a beam search with length normalization.
1.3.1,"Represent each candidate as (normalized prob, raw prob, sequence)"
1.3.1,This candidate sequence has already been terminated
1.3.1,Consider all possible tokens we could add to this candidate sequence.
1.3.1,update model with best param
1.3.1,Find optimal n_estimators based on original learning_rate
1.3.1,and early_stopping_rounds
1.3.1,"Since test size is 20%, when retrain model to whole data, expect"
1.3.1,n_estimator increased to 1/0.8 = 1.25 time.
1.3.1,Make sure user specified params are in the grid.
1.3.1,Change params back original params
1.3.1,TODO (LESWING) Lazy Load
1.3.1,TODO (LESWING) Lazy Load
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Check same predictions are made.
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Load trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Load trained model
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on train/test
1.3.1,Fit trained model
1.3.1,Eval model on train/test
1.3.1,Fit trained model
1.3.1,Eval model on train/test
1.3.1,Test Parameter getting and setting
1.3.1,Fit trained model
1.3.1,Eval model on train/test
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,n_samples = 100
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,n_samples = 100
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Gather Projection
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Gather Projection
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,Load mini log-solubility dataset.
1.3.1,Add layers
1.3.1,"output will be (n_atoms, 64)"
1.3.1,Need to add batch-norm separately to test/support due to differing
1.3.1,shapes.
1.3.1,"output will be (n_atoms, 64)"
1.3.1,"output will be (n_atoms, 64)"
1.3.1,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.3.1,n_pos/n_neg accordingly.
1.3.1,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.3.1,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.3.1,can measure model has memorized support).  Replacement is turned off to
1.3.1,ensure that support contains full training set. This checks that the
1.3.1,model has mastered memorization of provided support.
1.3.1,#################################################### DEBUG
1.3.1,TODO(rbharath): Check if something went wrong here...
1.3.1,Measure performance on 0-th task.
1.3.1,assert scores[0] > .9
1.3.1,#################################################### DEBUG
1.3.1,Load mini log-solubility dataset.
1.3.1,Add layers
1.3.1,"output will be (n_atoms, 64)"
1.3.1,Need to add batch-norm separately to test/support due to differing
1.3.1,shapes.
1.3.1,"output will be (n_atoms, 64)"
1.3.1,"output will be (n_atoms, 64)"
1.3.1,Apply an attention lstm layer
1.3.1,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.3.1,n_pos/n_neg accordingly.
1.3.1,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.3.1,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.3.1,can measure model has memorized support).  Replacement is turned off to
1.3.1,ensure that support contains full training set. This checks that the
1.3.1,model has mastered memorization of provided support.
1.3.1,Measure performance on 0-th task.
1.3.1,#################################################### DEBUG
1.3.1,TODO(rbharath): Check if something went wrong here...
1.3.1,Measure performance on 0-th task.
1.3.1,assert scores[0] > .85
1.3.1,#################################################### DEBUG
1.3.1,Load mini log-solubility dataset.
1.3.1,Add layers
1.3.1,"output will be (n_atoms, 64)"
1.3.1,Need to add batch-norm separately to test/support due to differing
1.3.1,shapes.
1.3.1,"output will be (n_atoms, 64)"
1.3.1,"output will be (n_atoms, 64)"
1.3.1,Apply a residual lstm layer
1.3.1,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.3.1,n_pos/n_neg accordingly.
1.3.1,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.3.1,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.3.1,can measure model has memorized support).  Replacement is turned off to
1.3.1,ensure that support contains full training set. This checks that the
1.3.1,model has mastered memorization of provided support.
1.3.1,Measure performance on 0-th task.
1.3.1,#################################################### DEBUG
1.3.1,TODO(rbharath): Check if something went wrong here...
1.3.1,Measure performance on 0-th task.
1.3.1,assert scores[0] > .9
1.3.1,#################################################### DEBUG
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on train
1.3.1,def test_singletask_to_multitask_classification(self):
1.3.1,n_features = 10
1.3.1,n_tasks = 17
1.3.1,tasks = range(n_tasks)
1.3.1,# Define train dataset
1.3.1,n_train = 100
1.3.1,"X_train = np.random.rand(n_train, n_features)"
1.3.1,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
1.3.1,w_train = np.ones_like(y_train)
1.3.1,"ids_train = [""C""] * n_train"
1.3.1,train_dataset = dc.data.DiskDataset.from_numpy(
1.3.1,"X_train, y_train, w_train, ids_train)"
1.3.1,# Define test dataset
1.3.1,n_test = 10
1.3.1,"X_test = np.random.rand(n_test, n_features)"
1.3.1,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
1.3.1,w_test = np.ones_like(y_test)
1.3.1,"ids_test = [""C""] * n_test"
1.3.1,test_dataset = dc.data.DiskDataset.from_numpy(
1.3.1,"X_test, y_test, w_test, ids_test)"
1.3.1,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
1.3.1,def model_builder(model_dir):
1.3.1,sklearn_model = LogisticRegression()
1.3.1,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.3.1,multitask_model = dc.models.SingletaskToMultitask(
1.3.1,"tasks, model_builder)"
1.3.1,# Fit trained model
1.3.1,multitask_model.fit(train_dataset)
1.3.1,multitask_model.save()
1.3.1,# Eval multitask_model on train/test
1.3.1,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
1.3.1,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
1.3.1,Generate data
1.3.1,Cleanup
1.3.1,Generate dummy dataset
1.3.1,Fit trained model
1.3.1,Eval model on test
1.3.1,Eval model on train
1.3.1,Fit trained model
1.3.1,Eval model on test
1.3.1,Fit trained model
1.3.1,Eval model on test
1.3.1,def test_sklearn_classification(self):
1.3.1,"""""""Test that sklearn models can learn on simple classification datasets."""""""
1.3.1,np.random.seed(123)
1.3.1,dataset = sklearn.datasets.load_digits(n_class=2)
1.3.1,"X, y = dataset.data, dataset.target"
1.3.1,frac_train = .7
1.3.1,n_samples = len(X)
1.3.1,n_train = int(frac_train*n_samples)
1.3.1,"X_train, y_train = X[:n_train], y[:n_train]"
1.3.1,"X_test, y_test = X[n_train:], y[n_train:]"
1.3.1,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
1.3.1,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
1.3.1,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.3.1,sklearn_model = LogisticRegression()
1.3.1,model = dc.models.SklearnModel(sklearn_model)
1.3.1,# Fit trained model
1.3.1,model.fit(train_dataset)
1.3.1,model.save()
1.3.1,# Eval model on test
1.3.1,"scores = model.evaluate(test_dataset, [classification_metric])"
1.3.1,assert scores[classification_metric.name] > .5
1.3.1,def test_sklearn_multitask_classification(self):
1.3.1,"""""""Test that sklearn models can learn on simple multitask classification."""""""
1.3.1,np.random.seed(123)
1.3.1,n_tasks = 4
1.3.1,tasks = range(n_tasks)
1.3.1,dataset = sklearn.datasets.load_digits(n_class=2)
1.3.1,"X, y = dataset.data, dataset.target"
1.3.1,"y = np.reshape(y, (len(y), 1))"
1.3.1,y = np.hstack([y] * n_tasks)
1.3.1,
1.3.1,frac_train = .7
1.3.1,n_samples = len(X)
1.3.1,n_train = int(frac_train*n_samples)
1.3.1,"X_train, y_train = X[:n_train], y[:n_train]"
1.3.1,"X_test, y_test = X[n_train:], y[n_train:]"
1.3.1,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
1.3.1,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
1.3.1,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.3.1,def model_builder(model_dir):
1.3.1,sklearn_model = LogisticRegression()
1.3.1,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.3.1,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
1.3.1,# Fit trained model
1.3.1,model.fit(train_dataset)
1.3.1,model.save()
1.3.1,# Eval model on test
1.3.1,"scores = model.evaluate(test_dataset, [classification_metric])"
1.3.1,for score in scores[classification_metric.name]:
1.3.1,assert score > .5
1.3.1,Set early stopping round = n_estimators so that esr won't work
1.3.1,Fit trained model
1.3.1,Eval model on test
1.3.1,Fit trained model
1.3.1,Eval model on test
1.3.1,Fit trained model
1.3.1,Eval model on test
1.3.1,Logistic regression doesn't support weights
1.3.1,Consistency check
1.3.1,Handle output layer
1.3.1,Iterate over all previous tasks.
1.3.1,prev_layers is a list with elements of size
1.3.1,"(batch_size, layer_sizes[i-1])"
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Save an initial checkpoint.
1.3.1,Turns out there are valid cases where we don't want pad-batches
1.3.1,on by default.
1.3.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.1,Run training op.
1.3.1,Always save a final checkpoint when complete.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Note that we divide by the batch size and not the number of
1.3.1,"non-zero weight examples in the batch.  Also, instead of using"
1.3.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.1,calculate with div/sum so it stays on the GPU.
1.3.1,aggregated costs
1.3.1,weight decay
1.3.1,Dummy placeholders
1.3.1,Dummy placeholders
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,Handle edge case when batch-size is 1.
1.3.1,Prune away any padding that was added
1.3.1,allow_soft_placement=True allows ops without a GPU implementation
1.3.1,to run on the CPU instead.
1.3.1,!/usr/bin/python
1.3.1,
1.3.1,Copyright 2015 Google Inc.
1.3.1,
1.3.1,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.3.1,you may not use this file except in compliance with the License.
1.3.1,You may obtain a copy of the License at
1.3.1,
1.3.1,http://www.apache.org/licenses/LICENSE-2.0
1.3.1,
1.3.1,"Unless required by applicable law or agreed to in writing, software"
1.3.1,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.3.1,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.3.1,See the License for the specific language governing permissions and
1.3.1,limitations under the License.
1.3.1,get the divisor
1.3.1,compute the requested central moment
1.3.1,"note that mean is a raw moment, not a central moment"
1.3.1,TODO(user): median is not implemented yet in TensorFlow
1.3.1,-*- coding: utf-8 -*-
1.3.1,"due to the different shape of weight(ndims=2) and bias(ndims=1),"
1.3.1,will using this version for logreg
1.3.1,exclude bias variables
1.3.1,setting up n_tasks nodes(output nodes)
1.3.1,label placeholders with size batch_size * 1
1.3.1,Note that we divide by the batch size and not the number of
1.3.1,"non-zero weight examples in the batch.  Also, instead of using"
1.3.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.1,calculate with div/sum so it stays on the GPU.
1.3.1,aggregated costs
1.3.1,weight decay
1.3.1,using self-defined regularization
1.3.1,adding output nodes of sigmoid function
1.3.1,"fix the size to be [?,1]"
1.3.1,Dummy placeholders
1.3.1,Dummy placeholders
1.3.1,run eval data through the model
1.3.1,transfer 2D prediction tensor to 2D x n_classes(=2)
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,run eval data through the model
1.3.1,transfer 2D prediction tensor to 2D x n_classes(=2)
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,"layer has shape [None, layer_sizes[i]]"
1.3.1,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.3.1,TODO(rbharath): Might want to make it feasible to have multiple
1.3.1,bypass layers.
1.3.1,Construct task bypass layer
1.3.1,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.3.1,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.3.1,"layer has shape [None, layer_sizes[i]]"
1.3.1,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.3.1,TODO(rbharath): Might want to make it feasible to have multiple
1.3.1,bypass layers.
1.3.1,Construct task bypass layer
1.3.1,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.3.1,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.3.1,Add the input features.
1.3.1,Add the dense layers
1.3.1,Compute the loss function for each label.
1.3.1,"Results is of shape (n_samples, n_tasks, n_classes)"
1.3.1,"retval is of shape (n_samples, n_tasks)"
1.3.1,Add the input features.
1.3.1,Add the dense layers
1.3.1,Compute the loss function for each label.
1.3.1,Run fit transformers on dummy dataset to determine n_features after transformation
1.3.1,Dummy placeholders
1.3.1,Dummy placeholders
1.3.1,Dummy placeholders
1.3.1,Dummy placeholders
1.3.1,Run fit transformers on dummy dataset to determine n_features after transformation
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Save an initial checkpoint.
1.3.1,Define the code that runs on a separate thread to feed data into the queue.
1.3.1,Main training loop.
1.3.1,Run training op.
1.3.1,We have reached the end of an epoch.
1.3.1,We have reached the end of the data.
1.3.1,Always save a final checkpoint when complete.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,Handle edge case when batch-size is 1.
1.3.1,Prune away any padding that was added
1.3.1,Handle case of 0-dimensional scalar output
1.3.1,Consistency check
1.3.1,Lazily created by _get_shared_session().
1.3.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.3.1,when subclass-overridden methods use the same scopes.
1.3.1,Setup graph
1.3.1,Create placeholders
1.3.1,Handle output layer
1.3.1,Iterate over all previous tasks.
1.3.1,prev_layers is a list with elements of size
1.3.1,"(batch_size, layer_sizes[i-1])"
1.3.1,Note that we divide by the batch size and not the number of
1.3.1,"non-zero weight examples in the batch.  Also, instead of using"
1.3.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.1,calculate with div/sum so it stays on the GPU.
1.3.1,aggregated costs
1.3.1,weight decay
1.3.1,Dummy placeholders
1.3.1,Dummy placeholders
1.3.1,run eval data through the model
1.3.1,"Shape (n_tasks, n__samples)"
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,Handle edge case when batch-size is 1.
1.3.1,with self._get_shared_session(train=True) as sess:
1.3.1,Save an initial checkpoint.
1.3.1,Always save a final checkpoint when complete.
1.3.1,Note that we divide by the batch size and not the number of
1.3.1,"non-zero weight examples in the batch.  Also, instead of using"
1.3.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.1,calculate with div/sum so it stays on the GPU.
1.3.1,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
1.3.1,Dummy placeholders
1.3.1,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
1.3.1,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
1.3.1,Dummy placeholders
1.3.1,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
1.3.1,allow_soft_placement=True allows ops without a GPU implementation
1.3.1,to run on the CPU instead.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Turns out there are valid cases where we don't want pad-batches
1.3.1,on by default.
1.3.1,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.1,if epoch%checkpoint_interval == checkpoint_interval-1:
1.3.1,"saver.save(sess, self._save_path, global_step=epoch)"
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,"(n_samples, n_classes)"
1.3.1,"(n_samples, n_tasks, n_classes)"
1.3.1,Save hyperparameters
1.3.1,Guard variable to make sure we don't Restore() this model
1.3.1,from a disk checkpoint more than once.
1.3.1,"Path to save checkpoint files, which matches the"
1.3.1,replicated supervisor's default path.
1.3.1,Lazily created by _get_shared_session().
1.3.1,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.3.1,when subclass-overridden methods use the same scopes.
1.3.1,Setup graph
1.3.1,Note that we divide by the batch size and not the number of
1.3.1,"non-zero weight examples in the batch.  Also, instead of using"
1.3.1,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.1,calculate with div/sum so it stays on the GPU.
1.3.1,aggregated costs
1.3.1,weight decay
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Save an initial checkpoint.
1.3.1,Define the code that runs on a separate thread to feed data into the queue.
1.3.1,Main training loop.
1.3.1,Run training op.
1.3.1,We have reached the end of an epoch.
1.3.1,We have reached the end of the data.
1.3.1,Always save a final checkpoint when complete.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,allow_soft_placement=True allows ops without a GPU implementation
1.3.1,to run on the CPU instead.
1.3.1,gpu memory growth option
1.3.1,gpu memory growth option
1.3.1,TODO(rbharath): Is setting train=False right here?
1.3.1,Discard any padded predictions
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,Special case to handle singletasks.
1.3.1,The iterbatches does padding with zero-weight examples on the last batch.
1.3.1,Remove padded examples.
1.3.1,TODO(rbharath): Verify this can be safely removed.
1.3.1,"def evaluate(self, dataset, metrics, transformers=[]):"
1.3.1,""""""""
1.3.1,Evaluates the performance of this model on specified dataset.
1.3.1,
1.3.1,Parameters
1.3.1,----------
1.3.1,dataset: dc.data.Dataset
1.3.1,Dataset object.
1.3.1,metric: deepchem.metrics.Metric
1.3.1,Evaluation metric
1.3.1,transformers: list
1.3.1,List of deepchem.transformers.Transformer
1.3.1,Returns
1.3.1,-------
1.3.1,dict
1.3.1,Maps tasks to scores under metric.
1.3.1,""""""""
1.3.1,"evaluator = Evaluator(self, dataset, transformers)"
1.3.1,scores = evaluator.compute_model_performance(metrics)
1.3.1,return scores
1.3.1,checkpoints look like model_dir/model.ckpt-N
1.3.1,"self._save_path is ""model_dir/model.ckpt"""
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,Note that softmax is already applied in construct_grpah
1.3.1,run eval data through the model
1.3.1,reshape to batch_size x n_tasks x ...
1.3.1,Handle edge case when batch-size is 1.
1.3.1,Prune away any padding that was added
1.3.1,Handle case of 0-dimensional scalar output
1.3.1,!/usr/bin/python
1.3.1,
1.3.1,Copyright 2015 Google Inc.
1.3.1,
1.3.1,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.3.1,you may not use this file except in compliance with the License.
1.3.1,You may obtain a copy of the License at
1.3.1,
1.3.1,http://www.apache.org/licenses/LICENSE-2.0
1.3.1,
1.3.1,"Unless required by applicable law or agreed to in writing, software"
1.3.1,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.3.1,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.3.1,See the License for the specific language governing permissions and
1.3.1,limitations under the License.
1.3.1,parse CheckpointState proto
1.3.1,parse path to actual checkpoint
1.3.1,the provided mask has to be the same shape as features
1.3.1,test k = 1..4
1.3.1,central moments
1.3.1,standardized moments
1.3.1,central across one axis
1.3.1,standardized across one axis
1.3.1,Fit just on task zero
1.3.1,Notice that we keep the session open
1.3.1,Fit on task one
1.3.1,The predictions for task zero should not change after training
1.3.1,on task one.
1.3.1,Keep track of the layers
1.3.1,"For graphical layers, add connectivity placeholders"
1.3.1,Add layer to the layer list
1.3.1,Keep track of the layers
1.3.1,Create graph topology and x
1.3.1,Keep track of the layers
1.3.1,Whether or not we have used the GraphGather layer yet
1.3.1,Update new value of x
1.3.1,Update new value of x
1.3.1,Update new value of x
1.3.1,Get train function
1.3.1,Initialize
1.3.1,################################################################### DEBUG
1.3.1,self.test_label_placeholder = Input(
1.3.1,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.3.1,"name=""label_placeholder""))"
1.3.1,self.test_weight_placeholder = Input(
1.3.1,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.3.1,"name=""weight_placeholder""))"
1.3.1,TODO(rbharath): Should weights for the support be used?
1.3.1,Support labels
1.3.1,self.support_label_placeholder = Input(
1.3.1,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
1.3.1,"name=""support_label_placeholder""))"
1.3.1,################################################################### DEBUG
1.3.1,Generate dictionary elements for support
1.3.1,Get graph information for test
1.3.1,Generate dictionary elements for test
1.3.1,Perform the optimization
1.3.1,Create different support sets
1.3.1,Get batch to try it out on
1.3.1,"Train on support set, batch pair"
1.3.1,Get featurization for test
1.3.1,"Shape (n_test, n_feat)"
1.3.1,Get featurization for support
1.3.1,"Shape (n_support, n_feat)"
1.3.1,Computes the inner part c() of the kernel
1.3.1,(the inset equation in section 2.1.1 of Matching networks paper).
1.3.1,Normalize
1.3.1,TODO(rbharath): euclidean kernel is broken!
1.3.1,elif self.similarity == 'euclidean':
1.3.1,"g = model_ops.euclidean_distance(test_feat, support_feat)"
1.3.1,"Note that gram matrix g has shape (n_test, n_support)"
1.3.1,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
1.3.1,https://arxiv.org/pdf/1606.04080v1.pdf
1.3.1,"Computes softmax across axis 1, (so sums distances to support set for"
1.3.1,each test entry) to get attention vector
1.3.1,"Shape (n_test, n_support)"
1.3.1,Weighted sum of support labels
1.3.1,"Shape (n_support, 1)"
1.3.1,pred is yhat in eqn (1) of Matching Networks.
1.3.1,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
1.3.1,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
1.3.1,"Shape (n_test,)"
1.3.1,Convert to logit space using inverse sigmoid (logit) function
1.3.1,logit function: log(pred) - log(1-pred)
1.3.1,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
1.3.1,in Cross Entropy calculation.
1.3.1,"Shape (n_test,)"
1.3.1,Get scores
1.3.1,Remove padded elements
1.3.1,Get scores
1.3.1,pred corresponds to prob(example == 1)
1.3.1,Remove padded elements
1.3.1,Get batches
1.3.1,TODO(rbharath): Add test for get_task_dataset_minus_support for
1.3.1,multitask case with missing data...
1.3.1,Join information for all tasks.
1.3.1,TODO(rbharath): Find a way to get rid of this import?
1.3.1,Extract model info
1.3.1,Get graph topology for x
1.3.1,Building outputs
1.3.1,Set epsilon
1.3.1,Initialize
1.3.1,"Path to save checkpoint files, which matches the"
1.3.1,replicated supervisor's default path.
1.3.1,Create target inputs
1.3.1,Get train function
1.3.1,TODO(rbharath): I believe this is total amount of data
1.3.1,Get graph information
1.3.1,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.3.1,the number of labeled data points in target_i. This is to normalize each task
1.3.1,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.3.1,Get other optimizer information
1.3.1,TODO(rbharath): Figure out how to handle phase appropriately
1.3.1,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.3.1,"tensors of shape (batch_size,)"
1.3.1,It's ok to divide by just the batch_size rather than the number of nonzero
1.3.1,examples (effect averages out)
1.3.1,Perform the optimization
1.3.1,TODO(rbharath): Disabling saving for now to try to debug.
1.3.1,run eval data through the model
1.3.1,"Shape (n_samples, n_tasks)"
1.3.1,Create target inputs
1.3.1,TODO(rbharath): Find a way to get rid of this import?
1.3.1,Obtain appropriate loss function
1.3.1,Extract model info
1.3.1,Get graph topology for x
1.3.1,Raw logit outputs
1.3.1,Set epsilon
1.3.1,Initialize
1.3.1,"Path to save checkpoint files, which matches the"
1.3.1,replicated supervisor's default path.
1.3.1,Create target inputs
1.3.1,############################################################### DEBUG
1.3.1,"print(""multitask classifier"")"
1.3.1,"print(""feat"")"
1.3.1,print(feat)
1.3.1,############################################################### DEBUG
1.3.1,Get train function
1.3.1,TODO(rbharath): I believe this is total amount of data
1.3.1,Get graph information
1.3.1,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.3.1,the number of labeled data points in target_i. This is to normalize each task
1.3.1,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.3.1,Get other optimizer information
1.3.1,TODO(rbharath): Figure out how to handle phase appropriately
1.3.1,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.3.1,"tensors of shape (batch_size,)"
1.3.1,Convert the labels into one-hot vector encodings.
1.3.1,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
1.3.1,un-softmaxed logits rather than softmax outputs.
1.3.1,It's ok to divide by just the batch_size rather than the number of nonzero
1.3.1,examples (effect averages out)
1.3.1,Perform the optimization
1.3.1,TODO(rbharath): Disabling saving for now to try to debug.
1.3.1,run eval data through the model
1.3.1,"Shape (n_samples, n_tasks)"
1.3.1,run eval data through the model
1.3.1,self.n_atoms = n_atoms
1.3.1,Define the list of tensors to be used as topology
1.3.1,Merge mol conv objects
1.3.1,Generate dicts
1.3.1,Define the list of tensors to be used as topology
1.3.1,Extract atom numbers
1.3.1,Generate dicts
1.3.1,molecule * atom(graph) => step => features
1.3.1,molecule * atom(graph) => step
1.3.1,molecule * atom(graph) => step
1.3.1,Define the list of tensors to be used as topology
1.3.1,calculation orders for a batch of molecules
1.3.1,padding atom features vector of each molecule with 0
1.3.1,self.n_atoms = n_atoms
1.3.1,Define the list of tensors to be used as topology
1.3.1,Extract atom numbers
1.3.1,Generate dicts
1.3.1,self.n_atoms = n_atoms
1.3.1,Define the list of tensors to be used as topology
1.3.1,Extract atom numbers
1.3.1,number of atoms in each molecule
1.3.1,index of pair features
1.3.1,number of pairs for each atom
1.3.1,atom features
1.3.1,pair features
1.3.1,Generate dicts
1.3.1,Associate each atom with cell it belongs to. O(N*n_cells)
1.3.1,"Shape (n_cells, k)"
1.3.1,"Shape (N, 1)"
1.3.1,Associate each cell with its neighbor cells. Assumes periodic boundary
1.3.1,"conditions, so does wrapround. O(constant)"
1.3.1,"Shape (n_cells, 26)"
1.3.1,"Shape (N, 26)"
1.3.1,"coords of shape (N, ndim)"
1.3.1,"Shape (N, 26, k, ndim)"
1.3.1,"Shape (N, 26, k)"
1.3.1,"Shape (N, 26, k)"
1.3.1,"Shape (N, 26, k, ndim)"
1.3.1,"For smaller systems especially, the periodic boundary conditions can"
1.3.1,result in neighboring cells being seen multiple times. Maybe use tf.unique to
1.3.1,make sure duplicate neighbors are ignored?
1.3.1,TODO(rbharath): How does distance need to be modified here to
1.3.1,account for periodic boundary conditions?
1.3.1,"Shape (N, 26, k)"
1.3.1,"Shape (N, 26*k)"
1.3.1,TODO(rbharath): This will cause an issue with duplicates!
1.3.1,"Shape (N, M)"
1.3.1,"N elts of size (M,) each"
1.3.1,"Shape (N, 26*k)"
1.3.1,"N elts of size (26*k,) each"
1.3.1,"N elts of size (M,) each"
1.3.1,"Shape (N, M)"
1.3.1,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.3.1,"N tensors of shape (n_cells, 1)"
1.3.1,"Shape (N*n_cells, 1) after tile"
1.3.1,"List of N tensors of shape (n_cells, 1)"
1.3.1,Lists of length N
1.3.1,Lists of length n_cells
1.3.1,Get indices of k atoms closest to each cell point
1.3.1,TODO(rbharath): tf.stack for tf 1.0
1.3.1,"Tensor of shape (n_cells, k, ndim)"
1.3.1,atoms_in_cells = tf.stack(atoms_in_cells)
1.3.1,"Tensor of shape (26, k, ndim)"
1.3.1,"Reshape to (26*k, ndim)"
1.3.1,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
1.3.1,"Dists of shape (26*k, 1)"
1.3.1,"Of shape (k, ndim)"
1.3.1,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.3.1,TODO(rbharath): Change this for tf 1.0
1.3.1,"n_cells tensors of shape (N, 1)"
1.3.1,"Shape (N*n_cells, 1) after tile"
1.3.1,"List of n_cells tensors of shape (N, 1)"
1.3.1,Lists of length n_cells
1.3.1,Lists of length n_cells
1.3.1,Get indices of k atoms closest to each cell point
1.3.1,"n_cells tensors of shape (k, ndim)"
1.3.1,"Tensor of shape (n_cells, k)"
1.3.1,TODO(rbharath):
1.3.1,- Need to find neighbors of the cells (+/- 1 in every dimension).
1.3.1,- Need to group closest atoms amongst cell neighbors
1.3.1,- Need to do another top_k to find indices of closest neighbors.
1.3.1,- Return N lists corresponding to neighbors for every atom.
1.3.1,TODO(rbharath): Do we need to handle periodic boundary conditions
1.3.1,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.3.1,"looking for 26 neighbors, which isn't right for boundary cells in"
1.3.1,the cube.
1.3.1,Number of neighbors of central cube in 3-space is
1.3.1,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
1.3.1,TODO(rbharath)
1.3.1,n_cells = int(cells.get_shape()[0])
1.3.1,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.3.1,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.3.1,"Tile (a, a, a, b, b, b, etc.)"
1.3.1,"Tile (a, b, c, a, b, c, ...)"
1.3.1,"Lists of n_cells tensors of shape (N, 1)"
1.3.1,Lists of length n_cells
1.3.1,Lists of length n_cells
1.3.1,Get indices of k atoms closest to each cell point
1.3.1,"n_cells tensors of shape (26,)"
1.3.1,TODO(rbharath): Make this handle minibatches
1.3.1,"Shape (N_protein+N_ligand, 3)"
1.3.1,"Shape (N_protein+N_ligand,)"
1.3.1,"Shape (N_protein+N_ligand, M)"
1.3.1,"Shape (N_protein+N_ligand,)"
1.3.1,"Shape (N_protein+N_ligand, 3)"
1.3.1,"Shape (N_protein+N_ligand,)"
1.3.1,"Shape (N_protein+N_ligand, M)"
1.3.1,"Shape (N_protein+N_ligand, M, 3)"
1.3.1,"Shape (N_protein+N_ligand, M)"
1.3.1,"Shape (N_protein+N_ligand, M, 3)"
1.3.1,"Shape (N_protein+N_ligand, M)"
1.3.1,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
1.3.1,"Shape (N_protein+N_ligand, M)"
1.3.1,"Shape (N_protein+N_ligand, M)"
1.3.1,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
1.3.1,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
1.3.1,computing free-energy. This implementation currently uses all interaction
1.3.1,terms. Not sure if this makes a difference.
1.3.1,"Shape (N_protein+N_ligand, M)"
1.3.1,Shape () -- scalar
1.3.1,# Gather Projection
1.3.1,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
1.3.1,There should be 8 layers in graph_model
1.3.1,assert len(graph_model.layers) == 6
1.3.1,Add layers
1.3.1,Need to add batch-norm separately to test/support due to differing
1.3.1,shapes.
1.3.1,Apply an attention lstm layer
1.3.1,Gather Projection
1.3.1,Add layers
1.3.1,Need to add batch-norm separately to test/support due to differing
1.3.1,shapes.
1.3.1,Apply an attention lstm layer
1.3.1,Gather Projection
1.3.1,Degrees from 1 to max_deg inclusive
1.3.1,TODO(rbharath): Should this be 0 to max_deg inclusive?
1.3.1,"Should have shape (?, deg)"
1.3.1,"Shape of atom_features should be (?, n_feat)"
1.3.1,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
1.3.1,TODO(rbharath): Check that this operation is differentiable.
1.3.1,The number of cells which we should theoretically have
1.3.1,The number of cells which we should theoretically have
1.3.1,"Each atom neighbors tensor should be (k, ndim) shaped."
1.3.1,The number of cells which we should theoretically have
1.3.1,TODO(rbharath): The test below only checks that shapes work out.
1.3.1,Need to do a correctness implementation vs. a simple CPU impl.
1.3.1,The number of cells which we should theoretically have
1.3.1,TODO(rbharath): The test below only checks that shapes work out.
1.3.1,Need to do a correctness implementation vs. a simple CPU impl.
1.3.1,The number of cells which we should theoretically have
1.3.1,TODO(rbharath): The test below only checks that shapes work out.
1.3.1,Need to do a correctness implementation vs. a simple CPU impl.
1.3.1,TODO(rbharath): Commenting this out due to weird segfaults
1.3.1,def test_vina_generate_conformers(self):
1.3.1,"""""""Test that Vina Model can generate conformers"""""""
1.3.1,data_dir = os.path.dirname(os.path.realpath(__file__))
1.3.1,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
1.3.1,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
1.3.1,max_protein_atoms = 3500
1.3.1,max_ligand_atoms = 100
1.3.1,"print(""Loading protein file"")"
1.3.1,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
1.3.1,protein_Z = pad_array(
1.3.1,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
1.3.1,max_protein_atoms)
1.3.1,"print(""Loading ligand file"")"
1.3.1,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
1.3.1,ligand_Z = pad_array(
1.3.1,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
1.3.1,max_ligand_atoms)
1.3.1,-*- coding: utf-8 -*-
1.3.1,Assigning featurizer if not user defined
1.3.1,loading datasets
1.3.1,Assembling train and valid datasets
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,Loading hyper parameters
1.3.1,Building tensorflow MultiTaskDNN model
1.3.1,Loading hyper parameters
1.3.1,Building tensorflow robust MultiTaskDNN model
1.3.1,Loading hyper parameters
1.3.1,Building tensorflow logistic regression model
1.3.1,Loading hyper parameters
1.3.1,Transform fingerprints to IRV features
1.3.1,Building tensorflow IRV model
1.3.1,Loading hyper parameters
1.3.1,Gather Projection
1.3.1,Loading hyper parameters
1.3.1,Loading hyper parameters
1.3.1,Building scikit random forest model
1.3.1,Loading hyper parameters
1.3.1,Building scikit learn Kernel SVM model
1.3.1,Loading hyper parameters
1.3.1,Building xgboost classification model
1.3.1,Loading hyper parameters
1.3.1,Building tensorflow MultiTaskDNN model
1.3.1,Loading hyper parameters
1.3.1,Initialize model folder
1.3.1,Loading hyper parameters
1.3.1,Gather Projection
1.3.1,Loading hyper parameters
1.3.1,Loading hyper parameters
1.3.1,Remove token for paddings
1.3.1,Loading hyper parameters
1.3.1,Building scikit random forest model
1.3.1,Loading hyper parameters
1.3.1,Building scikit learn Kernel Ridge Regression model
1.3.1,Loading hyper parameters
1.3.1,Building scikit learn Kernel Ridge Regression model
1.3.1,Loading hyper parameters
1.3.1,Building xgboost classification model
1.3.1,Loading hyperparameters
1.3.1,num positive/negative ligands
1.3.1,Set batch sizes for network
1.3.1,Model structure
1.3.1,Traning settings
1.3.1,Fit trained model
1.3.1,Evaluating low data model
1.3.1,-*- coding: utf-8 -*-
1.3.1,Assigning featurizer if not user defined
1.3.1,loading datasets
1.3.1,
1.3.1,Note by @XericZephyr. Reason why I spun off this function:
1.3.1,1. Some model needs dataset information.
1.3.1,2. It offers us possibility to **cache** the dataset
1.3.1,"if the featurizer runs very slow, e.g., GraphConv."
1.3.1,2+. The cache can even happen at Travis CI to accelerate
1.3.1,CI testing.
1.3.1,
1.3.1,loading datasets
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,Featurize qm9 dataset
1.3.1,transformers = [
1.3.1,"deepchem.trans.LogTransformer(transform_X=True),"
1.3.1,"deepchem.trans.NormalizationTransformer(transform_y=True,"
1.3.1,dataset=train_dataset)]
1.3.1,Set shard size low to avoid memory problems.
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Set some global variables up top
1.3.1,Featurize KAGGLE dataset
1.3.1,############################################################# TIMING
1.3.1,############################################################# TIMING
1.3.1,Featurize qm7 dataset
1.3.1,Featurize clintox dataset
1.3.1,Transform clintox dataset
1.3.1,Split clintox dataset
1.3.1,Featurize bbb dataset
1.3.1,Initialize transformers
1.3.1,Load nci dataset
1.3.1,Featurize nci dataset
1.3.1,Initialize transformers
1.3.1,Featurize HOPV dataset
1.3.1,Initialize transformers
1.3.1,Featurize PPB dataset
1.3.1,Initialize transformers
1.3.1,Load MUV dataset
1.3.1,Featurize MUV dataset
1.3.1,Initialize transformers
1.3.1,Featurize clearance dataset
1.3.1,Initialize transformers
1.3.1,Featurize TOXCAST dataset
1.3.1,Initialize transformers
1.3.1,Featurize bace dataset
1.3.1,Initialize transformers
1.3.1,Featurize bace dataset
1.3.1,Initialize transformers
1.3.1,Featurize Tox21 dataset
1.3.1,Initialize transformers
1.3.1,Featurize ChEMBL dataset
1.3.1,Initialize transformers
1.3.1,Featurize hiv dataset
1.3.1,Initialize transformers
1.3.1,Featurize SIDER dataset
1.3.1,Initialize transformers
1.3.1,Featurize SAMPL dataset
1.3.1,Initialize transformers
1.3.1,Featurize Delaney dataset
1.3.1,Initialize transformers
1.3.1,Featurize PCBA dataset
1.3.1,Initialize transformers
1.3.1,Featurize Lipophilicity dataset
1.3.1,Initialize transformers
1.3.1,"Float or int hyper parameters(ex. batch_size, learning_rate)"
1.3.1,List of float or int hyper parameters(ex. layer_sizes)
1.3.1,Number of parameters
1.3.1,Range of optimization
1.3.1,Dummy names
1.3.1,Input hyper parameters
1.3.1,Run benchmark
1.3.1,Record hyperparameters
1.3.1,Record performances
1.3.1,"GPGO maximize performance by default, set performance to its negative value for minimization"
1.3.1,Readout best hyper parameters
1.3.1,Compare best model to default hyperparameters
1.3.1,Record hyperparameters
1.3.1,Record performances
1.3.1,"Optimized model is better, return hyperparameters"
1.3.1,Return default hyperparameters
1.3.1,!/usr/bin/env python2
1.3.1,-*- coding: utf-8 -*-
1.3.1,TODO(rbharath): This function is complicated and monolithic. Is there a nice
1.3.1,way to refactor this?
1.3.1,arbitrarily return last model
1.3.1,Define train dataset
1.3.1,Define validation dataset
1.3.1,Have the worker threads generate the rollouts for this iteration.
1.3.1,Perform optimization.
1.3.1,Build the feed dict and run the optimizer.
1.3.1,Update the number of steps taken so far and perform checkpointing.
1.3.1,Merge all the rollouts into a single set of arrays.
1.3.1,Iterate slices.
1.3.1,Generate the rollout.
1.3.1,Compute an estimate of the reward for the rest of the episode.
1.3.1,Compute the discounted rewards and advantages.
1.3.1,Convert the actions to one-hot.
1.3.1,Rearrange the states into the proper set of arrays.
1.3.1,Return the processed arrays.
1.3.1,Generate the rollout.
1.3.1,Compute an estimate of the reward for the rest of the episode.
1.3.1,Compute the discounted rewards and advantages.
1.3.1,Convert the actions to one-hot.
1.3.1,Rearrange the states into the proper set of arrays.
1.3.1,Build the feed dict and apply gradients.
1.3.1,Assume all arrays are float32.
1.3.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
1.3.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
1.3.1,"game).  The average reward for any bet is slightly negative, so the best"
1.3.1,strategy is to walk away.
1.3.1,"This policy just learns a constant probability for each action, and a constant for the value."
1.3.1,Optimize it.
1.3.1,"It should have learned that the expected value is very close to zero, and that the best"
1.3.1,action is to walk away.
1.3.1,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
1.3.1,get the same result.
1.3.1,"Do the same thing, only using the ""restore"" argument to fit()."
1.3.1,The environment just has a constant state.
1.3.1,The policy includes a single recurrent layer.
1.3.1,"We don't care about actually optimizing it, so just run a few rollouts to make"
1.3.1,"sure fit() doesn't crash, then check the behavior of the GRU state."
1.3.1,"On the first call, the initial state should be all zeros."
1.3.1,It should still be zeros since we didn't save it last time.
1.3.1,It should be different now.
1.3.1,This should be the same as the previous one.
1.3.1,"Now we reset it, so we should get the same result as initially."
1.3.1,The environment is a plane in which the agent moves by steps until it reaches a randomly
1.3.1,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
1.3.1,"to learn by standard methods, since it may take a very long time to receive any feedback"
1.3.1,at all.  Using hindsight makes it much easier.
1.3.1,A simple policy with two hidden layers.
1.3.1,Optimize it.
1.3.1,Try running it a few times and see if it succeeds.
1.3.1,This is modeled after the Roulette-v0 environment from OpenAI Gym.
1.3.1,"The player can bet on any number from 0 to 36, or walk away (which ends the"
1.3.1,"game).  The average reward for any bet is slightly negative, so the best"
1.3.1,strategy is to walk away.
1.3.1,"This policy just learns a constant probability for each action, and a constant for the value."
1.3.1,Optimize it.
1.3.1,"It should have learned that the expected value is very close to zero, and that the best"
1.3.1,action is to walk away.
1.3.1,"Verify that we can create a new A3C object, reload the parameters from the first one, and"
1.3.1,get the same result.
1.3.1,"Do the same thing, only using the ""restore"" argument to fit()."
1.3.1,The environment just has a constant state.
1.3.1,The policy includes a single recurrent layer.
1.3.1,"We don't care about actually optimizing it, so just run a few rollouts to make"
1.3.1,"sure fit() doesn't crash, then check the behavior of the GRU state."
1.3.1,"On the first call, the initial state should be all zeros."
1.3.1,It should still be zeros since we didn't save it last time.
1.3.1,It should be different now.
1.3.1,This should be the same as the previous one.
1.3.1,"Now we reset it, so we should get the same result as initially."
1.3.1,The environment is a plane in which the agent moves by steps until it reaches a randomly
1.3.1,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
1.3.1,"to learn by standard methods, since it may take a very long time to receive any feedback"
1.3.1,at all.  Using hindsight makes it much easier.
1.3.1,A simple policy with two hidden layers.
1.3.1,Optimize it.
1.3.1,Try running it a few times and see if it succeeds.
1.3.1,Randomize who goes first
1.3.1,Illegal move -- the square is not empty
1.3.1,Move X
1.3.1,Did X Win
1.3.1,Did O Win
1.3.1,TODO (Bowen): make this function less memory intensive
1.3.1,set 1st column as the column index of dataframe
1.3.1,merge descriptor and activities dataframe into output dataframe based on
1.3.1,"the molecule name, which is the index for both dataframes (but named"
1.3.1,differently). Default merge is inner merge
1.3.1,need to manually set dataframe indexname after merge based on index
1.3.1,from deepchem.scripts.dock_dude import *
1.3.1,from ipyparallel import Client
1.3.1,rc = Client()
1.3.1,dview = rc[:]
1.3.1,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
1.3.1,
1.3.1,"If mol_id is not set, then use isomeric smiles as unique identifier"
1.3.1,iterator = data_df.iterrows()
1.3.1,TODO(rbharath): BROKEN!
1.3.1,Trim unwanted indexing fields
1.3.1,Connect to running ipython server
1.3.1,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
1.3.1,
1.3.1,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.3.1,you may not use this file except in compliance with the License.
1.3.1,You may obtain a copy of the License at
1.3.1,
1.3.1,http://www.apache.org/licenses/LICENSE-2.0
1.3.1,
1.3.1,"Unless required by applicable law or agreed to in writing, software"
1.3.1,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.3.1,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.3.1,See the License for the specific language governing permissions and
1.3.1,limitations under the License.
1.3.1,==============================================================================
1.3.1,Maps from a function name to a dictionary that describes how to
1.3.1,map from an old argument keyword to the new argument keyword.
1.3.1,Mapping from function to the new name of the function
1.3.1,Functions that were reordered should be changed to the new keyword args
1.3.1,"for safety, if positional arguments are used. If you have reversed the"
1.3.1,"positional arguments yourself, this could do the wrong thing."
1.3.1,Specially handled functions.
1.3.1,TODO(aselle): Could check for a literal list of bools and try to convert
1.3.1,them to indices.
1.3.1,all edits are lists of chars
1.3.1,Iterate of each line
1.3.1,sort by column so that edits are processed in order in order to make
1.3.1,indexing adjustments cumulative for changes that change the string
1.3.1,length
1.3.1,"Extract each line to a list of characters, because mutable lists"
1.3.1,"are editable, unlike immutable strings."
1.3.1,Record a description of the change
1.3.1,Make underscore buffers for underlining where in the line the edit was
1.3.1,Iterate for each edit
1.3.1,"Create effective start, end by accounting for change in length due"
1.3.1,to previous edits
1.3.1,Make sure the edit is changing what it should be changing
1.3.1,Make the edit
1.3.1,Create the underline highlighting of the before and after
1.3.1,Keep track of how to generate effective ranges
1.3.1,Finish the report comment
1.3.1,"Strangely, ast.ListComp returns the col_offset of the first token"
1.3.1,after the '[' token which appears to be a bug. Workaround by
1.3.1,explicitly finding the real start of the list comprehension.
1.3.1,loop over lines
1.3.1,Reverse the text to and regular expression search for whitespace
1.3.1,First find if a [ can be found with only whitespace between it and
1.3.1,col.
1.3.1,TODO(aselle):
1.3.1,"this is poor comment detection, but it is good enough for"
1.3.1,cases where the comment does not contain string literal starting/
1.3.1,ending characters. If ast gave us start and end locations of the
1.3.1,"ast nodes rather than just start, we could use string literal"
1.3.1,node ranges to filter out spurious #'s that appear in string
1.3.1,literals.
1.3.1,"Most other nodes return proper locations (with notably does not), but"
1.3.1,it is not possible to use that in an argument.
1.3.1,"Find a simple attribute name path e.g. ""tf.foo.bar"""
1.3.1,Make sure the func is marked as being part of a call
1.3.1,Call special handlers
1.3.1,Examine any non-keyword argument and make it into a keyword argument
1.3.1,if reordering required.
1.3.1,Examine each keyword argument and convert it to the final renamed form
1.3.1,TODO(aselle): We should scan backward to find the start of the
1.3.1,keyword key. Unfortunately ast does not give you the location of
1.3.1,"keyword keys, so we are forced to infer it from the keyword arg"
1.3.1,value.
1.3.1,"Write to a temporary file, just in case we are doing an implace modify."
1.3.1,Broad exceptions are required here because ast throws whatever it wants.
1.3.1,pylint: disable=broad-except
1.3.1,pylint: enable=broad-except
1.3.1,make sure output directory doesn't exist
1.3.1,make sure output directory does not overlap with root_directory
1.3.1,Collect list of files to process (we do this to correctly handle if the
1.3.1,user puts the output directory in some sub directory of the input dir)
1.3.1,import os
1.3.1,"from deepchem.utils.save import load_from_disk, save_to_disk"
1.3.1,from deepchem.featurizers.fingerprints import CircularFingerprint
1.3.1,from deepchem.featurizers.basic import RDKitDescriptors
1.3.1,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
1.3.1,from deepchem.featurizers.grid_featurizer import GridFeaturizer
1.3.1,from deepchem.featurizers.featurize import DataLoader
1.3.1,
1.3.1,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
1.3.1,"print(""About to load dataset form disk."")"
1.3.1,dataset = load_from_disk(dataset_file)
1.3.1,"print(""Loaded dataset."")"
1.3.1,
1.3.1,grid_featurizer = GridFeaturizer(
1.3.1,"voxel_width=16.0, feature_types=""voxel_combined"","
1.3.1,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.3.1,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.3.1,"parallel=True, flatten=True)"
1.3.1,featurizers = [CircularFingerprint(size=1024)]
1.3.1,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
1.3.1,
1.3.1,#Make a directory in which to store the featurized complexes.
1.3.1,"base_dir = ""../../../grid_nnscore_circular_features"""
1.3.1,if not os.path.exists(base_dir):
1.3.1,os.makedirs(base_dir)
1.3.1,"data_dir = os.path.join(base_dir, ""data"")"
1.3.1,if not os.path.exists(data_dir):
1.3.1,os.makedirs(data_dir)
1.3.1,
1.3.1,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
1.3.1,
1.3.1,"feature_dir = os.path.join(base_dir, ""features"")"
1.3.1,if not os.path.exists(feature_dir):
1.3.1,os.makedirs(feature_dir)
1.3.1,
1.3.1,"samples_dir = os.path.join(base_dir, ""samples"")"
1.3.1,if not os.path.exists(samples_dir):
1.3.1,os.makedirs(samples_dir)
1.3.1,
1.3.1,
1.3.1,
1.3.1,featurizers = compound_featurizers + complex_featurizers
1.3.1,"featurizer = DataLoader(tasks=[""label""],"
1.3.1,"smiles_field=""smiles"","
1.3.1,"protein_pdb_field=""protein_pdb"","
1.3.1,"ligand_pdb_field=""ligand_pdb"","
1.3.1,"compound_featurizers=compound_featurizers,"
1.3.1,"complex_featurizers=complex_featurizers,"
1.3.1,"id_field=""complex_id"","
1.3.1,verbose=False)
1.3.1,from ipyparallel import Client
1.3.1,c = Client()
1.3.1,"print(""c.ids"")"
1.3.1,print(c.ids)
1.3.1,dview = c[:]
1.3.1,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
1.3.1,"worker_pool=dview, shard_size=1024)"
1.3.1,
1.3.1,"save_to_disk(featurized_samples, featurized_samples_file)"
1.3.1,"print(""Preparing ligand %s"" % mol_name)"
1.3.0,!/usr/bin/env python3
1.3.0,-*- coding: utf-8 -*-
1.3.0,Datasets and models used in the benchmark test
1.3.0,"irv, rf, rf_regression should be assigned manually"
1.3.0,Will raise a CalledProcessError if fails.
1.3.0,!/usr/bin/env python3
1.3.0,-*- coding: utf-8 -*-
1.3.0,Datasets and models used in the benchmark test
1.3.0,Set numpy seed
1.3.0,##Load data###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Featurize Kinase dataset
1.3.0,##Load data###
1.3.0,num_trials = 5
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,Force matplotlib to not use any Xwindows backend.
1.3.0,##Load data###
1.3.0,the histogram of the data
1.3.0,Set numpy seed
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,num_trials = 5
1.3.0,Set some global variables up top
1.3.0,Fit trained model
1.3.0,Featurize PCBA dataset
1.3.0,Initialize transformers
1.3.0,Fit trained model
1.3.0,Load SWEET dataset
1.3.0,Featurize SWEET dataset
1.3.0,Initialize transformers
1.3.0,Set some global variables up top
1.3.0,removes directory if present -- warning
1.3.0,default split is 80-10-10 train-valid-test split
1.3.0,Fit Logistic Regression models
1.3.0,Fit Logistic Regression models
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,transformers = [
1.3.0,"dc.trans.LogTransformer(transform_X=True),"
1.3.0,"dc.trans.NormalizationTransformer(transform_y=True,"
1.3.0,dataset=train_dataset)]
1.3.0,Set shard size low to avoid memory problems.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Set some global variables up top
1.3.0,Featurize KAGGLE dataset
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,##Load data###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,##Load data###
1.3.0,"n_estimators=100, max_features=int(num_features/3),"
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,Featurize qm9 dataset
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Gather Projection
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Load tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Featurize Tox21 dataset
1.3.0,Initialize transformers
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Load tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Featurize FACTORS dataset
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,Force matplotlib to not use any Xwindows backend.
1.3.0,##Load data###
1.3.0,the histogram of the data
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Featurize qm7 dataset
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Batch size of models
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Batch size of models
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Batch size of models
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load QM8 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Featurize qm8 dataset
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Fit trained model
1.3.0,Set numpy seed
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,Set shard size low to avoid memory problems.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Set some global variables up top
1.3.0,Load dataset
1.3.0,Featurize ChEMBL dataset
1.3.0,Initialize transformers
1.3.0,Load ChEMBL dataset
1.3.0,Fit models
1.3.0,Do setup required for tf/keras models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Gather Projection
1.3.0,Fit trained model
1.3.0,DeepCrystal Technologies 2017 - Patrick Hop
1.3.0,MIT License - have fun!!
1.3.0,Set to higher values to get better numbers
1.3.0,======================================================================
1.3.0,"Run Benchmarks {GC-DNN, SVR, RF}"
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,Only for debug!
1.3.0,Load Delaney dataset
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Do setup required for tf/keras models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
1.3.0,Fit trained model
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Do setup required for tf/keras models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Gather Projection
1.3.0,Dense post-processing layer
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Featurize Delaney dataset
1.3.0,Initialize transformers
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load Delaney dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load MUV dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Evaluate train/test scores
1.3.0,Load MUV dataset
1.3.0,Featurize MUV dataset
1.3.0,Initialize transformers
1.3.0,Load MUV data
1.3.0,Build model
1.3.0,Fit trained model
1.3.0,Evaluate train/test scores
1.3.0,Extract active site
1.3.0,Featurize ligand
1.3.0,Default for CircularFingerprint
1.3.0,Featurize pocket
1.3.0,Note broadcast operation
1.3.0,Compute labels for pockets
1.3.0,Some complexes have labels but no PDB files. Filter these manually
1.3.0,Some of the ligand-names are of form (FMN ox). Use regex
1.3.0,to merge into form (FMN-ox)
1.3.0,Filter if missing PDB files
1.3.0,Load PDBBind dataset
1.3.0,Define featurizers
1.3.0,Featurize Dataset
1.3.0,########################################################## DEBUG
1.3.0,########################################################## DEBUG
1.3.0,For stable runs
1.3.0,Fit trained model
1.3.0,4-fold splits
1.3.0,num positive/negative ligands
1.3.0,10 trials on test-set
1.3.0,Define metric
1.3.0,Get supports on test-set
1.3.0,Compute accuracies
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Test model
1.3.0,Join information for all tasks.
1.3.0,4-fold splits
1.3.0,num positive/negative ligands
1.3.0,Define metric
1.3.0,Get supports on test-set
1.3.0,Compute accuracies
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Test model
1.3.0,Join information for all tasks.
1.3.0,4-fold splits
1.3.0,num positive/negative ligands
1.3.0,10 trials on test-set
1.3.0,Define metric
1.3.0,Get supports on test-set
1.3.0,Compute accuracies
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Fit trained model
1.3.0,Test model
1.3.0,Join information for all tasks.
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,number positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply an attention lstm layer
1.3.0,Number of folds for split
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,4-fold splits
1.3.0,num positive/negative ligands
1.3.0,10 trials on test-set
1.3.0,Define metric
1.3.0,Get supports on test-set
1.3.0,Compute accuracies
1.3.0,Train model on support
1.3.0,Test model
1.3.0,Join information for all tasks.
1.3.0,4-fold splits
1.3.0,num positive/negative ligands
1.3.0,10 trials on test-set
1.3.0,Define metric
1.3.0,Get supports on test-set
1.3.0,Compute accuracies
1.3.0,Train model on support
1.3.0,Test model
1.3.0,Join information for all tasks.
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,number positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply an attention lstm layer
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,number positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply an attention lstm layer
1.3.0,Number of folds for split
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Number of folds for split
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply a residual lstm layer
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply a residual lstm layer
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply a residual lstm layer
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply a residual lstm layer
1.3.0,Set some global variables up top
1.3.0,Featurize Tox21 dataset
1.3.0,Initialize transformers
1.3.0,Set some global variables up top
1.3.0,Featurize Tox21 dataset
1.3.0,Initialize transformers
1.3.0,Load MUV dataset
1.3.0,Featurize MUV dataset
1.3.0,Initialize transformers
1.3.0,Load MUV dataset
1.3.0,Featurize MUV dataset
1.3.0,Initialize transformers
1.3.0,Featurize SIDER dataset
1.3.0,Initialize transformers
1.3.0,Featurize SIDER dataset
1.3.0,Initialize transformers
1.3.0,Load the data.
1.3.0,"Toxcast has data on 6874 molecules and 617 tasks.  However, the data is very"
1.3.0,sparse: most tasks do not include data for most molecules.  It also is very
1.3.0,"unbalanced: there are many more negatives than positives.  For each task,"
1.3.0,create a list of alternating postives and negatives so each batch will have
1.3.0,equal numbers of both.
1.3.0,Create the model to train.  We use a simple fully connected network with
1.3.0,one hidden layer.
1.3.0,Define a MetaLearner describing the learning problem.
1.3.0,Run meta-learning on 80% of the tasks.
1.3.0,Validate on the remaining tasks.
1.3.0,Number of folds for split
1.3.0,Depth of attention module
1.3.0,number positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,Apply an attention lstm layer
1.3.0,4-fold splits
1.3.0,10 positive/negative ligands
1.3.0,10 trials on test-set
1.3.0,Sample supports without replacement (all pos/neg should be different)
1.3.0,Define metric
1.3.0,Get supports on test-set
1.3.0,Compute accuracies
1.3.0,Train model on support
1.3.0,Test model
1.3.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
1.3.0,Join information for all tasks.
1.3.0,Number of folds for split
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Number of features on conv-mols
1.3.0,Define metric
1.3.0,Train support model on train
1.3.0,Add layers
1.3.0,4-fold splits
1.3.0,num positive/negative ligands
1.3.0,Define metric
1.3.0,Get supports on test-set
1.3.0,Compute accuracies
1.3.0,Train model on support
1.3.0,Test model
1.3.0,Join information for all tasks.
1.3.0,replace with your own scratch directory
1.3.0,Number of conformations in each file increases exponentially.
1.3.0,Start with a smaller dataset before continuing. Use all of them
1.3.0,for production
1.3.0,"'ani_gdb_s03.h5',"
1.3.0,"'ani_gdb_s04.h5',"
1.3.0,"'ani_gdb_s05.h5',"
1.3.0,"'ani_gdb_s06.h5',"
1.3.0,"'ani_gdb_s07.h5',"
1.3.0,'ani_gdb_s08.h5'
1.3.0,Extract the data
1.3.0,Print the data
1.3.0,self-interaction energies taken from
1.3.0,https://github.com/isayev/ANI1_dataset README
1.3.0,flush once more at the end
1.3.0,"# For production, set nb_epoch to 100+"
1.3.0,"print(""Train scores"")"
1.3.0,print(train_scores)
1.3.0,"print(""Minimization of a single test set structure:"")"
1.3.0,"print(model.minimize_structure(coords, atomic_nums))"
1.3.0,Written by Roman Zubatyuk and Justin S. Smith
1.3.0,Modified by Yutong Zhao to make python2 compatible
1.3.0,opening file
1.3.0,print(store_loc)
1.3.0,print(type(v[0]))
1.3.0,print(k)
1.3.0,print(path)
1.3.0,Number of conformations in each file increases exponentially.
1.3.0,Start with a smaller dataset before continuing. Use all of them
1.3.0,for production
1.3.0,Extract the data
1.3.0,Note sensitivity = recall
1.3.0,NOTE THE RENAMING:
1.3.0,Note sensitivity = recall
1.3.0,Load nci dataset
1.3.0,Featurize nci dataset
1.3.0,Initialize transformers
1.3.0,Set some global variables up top
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load hiv dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Featurize hiv dataset
1.3.0,Initialize transformers
1.3.0,Only for debug!
1.3.0,Load hiv dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Fit trained model
1.3.0,Fit models
1.3.0,Batch size of models
1.3.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.3.0,Fit trained model
1.3.0,Load SIDER dataset
1.3.0,Featurize SIDER dataset
1.3.0,Initialize transformers
1.3.0,Featurize permeability dataset
1.3.0,Load Tox21 dataset
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load SAMPL dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load Tox21 dataset
1.3.0,Fit models
1.3.0,Do setup required for tf/keras models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Gather Projection
1.3.0,Dense post-processing layer
1.3.0,Fit trained model
1.3.0,Featurize SAMPL dataset
1.3.0,Initialize transformers
1.3.0,Load clintox dataset
1.3.0,Featurize clintox dataset
1.3.0,Transform clintox dataset
1.3.0,Split clintox dataset
1.3.0,Only for debug!
1.3.0,Load clintox dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load clintox dataset
1.3.0,Fit models
1.3.0,Do setup required for tf/keras models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Gather Projection
1.3.0,Fit trained model
1.3.0,-*- coding: utf-8 -*-
1.3.0,#############################################################################
1.3.0,## save dataset
1.3.0,#############################################################################
1.3.0,## load datasets
1.3.0,load sweetfda
1.3.0,load aact
1.3.0,## fixup smiles for matching
1.3.0,return smiles
1.3.0,map original smiles to converted smiles
1.3.0,"## join dataframes, index on smiles"
1.3.0,map original smiles back
1.3.0,## fill all nan with 0
1.3.0,## construct datasets
1.3.0,store in new datasets
1.3.0,## save datasets
1.3.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
1.3.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
1.3.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
1.3.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
1.3.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
1.3.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
1.3.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
1.3.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
1.3.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
1.3.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
1.3.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
1.3.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
1.3.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
1.3.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
1.3.0,For stable runs
1.3.0,Fit trained model
1.3.0,For stable runs
1.3.0,Fit trained model
1.3.0,Some complexes have labels but no PDB files. Filter these manually
1.3.0,Some of the ligand-names are of form (FMN ox). Use regex
1.3.0,to merge into form (FMN-ox)
1.3.0,Load PDBBind dataset
1.3.0,Define featurizers
1.3.0,"TODO(rbharath, enf, leswing): Figure out why pi_stack and cation_pi"
1.3.0,reduce validation performance
1.3.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.3.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.3.0,Featurize Dataset
1.3.0,Currently featurizes with shard_size=1
1.3.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.3.0,transformers = [
1.3.0,"dc.trans.LogTransformer(transform_X=True),"
1.3.0,"dc.trans.NormalizationTransformer(transform_y=True,"
1.3.0,dataset=train_dataset)]
1.3.0,Featurize UV dataset
1.3.0,##Load data###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Set numpy seed
1.3.0,##Load data###
1.3.0,##Create model###
1.3.0,Use R2 classification metric
1.3.0,"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
1.3.0,Only use for final evaluation
1.3.0,Force matplotlib to not use any Xwindows backend.
1.3.0,##Load data###
1.3.0,the histogram of the data
1.3.0,##Load data###
1.3.0,###################################################### DEBUG
1.3.0,###################################################### DEBUG
1.3.0,Load HOPV dataset
1.3.0,Fit models
1.3.0,Number of features on conv-mols
1.3.0,Batch size of models
1.3.0,Gather Projection
1.3.0,Fit trained model
1.3.0,Featurize HOPV dataset
1.3.0,Initialize transformers
1.3.0,Only for debug!
1.3.0,Load HOPV dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load HOPV dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load HOPV dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Only for debug!
1.3.0,Load HOPV dataset
1.3.0,Fit models
1.3.0,Fit trained model
1.3.0,Load TOXCAST dataset
1.3.0,Featurize TOXCAST dataset
1.3.0,Initialize transformers
1.3.0,Fit trained model
1.3.0,Processing of ToxCast data
1.3.0,Author - Aneesh Pappu
1.3.0,Loading dataframes and editing indices
1.3.0,Loop through rows of hitc matrix and replace codes with smiles strings
1.3.0,get corresponding casn
1.3.0,get corresponding smiles
1.3.0,write to cell
1.3.0,Tidy up and write to csv
1.3.0,-*- coding: utf-8 -*-
1.3.0,Save hyperparameters
1.3.0,-*- coding: utf-8 -*-
1.3.0,Save hyperparameters
1.3.0,setup optimizer
1.3.0,setup optimizer
1.3.0,"print(""tasK: %d"" %task)"
1.3.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
1.3.0,"print(""scores"")"
1.3.0,print(scores.size())
1.3.0,"print(""task_label"")"
1.3.0,print(task_label.size())
1.3.0,"task_loss =  self.criterion(scores, task_label)"
1.3.0,"print(""task_loss"")"
1.3.0,print(task_loss.size())
1.3.0,-*- coding: utf-8 -*-
1.3.0,Save hyperparameters
1.3.0,weight decay
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Turns out there are valid cases where we don't want pad-batches
1.3.0,on by default.
1.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.0,Run training op.
1.3.0,############################################################# TIMING
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,Special case to handle singletasks.
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,2017 DeepCrystal Technologies - Patrick Hop
1.3.0,
1.3.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
1.3.0,
1.3.0,MIT License - have fun!!
1.3.0,===========================================================
1.3.0,x = F.selu( fc(x) )
1.3.0,x = F.selu( fc(x) )
1.3.0,2017 DeepCrystal Technologies - Patrick Hop
1.3.0,
1.3.0,Data loading a splitting file
1.3.0,
1.3.0,MIT License - have fun!!
1.3.0,===========================================================
1.3.0,Set random seeds
1.3.0,Setup directories
1.3.0,Model constants
1.3.0,Load and transform datasets
1.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.0,Atomic convolution variables
1.3.0,at = atomic numbers (atom types)
1.3.0,"radial basis function parameters [cutoff, mean, width]"
1.3.0,Model hyperparameters
1.3.0,Initialize model
1.3.0,Fit model
1.3.0,Evaluate model
1.3.0,Set random seeds
1.3.0,Setup directories
1.3.0,Model constants
1.3.0,Load and transform datasets
1.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.0,Atomic convolution variables
1.3.0,at = atomic numbers (atom types)
1.3.0,"radial basis function parameters [cutoff, mean, width]"
1.3.0,Model hyperparameters
1.3.0,Initialize model
1.3.0,Fit model
1.3.0,Evaluate model
1.3.0,Set random seeds
1.3.0,Setup directories
1.3.0,Model constants
1.3.0,Load and transform datasets
1.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.0,Atomic convolution variables
1.3.0,at = atomic numbers (atom types)
1.3.0,"radial basis function parameters [cutoff, mean, width]"
1.3.0,Model hyperparameters
1.3.0,Initialize model
1.3.0,Fit model
1.3.0,Evaluate model
1.3.0,Set random seeds
1.3.0,Setup directories
1.3.0,Model constants
1.3.0,Load and transform datasets
1.3.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.3.0,Atomic convolution variables
1.3.0,at = atomic numbers (atom types)
1.3.0,"radial basis function parameters [cutoff, mean, width]"
1.3.0,Model hyperparameters
1.3.0,Initialize model
1.3.0,Fit model
1.3.0,Evaluate model
1.3.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
1.3.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
1.3.0,test_scores = test_evaluator.compute_model_performance(metric)
1.3.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
1.3.0,param.update(test_scores)
1.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.0,for transformer in transformers:
1.3.0,train_dataset = transformer.transform(train_dataset)
1.3.0,test_dataset = transformer.transform(test_dataset)
1.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.0,for transformer in transformers:
1.3.0,train_dataset = transformer.transform(train_dataset)
1.3.0,test_dataset = transformer.transform(test_dataset)
1.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.0,for transformer in transformers:
1.3.0,train_dataset = transformer.transform(train_dataset)
1.3.0,test_dataset = transformer.transform(test_dataset)
1.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.3.0,for transformer in transformers:
1.3.0,train_dataset = transformer.transform(train_dataset)
1.3.0,test_dataset = transformer.transform(test_dataset)
1.3.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.3.0,Create some directories for analysis
1.3.0,The base_dir holds the results of all analysis
1.3.0,Make directories to store the raw and featurized datasets.
1.3.0,Load PDBBind dataset
1.3.0,Define featurizers
1.3.0,Currently featurizes with shard_size=1
1.3.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.3.0,This could be done with openbabel in python
1.3.0,Compute cells for this molecule. O(constant)
1.3.0,min == max if molecule is planar in some direction
1.3.0,we should still create a bin
1.3.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
1.3.0,Note neighbors contains self!
1.3.0,Associate each atom with cell it belongs to. O(N)
1.3.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.3.0,"conditions, so does wrapround. O(constant)"
1.3.0,"For each atom, loop through all atoms in its cell and neighboring cells."
1.3.0,Accept as neighbors only those within threshold. This computation should be
1.3.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
1.3.0,Sort neighbors by distance
1.3.0,Pick up to max_num_neighbors
1.3.0,Type of data created by this featurizer
1.3.0,assumes that every array is of the same dimension
1.3.0,rem_dataset is remaining portion of dataset
1.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.0,to k-1.
1.3.0,returns list of per column sum of non zero elements
1.3.0,Compute number of actives needed per task.
1.3.0,loop through each column and obtain index required to splice out for
1.3.0,required fraction of hits
1.3.0,Find the first index where the cumulative number of actives equals
1.3.0,the actives_count
1.3.0,Note that np.where tells us last index required to exceed
1.3.0,"actives_count, so we actually want the following location"
1.3.0,TODO(rbharath): Refactor this split method to match API of other splits (or
1.3.0,potentially refactor those to match this.
1.3.0,Handle edge case where frac_split is 1
1.3.0,Create weight matrices fpor two haves.
1.3.0,copy over up to required index for weight first_split
1.3.0,check out if any rows in either w_1 or w_2 are just zeros
1.3.0,"Obtain original x, y, and w arrays and shuffle"
1.3.0,calculate percent split for valid (out of test and valid)
1.3.0,"split test data into valid and test, treating sub test set also as sparse"
1.3.0,rem_dataset is remaining portion of dataset
1.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.0,to k-1.
1.3.0,JSG Assert that split fractions can be written as proper fractions over 10.
1.3.0,This can be generalized in the future with some common demoninator determination.
1.3.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.3.0,Append remaining examples to train
1.3.0,Sort by increasing MW
1.3.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.3.0,for m_idx in cluster:
1.3.0,"continue until we find an active in all the tasks, otherwise we can't"
1.3.0,compute a meaningful AUC
1.3.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.3.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.3.0,Sort from largest to smallest scaffold sets
1.3.0,Sort from largest to smallest scaffold sets
1.3.0,"(n_samples, n_classes)"
1.3.0,"(n_samples, n_tasks, n_classes)"
1.3.0,Save hyperparameters
1.3.0,Guard variable to make sure we don't Restore() this model
1.3.0,from a disk checkpoint more than once.
1.3.0,"Path to save checkpoint files, which matches the"
1.3.0,replicated supervisor's default path.
1.3.0,Lazily created by _get_shared_session().
1.3.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.3.0,when subclass-overridden methods use the same scopes.
1.3.0,Setup graph
1.3.0,Note that we divide by the batch size and not the number of
1.3.0,"non-zero weight examples in the batch.  Also, instead of using"
1.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.0,calculate with div/sum so it stays on the GPU.
1.3.0,aggregated costs
1.3.0,weight decay
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Save an initial checkpoint.
1.3.0,Turns out there are valid cases where we don't want pad-batches
1.3.0,on by default.
1.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.0,Run training op.
1.3.0,Always save a final checkpoint when complete.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,allow_soft_placement=True allows ops without a GPU implementation
1.3.0,to run on the CPU instead.
1.3.0,TODO(rbharath): Is setting train=False right here?
1.3.0,Discard any padded predictions
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,Special case to handle singletasks.
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,TODO(rbharath): Verify this can be safely removed.
1.3.0,"def evaluate(self, dataset, metrics, transformers=[]):"
1.3.0,""""""""
1.3.0,Evaluates the performance of this model on specified dataset.
1.3.0,
1.3.0,Parameters
1.3.0,----------
1.3.0,dataset: dc.data.Dataset
1.3.0,Dataset object.
1.3.0,metric: deepchem.metrics.Metric
1.3.0,Evaluation metric
1.3.0,transformers: list
1.3.0,List of deepchem.transformers.Transformer
1.3.0,Returns
1.3.0,-------
1.3.0,dict
1.3.0,Maps tasks to scores under metric.
1.3.0,""""""""
1.3.0,"evaluator = Evaluator(self, dataset, transformers)"
1.3.0,scores = evaluator.compute_model_performance(metrics)
1.3.0,return scores
1.3.0,checkpoints look like logdir/model.ckpt-N
1.3.0,"self._save_path is ""logdir/model.ckpt"""
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,Note that softmax is already applied in construct_grpah
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,Handle edge case when batch-size is 1.
1.3.0,Prune away any padding that was added
1.3.0,Handle case of 0-dimensional scalar output
1.3.0,Dummy placeholders
1.3.0,Dummy placeholders
1.3.0,## AtomicNet fully-connected layer ops ###
1.3.0,## Atomicnet coordinate transform ops ###
1.3.0,## Atomicnet symmetry function kernel ops ###
1.3.0,## Atomicnet symmetry function ops ###
1.3.0,## Atomcnet symmetry function layer ops ###
1.3.0,We apply the radial pooling filter before atom type conv
1.3.0,to reduce computation
1.3.0,## Misc convenience ops ###
1.3.0,"Copied from the yt_project, commit e8fb57e"
1.3.0,yt/doc/extensions/notebook_sphinxext.py
1.3.0,https://bitbucket.org/yt_analysis/yt/src/e8fb57e66ca42e26052dadf054a5c782740abec9/doc/extensions/notebook_sphinxext.py?at=yt
1.3.0,Almost completely re-written by Matthew Harrigan to use nbconvert v4
1.3.0,1. Uneval notebook
1.3.0,2. Python
1.3.0,3. HTML (execute first)
1.3.0,Set per-cell timeout to 60 seconds
1.3.0,4. Eval'd notebook
1.3.0,Create link to notebook and script files
1.3.0,create notebook node
1.3.0,add dependency
1.3.0,-*- coding: utf-8 -*-
1.3.0,
1.3.0,"deepchem documentation build configuration file, created by"
1.3.0,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
1.3.0,
1.3.0,This file is execfile()d with the current directory set to its
1.3.0,containing dir.
1.3.0,
1.3.0,Note that not all possible configuration values are present in this
1.3.0,autogenerated file.
1.3.0,
1.3.0,All configuration values have a default; values that are commented out
1.3.0,serve to show the default.
1.3.0,"If extensions (or modules to document with autodoc) are in another directory,"
1.3.0,add these directories to sys.path here. If the directory is relative to the
1.3.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.3.0,"sys.path.insert(0, os.path.abspath('.'))"
1.3.0,-- General configuration ------------------------------------------------
1.3.0,"If your documentation needs a minimal Sphinx version, state it here."
1.3.0,needs_sphinx = '1.0'
1.3.0,"Add any Sphinx extension module names here, as strings. They can be"
1.3.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.3.0,ones.
1.3.0,"Add any paths that contain templates here, relative to this directory."
1.3.0,The suffix(es) of source filenames.
1.3.0,You can specify multiple suffix as a list of string:
1.3.0,"source_suffix = ['.rst', '.md']"
1.3.0,The encoding of source files.
1.3.0,source_encoding = 'utf-8-sig'
1.3.0,The master toctree document.
1.3.0,General information about the project.
1.3.0,"The version info for the project you're documenting, acts as replacement for"
1.3.0,"|version| and |release|, also used in various other places throughout the"
1.3.0,built documents.
1.3.0,
1.3.0,The short X.Y version.
1.3.0,"The full version, including alpha/beta/rc tags."
1.3.0,The language for content autogenerated by Sphinx. Refer to documentation
1.3.0,for a list of supported languages.
1.3.0,
1.3.0,This is also used if you do content translation via gettext catalogs.
1.3.0,"Usually you set ""language"" from the command line for these cases."
1.3.0,"There are two options for replacing |today|: either, you set today to some"
1.3.0,"non-false value, then it is used:"
1.3.0,today = ''
1.3.0,"Else, today_fmt is used as the format for a strftime call."
1.3.0,"today_fmt = '%B %d, %Y'"
1.3.0,"List of patterns, relative to source directory, that match files and"
1.3.0,directories to ignore when looking for source files.
1.3.0,The reST default role (used for this markup: `text`) to use for all
1.3.0,documents.
1.3.0,default_role = None
1.3.0,"If true, '()' will be appended to :func: etc. cross-reference text."
1.3.0,add_function_parentheses = True
1.3.0,"If true, the current module name will be prepended to all description"
1.3.0,unit titles (such as .. function::).
1.3.0,add_module_names = True
1.3.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
1.3.0,output. They are ignored by default.
1.3.0,show_authors = False
1.3.0,The name of the Pygments (syntax highlighting) style to use.
1.3.0,A list of ignored prefixes for module index sorting.
1.3.0,modindex_common_prefix = []
1.3.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
1.3.0,keep_warnings = False
1.3.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.3.0,-- Options for HTML output ----------------------------------------------
1.3.0,The theme to use for HTML and HTML Help pages.  See the documentation for
1.3.0,a list of builtin themes.
1.3.0,Theme options are theme-specific and customize the look and feel of a theme
1.3.0,"further.  For a list of options available for each theme, see the"
1.3.0,documentation.
1.3.0,html_theme_options = {}
1.3.0,"Add any paths that contain custom themes here, relative to this directory."
1.3.0,"The name for this set of Sphinx documents.  If None, it defaults to"
1.3.0,"""<project> v<release> documentation""."
1.3.0,html_title = None
1.3.0,A shorter title for the navigation bar.  Default is the same as html_title.
1.3.0,html_short_title = None
1.3.0,The name of an image file (relative to this directory) to place at the top
1.3.0,of the sidebar.
1.3.0,The name of an image file (within the static path) to use as favicon of the
1.3.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
1.3.0,pixels large.
1.3.0,html_favicon = None
1.3.0,"Add any paths that contain custom static files (such as style sheets) here,"
1.3.0,"relative to this directory. They are copied after the builtin static files,"
1.3.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.3.0,Add any extra paths that contain custom files (such as robots.txt or
1.3.0,".htaccess) here, relative to this directory. These files are copied"
1.3.0,directly to the root of the documentation.
1.3.0,html_extra_path = []
1.3.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
1.3.0,using the given strftime format.
1.3.0,"html_last_updated_fmt = '%b %d, %Y'"
1.3.0,"If true, SmartyPants will be used to convert quotes and dashes to"
1.3.0,typographically correct entities.
1.3.0,html_use_smartypants = True
1.3.0,"Custom sidebar templates, maps document names to template names."
1.3.0,html_sidebars = {}
1.3.0,"Additional templates that should be rendered to pages, maps page names to"
1.3.0,template names.
1.3.0,html_additional_pages = {}
1.3.0,"If false, no module index is generated."
1.3.0,html_domain_indices = True
1.3.0,"If false, no index is generated."
1.3.0,html_use_index = True
1.3.0,"If true, the index is split into individual pages for each letter."
1.3.0,html_split_index = False
1.3.0,"If true, links to the reST sources are added to the pages."
1.3.0,html_show_sourcelink = True
1.3.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
1.3.0,html_show_sphinx = True
1.3.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
1.3.0,html_show_copyright = True
1.3.0,"If true, an OpenSearch description file will be output, and all pages will"
1.3.0,contain a <link> tag referring to it.  The value of this option must be the
1.3.0,base URL from which the finished HTML is served.
1.3.0,html_use_opensearch = ''
1.3.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
1.3.0,html_file_suffix = None
1.3.0,Language to be used for generating the HTML full-text search index.
1.3.0,Sphinx supports the following languages:
1.3.0,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
1.3.0,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
1.3.0,html_search_language = 'en'
1.3.0,"A dictionary with options for the search language support, empty by default."
1.3.0,Now only 'ja' uses this config value
1.3.0,html_search_options = {'type': 'default'}
1.3.0,The name of a javascript file (relative to the configuration directory) that
1.3.0,"implements a search results scorer. If empty, the default will be used."
1.3.0,html_search_scorer = 'scorer.js'
1.3.0,Output file base name for HTML help builder.
1.3.0,-- Options for LaTeX output ---------------------------------------------
1.3.0,The paper size ('letterpaper' or 'a4paper').
1.3.0,"'papersize': 'letterpaper',"
1.3.0,"The font size ('10pt', '11pt' or '12pt')."
1.3.0,"'pointsize': '10pt',"
1.3.0,Additional stuff for the LaTeX preamble.
1.3.0,"'preamble': '',"
1.3.0,Latex figure (float) alignment
1.3.0,"'figure_align': 'htbp',"
1.3.0,Grouping the document tree into LaTeX files. List of tuples
1.3.0,"(source start file, target name, title,"
1.3.0,"author, documentclass [howto, manual, or own class])."
1.3.0,The name of an image file (relative to this directory) to place at the top of
1.3.0,the title page.
1.3.0,latex_logo = None
1.3.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
1.3.0,not chapters.
1.3.0,latex_use_parts = False
1.3.0,"If true, show page references after internal links."
1.3.0,latex_show_pagerefs = False
1.3.0,"If true, show URL addresses after external links."
1.3.0,latex_show_urls = False
1.3.0,Documents to append as an appendix to all manuals.
1.3.0,latex_appendices = []
1.3.0,"If false, no module index is generated."
1.3.0,latex_domain_indices = True
1.3.0,-- Options for manual page output ---------------------------------------
1.3.0,One entry per manual page. List of tuples
1.3.0,"(source start file, name, description, authors, manual section)."
1.3.0,"If true, show URL addresses after external links."
1.3.0,man_show_urls = False
1.3.0,-- Options for Texinfo output -------------------------------------------
1.3.0,Grouping the document tree into Texinfo files. List of tuples
1.3.0,"(source start file, target name, title, author,"
1.3.0,"dir menu entry, description, category)"
1.3.0,Documents to append as an appendix to all manuals.
1.3.0,texinfo_appendices = []
1.3.0,"If false, no module index is generated."
1.3.0,texinfo_domain_indices = True
1.3.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
1.3.0,texinfo_show_urls = 'footnote'
1.3.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
1.3.0,texinfo_no_detailmenu = False
1.3.0,Example configuration for intersphinx: refer to the Python standard library.
1.3.0,Higher is Better
1.3.0,The secret key is available as a secure environment variable
1.3.0,on travis-ci to push the build documentation to Amazon S3.
1.3.0,Perform recursive modification to set css mime types.
1.3.0,Perform recursive modification to set js mime types.
1.3.0,lines in the label file have format
1.3.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
1.3.0,"print line[0], line[3]"
1.3.0,Record inputs.
1.3.0,Create the output directory if necessary.
1.3.0,Create duplicate placeholders for meta-optimization.
1.3.0,Create the loss function for meta-optimization.
1.3.0,"In the final loss, use different placeholders for all inputs so the loss will be"
1.3.0,computed from a different batch.
1.3.0,Create variables for accumulating the gradients.
1.3.0,Create the optimizers for meta-optimization and task optimization.
1.3.0,Main optimization loop.
1.3.0,Do checkpointing.
1.3.0,This is a MetaLearner that learns to generate sine functions with variable
1.3.0,amplitude and phase.
1.3.0,Optimize it.
1.3.0,Test it out on some new tasks and see how it works.
1.3.0,Initially the model should do a bad job of fitting the sine function.
1.3.0,After one step of optimization it should do much better.
1.3.0,"Verify that we can create a new MAML object, reload the parameters from the first one, and"
1.3.0,get the same result.
1.3.0,"Do the same thing, only using the ""restore"" argument to fit()."
1.3.0,References
1.3.0,Arguments
1.3.0,Aliases.
1.3.0,Tensorflow correctly processes empty lists when using concat
1.3.0,"Sum along neighbors as well as self, and store"
1.3.0,Sum all neighbors using adjacency matrix
1.3.0,Get collection of modified atom features
1.3.0,Obtain relevant atoms for this degree
1.3.0,Get self atoms
1.3.0,Apply hidden affine to relevant atoms and append
1.3.0,Determine the min_deg=0 case
1.3.0,Only use the self layer
1.3.0,Combine all atoms back into the list
1.3.0,"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
1.3.0,Obtain the partitions for each of the molecules
1.3.0,Sum over atoms for each molecule
1.3.0,Get the final sparse representations
1.3.0,Store the summed atoms by degree
1.3.0,Tensorflow correctly processes empty lists when using concat
1.3.0,Get self atoms
1.3.0,Expand dims
1.3.0,always deg-1 for deg_adj_lists
1.3.0,TODO(rbharath): It's not clear where nb_affine comes from.
1.3.0,Is there a solid explanation here?
1.3.0,Generate the nb_affine weights and biases
1.3.0,Add trainable weights
1.3.0,Extract atom_features
1.3.0,Extract graph topology
1.3.0,Perform the mol conv
1.3.0,Extract nodes and membership
1.3.0,Extract atom_features
1.3.0,Extract graph topology
1.3.0,Perform the mol gather
1.3.0,Extract nodes
1.3.0,Extract atom_features
1.3.0,Extract graph topology
1.3.0,Perform the mol gather
1.3.0,"x is test set, xp is support set."
1.3.0,# Initializes trainable weights.
1.3.0,## Performs computations
1.3.0,Get initializations
1.3.0,r = self.r_init
1.3.0,Process using attention
1.3.0,"Eqn (4), appendix A.1 of Matching Networks paper"
1.3.0,Generate new aattention states
1.3.0,"def build(self, input_shape):"
1.3.0,"_, support_input_shape = input_shape  #Unpack"
1.3.0,n_feat = support_input_shape[1]
1.3.0,Support set lstm
1.3.0,Test lstm
1.3.0,Get initializations
1.3.0,Rename support
1.3.0,Process support xp using attention
1.3.0,Get linear combination of support set
1.3.0,Not sure if it helps to place the update here or later yet.  Will
1.3.0,decide
1.3.0,z = r
1.3.0,Process test x using attention
1.3.0,Generate new support attention states
1.3.0,Generate new test attention states
1.3.0,Redefine
1.3.0,"return [x+p, z+q]"
1.3.0,No other forget biases supported right now.
1.3.0,"def build(self, input_shape):"
1.3.0,Taken from Keras code [citation needed]
1.3.0,###################################################### DEBUG
1.3.0,"return o, [h, c]"
1.3.0,###################################################### DEBUG
1.3.0,"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
1.3.0,distance_hidden = self.activation(distance_hidden)
1.3.0,atom_features_hidden = self.activation(atom_features_hidden)
1.3.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.3.0,and embeddings of atom j(both gone through a hidden layer)
1.3.0,"for atom i, sum the influence from all other atom j in the molecule"
1.3.0,number of inputs each step
1.3.0,Add trainable weights
1.3.0,Extract atom_features
1.3.0,Basic features of every atom: (batch_size*max_atoms) * n_atom_features
1.3.0,calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
1.3.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.3.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.3.0,"step i calculates the graph features for atoms of index `parents[:,i,0]`"
1.3.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.3.0,"represent the same atoms of `parents[:, :, 0]`,"
1.3.0,different in that these index are positions in `atom_features`
1.3.0,"number of atoms in total, should equal `batch_size*max_atoms`"
1.3.0,initialize graph features for each graph
1.3.0,another row of zeros is generated for padded dummy atoms
1.3.0,`count`-th step
1.3.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.3.0,generating index for graph features used in the inputs
1.3.0,"extracting graph features for parents of the target atoms, then flatten"
1.3.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.3.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.3.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.3.0,of shape: (batch_size*max_atoms) * n_graph_features
1.3.0,representing the graph features of target atoms in each graph
1.3.0,index for targe atoms
1.3.0,update the graph features for target atoms
1.3.0,last step generates graph features for all target atom
1.3.0,Add trainable weights
1.3.0,Extract atom_features
1.3.0,sum all graph outputs
1.3.0,Aliases.
1.3.0,TODO(rbharath): What does this line do?
1.3.0,TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
1.3.0,This dictionary holds a mapping {graph: learning_phase}.
1.3.0,A learning phase is a bool tensor used to run Keras models in
1.3.0,either train mode (learning_phase == 1) or test mode (learning_phase == 0).
1.3.0,else: assume learning phase is a placeholder tensor.
1.3.0,need broadcasting
1.3.0,ensure that randomness is conditioned by the Numpy RNG
1.3.0,ensure that randomness is conditioned by the Numpy RNG
1.3.0,TODO(rbharath): Should probably swap this over to tf mode.
1.3.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.3.0,"expects logits, Keras expects probabilities."
1.3.0,scale preds so that the class probas of each sample sum to 1
1.3.0,manual computation of crossentropy
1.3.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.3.0,"expects logits, Keras expects probabilities."
1.3.0,if our output includes timesteps we need to reshape
1.3.0,Arguments
1.3.0,Returns
1.3.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.3.0,"expects logits, Keras expects probabilities."
1.3.0,transform back to logits
1.3.0,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
1.3.0,a tensor. Confusing with tf.zeros...
1.3.0,Transpose for mul
1.3.0,exclude bias variables
1.3.0,"tf.scalar_summary('Weight Decay Cost', cost)"
1.3.0,TODO(user): gradient clipping (see Minimize)
1.3.0,These properties should have been set
1.3.0,"by the child class, as appropriate."
1.3.0,These properties should be set by the user via keyword arguments.
1.3.0,"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
1.3.0,are only applicable to input layers: do not pass these keywords
1.3.0,to non-input layers.
1.3.0,In this case we will create an input layer
1.3.0,to insert before the current layer
1.3.0,Update self.losses
1.3.0,In case self.losses isn't settable
1.3.0,(i.e. it's a getter method).
1.3.0,In that case the `losses` property is
1.3.0,auto-computed and shouldn't be set.
1.3.0,Update self._per_input_updates
1.3.0,Updates indexed by None are unconditional
1.3.0,rather than input-dependent
1.3.0,outputs = to_list(self.call(x))
1.3.0,return outputs
1.3.0,TODO(rbharath): Keras uses a global var here to maintain
1.3.0,unique counts. This seems dangerous. How does tensorflow handle?
1.3.0,TODO(rbharath): Support this type of functional API.
1.3.0,If batch size not specified
1.3.0,Input shape
1.3.0,Output shape
1.3.0,References
1.3.0,Not Trainable
1.3.0,Not Trainable
1.3.0,need broadcasting
1.3.0,pick the normalized form of x corresponding to the training phase
1.3.0,sample-wise normalization
1.3.0,from deepchem.nn.model_ops import variable
1.3.0,Assuming convolution kernels (2D or 3D).
1.3.0,"TF kernel shape: (..., input_depth, depth)"
1.3.0,No specific assumptions.
1.3.0,References
1.3.0,References
1.3.0,References
1.3.0,References
1.3.0,Pick the one with the correct shape.
1.3.0,Arguments
1.3.0,Aliases.
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,Add trainable weights
1.3.0,Add trainable weights
1.3.0,Add trainable weights
1.3.0,Add trainable weights
1.3.0,"Output should be of shape (?, nb_filter)"
1.3.0,"Output should be of shape (batch_size, n_feat)"
1.3.0,Try concatenating the two lists of placeholders
1.3.0,Try concatenating the two lists of placeholders
1.3.0,Fit model on dataset
1.3.0,Fit model on dataset
1.3.0,"Should be an array of size (n_pocket_atoms, 3)"
1.3.0,"coords[triangle, 0] gives the x-dimension of all triangle points"
1.3.0,Take transpose to make sure rows correspond to atoms.
1.3.0,We voxelize so all grids have integral coordinates (convenience)
1.3.0,"If overlap of box with previously generated output boxes, return"
1.3.0,Carry forward mappings
1.3.0,We know that box has at least one atom not in outputs
1.3.0,Current box has been merged into box further down list.
1.3.0,No need to output current box
1.3.0,"protein_coords is (N, 3) tensor"
1.3.0,Load binding pocket model
1.3.0,TODO(rbharath): Shift refined to full once trained.
1.3.0,Fit model on dataset
1.3.0,Create featurizers
1.3.0,"if not ligand_file.endswith("".sdf""):"
1.3.0,"raise ValueError(""Only .sdf ligand files can be featurized."")"
1.3.0,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
1.3.0,ligand_mol2 = os.path.join(
1.3.0,"self.base_dir, ligand_basename + "".mol2"")"
1.3.0,
1.3.0,# Write mol2 file for ligand
1.3.0,obConversion = ob.OBConversion()
1.3.0,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
1.3.0,ob_mol = ob.OBMol()
1.3.0,"obConversion.ReadFile(ob_mol, str(ligand_file))"
1.3.0,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
1.3.0,
1.3.0,# Featurize ligand
1.3.0,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
1.3.0,if mol is None:
1.3.0,"return None, None"
1.3.0,# Default for CircularFingerprint
1.3.0,n_ligand_features = 1024
1.3.0,ligand_features = self.ligand_featurizer.featurize([mol])
1.3.0,
1.3.0,# Featurize pocket
1.3.0,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
1.3.0,"protein_file, ligand_file)"
1.3.0,n_pockets = len(pockets)
1.3.0,n_pocket_features = BindingPocketFeaturizer.n_features
1.3.0,
1.3.0,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
1.3.0,pocket_features = self.pocket_featurizer.featurize(
1.3.0,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
1.3.0,# Note broadcast operation
1.3.0,"features[:, :n_pocket_features] = pocket_features"
1.3.0,"features[:, n_pocket_features:] = ligand_features"
1.3.0,dataset = NumpyDataset(X=features)
1.3.0,pocket_preds = self.model.predict(dataset)
1.3.0,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
1.3.0,
1.3.0,# Find pockets which are active
1.3.0,active_pockets = []
1.3.0,active_pocket_atoms_map = {}
1.3.0,active_pocket_coords = []
1.3.0,for pocket_ind in range(len(pockets)):
1.3.0,#################################################### DEBUG
1.3.0,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
1.3.0,#if pocket_preds[pocket_ind] == 1:
1.3.0,if pocket_pred_proba[pocket_ind][1] > .15:
1.3.0,#################################################### DEBUG
1.3.0,pocket = pockets[pocket_ind]
1.3.0,active_pockets.append(pocket)
1.3.0,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
1.3.0,active_pocket_coords.append(pocket_coords[pocket_ind])
1.3.0,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
1.3.0,# TODO(LESWING)
1.3.0,"TODO(rbharath, enf): Figure out why pi_stack is slow and cation_pi"
1.3.0,causes segfaults.
1.3.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.3.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.3.0,TODO(rbharath): May want to move this file to S3 so we can ensure it's
1.3.0,always available.
1.3.0,Prepare receptor
1.3.0,Get protein centroid and range
1.3.0,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
1.3.0,going to be extremely slow!
1.3.0,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
1.3.0,first pocket.
1.3.0,Prepare receptor
1.3.0,TODO(rbharath): Generalize this so can support mol2 files as well.
1.3.0,Write Vina conf file
1.3.0,Define locations of log and output files
1.3.0,TODO(rbharath): Let user specify the number of poses required.
1.3.0,TODO(rbharath): Convert the output pdbqt to a pdb file.
1.3.0,Return docked files
1.3.0,Check returned files exist
1.3.0,Check returned files exist
1.3.0,Check returned files exist
1.3.0,Check returned files exist
1.3.0,Check returned files exist
1.3.0,Note this may download autodock Vina...
1.3.0,Note this may download autodock Vina...
1.3.0,Note this may download autodock Vina...
1.3.0,Check returned files exist
1.3.0,Note this may download autodock Vina...
1.3.0,Check returned files exist
1.3.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
1.3.0,box1 contained in box2
1.3.0,"box1 in box2, so complete overlap"
1.3.0,"4/5 atoms in box2 in box1, so 80 % overlap"
1.3.0,box2 contains box1
1.3.0,box1 contains box2
1.3.0,"box1 contains box2, box3"
1.3.0,Test that every atom in pocket maps exists
1.3.0,Check that the atoms is actually in protein
1.3.0,Test that every atom in pocket maps exists
1.3.0,Check that the atoms is actually in protein
1.3.0,Add active site to dict
1.3.0,The convention used is that the first task is the metric.
1.3.0,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
1.3.0,"an option in the Metric class. Instead, this should be possible to move into"
1.3.0,user-space as a custom task_averager function.
1.3.0,"TODO(rbharath, joegomes): What is this magic number?"
1.3.0,"If there are no nonzero examples, metric is ill-defined."
1.3.0,TODO(rbharath): This has been a major source of bugs. Is there a more
1.3.0,robust characterization of which metrics require class-probs and which
1.3.0,don't?
1.3.0,Reshape to handle 1-d edge cases
1.3.0,ids = df[id_field].values
1.3.0,Set missing data to have weight zero
1.3.0,TODO (ytz) this is a bandage solution to reorder the atoms so
1.3.0,that they're always in the same canonical order. Presumably this
1.3.0,should be correctly implemented in the future for graph mols.
1.3.0,Featurize task results iff they exist.
1.3.0,Filter out examples where featurization failed.
1.3.0,"For prospective data where results are unknown, it makes"
1.3.0,no sense to have y values or weights.
1.3.0,Remove support indices
1.3.0,Remove support indices
1.3.0,Remove support indices
1.3.0,Get task specific entries
1.3.0,Now just get weights for this task
1.3.0,Get task specific entries
1.3.0,Now just get weights for this task
1.3.0,Now just get weights for this task
1.3.0,Now just get weights for this task
1.3.0,Split data into pos and neg lists.
1.3.0,No replacement allowed for supports
1.3.0,Handle one-d vs. non one-d feature matrices
1.3.0,Init the iterator
1.3.0,Set initial iterator state
1.3.0,support = self.supports[task][self.trial_num]
1.3.0,Increment and update logic
1.3.0,Init the iterator
1.3.0,Set initial iterator state
1.3.0,support = self.supports[task][self.trial_num]
1.3.0,Increment and update logic
1.3.0,"By invariant of when this is called, can assume num_samples > 0"
1.3.0,and num_samples < batch_size
1.3.0,Fill in batch arrays
1.3.0,"By invariant of when this is called, can assume num_samples > 0"
1.3.0,and num_samples < batch_size
1.3.0,Fill in batch arrays
1.3.0,Only the first set of copy will be counted in training loss
1.3.0,The -1 indicates that y will be reshaped to have length -1
1.3.0,"Set labels to be zero, with zero weights"
1.3.0,note that this corresponds to the _construct_metadata column order
1.3.0,if not len(self.metadata_df):
1.3.0,"raise ValueError(""No data in dataset."")"
1.3.0,return next(self.metadata_df.iterrows())[1]['task_names']
1.3.0,Create temp directory to store resharded version
1.3.0,Write data in new shards
1.3.0,Handle spillover from last shard
1.3.0,These columns may be missing is the dataset is unlabelled.
1.3.0,"TODO(rbharath): This happens in tests sometimes, but don't understand why?"
1.3.0,Handle edge case.
1.3.0,if data_dir is None:
1.3.0,data_dir = tempfile.mkdtemp()
1.3.0,The -1 indicates that y will be reshaped to have length -1
1.3.0,"raw_data = (X, y, w, ids)"
1.3.0,Get full dataset in memory
1.3.0,Shuffle in memory
1.3.0,Write shuffled shards out to disk
1.3.0,Shuffle the arrays corresponding to each row in metadata_df
1.3.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
1.3.0,Handle edge case with empty indices
1.3.0,Find indices which rest in this shard
1.3.0,Need to offset indices to fit within shard_size
1.3.0,Handle the case of datasets with y/w missing
1.3.0,Updating counts
1.3.0,Break when all indices have been used up already
1.3.0,TODO(rbharath): Get rid of * import
1.3.0,Load MUV dataset
1.3.0,Do an approximate comparison since splits are sometimes slightly off from
1.3.0,the exact fraction.
1.3.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
1.3.0,reloading will cause the transform to be reapplied. This is undesirable in
1.3.0,almost all cases. Need to understand a method to fix this.
1.3.0,def test_shuffle(self):
1.3.0,"""""""Test that datasets can be merged."""""""
1.3.0,current_dir = os.path.dirname(os.path.realpath(__file__))
1.3.0,dataset_file = os.path.join(
1.3.0,"current_dir, ""../../models/tests/example.csv"")"
1.3.0,featurizer = dc.feat.CircularFingerprint(size=1024)
1.3.0,"tasks = [""log-solubility""]"
1.3.0,loader = dc.data.CSVLoader(
1.3.0,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
1.3.0,"dataset = loader.featurize(dataset_file, shard_size=2)"
1.3.0,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
1.3.0,dataset.ids)
1.3.0,orig_len = len(dataset)
1.3.0,dataset.shuffle(iterations=5)
1.3.0,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
1.3.0,dataset.ids)
1.3.0,
1.3.0,assert len(dataset) == orig_len
1.3.0,# The shuffling should have switched up the ordering
1.3.0,"assert not np.array_equal(orig_ids, new_ids)"
1.3.0,# But all the same entries should still be present
1.3.0,assert sorted(orig_ids) == sorted(new_ids)
1.3.0,# All the data should have same shape
1.3.0,assert X_orig.shape == X_new.shape
1.3.0,assert y_orig.shape == y_new.shape
1.3.0,assert w_orig.shape == w_new.shape
1.3.0,The shuffling should have switched up the ordering
1.3.0,But all the same entries should still be present
1.3.0,All the data should have same shape
1.3.0,The ids should now store the performed permutation. Check that the
1.3.0,original dataset is recoverable.
1.3.0,The ids should now store the performed permutation. Check that the
1.3.0,original dataset is recoverable.
1.3.0,Set some global variables up top
1.3.0,Featurize emols dataset
1.3.0,Generate dummy dataset
1.3.0,Generate dummy dataset
1.3.0,Generate dummy dataset
1.3.0,Set last n_samples/2 weights to 0
1.3.0,Check that no support elements are sample from zero-weight samples
1.3.0,Generate dummy dataset
1.3.0,Generate dummy dataset
1.3.0,Create support generator
1.3.0,Generate dummy dataset
1.3.0,Create support generator
1.3.0,Generate dummy dataset
1.3.0,Assert all support elements have been removed
1.3.0,Generate dummy dataset
1.3.0,Assert all remove elements have been removed
1.3.0,Generate dummy dataset
1.3.0,Assert all support elements have been removed
1.3.0,Generate dummy dataset
1.3.0,Assert all remove elements have been removed
1.3.0,Generate dummy dataset
1.3.0,Set last n_samples/2 weights to 0
1.3.0,Sample from first n_samples/2 elements for support
1.3.0,Should lie within first n_samples/2 samples only
1.3.0,Generate dummy dataset
1.3.0,Create support generator
1.3.0,Generate dummy dataset
1.3.0,Test on identity matrix
1.3.0,Generate random sparse features dataset
1.3.0,Test edge case with array of all zeros
1.3.0,Test cases where n_samples < 2*n_samples < batch_size
1.3.0,Test cases where n_samples < batch_size
1.3.0,Test case where n_samples == batch_size
1.3.0,Test case for object featurization.
1.3.0,Test case for more complicated object featurization
1.3.0,Test case with multidimensional data
1.3.0,Test cases where n_samples < 2*n_samples < batch_size
1.3.0,Test cases where n_samples < batch_size
1.3.0,Test case where n_samples == batch_size
1.3.0,Test case for object featurization.
1.3.0,Test case for more complicated object featurization
1.3.0,Test case with multidimensional data
1.3.0,Test first resharding worked
1.3.0,Test second resharding worked
1.3.0,Generate data
1.3.0,Generate data
1.3.0,Generate data
1.3.0,Transform it
1.3.0,Transform it
1.3.0,Splits featurized samples into train/test
1.3.0,Splits featurized samples into train/test
1.3.0,Splits featurized samples into train/test
1.3.0,"splittype = ""random"""
1.3.0,Splits featurized samples into train/test
1.3.0,Now perform move
1.3.0,Only for debug!
1.3.0,#Make directories to store the raw and featurized datasets.
1.3.0,Load dataset
1.3.0,Featurize tox21 dataset
1.3.0,###### Do featurization
1.3.0,Do train/valid split.
1.3.0,###### Do singletask load
1.3.0,################# Do comparison
1.3.0,Only for debug!
1.3.0,Set some global variables up top
1.3.0,Make directories to store the raw and featurized datasets.
1.3.0,Load dataset
1.3.0,Featurize tox21 dataset
1.3.0,For debugging purposes
1.3.0,###### Do multitask load
1.3.0,Do train/valid split.
1.3.0,###### Do singletask load
1.3.0,################# Do comparison
1.3.0,"task_type = ""regression"""
1.3.0,coding=utf-8
1.3.0,Note that transformers have to be undone in reversed order
1.3.0,Hack to allow for easy unpickling:
1.3.0,http://stefaanlippens.net/pickleproblem
1.3.0,"One, but not both, transform_X or tranform_y is true"
1.3.0,Use fact that bools add as ints in python
1.3.0,Control for pathological case with no variance.
1.3.0,"Get the reversed shape of z: (..., n_tasks, batch_size)"
1.3.0,Find the task dimension of z
1.3.0,Prevent broadcasting on wrong dimension
1.3.0,BalancingTransformer can only transform weights.
1.3.0,Compute weighting factors from dataset.
1.3.0,Ensure dataset is binary
1.3.0,Remove labels with zero weights
1.3.0,self.w = dataset.w
1.3.0,"TODO (flee2): for transform_y, figure out weights"
1.3.0,"print(""y will not be transformed by CDFTransformer, for now."")"
1.3.0,"print(""Cannot undo CDF Transformer, for now."")"
1.3.0,Need this for transform_y
1.3.0,array = np.transpose(array)
1.3.0,"print(""y will not be transformed by PowerTransformer, for now."")"
1.3.0,"print(""Cannot undo Power Transformer, for now."")"
1.3.0,the tf graph here pick up the (K+1) highest similarity values
1.3.0,and their indices
1.3.0,map the indices to labels
1.3.0,generating batch of data by slicing similarity matrix
1.3.0,into 100*reference_dataset_length
1.3.0,concatenate batches of data together
1.3.0,highest similarity is 1: target is in the reference
1.3.0,use the following K points
1.3.0,"highest less than 1: target not in the reference, use top K points"
1.3.0,calculate matrix multiplicatin on slices
1.3.0,concatenate the slices together
1.3.0,list of calculation orders for DAGs
1.3.0,stemming from one specific atom in the molecule
1.3.0,starting from the adjacency list derived by graphconv featurizer
1.3.0,"number of atoms, also number of DAGs"
1.3.0,"DAG on a molecule with k atoms includes k steps of calculation,"
1.3.0,each step calculating graph features for one atom.
1.3.0,`max_atoms` is the maximum number of steps
1.3.0,each iteration generates the DAG starting from atom with index `count`
1.3.0,"list of lists, elements represent the calculation orders"
1.3.0,for atoms in the current graph
1.3.0,starting from the target atom with index `count`
1.3.0,flags of whether the atom is already included in the DAG
1.3.0,atom `count` is in the DAG
1.3.0,recording number of radial propagation steps
1.3.0,"in the fisrt loop, atoms directly connected to `count` will be added"
1.3.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
1.3.0,will be added in the second loop(radial=1).
1.3.0,atoms i-bond away will be added in i-th loop
1.3.0,"when molecules have separate parts, starting from one part,"
1.3.0,it is not possible to include all atoms.
1.3.0,this break quit the loop when going into such condition
1.3.0,reinitialize targets for next iteration
1.3.0,atoms connected to current_atom
1.3.0,generate the dependency map of current DAG
1.3.0,atoms connected to `current_atoms`(and not included in the DAG)
1.3.0,"are added, and will be the `current_atoms` for next iteration."
1.3.0,"DAG starts from the target atom, calculation should go in reverse"
1.3.0,`edge[1]` is the parent of `edge[0]`
1.3.0,"after this loop, `parents[i]` includes all parents of atom i"
1.3.0,manually adding the atom index into its parents list
1.3.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
1.3.0,atoms with less parents(farther from the target atom) come first.
1.3.0,"graph features of atoms without parents will be first calculated,"
1.3.0,then atoms with more parents can be calculated in order
1.3.0,based on previously calculated graph features.
1.3.0,target atom of this DAG will be calculated in the last step
1.3.0,padding with `max_atoms`
1.3.0,padding
1.3.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
1.3.0,which is a max_atoms * max_atoms numpy array after padding
1.3.0,Calculate pairwise distance
1.3.0,Masking for valid atom index
1.3.0,Cutoff with threshold Rc
1.3.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is a y transformer
1.3.0,Check w is unchanged since this is a y transformer
1.3.0,Check y is now a logarithmic version of itself
1.3.0,Check that untransform does the right thing.
1.3.0,transforming y should raise an exception
1.3.0,transforming w should raise an exception
1.3.0,transforming X should be okay
1.3.0,Check ids are unchanged.
1.3.0,Check y is unchanged since this is a X transformer
1.3.0,Check w is unchanged since this is a y transformer
1.3.0,Check y is now a logarithmic version of itself
1.3.0,Check that untransform does the right thing.
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is a y transformer
1.3.0,Check w is unchanged since this is a y transformer
1.3.0,Check y is now a logarithmic version of itself
1.3.0,Check that untransform does the right thing.
1.3.0,Tests logarithmic data transformer with selection.
1.3.0,Check ids are unchanged.
1.3.0,Check y is unchanged since this is a X transformer
1.3.0,Check w is unchanged since this is a y transformer
1.3.0,Check y is now a logarithmic version of itself
1.3.0,Check that untransform does the right thing.
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is a y transformer
1.3.0,Check w is unchanged since this is a y transformer
1.3.0,"Check that y_t has zero mean, unit std."
1.3.0,Check that untransform does the right thing.
1.3.0,Check ids are unchanged.
1.3.0,Check y is unchanged since this is a X transformer
1.3.0,Check w is unchanged since this is a y transformer
1.3.0,"Check that X_t has zero mean, unit std."
1.3.0,np.set_printoptions(threshold='nan')
1.3.0,Entries with zero std are not normalized
1.3.0,TODO(rbharath): Untransform doesn't work properly for binary feature
1.3.0,vectors. Need to figure out what's wrong here. (low priority)
1.3.0,# Check that untransform does the right thing.
1.3.0,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
1.3.0,Check ids are unchanged.
1.3.0,Check y is unchanged since this is an X transformer
1.3.0,Check w is unchanged since this is an X transformer
1.3.0,Check X is now holding the proper values when sorted.
1.3.0,Test CDF transformer on Gaussian normal dataset.
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is an y transformer
1.3.0,Check w is unchanged since this is an y transformer
1.3.0,Check y is now holding the proper values when sorted.
1.3.0,Check that untransform does the right thing.
1.3.0,Check ids are unchanged.
1.3.0,Check y is unchanged since this is an X transformer
1.3.0,Check w is unchanged since this is an X transformer
1.3.0,Check X is now holding the proper values when sorted.
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is a y transformer
1.3.0,Check w is unchanged since this is a y transformer
1.3.0,Check y is now holding the proper values when sorted.
1.3.0,Check ids are unchanged.
1.3.0,Check y is unchanged since this is an X transformer
1.3.0,Check w is unchanged since this is an X transformer
1.3.0,Check X is now holding the proper values in each column.
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is an X transformer
1.3.0,Check w is unchanged since this is an X transformer
1.3.0,Check y is now holding the proper values in each column.
1.3.0,Check that untransform does the right thing.
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is a w transformer
1.3.0,Check y is unchanged since this is a w transformer
1.3.0,Assert that entries with zero weight retain zero weight
1.3.0,Check that sum of 0s equals sum of 1s in transformed for each task
1.3.0,Check ids are unchanged.
1.3.0,Check X is unchanged since this is a w transformer
1.3.0,Check y is unchanged since this is a w transformer
1.3.0,Assert that entries with zero weight retain zero weight
1.3.0,Check that sum of 0s equals sum of 1s in transformed for each task
1.3.0,TODO(rbharath): Use standard joblib once old-data has been regenerated.
1.3.0,"If gzipped, need to compute extension again"
1.3.0,Tasks are stored in .sdf.csv file
1.3.0,Structures are stored in .sdf file
1.3.0,First line of user-specified CSV *must* be header.
1.3.0,Try older joblib version for legacy files.
1.3.0,First line of user-specified CSV *must* be header.
1.3.0,First line of user-specified CSV *must* be header.
1.3.0,combine dataframes
1.3.0,working-with-3d-molecules
1.3.0,initial embedding
1.3.0,minimization and pruning
1.3.0,always keep lowest-energy conformer
1.3.0,discard conformers after max_conformers is reached
1.3.0,get RMSD to selected conformers
1.3.0,discard conformers within the RMSD threshold
1.3.0,create a new molecule to hold the chosen conformers
1.3.0,this ensures proper conformer IDs and energy-based ordering
1.3.0,TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
1.3.0,import nglview
1.3.0,import tempfile
1.3.0,import os
1.3.0,import mdtraj as md
1.3.0,import numpy as np
1.3.0,import tempfile
1.3.0,from rdkit import Chem
1.3.0,from rdkit.Chem import Draw
1.3.0,from itertools import islice
1.3.0,"from IPython.display import Image, HTML, display"
1.3.0,
1.3.0,"def combine_mdtraj(protein, ligand):"
1.3.0,chain = protein.topology.add_chain()
1.3.0,"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
1.3.0,for atom in ligand.topology.atoms:
1.3.0,"protein.topology.add_atom(atom.name, atom.element, residue)"
1.3.0,"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
1.3.0,protein.topology.create_standard_bonds()
1.3.0,return protein
1.3.0,
1.3.0,def visualize_complex(complex_mdtraj):
1.3.0,"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
1.3.0,"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
1.3.0,binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
1.3.0,binding_pocket_residues = [str(r) for r in binding_pocket_residues]
1.3.0,"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
1.3.0,
1.3.0,traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
1.3.0,ngltraj = nglview.NGLWidget( traj )
1.3.0,ngltraj.representations = [
1.3.0,"{ ""type"": ""cartoon"", ""params"": {"
1.3.0,"""sele"": ""protein"", ""color"": ""residueindex"""
1.3.0,"} },"
1.3.0,"{ ""type"": ""licorice"", ""params"": {"
1.3.0,"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
1.3.0,"} },"
1.3.0,"{ ""type"": ""ball+stick"", ""params"": {"
1.3.0,"""sele"": ""LIG"""
1.3.0,} }
1.3.0,]
1.3.0,return ngltraj
1.3.0,
1.3.0,def visualize_ligand(ligand_mdtraj):
1.3.0,traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
1.3.0,ngltraj = nglview.NGLWidget( traj )
1.3.0,ngltraj.representations = [
1.3.0,"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
1.3.0,return ngltraj
1.3.0,
1.3.0,def convert_lines_to_mdtraj(molecule_lines):
1.3.0,tempdir = tempfile.mkdtemp()
1.3.0,"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
1.3.0,"with open(molecule_file, ""wb"") as f:"
1.3.0,f.writelines(molecule_lines)
1.3.0,molecule_mdtraj = md.load(molecule_file)
1.3.0,return molecule_mdtraj
1.3.0,
1.3.0,def display_images(filenames):
1.3.0,"""""""Helper to pretty-print images."""""""
1.3.0,imagesList=''.join(
1.3.0,"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
1.3.0,% str(s) for s in sorted(filenames)])
1.3.0,display(HTML(imagesList))
1.3.0,
1.3.0,"def mols_to_pngs(mols, basename=""test""):"
1.3.0,"""""""Helper to write RDKit mols to png files."""""""
1.3.0,filenames = []
1.3.0,"for i, mol in enumerate(mols):"
1.3.0,"filename = ""%s%d.png"" % (basename, i)"
1.3.0,"Draw.MolToFile(mol, filename)"
1.3.0,filenames.append(filename)
1.3.0,return filenames
1.3.0,TODO(rbharath): This is now simple enough that we should probably get rid of
1.3.0,Evaluator object to avoid clutter.
1.3.0,Compute multitask metrics
1.3.0,Compute multitask metrics
1.3.0,Loosening atol to see if tests stop failing sporadically
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,a*x + b*y + c*z = dI think that
1.3.0,"self.x, self.y, self.z = x, y, z"
1.3.0,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
1.3.0,TODO(bramsundar): Should this be __copy__?
1.3.0,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
1.3.0,"return np.array([self.x, self.y, self.z])"
1.3.0,TODO(rbharath): Should this be an atom function?
1.3.0,"This line is necessary for babel to work, though many PDBs in"
1.3.0,the PDB would have this line commented out
1.3.0,now atom type (for pdbqt)
1.3.0,"If atomtype is not specified, but atomname is, set atomtype to the"
1.3.0,"first letter of atomname. This heuristic suffices for proteins,"
1.3.0,since no two-letter elements appear in standard amino acids.
1.3.0,Any number needs to be removed from the element name
1.3.0,"this only uses the rightmost three characters, essentially"
1.3.0,removing unique rotamer identification
1.3.0,"The normal vector to plane is n = [a, b, c]"
1.3.0,We first shift by basepoint (a point on given plane) to make math
1.3.0,simpler. basepoint is given by d/||n||^2 * n
1.3.0,The perpendicular component of diff to plane is
1.3.0,(n^T diff / ||n||^2) * n
1.3.0,TODO(LESWING)
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Get the degree id list (which corrects for min_deg)
1.3.0,Get the size of each degree block
1.3.0,Get the the start indices for items in each block
1.3.0,Get the node indices when they are reset when the degree changes
1.3.0,Convert to numpy array
1.3.0,Reorder old atom_features
1.3.0,Reorder old deg lists
1.3.0,Sort membership
1.3.0,Create old to new dictionary. not exactly intuitive
1.3.0,Reorder adjacency lists
1.3.0,Get numpy version of degree list for indexing
1.3.0,"Initialize adj_lists, which supports min_deg = 1 only"
1.3.0,Parse as deg separated
1.3.0,Get indices corresponding to the current degree
1.3.0,Extract and save adjacency list for the current degree
1.3.0,Construct the slice information
1.3.0,Get the cumulative indices after the first index
1.3.0,Set indices with zero sized slices to zero to avoid indexing errors
1.3.0,TODO(rbharath): Can this be removed?
1.3.0,Use random insted of zeros to prevent weird issues with summing to zero
1.3.0,Get atoms by degree
1.3.0,stack the atoms
1.3.0,Sort all atoms by degree.
1.3.0,"Get the size of each atom list separated by molecule id, then by degree"
1.3.0,Get the final size of each degree block
1.3.0,"Get the index at which each degree starts, not resetting after each degree"
1.3.0,And not stopping at any speciic molecule
1.3.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
1.3.0,first column telling the start indices of each degree block and the
1.3.0,second colum telling the size of each degree block
1.3.0,Input for tensorflow
1.3.0,Determines the membership (atom i belongs to membership[i] molecule)
1.3.0,"Get the index at which each deg starts, resetting after each degree"
1.3.0,(deg x num_mols) matrix describing the start indices when you count up the atoms
1.3.0,"in the final representation, stopping at each molecule,"
1.3.0,resetting every time the degree changes
1.3.0,Gets the degree resetting block indices for the atoms in each molecule
1.3.0,"Here, the indices reset when the molecules change, and reset when the"
1.3.0,degree changes
1.3.0,Get the degree id lookup list. It allows us to search for the degree of a
1.3.0,molecule mol_id with corresponding atom mol_atom_id using
1.3.0,"deg_id_lists[mol_id,mol_atom_id]"
1.3.0,This is used for convience in the following function (explained below)
1.3.0,Get the degree id (corrected for min_deg) of the considered atom
1.3.0,Return the final index of atom mol_atom_id in molecule mol_id.  Using
1.3.0,"the degree of this atom, must find the index in the molecule's original"
1.3.0,"degree block corresponding to degree id deg_id (second term), and then"
1.3.0,calculate which index this degree block ends up in the final
1.3.0,representation (first term). The sum of the two is the final indexn
1.3.0,Initialize the new degree separated adjacency lists
1.3.0,Update the old adjcency lists with the new atom indices and then combine
1.3.0,all together
1.3.0,Iterate through all the molecules
1.3.0,Get the adjacency lists for this molecule and current degree id
1.3.0,"Correct all atom indices to the final indices, and then save the"
1.3.0,results into the new adjacency lists
1.3.0,Increment once row is done
1.3.0,Get the final aggregated molecule
1.3.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
1.3.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
1.3.0,consistent with most QM software packages.
1.3.0,Type of data created by this featurizer
1.3.0,TODO(rbharath): Should this return a list?
1.3.0,Type of data created by this featurizer
1.3.0,generate SMILES for fragments
1.3.0,Initalize with 1
1.3.0,Allow 0 index to correspond to null molecule 1
1.3.0,Correct for null
1.3.0,"print(6-k-1, id)"
1.3.0,Correct for last one
1.3.0,"In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`"
1.3.0,first `bt_len` features are bond features(if applicable)
1.3.0,`bt_len`-th feature is if the pair of atoms are in the same ring
1.3.0,graph distance between two atoms
1.3.0,Euclidean distance between atoms
1.3.0,atoms `radial` bonds away from `a1`
1.3.0,atoms less than `radial` bonds away
1.3.0,find atoms `radial`+1 bonds away
1.3.0,"Since ConvMol is an object and not a numpy array, need to set dtype to"
1.3.0,object.
1.3.0,Get the node features
1.3.0,Stack nodes into an array
1.3.0,Get bond lists with reverse edges included
1.3.0,Get canonical adjacency list
1.3.0,"Distance is either graph distance(True) or Euclidean distance(False,"
1.3.0,only support datasets providing Cartesian coordinates)
1.3.0,Set dtype
1.3.0,If includes explicit hydrogens
1.3.0,Atom features
1.3.0,Stack nodes into an array
1.3.0,Get bond lists
1.3.0,Get canonical adjacency list
1.3.0,Calculate pair features
1.3.0,atom_name is of format RESX-ATOMTYPE
1.3.0,where X is a 1 to 4 digit number
1.3.0,list-of-available-descriptors.
1.3.0,(ytz): This is done to avoid future compatibility issues like inclusion of
1.3.0,the 3D descriptors or changing the feature size.
1.3.0,check for separate count and SMILES entries for each fragment
1.3.0,TODO test more formats for ligand
1.3.0,some users might try to read smiles with this function
1.3.0,adding hydrogens and charges is tested in dc.utils
1.3.0,3D vector with unit length
1.3.0,"very basic test, we check if rotations actually work in test_rotate_molecules"
1.3.0,check if distances do not change
1.3.0,check if it works for molecules with different numbers of atoms
1.3.0,"random coords between 0 and 1, so the max possible distance in sqrt(2)"
1.3.0,TODO test if dict contains smiles
1.3.0,check if results are the same if we provide precomputed distances
1.3.0,...but first check if we actually got two dicts
1.3.0,check if we get less features with smaller distance cutoff
1.3.0,ligands are typically small so all atoms might be present
1.3.0,check if using different ecfp_degree changes anything
1.3.0,TODO upperbound?
1.3.0,"20 points with coords between -5 and 5, centered at 0"
1.3.0,indices are positive
1.3.0,coordinates were properly translated and scaled
1.3.0,for coordinates outside of the box function should properly transform them
1.3.0,to indices and warn the user
1.3.0,"TODO check if function warns. There is assertWarns method in unittest,"
1.3.0,but it is not implemented in 2.7 and buggy in 3.5 (issue 29620)
1.3.0,"20 points with coords between -5 and 5, centered at 0"
1.3.0,3 pairs of indices
1.3.0,"protein is too big for the box, some features should be missing"
1.3.0,whole ligand should fit in the box
1.3.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
1.3.0,of degree 1 (connected only to central nitrogen).
1.3.0,5 atoms in compound
1.3.0,Get the adjacency lists grouped by degree
1.3.0,The 4 outer atoms connected to central nitrogen
1.3.0,Central nitrogen connected to everything else.
1.3.0,Only one carbon
1.3.0,"No bonds, so degree adjacency lists are empty"
1.3.0,3 carbonds in alkane
1.3.0,Outer two carbonds are connected to central carbon
1.3.0,Central carbon connected to outer two
1.3.0,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
1.3.0,this expected behavior? Need to think about API.
1.3.0,Do a manual distance computation and make
1.3.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
1.3.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
1.3.0,Do a manual distance computation and ensure that selected neighbor is
1.3.0,closest since we set max_num_neighbors = 1
1.3.0,Splits featurized samples into train/test
1.3.0,Artificial feature array.
1.3.0,0 atoms of degree 0
1.3.0,0 atoms of degree 1
1.3.0,4 atoms of degree 2
1.3.0,0 atoms of degree 3
1.3.0,0 atoms of degree 4
1.3.0,0 atoms of degree 5
1.3.0,0 atoms of degree 6
1.3.0,0 atoms of degree 7
1.3.0,0 atoms of degree 8
1.3.0,0 atoms of degree 9
1.3.0,0 atoms of degree 10
1.3.0,atom 4 has 0 neighbors
1.3.0,atom 0 has 2 neighbors
1.3.0,atom 1 has 2 neighbors
1.3.0,atom 2 has 2 neighbors
1.3.0,atom 3 has 3 neighbors.
1.3.0,Verify that atom features have been sorted by atom degree.
1.3.0,Sorting is done by atom degree as before. So the ordering goes
1.3.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
1.3.0,from new position to old position is
1.3.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
1.3.0,list respects this reordering and returns correct adjacency list.
1.3.0,### First example molecule
1.3.0,Artificial feature array.
1.3.0,### Second example molecule
1.3.0,## Third example molecule
1.3.0,Test agglomerate molecule method
1.3.0,No atoms of degree 0
1.3.0,3 atoms of degree 1
1.3.0,8 atoms of degree 2
1.3.0,1 atom of degree 3
1.3.0,0 atoms of degree 4
1.3.0,0 atoms of degree 5
1.3.0,Check that atoms are only connected to themselves.
1.3.0,Check that there's one atom of each degree.
1.3.0,assumes that every array is of the same dimension
1.3.0,rem_dataset is remaining portion of dataset
1.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.0,to k-1.
1.3.0,dict is needed in case groups aren't strictly flattened or
1.3.0,hashed by something non-integer like
1.3.0,returns list of per column sum of non zero elements
1.3.0,Compute number of actives needed per task.
1.3.0,loop through each column and obtain index required to splice out for
1.3.0,required fraction of hits
1.3.0,Find the first index where the cumulative number of actives equals
1.3.0,the actives_count
1.3.0,Note that np.where tells us last index required to exceed
1.3.0,"actives_count, so we actually want the following location"
1.3.0,TODO(rbharath): Refactor this split method to match API of other splits (or
1.3.0,potentially refactor those to match this.
1.3.0,Handle edge case where frac_split is 1
1.3.0,Create weight matrices fpor two haves.
1.3.0,copy over up to required index for weight first_split
1.3.0,check out if any rows in either w_1 or w_2 are just zeros
1.3.0,"Obtain original x, y, and w arrays and shuffle"
1.3.0,calculate percent split for valid (out of test and valid)
1.3.0,"split test data into valid and test, treating sub test set also as sparse"
1.3.0,rem_dataset is remaining portion of dataset
1.3.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.3.0,to k-1.
1.3.0,JSG Assert that split fractions can be written as proper fractions over 10.
1.3.0,This can be generalized in the future with some common demoninator determination.
1.3.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.3.0,Append remaining examples to train
1.3.0,Sort by increasing MW
1.3.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.3.0,for m_idx in cluster:
1.3.0,"continue until we find an active in all the tasks, otherwise we can't"
1.3.0,compute a meaningful AUC
1.3.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.3.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.3.0,Sort from largest to smallest scaffold sets
1.3.0,Pick the mol closest to everything as the first element of training
1.3.0,Pick the closest mol from what is left
1.3.0,Test is everything else
1.3.0,All datasets share features and identifiers by assumption.
1.3.0,TODO(rbharath): Get rid of * import
1.3.0,Note that the extra task goes to test
1.3.0,Number tasks per fold
1.3.0,Find the tasks that correspond to this test fold
1.3.0,Assert that all arrays look like they should
1.3.0,0 1 2 3 4 5 6 7 8 9
1.3.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
1.3.0,data. Make a test for properly splitting of sharded data. Perhaps using
1.3.0,reshard() to handle this?
1.3.0,Verify lengths is 10/k == 2
1.3.0,Verify that compounds in this fold are subset of original compounds
1.3.0,Verify that no two folds have overlapping compounds.
1.3.0,Verify lengths is 10/k == 2
1.3.0,Verify that compounds in this fold are subset of original compounds
1.3.0,Verify that no two folds have overlapping compounds.
1.3.0,Verify lengths is 10/k == 2
1.3.0,Verify that compounds in this fold are subset of original compounds
1.3.0,Verify that no two folds have overlapping compounds.
1.3.0,Test singletask case.
1.3.0,The split index should partition dataset in half.
1.3.0,Test singletask case.
1.3.0,Test case where some weights are zero (i.e. masked)
1.3.0,Set half the positives to have zero weight
1.3.0,There are 10 nonzero actives.
1.3.0,"The split index should partition this into half, so expect 5"
1.3.0,The split index should partition dataset in half.
1.3.0,Mask half the examples
1.3.0,The split index should partition dataset in half.
1.3.0,Test singletask case.
1.3.0,Should have split cleanly in half (picked random seed to ensure this)
1.3.0,Check positives are correctly distributed
1.3.0,Verify lengths is 100/k == 20
1.3.0,Note: This wouldn't work for multitask str
1.3.0,assert len(fold_dataset) == n_samples/K
1.3.0,Verify that each fold has n_positives/K = 4 positive examples.
1.3.0,Verify that compounds in this fold are subset of original compounds
1.3.0,Verify that no two folds have overlapping compounds.
1.3.0,sparsity is determined by number of w weights that are 0 for a given
1.3.0,task structure of w np array is such that each row corresponds to a
1.3.0,sample. The loaded sparse dataset has many rows with only zeros
1.3.0,verify that there are no rows (samples) in weights matrix w
1.3.0,that have no hits.
1.3.0,Path to save checkpoint files
1.3.0,first layer in model: check that it is an input layer
1.3.0,Add losses to graph
1.3.0,Loss for each batch element
1.3.0,Loss should be a float
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Save an initial checkpoint.
1.3.0,TODO(rbharath): Don't support example weighting yet.
1.3.0,Always save a final checkpoint when complete.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Arguments
1.3.0,Returns
1.3.0,Arguments
1.3.0,Returns
1.3.0,Arguments
1.3.0,Returns
1.3.0,Arguments
1.3.0,Returns
1.3.0,task_metadata_rows = {task: [] for task in tasks}
1.3.0,Extract those datapoints which are present for this task
1.3.0,Loading is done on-the-fly
1.3.0,TODO(rbharath/enf): We need a structured way to deal with potential GPU
1.3.0,memory overflows.
1.3.0,Discard any padded predictions
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,Special case to handle singletasks.
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,Calculate pairwise distance
1.3.0,Masking for valid atom index
1.3.0,Cutoff with threshold Rc
1.3.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
1.3.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
1.3.0,optimization to allow for tensorcontraction/broadcasted mmul
1.3.0,using a reshape trick. Note that the np and tf matmul behavior
1.3.0,differs when dealing with broadcasts
1.3.0,-*- coding: UTF-8 -*-
1.3.0,Reshape everything to match the input with the most dimensions.
1.3.0,"This probably means the variable hasn't been created yet, so try again"
1.3.0,with reuse set to false.
1.3.0,"H(x), with same number of input and output channels"
1.3.0,"T(x), with same number of input and output channels"
1.3.0,Calculate what the new shape will be.
1.3.0,"Shape (N_atoms, M_nbrs, ndim)"
1.3.0,"Shape (N_atoms, M_nbrs, ndim)"
1.3.0,"Shape (N_atoms, M_nbrs)"
1.3.0,"This probably means the variable hasn't been created yet, so try again"
1.3.0,with reuse set to false.
1.3.0,"This probably means the variable hasn't been created yet, so try again"
1.3.0,with reuse set to false.
1.3.0,"This probably means the variable hasn't been created yet, so try again"
1.3.0,with reuse set to false.
1.3.0,"This probably means the variable hasn't been created yet, so try again"
1.3.0,with reuse set to false.
1.3.0,TODO(rbharath): Note sure if this layer can be called with __call__
1.3.0,"meaningfully, so not going to support that functionality for now."
1.3.0,"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.3.0,Generate the nb_affine weights and biases
1.3.0,Extract atom_features
1.3.0,Extract graph topology
1.3.0,Perform the mol conv
1.3.0,"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
1.3.0,"self.max_deg, self.min_deg, self.W_list,"
1.3.0,self.b_list)
1.3.0,Sum all neighbors using adjacency matrix
1.3.0,Get collection of modified atom features
1.3.0,Obtain relevant atoms for this degree
1.3.0,Get self atoms
1.3.0,Apply hidden affine to relevant atoms and append
1.3.0,Determine the min_deg=0 case
1.3.0,Only use the self layer
1.3.0,Combine all atoms back into the list
1.3.0,Tensorflow correctly processes empty lists when using concat
1.3.0,"Sum along neighbors as well as self, and store"
1.3.0,Perform the mol gather
1.3.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
1.3.0,"self.max_degree, self.min_degree)"
1.3.0,Tensorflow correctly processes empty lists when using concat
1.3.0,Get self atoms
1.3.0,Expand dims
1.3.0,always deg-1 for deg_adj_lists
1.3.0,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.3.0,Extract graph topology
1.3.0,Perform the mol gather
1.3.0,Obtain the partitions for each of the molecules
1.3.0,Sum over atoms for each molecule
1.3.0,Get the final sparse representations
1.3.0,No other forget biases supported right now.
1.3.0,Taken from Keras code [citation needed]
1.3.0,"x is test set, xp is support set."
1.3.0,# Initializes trainable weights.
1.3.0,## Performs computations
1.3.0,Get initializations
1.3.0,Process using attention
1.3.0,"Eqn (4), appendix A.1 of Matching Networks paper"
1.3.0,Generate new attention states
1.3.0,Support set lstm
1.3.0,Test lstm
1.3.0,self.build()
1.3.0,Get initializations
1.3.0,Rename support
1.3.0,Process support xp using attention
1.3.0,Get linear combination of support set
1.3.0,Process test x using attention
1.3.0,Generate new support attention states
1.3.0,Generate new test attention states
1.3.0,Redefine
1.3.0,Number of rotatable bonds
1.3.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
1.3.0,a difference.
1.3.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
1.3.0,neighbors lists an argument instead of a part of this layer.
1.3.0,"Shape (N, M)"
1.3.0,"Shape (N, M)"
1.3.0,"Shape (N, M)"
1.3.0,Number of grid cells
1.3.0,TODO(rbharath): Support batching
1.3.0,"Shape (n_cells, ndim)"
1.3.0,"List of length N_atoms, each element of different length uniques_i"
1.3.0,"List of length N_atoms, each element of different length uniques_i"
1.3.0,"List of length N_atoms, each a tensor of shape"
1.3.0,"(uniques_i, ndim)"
1.3.0,Add phantom atoms that exist far outside the box
1.3.0,"List of length N_atoms, each of shape (1, ndim)"
1.3.0,TODO(rbharath): How does distance need to be modified here to
1.3.0,account for periodic boundary conditions?
1.3.0,List of length N_atoms each of shape (M_nbrs)
1.3.0,"N_atoms elts of size (M_nbrs,) each"
1.3.0,"Shape (N_atoms, 1)"
1.3.0,Find M_nbrs atoms closest to each cell
1.3.0,"Shape (n_cells, M_nbrs)"
1.3.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.3.0,"conditions, so does wrapround. O(constant)"
1.3.0,"Shape (n_cells, n_nbr_cells)"
1.3.0,"Shape (N_atoms, n_nbr_cells)"
1.3.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
1.3.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
1.3.0,"List of length N_atoms, each element length uniques_i"
1.3.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
1.3.0,element removed to remove self from list of neighbors. Need to verify
1.3.0,this holds more broadly or come up with robust alternative.
1.3.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.3.0,"Shape (N_atoms*n_cells, ndim) after tile"
1.3.0,Shape (N_atoms*n_cells)
1.3.0,"Shape (n_cells, N_atoms)"
1.3.0,Find k atoms closest to this cell. Notice negative sign since
1.3.0,tf.nn.top_k returns *largest* not smallest.
1.3.0,"Tensor of shape (n_cells, M_nbrs)"
1.3.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.3.0,"Shape (N_atoms*n_cells, 1) after tile"
1.3.0,9 neighbors in 2-space
1.3.0,TODO(rbharath): Shoddy handling of higher dimensions...
1.3.0,Number of cells for cube in 3-space is
1.3.0,TODO(rbharath): Do we need to handle periodic boundary conditions
1.3.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.3.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
1.3.0,the cube.
1.3.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.3.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.3.0,"Tile (a, a, a, b, b, b, etc.)"
1.3.0,"Tile (a, b, c, a, b, c, ...)"
1.3.0,N: Maximum number of atoms
1.3.0,M: Maximum number of neighbors
1.3.0,d: Number of coordinates/features/filters
1.3.0,B: Batch Size
1.3.0,We apply the radial pooling filter before atom type conv
1.3.0,to reduce computation
1.3.0,check that there isnt just one or zero inputs
1.3.0,create subspaces
1.3.0,create the alpha learnable parameters
1.3.0,"concatenate subspaces, reshape to size of original input, then stack"
1.3.0,"such that out_tensor has shape (2,?,original_cols)"
1.3.0,creates subspaces the same way it was done in AlphaShare
1.3.0,calculate squared Frobenius norm
1.3.0,"(TODO YTZ:) faster, less memory intensive way"
1.3.0,"r = tf.reduce_sum(tf.square(coordinates), 2)"
1.3.0,"r = tf.expand_dims(r, -1)"
1.3.0,"inner = 2*tf.matmul(coordinates, tf.transpose(coordinates, perm=[0,2,1]))"
1.3.0,"# inner = 2*tf.matmul(coordinates, coordinates, transpose_b=True)"
1.3.0,"d = r - inner + tf.transpose(r, perm=[0,2,1])"
1.3.0,d = tf.nn.relu(d) # fix numerical instabilities about diagonal
1.3.0,d = tf.sqrt(d) # does this have negative elements? may be unstable for diagonals
1.3.0,Calculate pairwise distance
1.3.0,Cutoff with threshold Rc
1.3.0,return d
1.3.0,tf.stack issues again...
1.3.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
1.3.0,We do not need the mask because every graph has self.num_vertices vertices now
1.3.0,So the Tensor has known dimensions
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.3.0,and embeddings of atom j(both gone through a hidden layer)
1.3.0,"for atom i, sum the influence from all other atom j in the molecule"
1.3.0,number of inputs each step
1.3.0,Add trainable weights
1.3.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.3.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.3.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.3.0,initialize graph features for each graph
1.3.0,initialize graph features for each graph
1.3.0,another row of zeros is generated for padded dummy atoms
1.3.0,`count`-th step
1.3.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.3.0,generating index for graph features used in the inputs
1.3.0,"extracting graph features for parents of the target atoms, then flatten"
1.3.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.3.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.3.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.3.0,of shape: (batch_size*max_atoms) * n_graph_features
1.3.0,representing the graph features of target atoms in each graph
1.3.0,index for targe atoms
1.3.0,update the graph features for target atoms
1.3.0,Add trainable weights
1.3.0,Extract atom_features
1.3.0,Extract atom_features
1.3.0,sum all graph outputs
1.3.0,"Default message function: edge network, update function: GRU"
1.3.0,more options to be implemented
1.3.0,Extract atom_features
1.3.0,Add trainable weights
1.3.0,Extract atom_features
1.3.0,Add another value(~-Inf) to prevent error in softmax
1.3.0,Model using this layer must set pad_batches=True
1.3.0,Perform one step of LSTM
1.3.0,Layer Management
1.3.0,Singular place to hold Tensor objects which don't serialize
1.3.0,These have to be reconstructed on restoring from pickle
1.3.0,See TensorGraph._get_tf() for more details on lazy construction
1.3.0,"Don't let this thread get ahead of the enqueue thread, since if"
1.3.0,"we try to read more batches than the total number that get queued,"
1.3.0,this thread will hang indefinitely.
1.3.0,Gather results for each output
1.3.0,"If only one output, just return array"
1.3.0,Ensure all training operators have been created.
1.3.0,Initialize variables.
1.3.0,"As a sanity check, make sure all tensors have the correct shape."
1.3.0,Remove out_tensor from the object to be pickled
1.3.0,Pickle itself
1.3.0,add out_tensor back to everyone
1.3.0,The loss doesn't depend on any variables.
1.3.0,Set by variable constructor.
1.3.0,Set by set_variable_initial_values().
1.3.0,Optimize submodel 1.  This should send var1 to 0 while leaving var2 unchanged.
1.3.0,Optimize the main loss.  This should send both variables toward 1.
1.3.0,Optimize submodel 2.  This should send var2 to 0 while leaving var1 unchanged.
1.3.0,See if it has done a plausible job of learning the distribution.
1.3.0,We have to set the gradient penalty very small because the generator's
1.3.0,"output is only a single number, so the default penalty would constrain"
1.3.0,it far too much.
1.3.0,See if it has done a plausible job of learning the distribution.
1.3.0,"This isn't a meaningful loss, but just for test"
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.3.0,"Now an (N, M) shape"
1.3.0,TODO(rbharath): Move this into a model directly
1.3.0,def test_vina(self):
1.3.0,"""""""Test that vina graph can be constructed in TensorGraph."""""""
1.3.0,N_protein = 4
1.3.0,N_ligand = 1
1.3.0,N_atoms = 5
1.3.0,M_nbrs = 2
1.3.0,ndim = 3
1.3.0,start = 0
1.3.0,stop = 4
1.3.0,nbr_cutoff = 1
1.3.0,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
1.3.0,start))
1.3.0,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
1.3.0,start))
1.3.0,y = NumpyDataset(np.random.rand(
1.3.0,"1,))"
1.3.0,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
1.3.0,"# used in the current implementation. This is obviously wrong, but need"
1.3.0,# to dig out why this is happening.
1.3.0,"prot_coords = Feature(shape=(N_protein, ndim))"
1.3.0,"ligand_coords = Feature(shape=(N_ligand, ndim))"
1.3.0,"labels = Label(shape=(1,))"
1.3.0,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
1.3.0,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
1.3.0,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
1.3.0,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
1.3.0,"# Now an (N, M) shape"
1.3.0,nbr_list = NeighborList(
1.3.0,"N_protein + N_ligand,"
1.3.0,"M_nbrs,"
1.3.0,"ndim,"
1.3.0,"nbr_cutoff,"
1.3.0,"start,"
1.3.0,"stop,"
1.3.0,in_layers=[coords])
1.3.0,"# Shape (N, M)"
1.3.0,dists = InteratomicL2Distances(
1.3.0,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
1.3.0,repulsion = VinaRepulsion(in_layers=[dists])
1.3.0,hydrophobic = VinaHydrophobic(in_layers=[dists])
1.3.0,hbond = VinaHydrogenBond(in_layers=[dists])
1.3.0,gauss_1 = VinaGaussianFirst(in_layers=[dists])
1.3.0,gauss_2 = VinaGaussianSecond(in_layers=[dists])
1.3.0,"# Shape (N, M)"
1.3.0,interactions = WeightedLinearCombo(
1.3.0,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
1.3.0,"# Shape (N, M)"
1.3.0,"thresholded = Cutoff(in_layers=[dists, interactions])"
1.3.0,"# Shape (N, M)"
1.3.0,free_energies = VinaNonlinearity(in_layers=[thresholded])
1.3.0,free_energy = ReduceSum(in_layers=[free_energies])
1.3.0,"loss = L2Loss(in_layers=[free_energy, labels])"
1.3.0,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
1.3.0,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
1.3.0,tg.set_loss(loss)
1.3.0,tg.fit_generator(databag.iterbatches(epochs=1))
1.3.0,TODO(rbharath): This test should pass. Fix it!
1.3.0,def test_graph_pool(self):
1.3.0,"""""""Test that GraphPool can be invoked."""""""
1.3.0,out_channels = 2
1.3.0,"n_atoms = 4 # In CCC and C, there are 4 atoms"
1.3.0,"raw_smiles = ['CCC', 'C']"
1.3.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
1.3.0,featurizer = ConvMolFeaturizer()
1.3.0,mols = featurizer.featurize(mols)
1.3.0,multi_mol = ConvMol.agglomerate_mols(mols)
1.3.0,atom_features = multi_mol.get_atom_features()
1.3.0,degree_slice = multi_mol.deg_slice
1.3.0,membership = multi_mol.membership
1.3.0,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
1.3.0,with self.test_session() as sess:
1.3.0,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
1.3.0,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
1.3.0,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
1.3.0,deg_adjs_tf = []
1.3.0,for deg_adj in deg_adjs:
1.3.0,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
1.3.0,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
1.3.0,out_tensor = GraphPool(out_channels)(*args)
1.3.0,sess.run(tf.global_variables_initializer())
1.3.0,out_tensor = out_tensor.eval()
1.3.0,"assert out_tensor.shape == (n_atoms, out_channels)"
1.3.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
1.3.0,Train the model on random sequences.  We aren't training long enough to
1.3.0,"really make it reliable, but I want to keep this test fast, and it should"
1.3.0,still be able to reproduce a reasonable fraction of input sequences.
1.3.0,Test it out.
1.3.0,Check that it got at least a quarter of them correct.
1.3.0,Actually training a VAE takes far too long for a unit test.  Just run a
1.3.0,"few steps of training to make sure nothing crashes, then check that the"
1.3.0,results are at least internally consistent.
1.3.0,use central difference since forward difference has a pretty high
1.3.0,approximation error
1.3.0,assert min_coords[1][0] != new_x[3]
1.3.0,assert min_coords[1][1] != new_x[4]
1.3.0,assert min_coords[1][2] != new_x[5]
1.3.0,Create the inputs.
1.3.0,Create the generator.
1.3.0,Create the discriminator.
1.3.0,Make a copy of the discriminator that takes the generator's output as
1.3.0,its input.
1.3.0,Make a list of all layers in the generator and discriminator.
1.3.0,Create submodels for training the generator and discriminator.
1.3.0,"Every call to fit_generator() will increment global_step, but we only"
1.3.0,"want it to get incremented once for the entire batch, so record the"
1.3.0,value and keep resetting it.
1.3.0,Train the discriminator.
1.3.0,Train the generator.
1.3.0,Write checkpoints and report progress.
1.3.0,Write out final results.
1.3.0,number of atoms in each molecule
1.3.0,index of pair features
1.3.0,number of pairs for each atom
1.3.0,atom features
1.3.0,pair features
1.3.0,calculation orders for a batch of molecules
1.3.0,padding atom features vector of each molecule with 0
1.3.0,Gather results for each output
1.3.0,Recording the number of samples in the input batch
1.3.0,GraphConvTensorGraph constantly outputs batch_size number of
1.3.0,"results, only valid samples should be appended to final results"
1.3.0,"If only one output, just return array"
1.3.0,Returns:
1.3.0,Concatenates along 0-th dimension
1.3.0,Returns:
1.3.0,Build placeholders
1.3.0,w_b act as the indicator of unique samples in the batch
1.3.0,number of atoms in each molecule
1.3.0,index of pair features
1.3.0,number of pairs for each atom
1.3.0,atom features
1.3.0,pair features
1.3.0,MPNN only accept padded input
1.3.0,MPNN only accept padded input
1.3.0,Extract number of unique samples in the batch from w_b
1.3.0,Only fetch the first set of unique samples
1.3.0,import tensorflow as tf
1.3.0,from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
1.3.0,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
1.3.0,
1.3.0,
1.3.0,class WeightedError(Layer):
1.3.0,
1.3.0,"def __call__(self, *parents):"
1.3.0,"entropy, weights = parents[0], parents[1]"
1.3.0,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
1.3.0,return self.out_tensor
1.3.0,
1.3.0,
1.3.0,"def tensorGraphMultitaskClassifier(n_tasks,"
1.3.0,"n_features,"
1.3.0,"layer_sizes=[500],"
1.3.0,"bypass_layer_sizes=[100],"
1.3.0,model_dir=None):
1.3.0,""""""""
1.3.0,TODO(LESWING) Add Dropout and regularization
1.3.0,
1.3.0,Parameters
1.3.0,----------
1.3.0,n_tasks
1.3.0,n_features
1.3.0,layer_sizes
1.3.0,bypass_layer_sizes
1.3.0,model_dir
1.3.0,
1.3.0,Returns
1.3.0,-------
1.3.0,
1.3.0,""""""""
1.3.0,g = MultiTaskTensorGraph(model_dir=model_dir)
1.3.0,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
1.3.0,g.add_layer(in_layer)
1.3.0,g.add_feature(in_layer)
1.3.0,
1.3.0,# Shared Dense Layers
1.3.0,prev_layer = in_layer
1.3.0,dense_layers = []
1.3.0,for i in range(len(layer_sizes)):
1.3.0,dense = Dense(
1.3.0,"out_channels=layer_sizes[i],"
1.3.0,"name=""SDENSE%s"" % i,"
1.3.0,activation_fn=tf.nn.relu)
1.3.0,"g.add_layer(dense, parents=[prev_layer])"
1.3.0,dense_layers.append(dense)
1.3.0,prev_layer = dense
1.3.0,
1.3.0,# Individual Bypass Layers
1.3.0,costs = []
1.3.0,for task in range(n_tasks):
1.3.0,prev_layer = in_layer
1.3.0,for i in range(len(bypass_layer_sizes)):
1.3.0,dense = Dense(
1.3.0,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
1.3.0,"g.add_layer(dense, parents=[prev_layer])"
1.3.0,prev_layer = dense
1.3.0,"joined_layer = Concat(name=""JOIN%s"" % task)"
1.3.0,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
1.3.0,
1.3.0,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
1.3.0,"g.add_layer(classification, parents=[joined_layer])"
1.3.0,
1.3.0,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
1.3.0,"g.add_layer(softmax, parents=[classification])"
1.3.0,g.add_output(softmax)
1.3.0,
1.3.0,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
1.3.0,g.add_layer(label)
1.3.0,g.add_label(label)
1.3.0,
1.3.0,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
1.3.0,"g.add_layer(cost, parents=[label, classification])"
1.3.0,costs.append(cost)
1.3.0,
1.3.0,"entropy = Concat(name=""ENT"")"
1.3.0,"g.add_layer(entropy, parents=costs)"
1.3.0,
1.3.0,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
1.3.0,g.add_layer(task_weights)
1.3.0,g.set_task_weights(task_weights)
1.3.0,
1.3.0,"loss = WeightedError(name=""ERROR"")"
1.3.0,"g.add_layer(loss, parents=[entropy, task_weights])"
1.3.0,g.set_loss(loss)
1.3.0,
1.3.0,return g
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,(ytz): this is really dirty but needed for restoring models
1.3.0,"Common symbols in SMILES, note that Cl and Br are regarded as single symbol"
1.3.0,SMILES strings
1.3.0,Maximum length is expanded to allow length variation during train and inference
1.3.0,'_' served as delimiter and padding
1.3.0,Initialize common characters as keys
1.3.0,Include space to avoid extra keys
1.3.0,"For 'Cl', 'Br', etc."
1.3.0,"Character not recognized, add to extra_keys"
1.3.0,Add all extra_keys to char_dict
1.3.0,Character embedding
1.3.0,Multiple convolutional layers with different filter widths
1.3.0,Max-over-time pooling
1.3.0,Concat features from all filters(one feature per filter)
1.3.0,Highway layer from https://arxiv.org/pdf/1505.00387.pdf
1.3.0,Transform SMILES string to integer vectors
1.3.0,Skip all spaces
1.3.0,"For 'Cl', 'Br', etc."
1.3.0,Padding with '_'
1.3.0,Do a simple greedy search.
1.3.0,Do a beam search with length normalization.
1.3.0,"Represent each candidate as (normalized prob, raw prob, sequence)"
1.3.0,This candidate sequence has already been terminated
1.3.0,Consider all possible tokens we could add to this candidate sequence.
1.3.0,update model with best param
1.3.0,Find optimal n_estimators based on original learning_rate
1.3.0,and early_stopping_rounds
1.3.0,"Since test size is 20%, when retrain model to whole data, expect"
1.3.0,n_estimator increased to 1/0.8 = 1.25 time.
1.3.0,Make sure user specified params are in the grid.
1.3.0,Change params back original params
1.3.0,TODO (LESWING) Lazy Load
1.3.0,TODO (LESWING) Lazy Load
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Check same predictions are made.
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Load trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Load trained model
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on train/test
1.3.0,Fit trained model
1.3.0,Eval model on train/test
1.3.0,Fit trained model
1.3.0,Eval model on train/test
1.3.0,Test Parameter getting and setting
1.3.0,Fit trained model
1.3.0,Eval model on train/test
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,n_samples = 100
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,n_samples = 100
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Gather Projection
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Gather Projection
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,Load mini log-solubility dataset.
1.3.0,Add layers
1.3.0,"output will be (n_atoms, 64)"
1.3.0,Need to add batch-norm separately to test/support due to differing
1.3.0,shapes.
1.3.0,"output will be (n_atoms, 64)"
1.3.0,"output will be (n_atoms, 64)"
1.3.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.3.0,n_pos/n_neg accordingly.
1.3.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.3.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.3.0,can measure model has memorized support).  Replacement is turned off to
1.3.0,ensure that support contains full training set. This checks that the
1.3.0,model has mastered memorization of provided support.
1.3.0,#################################################### DEBUG
1.3.0,TODO(rbharath): Check if something went wrong here...
1.3.0,Measure performance on 0-th task.
1.3.0,assert scores[0] > .9
1.3.0,#################################################### DEBUG
1.3.0,Load mini log-solubility dataset.
1.3.0,Add layers
1.3.0,"output will be (n_atoms, 64)"
1.3.0,Need to add batch-norm separately to test/support due to differing
1.3.0,shapes.
1.3.0,"output will be (n_atoms, 64)"
1.3.0,"output will be (n_atoms, 64)"
1.3.0,Apply an attention lstm layer
1.3.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.3.0,n_pos/n_neg accordingly.
1.3.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.3.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.3.0,can measure model has memorized support).  Replacement is turned off to
1.3.0,ensure that support contains full training set. This checks that the
1.3.0,model has mastered memorization of provided support.
1.3.0,Measure performance on 0-th task.
1.3.0,#################################################### DEBUG
1.3.0,TODO(rbharath): Check if something went wrong here...
1.3.0,Measure performance on 0-th task.
1.3.0,assert scores[0] > .85
1.3.0,#################################################### DEBUG
1.3.0,Load mini log-solubility dataset.
1.3.0,Add layers
1.3.0,"output will be (n_atoms, 64)"
1.3.0,Need to add batch-norm separately to test/support due to differing
1.3.0,shapes.
1.3.0,"output will be (n_atoms, 64)"
1.3.0,"output will be (n_atoms, 64)"
1.3.0,Apply a residual lstm layer
1.3.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.3.0,n_pos/n_neg accordingly.
1.3.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.3.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.3.0,can measure model has memorized support).  Replacement is turned off to
1.3.0,ensure that support contains full training set. This checks that the
1.3.0,model has mastered memorization of provided support.
1.3.0,Measure performance on 0-th task.
1.3.0,#################################################### DEBUG
1.3.0,TODO(rbharath): Check if something went wrong here...
1.3.0,Measure performance on 0-th task.
1.3.0,assert scores[0] > .9
1.3.0,#################################################### DEBUG
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on train
1.3.0,def test_singletask_to_multitask_classification(self):
1.3.0,n_features = 10
1.3.0,n_tasks = 17
1.3.0,tasks = range(n_tasks)
1.3.0,# Define train dataset
1.3.0,n_train = 100
1.3.0,"X_train = np.random.rand(n_train, n_features)"
1.3.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
1.3.0,w_train = np.ones_like(y_train)
1.3.0,"ids_train = [""C""] * n_train"
1.3.0,train_dataset = dc.data.DiskDataset.from_numpy(
1.3.0,"X_train, y_train, w_train, ids_train)"
1.3.0,# Define test dataset
1.3.0,n_test = 10
1.3.0,"X_test = np.random.rand(n_test, n_features)"
1.3.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
1.3.0,w_test = np.ones_like(y_test)
1.3.0,"ids_test = [""C""] * n_test"
1.3.0,test_dataset = dc.data.DiskDataset.from_numpy(
1.3.0,"X_test, y_test, w_test, ids_test)"
1.3.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
1.3.0,def model_builder(model_dir):
1.3.0,sklearn_model = LogisticRegression()
1.3.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.3.0,multitask_model = dc.models.SingletaskToMultitask(
1.3.0,"tasks, model_builder)"
1.3.0,# Fit trained model
1.3.0,multitask_model.fit(train_dataset)
1.3.0,multitask_model.save()
1.3.0,# Eval multitask_model on train/test
1.3.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
1.3.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
1.3.0,Generate data
1.3.0,Cleanup
1.3.0,Generate dummy dataset
1.3.0,Fit trained model
1.3.0,Eval model on test
1.3.0,Eval model on train
1.3.0,Fit trained model
1.3.0,Eval model on test
1.3.0,Fit trained model
1.3.0,Eval model on test
1.3.0,def test_sklearn_classification(self):
1.3.0,"""""""Test that sklearn models can learn on simple classification datasets."""""""
1.3.0,np.random.seed(123)
1.3.0,dataset = sklearn.datasets.load_digits(n_class=2)
1.3.0,"X, y = dataset.data, dataset.target"
1.3.0,frac_train = .7
1.3.0,n_samples = len(X)
1.3.0,n_train = int(frac_train*n_samples)
1.3.0,"X_train, y_train = X[:n_train], y[:n_train]"
1.3.0,"X_test, y_test = X[n_train:], y[n_train:]"
1.3.0,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
1.3.0,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
1.3.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.3.0,sklearn_model = LogisticRegression()
1.3.0,model = dc.models.SklearnModel(sklearn_model)
1.3.0,# Fit trained model
1.3.0,model.fit(train_dataset)
1.3.0,model.save()
1.3.0,# Eval model on test
1.3.0,"scores = model.evaluate(test_dataset, [classification_metric])"
1.3.0,assert scores[classification_metric.name] > .5
1.3.0,def test_sklearn_multitask_classification(self):
1.3.0,"""""""Test that sklearn models can learn on simple multitask classification."""""""
1.3.0,np.random.seed(123)
1.3.0,n_tasks = 4
1.3.0,tasks = range(n_tasks)
1.3.0,dataset = sklearn.datasets.load_digits(n_class=2)
1.3.0,"X, y = dataset.data, dataset.target"
1.3.0,"y = np.reshape(y, (len(y), 1))"
1.3.0,y = np.hstack([y] * n_tasks)
1.3.0,
1.3.0,frac_train = .7
1.3.0,n_samples = len(X)
1.3.0,n_train = int(frac_train*n_samples)
1.3.0,"X_train, y_train = X[:n_train], y[:n_train]"
1.3.0,"X_test, y_test = X[n_train:], y[n_train:]"
1.3.0,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
1.3.0,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
1.3.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.3.0,def model_builder(model_dir):
1.3.0,sklearn_model = LogisticRegression()
1.3.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.3.0,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
1.3.0,# Fit trained model
1.3.0,model.fit(train_dataset)
1.3.0,model.save()
1.3.0,# Eval model on test
1.3.0,"scores = model.evaluate(test_dataset, [classification_metric])"
1.3.0,for score in scores[classification_metric.name]:
1.3.0,assert score > .5
1.3.0,Set early stopping round = n_estimators so that esr won't work
1.3.0,Fit trained model
1.3.0,Eval model on test
1.3.0,Fit trained model
1.3.0,Eval model on test
1.3.0,Fit trained model
1.3.0,Eval model on test
1.3.0,Logistic regression doesn't support weights
1.3.0,Consistency check
1.3.0,Handle output layer
1.3.0,Iterate over all previous tasks.
1.3.0,prev_layers is a list with elements of size
1.3.0,"(batch_size, layer_sizes[i-1])"
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Save an initial checkpoint.
1.3.0,Turns out there are valid cases where we don't want pad-batches
1.3.0,on by default.
1.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.0,Run training op.
1.3.0,Always save a final checkpoint when complete.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Note that we divide by the batch size and not the number of
1.3.0,"non-zero weight examples in the batch.  Also, instead of using"
1.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.0,calculate with div/sum so it stays on the GPU.
1.3.0,aggregated costs
1.3.0,weight decay
1.3.0,Dummy placeholders
1.3.0,Dummy placeholders
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,Handle edge case when batch-size is 1.
1.3.0,Prune away any padding that was added
1.3.0,allow_soft_placement=True allows ops without a GPU implementation
1.3.0,to run on the CPU instead.
1.3.0,!/usr/bin/python
1.3.0,
1.3.0,Copyright 2015 Google Inc.
1.3.0,
1.3.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.3.0,you may not use this file except in compliance with the License.
1.3.0,You may obtain a copy of the License at
1.3.0,
1.3.0,http://www.apache.org/licenses/LICENSE-2.0
1.3.0,
1.3.0,"Unless required by applicable law or agreed to in writing, software"
1.3.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.3.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.3.0,See the License for the specific language governing permissions and
1.3.0,limitations under the License.
1.3.0,get the divisor
1.3.0,compute the requested central moment
1.3.0,"note that mean is a raw moment, not a central moment"
1.3.0,TODO(user): median is not implemented yet in TensorFlow
1.3.0,-*- coding: utf-8 -*-
1.3.0,"due to the different shape of weight(ndims=2) and bias(ndims=1),"
1.3.0,will using this version for logreg
1.3.0,exclude bias variables
1.3.0,setting up n_tasks nodes(output nodes)
1.3.0,label placeholders with size batch_size * 1
1.3.0,Note that we divide by the batch size and not the number of
1.3.0,"non-zero weight examples in the batch.  Also, instead of using"
1.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.0,calculate with div/sum so it stays on the GPU.
1.3.0,aggregated costs
1.3.0,weight decay
1.3.0,using self-defined regularization
1.3.0,adding output nodes of sigmoid function
1.3.0,"fix the size to be [?,1]"
1.3.0,Dummy placeholders
1.3.0,Dummy placeholders
1.3.0,run eval data through the model
1.3.0,transfer 2D prediction tensor to 2D x n_classes(=2)
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,run eval data through the model
1.3.0,transfer 2D prediction tensor to 2D x n_classes(=2)
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,"layer has shape [None, layer_sizes[i]]"
1.3.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.3.0,TODO(rbharath): Might want to make it feasible to have multiple
1.3.0,bypass layers.
1.3.0,Construct task bypass layer
1.3.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.3.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.3.0,"layer has shape [None, layer_sizes[i]]"
1.3.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.3.0,TODO(rbharath): Might want to make it feasible to have multiple
1.3.0,bypass layers.
1.3.0,Construct task bypass layer
1.3.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.3.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.3.0,Add the input features.
1.3.0,Add the dense layers
1.3.0,Compute the loss function for each label.
1.3.0,"Results is of shape (n_samples, n_tasks, n_classes)"
1.3.0,"retval is of shape (n_samples, n_tasks)"
1.3.0,Add the input features.
1.3.0,Add the dense layers
1.3.0,Compute the loss function for each label.
1.3.0,Run fit transformers on dummy dataset to determine n_features after transformation
1.3.0,Dummy placeholders
1.3.0,Dummy placeholders
1.3.0,Dummy placeholders
1.3.0,Dummy placeholders
1.3.0,Run fit transformers on dummy dataset to determine n_features after transformation
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Save an initial checkpoint.
1.3.0,Define the code that runs on a separate thread to feed data into the queue.
1.3.0,Main training loop.
1.3.0,Run training op.
1.3.0,We have reached the end of an epoch.
1.3.0,We have reached the end of the data.
1.3.0,Always save a final checkpoint when complete.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,Handle edge case when batch-size is 1.
1.3.0,Prune away any padding that was added
1.3.0,Handle case of 0-dimensional scalar output
1.3.0,Consistency check
1.3.0,Lazily created by _get_shared_session().
1.3.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.3.0,when subclass-overridden methods use the same scopes.
1.3.0,Setup graph
1.3.0,Create placeholders
1.3.0,Handle output layer
1.3.0,Iterate over all previous tasks.
1.3.0,prev_layers is a list with elements of size
1.3.0,"(batch_size, layer_sizes[i-1])"
1.3.0,Note that we divide by the batch size and not the number of
1.3.0,"non-zero weight examples in the batch.  Also, instead of using"
1.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.0,calculate with div/sum so it stays on the GPU.
1.3.0,aggregated costs
1.3.0,weight decay
1.3.0,Dummy placeholders
1.3.0,Dummy placeholders
1.3.0,run eval data through the model
1.3.0,"Shape (n_tasks, n__samples)"
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,Handle edge case when batch-size is 1.
1.3.0,with self._get_shared_session(train=True) as sess:
1.3.0,Save an initial checkpoint.
1.3.0,Always save a final checkpoint when complete.
1.3.0,Note that we divide by the batch size and not the number of
1.3.0,"non-zero weight examples in the batch.  Also, instead of using"
1.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.0,calculate with div/sum so it stays on the GPU.
1.3.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
1.3.0,Dummy placeholders
1.3.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
1.3.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
1.3.0,Dummy placeholders
1.3.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
1.3.0,allow_soft_placement=True allows ops without a GPU implementation
1.3.0,to run on the CPU instead.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Turns out there are valid cases where we don't want pad-batches
1.3.0,on by default.
1.3.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.3.0,if epoch%checkpoint_interval == checkpoint_interval-1:
1.3.0,"saver.save(sess, self._save_path, global_step=epoch)"
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,"(n_samples, n_classes)"
1.3.0,"(n_samples, n_tasks, n_classes)"
1.3.0,Save hyperparameters
1.3.0,Guard variable to make sure we don't Restore() this model
1.3.0,from a disk checkpoint more than once.
1.3.0,"Path to save checkpoint files, which matches the"
1.3.0,replicated supervisor's default path.
1.3.0,Lazily created by _get_shared_session().
1.3.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.3.0,when subclass-overridden methods use the same scopes.
1.3.0,Setup graph
1.3.0,Note that we divide by the batch size and not the number of
1.3.0,"non-zero weight examples in the batch.  Also, instead of using"
1.3.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.3.0,calculate with div/sum so it stays on the GPU.
1.3.0,aggregated costs
1.3.0,weight decay
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Save an initial checkpoint.
1.3.0,Define the code that runs on a separate thread to feed data into the queue.
1.3.0,Main training loop.
1.3.0,Run training op.
1.3.0,We have reached the end of an epoch.
1.3.0,We have reached the end of the data.
1.3.0,Always save a final checkpoint when complete.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,allow_soft_placement=True allows ops without a GPU implementation
1.3.0,to run on the CPU instead.
1.3.0,gpu memory growth option
1.3.0,gpu memory growth option
1.3.0,TODO(rbharath): Is setting train=False right here?
1.3.0,Discard any padded predictions
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,Special case to handle singletasks.
1.3.0,The iterbatches does padding with zero-weight examples on the last batch.
1.3.0,Remove padded examples.
1.3.0,TODO(rbharath): Verify this can be safely removed.
1.3.0,"def evaluate(self, dataset, metrics, transformers=[]):"
1.3.0,""""""""
1.3.0,Evaluates the performance of this model on specified dataset.
1.3.0,
1.3.0,Parameters
1.3.0,----------
1.3.0,dataset: dc.data.Dataset
1.3.0,Dataset object.
1.3.0,metric: deepchem.metrics.Metric
1.3.0,Evaluation metric
1.3.0,transformers: list
1.3.0,List of deepchem.transformers.Transformer
1.3.0,Returns
1.3.0,-------
1.3.0,dict
1.3.0,Maps tasks to scores under metric.
1.3.0,""""""""
1.3.0,"evaluator = Evaluator(self, dataset, transformers)"
1.3.0,scores = evaluator.compute_model_performance(metrics)
1.3.0,return scores
1.3.0,checkpoints look like model_dir/model.ckpt-N
1.3.0,"self._save_path is ""model_dir/model.ckpt"""
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,Note that softmax is already applied in construct_grpah
1.3.0,run eval data through the model
1.3.0,reshape to batch_size x n_tasks x ...
1.3.0,Handle edge case when batch-size is 1.
1.3.0,Prune away any padding that was added
1.3.0,Handle case of 0-dimensional scalar output
1.3.0,!/usr/bin/python
1.3.0,
1.3.0,Copyright 2015 Google Inc.
1.3.0,
1.3.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.3.0,you may not use this file except in compliance with the License.
1.3.0,You may obtain a copy of the License at
1.3.0,
1.3.0,http://www.apache.org/licenses/LICENSE-2.0
1.3.0,
1.3.0,"Unless required by applicable law or agreed to in writing, software"
1.3.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.3.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.3.0,See the License for the specific language governing permissions and
1.3.0,limitations under the License.
1.3.0,parse CheckpointState proto
1.3.0,parse path to actual checkpoint
1.3.0,the provided mask has to be the same shape as features
1.3.0,test k = 1..4
1.3.0,central moments
1.3.0,standardized moments
1.3.0,central across one axis
1.3.0,standardized across one axis
1.3.0,Fit just on task zero
1.3.0,Notice that we keep the session open
1.3.0,Fit on task one
1.3.0,The predictions for task zero should not change after training
1.3.0,on task one.
1.3.0,Keep track of the layers
1.3.0,"For graphical layers, add connectivity placeholders"
1.3.0,Add layer to the layer list
1.3.0,Keep track of the layers
1.3.0,Create graph topology and x
1.3.0,Keep track of the layers
1.3.0,Whether or not we have used the GraphGather layer yet
1.3.0,Update new value of x
1.3.0,Update new value of x
1.3.0,Update new value of x
1.3.0,Get train function
1.3.0,Initialize
1.3.0,################################################################### DEBUG
1.3.0,self.test_label_placeholder = Input(
1.3.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.3.0,"name=""label_placeholder""))"
1.3.0,self.test_weight_placeholder = Input(
1.3.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.3.0,"name=""weight_placeholder""))"
1.3.0,TODO(rbharath): Should weights for the support be used?
1.3.0,Support labels
1.3.0,self.support_label_placeholder = Input(
1.3.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
1.3.0,"name=""support_label_placeholder""))"
1.3.0,################################################################### DEBUG
1.3.0,Generate dictionary elements for support
1.3.0,Get graph information for test
1.3.0,Generate dictionary elements for test
1.3.0,Perform the optimization
1.3.0,Create different support sets
1.3.0,Get batch to try it out on
1.3.0,"Train on support set, batch pair"
1.3.0,Get featurization for test
1.3.0,"Shape (n_test, n_feat)"
1.3.0,Get featurization for support
1.3.0,"Shape (n_support, n_feat)"
1.3.0,Computes the inner part c() of the kernel
1.3.0,(the inset equation in section 2.1.1 of Matching networks paper).
1.3.0,Normalize
1.3.0,TODO(rbharath): euclidean kernel is broken!
1.3.0,elif self.similarity == 'euclidean':
1.3.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
1.3.0,"Note that gram matrix g has shape (n_test, n_support)"
1.3.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
1.3.0,https://arxiv.org/pdf/1606.04080v1.pdf
1.3.0,"Computes softmax across axis 1, (so sums distances to support set for"
1.3.0,each test entry) to get attention vector
1.3.0,"Shape (n_test, n_support)"
1.3.0,Weighted sum of support labels
1.3.0,"Shape (n_support, 1)"
1.3.0,pred is yhat in eqn (1) of Matching Networks.
1.3.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
1.3.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
1.3.0,"Shape (n_test,)"
1.3.0,Convert to logit space using inverse sigmoid (logit) function
1.3.0,logit function: log(pred) - log(1-pred)
1.3.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
1.3.0,in Cross Entropy calculation.
1.3.0,"Shape (n_test,)"
1.3.0,Get scores
1.3.0,Remove padded elements
1.3.0,Get scores
1.3.0,pred corresponds to prob(example == 1)
1.3.0,Remove padded elements
1.3.0,Get batches
1.3.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
1.3.0,multitask case with missing data...
1.3.0,Join information for all tasks.
1.3.0,TODO(rbharath): Find a way to get rid of this import?
1.3.0,Extract model info
1.3.0,Get graph topology for x
1.3.0,Building outputs
1.3.0,Set epsilon
1.3.0,Initialize
1.3.0,"Path to save checkpoint files, which matches the"
1.3.0,replicated supervisor's default path.
1.3.0,Create target inputs
1.3.0,Get train function
1.3.0,TODO(rbharath): I believe this is total amount of data
1.3.0,Get graph information
1.3.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.3.0,the number of labeled data points in target_i. This is to normalize each task
1.3.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.3.0,Get other optimizer information
1.3.0,TODO(rbharath): Figure out how to handle phase appropriately
1.3.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.3.0,"tensors of shape (batch_size,)"
1.3.0,It's ok to divide by just the batch_size rather than the number of nonzero
1.3.0,examples (effect averages out)
1.3.0,Perform the optimization
1.3.0,TODO(rbharath): Disabling saving for now to try to debug.
1.3.0,run eval data through the model
1.3.0,"Shape (n_samples, n_tasks)"
1.3.0,Create target inputs
1.3.0,TODO(rbharath): Find a way to get rid of this import?
1.3.0,Obtain appropriate loss function
1.3.0,Extract model info
1.3.0,Get graph topology for x
1.3.0,Raw logit outputs
1.3.0,Set epsilon
1.3.0,Initialize
1.3.0,"Path to save checkpoint files, which matches the"
1.3.0,replicated supervisor's default path.
1.3.0,Create target inputs
1.3.0,############################################################### DEBUG
1.3.0,"print(""multitask classifier"")"
1.3.0,"print(""feat"")"
1.3.0,print(feat)
1.3.0,############################################################### DEBUG
1.3.0,Get train function
1.3.0,TODO(rbharath): I believe this is total amount of data
1.3.0,Get graph information
1.3.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.3.0,the number of labeled data points in target_i. This is to normalize each task
1.3.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.3.0,Get other optimizer information
1.3.0,TODO(rbharath): Figure out how to handle phase appropriately
1.3.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.3.0,"tensors of shape (batch_size,)"
1.3.0,Convert the labels into one-hot vector encodings.
1.3.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
1.3.0,un-softmaxed logits rather than softmax outputs.
1.3.0,It's ok to divide by just the batch_size rather than the number of nonzero
1.3.0,examples (effect averages out)
1.3.0,Perform the optimization
1.3.0,TODO(rbharath): Disabling saving for now to try to debug.
1.3.0,run eval data through the model
1.3.0,"Shape (n_samples, n_tasks)"
1.3.0,run eval data through the model
1.3.0,self.n_atoms = n_atoms
1.3.0,Define the list of tensors to be used as topology
1.3.0,Merge mol conv objects
1.3.0,Generate dicts
1.3.0,Define the list of tensors to be used as topology
1.3.0,Extract atom numbers
1.3.0,Generate dicts
1.3.0,molecule * atom(graph) => step => features
1.3.0,molecule * atom(graph) => step
1.3.0,molecule * atom(graph) => step
1.3.0,Define the list of tensors to be used as topology
1.3.0,calculation orders for a batch of molecules
1.3.0,padding atom features vector of each molecule with 0
1.3.0,self.n_atoms = n_atoms
1.3.0,Define the list of tensors to be used as topology
1.3.0,Extract atom numbers
1.3.0,Generate dicts
1.3.0,self.n_atoms = n_atoms
1.3.0,Define the list of tensors to be used as topology
1.3.0,Extract atom numbers
1.3.0,number of atoms in each molecule
1.3.0,index of pair features
1.3.0,number of pairs for each atom
1.3.0,atom features
1.3.0,pair features
1.3.0,Generate dicts
1.3.0,Associate each atom with cell it belongs to. O(N*n_cells)
1.3.0,"Shape (n_cells, k)"
1.3.0,"Shape (N, 1)"
1.3.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.3.0,"conditions, so does wrapround. O(constant)"
1.3.0,"Shape (n_cells, 26)"
1.3.0,"Shape (N, 26)"
1.3.0,"coords of shape (N, ndim)"
1.3.0,"Shape (N, 26, k, ndim)"
1.3.0,"Shape (N, 26, k)"
1.3.0,"Shape (N, 26, k)"
1.3.0,"Shape (N, 26, k, ndim)"
1.3.0,"For smaller systems especially, the periodic boundary conditions can"
1.3.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
1.3.0,make sure duplicate neighbors are ignored?
1.3.0,TODO(rbharath): How does distance need to be modified here to
1.3.0,account for periodic boundary conditions?
1.3.0,"Shape (N, 26, k)"
1.3.0,"Shape (N, 26*k)"
1.3.0,TODO(rbharath): This will cause an issue with duplicates!
1.3.0,"Shape (N, M)"
1.3.0,"N elts of size (M,) each"
1.3.0,"Shape (N, 26*k)"
1.3.0,"N elts of size (26*k,) each"
1.3.0,"N elts of size (M,) each"
1.3.0,"Shape (N, M)"
1.3.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.3.0,"N tensors of shape (n_cells, 1)"
1.3.0,"Shape (N*n_cells, 1) after tile"
1.3.0,"List of N tensors of shape (n_cells, 1)"
1.3.0,Lists of length N
1.3.0,Lists of length n_cells
1.3.0,Get indices of k atoms closest to each cell point
1.3.0,TODO(rbharath): tf.stack for tf 1.0
1.3.0,"Tensor of shape (n_cells, k, ndim)"
1.3.0,atoms_in_cells = tf.stack(atoms_in_cells)
1.3.0,"Tensor of shape (26, k, ndim)"
1.3.0,"Reshape to (26*k, ndim)"
1.3.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
1.3.0,"Dists of shape (26*k, 1)"
1.3.0,"Of shape (k, ndim)"
1.3.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.3.0,TODO(rbharath): Change this for tf 1.0
1.3.0,"n_cells tensors of shape (N, 1)"
1.3.0,"Shape (N*n_cells, 1) after tile"
1.3.0,"List of n_cells tensors of shape (N, 1)"
1.3.0,Lists of length n_cells
1.3.0,Lists of length n_cells
1.3.0,Get indices of k atoms closest to each cell point
1.3.0,"n_cells tensors of shape (k, ndim)"
1.3.0,"Tensor of shape (n_cells, k)"
1.3.0,TODO(rbharath):
1.3.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
1.3.0,- Need to group closest atoms amongst cell neighbors
1.3.0,- Need to do another top_k to find indices of closest neighbors.
1.3.0,- Return N lists corresponding to neighbors for every atom.
1.3.0,TODO(rbharath): Do we need to handle periodic boundary conditions
1.3.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.3.0,"looking for 26 neighbors, which isn't right for boundary cells in"
1.3.0,the cube.
1.3.0,Number of neighbors of central cube in 3-space is
1.3.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
1.3.0,TODO(rbharath)
1.3.0,n_cells = int(cells.get_shape()[0])
1.3.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.3.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.3.0,"Tile (a, a, a, b, b, b, etc.)"
1.3.0,"Tile (a, b, c, a, b, c, ...)"
1.3.0,"Lists of n_cells tensors of shape (N, 1)"
1.3.0,Lists of length n_cells
1.3.0,Lists of length n_cells
1.3.0,Get indices of k atoms closest to each cell point
1.3.0,"n_cells tensors of shape (26,)"
1.3.0,TODO(rbharath): Make this handle minibatches
1.3.0,"Shape (N_protein+N_ligand, 3)"
1.3.0,"Shape (N_protein+N_ligand,)"
1.3.0,"Shape (N_protein+N_ligand, M)"
1.3.0,"Shape (N_protein+N_ligand,)"
1.3.0,"Shape (N_protein+N_ligand, 3)"
1.3.0,"Shape (N_protein+N_ligand,)"
1.3.0,"Shape (N_protein+N_ligand, M)"
1.3.0,"Shape (N_protein+N_ligand, M, 3)"
1.3.0,"Shape (N_protein+N_ligand, M)"
1.3.0,"Shape (N_protein+N_ligand, M, 3)"
1.3.0,"Shape (N_protein+N_ligand, M)"
1.3.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
1.3.0,"Shape (N_protein+N_ligand, M)"
1.3.0,"Shape (N_protein+N_ligand, M)"
1.3.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
1.3.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
1.3.0,computing free-energy. This implementation currently uses all interaction
1.3.0,terms. Not sure if this makes a difference.
1.3.0,"Shape (N_protein+N_ligand, M)"
1.3.0,Shape () -- scalar
1.3.0,# Gather Projection
1.3.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
1.3.0,There should be 8 layers in graph_model
1.3.0,assert len(graph_model.layers) == 6
1.3.0,Add layers
1.3.0,Need to add batch-norm separately to test/support due to differing
1.3.0,shapes.
1.3.0,Apply an attention lstm layer
1.3.0,Gather Projection
1.3.0,Add layers
1.3.0,Need to add batch-norm separately to test/support due to differing
1.3.0,shapes.
1.3.0,Apply an attention lstm layer
1.3.0,Gather Projection
1.3.0,Degrees from 1 to max_deg inclusive
1.3.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
1.3.0,"Should have shape (?, deg)"
1.3.0,"Shape of atom_features should be (?, n_feat)"
1.3.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
1.3.0,TODO(rbharath): Check that this operation is differentiable.
1.3.0,The number of cells which we should theoretically have
1.3.0,The number of cells which we should theoretically have
1.3.0,"Each atom neighbors tensor should be (k, ndim) shaped."
1.3.0,The number of cells which we should theoretically have
1.3.0,TODO(rbharath): The test below only checks that shapes work out.
1.3.0,Need to do a correctness implementation vs. a simple CPU impl.
1.3.0,The number of cells which we should theoretically have
1.3.0,TODO(rbharath): The test below only checks that shapes work out.
1.3.0,Need to do a correctness implementation vs. a simple CPU impl.
1.3.0,The number of cells which we should theoretically have
1.3.0,TODO(rbharath): The test below only checks that shapes work out.
1.3.0,Need to do a correctness implementation vs. a simple CPU impl.
1.3.0,TODO(rbharath): Commenting this out due to weird segfaults
1.3.0,def test_vina_generate_conformers(self):
1.3.0,"""""""Test that Vina Model can generate conformers"""""""
1.3.0,data_dir = os.path.dirname(os.path.realpath(__file__))
1.3.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
1.3.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
1.3.0,max_protein_atoms = 3500
1.3.0,max_ligand_atoms = 100
1.3.0,"print(""Loading protein file"")"
1.3.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
1.3.0,protein_Z = pad_array(
1.3.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
1.3.0,max_protein_atoms)
1.3.0,"print(""Loading ligand file"")"
1.3.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
1.3.0,ligand_Z = pad_array(
1.3.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
1.3.0,max_ligand_atoms)
1.3.0,-*- coding: utf-8 -*-
1.3.0,Assigning featurizer if not user defined
1.3.0,loading datasets
1.3.0,Assembling train and valid datasets
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,Loading hyper parameters
1.3.0,Building tensorflow MultiTaskDNN model
1.3.0,Loading hyper parameters
1.3.0,Building tensorflow robust MultiTaskDNN model
1.3.0,Loading hyper parameters
1.3.0,Building tensorflow logistic regression model
1.3.0,Loading hyper parameters
1.3.0,Transform fingerprints to IRV features
1.3.0,Building tensorflow IRV model
1.3.0,Loading hyper parameters
1.3.0,Gather Projection
1.3.0,Loading hyper parameters
1.3.0,Loading hyper parameters
1.3.0,Building scikit random forest model
1.3.0,Loading hyper parameters
1.3.0,Building scikit learn Kernel SVM model
1.3.0,Loading hyper parameters
1.3.0,Building xgboost classification model
1.3.0,Loading hyper parameters
1.3.0,Building tensorflow MultiTaskDNN model
1.3.0,Loading hyper parameters
1.3.0,Initialize model folder
1.3.0,Loading hyper parameters
1.3.0,Gather Projection
1.3.0,Loading hyper parameters
1.3.0,Loading hyper parameters
1.3.0,Remove token for paddings
1.3.0,Loading hyper parameters
1.3.0,Building scikit random forest model
1.3.0,Loading hyper parameters
1.3.0,Building scikit learn Kernel Ridge Regression model
1.3.0,Loading hyper parameters
1.3.0,Building scikit learn Kernel Ridge Regression model
1.3.0,Loading hyper parameters
1.3.0,Building xgboost classification model
1.3.0,Loading hyperparameters
1.3.0,num positive/negative ligands
1.3.0,Set batch sizes for network
1.3.0,Model structure
1.3.0,Traning settings
1.3.0,Fit trained model
1.3.0,Evaluating low data model
1.3.0,-*- coding: utf-8 -*-
1.3.0,Assigning featurizer if not user defined
1.3.0,loading datasets
1.3.0,
1.3.0,Note by @XericZephyr. Reason why I spun off this function:
1.3.0,1. Some model needs dataset information.
1.3.0,2. It offers us possibility to **cache** the dataset
1.3.0,"if the featurizer runs very slow, e.g., GraphConv."
1.3.0,2+. The cache can even happen at Travis CI to accelerate
1.3.0,CI testing.
1.3.0,
1.3.0,loading datasets
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,Featurize qm9 dataset
1.3.0,transformers = [
1.3.0,"deepchem.trans.LogTransformer(transform_X=True),"
1.3.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
1.3.0,dataset=train_dataset)]
1.3.0,Set shard size low to avoid memory problems.
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Set some global variables up top
1.3.0,Featurize KAGGLE dataset
1.3.0,############################################################# TIMING
1.3.0,############################################################# TIMING
1.3.0,Featurize qm7 dataset
1.3.0,Featurize clintox dataset
1.3.0,Transform clintox dataset
1.3.0,Split clintox dataset
1.3.0,Featurize bbb dataset
1.3.0,Initialize transformers
1.3.0,Load nci dataset
1.3.0,Featurize nci dataset
1.3.0,Initialize transformers
1.3.0,Featurize HOPV dataset
1.3.0,Initialize transformers
1.3.0,Featurize PPB dataset
1.3.0,Initialize transformers
1.3.0,Load MUV dataset
1.3.0,Featurize MUV dataset
1.3.0,Initialize transformers
1.3.0,Featurize clearance dataset
1.3.0,Initialize transformers
1.3.0,Featurize TOXCAST dataset
1.3.0,Initialize transformers
1.3.0,Featurize bace dataset
1.3.0,Initialize transformers
1.3.0,Featurize bace dataset
1.3.0,Initialize transformers
1.3.0,Featurize Tox21 dataset
1.3.0,Initialize transformers
1.3.0,Featurize ChEMBL dataset
1.3.0,Initialize transformers
1.3.0,Featurize hiv dataset
1.3.0,Initialize transformers
1.3.0,Featurize SIDER dataset
1.3.0,Initialize transformers
1.3.0,Featurize SAMPL dataset
1.3.0,Initialize transformers
1.3.0,Featurize Delaney dataset
1.3.0,Initialize transformers
1.3.0,Featurize PCBA dataset
1.3.0,Initialize transformers
1.3.0,Featurize Lipophilicity dataset
1.3.0,Initialize transformers
1.3.0,"Float or int hyper parameters(ex. batch_size, learning_rate)"
1.3.0,List of float or int hyper parameters(ex. layer_sizes)
1.3.0,Number of parameters
1.3.0,Range of optimization
1.3.0,Dummy names
1.3.0,Input hyper parameters
1.3.0,Run benchmark
1.3.0,Record hyperparameters
1.3.0,Record performances
1.3.0,"GPGO maximize performance by default, set performance to its negative value for minimization"
1.3.0,Readout best hyper parameters
1.3.0,Compare best model to default hyperparameters
1.3.0,Record hyperparameters
1.3.0,Record performances
1.3.0,"Optimized model is better, return hyperparameters"
1.3.0,Return default hyperparameters
1.3.0,!/usr/bin/env python2
1.3.0,-*- coding: utf-8 -*-
1.3.0,TODO(rbharath): This function is complicated and monolithic. Is there a nice
1.3.0,way to refactor this?
1.3.0,arbitrarily return last model
1.3.0,Define train dataset
1.3.0,Define validation dataset
1.3.0,Have the worker threads generate the rollouts for this iteration.
1.3.0,Perform optimization.
1.3.0,Build the feed dict and run the optimizer.
1.3.0,Update the number of steps taken so far and perform checkpointing.
1.3.0,Merge all the rollouts into a single set of arrays.
1.3.0,Iterate slices.
1.3.0,Generate the rollout.
1.3.0,Compute an estimate of the reward for the rest of the episode.
1.3.0,Compute the discounted rewards and advantages.
1.3.0,Convert the actions to one-hot.
1.3.0,Rearrange the states into the proper set of arrays.
1.3.0,Return the processed arrays.
1.3.0,Generate the rollout.
1.3.0,Compute an estimate of the reward for the rest of the episode.
1.3.0,Compute the discounted rewards and advantages.
1.3.0,Convert the actions to one-hot.
1.3.0,Rearrange the states into the proper set of arrays.
1.3.0,Build the feed dict and apply gradients.
1.3.0,Assume all arrays are float32.
1.3.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
1.3.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
1.3.0,"game).  The average reward for any bet is slightly negative, so the best"
1.3.0,strategy is to walk away.
1.3.0,"This policy just learns a constant probability for each action, and a constant for the value."
1.3.0,Optimize it.
1.3.0,"It should have learned that the expected value is very close to zero, and that the best"
1.3.0,action is to walk away.
1.3.0,"Verify that we can create a new PPO object, reload the parameters from the first one, and"
1.3.0,get the same result.
1.3.0,"Do the same thing, only using the ""restore"" argument to fit()."
1.3.0,The environment just has a constant state.
1.3.0,The policy includes a single recurrent layer.
1.3.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
1.3.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
1.3.0,"On the first call, the initial state should be all zeros."
1.3.0,It should still be zeros since we didn't save it last time.
1.3.0,It should be different now.
1.3.0,This should be the same as the previous one.
1.3.0,"Now we reset it, so we should get the same result as initially."
1.3.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
1.3.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
1.3.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
1.3.0,at all.  Using hindsight makes it much easier.
1.3.0,A simple policy with two hidden layers.
1.3.0,Optimize it.
1.3.0,Try running it a few times and see if it succeeds.
1.3.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
1.3.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
1.3.0,"game).  The average reward for any bet is slightly negative, so the best"
1.3.0,strategy is to walk away.
1.3.0,"This policy just learns a constant probability for each action, and a constant for the value."
1.3.0,Optimize it.
1.3.0,"It should have learned that the expected value is very close to zero, and that the best"
1.3.0,action is to walk away.
1.3.0,"Verify that we can create a new A3C object, reload the parameters from the first one, and"
1.3.0,get the same result.
1.3.0,"Do the same thing, only using the ""restore"" argument to fit()."
1.3.0,The environment just has a constant state.
1.3.0,The policy includes a single recurrent layer.
1.3.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
1.3.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
1.3.0,"On the first call, the initial state should be all zeros."
1.3.0,It should still be zeros since we didn't save it last time.
1.3.0,It should be different now.
1.3.0,This should be the same as the previous one.
1.3.0,"Now we reset it, so we should get the same result as initially."
1.3.0,The environment is a plane in which the agent moves by steps until it reaches a randomly
1.3.0,positioned goal.  No reward is given until it reaches the goal.  That makes it very hard
1.3.0,"to learn by standard methods, since it may take a very long time to receive any feedback"
1.3.0,at all.  Using hindsight makes it much easier.
1.3.0,A simple policy with two hidden layers.
1.3.0,Optimize it.
1.3.0,Try running it a few times and see if it succeeds.
1.3.0,Randomize who goes first
1.3.0,Illegal move -- the square is not empty
1.3.0,Move X
1.3.0,Did X Win
1.3.0,Did O Win
1.3.0,TODO (Bowen): make this function less memory intensive
1.3.0,set 1st column as the column index of dataframe
1.3.0,merge descriptor and activities dataframe into output dataframe based on
1.3.0,"the molecule name, which is the index for both dataframes (but named"
1.3.0,differently). Default merge is inner merge
1.3.0,need to manually set dataframe indexname after merge based on index
1.3.0,from deepchem.scripts.dock_dude import *
1.3.0,from ipyparallel import Client
1.3.0,rc = Client()
1.3.0,dview = rc[:]
1.3.0,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
1.3.0,
1.3.0,"If mol_id is not set, then use isomeric smiles as unique identifier"
1.3.0,iterator = data_df.iterrows()
1.3.0,TODO(rbharath): BROKEN!
1.3.0,Trim unwanted indexing fields
1.3.0,Connect to running ipython server
1.3.0,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
1.3.0,
1.3.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.3.0,you may not use this file except in compliance with the License.
1.3.0,You may obtain a copy of the License at
1.3.0,
1.3.0,http://www.apache.org/licenses/LICENSE-2.0
1.3.0,
1.3.0,"Unless required by applicable law or agreed to in writing, software"
1.3.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.3.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.3.0,See the License for the specific language governing permissions and
1.3.0,limitations under the License.
1.3.0,==============================================================================
1.3.0,Maps from a function name to a dictionary that describes how to
1.3.0,map from an old argument keyword to the new argument keyword.
1.3.0,Mapping from function to the new name of the function
1.3.0,Functions that were reordered should be changed to the new keyword args
1.3.0,"for safety, if positional arguments are used. If you have reversed the"
1.3.0,"positional arguments yourself, this could do the wrong thing."
1.3.0,Specially handled functions.
1.3.0,TODO(aselle): Could check for a literal list of bools and try to convert
1.3.0,them to indices.
1.3.0,all edits are lists of chars
1.3.0,Iterate of each line
1.3.0,sort by column so that edits are processed in order in order to make
1.3.0,indexing adjustments cumulative for changes that change the string
1.3.0,length
1.3.0,"Extract each line to a list of characters, because mutable lists"
1.3.0,"are editable, unlike immutable strings."
1.3.0,Record a description of the change
1.3.0,Make underscore buffers for underlining where in the line the edit was
1.3.0,Iterate for each edit
1.3.0,"Create effective start, end by accounting for change in length due"
1.3.0,to previous edits
1.3.0,Make sure the edit is changing what it should be changing
1.3.0,Make the edit
1.3.0,Create the underline highlighting of the before and after
1.3.0,Keep track of how to generate effective ranges
1.3.0,Finish the report comment
1.3.0,"Strangely, ast.ListComp returns the col_offset of the first token"
1.3.0,after the '[' token which appears to be a bug. Workaround by
1.3.0,explicitly finding the real start of the list comprehension.
1.3.0,loop over lines
1.3.0,Reverse the text to and regular expression search for whitespace
1.3.0,First find if a [ can be found with only whitespace between it and
1.3.0,col.
1.3.0,TODO(aselle):
1.3.0,"this is poor comment detection, but it is good enough for"
1.3.0,cases where the comment does not contain string literal starting/
1.3.0,ending characters. If ast gave us start and end locations of the
1.3.0,"ast nodes rather than just start, we could use string literal"
1.3.0,node ranges to filter out spurious #'s that appear in string
1.3.0,literals.
1.3.0,"Most other nodes return proper locations (with notably does not), but"
1.3.0,it is not possible to use that in an argument.
1.3.0,"Find a simple attribute name path e.g. ""tf.foo.bar"""
1.3.0,Make sure the func is marked as being part of a call
1.3.0,Call special handlers
1.3.0,Examine any non-keyword argument and make it into a keyword argument
1.3.0,if reordering required.
1.3.0,Examine each keyword argument and convert it to the final renamed form
1.3.0,TODO(aselle): We should scan backward to find the start of the
1.3.0,keyword key. Unfortunately ast does not give you the location of
1.3.0,"keyword keys, so we are forced to infer it from the keyword arg"
1.3.0,value.
1.3.0,"Write to a temporary file, just in case we are doing an implace modify."
1.3.0,Broad exceptions are required here because ast throws whatever it wants.
1.3.0,pylint: disable=broad-except
1.3.0,pylint: enable=broad-except
1.3.0,make sure output directory doesn't exist
1.3.0,make sure output directory does not overlap with root_directory
1.3.0,Collect list of files to process (we do this to correctly handle if the
1.3.0,user puts the output directory in some sub directory of the input dir)
1.3.0,import os
1.3.0,"from deepchem.utils.save import load_from_disk, save_to_disk"
1.3.0,from deepchem.featurizers.fingerprints import CircularFingerprint
1.3.0,from deepchem.featurizers.basic import RDKitDescriptors
1.3.0,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
1.3.0,from deepchem.featurizers.grid_featurizer import GridFeaturizer
1.3.0,from deepchem.featurizers.featurize import DataLoader
1.3.0,
1.3.0,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
1.3.0,"print(""About to load dataset form disk."")"
1.3.0,dataset = load_from_disk(dataset_file)
1.3.0,"print(""Loaded dataset."")"
1.3.0,
1.3.0,grid_featurizer = GridFeaturizer(
1.3.0,"voxel_width=16.0, feature_types=""voxel_combined"","
1.3.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.3.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.3.0,"parallel=True, flatten=True)"
1.3.0,featurizers = [CircularFingerprint(size=1024)]
1.3.0,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
1.3.0,
1.3.0,#Make a directory in which to store the featurized complexes.
1.3.0,"base_dir = ""../../../grid_nnscore_circular_features"""
1.3.0,if not os.path.exists(base_dir):
1.3.0,os.makedirs(base_dir)
1.3.0,"data_dir = os.path.join(base_dir, ""data"")"
1.3.0,if not os.path.exists(data_dir):
1.3.0,os.makedirs(data_dir)
1.3.0,
1.3.0,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
1.3.0,
1.3.0,"feature_dir = os.path.join(base_dir, ""features"")"
1.3.0,if not os.path.exists(feature_dir):
1.3.0,os.makedirs(feature_dir)
1.3.0,
1.3.0,"samples_dir = os.path.join(base_dir, ""samples"")"
1.3.0,if not os.path.exists(samples_dir):
1.3.0,os.makedirs(samples_dir)
1.3.0,
1.3.0,
1.3.0,
1.3.0,featurizers = compound_featurizers + complex_featurizers
1.3.0,"featurizer = DataLoader(tasks=[""label""],"
1.3.0,"smiles_field=""smiles"","
1.3.0,"protein_pdb_field=""protein_pdb"","
1.3.0,"ligand_pdb_field=""ligand_pdb"","
1.3.0,"compound_featurizers=compound_featurizers,"
1.3.0,"complex_featurizers=complex_featurizers,"
1.3.0,"id_field=""complex_id"","
1.3.0,verbose=False)
1.3.0,from ipyparallel import Client
1.3.0,c = Client()
1.3.0,"print(""c.ids"")"
1.3.0,print(c.ids)
1.3.0,dview = c[:]
1.3.0,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
1.3.0,"worker_pool=dview, shard_size=1024)"
1.3.0,
1.3.0,"save_to_disk(featurized_samples, featurized_samples_file)"
1.3.0,"print(""Preparing ligand %s"" % mol_name)"
1.2.0,!/usr/bin/env python3
1.2.0,-*- coding: utf-8 -*-
1.2.0,Assigning featurizer
1.2.0,Some exceptions in datasets
1.2.0,loading datasets
1.2.0,time_finish_loading-time_start is the time(s) used for dataset loading
1.2.0,dataset has customized features
1.2.0,running model
1.2.0,Initialize metrics
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow MultiTaskDNN model
1.2.0,Evaluating tensorflow MultiTaskDNN model
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow robust MultiTaskDNN model
1.2.0,Evaluating tensorflow robust MultiTaskDNN model
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow logistic regression model
1.2.0,Evaluating tensorflow logistic regression model
1.2.0,Loading hyper parameters
1.2.0,Transform fingerprints to IRV features
1.2.0,Building tensorflow IRV model
1.2.0,Evaluating tensorflow IRV model
1.2.0,Initialize model folder
1.2.0,Loading hyper parameters
1.2.0,Building graph convolution model
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,Evaluating graph convolution model
1.2.0,Loading hyper parameters
1.2.0,Building scikit random forest model
1.2.0,Evaluating scikit random forest model
1.2.0,Loading hyper parameters
1.2.0,Building xgboost classification model
1.2.0,Evaluating xgboost classification model
1.2.0,Initialize metrics
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow MultiTaskDNN model
1.2.0,Evaluating tensorflow MultiTaskDNN model
1.2.0,Initialize model folder
1.2.0,Loading hyper parameters
1.2.0,Building graph convoluwtion model
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,Evaluating graph convolution model
1.2.0,Loading hyper parameters
1.2.0,Building scikit random forest model
1.2.0,Evaluating scikit random forest model
1.2.0,Loading hyper parameters
1.2.0,Building xgboost classification model
1.2.0,Evaluating xgboost classification model
1.2.0,Global variables
1.2.0,Datasets and models used in the benchmark test
1.2.0,"irv, rf, rf_regression should be assigned manually"
1.2.0,input hyperparameters
1.2.0,!/usr/bin/env python3
1.2.0,-*- coding: utf-8 -*-
1.2.0,Datasets and models used in the benchmark test
1.2.0,!/usr/bin/env python3
1.2.0,-*- coding: utf-8 -*-
1.2.0,Datasets and models used in the benchmark test
1.2.0,"irv, rf, rf_regression should be assigned manually"
1.2.0,-*- coding: utf-8 -*-
1.2.0,main layer
1.2.0,bypass layer
1.2.0,penalty
1.2.0,general figure
1.2.0,learning rate
1.2.0,for graph-conv and random forest
1.2.0,Set numpy seed
1.2.0,##Load data###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Featurize Kinase dataset
1.2.0,##Load data###
1.2.0,num_trials = 5
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,Force matplotlib to not use any Xwindows backend.
1.2.0,##Load data###
1.2.0,the histogram of the data
1.2.0,Set numpy seed
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,num_trials = 5
1.2.0,Set some global variables up top
1.2.0,Fit trained model
1.2.0,Featurize PCBA dataset
1.2.0,Initialize transformers
1.2.0,Fit trained model
1.2.0,Load SWEET dataset
1.2.0,Featurize SWEET dataset
1.2.0,Initialize transformers
1.2.0,Set some global variables up top
1.2.0,removes directory if present -- warning
1.2.0,default split is 80-10-10 train-valid-test split
1.2.0,Fit Logistic Regression models
1.2.0,Fit Logistic Regression models
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,transformers = [
1.2.0,"dc.trans.LogTransformer(transform_X=True),"
1.2.0,"dc.trans.NormalizationTransformer(transform_y=True,"
1.2.0,dataset=train_dataset)]
1.2.0,Set shard size low to avoid memory problems.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Set some global variables up top
1.2.0,Featurize KAGGLE dataset
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,##Load data###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,##Load data###
1.2.0,"n_estimators=100, max_features=int(num_features/3),"
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,"Images are square 28x28 (batch, height, width, channel)"
1.2.0,Featurize qm9 dataset
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Featurize Tox21 dataset
1.2.0,Initialize transformers
1.2.0,Only for debug!
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Load tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Featurize FACTORS dataset
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,Force matplotlib to not use any Xwindows backend.
1.2.0,##Load data###
1.2.0,the histogram of the data
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Featurize qm7 dataset
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Batch size of models
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Batch size of models
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Featurize qm8 dataset
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Fit trained model
1.2.0,Set numpy seed
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,Set shard size low to avoid memory problems.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Set some global variables up top
1.2.0,Load dataset
1.2.0,Featurize ChEMBL dataset
1.2.0,Initialize transformers
1.2.0,Load ChEMBL dataset
1.2.0,Fit models
1.2.0,Do setup required for tf/keras models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,DeepCrystal Technologies 2017 - Patrick Hop
1.2.0,MIT License - have fun!!
1.2.0,======================================================================
1.2.0,"Run Benchmarks {GC-DNN, P-DNN, SVR, RF}"
1.2.0,we cant re-open the closed session...
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Do setup required for tf/keras models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
1.2.0,Fit trained model
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Do setup required for tf/keras models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Gather Projection
1.2.0,Dense post-processing layer
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Featurize Delaney dataset
1.2.0,Initialize transformers
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load Delaney dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load MUV dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Evaluate train/test scores
1.2.0,Load MUV dataset
1.2.0,Featurize MUV dataset
1.2.0,Initialize transformers
1.2.0,Load MUV data
1.2.0,Build model
1.2.0,Fit trained model
1.2.0,Evaluate train/test scores
1.2.0,Extract active site
1.2.0,Featurize ligand
1.2.0,Default for CircularFingerprint
1.2.0,Featurize pocket
1.2.0,Note broadcast operation
1.2.0,Compute labels for pockets
1.2.0,Some complexes have labels but no PDB files. Filter these manually
1.2.0,Some of the ligand-names are of form (FMN ox). Use regex
1.2.0,to merge into form (FMN-ox)
1.2.0,Filter if missing PDB files
1.2.0,Load PDBBind dataset
1.2.0,Define featurizers
1.2.0,Featurize Dataset
1.2.0,########################################################## DEBUG
1.2.0,########################################################## DEBUG
1.2.0,For stable runs
1.2.0,Fit trained model
1.2.0,4-fold splits
1.2.0,num positive/negative ligands
1.2.0,10 trials on test-set
1.2.0,Define metric
1.2.0,Get supports on test-set
1.2.0,Compute accuracies
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Test model
1.2.0,Join information for all tasks.
1.2.0,4-fold splits
1.2.0,num positive/negative ligands
1.2.0,Define metric
1.2.0,Get supports on test-set
1.2.0,Compute accuracies
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Test model
1.2.0,Join information for all tasks.
1.2.0,4-fold splits
1.2.0,num positive/negative ligands
1.2.0,10 trials on test-set
1.2.0,Define metric
1.2.0,Get supports on test-set
1.2.0,Compute accuracies
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Fit trained model
1.2.0,Test model
1.2.0,Join information for all tasks.
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,number positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply an attention lstm layer
1.2.0,Number of folds for split
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,4-fold splits
1.2.0,num positive/negative ligands
1.2.0,10 trials on test-set
1.2.0,Define metric
1.2.0,Get supports on test-set
1.2.0,Compute accuracies
1.2.0,Train model on support
1.2.0,Test model
1.2.0,Join information for all tasks.
1.2.0,4-fold splits
1.2.0,num positive/negative ligands
1.2.0,10 trials on test-set
1.2.0,Define metric
1.2.0,Get supports on test-set
1.2.0,Compute accuracies
1.2.0,Train model on support
1.2.0,Test model
1.2.0,Join information for all tasks.
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,number positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply an attention lstm layer
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,number positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply an attention lstm layer
1.2.0,Number of folds for split
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Number of folds for split
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply a residual lstm layer
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply a residual lstm layer
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply a residual lstm layer
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply a residual lstm layer
1.2.0,Set some global variables up top
1.2.0,Featurize Tox21 dataset
1.2.0,Initialize transformers
1.2.0,Set some global variables up top
1.2.0,Featurize Tox21 dataset
1.2.0,Initialize transformers
1.2.0,Load MUV dataset
1.2.0,Featurize MUV dataset
1.2.0,Initialize transformers
1.2.0,Load MUV dataset
1.2.0,Featurize MUV dataset
1.2.0,Initialize transformers
1.2.0,Featurize SIDER dataset
1.2.0,Initialize transformers
1.2.0,Featurize SIDER dataset
1.2.0,Initialize transformers
1.2.0,Number of folds for split
1.2.0,Depth of attention module
1.2.0,number positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,Apply an attention lstm layer
1.2.0,4-fold splits
1.2.0,10 positive/negative ligands
1.2.0,10 trials on test-set
1.2.0,Sample supports without replacement (all pos/neg should be different)
1.2.0,Define metric
1.2.0,Get supports on test-set
1.2.0,Compute accuracies
1.2.0,Train model on support
1.2.0,Test model
1.2.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
1.2.0,Join information for all tasks.
1.2.0,Number of folds for split
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Number of features on conv-mols
1.2.0,Define metric
1.2.0,Train support model on train
1.2.0,Add layers
1.2.0,4-fold splits
1.2.0,num positive/negative ligands
1.2.0,Define metric
1.2.0,Get supports on test-set
1.2.0,Compute accuracies
1.2.0,Train model on support
1.2.0,Test model
1.2.0,Join information for all tasks.
1.2.0,Note sensitivity = recall
1.2.0,NOTE THE RENAMING:
1.2.0,Note sensitivity = recall
1.2.0,Load nci dataset
1.2.0,Featurize nci dataset
1.2.0,Initialize transformers
1.2.0,Set some global variables up top
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load hiv dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Featurize hiv dataset
1.2.0,Initialize transformers
1.2.0,Only for debug!
1.2.0,Load hiv dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Fit trained model
1.2.0,Fit models
1.2.0,Batch size of models
1.2.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.2.0,Fit trained model
1.2.0,Load SIDER dataset
1.2.0,Featurize SIDER dataset
1.2.0,Initialize transformers
1.2.0,Featurize permeability dataset
1.2.0,Load Tox21 dataset
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load SAMPL dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load Tox21 dataset
1.2.0,Fit models
1.2.0,Do setup required for tf/keras models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Gather Projection
1.2.0,Dense post-processing layer
1.2.0,Fit trained model
1.2.0,Featurize SAMPL dataset
1.2.0,Initialize transformers
1.2.0,Load clintox dataset
1.2.0,Featurize clintox dataset
1.2.0,Transform clintox dataset
1.2.0,Split clintox dataset
1.2.0,Only for debug!
1.2.0,Load clintox dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load clintox dataset
1.2.0,Fit models
1.2.0,Do setup required for tf/keras models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,-*- coding: utf-8 -*-
1.2.0,#############################################################################
1.2.0,## save dataset
1.2.0,#############################################################################
1.2.0,## load datasets
1.2.0,load sweetfda
1.2.0,load aact
1.2.0,## fixup smiles for matching
1.2.0,return smiles
1.2.0,map original smiles to converted smiles
1.2.0,"## join dataframes, index on smiles"
1.2.0,map original smiles back
1.2.0,## fill all nan with 0
1.2.0,## construct datasets
1.2.0,store in new datasets
1.2.0,## save datasets
1.2.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
1.2.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
1.2.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
1.2.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
1.2.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
1.2.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
1.2.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
1.2.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
1.2.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
1.2.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
1.2.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
1.2.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
1.2.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
1.2.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
1.2.0,For stable runs
1.2.0,Fit trained model
1.2.0,For stable runs
1.2.0,Fit trained model
1.2.0,Some complexes have labels but no PDB files. Filter these manually
1.2.0,Some of the ligand-names are of form (FMN ox). Use regex
1.2.0,to merge into form (FMN-ox)
1.2.0,Load PDBBind dataset
1.2.0,Define featurizers
1.2.0,"TODO(rbharath, enf, leswing): Figure out why pi_stack and cation_pi"
1.2.0,reduce validation performance
1.2.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.2.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.2.0,Featurize Dataset
1.2.0,Currently featurizes with shard_size=1
1.2.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.2.0,transformers = [
1.2.0,"dc.trans.LogTransformer(transform_X=True),"
1.2.0,"dc.trans.NormalizationTransformer(transform_y=True,"
1.2.0,dataset=train_dataset)]
1.2.0,Featurize UV dataset
1.2.0,##Load data###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Set numpy seed
1.2.0,##Load data###
1.2.0,##Create model###
1.2.0,Use R2 classification metric
1.2.0,"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
1.2.0,Only use for final evaluation
1.2.0,Force matplotlib to not use any Xwindows backend.
1.2.0,##Load data###
1.2.0,the histogram of the data
1.2.0,##Load data###
1.2.0,###################################################### DEBUG
1.2.0,###################################################### DEBUG
1.2.0,Load HOPV dataset
1.2.0,Fit models
1.2.0,Number of features on conv-mols
1.2.0,Batch size of models
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,Featurize HOPV dataset
1.2.0,Initialize transformers
1.2.0,Only for debug!
1.2.0,Load HOPV dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load HOPV dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load HOPV dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Only for debug!
1.2.0,Load HOPV dataset
1.2.0,Fit models
1.2.0,Fit trained model
1.2.0,Load TOXCAST dataset
1.2.0,Featurize TOXCAST dataset
1.2.0,Initialize transformers
1.2.0,Fit trained model
1.2.0,Processing of ToxCast data
1.2.0,Author - Aneesh Pappu
1.2.0,Loading dataframes and editing indices
1.2.0,Loop through rows of hitc matrix and replace codes with smiles strings
1.2.0,get corresponding casn
1.2.0,get corresponding smiles
1.2.0,write to cell
1.2.0,Tidy up and write to csv
1.2.0,-*- coding: utf-8 -*-
1.2.0,Save hyperparameters
1.2.0,-*- coding: utf-8 -*-
1.2.0,Save hyperparameters
1.2.0,setup optimizer
1.2.0,setup optimizer
1.2.0,"print(""tasK: %d"" %task)"
1.2.0,"cores = torch.cat([scores, 1.-scores], dim=1)"
1.2.0,"print(""scores"")"
1.2.0,print(scores.size())
1.2.0,"print(""task_label"")"
1.2.0,print(task_label.size())
1.2.0,"task_loss =  self.criterion(scores, task_label)"
1.2.0,"print(""task_loss"")"
1.2.0,print(task_loss.size())
1.2.0,-*- coding: utf-8 -*-
1.2.0,Save hyperparameters
1.2.0,weight decay
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Turns out there are valid cases where we don't want pad-batches
1.2.0,on by default.
1.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.2.0,Run training op.
1.2.0,############################################################# TIMING
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,Special case to handle singletasks.
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,2017 DeepCrystal Technologies - Patrick Hop
1.2.0,
1.2.0,Message Passing Neural Network SELU [MPNN-S] for Chemical Multigraphs
1.2.0,
1.2.0,MIT License - have fun!!
1.2.0,===========================================================
1.2.0,x = F.selu( fc(x) )
1.2.0,x = F.selu( fc(x) )
1.2.0,2017 DeepCrystal Technologies - Patrick Hop
1.2.0,
1.2.0,Data loading a splitting file
1.2.0,
1.2.0,MIT License - have fun!!
1.2.0,===========================================================
1.2.0,Set random seeds
1.2.0,Setup directories
1.2.0,Model constants
1.2.0,Load and transform datasets
1.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.2.0,Atomic convolution variables
1.2.0,at = atomic numbers (atom types)
1.2.0,"radial basis function parameters [cutoff, mean, width]"
1.2.0,Model hyperparameters
1.2.0,Initialize model
1.2.0,Fit model
1.2.0,Evaluate model
1.2.0,Set random seeds
1.2.0,Setup directories
1.2.0,Model constants
1.2.0,Load and transform datasets
1.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.2.0,Atomic convolution variables
1.2.0,at = atomic numbers (atom types)
1.2.0,"radial basis function parameters [cutoff, mean, width]"
1.2.0,Model hyperparameters
1.2.0,Initialize model
1.2.0,Fit model
1.2.0,Evaluate model
1.2.0,Set random seeds
1.2.0,Setup directories
1.2.0,Model constants
1.2.0,Load and transform datasets
1.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.2.0,Atomic convolution variables
1.2.0,at = atomic numbers (atom types)
1.2.0,"radial basis function parameters [cutoff, mean, width]"
1.2.0,Model hyperparameters
1.2.0,Initialize model
1.2.0,Fit model
1.2.0,Evaluate model
1.2.0,Set random seeds
1.2.0,Setup directories
1.2.0,Model constants
1.2.0,Load and transform datasets
1.2.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.2.0,Atomic convolution variables
1.2.0,at = atomic numbers (atom types)
1.2.0,"radial basis function parameters [cutoff, mean, width]"
1.2.0,Model hyperparameters
1.2.0,Initialize model
1.2.0,Fit model
1.2.0,Evaluate model
1.2.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
1.2.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
1.2.0,test_scores = test_evaluator.compute_model_performance(metric)
1.2.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
1.2.0,param.update(test_scores)
1.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.2.0,for transformer in transformers:
1.2.0,train_dataset = transformer.transform(train_dataset)
1.2.0,test_dataset = transformer.transform(test_dataset)
1.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.2.0,for transformer in transformers:
1.2.0,train_dataset = transformer.transform(train_dataset)
1.2.0,test_dataset = transformer.transform(test_dataset)
1.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.2.0,for transformer in transformers:
1.2.0,train_dataset = transformer.transform(train_dataset)
1.2.0,test_dataset = transformer.transform(test_dataset)
1.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.2.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.2.0,for transformer in transformers:
1.2.0,train_dataset = transformer.transform(train_dataset)
1.2.0,test_dataset = transformer.transform(test_dataset)
1.2.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.2.0,Create some directories for analysis
1.2.0,The base_dir holds the results of all analysis
1.2.0,Make directories to store the raw and featurized datasets.
1.2.0,Load PDBBind dataset
1.2.0,Define featurizers
1.2.0,Currently featurizes with shard_size=1
1.2.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.2.0,This could be done with openbabel in python
1.2.0,Compute cells for this molecule. O(constant)
1.2.0,min == max if molecule is planar in some direction
1.2.0,we should still create a bin
1.2.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
1.2.0,Note neighbors contains self!
1.2.0,Associate each atom with cell it belongs to. O(N)
1.2.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.2.0,"conditions, so does wrapround. O(constant)"
1.2.0,"For each atom, loop through all atoms in its cell and neighboring cells."
1.2.0,Accept as neighbors only those within threshold. This computation should be
1.2.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
1.2.0,Sort neighbors by distance
1.2.0,Pick up to max_num_neighbors
1.2.0,Type of data created by this featurizer
1.2.0,assumes that every array is of the same dimension
1.2.0,rem_dataset is remaining portion of dataset
1.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.2.0,to k-1.
1.2.0,returns list of per column sum of non zero elements
1.2.0,Compute number of actives needed per task.
1.2.0,loop through each column and obtain index required to splice out for
1.2.0,required fraction of hits
1.2.0,Find the first index where the cumulative number of actives equals
1.2.0,the actives_count
1.2.0,Note that np.where tells us last index required to exceed
1.2.0,"actives_count, so we actually want the following location"
1.2.0,TODO(rbharath): Refactor this split method to match API of other splits (or
1.2.0,potentially refactor those to match this.
1.2.0,Handle edge case where frac_split is 1
1.2.0,Create weight matrices fpor two haves.
1.2.0,copy over up to required index for weight first_split
1.2.0,check out if any rows in either w_1 or w_2 are just zeros
1.2.0,"Obtain original x, y, and w arrays and shuffle"
1.2.0,calculate percent split for valid (out of test and valid)
1.2.0,"split test data into valid and test, treating sub test set also as sparse"
1.2.0,rem_dataset is remaining portion of dataset
1.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.2.0,to k-1.
1.2.0,JSG Assert that split fractions can be written as proper fractions over 10.
1.2.0,This can be generalized in the future with some common demoninator determination.
1.2.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.2.0,Append remaining examples to train
1.2.0,Sort by increasing MW
1.2.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.2.0,for m_idx in cluster:
1.2.0,"continue until we find an active in all the tasks, otherwise we can't"
1.2.0,compute a meaningful AUC
1.2.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.2.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.2.0,Sort from largest to smallest scaffold sets
1.2.0,Sort from largest to smallest scaffold sets
1.2.0,"(n_samples, n_classes)"
1.2.0,"(n_samples, n_tasks, n_classes)"
1.2.0,Save hyperparameters
1.2.0,Guard variable to make sure we don't Restore() this model
1.2.0,from a disk checkpoint more than once.
1.2.0,"Path to save checkpoint files, which matches the"
1.2.0,replicated supervisor's default path.
1.2.0,Lazily created by _get_shared_session().
1.2.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.2.0,when subclass-overridden methods use the same scopes.
1.2.0,Setup graph
1.2.0,Note that we divide by the batch size and not the number of
1.2.0,"non-zero weight examples in the batch.  Also, instead of using"
1.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.2.0,calculate with div/sum so it stays on the GPU.
1.2.0,aggregated costs
1.2.0,weight decay
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Save an initial checkpoint.
1.2.0,Turns out there are valid cases where we don't want pad-batches
1.2.0,on by default.
1.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.2.0,Run training op.
1.2.0,Always save a final checkpoint when complete.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,allow_soft_placement=True allows ops without a GPU implementation
1.2.0,to run on the CPU instead.
1.2.0,TODO(rbharath): Is setting train=False right here?
1.2.0,Discard any padded predictions
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,Special case to handle singletasks.
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,TODO(rbharath): Verify this can be safely removed.
1.2.0,"def evaluate(self, dataset, metrics, transformers=[]):"
1.2.0,""""""""
1.2.0,Evaluates the performance of this model on specified dataset.
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,dataset: dc.data.Dataset
1.2.0,Dataset object.
1.2.0,metric: deepchem.metrics.Metric
1.2.0,Evaluation metric
1.2.0,transformers: list
1.2.0,List of deepchem.transformers.Transformer
1.2.0,Returns
1.2.0,-------
1.2.0,dict
1.2.0,Maps tasks to scores under metric.
1.2.0,""""""""
1.2.0,"evaluator = Evaluator(self, dataset, transformers)"
1.2.0,scores = evaluator.compute_model_performance(metrics)
1.2.0,return scores
1.2.0,checkpoints look like logdir/model.ckpt-N
1.2.0,"self._save_path is ""logdir/model.ckpt"""
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,Note that softmax is already applied in construct_grpah
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,Handle edge case when batch-size is 1.
1.2.0,Prune away any padding that was added
1.2.0,Handle case of 0-dimensional scalar output
1.2.0,Dummy placeholders
1.2.0,Dummy placeholders
1.2.0,## AtomicNet fully-connected layer ops ###
1.2.0,## Atomicnet coordinate transform ops ###
1.2.0,## Atomicnet symmetry function kernel ops ###
1.2.0,## Atomicnet symmetry function ops ###
1.2.0,## Atomcnet symmetry function layer ops ###
1.2.0,We apply the radial pooling filter before atom type conv
1.2.0,to reduce computation
1.2.0,## Misc convenience ops ###
1.2.0,-*- coding: utf-8 -*-
1.2.0,
1.2.0,"deepchem documentation build configuration file, created by"
1.2.0,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
1.2.0,
1.2.0,This file is execfile()d with the current directory set to its
1.2.0,containing dir.
1.2.0,
1.2.0,Note that not all possible configuration values are present in this
1.2.0,autogenerated file.
1.2.0,
1.2.0,All configuration values have a default; values that are commented out
1.2.0,serve to show the default.
1.2.0,"If extensions (or modules to document with autodoc) are in another directory,"
1.2.0,add these directories to sys.path here. If the directory is relative to the
1.2.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.2.0,"sys.path.insert(0, os.path.abspath('.'))"
1.2.0,-- General configuration ------------------------------------------------
1.2.0,"If your documentation needs a minimal Sphinx version, state it here."
1.2.0,needs_sphinx = '1.0'
1.2.0,"Add any Sphinx extension module names here, as strings. They can be"
1.2.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.2.0,ones.
1.2.0,"Add any paths that contain templates here, relative to this directory."
1.2.0,The suffix(es) of source filenames.
1.2.0,You can specify multiple suffix as a list of string:
1.2.0,"source_suffix = ['.rst', '.md']"
1.2.0,The encoding of source files.
1.2.0,source_encoding = 'utf-8-sig'
1.2.0,The master toctree document.
1.2.0,General information about the project.
1.2.0,"The version info for the project you're documenting, acts as replacement for"
1.2.0,"|version| and |release|, also used in various other places throughout the"
1.2.0,built documents.
1.2.0,
1.2.0,The short X.Y version.
1.2.0,"The full version, including alpha/beta/rc tags."
1.2.0,The language for content autogenerated by Sphinx. Refer to documentation
1.2.0,for a list of supported languages.
1.2.0,
1.2.0,This is also used if you do content translation via gettext catalogs.
1.2.0,"Usually you set ""language"" from the command line for these cases."
1.2.0,"There are two options for replacing |today|: either, you set today to some"
1.2.0,"non-false value, then it is used:"
1.2.0,today = ''
1.2.0,"Else, today_fmt is used as the format for a strftime call."
1.2.0,"today_fmt = '%B %d, %Y'"
1.2.0,"List of patterns, relative to source directory, that match files and"
1.2.0,directories to ignore when looking for source files.
1.2.0,The reST default role (used for this markup: `text`) to use for all
1.2.0,documents.
1.2.0,default_role = None
1.2.0,"If true, '()' will be appended to :func: etc. cross-reference text."
1.2.0,add_function_parentheses = True
1.2.0,"If true, the current module name will be prepended to all description"
1.2.0,unit titles (such as .. function::).
1.2.0,add_module_names = True
1.2.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
1.2.0,output. They are ignored by default.
1.2.0,show_authors = False
1.2.0,The name of the Pygments (syntax highlighting) style to use.
1.2.0,A list of ignored prefixes for module index sorting.
1.2.0,modindex_common_prefix = []
1.2.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
1.2.0,keep_warnings = False
1.2.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.2.0,-- Options for HTML output ----------------------------------------------
1.2.0,The theme to use for HTML and HTML Help pages.  See the documentation for
1.2.0,a list of builtin themes.
1.2.0,Theme options are theme-specific and customize the look and feel of a theme
1.2.0,"further.  For a list of options available for each theme, see the"
1.2.0,documentation.
1.2.0,html_theme_options = {}
1.2.0,"Add any paths that contain custom themes here, relative to this directory."
1.2.0,"The name for this set of Sphinx documents.  If None, it defaults to"
1.2.0,"""<project> v<release> documentation""."
1.2.0,html_title = None
1.2.0,A shorter title for the navigation bar.  Default is the same as html_title.
1.2.0,html_short_title = None
1.2.0,The name of an image file (relative to this directory) to place at the top
1.2.0,of the sidebar.
1.2.0,The name of an image file (within the static path) to use as favicon of the
1.2.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
1.2.0,pixels large.
1.2.0,html_favicon = None
1.2.0,"Add any paths that contain custom static files (such as style sheets) here,"
1.2.0,"relative to this directory. They are copied after the builtin static files,"
1.2.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.2.0,Add any extra paths that contain custom files (such as robots.txt or
1.2.0,".htaccess) here, relative to this directory. These files are copied"
1.2.0,directly to the root of the documentation.
1.2.0,html_extra_path = []
1.2.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
1.2.0,using the given strftime format.
1.2.0,"html_last_updated_fmt = '%b %d, %Y'"
1.2.0,"If true, SmartyPants will be used to convert quotes and dashes to"
1.2.0,typographically correct entities.
1.2.0,html_use_smartypants = True
1.2.0,"Custom sidebar templates, maps document names to template names."
1.2.0,html_sidebars = {}
1.2.0,"Additional templates that should be rendered to pages, maps page names to"
1.2.0,template names.
1.2.0,html_additional_pages = {}
1.2.0,"If false, no module index is generated."
1.2.0,html_domain_indices = True
1.2.0,"If false, no index is generated."
1.2.0,html_use_index = True
1.2.0,"If true, the index is split into individual pages for each letter."
1.2.0,html_split_index = False
1.2.0,"If true, links to the reST sources are added to the pages."
1.2.0,html_show_sourcelink = True
1.2.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
1.2.0,html_show_sphinx = True
1.2.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
1.2.0,html_show_copyright = True
1.2.0,"If true, an OpenSearch description file will be output, and all pages will"
1.2.0,contain a <link> tag referring to it.  The value of this option must be the
1.2.0,base URL from which the finished HTML is served.
1.2.0,html_use_opensearch = ''
1.2.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
1.2.0,html_file_suffix = None
1.2.0,Language to be used for generating the HTML full-text search index.
1.2.0,Sphinx supports the following languages:
1.2.0,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
1.2.0,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
1.2.0,html_search_language = 'en'
1.2.0,"A dictionary with options for the search language support, empty by default."
1.2.0,Now only 'ja' uses this config value
1.2.0,html_search_options = {'type': 'default'}
1.2.0,The name of a javascript file (relative to the configuration directory) that
1.2.0,"implements a search results scorer. If empty, the default will be used."
1.2.0,html_search_scorer = 'scorer.js'
1.2.0,Output file base name for HTML help builder.
1.2.0,-- Options for LaTeX output ---------------------------------------------
1.2.0,The paper size ('letterpaper' or 'a4paper').
1.2.0,"'papersize': 'letterpaper',"
1.2.0,"The font size ('10pt', '11pt' or '12pt')."
1.2.0,"'pointsize': '10pt',"
1.2.0,Additional stuff for the LaTeX preamble.
1.2.0,"'preamble': '',"
1.2.0,Latex figure (float) alignment
1.2.0,"'figure_align': 'htbp',"
1.2.0,Grouping the document tree into LaTeX files. List of tuples
1.2.0,"(source start file, target name, title,"
1.2.0,"author, documentclass [howto, manual, or own class])."
1.2.0,The name of an image file (relative to this directory) to place at the top of
1.2.0,the title page.
1.2.0,latex_logo = None
1.2.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
1.2.0,not chapters.
1.2.0,latex_use_parts = False
1.2.0,"If true, show page references after internal links."
1.2.0,latex_show_pagerefs = False
1.2.0,"If true, show URL addresses after external links."
1.2.0,latex_show_urls = False
1.2.0,Documents to append as an appendix to all manuals.
1.2.0,latex_appendices = []
1.2.0,"If false, no module index is generated."
1.2.0,latex_domain_indices = True
1.2.0,-- Options for manual page output ---------------------------------------
1.2.0,One entry per manual page. List of tuples
1.2.0,"(source start file, name, description, authors, manual section)."
1.2.0,"If true, show URL addresses after external links."
1.2.0,man_show_urls = False
1.2.0,-- Options for Texinfo output -------------------------------------------
1.2.0,Grouping the document tree into Texinfo files. List of tuples
1.2.0,"(source start file, target name, title, author,"
1.2.0,"dir menu entry, description, category)"
1.2.0,Documents to append as an appendix to all manuals.
1.2.0,texinfo_appendices = []
1.2.0,"If false, no module index is generated."
1.2.0,texinfo_domain_indices = True
1.2.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
1.2.0,texinfo_show_urls = 'footnote'
1.2.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
1.2.0,texinfo_no_detailmenu = False
1.2.0,Example configuration for intersphinx: refer to the Python standard library.
1.2.0,Higher is Better
1.2.0,The secret key is available as a secure environment variable
1.2.0,on travis-ci to push the build documentation to Amazon S3.
1.2.0,########################################################### DEBUG
1.2.0,########################################################### DEBUG
1.2.0,s3cmd -M -H sync docs/_build/ s3://deepchem.io/
1.2.0,########################################################### DEBUG
1.2.0,########################################################### DEBUG
1.2.0,lines in the label file have format
1.2.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
1.2.0,"print line[0], line[3]"
1.2.0,References
1.2.0,Arguments
1.2.0,Aliases.
1.2.0,Tensorflow correctly processes empty lists when using concat
1.2.0,"Sum along neighbors as well as self, and store"
1.2.0,Sum all neighbors using adjacency matrix
1.2.0,Get collection of modified atom features
1.2.0,Obtain relevant atoms for this degree
1.2.0,Get self atoms
1.2.0,Apply hidden affine to relevant atoms and append
1.2.0,Determine the min_deg=0 case
1.2.0,Only use the self layer
1.2.0,Combine all atoms back into the list
1.2.0,"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
1.2.0,Obtain the partitions for each of the molecules
1.2.0,Sum over atoms for each molecule
1.2.0,Get the final sparse representations
1.2.0,Store the summed atoms by degree
1.2.0,Tensorflow correctly processes empty lists when using concat
1.2.0,Get self atoms
1.2.0,Expand dims
1.2.0,always deg-1 for deg_adj_lists
1.2.0,TODO(rbharath): It's not clear where nb_affine comes from.
1.2.0,Is there a solid explanation here?
1.2.0,Generate the nb_affine weights and biases
1.2.0,Add trainable weights
1.2.0,Extract atom_features
1.2.0,Extract graph topology
1.2.0,Perform the mol conv
1.2.0,Extract nodes and membership
1.2.0,Extract atom_features
1.2.0,Extract graph topology
1.2.0,Perform the mol gather
1.2.0,Extract nodes
1.2.0,Extract atom_features
1.2.0,Extract graph topology
1.2.0,Perform the mol gather
1.2.0,"x is test set, xp is support set."
1.2.0,# Initializes trainable weights.
1.2.0,## Performs computations
1.2.0,Get initializations
1.2.0,r = self.r_init
1.2.0,Process using attention
1.2.0,"Eqn (4), appendix A.1 of Matching Networks paper"
1.2.0,Generate new aattention states
1.2.0,"def build(self, input_shape):"
1.2.0,"_, support_input_shape = input_shape  #Unpack"
1.2.0,n_feat = support_input_shape[1]
1.2.0,Support set lstm
1.2.0,Test lstm
1.2.0,Get initializations
1.2.0,Rename support
1.2.0,Process support xp using attention
1.2.0,Get linear combination of support set
1.2.0,Not sure if it helps to place the update here or later yet.  Will
1.2.0,decide
1.2.0,z = r
1.2.0,Process test x using attention
1.2.0,Generate new support attention states
1.2.0,Generate new test attention states
1.2.0,Redefine
1.2.0,"return [x+p, z+q]"
1.2.0,No other forget biases supported right now.
1.2.0,"def build(self, input_shape):"
1.2.0,Taken from Keras code [citation needed]
1.2.0,###################################################### DEBUG
1.2.0,"return o, [h, c]"
1.2.0,###################################################### DEBUG
1.2.0,"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
1.2.0,distance_hidden = self.activation(distance_hidden)
1.2.0,atom_features_hidden = self.activation(atom_features_hidden)
1.2.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.2.0,and embeddings of atom j(both gone through a hidden layer)
1.2.0,"for atom i, sum the influence from all other atom j in the molecule"
1.2.0,number of inputs each step
1.2.0,Add trainable weights
1.2.0,Extract atom_features
1.2.0,Basic features of every atom: (batch_size*max_atoms) * n_atom_features
1.2.0,calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
1.2.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.2.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.2.0,"step i calculates the graph features for atoms of index `parents[:,i,0]`"
1.2.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.2.0,"represent the same atoms of `parents[:, :, 0]`,"
1.2.0,different in that these index are positions in `atom_features`
1.2.0,"number of atoms in total, should equal `batch_size*max_atoms`"
1.2.0,initialize graph features for each graph
1.2.0,another row of zeros is generated for padded dummy atoms
1.2.0,`count`-th step
1.2.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.2.0,generating index for graph features used in the inputs
1.2.0,"extracting graph features for parents of the target atoms, then flatten"
1.2.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.2.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.2.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.2.0,of shape: (batch_size*max_atoms) * n_graph_features
1.2.0,representing the graph features of target atoms in each graph
1.2.0,index for targe atoms
1.2.0,update the graph features for target atoms
1.2.0,last step generates graph features for all target atom
1.2.0,Add trainable weights
1.2.0,Extract atom_features
1.2.0,sum all graph outputs
1.2.0,Aliases.
1.2.0,TODO(rbharath): What does this line do?
1.2.0,TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
1.2.0,This dictionary holds a mapping {graph: learning_phase}.
1.2.0,A learning phase is a bool tensor used to run Keras models in
1.2.0,either train mode (learning_phase == 1) or test mode (learning_phase == 0).
1.2.0,else: assume learning phase is a placeholder tensor.
1.2.0,need broadcasting
1.2.0,ensure that randomness is conditioned by the Numpy RNG
1.2.0,ensure that randomness is conditioned by the Numpy RNG
1.2.0,TODO(rbharath): Should probably swap this over to tf mode.
1.2.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.2.0,"expects logits, Keras expects probabilities."
1.2.0,scale preds so that the class probas of each sample sum to 1
1.2.0,manual computation of crossentropy
1.2.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.2.0,"expects logits, Keras expects probabilities."
1.2.0,if our output includes timesteps we need to reshape
1.2.0,Arguments
1.2.0,Returns
1.2.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.2.0,"expects logits, Keras expects probabilities."
1.2.0,transform back to logits
1.2.0,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
1.2.0,a tensor. Confusing with tf.zeros...
1.2.0,Transpose for mul
1.2.0,exclude bias variables
1.2.0,"tf.scalar_summary('Weight Decay Cost', cost)"
1.2.0,TODO(user): gradient clipping (see Minimize)
1.2.0,These properties should have been set
1.2.0,"by the child class, as appropriate."
1.2.0,These properties should be set by the user via keyword arguments.
1.2.0,"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
1.2.0,are only applicable to input layers: do not pass these keywords
1.2.0,to non-input layers.
1.2.0,In this case we will create an input layer
1.2.0,to insert before the current layer
1.2.0,Update self.losses
1.2.0,In case self.losses isn't settable
1.2.0,(i.e. it's a getter method).
1.2.0,In that case the `losses` property is
1.2.0,auto-computed and shouldn't be set.
1.2.0,Update self._per_input_updates
1.2.0,Updates indexed by None are unconditional
1.2.0,rather than input-dependent
1.2.0,outputs = to_list(self.call(x))
1.2.0,return outputs
1.2.0,TODO(rbharath): Keras uses a global var here to maintain
1.2.0,unique counts. This seems dangerous. How does tensorflow handle?
1.2.0,TODO(rbharath): Support this type of functional API.
1.2.0,If batch size not specified
1.2.0,Input shape
1.2.0,Output shape
1.2.0,References
1.2.0,Not Trainable
1.2.0,Not Trainable
1.2.0,need broadcasting
1.2.0,pick the normalized form of x corresponding to the training phase
1.2.0,sample-wise normalization
1.2.0,from deepchem.nn.model_ops import variable
1.2.0,Assuming convolution kernels (2D or 3D).
1.2.0,"TF kernel shape: (..., input_depth, depth)"
1.2.0,No specific assumptions.
1.2.0,References
1.2.0,References
1.2.0,References
1.2.0,References
1.2.0,Pick the one with the correct shape.
1.2.0,Arguments
1.2.0,Aliases.
1.2.0,!/usr/bin/env python2
1.2.0,-*- coding: utf-8 -*-
1.2.0,Add trainable weights
1.2.0,Add trainable weights
1.2.0,Add trainable weights
1.2.0,Add trainable weights
1.2.0,def test_batch_normalization(self):
1.2.0,"""""""Tests that batch normalization layers can be created."""""""
1.2.0,"Output should be of shape (?, nb_filter)"
1.2.0,"Output should be of shape (batch_size, n_feat)"
1.2.0,Try concatenating the two lists of placeholders
1.2.0,Try concatenating the two lists of placeholders
1.2.0,Fit model on dataset
1.2.0,Fit model on dataset
1.2.0,"Should be an array of size (n_pocket_atoms, 3)"
1.2.0,"coords[triangle, 0] gives the x-dimension of all triangle points"
1.2.0,Take transpose to make sure rows correspond to atoms.
1.2.0,We voxelize so all grids have integral coordinates (convenience)
1.2.0,"If overlap of box with previously generated output boxes, return"
1.2.0,Carry forward mappings
1.2.0,We know that box has at least one atom not in outputs
1.2.0,Current box has been merged into box further down list.
1.2.0,No need to output current box
1.2.0,"protein_coords is (N, 3) tensor"
1.2.0,Load binding pocket model
1.2.0,TODO(rbharath): Shift refined to full once trained.
1.2.0,Fit model on dataset
1.2.0,Create featurizers
1.2.0,"if not ligand_file.endswith("".sdf""):"
1.2.0,"raise ValueError(""Only .sdf ligand files can be featurized."")"
1.2.0,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
1.2.0,ligand_mol2 = os.path.join(
1.2.0,"self.base_dir, ligand_basename + "".mol2"")"
1.2.0,
1.2.0,# Write mol2 file for ligand
1.2.0,obConversion = ob.OBConversion()
1.2.0,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
1.2.0,ob_mol = ob.OBMol()
1.2.0,"obConversion.ReadFile(ob_mol, str(ligand_file))"
1.2.0,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
1.2.0,
1.2.0,# Featurize ligand
1.2.0,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
1.2.0,if mol is None:
1.2.0,"return None, None"
1.2.0,# Default for CircularFingerprint
1.2.0,n_ligand_features = 1024
1.2.0,ligand_features = self.ligand_featurizer.featurize([mol])
1.2.0,
1.2.0,# Featurize pocket
1.2.0,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
1.2.0,"protein_file, ligand_file)"
1.2.0,n_pockets = len(pockets)
1.2.0,n_pocket_features = BindingPocketFeaturizer.n_features
1.2.0,
1.2.0,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
1.2.0,pocket_features = self.pocket_featurizer.featurize(
1.2.0,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
1.2.0,# Note broadcast operation
1.2.0,"features[:, :n_pocket_features] = pocket_features"
1.2.0,"features[:, n_pocket_features:] = ligand_features"
1.2.0,dataset = NumpyDataset(X=features)
1.2.0,pocket_preds = self.model.predict(dataset)
1.2.0,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
1.2.0,
1.2.0,# Find pockets which are active
1.2.0,active_pockets = []
1.2.0,active_pocket_atoms_map = {}
1.2.0,active_pocket_coords = []
1.2.0,for pocket_ind in range(len(pockets)):
1.2.0,#################################################### DEBUG
1.2.0,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
1.2.0,#if pocket_preds[pocket_ind] == 1:
1.2.0,if pocket_pred_proba[pocket_ind][1] > .15:
1.2.0,#################################################### DEBUG
1.2.0,pocket = pockets[pocket_ind]
1.2.0,active_pockets.append(pocket)
1.2.0,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
1.2.0,active_pocket_coords.append(pocket_coords[pocket_ind])
1.2.0,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
1.2.0,# TODO(LESWING)
1.2.0,"TODO(rbharath, enf): Figure out why pi_stack is slow and cation_pi"
1.2.0,causes segfaults.
1.2.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.2.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.2.0,TODO(rbharath): May want to move this file to S3 so we can ensure it's
1.2.0,always available.
1.2.0,Prepare receptor
1.2.0,Get protein centroid and range
1.2.0,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
1.2.0,going to be extremely slow!
1.2.0,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
1.2.0,first pocket.
1.2.0,Prepare receptor
1.2.0,TODO(rbharath): Generalize this so can support mol2 files as well.
1.2.0,Write Vina conf file
1.2.0,Define locations of log and output files
1.2.0,TODO(rbharath): Let user specify the number of poses required.
1.2.0,TODO(rbharath): Convert the output pdbqt to a pdb file.
1.2.0,Return docked files
1.2.0,Check returned files exist
1.2.0,Check returned files exist
1.2.0,Check returned files exist
1.2.0,Check returned files exist
1.2.0,Check returned files exist
1.2.0,Note this may download autodock Vina...
1.2.0,Note this may download autodock Vina...
1.2.0,Note this may download autodock Vina...
1.2.0,Check returned files exist
1.2.0,Note this may download autodock Vina...
1.2.0,Check returned files exist
1.2.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
1.2.0,box1 contained in box2
1.2.0,"box1 in box2, so complete overlap"
1.2.0,"4/5 atoms in box2 in box1, so 80 % overlap"
1.2.0,box2 contains box1
1.2.0,box1 contains box2
1.2.0,"box1 contains box2, box3"
1.2.0,Test that every atom in pocket maps exists
1.2.0,Check that the atoms is actually in protein
1.2.0,Test that every atom in pocket maps exists
1.2.0,Check that the atoms is actually in protein
1.2.0,Add active site to dict
1.2.0,The convention used is that the first task is the metric.
1.2.0,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
1.2.0,"an option in the Metric class. Instead, this should be possible to move into"
1.2.0,user-space as a custom task_averager function.
1.2.0,"TODO(rbharath, joegomes): What is this magic number?"
1.2.0,"If there are no nonzero examples, metric is ill-defined."
1.2.0,TODO(rbharath): This has been a major source of bugs. Is there a more
1.2.0,robust characterization of which metrics require class-probs and which
1.2.0,don't?
1.2.0,Reshape to handle 1-d edge cases
1.2.0,ids = df[id_field].values
1.2.0,Set missing data to have weight zero
1.2.0,TODO (ytz) this is a bandage solution to reorder the atoms so
1.2.0,that they're always in the same canonical order. Presumably this
1.2.0,should be correctly implemented in the future for graph mols.
1.2.0,Featurize task results iff they exist.
1.2.0,Filter out examples where featurization failed.
1.2.0,"For prospective data where results are unknown, it makes"
1.2.0,no sense to have y values or weights.
1.2.0,Remove support indices
1.2.0,Remove support indices
1.2.0,Remove support indices
1.2.0,Get task specific entries
1.2.0,Now just get weights for this task
1.2.0,Get task specific entries
1.2.0,Now just get weights for this task
1.2.0,Now just get weights for this task
1.2.0,Now just get weights for this task
1.2.0,Split data into pos and neg lists.
1.2.0,No replacement allowed for supports
1.2.0,Handle one-d vs. non one-d feature matrices
1.2.0,Init the iterator
1.2.0,Set initial iterator state
1.2.0,support = self.supports[task][self.trial_num]
1.2.0,Increment and update logic
1.2.0,Init the iterator
1.2.0,Set initial iterator state
1.2.0,support = self.supports[task][self.trial_num]
1.2.0,Increment and update logic
1.2.0,"By invariant of when this is called, can assume num_samples > 0"
1.2.0,and num_samples < batch_size
1.2.0,Fill in batch arrays
1.2.0,"By invariant of when this is called, can assume num_samples > 0"
1.2.0,and num_samples < batch_size
1.2.0,Fill in batch arrays
1.2.0,The -1 indicates that y will be reshaped to have length -1
1.2.0,"Set labels to be zero, with zero weights"
1.2.0,note that this corresponds to the _construct_metadata column order
1.2.0,if not len(self.metadata_df):
1.2.0,"raise ValueError(""No data in dataset."")"
1.2.0,return next(self.metadata_df.iterrows())[1]['task_names']
1.2.0,Create temp directory to store resharded version
1.2.0,Write data in new shards
1.2.0,Handle spillover from last shard
1.2.0,These columns may be missing is the dataset is unlabelled.
1.2.0,"TODO(rbharath): This happens in tests sometimes, but don't understand why?"
1.2.0,Handle edge case.
1.2.0,if data_dir is None:
1.2.0,data_dir = tempfile.mkdtemp()
1.2.0,The -1 indicates that y will be reshaped to have length -1
1.2.0,"raw_data = (X, y, w, ids)"
1.2.0,Get full dataset in memory
1.2.0,Shuffle in memory
1.2.0,Write shuffled shards out to disk
1.2.0,Shuffle the arrays corresponding to each row in metadata_df
1.2.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
1.2.0,Handle edge case with empty indices
1.2.0,Find indices which rest in this shard
1.2.0,Need to offset indices to fit within shard_size
1.2.0,Handle the case of datasets with y/w missing
1.2.0,Updating counts
1.2.0,Break when all indices have been used up already
1.2.0,TODO(rbharath): Get rid of * import
1.2.0,Load MUV dataset
1.2.0,Do an approximate comparison since splits are sometimes slightly off from
1.2.0,the exact fraction.
1.2.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
1.2.0,reloading will cause the transform to be reapplied. This is undesirable in
1.2.0,almost all cases. Need to understand a method to fix this.
1.2.0,def test_shuffle(self):
1.2.0,"""""""Test that datasets can be merged."""""""
1.2.0,current_dir = os.path.dirname(os.path.realpath(__file__))
1.2.0,dataset_file = os.path.join(
1.2.0,"current_dir, ""../../models/tests/example.csv"")"
1.2.0,featurizer = dc.feat.CircularFingerprint(size=1024)
1.2.0,"tasks = [""log-solubility""]"
1.2.0,loader = dc.data.CSVLoader(
1.2.0,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
1.2.0,"dataset = loader.featurize(dataset_file, shard_size=2)"
1.2.0,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
1.2.0,dataset.ids)
1.2.0,orig_len = len(dataset)
1.2.0,dataset.shuffle(iterations=5)
1.2.0,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
1.2.0,dataset.ids)
1.2.0,
1.2.0,assert len(dataset) == orig_len
1.2.0,# The shuffling should have switched up the ordering
1.2.0,"assert not np.array_equal(orig_ids, new_ids)"
1.2.0,# But all the same entries should still be present
1.2.0,assert sorted(orig_ids) == sorted(new_ids)
1.2.0,# All the data should have same shape
1.2.0,assert X_orig.shape == X_new.shape
1.2.0,assert y_orig.shape == y_new.shape
1.2.0,assert w_orig.shape == w_new.shape
1.2.0,The shuffling should have switched up the ordering
1.2.0,But all the same entries should still be present
1.2.0,All the data should have same shape
1.2.0,The ids should now store the performed permutation. Check that the
1.2.0,original dataset is recoverable.
1.2.0,The ids should now store the performed permutation. Check that the
1.2.0,original dataset is recoverable.
1.2.0,Set some global variables up top
1.2.0,Featurize emols dataset
1.2.0,Generate dummy dataset
1.2.0,Generate dummy dataset
1.2.0,Generate dummy dataset
1.2.0,Set last n_samples/2 weights to 0
1.2.0,Check that no support elements are sample from zero-weight samples
1.2.0,Generate dummy dataset
1.2.0,Generate dummy dataset
1.2.0,Create support generator
1.2.0,Generate dummy dataset
1.2.0,Create support generator
1.2.0,Generate dummy dataset
1.2.0,Assert all support elements have been removed
1.2.0,Generate dummy dataset
1.2.0,Assert all remove elements have been removed
1.2.0,Generate dummy dataset
1.2.0,Assert all support elements have been removed
1.2.0,Generate dummy dataset
1.2.0,Assert all remove elements have been removed
1.2.0,Generate dummy dataset
1.2.0,Set last n_samples/2 weights to 0
1.2.0,Sample from first n_samples/2 elements for support
1.2.0,Should lie within first n_samples/2 samples only
1.2.0,Generate dummy dataset
1.2.0,Create support generator
1.2.0,Generate dummy dataset
1.2.0,Test on identity matrix
1.2.0,Generate random sparse features dataset
1.2.0,Test edge case with array of all zeros
1.2.0,Test cases where n_samples < 2*n_samples < batch_size
1.2.0,Test cases where n_samples < batch_size
1.2.0,Test case where n_samples == batch_size
1.2.0,Test case for object featurization.
1.2.0,Test case for more complicated object featurization
1.2.0,Test case with multidimensional data
1.2.0,Test cases where n_samples < 2*n_samples < batch_size
1.2.0,Test cases where n_samples < batch_size
1.2.0,Test case where n_samples == batch_size
1.2.0,Test case for object featurization.
1.2.0,Test case for more complicated object featurization
1.2.0,Test case with multidimensional data
1.2.0,Test first resharding worked
1.2.0,Test second resharding worked
1.2.0,Generate data
1.2.0,Generate data
1.2.0,Generate data
1.2.0,Transform it
1.2.0,Transform it
1.2.0,Splits featurized samples into train/test
1.2.0,Splits featurized samples into train/test
1.2.0,Splits featurized samples into train/test
1.2.0,"splittype = ""random"""
1.2.0,Splits featurized samples into train/test
1.2.0,Now perform move
1.2.0,Only for debug!
1.2.0,#Make directories to store the raw and featurized datasets.
1.2.0,Load dataset
1.2.0,Featurize tox21 dataset
1.2.0,###### Do featurization
1.2.0,Do train/valid split.
1.2.0,###### Do singletask load
1.2.0,################# Do comparison
1.2.0,Only for debug!
1.2.0,Set some global variables up top
1.2.0,Make directories to store the raw and featurized datasets.
1.2.0,Load dataset
1.2.0,Featurize tox21 dataset
1.2.0,For debugging purposes
1.2.0,###### Do multitask load
1.2.0,Do train/valid split.
1.2.0,###### Do singletask load
1.2.0,################# Do comparison
1.2.0,"task_type = ""regression"""
1.2.0,coding=utf-8
1.2.0,Note that transformers have to be undone in reversed order
1.2.0,Hack to allow for easy unpickling:
1.2.0,http://stefaanlippens.net/pickleproblem
1.2.0,"One, but not both, transform_X or tranform_y is true"
1.2.0,Use fact that bools add as ints in python
1.2.0,Control for pathological case with no variance.
1.2.0,BalancingTransformer can only transform weights.
1.2.0,Compute weighting factors from dataset.
1.2.0,Ensure dataset is binary
1.2.0,Remove labels with zero weights
1.2.0,self.w = dataset.w
1.2.0,"TODO (flee2): for transform_y, figure out weights"
1.2.0,"print(""y will not be transformed by CDFTransformer, for now."")"
1.2.0,"print(""Cannot undo CDF Transformer, for now."")"
1.2.0,Need this for transform_y
1.2.0,array = np.transpose(array)
1.2.0,"print(""y will not be transformed by PowerTransformer, for now."")"
1.2.0,"print(""Cannot undo Power Transformer, for now."")"
1.2.0,the tf graph here pick up the (K+1) highest similarity values
1.2.0,and their indices
1.2.0,map the indices to labels
1.2.0,generating batch of data by slicing similarity matrix
1.2.0,into 100*reference_dataset_length
1.2.0,concatenate batches of data together
1.2.0,highest similarity is 1: target is in the reference
1.2.0,use the following K points
1.2.0,"highest less than 1: target not in the reference, use top K points"
1.2.0,calculate matrix multiplicatin on slices
1.2.0,concatenate the slices together
1.2.0,list of calculation orders for DAGs
1.2.0,stemming from one specific atom in the molecule
1.2.0,starting from the adjacency list derived by graphconv featurizer
1.2.0,"number of atoms, also number of DAGs"
1.2.0,"DAG on a molecule with k atoms includes k steps of calculation,"
1.2.0,each step calculating graph features for one atom.
1.2.0,`max_atoms` is the maximum number of steps
1.2.0,each iteration generates the DAG starting from atom with index `count`
1.2.0,"list of lists, elements represent the calculation orders"
1.2.0,for atoms in the current graph
1.2.0,starting from the target atom with index `count`
1.2.0,flags of whether the atom is already included in the DAG
1.2.0,atom `count` is in the DAG
1.2.0,recording number of radial propagation steps
1.2.0,"in the fisrt loop, atoms directly connected to `count` will be added"
1.2.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
1.2.0,will be added in the second loop(radial=1).
1.2.0,atoms i-bond away will be added in i-th loop
1.2.0,"when molecules have separate parts, starting from one part,"
1.2.0,it is not possible to include all atoms.
1.2.0,this break quit the loop when going into such condition
1.2.0,reinitialize targets for next iteration
1.2.0,atoms connected to current_atom
1.2.0,generate the dependency map of current DAG
1.2.0,atoms connected to `current_atoms`(and not included in the DAG)
1.2.0,"are added, and will be the `current_atoms` for next iteration."
1.2.0,"DAG starts from the target atom, calculation should go in reverse"
1.2.0,`edge[1]` is the parent of `edge[0]`
1.2.0,"after this loop, `parents[i]` includes all parents of atom i"
1.2.0,manually adding the atom index into its parents list
1.2.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
1.2.0,atoms with less parents(farther from the target atom) come first.
1.2.0,"graph features of atoms without parents will be first calculated,"
1.2.0,then atoms with more parents can be calculated in order
1.2.0,based on previously calculated graph features.
1.2.0,target atom of this DAG will be calculated in the last step
1.2.0,padding with `max_atoms`
1.2.0,padding
1.2.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
1.2.0,which is a max_atoms * max_atoms numpy array after padding
1.2.0,Calculate pairwise distance
1.2.0,Masking for valid atom index
1.2.0,Cutoff with threshold Rc
1.2.0,Define angle theta = arccos(R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance))
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is a y transformer
1.2.0,Check w is unchanged since this is a y transformer
1.2.0,Check y is now a logarithmic version of itself
1.2.0,Check that untransform does the right thing.
1.2.0,transforming y should raise an exception
1.2.0,transforming w should raise an exception
1.2.0,transforming X should be okay
1.2.0,Check ids are unchanged.
1.2.0,Check y is unchanged since this is a X transformer
1.2.0,Check w is unchanged since this is a y transformer
1.2.0,Check y is now a logarithmic version of itself
1.2.0,Check that untransform does the right thing.
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is a y transformer
1.2.0,Check w is unchanged since this is a y transformer
1.2.0,Check y is now a logarithmic version of itself
1.2.0,Check that untransform does the right thing.
1.2.0,Tests logarithmic data transformer with selection.
1.2.0,Check ids are unchanged.
1.2.0,Check y is unchanged since this is a X transformer
1.2.0,Check w is unchanged since this is a y transformer
1.2.0,Check y is now a logarithmic version of itself
1.2.0,Check that untransform does the right thing.
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is a y transformer
1.2.0,Check w is unchanged since this is a y transformer
1.2.0,"Check that y_t has zero mean, unit std."
1.2.0,Check that untransform does the right thing.
1.2.0,Check ids are unchanged.
1.2.0,Check y is unchanged since this is a X transformer
1.2.0,Check w is unchanged since this is a y transformer
1.2.0,"Check that X_t has zero mean, unit std."
1.2.0,np.set_printoptions(threshold='nan')
1.2.0,Entries with zero std are not normalized
1.2.0,TODO(rbharath): Untransform doesn't work properly for binary feature
1.2.0,vectors. Need to figure out what's wrong here. (low priority)
1.2.0,# Check that untransform does the right thing.
1.2.0,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
1.2.0,Check ids are unchanged.
1.2.0,Check y is unchanged since this is an X transformer
1.2.0,Check w is unchanged since this is an X transformer
1.2.0,Check X is now holding the proper values when sorted.
1.2.0,Test CDF transformer on Gaussian normal dataset.
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is an y transformer
1.2.0,Check w is unchanged since this is an y transformer
1.2.0,Check y is now holding the proper values when sorted.
1.2.0,Check that untransform does the right thing.
1.2.0,Check ids are unchanged.
1.2.0,Check y is unchanged since this is an X transformer
1.2.0,Check w is unchanged since this is an X transformer
1.2.0,Check X is now holding the proper values when sorted.
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is a y transformer
1.2.0,Check w is unchanged since this is a y transformer
1.2.0,Check y is now holding the proper values when sorted.
1.2.0,Check ids are unchanged.
1.2.0,Check y is unchanged since this is an X transformer
1.2.0,Check w is unchanged since this is an X transformer
1.2.0,Check X is now holding the proper values in each column.
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is an X transformer
1.2.0,Check w is unchanged since this is an X transformer
1.2.0,Check y is now holding the proper values in each column.
1.2.0,Check that untransform does the right thing.
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is a w transformer
1.2.0,Check y is unchanged since this is a w transformer
1.2.0,Assert that entries with zero weight retain zero weight
1.2.0,Check that sum of 0s equals sum of 1s in transformed for each task
1.2.0,Check ids are unchanged.
1.2.0,Check X is unchanged since this is a w transformer
1.2.0,Check y is unchanged since this is a w transformer
1.2.0,Assert that entries with zero weight retain zero weight
1.2.0,Check that sum of 0s equals sum of 1s in transformed for each task
1.2.0,TODO(rbharath): Use standard joblib once old-data has been regenerated.
1.2.0,"If gzipped, need to compute extension again"
1.2.0,Tasks are stored in .sdf.csv file
1.2.0,Structures are stored in .sdf file
1.2.0,First line of user-specified CSV *must* be header.
1.2.0,Try older joblib version for legacy files.
1.2.0,First line of user-specified CSV *must* be header.
1.2.0,First line of user-specified CSV *must* be header.
1.2.0,combine dataframes
1.2.0,working-with-3d-molecules
1.2.0,initial embedding
1.2.0,minimization and pruning
1.2.0,always keep lowest-energy conformer
1.2.0,discard conformers after max_conformers is reached
1.2.0,get RMSD to selected conformers
1.2.0,discard conformers within the RMSD threshold
1.2.0,create a new molecule to hold the chosen conformers
1.2.0,this ensures proper conformer IDs and energy-based ordering
1.2.0,TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
1.2.0,import nglview
1.2.0,import tempfile
1.2.0,import os
1.2.0,import mdtraj as md
1.2.0,import numpy as np
1.2.0,import tempfile
1.2.0,from rdkit import Chem
1.2.0,from rdkit.Chem import Draw
1.2.0,from itertools import islice
1.2.0,"from IPython.display import Image, HTML, display"
1.2.0,
1.2.0,"def combine_mdtraj(protein, ligand):"
1.2.0,chain = protein.topology.add_chain()
1.2.0,"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
1.2.0,for atom in ligand.topology.atoms:
1.2.0,"protein.topology.add_atom(atom.name, atom.element, residue)"
1.2.0,"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
1.2.0,protein.topology.create_standard_bonds()
1.2.0,return protein
1.2.0,
1.2.0,def visualize_complex(complex_mdtraj):
1.2.0,"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
1.2.0,"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
1.2.0,binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
1.2.0,binding_pocket_residues = [str(r) for r in binding_pocket_residues]
1.2.0,"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
1.2.0,
1.2.0,traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
1.2.0,ngltraj = nglview.NGLWidget( traj )
1.2.0,ngltraj.representations = [
1.2.0,"{ ""type"": ""cartoon"", ""params"": {"
1.2.0,"""sele"": ""protein"", ""color"": ""residueindex"""
1.2.0,"} },"
1.2.0,"{ ""type"": ""licorice"", ""params"": {"
1.2.0,"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
1.2.0,"} },"
1.2.0,"{ ""type"": ""ball+stick"", ""params"": {"
1.2.0,"""sele"": ""LIG"""
1.2.0,} }
1.2.0,]
1.2.0,return ngltraj
1.2.0,
1.2.0,def visualize_ligand(ligand_mdtraj):
1.2.0,traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
1.2.0,ngltraj = nglview.NGLWidget( traj )
1.2.0,ngltraj.representations = [
1.2.0,"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
1.2.0,return ngltraj
1.2.0,
1.2.0,def convert_lines_to_mdtraj(molecule_lines):
1.2.0,tempdir = tempfile.mkdtemp()
1.2.0,"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
1.2.0,"with open(molecule_file, ""wb"") as f:"
1.2.0,f.writelines(molecule_lines)
1.2.0,molecule_mdtraj = md.load(molecule_file)
1.2.0,return molecule_mdtraj
1.2.0,
1.2.0,def display_images(filenames):
1.2.0,"""""""Helper to pretty-print images."""""""
1.2.0,imagesList=''.join(
1.2.0,"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
1.2.0,% str(s) for s in sorted(filenames)])
1.2.0,display(HTML(imagesList))
1.2.0,
1.2.0,"def mols_to_pngs(mols, basename=""test""):"
1.2.0,"""""""Helper to write RDKit mols to png files."""""""
1.2.0,filenames = []
1.2.0,"for i, mol in enumerate(mols):"
1.2.0,"filename = ""%s%d.png"" % (basename, i)"
1.2.0,"Draw.MolToFile(mol, filename)"
1.2.0,filenames.append(filename)
1.2.0,return filenames
1.2.0,TODO(rbharath): This is now simple enough that we should probably get rid of
1.2.0,Evaluator object to avoid clutter.
1.2.0,Compute multitask metrics
1.2.0,Compute multitask metrics
1.2.0,Loosening atol to see if tests stop failing sporadically
1.2.0,!/usr/bin/env python2
1.2.0,-*- coding: utf-8 -*-
1.2.0,a*x + b*y + c*z = dI think that
1.2.0,"self.x, self.y, self.z = x, y, z"
1.2.0,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
1.2.0,TODO(bramsundar): Should this be __copy__?
1.2.0,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
1.2.0,"return np.array([self.x, self.y, self.z])"
1.2.0,TODO(rbharath): Should this be an atom function?
1.2.0,"This line is necessary for babel to work, though many PDBs in"
1.2.0,the PDB would have this line commented out
1.2.0,now atom type (for pdbqt)
1.2.0,"If atomtype is not specified, but atomname is, set atomtype to the"
1.2.0,"first letter of atomname. This heuristic suffices for proteins,"
1.2.0,since no two-letter elements appear in standard amino acids.
1.2.0,Any number needs to be removed from the element name
1.2.0,"this only uses the rightmost three characters, essentially"
1.2.0,removing unique rotamer identification
1.2.0,"The normal vector to plane is n = [a, b, c]"
1.2.0,We first shift by basepoint (a point on given plane) to make math
1.2.0,simpler. basepoint is given by d/||n||^2 * n
1.2.0,The perpendicular component of diff to plane is
1.2.0,(n^T diff / ||n||^2) * n
1.2.0,TODO(LESWING)
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,TODO(enf): make array index checking not a try-catch statement.
1.2.0,Get the degree id list (which corrects for min_deg)
1.2.0,Get the size of each degree block
1.2.0,Get the the start indices for items in each block
1.2.0,Get the node indices when they are reset when the degree changes
1.2.0,Convert to numpy array
1.2.0,Reorder old atom_features
1.2.0,Reorder old deg lists
1.2.0,Sort membership
1.2.0,Create old to new dictionary. not exactly intuitive
1.2.0,Reorder adjacency lists
1.2.0,Get numpy version of degree list for indexing
1.2.0,"Initialize adj_lists, which supports min_deg = 1 only"
1.2.0,Parse as deg separated
1.2.0,Get indices corresponding to the current degree
1.2.0,Extract and save adjacency list for the current degree
1.2.0,Construct the slice information
1.2.0,Get the cumulative indices after the first index
1.2.0,Set indices with zero sized slices to zero to avoid indexing errors
1.2.0,TODO(rbharath): Can this be removed?
1.2.0,Use random insted of zeros to prevent weird issues with summing to zero
1.2.0,Get atoms by degree
1.2.0,stack the atoms
1.2.0,Sort all atoms by degree.
1.2.0,"Get the size of each atom list separated by molecule id, then by degree"
1.2.0,Get the final size of each degree block
1.2.0,"Get the index at which each degree starts, not resetting after each degree"
1.2.0,And not stopping at any speciic molecule
1.2.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
1.2.0,first column telling the start indices of each degree block and the
1.2.0,second colum telling the size of each degree block
1.2.0,Input for tensorflow
1.2.0,Determines the membership (atom i belongs to membership[i] molecule)
1.2.0,"Get the index at which each deg starts, resetting after each degree"
1.2.0,(deg x num_mols) matrix describing the start indices when you count up the atoms
1.2.0,"in the final representation, stopping at each molecule,"
1.2.0,resetting every time the degree changes
1.2.0,Gets the degree resetting block indices for the atoms in each molecule
1.2.0,"Here, the indices reset when the molecules change, and reset when the"
1.2.0,degree changes
1.2.0,Get the degree id lookup list. It allows us to search for the degree of a
1.2.0,molecule mol_id with corresponding atom mol_atom_id using
1.2.0,"deg_id_lists[mol_id,mol_atom_id]"
1.2.0,This is used for convience in the following function (explained below)
1.2.0,Get the degree id (corrected for min_deg) of the considered atom
1.2.0,Return the final index of atom mol_atom_id in molecule mol_id.  Using
1.2.0,"the degree of this atom, must find the index in the molecule's original"
1.2.0,"degree block corresponding to degree id deg_id (second term), and then"
1.2.0,calculate which index this degree block ends up in the final
1.2.0,representation (first term). The sum of the two is the final indexn
1.2.0,Initialize the new degree separated adjacency lists
1.2.0,Update the old adjcency lists with the new atom indices and then combine
1.2.0,all together
1.2.0,Iterate through all the molecules
1.2.0,Get the adjacency lists for this molecule and current degree id
1.2.0,"Correct all atom indices to the final indices, and then save the"
1.2.0,results into the new adjacency lists
1.2.0,Increment once row is done
1.2.0,Get the final aggregated molecule
1.2.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
1.2.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
1.2.0,consistent with most QM software packages.
1.2.0,Type of data created by this featurizer
1.2.0,TODO(rbharath): Should this return a list?
1.2.0,Type of data created by this featurizer
1.2.0,generate SMILES for fragments
1.2.0,Initalize with 1
1.2.0,Allow 0 index to correspond to null molecule 1
1.2.0,Correct for null
1.2.0,"print(6-k-1, id)"
1.2.0,Correct for last one
1.2.0,first `bt_len` features are bond features(if applicable)
1.2.0,`bt_len`-th feature is if the pair of atoms are in the same ring
1.2.0,graph distance between two atoms
1.2.0,atoms `radial` bonds away from `a1`
1.2.0,atoms less than `radial` bonds away
1.2.0,find atoms `radial`+1 bonds away
1.2.0,"Since ConvMol is an object and not a numpy array, need to set dtype to"
1.2.0,object.
1.2.0,Get the node features
1.2.0,Stack nodes into an array
1.2.0,Get bond lists with reverse edges included
1.2.0,Get canonical adjacency list
1.2.0,Set dtype
1.2.0,Atom features
1.2.0,Stack nodes into an array
1.2.0,Get bond lists
1.2.0,Get canonical adjacency list
1.2.0,Calculate pair features
1.2.0,atom_name is of format RESX-ATOMTYPE
1.2.0,where X is a 1 to 4 digit number
1.2.0,list-of-available-descriptors.
1.2.0,(ytz): This is done to avoid future compatibility issues like inclusion of
1.2.0,the 3D descriptors or changing the feature size.
1.2.0,check for separate count and SMILES entries for each fragment
1.2.0,"Note there is a central nitrogen of degree 4, with 4 carbons"
1.2.0,of degree 1 (connected only to central nitrogen).
1.2.0,5 atoms in compound
1.2.0,Get the adjacency lists grouped by degree
1.2.0,The 4 outer atoms connected to central nitrogen
1.2.0,Central nitrogen connected to everything else.
1.2.0,Only one carbon
1.2.0,"No bonds, so degree adjacency lists are empty"
1.2.0,3 carbonds in alkane
1.2.0,Outer two carbonds are connected to central carbon
1.2.0,Central carbon connected to outer two
1.2.0,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
1.2.0,this expected behavior? Need to think about API.
1.2.0,Do a manual distance computation and make
1.2.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
1.2.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
1.2.0,Do a manual distance computation and ensure that selected neighbor is
1.2.0,closest since we set max_num_neighbors = 1
1.2.0,Splits featurized samples into train/test
1.2.0,Artificial feature array.
1.2.0,0 atoms of degree 0
1.2.0,0 atoms of degree 1
1.2.0,4 atoms of degree 2
1.2.0,0 atoms of degree 3
1.2.0,0 atoms of degree 4
1.2.0,0 atoms of degree 5
1.2.0,0 atoms of degree 6
1.2.0,0 atoms of degree 7
1.2.0,0 atoms of degree 8
1.2.0,0 atoms of degree 9
1.2.0,0 atoms of degree 10
1.2.0,atom 4 has 0 neighbors
1.2.0,atom 0 has 2 neighbors
1.2.0,atom 1 has 2 neighbors
1.2.0,atom 2 has 2 neighbors
1.2.0,atom 3 has 3 neighbors.
1.2.0,Verify that atom features have been sorted by atom degree.
1.2.0,Sorting is done by atom degree as before. So the ordering goes
1.2.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
1.2.0,from new position to old position is
1.2.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
1.2.0,list respects this reordering and returns correct adjacency list.
1.2.0,### First example molecule
1.2.0,Artificial feature array.
1.2.0,### Second example molecule
1.2.0,## Third example molecule
1.2.0,Test agglomerate molecule method
1.2.0,No atoms of degree 0
1.2.0,3 atoms of degree 1
1.2.0,8 atoms of degree 2
1.2.0,1 atom of degree 3
1.2.0,0 atoms of degree 4
1.2.0,0 atoms of degree 5
1.2.0,Check that atoms are only connected to themselves.
1.2.0,Check that there's one atom of each degree.
1.2.0,assumes that every array is of the same dimension
1.2.0,rem_dataset is remaining portion of dataset
1.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.2.0,to k-1.
1.2.0,returns list of per column sum of non zero elements
1.2.0,Compute number of actives needed per task.
1.2.0,loop through each column and obtain index required to splice out for
1.2.0,required fraction of hits
1.2.0,Find the first index where the cumulative number of actives equals
1.2.0,the actives_count
1.2.0,Note that np.where tells us last index required to exceed
1.2.0,"actives_count, so we actually want the following location"
1.2.0,TODO(rbharath): Refactor this split method to match API of other splits (or
1.2.0,potentially refactor those to match this.
1.2.0,Handle edge case where frac_split is 1
1.2.0,Create weight matrices fpor two haves.
1.2.0,copy over up to required index for weight first_split
1.2.0,check out if any rows in either w_1 or w_2 are just zeros
1.2.0,"Obtain original x, y, and w arrays and shuffle"
1.2.0,calculate percent split for valid (out of test and valid)
1.2.0,"split test data into valid and test, treating sub test set also as sparse"
1.2.0,rem_dataset is remaining portion of dataset
1.2.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.2.0,to k-1.
1.2.0,JSG Assert that split fractions can be written as proper fractions over 10.
1.2.0,This can be generalized in the future with some common demoninator determination.
1.2.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.2.0,Append remaining examples to train
1.2.0,Sort by increasing MW
1.2.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.2.0,for m_idx in cluster:
1.2.0,"continue until we find an active in all the tasks, otherwise we can't"
1.2.0,compute a meaningful AUC
1.2.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.2.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.2.0,Sort from largest to smallest scaffold sets
1.2.0,Pick the mol closest to everything as the first element of training
1.2.0,Pick the closest mol from what is left
1.2.0,Test is everything else
1.2.0,All datasets share features and identifiers by assumption.
1.2.0,TODO(rbharath): Get rid of * import
1.2.0,Note that the extra task goes to test
1.2.0,Number tasks per fold
1.2.0,Find the tasks that correspond to this test fold
1.2.0,Assert that all arrays look like they should
1.2.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
1.2.0,data. Make a test for properly splitting of sharded data. Perhaps using
1.2.0,reshard() to handle this?
1.2.0,Verify lengths is 10/k == 2
1.2.0,Verify that compounds in this fold are subset of original compounds
1.2.0,Verify that no two folds have overlapping compounds.
1.2.0,Verify lengths is 10/k == 2
1.2.0,Verify that compounds in this fold are subset of original compounds
1.2.0,Verify that no two folds have overlapping compounds.
1.2.0,Verify lengths is 10/k == 2
1.2.0,Verify that compounds in this fold are subset of original compounds
1.2.0,Verify that no two folds have overlapping compounds.
1.2.0,Test singletask case.
1.2.0,The split index should partition dataset in half.
1.2.0,Test singletask case.
1.2.0,Test case where some weights are zero (i.e. masked)
1.2.0,Set half the positives to have zero weight
1.2.0,There are 10 nonzero actives.
1.2.0,"The split index should partition this into half, so expect 5"
1.2.0,The split index should partition dataset in half.
1.2.0,Mask half the examples
1.2.0,The split index should partition dataset in half.
1.2.0,Test singletask case.
1.2.0,Should have split cleanly in half (picked random seed to ensure this)
1.2.0,Check positives are correctly distributed
1.2.0,Verify lengths is 100/k == 20
1.2.0,Note: This wouldn't work for multitask str
1.2.0,assert len(fold_dataset) == n_samples/K
1.2.0,Verify that each fold has n_positives/K = 4 positive examples.
1.2.0,Verify that compounds in this fold are subset of original compounds
1.2.0,Verify that no two folds have overlapping compounds.
1.2.0,sparsity is determined by number of w weights that are 0 for a given
1.2.0,task structure of w np array is such that each row corresponds to a
1.2.0,sample. The loaded sparse dataset has many rows with only zeros
1.2.0,verify that there are no rows (samples) in weights matrix w
1.2.0,that have no hits.
1.2.0,Path to save checkpoint files
1.2.0,first layer in model: check that it is an input layer
1.2.0,Add losses to graph
1.2.0,Loss for each batch element
1.2.0,Loss should be a float
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Save an initial checkpoint.
1.2.0,TODO(rbharath): Don't support example weighting yet.
1.2.0,Always save a final checkpoint when complete.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Arguments
1.2.0,Returns
1.2.0,Arguments
1.2.0,Returns
1.2.0,Arguments
1.2.0,Returns
1.2.0,Arguments
1.2.0,Returns
1.2.0,task_metadata_rows = {task: [] for task in tasks}
1.2.0,Extract those datapoints which are present for this task
1.2.0,Loading is done on-the-fly
1.2.0,TODO(rbharath/enf): We need a structured way to deal with potential GPU
1.2.0,memory overflows.
1.2.0,Discard any padded predictions
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,Special case to handle singletasks.
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,!/usr/bin/env python2
1.2.0,-*- coding: utf-8 -*-
1.2.0,Calculate pairwise distance
1.2.0,Masking for valid atom index
1.2.0,Cutoff with threshold Rc
1.2.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
1.2.0,Define angle theta = R_ij(Vector) dot R_ik(Vector)/R_ij(distance)/R_ik(distance)
1.2.0,Reshape everything to match the input with the most dimensions.
1.2.0,"This probably means the variable hasn't been created yet, so try again"
1.2.0,with reuse set to false.
1.2.0,"Shape (N_atoms, M_nbrs, ndim)"
1.2.0,"Shape (N_atoms, M_nbrs, ndim)"
1.2.0,"Shape (N_atoms, M_nbrs)"
1.2.0,TODO(rbharath): Note sure if this layer can be called with __call__
1.2.0,"meaningfully, so not going to support that functionality for now."
1.2.0,"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.2.0,Generate the nb_affine weights and biases
1.2.0,Extract atom_features
1.2.0,Extract graph topology
1.2.0,Perform the mol conv
1.2.0,"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
1.2.0,"self.max_deg, self.min_deg, self.W_list,"
1.2.0,self.b_list)
1.2.0,Sum all neighbors using adjacency matrix
1.2.0,Get collection of modified atom features
1.2.0,Obtain relevant atoms for this degree
1.2.0,Get self atoms
1.2.0,Apply hidden affine to relevant atoms and append
1.2.0,Determine the min_deg=0 case
1.2.0,Only use the self layer
1.2.0,Combine all atoms back into the list
1.2.0,Tensorflow correctly processes empty lists when using concat
1.2.0,"Sum along neighbors as well as self, and store"
1.2.0,Perform the mol gather
1.2.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
1.2.0,"self.max_degree, self.min_degree)"
1.2.0,Tensorflow correctly processes empty lists when using concat
1.2.0,Get self atoms
1.2.0,Expand dims
1.2.0,always deg-1 for deg_adj_lists
1.2.0,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.2.0,Extract graph topology
1.2.0,Perform the mol gather
1.2.0,Obtain the partitions for each of the molecules
1.2.0,Sum over atoms for each molecule
1.2.0,Get the final sparse representations
1.2.0,Number of rotatable bonds
1.2.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
1.2.0,a difference.
1.2.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
1.2.0,neighbors lists an argument instead of a part of this layer.
1.2.0,"Shape (N, M)"
1.2.0,"Shape (N, M)"
1.2.0,"Shape (N, M)"
1.2.0,Number of grid cells
1.2.0,TODO(rbharath): Support batching
1.2.0,"Shape (n_cells, ndim)"
1.2.0,"List of length N_atoms, each element of different length uniques_i"
1.2.0,"List of length N_atoms, each element of different length uniques_i"
1.2.0,"List of length N_atoms, each a tensor of shape"
1.2.0,"(uniques_i, ndim)"
1.2.0,Add phantom atoms that exist far outside the box
1.2.0,"List of length N_atoms, each of shape (1, ndim)"
1.2.0,TODO(rbharath): How does distance need to be modified here to
1.2.0,account for periodic boundary conditions?
1.2.0,List of length N_atoms each of shape (M_nbrs)
1.2.0,"N_atoms elts of size (M_nbrs,) each"
1.2.0,"Shape (N_atoms, 1)"
1.2.0,Find M_nbrs atoms closest to each cell
1.2.0,"Shape (n_cells, M_nbrs)"
1.2.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.2.0,"conditions, so does wrapround. O(constant)"
1.2.0,"Shape (n_cells, n_nbr_cells)"
1.2.0,"Shape (N_atoms, n_nbr_cells)"
1.2.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
1.2.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
1.2.0,"List of length N_atoms, each element length uniques_i"
1.2.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
1.2.0,element removed to remove self from list of neighbors. Need to verify
1.2.0,this holds more broadly or come up with robust alternative.
1.2.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.2.0,"Shape (N_atoms*n_cells, ndim) after tile"
1.2.0,Shape (N_atoms*n_cells)
1.2.0,"Shape (n_cells, N_atoms)"
1.2.0,Find k atoms closest to this cell. Notice negative sign since
1.2.0,tf.nn.top_k returns *largest* not smallest.
1.2.0,"Tensor of shape (n_cells, M_nbrs)"
1.2.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.2.0,"Shape (N_atoms*n_cells, 1) after tile"
1.2.0,9 neighbors in 2-space
1.2.0,TODO(rbharath): Shoddy handling of higher dimensions...
1.2.0,Number of cells for cube in 3-space is
1.2.0,TODO(rbharath): Do we need to handle periodic boundary conditions
1.2.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.2.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
1.2.0,the cube.
1.2.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.2.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.2.0,"Tile (a, a, a, b, b, b, etc.)"
1.2.0,"Tile (a, b, c, a, b, c, ...)"
1.2.0,N: Maximum number of atoms
1.2.0,M: Maximum number of neighbors
1.2.0,d: Number of coordinates/features/filters
1.2.0,B: Batch Size
1.2.0,We apply the radial pooling filter before atom type conv
1.2.0,to reduce computation
1.2.0,!/usr/bin/env python2
1.2.0,-*- coding: utf-8 -*-
1.2.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.2.0,and embeddings of atom j(both gone through a hidden layer)
1.2.0,"for atom i, sum the influence from all other atom j in the molecule"
1.2.0,number of inputs each step
1.2.0,Add trainable weights
1.2.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.2.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.2.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.2.0,initialize graph features for each graph
1.2.0,initialize graph features for each graph
1.2.0,another row of zeros is generated for padded dummy atoms
1.2.0,`count`-th step
1.2.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.2.0,generating index for graph features used in the inputs
1.2.0,"extracting graph features for parents of the target atoms, then flatten"
1.2.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.2.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.2.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.2.0,of shape: (batch_size*max_atoms) * n_graph_features
1.2.0,representing the graph features of target atoms in each graph
1.2.0,index for targe atoms
1.2.0,update the graph features for target atoms
1.2.0,Add trainable weights
1.2.0,Extract atom_features
1.2.0,Extract atom_features
1.2.0,sum all graph outputs
1.2.0,Layer Management
1.2.0,Singular place to hold Tensor objects which don't serialize
1.2.0,These have to be reconstructed on restoring from pickle
1.2.0,See TensorGraph._get_tf() for more details on lazy construction
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Arguments
1.2.0,Returns
1.2.0,Arguments
1.2.0,Returns
1.2.0,Remove out_tensor from the object to be pickled
1.2.0,Pickle itself
1.2.0,add out_tensor back to everyone
1.2.0,"This isn't a meaningful loss, but just for test"
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.2.0,"Now an (N, M) shape"
1.2.0,TODO(rbharath): Move this into a model directly
1.2.0,def test_vina(self):
1.2.0,"""""""Test that vina graph can be constructed in TensorGraph."""""""
1.2.0,N_protein = 4
1.2.0,N_ligand = 1
1.2.0,N_atoms = 5
1.2.0,M_nbrs = 2
1.2.0,ndim = 3
1.2.0,start = 0
1.2.0,stop = 4
1.2.0,nbr_cutoff = 1
1.2.0,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
1.2.0,start))
1.2.0,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
1.2.0,start))
1.2.0,y = NumpyDataset(np.random.rand(
1.2.0,"1,))"
1.2.0,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
1.2.0,"# used in the current implementation. This is obviously wrong, but need"
1.2.0,# to dig out why this is happening.
1.2.0,"prot_coords = Feature(shape=(N_protein, ndim))"
1.2.0,"ligand_coords = Feature(shape=(N_ligand, ndim))"
1.2.0,"labels = Label(shape=(1,))"
1.2.0,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
1.2.0,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
1.2.0,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
1.2.0,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
1.2.0,"# Now an (N, M) shape"
1.2.0,nbr_list = NeighborList(
1.2.0,"N_protein + N_ligand,"
1.2.0,"M_nbrs,"
1.2.0,"ndim,"
1.2.0,"nbr_cutoff,"
1.2.0,"start,"
1.2.0,"stop,"
1.2.0,in_layers=[coords])
1.2.0,"# Shape (N, M)"
1.2.0,dists = InteratomicL2Distances(
1.2.0,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
1.2.0,repulsion = VinaRepulsion(in_layers=[dists])
1.2.0,hydrophobic = VinaHydrophobic(in_layers=[dists])
1.2.0,hbond = VinaHydrogenBond(in_layers=[dists])
1.2.0,gauss_1 = VinaGaussianFirst(in_layers=[dists])
1.2.0,gauss_2 = VinaGaussianSecond(in_layers=[dists])
1.2.0,"# Shape (N, M)"
1.2.0,interactions = WeightedLinearCombo(
1.2.0,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
1.2.0,"# Shape (N, M)"
1.2.0,"thresholded = Cutoff(in_layers=[dists, interactions])"
1.2.0,"# Shape (N, M)"
1.2.0,free_energies = VinaNonlinearity(in_layers=[thresholded])
1.2.0,free_energy = ReduceSum(in_layers=[free_energies])
1.2.0,"loss = L2Loss(in_layers=[free_energy, labels])"
1.2.0,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
1.2.0,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
1.2.0,tg.set_loss(loss)
1.2.0,tg.fit_generator(databag.iterbatches(epochs=1))
1.2.0,TODO(rbharath): This test should pass. Fix it!
1.2.0,def test_graph_pool(self):
1.2.0,"""""""Test that GraphPool can be invoked."""""""
1.2.0,out_channels = 2
1.2.0,"n_atoms = 4 # In CCC and C, there are 4 atoms"
1.2.0,"raw_smiles = ['CCC', 'C']"
1.2.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
1.2.0,featurizer = ConvMolFeaturizer()
1.2.0,mols = featurizer.featurize(mols)
1.2.0,multi_mol = ConvMol.agglomerate_mols(mols)
1.2.0,atom_features = multi_mol.get_atom_features()
1.2.0,degree_slice = multi_mol.deg_slice
1.2.0,membership = multi_mol.membership
1.2.0,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
1.2.0,with self.test_session() as sess:
1.2.0,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
1.2.0,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
1.2.0,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
1.2.0,deg_adjs_tf = []
1.2.0,for deg_adj in deg_adjs:
1.2.0,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
1.2.0,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
1.2.0,out_tensor = GraphPool(out_channels)(*args)
1.2.0,sess.run(tf.global_variables_initializer())
1.2.0,out_tensor = out_tensor.eval()
1.2.0,"assert out_tensor.shape == (n_atoms, out_channels)"
1.2.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
1.2.0,number of atoms in each molecule
1.2.0,index of pair features
1.2.0,number of pairs for each atom
1.2.0,atom features
1.2.0,pair features
1.2.0,calculation orders for a batch of molecules
1.2.0,padding atom features vector of each molecule with 0
1.2.0,import tensorflow as tf
1.2.0,from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
1.2.0,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
1.2.0,
1.2.0,
1.2.0,class WeightedError(Layer):
1.2.0,
1.2.0,"def __call__(self, *parents):"
1.2.0,"entropy, weights = parents[0], parents[1]"
1.2.0,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
1.2.0,return self.out_tensor
1.2.0,
1.2.0,
1.2.0,"def tensorGraphMultitaskClassifier(n_tasks,"
1.2.0,"n_features,"
1.2.0,"layer_sizes=[500],"
1.2.0,"bypass_layer_sizes=[100],"
1.2.0,model_dir=None):
1.2.0,""""""""
1.2.0,TODO(LESWING) Add Dropout and regularization
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,n_tasks
1.2.0,n_features
1.2.0,layer_sizes
1.2.0,bypass_layer_sizes
1.2.0,model_dir
1.2.0,
1.2.0,Returns
1.2.0,-------
1.2.0,
1.2.0,""""""""
1.2.0,g = MultiTaskTensorGraph(model_dir=model_dir)
1.2.0,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
1.2.0,g.add_layer(in_layer)
1.2.0,g.add_feature(in_layer)
1.2.0,
1.2.0,# Shared Dense Layers
1.2.0,prev_layer = in_layer
1.2.0,dense_layers = []
1.2.0,for i in range(len(layer_sizes)):
1.2.0,dense = Dense(
1.2.0,"out_channels=layer_sizes[i],"
1.2.0,"name=""SDENSE%s"" % i,"
1.2.0,activation_fn=tf.nn.relu)
1.2.0,"g.add_layer(dense, parents=[prev_layer])"
1.2.0,dense_layers.append(dense)
1.2.0,prev_layer = dense
1.2.0,
1.2.0,# Individual Bypass Layers
1.2.0,costs = []
1.2.0,for task in range(n_tasks):
1.2.0,prev_layer = in_layer
1.2.0,for i in range(len(bypass_layer_sizes)):
1.2.0,dense = Dense(
1.2.0,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
1.2.0,"g.add_layer(dense, parents=[prev_layer])"
1.2.0,prev_layer = dense
1.2.0,"joined_layer = Concat(name=""JOIN%s"" % task)"
1.2.0,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
1.2.0,
1.2.0,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
1.2.0,"g.add_layer(classification, parents=[joined_layer])"
1.2.0,
1.2.0,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
1.2.0,"g.add_layer(softmax, parents=[classification])"
1.2.0,g.add_output(softmax)
1.2.0,
1.2.0,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
1.2.0,g.add_layer(label)
1.2.0,g.add_label(label)
1.2.0,
1.2.0,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
1.2.0,"g.add_layer(cost, parents=[label, classification])"
1.2.0,costs.append(cost)
1.2.0,
1.2.0,"entropy = Concat(name=""ENT"")"
1.2.0,"g.add_layer(entropy, parents=costs)"
1.2.0,
1.2.0,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
1.2.0,g.add_layer(task_weights)
1.2.0,g.set_task_weights(task_weights)
1.2.0,
1.2.0,"loss = WeightedError(name=""ERROR"")"
1.2.0,"g.add_layer(loss, parents=[entropy, task_weights])"
1.2.0,g.set_loss(loss)
1.2.0,
1.2.0,return g
1.2.0,!/usr/bin/env python2
1.2.0,-*- coding: utf-8 -*-
1.2.0,update model with best param
1.2.0,Find optimal n_estimators based on original learning_rate
1.2.0,and early_stopping_rounds
1.2.0,"Since test size is 20%, when retrain model to whole data, expect"
1.2.0,n_estimator increased to 1/0.8 = 1.25 time.
1.2.0,Make sure user specified params are in the grid.
1.2.0,Change params back original params
1.2.0,TODO (LESWING) Lazy Load
1.2.0,TODO (LESWING) Lazy Load
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Check same predictions are made.
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Load trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Load trained model
1.2.0,Eval model on train
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Fit trained model
1.2.0,Eval model on train/test
1.2.0,Fit trained model
1.2.0,Eval model on train/test
1.2.0,Fit trained model
1.2.0,Eval model on train/test
1.2.0,Test Parameter getting and setting
1.2.0,Fit trained model
1.2.0,Eval model on train/test
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,n_samples = 100
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,n_samples = 100
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Gather Projection
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,Load mini log-solubility dataset.
1.2.0,Add layers
1.2.0,"output will be (n_atoms, 64)"
1.2.0,Need to add batch-norm separately to test/support due to differing
1.2.0,shapes.
1.2.0,"output will be (n_atoms, 64)"
1.2.0,"output will be (n_atoms, 64)"
1.2.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.2.0,n_pos/n_neg accordingly.
1.2.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.2.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.2.0,can measure model has memorized support).  Replacement is turned off to
1.2.0,ensure that support contains full training set. This checks that the
1.2.0,model has mastered memorization of provided support.
1.2.0,#################################################### DEBUG
1.2.0,TODO(rbharath): Check if something went wrong here...
1.2.0,Measure performance on 0-th task.
1.2.0,assert scores[0] > .9
1.2.0,#################################################### DEBUG
1.2.0,Load mini log-solubility dataset.
1.2.0,Add layers
1.2.0,"output will be (n_atoms, 64)"
1.2.0,Need to add batch-norm separately to test/support due to differing
1.2.0,shapes.
1.2.0,"output will be (n_atoms, 64)"
1.2.0,"output will be (n_atoms, 64)"
1.2.0,Apply an attention lstm layer
1.2.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.2.0,n_pos/n_neg accordingly.
1.2.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.2.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.2.0,can measure model has memorized support).  Replacement is turned off to
1.2.0,ensure that support contains full training set. This checks that the
1.2.0,model has mastered memorization of provided support.
1.2.0,Measure performance on 0-th task.
1.2.0,#################################################### DEBUG
1.2.0,TODO(rbharath): Check if something went wrong here...
1.2.0,Measure performance on 0-th task.
1.2.0,assert scores[0] > .85
1.2.0,#################################################### DEBUG
1.2.0,Load mini log-solubility dataset.
1.2.0,Add layers
1.2.0,"output will be (n_atoms, 64)"
1.2.0,Need to add batch-norm separately to test/support due to differing
1.2.0,shapes.
1.2.0,"output will be (n_atoms, 64)"
1.2.0,"output will be (n_atoms, 64)"
1.2.0,Apply a residual lstm layer
1.2.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.2.0,n_pos/n_neg accordingly.
1.2.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.2.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.2.0,can measure model has memorized support).  Replacement is turned off to
1.2.0,ensure that support contains full training set. This checks that the
1.2.0,model has mastered memorization of provided support.
1.2.0,Measure performance on 0-th task.
1.2.0,#################################################### DEBUG
1.2.0,TODO(rbharath): Check if something went wrong here...
1.2.0,Measure performance on 0-th task.
1.2.0,assert scores[0] > .9
1.2.0,#################################################### DEBUG
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on train
1.2.0,def test_singletask_to_multitask_classification(self):
1.2.0,n_features = 10
1.2.0,n_tasks = 17
1.2.0,tasks = range(n_tasks)
1.2.0,# Define train dataset
1.2.0,n_train = 100
1.2.0,"X_train = np.random.rand(n_train, n_features)"
1.2.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
1.2.0,w_train = np.ones_like(y_train)
1.2.0,"ids_train = [""C""] * n_train"
1.2.0,train_dataset = dc.data.DiskDataset.from_numpy(
1.2.0,"X_train, y_train, w_train, ids_train)"
1.2.0,# Define test dataset
1.2.0,n_test = 10
1.2.0,"X_test = np.random.rand(n_test, n_features)"
1.2.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
1.2.0,w_test = np.ones_like(y_test)
1.2.0,"ids_test = [""C""] * n_test"
1.2.0,test_dataset = dc.data.DiskDataset.from_numpy(
1.2.0,"X_test, y_test, w_test, ids_test)"
1.2.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
1.2.0,def model_builder(model_dir):
1.2.0,sklearn_model = LogisticRegression()
1.2.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.2.0,multitask_model = dc.models.SingletaskToMultitask(
1.2.0,"tasks, model_builder)"
1.2.0,# Fit trained model
1.2.0,multitask_model.fit(train_dataset)
1.2.0,multitask_model.save()
1.2.0,# Eval multitask_model on train/test
1.2.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
1.2.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
1.2.0,Generate data
1.2.0,Cleanup
1.2.0,Generate dummy dataset
1.2.0,Fit trained model
1.2.0,Eval model on test
1.2.0,Eval model on train
1.2.0,Fit trained model
1.2.0,Eval model on test
1.2.0,Fit trained model
1.2.0,Eval model on test
1.2.0,def test_sklearn_classification(self):
1.2.0,"""""""Test that sklearn models can learn on simple classification datasets."""""""
1.2.0,np.random.seed(123)
1.2.0,dataset = sklearn.datasets.load_digits(n_class=2)
1.2.0,"X, y = dataset.data, dataset.target"
1.2.0,frac_train = .7
1.2.0,n_samples = len(X)
1.2.0,n_train = int(frac_train*n_samples)
1.2.0,"X_train, y_train = X[:n_train], y[:n_train]"
1.2.0,"X_test, y_test = X[n_train:], y[n_train:]"
1.2.0,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
1.2.0,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
1.2.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.2.0,sklearn_model = LogisticRegression()
1.2.0,model = dc.models.SklearnModel(sklearn_model)
1.2.0,# Fit trained model
1.2.0,model.fit(train_dataset)
1.2.0,model.save()
1.2.0,# Eval model on test
1.2.0,"scores = model.evaluate(test_dataset, [classification_metric])"
1.2.0,assert scores[classification_metric.name] > .5
1.2.0,def test_sklearn_multitask_classification(self):
1.2.0,"""""""Test that sklearn models can learn on simple multitask classification."""""""
1.2.0,np.random.seed(123)
1.2.0,n_tasks = 4
1.2.0,tasks = range(n_tasks)
1.2.0,dataset = sklearn.datasets.load_digits(n_class=2)
1.2.0,"X, y = dataset.data, dataset.target"
1.2.0,"y = np.reshape(y, (len(y), 1))"
1.2.0,y = np.hstack([y] * n_tasks)
1.2.0,
1.2.0,frac_train = .7
1.2.0,n_samples = len(X)
1.2.0,n_train = int(frac_train*n_samples)
1.2.0,"X_train, y_train = X[:n_train], y[:n_train]"
1.2.0,"X_test, y_test = X[n_train:], y[n_train:]"
1.2.0,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
1.2.0,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
1.2.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.2.0,def model_builder(model_dir):
1.2.0,sklearn_model = LogisticRegression()
1.2.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.2.0,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
1.2.0,# Fit trained model
1.2.0,model.fit(train_dataset)
1.2.0,model.save()
1.2.0,# Eval model on test
1.2.0,"scores = model.evaluate(test_dataset, [classification_metric])"
1.2.0,for score in scores[classification_metric.name]:
1.2.0,assert score > .5
1.2.0,Set early stopping round = n_estimators so that esr won't work
1.2.0,Fit trained model
1.2.0,Eval model on test
1.2.0,Fit trained model
1.2.0,Eval model on test
1.2.0,Fit trained model
1.2.0,Eval model on test
1.2.0,Logistic regression doesn't support weights
1.2.0,Consistency check
1.2.0,Handle output layer
1.2.0,Iterate over all previous tasks.
1.2.0,prev_layers is a list with elements of size
1.2.0,"(batch_size, layer_sizes[i-1])"
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Save an initial checkpoint.
1.2.0,Turns out there are valid cases where we don't want pad-batches
1.2.0,on by default.
1.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.2.0,Run training op.
1.2.0,Always save a final checkpoint when complete.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Note that we divide by the batch size and not the number of
1.2.0,"non-zero weight examples in the batch.  Also, instead of using"
1.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.2.0,calculate with div/sum so it stays on the GPU.
1.2.0,aggregated costs
1.2.0,weight decay
1.2.0,Dummy placeholders
1.2.0,Dummy placeholders
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,Handle edge case when batch-size is 1.
1.2.0,Prune away any padding that was added
1.2.0,allow_soft_placement=True allows ops without a GPU implementation
1.2.0,to run on the CPU instead.
1.2.0,"""""""Ops for graph construction."""""""
1.2.0,from __future__ import print_function
1.2.0,from __future__ import division
1.2.0,from __future__ import unicode_literals
1.2.0,
1.2.0,import sys
1.2.0,import traceback
1.2.0,import tensorflow as tf
1.2.0,from keras import backend as K
1.2.0,
1.2.0,"def cosine_distances(test, support):"
1.2.0,"""""""Computes pairwise cosine distances between provided tensors"
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,test: tf.Tensor
1.2.0,"Of shape (n_test, n_feat)"
1.2.0,support: tf.Tensor
1.2.0,"Of shape (n_support, n_feat)"
1.2.0,
1.2.0,Returns
1.2.0,-------
1.2.0,tf.Tensor:
1.2.0,"Of shape (n_test, n_support)"
1.2.0,""""""""
1.2.0,"rnorm_test = tf.rsqrt(tf.reduce_sum(tf.square(test), 1,"
1.2.0,keep_dims=True)) + K.epsilon()
1.2.0,"rnorm_support = tf.rsqrt(tf.reduce_sum(tf.square(support), 1,"
1.2.0,keep_dims=True)) + K.epsilon()
1.2.0,test_normalized = test * rnorm_test
1.2.0,support_normalized = support * rnorm_support
1.2.0,
1.2.0,# Transpose for mul
1.2.0,"support_normalized_t = tf.transpose(support_normalized, perm=[1,0])"
1.2.0,"g = tf.matmul(test_normalized, support_normalized_t)  # Gram matrix"
1.2.0,return g
1.2.0,
1.2.0,"def euclidean_distance(test, support, max_dist_sq=20):"
1.2.0,"""""""Computes pairwise euclidean distances between provided tensors"
1.2.0,
1.2.0,TODO(rbharath): BROKEN! THIS DOESN'T WORK!
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,test: tf.Tensor
1.2.0,"Of shape (n_test, n_feat)"
1.2.0,support: tf.Tensor
1.2.0,"Of shape (n_support, n_feat)"
1.2.0,"max_dist_sq: float, optional"
1.2.0,Maximum pairwise distance allowed.
1.2.0,
1.2.0,Returns
1.2.0,-------
1.2.0,tf.Tensor:
1.2.0,"Of shape (n_test, n_support)"
1.2.0,""""""""
1.2.0,"test = tf.expand_dims(test, 1)"
1.2.0,"support = tf.expand_dims(support, 0)"
1.2.0,"g = -tf.maximum(tf.reduce_sum(tf.square(test - support), 2), max_dist_sq)"
1.2.0,return g
1.2.0,
1.2.0,"def add_bias(tensor, init=None, name=None):"
1.2.0,"""""""Add a bias term to a tensor."
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,tensor: tf.Tensor
1.2.0,Variable tensor.
1.2.0,init: float
1.2.0,Bias initializer. Defaults to zero.
1.2.0,name: str
1.2.0,Name for this op. Defaults to tensor.op.name.
1.2.0,
1.2.0,Returns
1.2.0,-------
1.2.0,tf.Tensor
1.2.0,A biased tensor with the same shape as the input tensor.
1.2.0,""""""""
1.2.0,if init is None:
1.2.0,init = tf.zeros([tensor.get_shape()[-1].value])
1.2.0,"with tf.name_scope(name, tensor.op.name, [tensor]):"
1.2.0,"b = tf.Variable(init, name='b')"
1.2.0,"return tf.nn.bias_add(tensor, b)"
1.2.0,
1.2.0,
1.2.0,"def dropout(tensor, dropout_prob, training=True, training_only=True):"
1.2.0,"""""""Random dropout."
1.2.0,
1.2.0,"This implementation supports ""always-on"" dropout (training_only=False), which"
1.2.0,"can be used to calculate model uncertainty. See Gal and Ghahramani,"
1.2.0,http://arxiv.org/abs/1506.02142.
1.2.0,
1.2.0,"NOTE(user): To simplify the implementation, I have chosen not to reverse"
1.2.0,the scaling that occurs in tf.nn.dropout when using dropout during
1.2.0,inference. This shouldn't be an issue since the activations will be scaled
1.2.0,by the same constant in both training and inference. This means that there
1.2.0,are no training-time differences between networks that use dropout during
1.2.0,inference and those that do not.
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,tensor: tf.Tensor
1.2.0,Input tensor.
1.2.0,dropout_prob: float
1.2.0,Float giving dropout probability for weights (NOT keep probability).
1.2.0,training_only: bool
1.2.0,"Boolean. If True (standard dropout), apply dropout only"
1.2.0,"during training. If False, apply dropout during inference as well."
1.2.0,
1.2.0,Returns
1.2.0,-------
1.2.0,tf.Tensor:
1.2.0,A tensor with the same shape as the input tensor.
1.2.0,""""""""
1.2.0,if not dropout_prob:
1.2.0,return tensor  # do nothing
1.2.0,keep_prob = 1.0 - dropout_prob
1.2.0,if training or not training_only:
1.2.0,"tensor = tf.nn.dropout(tensor, keep_prob)"
1.2.0,return tensor
1.2.0,
1.2.0,
1.2.0,"def fully_connected_layer(tensor, size=None, weight_init=None, bias_init=None,"
1.2.0,name=None):
1.2.0,"""""""Fully connected layer."
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,tensor: tf.Tensor
1.2.0,Input tensor.
1.2.0,size: int
1.2.0,Number of output nodes for this layer.
1.2.0,weight_init: float
1.2.0,Weight initializer.
1.2.0,bias_init: float
1.2.0,Bias initializer.
1.2.0,name: str
1.2.0,Name for this op. Defaults to 'fully_connected'.
1.2.0,
1.2.0,Returns
1.2.0,-------
1.2.0,tf.Tensor:
1.2.0,A new tensor representing the output of the fully connected layer.
1.2.0,
1.2.0,Raises
1.2.0,------
1.2.0,ValueError
1.2.0,If input tensor is not 2D.
1.2.0,""""""""
1.2.0,if len(tensor.get_shape()) != 2:
1.2.0,"raise ValueError('Dense layer input must be 2D, not %dD'"
1.2.0,% len(tensor.get_shape()))
1.2.0,if weight_init is None:
1.2.0,num_features = tensor.get_shape()[-1].value
1.2.0,"weight_init = tf.truncated_normal([num_features, size], stddev=0.01)"
1.2.0,if bias_init is None:
1.2.0,bias_init = tf.zeros([size])
1.2.0,
1.2.0,"with tf.name_scope(name, 'fully_connected', [tensor]):"
1.2.0,"w = tf.Variable(weight_init, name='w', dtype=tf.float32)"
1.2.0,"b = tf.Variable(bias_init, name='b', dtype=tf.float32)"
1.2.0,"return tf.nn.xw_plus_b(tensor, w, b)"
1.2.0,
1.2.0,"def weight_decay(penalty_type, penalty):"
1.2.0,"""""""Add weight decay."
1.2.0,
1.2.0,Args:
1.2.0,model: TensorflowGraph.
1.2.0,
1.2.0,Returns:
1.2.0,A scalar tensor containing the weight decay cost.
1.2.0,
1.2.0,Raises:
1.2.0,NotImplementedError: If an unsupported penalty type is requested.
1.2.0,""""""""
1.2.0,variables = []
1.2.0,# exclude bias variables
1.2.0,for v in tf.trainable_variables():
1.2.0,if v.get_shape().ndims == 2:
1.2.0,variables.append(v)
1.2.0,
1.2.0,with tf.name_scope('weight_decay'):
1.2.0,if penalty_type == 'l1':
1.2.0,cost = tf.add_n([tf.reduce_sum(tf.abs(v)) for v in variables])
1.2.0,elif penalty_type == 'l2':
1.2.0,cost = tf.add_n([tf.nn.l2_loss(v) for v in variables])
1.2.0,else:
1.2.0,raise NotImplementedError('Unsupported penalty_type %s' % penalty_type)
1.2.0,cost *= penalty
1.2.0,"tf.scalar_summary('Weight Decay Cost', cost)"
1.2.0,return cost
1.2.0,
1.2.0,
1.2.0,"def multitask_logits(features, num_tasks, num_classes=2, weight_init=None,"
1.2.0,"bias_init=None, dropout_prob=None, name=None):"
1.2.0,"""""""Create a logit tensor for each classification task."
1.2.0,
1.2.0,Args:
1.2.0,features: A 2D tensor with dimensions batch_size x num_features.
1.2.0,num_tasks: Number of classification tasks.
1.2.0,num_classes: Number of classes for each task.
1.2.0,weight_init: Weight initializer.
1.2.0,bias_init: Bias initializer.
1.2.0,dropout_prob: Float giving dropout probability for weights (NOT keep
1.2.0,probability).
1.2.0,name: Name for this op. Defaults to 'multitask_logits'.
1.2.0,
1.2.0,Returns:
1.2.0,A list of logit tensors; one for each classification task.
1.2.0,""""""""
1.2.0,logits_list = []
1.2.0,with tf.name_scope('multitask_logits'):
1.2.0,for task_idx in range(num_tasks):
1.2.0,"with tf.name_scope(name,"
1.2.0,"('task' + str(task_idx).zfill(len(str(num_tasks)))), [features]):"
1.2.0,logits_list.append(
1.2.0,"logits(features, num_classes, weight_init=weight_init,"
1.2.0,"bias_init=bias_init, dropout_prob=dropout_prob))"
1.2.0,return logits_list
1.2.0,
1.2.0,
1.2.0,"def logits(features, num_classes=2, weight_init=None, bias_init=None,"
1.2.0,"dropout_prob=None, name=None):"
1.2.0,"""""""Create a logits tensor for a single classification task."
1.2.0,
1.2.0,You almost certainly don't want dropout on there -- it's like randomly setting
1.2.0,the (unscaled) probability of a target class to 0.5.
1.2.0,
1.2.0,Args:
1.2.0,features: A 2D tensor with dimensions batch_size x num_features.
1.2.0,num_classes: Number of classes for each task.
1.2.0,weight_init: Weight initializer.
1.2.0,bias_init: Bias initializer.
1.2.0,dropout_prob: Float giving dropout probability for weights (NOT keep
1.2.0,probability).
1.2.0,name: Name for this op.
1.2.0,
1.2.0,Returns:
1.2.0,A logits tensor with shape batch_size x num_classes.
1.2.0,""""""""
1.2.0,"with tf.name_scope(name, 'logits', [features]) as name:"
1.2.0,return dropout(
1.2.0,"fully_connected_layer(features, num_classes, weight_init=weight_init,"
1.2.0,"bias_init=bias_init, name=name),"
1.2.0,dropout_prob)
1.2.0,
1.2.0,
1.2.0,"def softmax_N(tensor, name=None):"
1.2.0,"""""""Apply softmax across last dimension of a tensor."
1.2.0,
1.2.0,Args:
1.2.0,tensor: Input tensor.
1.2.0,"name: Name for this op. If None, defaults to 'softmax_N'."
1.2.0,
1.2.0,Returns:
1.2.0,A tensor with softmax-normalized values on the last dimension.
1.2.0,""""""""
1.2.0,"with tf.name_scope(name, 'softmax_N', [tensor]):"
1.2.0,exp_tensor = tf.exp(tensor)
1.2.0,reduction_indices = [tensor.get_shape().ndims - 1]
1.2.0,"return tf.div(exp_tensor,"
1.2.0,"tf.reduce_sum(exp_tensor,"
1.2.0,"reduction_indices=reduction_indices,"
1.2.0,keep_dims=True))
1.2.0,
1.2.0,"def optimizer(optimizer=""adam"", learning_rate=.001, momentum=.9):"
1.2.0,"""""""Create model optimizer."
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,"optimizer: str, optional"
1.2.0,Name of optimizer
1.2.0,"learning_rate: float, optional"
1.2.0,Learning rate for algorithm
1.2.0,"momentum: float, optional"
1.2.0,Momentum rate
1.2.0,
1.2.0,Returns
1.2.0,-------
1.2.0,A training Optimizer.
1.2.0,
1.2.0,Raises:
1.2.0,NotImplementedError: If an unsupported optimizer is requested.
1.2.0,""""""""
1.2.0,# TODO(user): gradient clipping (see Minimize)
1.2.0,if optimizer == 'adagrad':
1.2.0,train_op = tf.train.AdagradOptimizer(learning_rate)
1.2.0,elif optimizer == 'adam':
1.2.0,train_op = tf.train.AdamOptimizer(learning_rate)
1.2.0,elif optimizer == 'momentum':
1.2.0,"train_op = tf.train.MomentumOptimizer(learning_rate,"
1.2.0,momentum)
1.2.0,elif optimizer == 'rmsprop':
1.2.0,"train_op = tf.train.RMSPropOptimizer(learning_rate,"
1.2.0,momentum)
1.2.0,elif optimizer == 'sgd':
1.2.0,train_op = tf.train.GradientDescentOptimizer(learning_rate)
1.2.0,else:
1.2.0,raise NotImplementedError('Unsupported optimizer %s' % optimizer)
1.2.0,return train_op
1.2.0,!/usr/bin/python
1.2.0,
1.2.0,Copyright 2015 Google Inc.
1.2.0,
1.2.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.2.0,you may not use this file except in compliance with the License.
1.2.0,You may obtain a copy of the License at
1.2.0,
1.2.0,http://www.apache.org/licenses/LICENSE-2.0
1.2.0,
1.2.0,"Unless required by applicable law or agreed to in writing, software"
1.2.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.2.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.2.0,See the License for the specific language governing permissions and
1.2.0,limitations under the License.
1.2.0,get the divisor
1.2.0,compute the requested central moment
1.2.0,"note that mean is a raw moment, not a central moment"
1.2.0,TODO(user): median is not implemented yet in TensorFlow
1.2.0,-*- coding: utf-8 -*-
1.2.0,"due to the different shape of weight(ndims=2) and bias(ndims=1),"
1.2.0,will using this version for logreg
1.2.0,exclude bias variables
1.2.0,setting up n_tasks nodes(output nodes)
1.2.0,label placeholders with size batch_size * 1
1.2.0,Note that we divide by the batch size and not the number of
1.2.0,"non-zero weight examples in the batch.  Also, instead of using"
1.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.2.0,calculate with div/sum so it stays on the GPU.
1.2.0,aggregated costs
1.2.0,weight decay
1.2.0,using self-defined regularization
1.2.0,adding output nodes of sigmoid function
1.2.0,"fix the size to be [?,1]"
1.2.0,Dummy placeholders
1.2.0,Dummy placeholders
1.2.0,run eval data through the model
1.2.0,transfer 2D prediction tensor to 2D x n_classes(=2)
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,run eval data through the model
1.2.0,transfer 2D prediction tensor to 2D x n_classes(=2)
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,"layer has shape [None, layer_sizes[i]]"
1.2.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.2.0,TODO(rbharath): Might want to make it feasible to have multiple
1.2.0,bypass layers.
1.2.0,Construct task bypass layer
1.2.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.2.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.2.0,"layer has shape [None, layer_sizes[i]]"
1.2.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.2.0,TODO(rbharath): Might want to make it feasible to have multiple
1.2.0,bypass layers.
1.2.0,Construct task bypass layer
1.2.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.2.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.2.0,Add the input features.
1.2.0,Add the dense layers
1.2.0,Compute the loss function for each label.
1.2.0,Add the input features.
1.2.0,Add the dense layers
1.2.0,Compute the loss function for each label.
1.2.0,Run fit transformers on dummy dataset to determine n_features after transformation
1.2.0,Dummy placeholders
1.2.0,Dummy placeholders
1.2.0,Dummy placeholders
1.2.0,Dummy placeholders
1.2.0,Run fit transformers on dummy dataset to determine n_features after transformation
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Save an initial checkpoint.
1.2.0,Define the code that runs on a separate thread to feed data into the queue.
1.2.0,Main training loop.
1.2.0,Run training op.
1.2.0,We have reached the end of an epoch.
1.2.0,We have reached the end of the data.
1.2.0,Always save a final checkpoint when complete.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,Handle edge case when batch-size is 1.
1.2.0,Prune away any padding that was added
1.2.0,Handle case of 0-dimensional scalar output
1.2.0,Consistency check
1.2.0,Lazily created by _get_shared_session().
1.2.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.2.0,when subclass-overridden methods use the same scopes.
1.2.0,Setup graph
1.2.0,Create placeholders
1.2.0,Handle output layer
1.2.0,Iterate over all previous tasks.
1.2.0,prev_layers is a list with elements of size
1.2.0,"(batch_size, layer_sizes[i-1])"
1.2.0,Note that we divide by the batch size and not the number of
1.2.0,"non-zero weight examples in the batch.  Also, instead of using"
1.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.2.0,calculate with div/sum so it stays on the GPU.
1.2.0,aggregated costs
1.2.0,weight decay
1.2.0,Dummy placeholders
1.2.0,Dummy placeholders
1.2.0,run eval data through the model
1.2.0,"Shape (n_tasks, n__samples)"
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,Handle edge case when batch-size is 1.
1.2.0,with self._get_shared_session(train=True) as sess:
1.2.0,Save an initial checkpoint.
1.2.0,Always save a final checkpoint when complete.
1.2.0,Note that we divide by the batch size and not the number of
1.2.0,"non-zero weight examples in the batch.  Also, instead of using"
1.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.2.0,calculate with div/sum so it stays on the GPU.
1.2.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
1.2.0,Dummy placeholders
1.2.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
1.2.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
1.2.0,Dummy placeholders
1.2.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
1.2.0,allow_soft_placement=True allows ops without a GPU implementation
1.2.0,to run on the CPU instead.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Turns out there are valid cases where we don't want pad-batches
1.2.0,on by default.
1.2.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.2.0,if epoch%checkpoint_interval == checkpoint_interval-1:
1.2.0,"saver.save(sess, self._save_path, global_step=epoch)"
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,"(n_samples, n_classes)"
1.2.0,"(n_samples, n_tasks, n_classes)"
1.2.0,Save hyperparameters
1.2.0,Guard variable to make sure we don't Restore() this model
1.2.0,from a disk checkpoint more than once.
1.2.0,"Path to save checkpoint files, which matches the"
1.2.0,replicated supervisor's default path.
1.2.0,Lazily created by _get_shared_session().
1.2.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.2.0,when subclass-overridden methods use the same scopes.
1.2.0,Setup graph
1.2.0,Note that we divide by the batch size and not the number of
1.2.0,"non-zero weight examples in the batch.  Also, instead of using"
1.2.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.2.0,calculate with div/sum so it stays on the GPU.
1.2.0,aggregated costs
1.2.0,weight decay
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Save an initial checkpoint.
1.2.0,Define the code that runs on a separate thread to feed data into the queue.
1.2.0,Main training loop.
1.2.0,Run training op.
1.2.0,We have reached the end of an epoch.
1.2.0,We have reached the end of the data.
1.2.0,Always save a final checkpoint when complete.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,allow_soft_placement=True allows ops without a GPU implementation
1.2.0,to run on the CPU instead.
1.2.0,TODO(rbharath): Is setting train=False right here?
1.2.0,Discard any padded predictions
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,Special case to handle singletasks.
1.2.0,The iterbatches does padding with zero-weight examples on the last batch.
1.2.0,Remove padded examples.
1.2.0,TODO(rbharath): Verify this can be safely removed.
1.2.0,"def evaluate(self, dataset, metrics, transformers=[]):"
1.2.0,""""""""
1.2.0,Evaluates the performance of this model on specified dataset.
1.2.0,
1.2.0,Parameters
1.2.0,----------
1.2.0,dataset: dc.data.Dataset
1.2.0,Dataset object.
1.2.0,metric: deepchem.metrics.Metric
1.2.0,Evaluation metric
1.2.0,transformers: list
1.2.0,List of deepchem.transformers.Transformer
1.2.0,Returns
1.2.0,-------
1.2.0,dict
1.2.0,Maps tasks to scores under metric.
1.2.0,""""""""
1.2.0,"evaluator = Evaluator(self, dataset, transformers)"
1.2.0,scores = evaluator.compute_model_performance(metrics)
1.2.0,return scores
1.2.0,checkpoints look like model_dir/model.ckpt-N
1.2.0,"self._save_path is ""model_dir/model.ckpt"""
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,Note that softmax is already applied in construct_grpah
1.2.0,run eval data through the model
1.2.0,reshape to batch_size x n_tasks x ...
1.2.0,Handle edge case when batch-size is 1.
1.2.0,Prune away any padding that was added
1.2.0,Handle case of 0-dimensional scalar output
1.2.0,!/usr/bin/python
1.2.0,
1.2.0,Copyright 2015 Google Inc.
1.2.0,
1.2.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.2.0,you may not use this file except in compliance with the License.
1.2.0,You may obtain a copy of the License at
1.2.0,
1.2.0,http://www.apache.org/licenses/LICENSE-2.0
1.2.0,
1.2.0,"Unless required by applicable law or agreed to in writing, software"
1.2.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.2.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.2.0,See the License for the specific language governing permissions and
1.2.0,limitations under the License.
1.2.0,parse CheckpointState proto
1.2.0,parse path to actual checkpoint
1.2.0,the provided mask has to be the same shape as features
1.2.0,test k = 1..4
1.2.0,central moments
1.2.0,standardized moments
1.2.0,central across one axis
1.2.0,standardized across one axis
1.2.0,Fit just on task zero
1.2.0,Notice that we keep the session open
1.2.0,Fit on task one
1.2.0,The predictions for task zero should not change after training
1.2.0,on task one.
1.2.0,Keep track of the layers
1.2.0,############################################ DEBUG
1.2.0,"print(""start - add()"")"
1.2.0,"print(""self.output"")"
1.2.0,print(self.output)
1.2.0,############################################ DEBUG
1.2.0,"For graphical layers, add connectivity placeholders"
1.2.0,############################################ DEBUG
1.2.0,"print(""end- add()"")"
1.2.0,"print(""self.output"")"
1.2.0,print(self.output)
1.2.0,############################################ DEBUG
1.2.0,Add layer to the layer list
1.2.0,Keep track of the layers
1.2.0,Create graph topology and x
1.2.0,Keep track of the layers
1.2.0,Whether or not we have used the GraphGather layer yet
1.2.0,Update new value of x
1.2.0,Update new value of x
1.2.0,Update new value of x
1.2.0,Get train function
1.2.0,Initialize
1.2.0,################################################################### DEBUG
1.2.0,self.test_label_placeholder = Input(
1.2.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.2.0,"name=""label_placeholder""))"
1.2.0,self.test_weight_placeholder = Input(
1.2.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.2.0,"name=""weight_placeholder""))"
1.2.0,TODO(rbharath): Should weights for the support be used?
1.2.0,Support labels
1.2.0,self.support_label_placeholder = Input(
1.2.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
1.2.0,"name=""support_label_placeholder""))"
1.2.0,################################################################### DEBUG
1.2.0,Generate dictionary elements for support
1.2.0,Get graph information for test
1.2.0,Generate dictionary elements for test
1.2.0,Perform the optimization
1.2.0,Create different support sets
1.2.0,Get batch to try it out on
1.2.0,"Train on support set, batch pair"
1.2.0,Get featurization for test
1.2.0,"Shape (n_test, n_feat)"
1.2.0,Get featurization for support
1.2.0,"Shape (n_support, n_feat)"
1.2.0,Computes the inner part c() of the kernel
1.2.0,(the inset equation in section 2.1.1 of Matching networks paper).
1.2.0,Normalize
1.2.0,TODO(rbharath): euclidean kernel is broken!
1.2.0,elif self.similarity == 'euclidean':
1.2.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
1.2.0,"Note that gram matrix g has shape (n_test, n_support)"
1.2.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
1.2.0,https://arxiv.org/pdf/1606.04080v1.pdf
1.2.0,"Computes softmax across axis 1, (so sums distances to support set for"
1.2.0,each test entry) to get attention vector
1.2.0,"Shape (n_test, n_support)"
1.2.0,Weighted sum of support labels
1.2.0,"Shape (n_support, 1)"
1.2.0,pred is yhat in eqn (1) of Matching Networks.
1.2.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
1.2.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
1.2.0,"Shape (n_test,)"
1.2.0,Convert to logit space using inverse sigmoid (logit) function
1.2.0,logit function: log(pred) - log(1-pred)
1.2.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
1.2.0,in Cross Entropy calculation.
1.2.0,"Shape (n_test,)"
1.2.0,Get scores
1.2.0,Remove padded elements
1.2.0,Get scores
1.2.0,pred corresponds to prob(example == 1)
1.2.0,Remove padded elements
1.2.0,Get batches
1.2.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
1.2.0,multitask case with missing data...
1.2.0,Join information for all tasks.
1.2.0,TODO(rbharath): Find a way to get rid of this import?
1.2.0,Extract model info
1.2.0,Get graph topology for x
1.2.0,Building outputs
1.2.0,Set epsilon
1.2.0,Initialize
1.2.0,"Path to save checkpoint files, which matches the"
1.2.0,replicated supervisor's default path.
1.2.0,Create target inputs
1.2.0,Get train function
1.2.0,TODO(rbharath): I believe this is total amount of data
1.2.0,Get graph information
1.2.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.2.0,the number of labeled data points in target_i. This is to normalize each task
1.2.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.2.0,Get other optimizer information
1.2.0,TODO(rbharath): Figure out how to handle phase appropriately
1.2.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.2.0,"tensors of shape (batch_size,)"
1.2.0,It's ok to divide by just the batch_size rather than the number of nonzero
1.2.0,examples (effect averages out)
1.2.0,Perform the optimization
1.2.0,TODO(rbharath): Disabling saving for now to try to debug.
1.2.0,run eval data through the model
1.2.0,"Shape (n_samples, n_tasks)"
1.2.0,Create target inputs
1.2.0,TODO(rbharath): Find a way to get rid of this import?
1.2.0,Obtain appropriate loss function
1.2.0,Extract model info
1.2.0,Get graph topology for x
1.2.0,############################################################ DEBUG
1.2.0,self.feat_dim = self.model.get_num_output_features()
1.2.0,############################################################ DEBUG
1.2.0,Raw logit outputs
1.2.0,Set epsilon
1.2.0,Initialize
1.2.0,"Path to save checkpoint files, which matches the"
1.2.0,replicated supervisor's default path.
1.2.0,Create target inputs
1.2.0,############################################################### DEBUG
1.2.0,"print(""multitask classifier"")"
1.2.0,"print(""feat"")"
1.2.0,print(feat)
1.2.0,############################################################### DEBUG
1.2.0,Get train function
1.2.0,TODO(rbharath): I believe this is total amount of data
1.2.0,Get graph information
1.2.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.2.0,the number of labeled data points in target_i. This is to normalize each task
1.2.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.2.0,Get other optimizer information
1.2.0,TODO(rbharath): Figure out how to handle phase appropriately
1.2.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.2.0,"tensors of shape (batch_size,)"
1.2.0,Convert the labels into one-hot vector encodings.
1.2.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
1.2.0,un-softmaxed logits rather than softmax outputs.
1.2.0,It's ok to divide by just the batch_size rather than the number of nonzero
1.2.0,examples (effect averages out)
1.2.0,Perform the optimization
1.2.0,TODO(rbharath): Disabling saving for now to try to debug.
1.2.0,run eval data through the model
1.2.0,"Shape (n_samples, n_tasks)"
1.2.0,run eval data through the model
1.2.0,self.n_atoms = n_atoms
1.2.0,Define the list of tensors to be used as topology
1.2.0,Merge mol conv objects
1.2.0,Generate dicts
1.2.0,self.n_atoms = n_atoms
1.2.0,Define the list of tensors to be used as topology
1.2.0,Extract atom numbers
1.2.0,Generate dicts
1.2.0,molecule * atom(graph) => step => features
1.2.0,molecule * atom(graph) => step
1.2.0,molecule * atom(graph) => step
1.2.0,Define the list of tensors to be used as topology
1.2.0,calculation orders for a batch of molecules
1.2.0,padding atom features vector of each molecule with 0
1.2.0,self.n_atoms = n_atoms
1.2.0,Define the list of tensors to be used as topology
1.2.0,Extract atom numbers
1.2.0,Generate dicts
1.2.0,self.n_atoms = n_atoms
1.2.0,Define the list of tensors to be used as topology
1.2.0,Extract atom numbers
1.2.0,number of atoms in each molecule
1.2.0,index of pair features
1.2.0,number of pairs for each atom
1.2.0,atom features
1.2.0,pair features
1.2.0,Generate dicts
1.2.0,Associate each atom with cell it belongs to. O(N*n_cells)
1.2.0,"Shape (n_cells, k)"
1.2.0,"Shape (N, 1)"
1.2.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.2.0,"conditions, so does wrapround. O(constant)"
1.2.0,"Shape (n_cells, 26)"
1.2.0,"Shape (N, 26)"
1.2.0,"coords of shape (N, ndim)"
1.2.0,"Shape (N, 26, k, ndim)"
1.2.0,"Shape (N, 26, k)"
1.2.0,"Shape (N, 26, k)"
1.2.0,"Shape (N, 26, k, ndim)"
1.2.0,"For smaller systems especially, the periodic boundary conditions can"
1.2.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
1.2.0,make sure duplicate neighbors are ignored?
1.2.0,TODO(rbharath): How does distance need to be modified here to
1.2.0,account for periodic boundary conditions?
1.2.0,"Shape (N, 26, k)"
1.2.0,"Shape (N, 26*k)"
1.2.0,TODO(rbharath): This will cause an issue with duplicates!
1.2.0,"Shape (N, M)"
1.2.0,"N elts of size (M,) each"
1.2.0,"Shape (N, 26*k)"
1.2.0,"N elts of size (26*k,) each"
1.2.0,"N elts of size (M,) each"
1.2.0,"Shape (N, M)"
1.2.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.2.0,"N tensors of shape (n_cells, 1)"
1.2.0,"Shape (N*n_cells, 1) after tile"
1.2.0,"List of N tensors of shape (n_cells, 1)"
1.2.0,Lists of length N
1.2.0,Lists of length n_cells
1.2.0,Get indices of k atoms closest to each cell point
1.2.0,TODO(rbharath): tf.stack for tf 1.0
1.2.0,"Tensor of shape (n_cells, k, ndim)"
1.2.0,atoms_in_cells = tf.stack(atoms_in_cells)
1.2.0,"Tensor of shape (26, k, ndim)"
1.2.0,"Reshape to (26*k, ndim)"
1.2.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
1.2.0,"Dists of shape (26*k, 1)"
1.2.0,"Of shape (k, ndim)"
1.2.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.2.0,TODO(rbharath): Change this for tf 1.0
1.2.0,"n_cells tensors of shape (N, 1)"
1.2.0,"Shape (N*n_cells, 1) after tile"
1.2.0,"List of n_cells tensors of shape (N, 1)"
1.2.0,Lists of length n_cells
1.2.0,Lists of length n_cells
1.2.0,Get indices of k atoms closest to each cell point
1.2.0,"n_cells tensors of shape (k, ndim)"
1.2.0,"Tensor of shape (n_cells, k)"
1.2.0,TODO(rbharath):
1.2.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
1.2.0,- Need to group closest atoms amongst cell neighbors
1.2.0,- Need to do another top_k to find indices of closest neighbors.
1.2.0,- Return N lists corresponding to neighbors for every atom.
1.2.0,TODO(rbharath): Do we need to handle periodic boundary conditions
1.2.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.2.0,"looking for 26 neighbors, which isn't right for boundary cells in"
1.2.0,the cube.
1.2.0,Number of neighbors of central cube in 3-space is
1.2.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
1.2.0,TODO(rbharath)
1.2.0,n_cells = int(cells.get_shape()[0])
1.2.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.2.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.2.0,"Tile (a, a, a, b, b, b, etc.)"
1.2.0,"Tile (a, b, c, a, b, c, ...)"
1.2.0,"Lists of n_cells tensors of shape (N, 1)"
1.2.0,Lists of length n_cells
1.2.0,Lists of length n_cells
1.2.0,Get indices of k atoms closest to each cell point
1.2.0,"n_cells tensors of shape (26,)"
1.2.0,TODO(rbharath): Make this handle minibatches
1.2.0,"Shape (N_protein+N_ligand, 3)"
1.2.0,"Shape (N_protein+N_ligand,)"
1.2.0,"Shape (N_protein+N_ligand, M)"
1.2.0,"Shape (N_protein+N_ligand,)"
1.2.0,"Shape (N_protein+N_ligand, 3)"
1.2.0,"Shape (N_protein+N_ligand,)"
1.2.0,"Shape (N_protein+N_ligand, M)"
1.2.0,"Shape (N_protein+N_ligand, M, 3)"
1.2.0,"Shape (N_protein+N_ligand, M)"
1.2.0,"Shape (N_protein+N_ligand, M, 3)"
1.2.0,"Shape (N_protein+N_ligand, M)"
1.2.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
1.2.0,"Shape (N_protein+N_ligand, M)"
1.2.0,"Shape (N_protein+N_ligand, M)"
1.2.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
1.2.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
1.2.0,computing free-energy. This implementation currently uses all interaction
1.2.0,terms. Not sure if this makes a difference.
1.2.0,"Shape (N_protein+N_ligand, M)"
1.2.0,Shape () -- scalar
1.2.0,# Gather Projection
1.2.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
1.2.0,There should be 8 layers in graph_model
1.2.0,assert len(graph_model.layers) == 6
1.2.0,Add layers
1.2.0,Need to add batch-norm separately to test/support due to differing
1.2.0,shapes.
1.2.0,Apply an attention lstm layer
1.2.0,Gather Projection
1.2.0,Add layers
1.2.0,Need to add batch-norm separately to test/support due to differing
1.2.0,shapes.
1.2.0,Apply an attention lstm layer
1.2.0,Gather Projection
1.2.0,Degrees from 1 to max_deg inclusive
1.2.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
1.2.0,"Should have shape (?, deg)"
1.2.0,"Shape of atom_features should be (?, n_feat)"
1.2.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
1.2.0,TODO(rbharath): Check that this operation is differentiable.
1.2.0,The number of cells which we should theoretically have
1.2.0,The number of cells which we should theoretically have
1.2.0,"Each atom neighbors tensor should be (k, ndim) shaped."
1.2.0,The number of cells which we should theoretically have
1.2.0,TODO(rbharath): The test below only checks that shapes work out.
1.2.0,Need to do a correctness implementation vs. a simple CPU impl.
1.2.0,The number of cells which we should theoretically have
1.2.0,TODO(rbharath): The test below only checks that shapes work out.
1.2.0,Need to do a correctness implementation vs. a simple CPU impl.
1.2.0,The number of cells which we should theoretically have
1.2.0,TODO(rbharath): The test below only checks that shapes work out.
1.2.0,Need to do a correctness implementation vs. a simple CPU impl.
1.2.0,TODO(rbharath): Commenting this out due to weird segfaults
1.2.0,def test_vina_generate_conformers(self):
1.2.0,"""""""Test that Vina Model can generate conformers"""""""
1.2.0,data_dir = os.path.dirname(os.path.realpath(__file__))
1.2.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
1.2.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
1.2.0,max_protein_atoms = 3500
1.2.0,max_ligand_atoms = 100
1.2.0,"print(""Loading protein file"")"
1.2.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
1.2.0,protein_Z = pad_array(
1.2.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
1.2.0,max_protein_atoms)
1.2.0,"print(""Loading ligand file"")"
1.2.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
1.2.0,ligand_Z = pad_array(
1.2.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
1.2.0,max_ligand_atoms)
1.2.0,-*- coding: utf-8 -*-
1.2.0,Assigning featurizer if not user defined
1.2.0,loading datasets
1.2.0,Assembling train and valid datasets
1.2.0,!/usr/bin/env python2
1.2.0,-*- coding: utf-8 -*-
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow MultiTaskDNN model
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow robust MultiTaskDNN model
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow logistic regression model
1.2.0,Loading hyper parameters
1.2.0,Transform fingerprints to IRV features
1.2.0,Building tensorflow IRV model
1.2.0,Loading hyper parameters
1.2.0,Gather Projection
1.2.0,Loading hyper parameters
1.2.0,Loading hyper parameters
1.2.0,Building scikit random forest model
1.2.0,Loading hyper parameters
1.2.0,Building xgboost classification model
1.2.0,Loading hyper parameters
1.2.0,Building tensorflow MultiTaskDNN model
1.2.0,Loading hyper parameters
1.2.0,Initialize model folder
1.2.0,Loading hyper parameters
1.2.0,Gather Projection
1.2.0,Loading hyper parameters
1.2.0,Loading hyper parameters
1.2.0,Loading hyper parameters
1.2.0,Building scikit random forest model
1.2.0,Loading hyper parameters
1.2.0,Building xgboost classification model
1.2.0,Loading hyperparameters
1.2.0,num positive/negative ligands
1.2.0,Set batch sizes for network
1.2.0,Model structure
1.2.0,Traning settings
1.2.0,Fit trained model
1.2.0,Evaluating low data model
1.2.0,-*- coding: utf-8 -*-
1.2.0,Assigning featurizer if not user defined
1.2.0,loading datasets
1.2.0,
1.2.0,Note by @XericZephyr. Reason why I spun off this function:
1.2.0,1. Some model needs dataset information.
1.2.0,2. It offers us possibility to **cache** the dataset
1.2.0,"if the featurizer runs very slow, e.g., GraphConv."
1.2.0,2+. The cache can even happen at Travis CI to accelerate
1.2.0,CI testing.
1.2.0,
1.2.0,loading datasets
1.2.0,!/usr/bin/env python2
1.2.0,-*- coding: utf-8 -*-
1.2.0,Featurize qm9 dataset
1.2.0,transformers = [
1.2.0,"deepchem.trans.LogTransformer(transform_X=True),"
1.2.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
1.2.0,dataset=train_dataset)]
1.2.0,Set shard size low to avoid memory problems.
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Set some global variables up top
1.2.0,Featurize KAGGLE dataset
1.2.0,############################################################# TIMING
1.2.0,############################################################# TIMING
1.2.0,Featurize qm7 dataset
1.2.0,Featurize clintox dataset
1.2.0,Transform clintox dataset
1.2.0,Split clintox dataset
1.2.0,Featurize bbb dataset
1.2.0,Initialize transformers
1.2.0,Load nci dataset
1.2.0,Featurize nci dataset
1.2.0,Initialize transformers
1.2.0,Featurize HOPV dataset
1.2.0,Initialize transformers
1.2.0,Featurize PPB dataset
1.2.0,Initialize transformers
1.2.0,Load MUV dataset
1.2.0,Featurize MUV dataset
1.2.0,Initialize transformers
1.2.0,Featurize clearance dataset
1.2.0,Initialize transformers
1.2.0,Featurize TOXCAST dataset
1.2.0,Initialize transformers
1.2.0,Featurize bace dataset
1.2.0,Initialize transformers
1.2.0,Featurize bace dataset
1.2.0,Initialize transformers
1.2.0,Featurize Tox21 dataset
1.2.0,Initialize transformers
1.2.0,Featurize ChEMBL dataset
1.2.0,Initialize transformers
1.2.0,Featurize hiv dataset
1.2.0,Initialize transformers
1.2.0,Featurize SIDER dataset
1.2.0,Initialize transformers
1.2.0,Featurize SAMPL dataset
1.2.0,Initialize transformers
1.2.0,Featurize Delaney dataset
1.2.0,Initialize transformers
1.2.0,Featurize PCBA dataset
1.2.0,Initialize transformers
1.2.0,Featurize Lipophilicity dataset
1.2.0,Initialize transformers
1.2.0,TODO(rbharath): This function is complicated and monolithic. Is there a nice
1.2.0,way to refactor this?
1.2.0,arbitrarily return last model
1.2.0,Define train dataset
1.2.0,Define validation dataset
1.2.0,Generate the rollout.
1.2.0,Compute an estimate of the reward for the rest of the episode.
1.2.0,Compute the output arrays.
1.2.0,This is modeled after the Roulette-v0 environment from OpenAI Gym.
1.2.0,"The player can bet on any number from 0 to 36, or walk away (which ends the"
1.2.0,"game).  The average reward for any bet is slightly negative, so the best"
1.2.0,strategy is to walk away.
1.2.0,"This policy just learns a constant probability for each action, and a constant for the value."
1.2.0,Optimize it.
1.2.0,"It should have learned that the expected value is very close to zero, and that the best"
1.2.0,action is to walk away.
1.2.0,"Verify that we can create a new A3C object, reload the parameters from the first one, and"
1.2.0,get the same result.
1.2.0,"Do the same thing, only using the ""restore"" argument to fit()."
1.2.0,The environment just has a constant state.
1.2.0,The policy includes a single recurrent layer.
1.2.0,"We don't care about actually optimizing it, so just run a few rollouts to make"
1.2.0,"sure fit() doesn't crash, then check the behavior of the GRU state."
1.2.0,"On the first call, the initial state should be all zeros."
1.2.0,It should still be zeros since we didn't save it last time.
1.2.0,It should be different now.
1.2.0,This should be the same as the previous one.
1.2.0,"Now we reset it, so we should get the same result as initially."
1.2.0,Randomize who goes first
1.2.0,Illegal move -- the square is not empty
1.2.0,Move X
1.2.0,Did X Win
1.2.0,Did O Win
1.2.0,TODO (Bowen): make this function less memory intensive
1.2.0,set 1st column as the column index of dataframe
1.2.0,merge descriptor and activities dataframe into output dataframe based on
1.2.0,"the molecule name, which is the index for both dataframes (but named"
1.2.0,differently). Default merge is inner merge
1.2.0,need to manually set dataframe indexname after merge based on index
1.2.0,from deepchem.scripts.dock_dude import *
1.2.0,from ipyparallel import Client
1.2.0,rc = Client()
1.2.0,dview = rc[:]
1.2.0,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
1.2.0,
1.2.0,"If mol_id is not set, then use isomeric smiles as unique identifier"
1.2.0,iterator = data_df.iterrows()
1.2.0,TODO(rbharath): BROKEN!
1.2.0,Trim unwanted indexing fields
1.2.0,Connect to running ipython server
1.2.0,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
1.2.0,
1.2.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.2.0,you may not use this file except in compliance with the License.
1.2.0,You may obtain a copy of the License at
1.2.0,
1.2.0,http://www.apache.org/licenses/LICENSE-2.0
1.2.0,
1.2.0,"Unless required by applicable law or agreed to in writing, software"
1.2.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.2.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.2.0,See the License for the specific language governing permissions and
1.2.0,limitations under the License.
1.2.0,==============================================================================
1.2.0,Maps from a function name to a dictionary that describes how to
1.2.0,map from an old argument keyword to the new argument keyword.
1.2.0,Mapping from function to the new name of the function
1.2.0,Functions that were reordered should be changed to the new keyword args
1.2.0,"for safety, if positional arguments are used. If you have reversed the"
1.2.0,"positional arguments yourself, this could do the wrong thing."
1.2.0,Specially handled functions.
1.2.0,TODO(aselle): Could check for a literal list of bools and try to convert
1.2.0,them to indices.
1.2.0,all edits are lists of chars
1.2.0,Iterate of each line
1.2.0,sort by column so that edits are processed in order in order to make
1.2.0,indexing adjustments cumulative for changes that change the string
1.2.0,length
1.2.0,"Extract each line to a list of characters, because mutable lists"
1.2.0,"are editable, unlike immutable strings."
1.2.0,Record a description of the change
1.2.0,Make underscore buffers for underlining where in the line the edit was
1.2.0,Iterate for each edit
1.2.0,"Create effective start, end by accounting for change in length due"
1.2.0,to previous edits
1.2.0,Make sure the edit is changing what it should be changing
1.2.0,Make the edit
1.2.0,Create the underline highlighting of the before and after
1.2.0,Keep track of how to generate effective ranges
1.2.0,Finish the report comment
1.2.0,"Strangely, ast.ListComp returns the col_offset of the first token"
1.2.0,after the '[' token which appears to be a bug. Workaround by
1.2.0,explicitly finding the real start of the list comprehension.
1.2.0,loop over lines
1.2.0,Reverse the text to and regular expression search for whitespace
1.2.0,First find if a [ can be found with only whitespace between it and
1.2.0,col.
1.2.0,TODO(aselle):
1.2.0,"this is poor comment detection, but it is good enough for"
1.2.0,cases where the comment does not contain string literal starting/
1.2.0,ending characters. If ast gave us start and end locations of the
1.2.0,"ast nodes rather than just start, we could use string literal"
1.2.0,node ranges to filter out spurious #'s that appear in string
1.2.0,literals.
1.2.0,"Most other nodes return proper locations (with notably does not), but"
1.2.0,it is not possible to use that in an argument.
1.2.0,"Find a simple attribute name path e.g. ""tf.foo.bar"""
1.2.0,Make sure the func is marked as being part of a call
1.2.0,Call special handlers
1.2.0,Examine any non-keyword argument and make it into a keyword argument
1.2.0,if reordering required.
1.2.0,Examine each keyword argument and convert it to the final renamed form
1.2.0,TODO(aselle): We should scan backward to find the start of the
1.2.0,keyword key. Unfortunately ast does not give you the location of
1.2.0,"keyword keys, so we are forced to infer it from the keyword arg"
1.2.0,value.
1.2.0,"Write to a temporary file, just in case we are doing an implace modify."
1.2.0,Broad exceptions are required here because ast throws whatever it wants.
1.2.0,pylint: disable=broad-except
1.2.0,pylint: enable=broad-except
1.2.0,make sure output directory doesn't exist
1.2.0,make sure output directory does not overlap with root_directory
1.2.0,Collect list of files to process (we do this to correctly handle if the
1.2.0,user puts the output directory in some sub directory of the input dir)
1.2.0,import os
1.2.0,"from deepchem.utils.save import load_from_disk, save_to_disk"
1.2.0,from deepchem.featurizers.fingerprints import CircularFingerprint
1.2.0,from deepchem.featurizers.basic import RDKitDescriptors
1.2.0,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
1.2.0,from deepchem.featurizers.grid_featurizer import GridFeaturizer
1.2.0,from deepchem.featurizers.featurize import DataLoader
1.2.0,
1.2.0,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
1.2.0,"print(""About to load dataset form disk."")"
1.2.0,dataset = load_from_disk(dataset_file)
1.2.0,"print(""Loaded dataset."")"
1.2.0,
1.2.0,grid_featurizer = GridFeaturizer(
1.2.0,"voxel_width=16.0, feature_types=""voxel_combined"","
1.2.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.2.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.2.0,"parallel=True, flatten=True)"
1.2.0,featurizers = [CircularFingerprint(size=1024)]
1.2.0,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
1.2.0,
1.2.0,#Make a directory in which to store the featurized complexes.
1.2.0,"base_dir = ""../../../grid_nnscore_circular_features"""
1.2.0,if not os.path.exists(base_dir):
1.2.0,os.makedirs(base_dir)
1.2.0,"data_dir = os.path.join(base_dir, ""data"")"
1.2.0,if not os.path.exists(data_dir):
1.2.0,os.makedirs(data_dir)
1.2.0,
1.2.0,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
1.2.0,
1.2.0,"feature_dir = os.path.join(base_dir, ""features"")"
1.2.0,if not os.path.exists(feature_dir):
1.2.0,os.makedirs(feature_dir)
1.2.0,
1.2.0,"samples_dir = os.path.join(base_dir, ""samples"")"
1.2.0,if not os.path.exists(samples_dir):
1.2.0,os.makedirs(samples_dir)
1.2.0,
1.2.0,
1.2.0,
1.2.0,featurizers = compound_featurizers + complex_featurizers
1.2.0,"featurizer = DataLoader(tasks=[""label""],"
1.2.0,"smiles_field=""smiles"","
1.2.0,"protein_pdb_field=""protein_pdb"","
1.2.0,"ligand_pdb_field=""ligand_pdb"","
1.2.0,"compound_featurizers=compound_featurizers,"
1.2.0,"complex_featurizers=complex_featurizers,"
1.2.0,"id_field=""complex_id"","
1.2.0,verbose=False)
1.2.0,from ipyparallel import Client
1.2.0,c = Client()
1.2.0,"print(""c.ids"")"
1.2.0,print(c.ids)
1.2.0,dview = c[:]
1.2.0,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
1.2.0,"worker_pool=dview, shard_size=1024)"
1.2.0,
1.2.0,"save_to_disk(featurized_samples, featurized_samples_file)"
1.2.0,"print(""Preparing ligand %s"" % mol_name)"
1.1.0,!/usr/bin/env python3
1.1.0,-*- coding: utf-8 -*-
1.1.0,Assigning featurizer
1.1.0,Some exceptions in datasets
1.1.0,loading datasets
1.1.0,time_finish_loading-time_start is the time(s) used for dataset loading
1.1.0,dataset has customized features
1.1.0,running model
1.1.0,Initialize metrics
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow MultiTaskDNN model
1.1.0,Evaluating tensorflow MultiTaskDNN model
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow robust MultiTaskDNN model
1.1.0,Evaluating tensorflow robust MultiTaskDNN model
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow logistic regression model
1.1.0,Evaluating tensorflow logistic regression model
1.1.0,Loading hyper parameters
1.1.0,Transform fingerprints to IRV features
1.1.0,Building tensorflow IRV model
1.1.0,Evaluating tensorflow IRV model
1.1.0,Initialize model folder
1.1.0,Loading hyper parameters
1.1.0,Building graph convolution model
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,Evaluating graph convolution model
1.1.0,Loading hyper parameters
1.1.0,Building scikit random forest model
1.1.0,Evaluating scikit random forest model
1.1.0,Loading hyper parameters
1.1.0,Building xgboost classification model
1.1.0,Evaluating xgboost classification model
1.1.0,Initialize metrics
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow MultiTaskDNN model
1.1.0,Evaluating tensorflow MultiTaskDNN model
1.1.0,Initialize model folder
1.1.0,Loading hyper parameters
1.1.0,Building graph convoluwtion model
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,Evaluating graph convolution model
1.1.0,Loading hyper parameters
1.1.0,Building scikit random forest model
1.1.0,Evaluating scikit random forest model
1.1.0,Loading hyper parameters
1.1.0,Building xgboost classification model
1.1.0,Evaluating xgboost classification model
1.1.0,Global variables
1.1.0,Datasets and models used in the benchmark test
1.1.0,"irv, rf, rf_regression should be assigned manually"
1.1.0,input hyperparameters
1.1.0,!/usr/bin/env python3
1.1.0,-*- coding: utf-8 -*-
1.1.0,Datasets and models used in the benchmark test
1.1.0,!/usr/bin/env python3
1.1.0,-*- coding: utf-8 -*-
1.1.0,Datasets and models used in the benchmark test
1.1.0,"irv, rf, rf_regression should be assigned manually"
1.1.0,-*- coding: utf-8 -*-
1.1.0,main layer
1.1.0,bypass layer
1.1.0,penalty
1.1.0,general figure
1.1.0,learning rate
1.1.0,for graph-conv and random forest
1.1.0,Set numpy seed
1.1.0,##Load data###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Featurize Kinase dataset
1.1.0,##Load data###
1.1.0,num_trials = 5
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,Force matplotlib to not use any Xwindows backend.
1.1.0,##Load data###
1.1.0,the histogram of the data
1.1.0,Set numpy seed
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,num_trials = 5
1.1.0,Set some global variables up top
1.1.0,Fit trained model
1.1.0,Featurize PCBA dataset
1.1.0,Initialize transformers
1.1.0,Fit trained model
1.1.0,Load SWEET dataset
1.1.0,Featurize SWEET dataset
1.1.0,Initialize transformers
1.1.0,Set some global variables up top
1.1.0,removes directory if present -- warning
1.1.0,default split is 80-10-10 train-valid-test split
1.1.0,Fit Logistic Regression models
1.1.0,Fit Logistic Regression models
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,transformers = [
1.1.0,"dc.trans.LogTransformer(transform_X=True),"
1.1.0,"dc.trans.NormalizationTransformer(transform_y=True,"
1.1.0,dataset=train_dataset)]
1.1.0,Set shard size low to avoid memory problems.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Set some global variables up top
1.1.0,Featurize KAGGLE dataset
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,##Load data###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,##Load data###
1.1.0,"n_estimators=100, max_features=int(num_features/3),"
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,"Images are square 28x28 (batch, height, width, channel)"
1.1.0,Featurize qm9 dataset
1.1.0,Fit trained model
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Featurize Tox21 dataset
1.1.0,Initialize transformers
1.1.0,Only for debug!
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Load tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Featurize FACTORS dataset
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,Force matplotlib to not use any Xwindows backend.
1.1.0,##Load data###
1.1.0,the histogram of the data
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Featurize qm7 dataset
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Fit trained model
1.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_X=True, dataset=train_dataset), dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.1.0,Fit trained model
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Fit trained model
1.1.0,Featurize qm8 dataset
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Fit trained model
1.1.0,Set numpy seed
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,Set shard size low to avoid memory problems.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Set some global variables up top
1.1.0,Load dataset
1.1.0,Featurize ChEMBL dataset
1.1.0,Initialize transformers
1.1.0,Load ChEMBL dataset
1.1.0,Fit models
1.1.0,Do setup required for tf/keras models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,DeepCrystal Technologies 2017 - Patrick Hop
1.1.0,MIT License - have fun!!
1.1.0,======================================================================
1.1.0,"Run Benchmarks {GC-DNN, P-DNN, SVR, RF}"
1.1.0,we cant re-open the closed session...
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Do setup required for tf/keras models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,"graph.add(dc.nn.WeaveLayer(max_atoms, 50, 50))"
1.1.0,Fit trained model
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Do setup required for tf/keras models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Gather Projection
1.1.0,Dense post-processing layer
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Featurize Delaney dataset
1.1.0,Initialize transformers
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load Delaney dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load MUV dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Evaluate train/test scores
1.1.0,Load MUV dataset
1.1.0,Featurize MUV dataset
1.1.0,Initialize transformers
1.1.0,Load MUV data
1.1.0,Build model
1.1.0,Fit trained model
1.1.0,Evaluate train/test scores
1.1.0,Extract active site
1.1.0,Featurize ligand
1.1.0,Default for CircularFingerprint
1.1.0,Featurize pocket
1.1.0,Note broadcast operation
1.1.0,Compute labels for pockets
1.1.0,Some complexes have labels but no PDB files. Filter these manually
1.1.0,Some of the ligand-names are of form (FMN ox). Use regex
1.1.0,to merge into form (FMN-ox)
1.1.0,Filter if missing PDB files
1.1.0,Load PDBBind dataset
1.1.0,Define featurizers
1.1.0,Featurize Dataset
1.1.0,########################################################## DEBUG
1.1.0,########################################################## DEBUG
1.1.0,For stable runs
1.1.0,Fit trained model
1.1.0,4-fold splits
1.1.0,num positive/negative ligands
1.1.0,10 trials on test-set
1.1.0,Define metric
1.1.0,Get supports on test-set
1.1.0,Compute accuracies
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Test model
1.1.0,Join information for all tasks.
1.1.0,4-fold splits
1.1.0,num positive/negative ligands
1.1.0,Define metric
1.1.0,Get supports on test-set
1.1.0,Compute accuracies
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Test model
1.1.0,Join information for all tasks.
1.1.0,4-fold splits
1.1.0,num positive/negative ligands
1.1.0,10 trials on test-set
1.1.0,Define metric
1.1.0,Get supports on test-set
1.1.0,Compute accuracies
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Fit trained model
1.1.0,Test model
1.1.0,Join information for all tasks.
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,number positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply an attention lstm layer
1.1.0,Number of folds for split
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,4-fold splits
1.1.0,num positive/negative ligands
1.1.0,10 trials on test-set
1.1.0,Define metric
1.1.0,Get supports on test-set
1.1.0,Compute accuracies
1.1.0,Train model on support
1.1.0,Test model
1.1.0,Join information for all tasks.
1.1.0,4-fold splits
1.1.0,num positive/negative ligands
1.1.0,10 trials on test-set
1.1.0,Define metric
1.1.0,Get supports on test-set
1.1.0,Compute accuracies
1.1.0,Train model on support
1.1.0,Test model
1.1.0,Join information for all tasks.
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,number positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply an attention lstm layer
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,number positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply an attention lstm layer
1.1.0,Number of folds for split
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Number of folds for split
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply a residual lstm layer
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply a residual lstm layer
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply a residual lstm layer
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply a residual lstm layer
1.1.0,Set some global variables up top
1.1.0,Featurize Tox21 dataset
1.1.0,Initialize transformers
1.1.0,Set some global variables up top
1.1.0,Featurize Tox21 dataset
1.1.0,Initialize transformers
1.1.0,Load MUV dataset
1.1.0,Featurize MUV dataset
1.1.0,Initialize transformers
1.1.0,Load MUV dataset
1.1.0,Featurize MUV dataset
1.1.0,Initialize transformers
1.1.0,Featurize SIDER dataset
1.1.0,Initialize transformers
1.1.0,Featurize SIDER dataset
1.1.0,Initialize transformers
1.1.0,Number of folds for split
1.1.0,Depth of attention module
1.1.0,number positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,Apply an attention lstm layer
1.1.0,4-fold splits
1.1.0,10 positive/negative ligands
1.1.0,10 trials on test-set
1.1.0,Sample supports without replacement (all pos/neg should be different)
1.1.0,Define metric
1.1.0,Get supports on test-set
1.1.0,Compute accuracies
1.1.0,Train model on support
1.1.0,Test model
1.1.0,"print(""Score on task %s is %s"" % (str(task), str(score)))"
1.1.0,Join information for all tasks.
1.1.0,Number of folds for split
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Number of features on conv-mols
1.1.0,Define metric
1.1.0,Train support model on train
1.1.0,Add layers
1.1.0,4-fold splits
1.1.0,num positive/negative ligands
1.1.0,Define metric
1.1.0,Get supports on test-set
1.1.0,Compute accuracies
1.1.0,Train model on support
1.1.0,Test model
1.1.0,Join information for all tasks.
1.1.0,Note sensitivity = recall
1.1.0,NOTE THE RENAMING:
1.1.0,Note sensitivity = recall
1.1.0,Load nci dataset
1.1.0,Featurize nci dataset
1.1.0,Initialize transformers
1.1.0,Set some global variables up top
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load hiv dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Featurize hiv dataset
1.1.0,Initialize transformers
1.1.0,Only for debug!
1.1.0,Load hiv dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Fit trained model
1.1.0,Fit models
1.1.0,Batch size of models
1.1.0,"graph.add(dc.nn.AlternateWeaveLayer(max_atoms, 50, 50))"
1.1.0,Fit trained model
1.1.0,Load SIDER dataset
1.1.0,Featurize SIDER dataset
1.1.0,Initialize transformers
1.1.0,Only for debug!
1.1.0,Load SAMPL dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load Tox21 dataset
1.1.0,Fit models
1.1.0,Do setup required for tf/keras models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Gather Projection
1.1.0,Dense post-processing layer
1.1.0,Fit trained model
1.1.0,Featurize SAMPL dataset
1.1.0,Initialize transformers
1.1.0,Load clintox dataset
1.1.0,Featurize clintox dataset
1.1.0,Transform clintox dataset
1.1.0,Split clintox dataset
1.1.0,Only for debug!
1.1.0,Load clintox dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load clintox dataset
1.1.0,Fit models
1.1.0,Do setup required for tf/keras models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,-*- coding: utf-8 -*-
1.1.0,#############################################################################
1.1.0,## save dataset
1.1.0,#############################################################################
1.1.0,## load datasets
1.1.0,load sweetfda
1.1.0,load aact
1.1.0,## fixup smiles for matching
1.1.0,return smiles
1.1.0,map original smiles to converted smiles
1.1.0,"## join dataframes, index on smiles"
1.1.0,map original smiles back
1.1.0,## fill all nan with 0
1.1.0,## construct datasets
1.1.0,store in new datasets
1.1.0,## save datasets
1.1.0,"fout = ""aacttox_sweetfda_phase_multiclass.csv"""
1.1.0,"cols = ['smiles', 'FDA_APPROVED', 'CT_TOX','CT_TOX_PHASE']"
1.1.0,"pd.DataFrame(datasets[1], columns=cols).to_csv(fout, index=False)"
1.1.0,"fout = ""aacttox_sweetfda_cto_singletask.csv"""
1.1.0,"columns=['smiles', 'CLINICAL_TRIAL_OUTCOME']"
1.1.0,"pd.DataFrame(datasets[2], columns=cols).to_csv(fout, index=False)"
1.1.0,"fout = ""aacttox_sweetfda_cto_fdatox.csv"""
1.1.0,"columns = ['smiles', 'CLINICAL_TRIAL_OUTCOME', 'FDA_APPROVED_TOX']"
1.1.0,"pd.DataFrame(datasets[3], columns=cols).to_csv(fout, index=False)"
1.1.0,"fout = ""aacttox_sweetfda_phase_multitask.csv"""
1.1.0,"columns=['smiles', 'FDA_APPROVED', 'CT_TOX',"
1.1.0,"'CT_TOX_PHASE_1', 'CT_TOX_PHASE_2',"
1.1.0,"'CT_TOX_PHASE_3', 'CT_TOX_PHASE_4']"
1.1.0,"pd.DataFrame(datasets[4], columns=cols).to_csv(fout, index=False)"
1.1.0,For stable runs
1.1.0,Fit trained model
1.1.0,For stable runs
1.1.0,Fit trained model
1.1.0,Some complexes have labels but no PDB files. Filter these manually
1.1.0,Some of the ligand-names are of form (FMN ox). Use regex
1.1.0,to merge into form (FMN-ox)
1.1.0,Load PDBBind dataset
1.1.0,Define featurizers
1.1.0,"TODO(rbharath, enf, leswing): Figure out why pi_stack and cation_pi"
1.1.0,reduce validation performance
1.1.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.1.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.1.0,Featurize Dataset
1.1.0,Currently featurizes with shard_size=1
1.1.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.1.0,transformers = [
1.1.0,"dc.trans.LogTransformer(transform_X=True),"
1.1.0,"dc.trans.NormalizationTransformer(transform_y=True,"
1.1.0,dataset=train_dataset)]
1.1.0,Featurize UV dataset
1.1.0,##Load data###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Set numpy seed
1.1.0,##Load data###
1.1.0,##Create model###
1.1.0,Use R2 classification metric
1.1.0,"model.old_fit(train_dataset, nb_epoch=nb_epoch)"
1.1.0,Only use for final evaluation
1.1.0,Force matplotlib to not use any Xwindows backend.
1.1.0,##Load data###
1.1.0,the histogram of the data
1.1.0,##Load data###
1.1.0,###################################################### DEBUG
1.1.0,###################################################### DEBUG
1.1.0,Load HOPV dataset
1.1.0,Fit models
1.1.0,Number of features on conv-mols
1.1.0,Batch size of models
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,Featurize HOPV dataset
1.1.0,Initialize transformers
1.1.0,Only for debug!
1.1.0,Load HOPV dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load HOPV dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load HOPV dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Only for debug!
1.1.0,Load HOPV dataset
1.1.0,Fit models
1.1.0,Fit trained model
1.1.0,Load TOXCAST dataset
1.1.0,Featurize TOXCAST dataset
1.1.0,Initialize transformers
1.1.0,Fit trained model
1.1.0,Processing of ToxCast data
1.1.0,Author - Aneesh Pappu
1.1.0,Loading dataframes and editing indices
1.1.0,Loop through rows of hitc matrix and replace codes with smiles strings
1.1.0,get corresponding casn
1.1.0,get corresponding smiles
1.1.0,write to cell
1.1.0,Tidy up and write to csv
1.1.0,-*- coding: utf-8 -*-
1.1.0,Save hyperparameters
1.1.0,-*- coding: utf-8 -*-
1.1.0,Save hyperparameters
1.1.0,-*- coding: utf-8 -*-
1.1.0,Save hyperparameters
1.1.0,weight decay
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Turns out there are valid cases where we don't want pad-batches
1.1.0,on by default.
1.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.1.0,Run training op.
1.1.0,############################################################# TIMING
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,Special case to handle singletasks.
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,2017 DeepCrystal Technologies - Patrick Hop
1.1.0,
1.1.0,Message Passing Neural Network for Chemical Multigraphs
1.1.0,
1.1.0,MIT License - have fun!!
1.1.0,===========================================================
1.1.0,A = {}
1.1.0,"valid_bonds = {'SINGLE', 'DOUBLE', 'TRIPLE', 'AROMATIC'}"
1.1.0,for valid_bond in valid_bonds:
1.1.0,"A[valid_bond] = nn.Linear(75, 75)"
1.1.0,"GRU = nn.GRU(150, 75, 1)"
1.1.0,"flow_delta = Variable(torch.zeros(1, 1))"
1.1.0,"h_t = Variable(torch.zeros(1, 1, 75))"
1.1.0,bond_type = e_vw.GetBondType()
1.1.0,A_vw = A[str(e_vw.GetBondType())]
1.1.0,"gru_act, h_t = GRU(catted.view(1, 1, 150), h_t)"
1.1.0,measure convergence
1.1.0,pdist = nn.PairwiseDistance(2)
1.1.0,"flow_delta = flow_delta + torch.sum(pdist(gru_act.view(1, 75), h[v]))"
1.1.0,"h[v] = gru_act.view(1, 75)"
1.1.0,"print '    flow delta [%i] [%f]' % (k, flow_delta.data.numpy()[0])"
1.1.0,training loop
1.1.0,"{'params': A['DOUBLE'].parameters()},"
1.1.0,"{'params': A['TRIPLE'].parameters()},"
1.1.0,"{'params': A['AROMATIC'].parameters()},"
1.1.0,"{'params': GRU.parameters()},"
1.1.0,"rf.fit(np_fps_train, train_labels)"
1.1.0,labels = rf.predict(val_fps)
1.1.0,Set random seeds
1.1.0,Setup directories
1.1.0,Model constants
1.1.0,Load and transform datasets
1.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.1.0,Atomic convolution variables
1.1.0,at = atomic numbers (atom types)
1.1.0,"radial basis function parameters [cutoff, mean, width]"
1.1.0,Model hyperparameters
1.1.0,Initialize model
1.1.0,Fit model
1.1.0,Evaluate model
1.1.0,Set random seeds
1.1.0,Setup directories
1.1.0,Model constants
1.1.0,Load and transform datasets
1.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.1.0,Atomic convolution variables
1.1.0,at = atomic numbers (atom types)
1.1.0,"radial basis function parameters [cutoff, mean, width]"
1.1.0,Model hyperparameters
1.1.0,Initialize model
1.1.0,Fit model
1.1.0,Evaluate model
1.1.0,Set random seeds
1.1.0,Setup directories
1.1.0,Model constants
1.1.0,Load and transform datasets
1.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.1.0,Atomic convolution variables
1.1.0,at = atomic numbers (atom types)
1.1.0,"radial basis function parameters [cutoff, mean, width]"
1.1.0,Model hyperparameters
1.1.0,Initialize model
1.1.0,Fit model
1.1.0,Evaluate model
1.1.0,Set random seeds
1.1.0,Setup directories
1.1.0,Model constants
1.1.0,Load and transform datasets
1.1.0,convert -logKi to dG = +RTlogKi [kJ/mol]
1.1.0,Atomic convolution variables
1.1.0,at = atomic numbers (atom types)
1.1.0,"radial basis function parameters [cutoff, mean, width]"
1.1.0,Model hyperparameters
1.1.0,Initialize model
1.1.0,Fit model
1.1.0,Evaluate model
1.1.0,test_evaluator = dc.utils.evaluate.GeneratorEvaluator(
1.1.0,"tg, feed_dict_generator(test_dataset, batch_size), transformers, [label])"
1.1.0,test_scores = test_evaluator.compute_model_performance(metric)
1.1.0,"test_scores = {""%s_test"" % k: v for k, v in test_scores.items()}"
1.1.0,param.update(test_scores)
1.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.1.0,for transformer in transformers:
1.1.0,train_dataset = transformer.transform(train_dataset)
1.1.0,test_dataset = transformer.transform(test_dataset)
1.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.1.0,for transformer in transformers:
1.1.0,train_dataset = transformer.transform(train_dataset)
1.1.0,test_dataset = transformer.transform(test_dataset)
1.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.1.0,for transformer in transformers:
1.1.0,train_dataset = transformer.transform(train_dataset)
1.1.0,test_dataset = transformer.transform(test_dataset)
1.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.1.0,"transformers = [dc.trans.NormalizationTransformer(transform_y=True, dataset=train_dataset)]"
1.1.0,for transformer in transformers:
1.1.0,train_dataset = transformer.transform(train_dataset)
1.1.0,test_dataset = transformer.transform(test_dataset)
1.1.0,"radial = [[12.0], [0.0, 4.0, 8.0], [0.4]]"
1.1.0,Create some directories for analysis
1.1.0,The base_dir holds the results of all analysis
1.1.0,Make directories to store the raw and featurized datasets.
1.1.0,Load PDBBind dataset
1.1.0,Define featurizers
1.1.0,Currently featurizes with shard_size=1
1.1.0,Dataset can be reshard: dataset = dataset.reshard(48) for example
1.1.0,This could be done with openbabel in python
1.1.0,Compute cells for this molecule. O(constant)
1.1.0,min == max if molecule is planar in some direction
1.1.0,we should still create a bin
1.1.0,TODO(JSG): Implement non-PBC version.  For now this seems fine ..
1.1.0,Note neighbors contains self!
1.1.0,Associate each atom with cell it belongs to. O(N)
1.1.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.1.0,"conditions, so does wrapround. O(constant)"
1.1.0,"For each atom, loop through all atoms in its cell and neighboring cells."
1.1.0,Accept as neighbors only those within threshold. This computation should be
1.1.0,"O(Nm), where m is the number of atoms within a set of neighboring-cells."
1.1.0,Sort neighbors by distance
1.1.0,Pick up to max_num_neighbors
1.1.0,Type of data created by this featurizer
1.1.0,assumes that every array is of the same dimension
1.1.0,rem_dataset is remaining portion of dataset
1.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.1.0,to k-1.
1.1.0,returns list of per column sum of non zero elements
1.1.0,Compute number of actives needed per task.
1.1.0,loop through each column and obtain index required to splice out for
1.1.0,required fraction of hits
1.1.0,Find the first index where the cumulative number of actives equals
1.1.0,the actives_count
1.1.0,Note that np.where tells us last index required to exceed
1.1.0,"actives_count, so we actually want the following location"
1.1.0,TODO(rbharath): Refactor this split method to match API of other splits (or
1.1.0,potentially refactor those to match this.
1.1.0,Handle edge case where frac_split is 1
1.1.0,Create weight matrices fpor two haves.
1.1.0,copy over up to required index for weight first_split
1.1.0,check out if any rows in either w_1 or w_2 are just zeros
1.1.0,"Obtain original x, y, and w arrays and shuffle"
1.1.0,calculate percent split for valid (out of test and valid)
1.1.0,"split test data into valid and test, treating sub test set also as sparse"
1.1.0,rem_dataset is remaining portion of dataset
1.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.1.0,to k-1.
1.1.0,JSG Assert that split fractions can be written as proper fractions over 10.
1.1.0,This can be generalized in the future with some common demoninator determination.
1.1.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.1.0,Append remaining examples to train
1.1.0,Sort by increasing MW
1.1.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.1.0,for m_idx in cluster:
1.1.0,"continue until we find an active in all the tasks, otherwise we can't"
1.1.0,compute a meaningful AUC
1.1.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.1.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.1.0,Sort from largest to smallest scaffold sets
1.1.0,Sort from largest to smallest scaffold sets
1.1.0,"(n_samples, n_classes)"
1.1.0,"(n_samples, n_tasks, n_classes)"
1.1.0,Save hyperparameters
1.1.0,Guard variable to make sure we don't Restore() this model
1.1.0,from a disk checkpoint more than once.
1.1.0,"Path to save checkpoint files, which matches the"
1.1.0,replicated supervisor's default path.
1.1.0,Lazily created by _get_shared_session().
1.1.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.1.0,when subclass-overridden methods use the same scopes.
1.1.0,Setup graph
1.1.0,Note that we divide by the batch size and not the number of
1.1.0,"non-zero weight examples in the batch.  Also, instead of using"
1.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.1.0,calculate with div/sum so it stays on the GPU.
1.1.0,aggregated costs
1.1.0,weight decay
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Save an initial checkpoint.
1.1.0,Turns out there are valid cases where we don't want pad-batches
1.1.0,on by default.
1.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.1.0,Run training op.
1.1.0,Always save a final checkpoint when complete.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,allow_soft_placement=True allows ops without a GPU implementation
1.1.0,to run on the CPU instead.
1.1.0,TODO(rbharath): Is setting train=False right here?
1.1.0,Discard any padded predictions
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,Special case to handle singletasks.
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,TODO(rbharath): Verify this can be safely removed.
1.1.0,"def evaluate(self, dataset, metrics, transformers=[]):"
1.1.0,""""""""
1.1.0,Evaluates the performance of this model on specified dataset.
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,dataset: dc.data.Dataset
1.1.0,Dataset object.
1.1.0,metric: deepchem.metrics.Metric
1.1.0,Evaluation metric
1.1.0,transformers: list
1.1.0,List of deepchem.transformers.Transformer
1.1.0,Returns
1.1.0,-------
1.1.0,dict
1.1.0,Maps tasks to scores under metric.
1.1.0,""""""""
1.1.0,"evaluator = Evaluator(self, dataset, transformers)"
1.1.0,scores = evaluator.compute_model_performance(metrics)
1.1.0,return scores
1.1.0,checkpoints look like logdir/model.ckpt-N
1.1.0,"self._save_path is ""logdir/model.ckpt"""
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,Note that softmax is already applied in construct_grpah
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,Handle edge case when batch-size is 1.
1.1.0,Prune away any padding that was added
1.1.0,Handle case of 0-dimensional scalar output
1.1.0,Dummy placeholders
1.1.0,Dummy placeholders
1.1.0,## AtomicNet fully-connected layer ops ###
1.1.0,## Atomicnet coordinate transform ops ###
1.1.0,## Atomicnet symmetry function kernel ops ###
1.1.0,## Atomicnet symmetry function ops ###
1.1.0,## Atomcnet symmetry function layer ops ###
1.1.0,We apply the radial pooling filter before atom type conv
1.1.0,to reduce computation
1.1.0,## Misc convenience ops ###
1.1.0,-*- coding: utf-8 -*-
1.1.0,
1.1.0,"deepchem documentation build configuration file, created by"
1.1.0,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
1.1.0,
1.1.0,This file is execfile()d with the current directory set to its
1.1.0,containing dir.
1.1.0,
1.1.0,Note that not all possible configuration values are present in this
1.1.0,autogenerated file.
1.1.0,
1.1.0,All configuration values have a default; values that are commented out
1.1.0,serve to show the default.
1.1.0,"If extensions (or modules to document with autodoc) are in another directory,"
1.1.0,add these directories to sys.path here. If the directory is relative to the
1.1.0,"documentation root, use os.path.abspath to make it absolute, like shown here."
1.1.0,"sys.path.insert(0, os.path.abspath('.'))"
1.1.0,-- General configuration ------------------------------------------------
1.1.0,"If your documentation needs a minimal Sphinx version, state it here."
1.1.0,needs_sphinx = '1.0'
1.1.0,"Add any Sphinx extension module names here, as strings. They can be"
1.1.0,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
1.1.0,ones.
1.1.0,"Add any paths that contain templates here, relative to this directory."
1.1.0,The suffix(es) of source filenames.
1.1.0,You can specify multiple suffix as a list of string:
1.1.0,"source_suffix = ['.rst', '.md']"
1.1.0,The encoding of source files.
1.1.0,source_encoding = 'utf-8-sig'
1.1.0,The master toctree document.
1.1.0,General information about the project.
1.1.0,"The version info for the project you're documenting, acts as replacement for"
1.1.0,"|version| and |release|, also used in various other places throughout the"
1.1.0,built documents.
1.1.0,
1.1.0,The short X.Y version.
1.1.0,"The full version, including alpha/beta/rc tags."
1.1.0,The language for content autogenerated by Sphinx. Refer to documentation
1.1.0,for a list of supported languages.
1.1.0,
1.1.0,This is also used if you do content translation via gettext catalogs.
1.1.0,"Usually you set ""language"" from the command line for these cases."
1.1.0,"There are two options for replacing |today|: either, you set today to some"
1.1.0,"non-false value, then it is used:"
1.1.0,today = ''
1.1.0,"Else, today_fmt is used as the format for a strftime call."
1.1.0,"today_fmt = '%B %d, %Y'"
1.1.0,"List of patterns, relative to source directory, that match files and"
1.1.0,directories to ignore when looking for source files.
1.1.0,The reST default role (used for this markup: `text`) to use for all
1.1.0,documents.
1.1.0,default_role = None
1.1.0,"If true, '()' will be appended to :func: etc. cross-reference text."
1.1.0,add_function_parentheses = True
1.1.0,"If true, the current module name will be prepended to all description"
1.1.0,unit titles (such as .. function::).
1.1.0,add_module_names = True
1.1.0,"If true, sectionauthor and moduleauthor directives will be shown in the"
1.1.0,output. They are ignored by default.
1.1.0,show_authors = False
1.1.0,The name of the Pygments (syntax highlighting) style to use.
1.1.0,A list of ignored prefixes for module index sorting.
1.1.0,modindex_common_prefix = []
1.1.0,"If true, keep warnings as ""system message"" paragraphs in the built documents."
1.1.0,keep_warnings = False
1.1.0,"If true, `todo` and `todoList` produce output, else they produce nothing."
1.1.0,-- Options for HTML output ----------------------------------------------
1.1.0,The theme to use for HTML and HTML Help pages.  See the documentation for
1.1.0,a list of builtin themes.
1.1.0,Theme options are theme-specific and customize the look and feel of a theme
1.1.0,"further.  For a list of options available for each theme, see the"
1.1.0,documentation.
1.1.0,html_theme_options = {}
1.1.0,"Add any paths that contain custom themes here, relative to this directory."
1.1.0,"The name for this set of Sphinx documents.  If None, it defaults to"
1.1.0,"""<project> v<release> documentation""."
1.1.0,html_title = None
1.1.0,A shorter title for the navigation bar.  Default is the same as html_title.
1.1.0,html_short_title = None
1.1.0,The name of an image file (relative to this directory) to place at the top
1.1.0,of the sidebar.
1.1.0,The name of an image file (within the static path) to use as favicon of the
1.1.0,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
1.1.0,pixels large.
1.1.0,html_favicon = None
1.1.0,"Add any paths that contain custom static files (such as style sheets) here,"
1.1.0,"relative to this directory. They are copied after the builtin static files,"
1.1.0,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
1.1.0,Add any extra paths that contain custom files (such as robots.txt or
1.1.0,".htaccess) here, relative to this directory. These files are copied"
1.1.0,directly to the root of the documentation.
1.1.0,html_extra_path = []
1.1.0,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
1.1.0,using the given strftime format.
1.1.0,"html_last_updated_fmt = '%b %d, %Y'"
1.1.0,"If true, SmartyPants will be used to convert quotes and dashes to"
1.1.0,typographically correct entities.
1.1.0,html_use_smartypants = True
1.1.0,"Custom sidebar templates, maps document names to template names."
1.1.0,html_sidebars = {}
1.1.0,"Additional templates that should be rendered to pages, maps page names to"
1.1.0,template names.
1.1.0,html_additional_pages = {}
1.1.0,"If false, no module index is generated."
1.1.0,html_domain_indices = True
1.1.0,"If false, no index is generated."
1.1.0,html_use_index = True
1.1.0,"If true, the index is split into individual pages for each letter."
1.1.0,html_split_index = False
1.1.0,"If true, links to the reST sources are added to the pages."
1.1.0,html_show_sourcelink = True
1.1.0,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
1.1.0,html_show_sphinx = True
1.1.0,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
1.1.0,html_show_copyright = True
1.1.0,"If true, an OpenSearch description file will be output, and all pages will"
1.1.0,contain a <link> tag referring to it.  The value of this option must be the
1.1.0,base URL from which the finished HTML is served.
1.1.0,html_use_opensearch = ''
1.1.0,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
1.1.0,html_file_suffix = None
1.1.0,Language to be used for generating the HTML full-text search index.
1.1.0,Sphinx supports the following languages:
1.1.0,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
1.1.0,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
1.1.0,html_search_language = 'en'
1.1.0,"A dictionary with options for the search language support, empty by default."
1.1.0,Now only 'ja' uses this config value
1.1.0,html_search_options = {'type': 'default'}
1.1.0,The name of a javascript file (relative to the configuration directory) that
1.1.0,"implements a search results scorer. If empty, the default will be used."
1.1.0,html_search_scorer = 'scorer.js'
1.1.0,Output file base name for HTML help builder.
1.1.0,-- Options for LaTeX output ---------------------------------------------
1.1.0,The paper size ('letterpaper' or 'a4paper').
1.1.0,"'papersize': 'letterpaper',"
1.1.0,"The font size ('10pt', '11pt' or '12pt')."
1.1.0,"'pointsize': '10pt',"
1.1.0,Additional stuff for the LaTeX preamble.
1.1.0,"'preamble': '',"
1.1.0,Latex figure (float) alignment
1.1.0,"'figure_align': 'htbp',"
1.1.0,Grouping the document tree into LaTeX files. List of tuples
1.1.0,"(source start file, target name, title,"
1.1.0,"author, documentclass [howto, manual, or own class])."
1.1.0,The name of an image file (relative to this directory) to place at the top of
1.1.0,the title page.
1.1.0,latex_logo = None
1.1.0,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
1.1.0,not chapters.
1.1.0,latex_use_parts = False
1.1.0,"If true, show page references after internal links."
1.1.0,latex_show_pagerefs = False
1.1.0,"If true, show URL addresses after external links."
1.1.0,latex_show_urls = False
1.1.0,Documents to append as an appendix to all manuals.
1.1.0,latex_appendices = []
1.1.0,"If false, no module index is generated."
1.1.0,latex_domain_indices = True
1.1.0,-- Options for manual page output ---------------------------------------
1.1.0,One entry per manual page. List of tuples
1.1.0,"(source start file, name, description, authors, manual section)."
1.1.0,"If true, show URL addresses after external links."
1.1.0,man_show_urls = False
1.1.0,-- Options for Texinfo output -------------------------------------------
1.1.0,Grouping the document tree into Texinfo files. List of tuples
1.1.0,"(source start file, target name, title, author,"
1.1.0,"dir menu entry, description, category)"
1.1.0,Documents to append as an appendix to all manuals.
1.1.0,texinfo_appendices = []
1.1.0,"If false, no module index is generated."
1.1.0,texinfo_domain_indices = True
1.1.0,"How to display URL addresses: 'footnote', 'no', or 'inline'."
1.1.0,texinfo_show_urls = 'footnote'
1.1.0,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
1.1.0,texinfo_no_detailmenu = False
1.1.0,Example configuration for intersphinx: refer to the Python standard library.
1.1.0,Higher is Better
1.1.0,The secret key is available as a secure environment variable
1.1.0,on travis-ci to push the build documentation to Amazon S3.
1.1.0,########################################################### DEBUG
1.1.0,########################################################### DEBUG
1.1.0,s3cmd -M -H sync docs/_build/ s3://deepchem.io/
1.1.0,########################################################### DEBUG
1.1.0,########################################################### DEBUG
1.1.0,lines in the label file have format
1.1.0,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
1.1.0,"print line[0], line[3]"
1.1.0,References
1.1.0,Arguments
1.1.0,Aliases.
1.1.0,Tensorflow correctly processes empty lists when using concat
1.1.0,"Sum along neighbors as well as self, and store"
1.1.0,Sum all neighbors using adjacency matrix
1.1.0,Get collection of modified atom features
1.1.0,Obtain relevant atoms for this degree
1.1.0,Get self atoms
1.1.0,Apply hidden affine to relevant atoms and append
1.1.0,Determine the min_deg=0 case
1.1.0,Only use the self layer
1.1.0,Combine all atoms back into the list
1.1.0,"WARNING: Does not work for Batch Size 1! If batch_size = 1, then use reduce_sum!"
1.1.0,Obtain the partitions for each of the molecules
1.1.0,Sum over atoms for each molecule
1.1.0,Get the final sparse representations
1.1.0,Store the summed atoms by degree
1.1.0,Tensorflow correctly processes empty lists when using concat
1.1.0,Get self atoms
1.1.0,Expand dims
1.1.0,always deg-1 for deg_adj_lists
1.1.0,TODO(rbharath): It's not clear where nb_affine comes from.
1.1.0,Is there a solid explanation here?
1.1.0,Generate the nb_affine weights and biases
1.1.0,Add trainable weights
1.1.0,Extract atom_features
1.1.0,Extract graph topology
1.1.0,Perform the mol conv
1.1.0,Extract nodes and membership
1.1.0,Extract atom_features
1.1.0,Extract graph topology
1.1.0,Perform the mol gather
1.1.0,Extract nodes
1.1.0,Extract atom_features
1.1.0,Extract graph topology
1.1.0,Perform the mol gather
1.1.0,"x is test set, xp is support set."
1.1.0,# Initializes trainable weights.
1.1.0,## Performs computations
1.1.0,Get initializations
1.1.0,r = self.r_init
1.1.0,Process using attention
1.1.0,"Eqn (4), appendix A.1 of Matching Networks paper"
1.1.0,Generate new aattention states
1.1.0,"def build(self, input_shape):"
1.1.0,"_, support_input_shape = input_shape  #Unpack"
1.1.0,n_feat = support_input_shape[1]
1.1.0,Support set lstm
1.1.0,Test lstm
1.1.0,Get initializations
1.1.0,Rename support
1.1.0,Process support xp using attention
1.1.0,Get linear combination of support set
1.1.0,Not sure if it helps to place the update here or later yet.  Will
1.1.0,decide
1.1.0,z = r
1.1.0,Process test x using attention
1.1.0,Generate new support attention states
1.1.0,Generate new test attention states
1.1.0,Redefine
1.1.0,"return [x+p, z+q]"
1.1.0,No other forget biases supported right now.
1.1.0,"def build(self, input_shape):"
1.1.0,Taken from Keras code [citation needed]
1.1.0,###################################################### DEBUG
1.1.0,"return o, [h, c]"
1.1.0,###################################################### DEBUG
1.1.0,"self.b_fc = model_ops.zeros(shape=[self.n_embedding,])"
1.1.0,distance_hidden = self.activation(distance_hidden)
1.1.0,atom_features_hidden = self.activation(atom_features_hidden)
1.1.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.1.0,and embeddings of atom j(both gone through a hidden layer)
1.1.0,"for atom i, sum the influence from all other atom j in the molecule"
1.1.0,number of inputs each step
1.1.0,Add trainable weights
1.1.0,Extract atom_features
1.1.0,Basic features of every atom: (batch_size*max_atoms) * n_atom_features
1.1.0,calculation orders of graph: (batch_size*max_atoms) * max_atoms * max_atoms
1.1.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.1.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.1.0,"step i calculates the graph features for atoms of index `parents[:,i,0]`"
1.1.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.1.0,"represent the same atoms of `parents[:, :, 0]`,"
1.1.0,different in that these index are positions in `atom_features`
1.1.0,"number of atoms in total, should equal `batch_size*max_atoms`"
1.1.0,initialize graph features for each graph
1.1.0,another row of zeros is generated for padded dummy atoms
1.1.0,`count`-th step
1.1.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.1.0,generating index for graph features used in the inputs
1.1.0,"extracting graph features for parents of the target atoms, then flatten"
1.1.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.1.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.1.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.1.0,of shape: (batch_size*max_atoms) * n_graph_features
1.1.0,representing the graph features of target atoms in each graph
1.1.0,index for targe atoms
1.1.0,update the graph features for target atoms
1.1.0,last step generates graph features for all target atom
1.1.0,Add trainable weights
1.1.0,Extract atom_features
1.1.0,sum all graph outputs
1.1.0,Aliases.
1.1.0,TODO(rbharath): What does this line do?
1.1.0,TODO(rbharath): REMOVE GLOBAL VARS! BREAKS DEEPCHEM STYLE!
1.1.0,This dictionary holds a mapping {graph: learning_phase}.
1.1.0,A learning phase is a bool tensor used to run Keras models in
1.1.0,either train mode (learning_phase == 1) or test mode (learning_phase == 0).
1.1.0,else: assume learning phase is a placeholder tensor.
1.1.0,need broadcasting
1.1.0,ensure that randomness is conditioned by the Numpy RNG
1.1.0,ensure that randomness is conditioned by the Numpy RNG
1.1.0,TODO(rbharath): Should probably swap this over to tf mode.
1.1.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.1.0,"expects logits, Keras expects probabilities."
1.1.0,scale preds so that the class probas of each sample sum to 1
1.1.0,manual computation of crossentropy
1.1.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.1.0,"expects logits, Keras expects probabilities."
1.1.0,if our output includes timesteps we need to reshape
1.1.0,Arguments
1.1.0,Returns
1.1.0,Note: tf.nn.softmax_cross_entropy_with_logits
1.1.0,"expects logits, Keras expects probabilities."
1.1.0,transform back to logits
1.1.0,"TODO(rbharath): Need to rename this. This makes a variable, not just creates"
1.1.0,a tensor. Confusing with tf.zeros...
1.1.0,Transpose for mul
1.1.0,exclude bias variables
1.1.0,"tf.scalar_summary('Weight Decay Cost', cost)"
1.1.0,TODO(user): gradient clipping (see Minimize)
1.1.0,These properties should have been set
1.1.0,"by the child class, as appropriate."
1.1.0,These properties should be set by the user via keyword arguments.
1.1.0,"note that 'input_dtype', 'input_shape' and 'batch_input_shape'"
1.1.0,are only applicable to input layers: do not pass these keywords
1.1.0,to non-input layers.
1.1.0,In this case we will create an input layer
1.1.0,to insert before the current layer
1.1.0,Update self.losses
1.1.0,In case self.losses isn't settable
1.1.0,(i.e. it's a getter method).
1.1.0,In that case the `losses` property is
1.1.0,auto-computed and shouldn't be set.
1.1.0,Update self._per_input_updates
1.1.0,Updates indexed by None are unconditional
1.1.0,rather than input-dependent
1.1.0,outputs = to_list(self.call(x))
1.1.0,return outputs
1.1.0,TODO(rbharath): Keras uses a global var here to maintain
1.1.0,unique counts. This seems dangerous. How does tensorflow handle?
1.1.0,TODO(rbharath): Support this type of functional API.
1.1.0,If batch size not specified
1.1.0,Input shape
1.1.0,Output shape
1.1.0,References
1.1.0,Not Trainable
1.1.0,Not Trainable
1.1.0,need broadcasting
1.1.0,pick the normalized form of x corresponding to the training phase
1.1.0,sample-wise normalization
1.1.0,from deepchem.nn.model_ops import variable
1.1.0,Assuming convolution kernels (2D or 3D).
1.1.0,"TF kernel shape: (..., input_depth, depth)"
1.1.0,No specific assumptions.
1.1.0,References
1.1.0,References
1.1.0,References
1.1.0,References
1.1.0,Pick the one with the correct shape.
1.1.0,Arguments
1.1.0,Aliases.
1.1.0,!/usr/bin/env python2
1.1.0,-*- coding: utf-8 -*-
1.1.0,Add trainable weights
1.1.0,Add trainable weights
1.1.0,Add trainable weights
1.1.0,Add trainable weights
1.1.0,def test_batch_normalization(self):
1.1.0,"""""""Tests that batch normalization layers can be created."""""""
1.1.0,"Output should be of shape (?, nb_filter)"
1.1.0,"Output should be of shape (batch_size, n_feat)"
1.1.0,Try concatenating the two lists of placeholders
1.1.0,Try concatenating the two lists of placeholders
1.1.0,Fit model on dataset
1.1.0,Fit model on dataset
1.1.0,"Should be an array of size (n_pocket_atoms, 3)"
1.1.0,"coords[triangle, 0] gives the x-dimension of all triangle points"
1.1.0,Take transpose to make sure rows correspond to atoms.
1.1.0,We voxelize so all grids have integral coordinates (convenience)
1.1.0,"If overlap of box with previously generated output boxes, return"
1.1.0,Carry forward mappings
1.1.0,We know that box has at least one atom not in outputs
1.1.0,Current box has been merged into box further down list.
1.1.0,No need to output current box
1.1.0,"protein_coords is (N, 3) tensor"
1.1.0,Load binding pocket model
1.1.0,TODO(rbharath): Shift refined to full once trained.
1.1.0,Fit model on dataset
1.1.0,Create featurizers
1.1.0,"if not ligand_file.endswith("".sdf""):"
1.1.0,"raise ValueError(""Only .sdf ligand files can be featurized."")"
1.1.0,"ligand_basename = os.path.basename(ligand_file).split(""."")[0]"
1.1.0,ligand_mol2 = os.path.join(
1.1.0,"self.base_dir, ligand_basename + "".mol2"")"
1.1.0,
1.1.0,# Write mol2 file for ligand
1.1.0,obConversion = ob.OBConversion()
1.1.0,"conv_out = obConversion.SetInAndOutFormats(str(""sdf""), str(""mol2""))"
1.1.0,ob_mol = ob.OBMol()
1.1.0,"obConversion.ReadFile(ob_mol, str(ligand_file))"
1.1.0,"obConversion.WriteFile(ob_mol, str(ligand_mol2))"
1.1.0,
1.1.0,# Featurize ligand
1.1.0,"mol = Chem.MolFromMol2File(str(ligand_mol2), removeHs=False)"
1.1.0,if mol is None:
1.1.0,"return None, None"
1.1.0,# Default for CircularFingerprint
1.1.0,n_ligand_features = 1024
1.1.0,ligand_features = self.ligand_featurizer.featurize([mol])
1.1.0,
1.1.0,# Featurize pocket
1.1.0,"pockets, pocket_atoms_map, pocket_coords = self.convex_finder.find_pockets("
1.1.0,"protein_file, ligand_file)"
1.1.0,n_pockets = len(pockets)
1.1.0,n_pocket_features = BindingPocketFeaturizer.n_features
1.1.0,
1.1.0,"features = np.zeros((n_pockets, n_pocket_features+n_ligand_features))"
1.1.0,pocket_features = self.pocket_featurizer.featurize(
1.1.0,"protein_file, pockets, pocket_atoms_map, pocket_coords)"
1.1.0,# Note broadcast operation
1.1.0,"features[:, :n_pocket_features] = pocket_features"
1.1.0,"features[:, n_pocket_features:] = ligand_features"
1.1.0,dataset = NumpyDataset(X=features)
1.1.0,pocket_preds = self.model.predict(dataset)
1.1.0,pocket_pred_proba = np.squeeze(self.model.predict_proba(dataset))
1.1.0,
1.1.0,# Find pockets which are active
1.1.0,active_pockets = []
1.1.0,active_pocket_atoms_map = {}
1.1.0,active_pocket_coords = []
1.1.0,for pocket_ind in range(len(pockets)):
1.1.0,#################################################### DEBUG
1.1.0,"# TODO(rbharath): For now, using a weak cutoff. Fix later."
1.1.0,#if pocket_preds[pocket_ind] == 1:
1.1.0,if pocket_pred_proba[pocket_ind][1] > .15:
1.1.0,#################################################### DEBUG
1.1.0,pocket = pockets[pocket_ind]
1.1.0,active_pockets.append(pocket)
1.1.0,active_pocket_atoms_map[pocket] = pocket_atoms_map[pocket]
1.1.0,active_pocket_coords.append(pocket_coords[pocket_ind])
1.1.0,"return active_pockets, active_pocket_atoms_map, active_pocket_coords"
1.1.0,# TODO(LESWING)
1.1.0,"TODO(rbharath, enf): Figure out why pi_stack is slow and cation_pi"
1.1.0,causes segfaults.
1.1.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.1.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.1.0,TODO(rbharath): May want to move this file to S3 so we can ensure it's
1.1.0,always available.
1.1.0,Prepare receptor
1.1.0,Get protein centroid and range
1.1.0,"TODO(rbharath): Need to add some way to identify binding pocket, or this is"
1.1.0,going to be extremely slow!
1.1.0,TODO(rbharath): Handle multiple pockets instead of arbitrarily selecting
1.1.0,first pocket.
1.1.0,Prepare receptor
1.1.0,TODO(rbharath): Generalize this so can support mol2 files as well.
1.1.0,Write Vina conf file
1.1.0,Define locations of log and output files
1.1.0,TODO(rbharath): Let user specify the number of poses required.
1.1.0,TODO(rbharath): Convert the output pdbqt to a pdb file.
1.1.0,Return docked files
1.1.0,Check returned files exist
1.1.0,Check returned files exist
1.1.0,Check returned files exist
1.1.0,Check returned files exist
1.1.0,Check returned files exist
1.1.0,Note this may download autodock Vina...
1.1.0,Note this may download autodock Vina...
1.1.0,Note this may download autodock Vina...
1.1.0,Check returned files exist
1.1.0,Note this may download autodock Vina...
1.1.0,Check returned files exist
1.1.0,"Pocket is of form ((x_min, x_max), (y_min, y_max), (z_min, z_max))"
1.1.0,box1 contained in box2
1.1.0,"box1 in box2, so complete overlap"
1.1.0,"4/5 atoms in box2 in box1, so 80 % overlap"
1.1.0,box2 contains box1
1.1.0,box1 contains box2
1.1.0,"box1 contains box2, box3"
1.1.0,Test that every atom in pocket maps exists
1.1.0,Check that the atoms is actually in protein
1.1.0,Test that every atom in pocket maps exists
1.1.0,Check that the atoms is actually in protein
1.1.0,Add active site to dict
1.1.0,The convention used is that the first task is the metric.
1.1.0,"TODO(rbharath, joegomes): This doesn't seem like it should be hard-coded as"
1.1.0,"an option in the Metric class. Instead, this should be possible to move into"
1.1.0,user-space as a custom task_averager function.
1.1.0,"TODO(rbharath, joegomes): What is this magic number?"
1.1.0,"If there are no nonzero examples, metric is ill-defined."
1.1.0,TODO(rbharath): This has been a major source of bugs. Is there a more
1.1.0,robust characterization of which metrics require class-probs and which
1.1.0,don't?
1.1.0,Reshape to handle 1-d edge cases
1.1.0,ids = df[id_field].values
1.1.0,Set missing data to have weight zero
1.1.0,TODO (ytz) this is a bandage solution to reorder the atoms so
1.1.0,that they're always in the same canonical order. Presumably this
1.1.0,should be correctly implemented in the future for graph mols.
1.1.0,Featurize task results iff they exist.
1.1.0,Filter out examples where featurization failed.
1.1.0,"For prospective data where results are unknown, it makes"
1.1.0,no sense to have y values or weights.
1.1.0,Remove support indices
1.1.0,Remove support indices
1.1.0,Remove support indices
1.1.0,Get task specific entries
1.1.0,Now just get weights for this task
1.1.0,Get task specific entries
1.1.0,Now just get weights for this task
1.1.0,Now just get weights for this task
1.1.0,Now just get weights for this task
1.1.0,Split data into pos and neg lists.
1.1.0,No replacement allowed for supports
1.1.0,Handle one-d vs. non one-d feature matrices
1.1.0,Init the iterator
1.1.0,Set initial iterator state
1.1.0,support = self.supports[task][self.trial_num]
1.1.0,Increment and update logic
1.1.0,Init the iterator
1.1.0,Set initial iterator state
1.1.0,support = self.supports[task][self.trial_num]
1.1.0,Increment and update logic
1.1.0,"By invariant of when this is called, can assume num_samples > 0"
1.1.0,and num_samples < batch_size
1.1.0,Fill in batch arrays
1.1.0,"By invariant of when this is called, can assume num_samples > 0"
1.1.0,and num_samples < batch_size
1.1.0,Fill in batch arrays
1.1.0,The -1 indicates that y will be reshaped to have length -1
1.1.0,"Set labels to be zero, with zero weights"
1.1.0,note that this corresponds to the _construct_metadata column order
1.1.0,if not len(self.metadata_df):
1.1.0,"raise ValueError(""No data in dataset."")"
1.1.0,return next(self.metadata_df.iterrows())[1]['task_names']
1.1.0,Create temp directory to store resharded version
1.1.0,Write data in new shards
1.1.0,Handle spillover from last shard
1.1.0,These columns may be missing is the dataset is unlabelled.
1.1.0,"TODO(rbharath): This happens in tests sometimes, but don't understand why?"
1.1.0,Handle edge case.
1.1.0,if data_dir is None:
1.1.0,data_dir = tempfile.mkdtemp()
1.1.0,The -1 indicates that y will be reshaped to have length -1
1.1.0,"raw_data = (X, y, w, ids)"
1.1.0,Get full dataset in memory
1.1.0,Shuffle in memory
1.1.0,Write shuffled shards out to disk
1.1.0,Shuffle the arrays corresponding to each row in metadata_df
1.1.0,TODO (ytz): Under what condition does this exist but the file itself doesn't?
1.1.0,Handle edge case with empty indices
1.1.0,Find indices which rest in this shard
1.1.0,Need to offset indices to fit within shard_size
1.1.0,Handle the case of datasets with y/w missing
1.1.0,Updating counts
1.1.0,Break when all indices have been used up already
1.1.0,TODO(rbharath): Get rid of * import
1.1.0,Load MUV dataset
1.1.0,Do an approximate comparison since splits are sometimes slightly off from
1.1.0,the exact fraction.
1.1.0,"TODO(rbharath): Transformers don't play nice with reload! Namely,"
1.1.0,reloading will cause the transform to be reapplied. This is undesirable in
1.1.0,almost all cases. Need to understand a method to fix this.
1.1.0,def test_shuffle(self):
1.1.0,"""""""Test that datasets can be merged."""""""
1.1.0,current_dir = os.path.dirname(os.path.realpath(__file__))
1.1.0,dataset_file = os.path.join(
1.1.0,"current_dir, ""../../models/tests/example.csv"")"
1.1.0,featurizer = dc.feat.CircularFingerprint(size=1024)
1.1.0,"tasks = [""log-solubility""]"
1.1.0,loader = dc.data.CSVLoader(
1.1.0,"tasks=tasks, smiles_field=""smiles"", featurizer=featurizer)"
1.1.0,"dataset = loader.featurize(dataset_file, shard_size=2)"
1.1.0,"X_orig, y_orig, w_orig, orig_ids = (dataset.X, dataset.y, dataset.w,"
1.1.0,dataset.ids)
1.1.0,orig_len = len(dataset)
1.1.0,dataset.shuffle(iterations=5)
1.1.0,"X_new, y_new, w_new, new_ids = (dataset.X, dataset.y, dataset.w,"
1.1.0,dataset.ids)
1.1.0,
1.1.0,assert len(dataset) == orig_len
1.1.0,# The shuffling should have switched up the ordering
1.1.0,"assert not np.array_equal(orig_ids, new_ids)"
1.1.0,# But all the same entries should still be present
1.1.0,assert sorted(orig_ids) == sorted(new_ids)
1.1.0,# All the data should have same shape
1.1.0,assert X_orig.shape == X_new.shape
1.1.0,assert y_orig.shape == y_new.shape
1.1.0,assert w_orig.shape == w_new.shape
1.1.0,The shuffling should have switched up the ordering
1.1.0,But all the same entries should still be present
1.1.0,All the data should have same shape
1.1.0,The ids should now store the performed permutation. Check that the
1.1.0,original dataset is recoverable.
1.1.0,The ids should now store the performed permutation. Check that the
1.1.0,original dataset is recoverable.
1.1.0,Set some global variables up top
1.1.0,Featurize emols dataset
1.1.0,Generate dummy dataset
1.1.0,Generate dummy dataset
1.1.0,Generate dummy dataset
1.1.0,Set last n_samples/2 weights to 0
1.1.0,Check that no support elements are sample from zero-weight samples
1.1.0,Generate dummy dataset
1.1.0,Generate dummy dataset
1.1.0,Create support generator
1.1.0,Generate dummy dataset
1.1.0,Create support generator
1.1.0,Generate dummy dataset
1.1.0,Assert all support elements have been removed
1.1.0,Generate dummy dataset
1.1.0,Assert all remove elements have been removed
1.1.0,Generate dummy dataset
1.1.0,Assert all support elements have been removed
1.1.0,Generate dummy dataset
1.1.0,Assert all remove elements have been removed
1.1.0,Generate dummy dataset
1.1.0,Set last n_samples/2 weights to 0
1.1.0,Sample from first n_samples/2 elements for support
1.1.0,Should lie within first n_samples/2 samples only
1.1.0,Generate dummy dataset
1.1.0,Create support generator
1.1.0,Generate dummy dataset
1.1.0,Test on identity matrix
1.1.0,Generate random sparse features dataset
1.1.0,Test edge case with array of all zeros
1.1.0,Test cases where n_samples < 2*n_samples < batch_size
1.1.0,Test cases where n_samples < batch_size
1.1.0,Test case where n_samples == batch_size
1.1.0,Test case for object featurization.
1.1.0,Test case for more complicated object featurization
1.1.0,Test case with multidimensional data
1.1.0,Test cases where n_samples < 2*n_samples < batch_size
1.1.0,Test cases where n_samples < batch_size
1.1.0,Test case where n_samples == batch_size
1.1.0,Test case for object featurization.
1.1.0,Test case for more complicated object featurization
1.1.0,Test case with multidimensional data
1.1.0,Test first resharding worked
1.1.0,Test second resharding worked
1.1.0,Generate data
1.1.0,Generate data
1.1.0,Generate data
1.1.0,Transform it
1.1.0,Transform it
1.1.0,Splits featurized samples into train/test
1.1.0,Splits featurized samples into train/test
1.1.0,Splits featurized samples into train/test
1.1.0,"splittype = ""random"""
1.1.0,Splits featurized samples into train/test
1.1.0,Now perform move
1.1.0,Only for debug!
1.1.0,#Make directories to store the raw and featurized datasets.
1.1.0,Load dataset
1.1.0,Featurize tox21 dataset
1.1.0,###### Do featurization
1.1.0,Do train/valid split.
1.1.0,###### Do singletask load
1.1.0,################# Do comparison
1.1.0,Only for debug!
1.1.0,Set some global variables up top
1.1.0,Make directories to store the raw and featurized datasets.
1.1.0,Load dataset
1.1.0,Featurize tox21 dataset
1.1.0,For debugging purposes
1.1.0,###### Do multitask load
1.1.0,Do train/valid split.
1.1.0,###### Do singletask load
1.1.0,################# Do comparison
1.1.0,"task_type = ""regression"""
1.1.0,coding=utf-8
1.1.0,Note that transformers have to be undone in reversed order
1.1.0,Hack to allow for easy unpickling:
1.1.0,http://stefaanlippens.net/pickleproblem
1.1.0,"One, but not both, transform_X or tranform_y is true"
1.1.0,Use fact that bools add as ints in python
1.1.0,Control for pathological case with no variance.
1.1.0,BalancingTransformer can only transform weights.
1.1.0,Compute weighting factors from dataset.
1.1.0,Ensure dataset is binary
1.1.0,Remove labels with zero weights
1.1.0,self.w = dataset.w
1.1.0,"TODO (flee2): for transform_y, figure out weights"
1.1.0,"print(""y will not be transformed by CDFTransformer, for now."")"
1.1.0,"print(""Cannot undo CDF Transformer, for now."")"
1.1.0,Need this for transform_y
1.1.0,array = np.transpose(array)
1.1.0,"print(""y will not be transformed by PowerTransformer, for now."")"
1.1.0,"print(""Cannot undo Power Transformer, for now."")"
1.1.0,the tf graph here pick up the (K+1) highest similarity values
1.1.0,and their indices
1.1.0,map the indices to labels
1.1.0,generating batch of data by slicing similarity matrix
1.1.0,into 100*reference_dataset_length
1.1.0,concatenate batches of data together
1.1.0,highest similarity is 1: target is in the reference
1.1.0,use the following K points
1.1.0,"highest less than 1: target not in the reference, use top K points"
1.1.0,calculate matrix multiplicatin on slices
1.1.0,concatenate the slices together
1.1.0,list of calculation orders for DAGs
1.1.0,stemming from one specific atom in the molecule
1.1.0,starting from the adjacency list derived by graphconv featurizer
1.1.0,"number of atoms, also number of DAGs"
1.1.0,"DAG on a molecule with k atoms includes k steps of calculation,"
1.1.0,each step calculating graph features for one atom.
1.1.0,`max_atoms` is the maximum number of steps
1.1.0,each iteration generates the DAG starting from atom with index `count`
1.1.0,"list of lists, elements represent the calculation orders"
1.1.0,for atoms in the current graph
1.1.0,starting from the target atom with index `count`
1.1.0,flags of whether the atom is already included in the DAG
1.1.0,atom `count` is in the DAG
1.1.0,recording number of radial propagation steps
1.1.0,"in the fisrt loop, atoms directly connected to `count` will be added"
1.1.0,"into the DAG(radial=0), then atoms two-bond away from `count`"
1.1.0,will be added in the second loop(radial=1).
1.1.0,atoms i-bond away will be added in i-th loop
1.1.0,"when molecules have separate parts, starting from one part,"
1.1.0,it is not possible to include all atoms.
1.1.0,this break quit the loop when going into such condition
1.1.0,reinitialize targets for next iteration
1.1.0,atoms connected to current_atom
1.1.0,generate the dependency map of current DAG
1.1.0,atoms connected to `current_atoms`(and not included in the DAG)
1.1.0,"are added, and will be the `current_atoms` for next iteration."
1.1.0,"into next iteration, finding atoms connected one more bond away"
1.1.0,"DAG starts from the target atom, calculation should go in reverse"
1.1.0,`edge[1]` is the parent of `edge[0]`
1.1.0,all the parents of `edge[1]` is also the parents of `edge[0]`
1.1.0,"after this loop, `parents[i]` includes all parents of atom i"
1.1.0,manually adding the atom index into its parents list
1.1.0,"after this loop, `parents[i][0]` is i, `parents[i][1:]` are all parents of atom i"
1.1.0,atoms with less parents(farther from the target atom) come first.
1.1.0,"graph features of atoms without parents will be first calculated,"
1.1.0,then atoms with more parents can be calculated in order
1.1.0,based on previously calculated graph features.
1.1.0,target atom of this DAG will be calculated in the last step
1.1.0,padding with `max_atoms`
1.1.0,padding
1.1.0,"`parents[i]` is the calculation order for the DAG stemming from atom i,"
1.1.0,which is a max_atoms * max_atoms numpy array after padding
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is a y transformer
1.1.0,Check w is unchanged since this is a y transformer
1.1.0,Check y is now a logarithmic version of itself
1.1.0,Check that untransform does the right thing.
1.1.0,transforming y should raise an exception
1.1.0,transforming w should raise an exception
1.1.0,transforming X should be okay
1.1.0,Check ids are unchanged.
1.1.0,Check y is unchanged since this is a X transformer
1.1.0,Check w is unchanged since this is a y transformer
1.1.0,Check y is now a logarithmic version of itself
1.1.0,Check that untransform does the right thing.
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is a y transformer
1.1.0,Check w is unchanged since this is a y transformer
1.1.0,Check y is now a logarithmic version of itself
1.1.0,Check that untransform does the right thing.
1.1.0,Tests logarithmic data transformer with selection.
1.1.0,Check ids are unchanged.
1.1.0,Check y is unchanged since this is a X transformer
1.1.0,Check w is unchanged since this is a y transformer
1.1.0,Check y is now a logarithmic version of itself
1.1.0,Check that untransform does the right thing.
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is a y transformer
1.1.0,Check w is unchanged since this is a y transformer
1.1.0,"Check that y_t has zero mean, unit std."
1.1.0,Check that untransform does the right thing.
1.1.0,Check ids are unchanged.
1.1.0,Check y is unchanged since this is a X transformer
1.1.0,Check w is unchanged since this is a y transformer
1.1.0,"Check that X_t has zero mean, unit std."
1.1.0,np.set_printoptions(threshold='nan')
1.1.0,Entries with zero std are not normalized
1.1.0,TODO(rbharath): Untransform doesn't work properly for binary feature
1.1.0,vectors. Need to figure out what's wrong here. (low priority)
1.1.0,# Check that untransform does the right thing.
1.1.0,"np.testing.assert_allclose(normalization_transformer.untransform(X_t), X)"
1.1.0,Check ids are unchanged.
1.1.0,Check y is unchanged since this is an X transformer
1.1.0,Check w is unchanged since this is an X transformer
1.1.0,Check X is now holding the proper values when sorted.
1.1.0,Test CDF transformer on Gaussian normal dataset.
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is an y transformer
1.1.0,Check w is unchanged since this is an y transformer
1.1.0,Check y is now holding the proper values when sorted.
1.1.0,Check that untransform does the right thing.
1.1.0,Check ids are unchanged.
1.1.0,Check y is unchanged since this is an X transformer
1.1.0,Check w is unchanged since this is an X transformer
1.1.0,Check X is now holding the proper values when sorted.
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is a y transformer
1.1.0,Check w is unchanged since this is a y transformer
1.1.0,Check y is now holding the proper values when sorted.
1.1.0,Check ids are unchanged.
1.1.0,Check y is unchanged since this is an X transformer
1.1.0,Check w is unchanged since this is an X transformer
1.1.0,Check X is now holding the proper values in each column.
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is an X transformer
1.1.0,Check w is unchanged since this is an X transformer
1.1.0,Check y is now holding the proper values in each column.
1.1.0,Check that untransform does the right thing.
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is a w transformer
1.1.0,Check y is unchanged since this is a w transformer
1.1.0,Assert that entries with zero weight retain zero weight
1.1.0,Check that sum of 0s equals sum of 1s in transformed for each task
1.1.0,Check ids are unchanged.
1.1.0,Check X is unchanged since this is a w transformer
1.1.0,Check y is unchanged since this is a w transformer
1.1.0,Assert that entries with zero weight retain zero weight
1.1.0,Check that sum of 0s equals sum of 1s in transformed for each task
1.1.0,TODO(rbharath): Use standard joblib once old-data has been regenerated.
1.1.0,"If gzipped, need to compute extension again"
1.1.0,Tasks are stored in .sdf.csv file
1.1.0,Structures are stored in .sdf file
1.1.0,First line of user-specified CSV *must* be header.
1.1.0,Try older joblib version for legacy files.
1.1.0,First line of user-specified CSV *must* be header.
1.1.0,First line of user-specified CSV *must* be header.
1.1.0,combine dataframes
1.1.0,working-with-3d-molecules
1.1.0,initial embedding
1.1.0,minimization and pruning
1.1.0,always keep lowest-energy conformer
1.1.0,discard conformers after max_conformers is reached
1.1.0,get RMSD to selected conformers
1.1.0,discard conformers within the RMSD threshold
1.1.0,create a new molecule to hold the chosen conformers
1.1.0,this ensures proper conformer IDs and energy-based ordering
1.1.0,TODO(rbharath): Commenting out this file for now. Will be moved to a new repository.
1.1.0,import nglview
1.1.0,import tempfile
1.1.0,import os
1.1.0,import mdtraj as md
1.1.0,import numpy as np
1.1.0,import tempfile
1.1.0,from rdkit import Chem
1.1.0,from rdkit.Chem import Draw
1.1.0,from itertools import islice
1.1.0,"from IPython.display import Image, HTML, display"
1.1.0,
1.1.0,"def combine_mdtraj(protein, ligand):"
1.1.0,chain = protein.topology.add_chain()
1.1.0,"residue = protein.topology.add_residue(""LIG"", chain, resSeq=1)"
1.1.0,for atom in ligand.topology.atoms:
1.1.0,"protein.topology.add_atom(atom.name, atom.element, residue)"
1.1.0,"protein.xyz = np.hstack([protein.xyz, ligand.xyz])"
1.1.0,protein.topology.create_standard_bonds()
1.1.0,return protein
1.1.0,
1.1.0,def visualize_complex(complex_mdtraj):
1.1.0,"ligand_atoms = [a.index for a in complex_mdtraj.topology.atoms if ""LIG"" in str(a.residue)]"
1.1.0,"binding_pocket_atoms = md.compute_neighbors(complex_mdtraj, 0.5, ligand_atoms)[0]"
1.1.0,binding_pocket_residues = list(set([complex_mdtraj.topology.atom(a).residue.resSeq for a in binding_pocket_atoms]))
1.1.0,binding_pocket_residues = [str(r) for r in binding_pocket_residues]
1.1.0,"binding_pocket_residues = "" or "".join(binding_pocket_residues)"
1.1.0,
1.1.0,traj = nglview.MDTrajTrajectory( complex_mdtraj ) # load file from RCSB PDB
1.1.0,ngltraj = nglview.NGLWidget( traj )
1.1.0,ngltraj.representations = [
1.1.0,"{ ""type"": ""cartoon"", ""params"": {"
1.1.0,"""sele"": ""protein"", ""color"": ""residueindex"""
1.1.0,"} },"
1.1.0,"{ ""type"": ""licorice"", ""params"": {"
1.1.0,"""sele"": ""(not hydrogen) and (%s)"" %  binding_pocket_residues"
1.1.0,"} },"
1.1.0,"{ ""type"": ""ball+stick"", ""params"": {"
1.1.0,"""sele"": ""LIG"""
1.1.0,} }
1.1.0,]
1.1.0,return ngltraj
1.1.0,
1.1.0,def visualize_ligand(ligand_mdtraj):
1.1.0,traj = nglview.MDTrajTrajectory( ligand_mdtraj ) # load file from RCSB PDB
1.1.0,ngltraj = nglview.NGLWidget( traj )
1.1.0,ngltraj.representations = [
1.1.0,"{ ""type"": ""ball+stick"", ""params"": {""sele"": ""all"" } } ]"
1.1.0,return ngltraj
1.1.0,
1.1.0,def convert_lines_to_mdtraj(molecule_lines):
1.1.0,tempdir = tempfile.mkdtemp()
1.1.0,"molecule_file = os.path.join(tempdir, ""molecule.pdb"")"
1.1.0,"with open(molecule_file, ""wb"") as f:"
1.1.0,f.writelines(molecule_lines)
1.1.0,molecule_mdtraj = md.load(molecule_file)
1.1.0,return molecule_mdtraj
1.1.0,
1.1.0,def display_images(filenames):
1.1.0,"""""""Helper to pretty-print images."""""""
1.1.0,imagesList=''.join(
1.1.0,"[""<img style='width: 140px; margin: 0px; float: left; border: 1px solid black;' src='%s' />"""
1.1.0,% str(s) for s in sorted(filenames)])
1.1.0,display(HTML(imagesList))
1.1.0,
1.1.0,"def mols_to_pngs(mols, basename=""test""):"
1.1.0,"""""""Helper to write RDKit mols to png files."""""""
1.1.0,filenames = []
1.1.0,"for i, mol in enumerate(mols):"
1.1.0,"filename = ""%s%d.png"" % (basename, i)"
1.1.0,"Draw.MolToFile(mol, filename)"
1.1.0,filenames.append(filename)
1.1.0,return filenames
1.1.0,TODO(rbharath): This is now simple enough that we should probably get rid of
1.1.0,Evaluator object to avoid clutter.
1.1.0,Compute multitask metrics
1.1.0,Compute multitask metrics
1.1.0,Loosening atol to see if tests stop failing sporadically
1.1.0,!/usr/bin/env python2
1.1.0,-*- coding: utf-8 -*-
1.1.0,a*x + b*y + c*z = dI think that
1.1.0,"self.x, self.y, self.z = x, y, z"
1.1.0,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
1.1.0,TODO(bramsundar): Should this be __copy__?
1.1.0,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
1.1.0,"return np.array([self.x, self.y, self.z])"
1.1.0,TODO(rbharath): Should this be an atom function?
1.1.0,"This line is necessary for babel to work, though many PDBs in"
1.1.0,the PDB would have this line commented out
1.1.0,now atom type (for pdbqt)
1.1.0,"If atomtype is not specified, but atomname is, set atomtype to the"
1.1.0,"first letter of atomname. This heuristic suffices for proteins,"
1.1.0,since no two-letter elements appear in standard amino acids.
1.1.0,Any number needs to be removed from the element name
1.1.0,"this only uses the rightmost three characters, essentially"
1.1.0,removing unique rotamer identification
1.1.0,"The normal vector to plane is n = [a, b, c]"
1.1.0,We first shift by basepoint (a point on given plane) to make math
1.1.0,simpler. basepoint is given by d/||n||^2 * n
1.1.0,The perpendicular component of diff to plane is
1.1.0,(n^T diff / ||n||^2) * n
1.1.0,TODO(LESWING)
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,TODO(enf): make array index checking not a try-catch statement.
1.1.0,Get the degree id list (which corrects for min_deg)
1.1.0,Get the size of each degree block
1.1.0,Get the the start indices for items in each block
1.1.0,Get the node indices when they are reset when the degree changes
1.1.0,Convert to numpy array
1.1.0,Reorder old atom_features
1.1.0,Reorder old deg lists
1.1.0,Sort membership
1.1.0,Create old to new dictionary. not exactly intuitive
1.1.0,Reorder adjacency lists
1.1.0,Get numpy version of degree list for indexing
1.1.0,"Initialize adj_lists, which supports min_deg = 1 only"
1.1.0,Parse as deg separated
1.1.0,Get indices corresponding to the current degree
1.1.0,Extract and save adjacency list for the current degree
1.1.0,Construct the slice information
1.1.0,Get the cumulative indices after the first index
1.1.0,Set indices with zero sized slices to zero to avoid indexing errors
1.1.0,TODO(rbharath): Can this be removed?
1.1.0,Use random insted of zeros to prevent weird issues with summing to zero
1.1.0,Get atoms by degree
1.1.0,stack the atoms
1.1.0,Sort all atoms by degree.
1.1.0,"Get the size of each atom list separated by molecule id, then by degree"
1.1.0,Get the final size of each degree block
1.1.0,"Get the index at which each degree starts, not resetting after each degree"
1.1.0,And not stopping at any speciic molecule
1.1.0,"Get the tensorflow object required for slicing (deg x 2) matrix, with the"
1.1.0,first column telling the start indices of each degree block and the
1.1.0,second colum telling the size of each degree block
1.1.0,Input for tensorflow
1.1.0,Determines the membership (atom i belongs to membership[i] molecule)
1.1.0,"Get the index at which each deg starts, resetting after each degree"
1.1.0,(deg x num_mols) matrix describing the start indices when you count up the atoms
1.1.0,"in the final representation, stopping at each molecule,"
1.1.0,resetting every time the degree changes
1.1.0,Gets the degree resetting block indices for the atoms in each molecule
1.1.0,"Here, the indices reset when the molecules change, and reset when the"
1.1.0,degree changes
1.1.0,Get the degree id lookup list. It allows us to search for the degree of a
1.1.0,molecule mol_id with corresponding atom mol_atom_id using
1.1.0,"deg_id_lists[mol_id,mol_atom_id]"
1.1.0,This is used for convience in the following function (explained below)
1.1.0,Get the degree id (corrected for min_deg) of the considered atom
1.1.0,Return the final index of atom mol_atom_id in molecule mol_id.  Using
1.1.0,"the degree of this atom, must find the index in the molecule's original"
1.1.0,"degree block corresponding to degree id deg_id (second term), and then"
1.1.0,calculate which index this degree block ends up in the final
1.1.0,representation (first term). The sum of the two is the final indexn
1.1.0,Initialize the new degree separated adjacency lists
1.1.0,Update the old adjcency lists with the new atom indices and then combine
1.1.0,all together
1.1.0,Iterate through all the molecules
1.1.0,Get the adjacency lists for this molecule and current degree id
1.1.0,"Correct all atom indices to the final indices, and then save the"
1.1.0,results into the new adjacency lists
1.1.0,Increment once row is done
1.1.0,Get the final aggregated molecule
1.1.0,RDKit stores atomic coordinates in Angstrom. Atomic unit of length is the
1.1.0,bohr (1 bohr = 0.529177 Angstrom). Converting units makes gradient calculation
1.1.0,consistent with most QM software packages.
1.1.0,Type of data created by this featurizer
1.1.0,TODO(rbharath): Should this return a list?
1.1.0,Type of data created by this featurizer
1.1.0,generate SMILES for fragments
1.1.0,Initalize with 1
1.1.0,Allow 0 index to correspond to null molecule 1
1.1.0,Correct for null
1.1.0,"print(6-k-1, id)"
1.1.0,Correct for last one
1.1.0,first `bt_len` features are bond features(if applicable)
1.1.0,`bt_len`-th feature is if the pair of atoms are in the same ring
1.1.0,graph distance between two atoms
1.1.0,atoms `radial` bonds away from `a1`
1.1.0,atoms less than `radial` bonds away
1.1.0,find atoms `radial`+1 bonds away
1.1.0,"Since ConvMol is an object and not a numpy array, need to set dtype to"
1.1.0,object.
1.1.0,Get the node features
1.1.0,Stack nodes into an array
1.1.0,Get bond lists with reverse edges included
1.1.0,Get canonical adjacency list
1.1.0,Set dtype
1.1.0,Atom features
1.1.0,Stack nodes into an array
1.1.0,Get bond lists
1.1.0,Get canonical adjacency list
1.1.0,Calculate pair features
1.1.0,atom_name is of format RESX-ATOMTYPE
1.1.0,where X is a 1 to 4 digit number
1.1.0,list-of-available-descriptors.
1.1.0,(ytz): This is done to avoid future compatibility issues like inclusion of
1.1.0,the 3D descriptors or changing the feature size.
1.1.0,check for separate count and SMILES entries for each fragment
1.1.0,"Note there is a central carbon of degree 4, with 3 carbons and"
1.1.0,one nitrogen of degree 1 (connected only to central carbon).
1.1.0,5 atoms in compound
1.1.0,Get the adjacency lists grouped by degree
1.1.0,The 4 outer atoms connected to central carbon
1.1.0,Central carbon connected to everything else.
1.1.0,Only one carbon
1.1.0,"No bonds, so degree adjacency lists are empty"
1.1.0,3 carbonds in alkane
1.1.0,Outer two carbonds are connected to central carbon
1.1.0,Central carbon connected to outer two
1.1.0,"TODO(rbharath, joegomes): Why does AtomicCoordinates return a list? Is"
1.1.0,this expected behavior? Need to think about API.
1.1.0,Do a manual distance computation and make
1.1.0,Test with cutoff 0 angstroms. There should be no neighbors in this case.
1.1.0,Test with cutoff 100 angstroms. Everything should be neighbors now.
1.1.0,Do a manual distance computation and ensure that selected neighbor is
1.1.0,closest since we set max_num_neighbors = 1
1.1.0,Splits featurized samples into train/test
1.1.0,Artificial feature array.
1.1.0,0 atoms of degree 0
1.1.0,0 atoms of degree 1
1.1.0,4 atoms of degree 2
1.1.0,0 atoms of degree 3
1.1.0,0 atoms of degree 4
1.1.0,0 atoms of degree 5
1.1.0,0 atoms of degree 6
1.1.0,0 atoms of degree 7
1.1.0,0 atoms of degree 8
1.1.0,0 atoms of degree 9
1.1.0,0 atoms of degree 10
1.1.0,atom 4 has 0 neighbors
1.1.0,atom 0 has 2 neighbors
1.1.0,atom 1 has 2 neighbors
1.1.0,atom 2 has 2 neighbors
1.1.0,atom 3 has 3 neighbors.
1.1.0,Verify that atom features have been sorted by atom degree.
1.1.0,Sorting is done by atom degree as before. So the ordering goes
1.1.0,"4, 0, 1, 2, 3 now in terms of the original ordering. The mapping"
1.1.0,from new position to old position is
1.1.0,"{(4, 0), (0, 1), (1, 2), (2, 3), (3, 4)}. Check that adjacency"
1.1.0,list respects this reordering and returns correct adjacency list.
1.1.0,### First example molecule
1.1.0,Artificial feature array.
1.1.0,### Second example molecule
1.1.0,## Third example molecule
1.1.0,Test agglomerate molecule method
1.1.0,No atoms of degree 0
1.1.0,3 atoms of degree 1
1.1.0,8 atoms of degree 2
1.1.0,1 atom of degree 3
1.1.0,0 atoms of degree 4
1.1.0,0 atoms of degree 5
1.1.0,Check that atoms are only connected to themselves.
1.1.0,Check that there's one atom of each degree.
1.1.0,assumes that every array is of the same dimension
1.1.0,rem_dataset is remaining portion of dataset
1.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.1.0,to k-1.
1.1.0,returns list of per column sum of non zero elements
1.1.0,Compute number of actives needed per task.
1.1.0,loop through each column and obtain index required to splice out for
1.1.0,required fraction of hits
1.1.0,Find the first index where the cumulative number of actives equals
1.1.0,the actives_count
1.1.0,Note that np.where tells us last index required to exceed
1.1.0,"actives_count, so we actually want the following location"
1.1.0,TODO(rbharath): Refactor this split method to match API of other splits (or
1.1.0,potentially refactor those to match this.
1.1.0,Handle edge case where frac_split is 1
1.1.0,Create weight matrices fpor two haves.
1.1.0,copy over up to required index for weight first_split
1.1.0,check out if any rows in either w_1 or w_2 are just zeros
1.1.0,"Obtain original x, y, and w arrays and shuffle"
1.1.0,calculate percent split for valid (out of test and valid)
1.1.0,"split test data into valid and test, treating sub test set also as sparse"
1.1.0,rem_dataset is remaining portion of dataset
1.1.0,Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up
1.1.0,to k-1.
1.1.0,JSG Assert that split fractions can be written as proper fractions over 10.
1.1.0,This can be generalized in the future with some common demoninator determination.
1.1.0,This will work for 80/20 train/test or 80/10/10 train/valid/test (most use cases).
1.1.0,Append remaining examples to train
1.1.0,Sort by increasing MW
1.1.0,(ytz): this is directly copypasta'd from Greg Landrum's clustering example.
1.1.0,for m_idx in cluster:
1.1.0,"continue until we find an active in all the tasks, otherwise we can't"
1.1.0,compute a meaningful AUC
1.1.0,"TODO (ytz): really, we want at least one active and inactive in both scenarios."
1.1.0,TODO (Ytz): for regression tasks we'd stop after only one cluster.
1.1.0,Sort from largest to smallest scaffold sets
1.1.0,All datasets share features and identifiers by assumption.
1.1.0,TODO(rbharath): Get rid of * import
1.1.0,Note that the extra task goes to test
1.1.0,Number tasks per fold
1.1.0,Find the tasks that correspond to this test fold
1.1.0,Assert that all arrays look like they should
1.1.0,TODO(rbharath): The IndexSplitter() had a bug with splitting sharded
1.1.0,data. Make a test for properly splitting of sharded data. Perhaps using
1.1.0,reshard() to handle this?
1.1.0,Verify lengths is 10/k == 2
1.1.0,Verify that compounds in this fold are subset of original compounds
1.1.0,Verify that no two folds have overlapping compounds.
1.1.0,Verify lengths is 10/k == 2
1.1.0,Verify that compounds in this fold are subset of original compounds
1.1.0,Verify that no two folds have overlapping compounds.
1.1.0,Verify lengths is 10/k == 2
1.1.0,Verify that compounds in this fold are subset of original compounds
1.1.0,Verify that no two folds have overlapping compounds.
1.1.0,Test singletask case.
1.1.0,The split index should partition dataset in half.
1.1.0,Test singletask case.
1.1.0,Test case where some weights are zero (i.e. masked)
1.1.0,Set half the positives to have zero weight
1.1.0,There are 10 nonzero actives.
1.1.0,"The split index should partition this into half, so expect 5"
1.1.0,The split index should partition dataset in half.
1.1.0,Mask half the examples
1.1.0,The split index should partition dataset in half.
1.1.0,Test singletask case.
1.1.0,Should have split cleanly in half (picked random seed to ensure this)
1.1.0,Check positives are correctly distributed
1.1.0,Verify lengths is 100/k == 20
1.1.0,Note: This wouldn't work for multitask str
1.1.0,assert len(fold_dataset) == n_samples/K
1.1.0,Verify that each fold has n_positives/K = 4 positive examples.
1.1.0,Verify that compounds in this fold are subset of original compounds
1.1.0,Verify that no two folds have overlapping compounds.
1.1.0,sparsity is determined by number of w weights that are 0 for a given
1.1.0,task structure of w np array is such that each row corresponds to a
1.1.0,sample. The loaded sparse dataset has many rows with only zeros
1.1.0,verify that there are no rows (samples) in weights matrix w
1.1.0,that have no hits.
1.1.0,Path to save checkpoint files
1.1.0,first layer in model: check that it is an input layer
1.1.0,Add losses to graph
1.1.0,Loss for each batch element
1.1.0,Loss should be a float
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Save an initial checkpoint.
1.1.0,TODO(rbharath): Don't support example weighting yet.
1.1.0,Always save a final checkpoint when complete.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Arguments
1.1.0,Returns
1.1.0,Arguments
1.1.0,Returns
1.1.0,Arguments
1.1.0,Returns
1.1.0,Arguments
1.1.0,Returns
1.1.0,task_metadata_rows = {task: [] for task in tasks}
1.1.0,Extract those datapoints which are present for this task
1.1.0,Loading is done on-the-fly
1.1.0,TODO(rbharath/enf): We need a structured way to deal with potential GPU
1.1.0,memory overflows.
1.1.0,Discard any padded predictions
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,Special case to handle singletasks.
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,"Shape (N_atoms, M_nbrs, ndim)"
1.1.0,"Shape (N_atoms, M_nbrs, ndim)"
1.1.0,"Shape (N_atoms, M_nbrs)"
1.1.0,TODO(rbharath): Note sure if this layer can be called with __call__
1.1.0,"meaningfully, so not going to support that functionality for now."
1.1.0,"in_layers = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.1.0,Generate the nb_affine weights and biases
1.1.0,Extract atom_features
1.1.0,Extract graph topology
1.1.0,Perform the mol conv
1.1.0,"atom_features = graph_conv(atom_features, deg_adj_lists, deg_slice,"
1.1.0,"self.max_deg, self.min_deg, self.W_list,"
1.1.0,self.b_list)
1.1.0,Sum all neighbors using adjacency matrix
1.1.0,Get collection of modified atom features
1.1.0,Obtain relevant atoms for this degree
1.1.0,Get self atoms
1.1.0,Apply hidden affine to relevant atoms and append
1.1.0,Determine the min_deg=0 case
1.1.0,Only use the self layer
1.1.0,Combine all atoms back into the list
1.1.0,Tensorflow correctly processes empty lists when using concat
1.1.0,"Sum along neighbors as well as self, and store"
1.1.0,Perform the mol gather
1.1.0,"atom_features = graph_pool(atom_features, deg_adj_lists, deg_slice,"
1.1.0,"self.max_degree, self.min_degree)"
1.1.0,Tensorflow correctly processes empty lists when using concat
1.1.0,Get self atoms
1.1.0,Expand dims
1.1.0,always deg-1 for deg_adj_lists
1.1.0,"x = [atom_features, deg_slice, membership, deg_adj_list placeholders...]"
1.1.0,Extract graph topology
1.1.0,Perform the mol gather
1.1.0,Obtain the partitions for each of the molecules
1.1.0,Sum over atoms for each molecule
1.1.0,Get the final sparse representations
1.1.0,Number of rotatable bonds
1.1.0,TODO(rbharath): Vina actually sets this per-molecule. See if makes
1.1.0,a difference.
1.1.0,TODO(rbharath): This layer shouldn't be neighbor-listing. Make
1.1.0,neighbors lists an argument instead of a part of this layer.
1.1.0,"Shape (N, M)"
1.1.0,"Shape (N, M)"
1.1.0,"Shape (N, M)"
1.1.0,Number of grid cells
1.1.0,TODO(rbharath): Support batching
1.1.0,"Shape (n_cells, ndim)"
1.1.0,"List of length N_atoms, each element of different length uniques_i"
1.1.0,"List of length N_atoms, each element of different length uniques_i"
1.1.0,"List of length N_atoms, each a tensor of shape"
1.1.0,"(uniques_i, ndim)"
1.1.0,Add phantom atoms that exist far outside the box
1.1.0,"List of length N_atoms, each of shape (1, ndim)"
1.1.0,TODO(rbharath): How does distance need to be modified here to
1.1.0,account for periodic boundary conditions?
1.1.0,List of length N_atoms each of shape (M_nbrs)
1.1.0,"N_atoms elts of size (M_nbrs,) each"
1.1.0,"Shape (N_atoms, 1)"
1.1.0,Find M_nbrs atoms closest to each cell
1.1.0,"Shape (n_cells, M_nbrs)"
1.1.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.1.0,"conditions, so does wrapround. O(constant)"
1.1.0,"Shape (n_cells, n_nbr_cells)"
1.1.0,"Shape (N_atoms, n_nbr_cells)"
1.1.0,"Shape (N_atoms, n_nbr_cells, M_nbrs)"
1.1.0,"Shape (N_atoms, n_nbr_cells*M_nbrs)"
1.1.0,"List of length N_atoms, each element length uniques_i"
1.1.0,TODO(rbharath): FRAGILE! Uses fact that identity seems to be the first
1.1.0,element removed to remove self from list of neighbors. Need to verify
1.1.0,this holds more broadly or come up with robust alternative.
1.1.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.1.0,"Shape (N_atoms*n_cells, ndim) after tile"
1.1.0,Shape (N_atoms*n_cells)
1.1.0,"Shape (n_cells, N_atoms)"
1.1.0,Find k atoms closest to this cell. Notice negative sign since
1.1.0,tf.nn.top_k returns *largest* not smallest.
1.1.0,"Tensor of shape (n_cells, M_nbrs)"
1.1.0,"Tile both cells and coords to form arrays of size (N_atoms*n_cells, ndim)"
1.1.0,"Shape (N_atoms*n_cells, 1) after tile"
1.1.0,9 neighbors in 2-space
1.1.0,TODO(rbharath): Shoddy handling of higher dimensions...
1.1.0,Number of cells for cube in 3-space is
1.1.0,TODO(rbharath): Do we need to handle periodic boundary conditions
1.1.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.1.0,"looking for n_nbr_cells neighbors, which isn't right for boundary cells in"
1.1.0,the cube.
1.1.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.1.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.1.0,"Tile (a, a, a, b, b, b, etc.)"
1.1.0,"Tile (a, b, c, a, b, c, ...)"
1.1.0,N: Maximum number of atoms
1.1.0,M: Maximum number of neighbors
1.1.0,d: Number of coordinates/features/filters
1.1.0,B: Batch Size
1.1.0,We apply the radial pooling filter before atom type conv
1.1.0,to reduce computation
1.1.0,!/usr/bin/env python2
1.1.0,-*- coding: utf-8 -*-
1.1.0,"for atom i in a molecule m, this step multiplies together distance info of atom pair(i,j)"
1.1.0,and embeddings of atom j(both gone through a hidden layer)
1.1.0,"for atom i, sum the influence from all other atom j in the molecule"
1.1.0,number of inputs each step
1.1.0,Add trainable weights
1.1.0,"each atom corresponds to a graph, which is represented by the `max_atoms*max_atoms` int32 matrix of index"
1.1.0,each gragh include `max_atoms` of steps(corresponding to rows) of calculating graph features
1.1.0,target atoms for each step: (batch_size*max_atoms) * max_atoms
1.1.0,initialize graph features for each graph
1.1.0,initialize graph features for each graph
1.1.0,another row of zeros is generated for padded dummy atoms
1.1.0,`count`-th step
1.1.0,extracting atom features of target atoms: (batch_size*max_atoms) * n_atom_features
1.1.0,generating index for graph features used in the inputs
1.1.0,"extracting graph features for parents of the target atoms, then flatten"
1.1.0,shape: (batch_size*max_atoms) * [(max_atoms-1)*n_graph_features]
1.1.0,concat into the input tensor: (batch_size*max_atoms) * n_inputs
1.1.0,DAGgraph_step maps from batch_inputs to a batch of graph_features
1.1.0,of shape: (batch_size*max_atoms) * n_graph_features
1.1.0,representing the graph features of target atoms in each graph
1.1.0,index for targe atoms
1.1.0,update the graph features for target atoms
1.1.0,Add trainable weights
1.1.0,Extract atom_features
1.1.0,Extract atom_features
1.1.0,sum all graph outputs
1.1.0,Layer Management
1.1.0,Singular place to hold Tensor objects which don't serialize
1.1.0,These have to be reconstructed on restoring from pickle
1.1.0,See TensorGraph._get_tf() for more details on lazy construction
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Arguments
1.1.0,Returns
1.1.0,Arguments
1.1.0,Returns
1.1.0,Remove out_tensor from the object to be pickled
1.1.0,Pickle itself
1.1.0,add out_tensor back to everyone
1.1.0,"This isn't a meaningful loss, but just for test"
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,1 and 2 are nbrs. 8 and 9 are nbrs
1.1.0,"Now an (N, M) shape"
1.1.0,TODO(rbharath): Move this into a model directly
1.1.0,def test_vina(self):
1.1.0,"""""""Test that vina graph can be constructed in TensorGraph."""""""
1.1.0,N_protein = 4
1.1.0,N_ligand = 1
1.1.0,N_atoms = 5
1.1.0,M_nbrs = 2
1.1.0,ndim = 3
1.1.0,start = 0
1.1.0,stop = 4
1.1.0,nbr_cutoff = 1
1.1.0,"X_prot = NumpyDataset(start + np.random.rand(N_protein, ndim) * (stop -"
1.1.0,start))
1.1.0,"X_ligand = NumpyDataset(start + np.random.rand(N_ligand, ndim) * (stop -"
1.1.0,start))
1.1.0,y = NumpyDataset(np.random.rand(
1.1.0,"1,))"
1.1.0,"# TODO(rbharath): Mysteriously, the actual atom types aren't"
1.1.0,"# used in the current implementation. This is obviously wrong, but need"
1.1.0,# to dig out why this is happening.
1.1.0,"prot_coords = Feature(shape=(N_protein, ndim))"
1.1.0,"ligand_coords = Feature(shape=(N_ligand, ndim))"
1.1.0,"labels = Label(shape=(1,))"
1.1.0,"coords = Concat(in_layers=[prot_coords, ligand_coords], axis=0)"
1.1.0,"#prot_Z = Feature(shape=(N_protein,), dtype=tf.int32)"
1.1.0,"#ligand_Z = Feature(shape=(N_ligand,), dtype=tf.int32)"
1.1.0,"#Z = Concat(in_layers=[prot_Z, ligand_Z], axis=0)"
1.1.0,"# Now an (N, M) shape"
1.1.0,nbr_list = NeighborList(
1.1.0,"N_protein + N_ligand,"
1.1.0,"M_nbrs,"
1.1.0,"ndim,"
1.1.0,"nbr_cutoff,"
1.1.0,"start,"
1.1.0,"stop,"
1.1.0,in_layers=[coords])
1.1.0,"# Shape (N, M)"
1.1.0,dists = InteratomicL2Distances(
1.1.0,"N_protein + N_ligand, M_nbrs, ndim, in_layers=[coords, nbr_list])"
1.1.0,repulsion = VinaRepulsion(in_layers=[dists])
1.1.0,hydrophobic = VinaHydrophobic(in_layers=[dists])
1.1.0,hbond = VinaHydrogenBond(in_layers=[dists])
1.1.0,gauss_1 = VinaGaussianFirst(in_layers=[dists])
1.1.0,gauss_2 = VinaGaussianSecond(in_layers=[dists])
1.1.0,"# Shape (N, M)"
1.1.0,interactions = WeightedLinearCombo(
1.1.0,"in_layers=[repulsion, hydrophobic, hbond, gauss_1, gauss_2])"
1.1.0,"# Shape (N, M)"
1.1.0,"thresholded = Cutoff(in_layers=[dists, interactions])"
1.1.0,"# Shape (N, M)"
1.1.0,free_energies = VinaNonlinearity(in_layers=[thresholded])
1.1.0,free_energy = ReduceSum(in_layers=[free_energies])
1.1.0,"loss = L2Loss(in_layers=[free_energy, labels])"
1.1.0,"databag = Databag({prot_coords: X_prot, ligand_coords: X_ligand, labels: y})"
1.1.0,"tg = dc.models.TensorGraph(learning_rate=0.1, use_queue=False)"
1.1.0,tg.set_loss(loss)
1.1.0,tg.fit_generator(databag.iterbatches(epochs=1))
1.1.0,TODO(rbharath): This test should pass. Fix it!
1.1.0,def test_graph_pool(self):
1.1.0,"""""""Test that GraphPool can be invoked."""""""
1.1.0,out_channels = 2
1.1.0,"n_atoms = 4 # In CCC and C, there are 4 atoms"
1.1.0,"raw_smiles = ['CCC', 'C']"
1.1.0,mols = [rdkit.Chem.MolFromSmiles(s) for s in raw_smiles]
1.1.0,featurizer = ConvMolFeaturizer()
1.1.0,mols = featurizer.featurize(mols)
1.1.0,multi_mol = ConvMol.agglomerate_mols(mols)
1.1.0,atom_features = multi_mol.get_atom_features()
1.1.0,degree_slice = multi_mol.deg_slice
1.1.0,membership = multi_mol.membership
1.1.0,deg_adjs = multi_mol.get_deg_adjacency_lists()[1:]
1.1.0,with self.test_session() as sess:
1.1.0,"atom_features = tf.convert_to_tensor(atom_features, dtype=tf.float32)"
1.1.0,"degree_slice = tf.convert_to_tensor(degree_slice, dtype=tf.int32)"
1.1.0,"membership = tf.convert_to_tensor(membership, dtype=tf.int32)"
1.1.0,deg_adjs_tf = []
1.1.0,for deg_adj in deg_adjs:
1.1.0,"deg_adjs_tf.append(tf.convert_to_tensor(deg_adj, dtype=tf.int32))"
1.1.0,"args = [atom_features, degree_slice, membership] + deg_adjs_tf"
1.1.0,out_tensor = GraphPool(out_channels)(*args)
1.1.0,sess.run(tf.global_variables_initializer())
1.1.0,out_tensor = out_tensor.eval()
1.1.0,"assert out_tensor.shape == (n_atoms, out_channels)"
1.1.0,TODO(rbharath): Why is it 2*n_features instead of n_features?
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,number of atoms in each molecule
1.1.0,index of pair features
1.1.0,number of pairs for each atom
1.1.0,atom features
1.1.0,pair features
1.1.0,calculation orders for a batch of molecules
1.1.0,padding atom features vector of each molecule with 0
1.1.0,import tensorflow as tf
1.1.0,from deepchem.models.tensorgraph.tensor_graph import MultiTaskTensorGraph
1.1.0,"from deepchem.models.tensorgraph.layers import Input, Dense, Concat, SoftMax, SoftMaxCrossEntropy, Layer"
1.1.0,
1.1.0,
1.1.0,class WeightedError(Layer):
1.1.0,
1.1.0,"def __call__(self, *parents):"
1.1.0,"entropy, weights = parents[0], parents[1]"
1.1.0,self.out_tensor = tf.reduce_sum(entropy.out_tensor * weights.out_tensor)
1.1.0,return self.out_tensor
1.1.0,
1.1.0,
1.1.0,"def tensorGraphMultitaskClassifier(n_tasks,"
1.1.0,"n_features,"
1.1.0,"layer_sizes=[500],"
1.1.0,"bypass_layer_sizes=[100],"
1.1.0,model_dir=None):
1.1.0,""""""""
1.1.0,TODO(LESWING) Add Dropout and regularization
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,n_tasks
1.1.0,n_features
1.1.0,layer_sizes
1.1.0,bypass_layer_sizes
1.1.0,model_dir
1.1.0,
1.1.0,Returns
1.1.0,-------
1.1.0,
1.1.0,""""""""
1.1.0,g = MultiTaskTensorGraph(model_dir=model_dir)
1.1.0,"in_layer = Input(shape=(None, n_features), name=""FEATURE"")"
1.1.0,g.add_layer(in_layer)
1.1.0,g.add_feature(in_layer)
1.1.0,
1.1.0,# Shared Dense Layers
1.1.0,prev_layer = in_layer
1.1.0,dense_layers = []
1.1.0,for i in range(len(layer_sizes)):
1.1.0,dense = Dense(
1.1.0,"out_channels=layer_sizes[i],"
1.1.0,"name=""SDENSE%s"" % i,"
1.1.0,activation_fn=tf.nn.relu)
1.1.0,"g.add_layer(dense, parents=[prev_layer])"
1.1.0,dense_layers.append(dense)
1.1.0,prev_layer = dense
1.1.0,
1.1.0,# Individual Bypass Layers
1.1.0,costs = []
1.1.0,for task in range(n_tasks):
1.1.0,prev_layer = in_layer
1.1.0,for i in range(len(bypass_layer_sizes)):
1.1.0,dense = Dense(
1.1.0,"out_channels=bypass_layer_sizes[i], name=""BDENSE%s_%s"" % (i, task))"
1.1.0,"g.add_layer(dense, parents=[prev_layer])"
1.1.0,prev_layer = dense
1.1.0,"joined_layer = Concat(name=""JOIN%s"" % task)"
1.1.0,"g.add_layer(joined_layer, parents=[dense_layers[-1], prev_layer])"
1.1.0,
1.1.0,"classification = Dense(out_channels=2, name=""GUESS%s"" % task)"
1.1.0,"g.add_layer(classification, parents=[joined_layer])"
1.1.0,
1.1.0,"softmax = SoftMax(name=""SOFTMAX%s"" % task)"
1.1.0,"g.add_layer(softmax, parents=[classification])"
1.1.0,g.add_output(softmax)
1.1.0,
1.1.0,"label = Input(shape=(None, 2), name=""LABEL%s"" % task)"
1.1.0,g.add_layer(label)
1.1.0,g.add_label(label)
1.1.0,
1.1.0,"cost = SoftMaxCrossEntropy(name=""COST%s"" % task)"
1.1.0,"g.add_layer(cost, parents=[label, classification])"
1.1.0,costs.append(cost)
1.1.0,
1.1.0,"entropy = Concat(name=""ENT"")"
1.1.0,"g.add_layer(entropy, parents=costs)"
1.1.0,
1.1.0,"task_weights = Input(shape=(None, n_tasks), name=""W"")"
1.1.0,g.add_layer(task_weights)
1.1.0,g.set_task_weights(task_weights)
1.1.0,
1.1.0,"loss = WeightedError(name=""ERROR"")"
1.1.0,"g.add_layer(loss, parents=[entropy, task_weights])"
1.1.0,g.set_loss(loss)
1.1.0,
1.1.0,return g
1.1.0,update model with best param
1.1.0,Find optimal n_estimators based on original learning_rate
1.1.0,and early_stopping_rounds
1.1.0,"Since test size is 20%, when retrain model to whole data, expect"
1.1.0,n_estimator increased to 1/0.8 = 1.25 time.
1.1.0,Make sure user specified params are in the grid.
1.1.0,Change params back original params
1.1.0,TODO (LESWING) Lazy Load
1.1.0,TODO (LESWING) Lazy Load
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Check same predictions are made.
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Load trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Load trained model
1.1.0,Eval model on train
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Fit trained model
1.1.0,Eval model on train/test
1.1.0,Fit trained model
1.1.0,Eval model on train/test
1.1.0,Fit trained model
1.1.0,Eval model on train/test
1.1.0,Test Parameter getting and setting
1.1.0,Fit trained model
1.1.0,Eval model on train/test
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,"TODO(rbharath): This breaks with optimizer=""momentum"". Why?"
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,n_samples = 100
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,n_samples = 100
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Gather Projection
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,Load mini log-solubility dataset.
1.1.0,Add layers
1.1.0,"output will be (n_atoms, 64)"
1.1.0,Need to add batch-norm separately to test/support due to differing
1.1.0,shapes.
1.1.0,"output will be (n_atoms, 64)"
1.1.0,"output will be (n_atoms, 64)"
1.1.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.1.0,n_pos/n_neg accordingly.
1.1.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.1.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.1.0,can measure model has memorized support).  Replacement is turned off to
1.1.0,ensure that support contains full training set. This checks that the
1.1.0,model has mastered memorization of provided support.
1.1.0,#################################################### DEBUG
1.1.0,TODO(rbharath): Check if something went wrong here...
1.1.0,Measure performance on 0-th task.
1.1.0,assert scores[0] > .9
1.1.0,#################################################### DEBUG
1.1.0,Load mini log-solubility dataset.
1.1.0,Add layers
1.1.0,"output will be (n_atoms, 64)"
1.1.0,Need to add batch-norm separately to test/support due to differing
1.1.0,shapes.
1.1.0,"output will be (n_atoms, 64)"
1.1.0,"output will be (n_atoms, 64)"
1.1.0,Apply an attention lstm layer
1.1.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.1.0,n_pos/n_neg accordingly.
1.1.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.1.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.1.0,can measure model has memorized support).  Replacement is turned off to
1.1.0,ensure that support contains full training set. This checks that the
1.1.0,model has mastered memorization of provided support.
1.1.0,Measure performance on 0-th task.
1.1.0,#################################################### DEBUG
1.1.0,TODO(rbharath): Check if something went wrong here...
1.1.0,Measure performance on 0-th task.
1.1.0,assert scores[0] > .85
1.1.0,#################################################### DEBUG
1.1.0,Load mini log-solubility dataset.
1.1.0,Add layers
1.1.0,"output will be (n_atoms, 64)"
1.1.0,Need to add batch-norm separately to test/support due to differing
1.1.0,shapes.
1.1.0,"output will be (n_atoms, 64)"
1.1.0,"output will be (n_atoms, 64)"
1.1.0,Apply a residual lstm layer
1.1.0,"Fit trained model. Dataset has 6 positives and 4 negatives, so set"
1.1.0,n_pos/n_neg accordingly.
1.1.0,"Eval model on train. Dataset has 6 positives and 4 negatives, so set"
1.1.0,n_pos/n_neg accordingly. Note that support is *not* excluded (so we
1.1.0,can measure model has memorized support).  Replacement is turned off to
1.1.0,ensure that support contains full training set. This checks that the
1.1.0,model has mastered memorization of provided support.
1.1.0,Measure performance on 0-th task.
1.1.0,#################################################### DEBUG
1.1.0,TODO(rbharath): Check if something went wrong here...
1.1.0,Measure performance on 0-th task.
1.1.0,assert scores[0] > .9
1.1.0,#################################################### DEBUG
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on train
1.1.0,def test_singletask_to_multitask_classification(self):
1.1.0,n_features = 10
1.1.0,n_tasks = 17
1.1.0,tasks = range(n_tasks)
1.1.0,# Define train dataset
1.1.0,n_train = 100
1.1.0,"X_train = np.random.rand(n_train, n_features)"
1.1.0,"y_train = np.random.randint(2, size=(n_train, n_tasks))"
1.1.0,w_train = np.ones_like(y_train)
1.1.0,"ids_train = [""C""] * n_train"
1.1.0,train_dataset = dc.data.DiskDataset.from_numpy(
1.1.0,"X_train, y_train, w_train, ids_train)"
1.1.0,# Define test dataset
1.1.0,n_test = 10
1.1.0,"X_test = np.random.rand(n_test, n_features)"
1.1.0,"y_test = np.random.randint(2, size=(n_test, n_tasks))"
1.1.0,w_test = np.ones_like(y_test)
1.1.0,"ids_test = [""C""] * n_test"
1.1.0,test_dataset = dc.data.DiskDataset.from_numpy(
1.1.0,"X_test, y_test, w_test, ids_test)"
1.1.0,classification_metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score)]
1.1.0,def model_builder(model_dir):
1.1.0,sklearn_model = LogisticRegression()
1.1.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.1.0,multitask_model = dc.models.SingletaskToMultitask(
1.1.0,"tasks, model_builder)"
1.1.0,# Fit trained model
1.1.0,multitask_model.fit(train_dataset)
1.1.0,multitask_model.save()
1.1.0,# Eval multitask_model on train/test
1.1.0,"_ = multitask_model.evaluate(train_dataset, classification_metrics)"
1.1.0,"_ = multitask_model.evaluate(test_dataset, classification_metrics)"
1.1.0,Generate data
1.1.0,Cleanup
1.1.0,Generate dummy dataset
1.1.0,Fit trained model
1.1.0,Eval model on test
1.1.0,Eval model on train
1.1.0,Fit trained model
1.1.0,Eval model on test
1.1.0,Fit trained model
1.1.0,Eval model on test
1.1.0,def test_sklearn_classification(self):
1.1.0,"""""""Test that sklearn models can learn on simple classification datasets."""""""
1.1.0,np.random.seed(123)
1.1.0,dataset = sklearn.datasets.load_digits(n_class=2)
1.1.0,"X, y = dataset.data, dataset.target"
1.1.0,frac_train = .7
1.1.0,n_samples = len(X)
1.1.0,n_train = int(frac_train*n_samples)
1.1.0,"X_train, y_train = X[:n_train], y[:n_train]"
1.1.0,"X_test, y_test = X[n_train:], y[n_train:]"
1.1.0,"train_dataset = dc.data.NumpyDataset(X_train, y_train)"
1.1.0,"test_dataset = dc.data.NumpyDataset(X_test, y_test)"
1.1.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.1.0,sklearn_model = LogisticRegression()
1.1.0,model = dc.models.SklearnModel(sklearn_model)
1.1.0,# Fit trained model
1.1.0,model.fit(train_dataset)
1.1.0,model.save()
1.1.0,# Eval model on test
1.1.0,"scores = model.evaluate(test_dataset, [classification_metric])"
1.1.0,assert scores[classification_metric.name] > .5
1.1.0,def test_sklearn_multitask_classification(self):
1.1.0,"""""""Test that sklearn models can learn on simple multitask classification."""""""
1.1.0,np.random.seed(123)
1.1.0,n_tasks = 4
1.1.0,tasks = range(n_tasks)
1.1.0,dataset = sklearn.datasets.load_digits(n_class=2)
1.1.0,"X, y = dataset.data, dataset.target"
1.1.0,"y = np.reshape(y, (len(y), 1))"
1.1.0,y = np.hstack([y] * n_tasks)
1.1.0,
1.1.0,frac_train = .7
1.1.0,n_samples = len(X)
1.1.0,n_train = int(frac_train*n_samples)
1.1.0,"X_train, y_train = X[:n_train], y[:n_train]"
1.1.0,"X_test, y_test = X[n_train:], y[n_train:]"
1.1.0,"train_dataset = dc.data.DiskDataset.from_numpy(X_train, y_train)"
1.1.0,"test_dataset = dc.data.DiskDataset.from_numpy(X_test, y_test)"
1.1.0,classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
1.1.0,def model_builder(model_dir):
1.1.0,sklearn_model = LogisticRegression()
1.1.0,"return dc.models.SklearnModel(sklearn_model, model_dir)"
1.1.0,"model = dc.models.SingletaskToMultitask(tasks, model_builder)"
1.1.0,# Fit trained model
1.1.0,model.fit(train_dataset)
1.1.0,model.save()
1.1.0,# Eval model on test
1.1.0,"scores = model.evaluate(test_dataset, [classification_metric])"
1.1.0,for score in scores[classification_metric.name]:
1.1.0,assert score > .5
1.1.0,Set early stopping round = n_estimators so that esr won't work
1.1.0,Fit trained model
1.1.0,Eval model on test
1.1.0,Fit trained model
1.1.0,Eval model on test
1.1.0,Fit trained model
1.1.0,Eval model on test
1.1.0,Logistic regression doesn't support weights
1.1.0,Consistency check
1.1.0,Handle output layer
1.1.0,Iterate over all previous tasks.
1.1.0,prev_layers is a list with elements of size
1.1.0,"(batch_size, layer_sizes[i-1])"
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Save an initial checkpoint.
1.1.0,Turns out there are valid cases where we don't want pad-batches
1.1.0,on by default.
1.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.1.0,Run training op.
1.1.0,Always save a final checkpoint when complete.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Note that we divide by the batch size and not the number of
1.1.0,"non-zero weight examples in the batch.  Also, instead of using"
1.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.1.0,calculate with div/sum so it stays on the GPU.
1.1.0,aggregated costs
1.1.0,weight decay
1.1.0,Dummy placeholders
1.1.0,Dummy placeholders
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,Handle edge case when batch-size is 1.
1.1.0,Prune away any padding that was added
1.1.0,allow_soft_placement=True allows ops without a GPU implementation
1.1.0,to run on the CPU instead.
1.1.0,"""""""Ops for graph construction."""""""
1.1.0,from __future__ import print_function
1.1.0,from __future__ import division
1.1.0,from __future__ import unicode_literals
1.1.0,
1.1.0,import sys
1.1.0,import traceback
1.1.0,import tensorflow as tf
1.1.0,from keras import backend as K
1.1.0,
1.1.0,"def cosine_distances(test, support):"
1.1.0,"""""""Computes pairwise cosine distances between provided tensors"
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,test: tf.Tensor
1.1.0,"Of shape (n_test, n_feat)"
1.1.0,support: tf.Tensor
1.1.0,"Of shape (n_support, n_feat)"
1.1.0,
1.1.0,Returns
1.1.0,-------
1.1.0,tf.Tensor:
1.1.0,"Of shape (n_test, n_support)"
1.1.0,""""""""
1.1.0,"rnorm_test = tf.rsqrt(tf.reduce_sum(tf.square(test), 1,"
1.1.0,keep_dims=True)) + K.epsilon()
1.1.0,"rnorm_support = tf.rsqrt(tf.reduce_sum(tf.square(support), 1,"
1.1.0,keep_dims=True)) + K.epsilon()
1.1.0,test_normalized = test * rnorm_test
1.1.0,support_normalized = support * rnorm_support
1.1.0,
1.1.0,# Transpose for mul
1.1.0,"support_normalized_t = tf.transpose(support_normalized, perm=[1,0])"
1.1.0,"g = tf.matmul(test_normalized, support_normalized_t)  # Gram matrix"
1.1.0,return g
1.1.0,
1.1.0,"def euclidean_distance(test, support, max_dist_sq=20):"
1.1.0,"""""""Computes pairwise euclidean distances between provided tensors"
1.1.0,
1.1.0,TODO(rbharath): BROKEN! THIS DOESN'T WORK!
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,test: tf.Tensor
1.1.0,"Of shape (n_test, n_feat)"
1.1.0,support: tf.Tensor
1.1.0,"Of shape (n_support, n_feat)"
1.1.0,"max_dist_sq: float, optional"
1.1.0,Maximum pairwise distance allowed.
1.1.0,
1.1.0,Returns
1.1.0,-------
1.1.0,tf.Tensor:
1.1.0,"Of shape (n_test, n_support)"
1.1.0,""""""""
1.1.0,"test = tf.expand_dims(test, 1)"
1.1.0,"support = tf.expand_dims(support, 0)"
1.1.0,"g = -tf.maximum(tf.reduce_sum(tf.square(test - support), 2), max_dist_sq)"
1.1.0,return g
1.1.0,
1.1.0,"def add_bias(tensor, init=None, name=None):"
1.1.0,"""""""Add a bias term to a tensor."
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,tensor: tf.Tensor
1.1.0,Variable tensor.
1.1.0,init: float
1.1.0,Bias initializer. Defaults to zero.
1.1.0,name: str
1.1.0,Name for this op. Defaults to tensor.op.name.
1.1.0,
1.1.0,Returns
1.1.0,-------
1.1.0,tf.Tensor
1.1.0,A biased tensor with the same shape as the input tensor.
1.1.0,""""""""
1.1.0,if init is None:
1.1.0,init = tf.zeros([tensor.get_shape()[-1].value])
1.1.0,"with tf.name_scope(name, tensor.op.name, [tensor]):"
1.1.0,"b = tf.Variable(init, name='b')"
1.1.0,"return tf.nn.bias_add(tensor, b)"
1.1.0,
1.1.0,
1.1.0,"def dropout(tensor, dropout_prob, training=True, training_only=True):"
1.1.0,"""""""Random dropout."
1.1.0,
1.1.0,"This implementation supports ""always-on"" dropout (training_only=False), which"
1.1.0,"can be used to calculate model uncertainty. See Gal and Ghahramani,"
1.1.0,http://arxiv.org/abs/1506.02142.
1.1.0,
1.1.0,"NOTE(user): To simplify the implementation, I have chosen not to reverse"
1.1.0,the scaling that occurs in tf.nn.dropout when using dropout during
1.1.0,inference. This shouldn't be an issue since the activations will be scaled
1.1.0,by the same constant in both training and inference. This means that there
1.1.0,are no training-time differences between networks that use dropout during
1.1.0,inference and those that do not.
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,tensor: tf.Tensor
1.1.0,Input tensor.
1.1.0,dropout_prob: float
1.1.0,Float giving dropout probability for weights (NOT keep probability).
1.1.0,training_only: bool
1.1.0,"Boolean. If True (standard dropout), apply dropout only"
1.1.0,"during training. If False, apply dropout during inference as well."
1.1.0,
1.1.0,Returns
1.1.0,-------
1.1.0,tf.Tensor:
1.1.0,A tensor with the same shape as the input tensor.
1.1.0,""""""""
1.1.0,if not dropout_prob:
1.1.0,return tensor  # do nothing
1.1.0,keep_prob = 1.0 - dropout_prob
1.1.0,if training or not training_only:
1.1.0,"tensor = tf.nn.dropout(tensor, keep_prob)"
1.1.0,return tensor
1.1.0,
1.1.0,
1.1.0,"def fully_connected_layer(tensor, size=None, weight_init=None, bias_init=None,"
1.1.0,name=None):
1.1.0,"""""""Fully connected layer."
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,tensor: tf.Tensor
1.1.0,Input tensor.
1.1.0,size: int
1.1.0,Number of output nodes for this layer.
1.1.0,weight_init: float
1.1.0,Weight initializer.
1.1.0,bias_init: float
1.1.0,Bias initializer.
1.1.0,name: str
1.1.0,Name for this op. Defaults to 'fully_connected'.
1.1.0,
1.1.0,Returns
1.1.0,-------
1.1.0,tf.Tensor:
1.1.0,A new tensor representing the output of the fully connected layer.
1.1.0,
1.1.0,Raises
1.1.0,------
1.1.0,ValueError
1.1.0,If input tensor is not 2D.
1.1.0,""""""""
1.1.0,if len(tensor.get_shape()) != 2:
1.1.0,"raise ValueError('Dense layer input must be 2D, not %dD'"
1.1.0,% len(tensor.get_shape()))
1.1.0,if weight_init is None:
1.1.0,num_features = tensor.get_shape()[-1].value
1.1.0,"weight_init = tf.truncated_normal([num_features, size], stddev=0.01)"
1.1.0,if bias_init is None:
1.1.0,bias_init = tf.zeros([size])
1.1.0,
1.1.0,"with tf.name_scope(name, 'fully_connected', [tensor]):"
1.1.0,"w = tf.Variable(weight_init, name='w', dtype=tf.float32)"
1.1.0,"b = tf.Variable(bias_init, name='b', dtype=tf.float32)"
1.1.0,"return tf.nn.xw_plus_b(tensor, w, b)"
1.1.0,
1.1.0,"def weight_decay(penalty_type, penalty):"
1.1.0,"""""""Add weight decay."
1.1.0,
1.1.0,Args:
1.1.0,model: TensorflowGraph.
1.1.0,
1.1.0,Returns:
1.1.0,A scalar tensor containing the weight decay cost.
1.1.0,
1.1.0,Raises:
1.1.0,NotImplementedError: If an unsupported penalty type is requested.
1.1.0,""""""""
1.1.0,variables = []
1.1.0,# exclude bias variables
1.1.0,for v in tf.trainable_variables():
1.1.0,if v.get_shape().ndims == 2:
1.1.0,variables.append(v)
1.1.0,
1.1.0,with tf.name_scope('weight_decay'):
1.1.0,if penalty_type == 'l1':
1.1.0,cost = tf.add_n([tf.reduce_sum(tf.abs(v)) for v in variables])
1.1.0,elif penalty_type == 'l2':
1.1.0,cost = tf.add_n([tf.nn.l2_loss(v) for v in variables])
1.1.0,else:
1.1.0,raise NotImplementedError('Unsupported penalty_type %s' % penalty_type)
1.1.0,cost *= penalty
1.1.0,"tf.scalar_summary('Weight Decay Cost', cost)"
1.1.0,return cost
1.1.0,
1.1.0,
1.1.0,"def multitask_logits(features, num_tasks, num_classes=2, weight_init=None,"
1.1.0,"bias_init=None, dropout_prob=None, name=None):"
1.1.0,"""""""Create a logit tensor for each classification task."
1.1.0,
1.1.0,Args:
1.1.0,features: A 2D tensor with dimensions batch_size x num_features.
1.1.0,num_tasks: Number of classification tasks.
1.1.0,num_classes: Number of classes for each task.
1.1.0,weight_init: Weight initializer.
1.1.0,bias_init: Bias initializer.
1.1.0,dropout_prob: Float giving dropout probability for weights (NOT keep
1.1.0,probability).
1.1.0,name: Name for this op. Defaults to 'multitask_logits'.
1.1.0,
1.1.0,Returns:
1.1.0,A list of logit tensors; one for each classification task.
1.1.0,""""""""
1.1.0,logits_list = []
1.1.0,with tf.name_scope('multitask_logits'):
1.1.0,for task_idx in range(num_tasks):
1.1.0,"with tf.name_scope(name,"
1.1.0,"('task' + str(task_idx).zfill(len(str(num_tasks)))), [features]):"
1.1.0,logits_list.append(
1.1.0,"logits(features, num_classes, weight_init=weight_init,"
1.1.0,"bias_init=bias_init, dropout_prob=dropout_prob))"
1.1.0,return logits_list
1.1.0,
1.1.0,
1.1.0,"def logits(features, num_classes=2, weight_init=None, bias_init=None,"
1.1.0,"dropout_prob=None, name=None):"
1.1.0,"""""""Create a logits tensor for a single classification task."
1.1.0,
1.1.0,You almost certainly don't want dropout on there -- it's like randomly setting
1.1.0,the (unscaled) probability of a target class to 0.5.
1.1.0,
1.1.0,Args:
1.1.0,features: A 2D tensor with dimensions batch_size x num_features.
1.1.0,num_classes: Number of classes for each task.
1.1.0,weight_init: Weight initializer.
1.1.0,bias_init: Bias initializer.
1.1.0,dropout_prob: Float giving dropout probability for weights (NOT keep
1.1.0,probability).
1.1.0,name: Name for this op.
1.1.0,
1.1.0,Returns:
1.1.0,A logits tensor with shape batch_size x num_classes.
1.1.0,""""""""
1.1.0,"with tf.name_scope(name, 'logits', [features]) as name:"
1.1.0,return dropout(
1.1.0,"fully_connected_layer(features, num_classes, weight_init=weight_init,"
1.1.0,"bias_init=bias_init, name=name),"
1.1.0,dropout_prob)
1.1.0,
1.1.0,
1.1.0,"def softmax_N(tensor, name=None):"
1.1.0,"""""""Apply softmax across last dimension of a tensor."
1.1.0,
1.1.0,Args:
1.1.0,tensor: Input tensor.
1.1.0,"name: Name for this op. If None, defaults to 'softmax_N'."
1.1.0,
1.1.0,Returns:
1.1.0,A tensor with softmax-normalized values on the last dimension.
1.1.0,""""""""
1.1.0,"with tf.name_scope(name, 'softmax_N', [tensor]):"
1.1.0,exp_tensor = tf.exp(tensor)
1.1.0,reduction_indices = [tensor.get_shape().ndims - 1]
1.1.0,"return tf.div(exp_tensor,"
1.1.0,"tf.reduce_sum(exp_tensor,"
1.1.0,"reduction_indices=reduction_indices,"
1.1.0,keep_dims=True))
1.1.0,
1.1.0,"def optimizer(optimizer=""adam"", learning_rate=.001, momentum=.9):"
1.1.0,"""""""Create model optimizer."
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,"optimizer: str, optional"
1.1.0,Name of optimizer
1.1.0,"learning_rate: float, optional"
1.1.0,Learning rate for algorithm
1.1.0,"momentum: float, optional"
1.1.0,Momentum rate
1.1.0,
1.1.0,Returns
1.1.0,-------
1.1.0,A training Optimizer.
1.1.0,
1.1.0,Raises:
1.1.0,NotImplementedError: If an unsupported optimizer is requested.
1.1.0,""""""""
1.1.0,# TODO(user): gradient clipping (see Minimize)
1.1.0,if optimizer == 'adagrad':
1.1.0,train_op = tf.train.AdagradOptimizer(learning_rate)
1.1.0,elif optimizer == 'adam':
1.1.0,train_op = tf.train.AdamOptimizer(learning_rate)
1.1.0,elif optimizer == 'momentum':
1.1.0,"train_op = tf.train.MomentumOptimizer(learning_rate,"
1.1.0,momentum)
1.1.0,elif optimizer == 'rmsprop':
1.1.0,"train_op = tf.train.RMSPropOptimizer(learning_rate,"
1.1.0,momentum)
1.1.0,elif optimizer == 'sgd':
1.1.0,train_op = tf.train.GradientDescentOptimizer(learning_rate)
1.1.0,else:
1.1.0,raise NotImplementedError('Unsupported optimizer %s' % optimizer)
1.1.0,return train_op
1.1.0,!/usr/bin/python
1.1.0,
1.1.0,Copyright 2015 Google Inc.
1.1.0,
1.1.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.1.0,you may not use this file except in compliance with the License.
1.1.0,You may obtain a copy of the License at
1.1.0,
1.1.0,http://www.apache.org/licenses/LICENSE-2.0
1.1.0,
1.1.0,"Unless required by applicable law or agreed to in writing, software"
1.1.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.1.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.1.0,See the License for the specific language governing permissions and
1.1.0,limitations under the License.
1.1.0,get the divisor
1.1.0,compute the requested central moment
1.1.0,"note that mean is a raw moment, not a central moment"
1.1.0,TODO(user): median is not implemented yet in TensorFlow
1.1.0,-*- coding: utf-8 -*-
1.1.0,"due to the different shape of weight(ndims=2) and bias(ndims=1),"
1.1.0,will using this version for logreg
1.1.0,exclude bias variables
1.1.0,setting up n_tasks nodes(output nodes)
1.1.0,label placeholders with size batch_size * 1
1.1.0,Note that we divide by the batch size and not the number of
1.1.0,"non-zero weight examples in the batch.  Also, instead of using"
1.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.1.0,calculate with div/sum so it stays on the GPU.
1.1.0,aggregated costs
1.1.0,weight decay
1.1.0,using self-defined regularization
1.1.0,adding output nodes of sigmoid function
1.1.0,"fix the size to be [?,1]"
1.1.0,Dummy placeholders
1.1.0,Dummy placeholders
1.1.0,run eval data through the model
1.1.0,transfer 2D prediction tensor to 2D x n_classes(=2)
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,run eval data through the model
1.1.0,transfer 2D prediction tensor to 2D x n_classes(=2)
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,"layer has shape [None, layer_sizes[i]]"
1.1.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.1.0,TODO(rbharath): Might want to make it feasible to have multiple
1.1.0,bypass layers.
1.1.0,Construct task bypass layer
1.1.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.1.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.1.0,"layer has shape [None, layer_sizes[i]]"
1.1.0,"top_multitask_layer has shape [None, layer_sizes[-1]]"
1.1.0,TODO(rbharath): Might want to make it feasible to have multiple
1.1.0,bypass layers.
1.1.0,Construct task bypass layer
1.1.0,"bypass_layer has shape [None, bypass_layer_sizes[i]]"
1.1.0,"task_layer has shape [None, layer_sizes[-1] + bypass_layer_sizes[-1]]"
1.1.0,Add the input features.
1.1.0,Add the dense layers
1.1.0,Compute the loss function for each label.
1.1.0,Add the input features.
1.1.0,Add the dense layers
1.1.0,Compute the loss function for each label.
1.1.0,Run fit transformers on dummy dataset to determine n_features after transformation
1.1.0,Dummy placeholders
1.1.0,Dummy placeholders
1.1.0,Dummy placeholders
1.1.0,Dummy placeholders
1.1.0,Run fit transformers on dummy dataset to determine n_features after transformation
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Save an initial checkpoint.
1.1.0,Define the code that runs on a separate thread to feed data into the queue.
1.1.0,Main training loop.
1.1.0,Run training op.
1.1.0,We have reached the end of an epoch.
1.1.0,We have reached the end of the data.
1.1.0,Always save a final checkpoint when complete.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,Handle edge case when batch-size is 1.
1.1.0,Prune away any padding that was added
1.1.0,Handle case of 0-dimensional scalar output
1.1.0,Consistency check
1.1.0,Lazily created by _get_shared_session().
1.1.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.1.0,when subclass-overridden methods use the same scopes.
1.1.0,Setup graph
1.1.0,Create placeholders
1.1.0,Handle output layer
1.1.0,Iterate over all previous tasks.
1.1.0,prev_layers is a list with elements of size
1.1.0,"(batch_size, layer_sizes[i-1])"
1.1.0,Note that we divide by the batch size and not the number of
1.1.0,"non-zero weight examples in the batch.  Also, instead of using"
1.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.1.0,calculate with div/sum so it stays on the GPU.
1.1.0,aggregated costs
1.1.0,weight decay
1.1.0,Dummy placeholders
1.1.0,Dummy placeholders
1.1.0,run eval data through the model
1.1.0,"Shape (n_tasks, n__samples)"
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,Handle edge case when batch-size is 1.
1.1.0,with self._get_shared_session(train=True) as sess:
1.1.0,Save an initial checkpoint.
1.1.0,Always save a final checkpoint when complete.
1.1.0,Note that we divide by the batch size and not the number of
1.1.0,"non-zero weight examples in the batch.  Also, instead of using"
1.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.1.0,calculate with div/sum so it stays on the GPU.
1.1.0,"orig_dict[""labels_%d"" % task] = np.reshape(y_b[:, task], (n_samples, 1))"
1.1.0,Dummy placeholders
1.1.0,"orig_dict[""labels_%d"" % task] = np.zeros((n_samples, 1))"
1.1.0,"orig_dict[""weights_%d"" % task] = np.reshape(w_b[:, task], (n_samples, 1))"
1.1.0,Dummy placeholders
1.1.0,"orig_dict[""weights_%d"" % task] = np.zeros((n_samples, 1))"
1.1.0,allow_soft_placement=True allows ops without a GPU implementation
1.1.0,to run on the CPU instead.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Turns out there are valid cases where we don't want pad-batches
1.1.0,on by default.
1.1.0,"dataset.iterbatches(batch_size, pad_batches=True)):"
1.1.0,if epoch%checkpoint_interval == checkpoint_interval-1:
1.1.0,"saver.save(sess, self._save_path, global_step=epoch)"
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,"(n_samples, n_classes)"
1.1.0,"(n_samples, n_tasks, n_classes)"
1.1.0,Save hyperparameters
1.1.0,Guard variable to make sure we don't Restore() this model
1.1.0,from a disk checkpoint more than once.
1.1.0,"Path to save checkpoint files, which matches the"
1.1.0,replicated supervisor's default path.
1.1.0,Lazily created by _get_shared_session().
1.1.0,"Cache of TensorFlow scopes, to prevent '_1' appended scope names"
1.1.0,when subclass-overridden methods use the same scopes.
1.1.0,Setup graph
1.1.0,Note that we divide by the batch size and not the number of
1.1.0,"non-zero weight examples in the batch.  Also, instead of using"
1.1.0,tf.reduce_mean (which can put ops on the CPU) we explicitly
1.1.0,calculate with div/sum so it stays on the GPU.
1.1.0,aggregated costs
1.1.0,weight decay
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Save an initial checkpoint.
1.1.0,Define the code that runs on a separate thread to feed data into the queue.
1.1.0,Main training loop.
1.1.0,Run training op.
1.1.0,We have reached the end of an epoch.
1.1.0,We have reached the end of the data.
1.1.0,Always save a final checkpoint when complete.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,allow_soft_placement=True allows ops without a GPU implementation
1.1.0,to run on the CPU instead.
1.1.0,TODO(rbharath): Is setting train=False right here?
1.1.0,Discard any padded predictions
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,Special case to handle singletasks.
1.1.0,The iterbatches does padding with zero-weight examples on the last batch.
1.1.0,Remove padded examples.
1.1.0,TODO(rbharath): Verify this can be safely removed.
1.1.0,"def evaluate(self, dataset, metrics, transformers=[]):"
1.1.0,""""""""
1.1.0,Evaluates the performance of this model on specified dataset.
1.1.0,
1.1.0,Parameters
1.1.0,----------
1.1.0,dataset: dc.data.Dataset
1.1.0,Dataset object.
1.1.0,metric: deepchem.metrics.Metric
1.1.0,Evaluation metric
1.1.0,transformers: list
1.1.0,List of deepchem.transformers.Transformer
1.1.0,Returns
1.1.0,-------
1.1.0,dict
1.1.0,Maps tasks to scores under metric.
1.1.0,""""""""
1.1.0,"evaluator = Evaluator(self, dataset, transformers)"
1.1.0,scores = evaluator.compute_model_performance(metrics)
1.1.0,return scores
1.1.0,checkpoints look like logdir/model.ckpt-N
1.1.0,"self._save_path is ""logdir/model.ckpt"""
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,Note that softmax is already applied in construct_grpah
1.1.0,run eval data through the model
1.1.0,reshape to batch_size x n_tasks x ...
1.1.0,Handle edge case when batch-size is 1.
1.1.0,Prune away any padding that was added
1.1.0,Handle case of 0-dimensional scalar output
1.1.0,!/usr/bin/python
1.1.0,
1.1.0,Copyright 2015 Google Inc.
1.1.0,
1.1.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.1.0,you may not use this file except in compliance with the License.
1.1.0,You may obtain a copy of the License at
1.1.0,
1.1.0,http://www.apache.org/licenses/LICENSE-2.0
1.1.0,
1.1.0,"Unless required by applicable law or agreed to in writing, software"
1.1.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.1.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.1.0,See the License for the specific language governing permissions and
1.1.0,limitations under the License.
1.1.0,parse CheckpointState proto
1.1.0,parse path to actual checkpoint
1.1.0,the provided mask has to be the same shape as features
1.1.0,test k = 1..4
1.1.0,central moments
1.1.0,standardized moments
1.1.0,central across one axis
1.1.0,standardized across one axis
1.1.0,Fit just on task zero
1.1.0,Notice that we keep the session open
1.1.0,Fit on task one
1.1.0,The predictions for task zero should not change after training
1.1.0,on task one.
1.1.0,Keep track of the layers
1.1.0,############################################ DEBUG
1.1.0,"print(""start - add()"")"
1.1.0,"print(""self.output"")"
1.1.0,print(self.output)
1.1.0,############################################ DEBUG
1.1.0,"For graphical layers, add connectivity placeholders"
1.1.0,############################################ DEBUG
1.1.0,"print(""end- add()"")"
1.1.0,"print(""self.output"")"
1.1.0,print(self.output)
1.1.0,############################################ DEBUG
1.1.0,Add layer to the layer list
1.1.0,Keep track of the layers
1.1.0,Create graph topology and x
1.1.0,Keep track of the layers
1.1.0,Whether or not we have used the GraphGather layer yet
1.1.0,Update new value of x
1.1.0,Update new value of x
1.1.0,Update new value of x
1.1.0,Get train function
1.1.0,Initialize
1.1.0,################################################################### DEBUG
1.1.0,self.test_label_placeholder = Input(
1.1.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.1.0,"name=""label_placeholder""))"
1.1.0,self.test_weight_placeholder = Input(
1.1.0,"tensor=tf.placeholder(dtype='float32', shape=(self.test_batch_size),"
1.1.0,"name=""weight_placeholder""))"
1.1.0,TODO(rbharath): Should weights for the support be used?
1.1.0,Support labels
1.1.0,self.support_label_placeholder = Input(
1.1.0,"tensor=tf.placeholder(dtype='float32', shape=[self.support_batch_size],"
1.1.0,"name=""support_label_placeholder""))"
1.1.0,################################################################### DEBUG
1.1.0,Generate dictionary elements for support
1.1.0,Get graph information for test
1.1.0,Generate dictionary elements for test
1.1.0,Perform the optimization
1.1.0,Create different support sets
1.1.0,Get batch to try it out on
1.1.0,"Train on support set, batch pair"
1.1.0,Get featurization for test
1.1.0,"Shape (n_test, n_feat)"
1.1.0,Get featurization for support
1.1.0,"Shape (n_support, n_feat)"
1.1.0,Computes the inner part c() of the kernel
1.1.0,(the inset equation in section 2.1.1 of Matching networks paper).
1.1.0,Normalize
1.1.0,TODO(rbharath): euclidean kernel is broken!
1.1.0,elif self.similarity == 'euclidean':
1.1.0,"g = model_ops.euclidean_distance(test_feat, support_feat)"
1.1.0,"Note that gram matrix g has shape (n_test, n_support)"
1.1.0,"soft corresponds to a(xhat, x_i) in eqn (1) of Matching Networks paper"
1.1.0,https://arxiv.org/pdf/1606.04080v1.pdf
1.1.0,"Computes softmax across axis 1, (so sums distances to support set for"
1.1.0,each test entry) to get attention vector
1.1.0,"Shape (n_test, n_support)"
1.1.0,Weighted sum of support labels
1.1.0,"Shape (n_support, 1)"
1.1.0,pred is yhat in eqn (1) of Matching Networks.
1.1.0,"Shape squeeze((n_test, n_support) * (n_support, 1)) = (n_test,)"
1.1.0,"Clip softmax probabilities to range [epsilon, 1-epsilon]"
1.1.0,"Shape (n_test,)"
1.1.0,Convert to logit space using inverse sigmoid (logit) function
1.1.0,logit function: log(pred) - log(1-pred)
1.1.0,Used to invoke tf.nn.sigmoid_cross_entropy_with_logits
1.1.0,in Cross Entropy calculation.
1.1.0,"Shape (n_test,)"
1.1.0,Get scores
1.1.0,Remove padded elements
1.1.0,Get scores
1.1.0,pred corresponds to prob(example == 1)
1.1.0,Remove padded elements
1.1.0,Get batches
1.1.0,TODO(rbharath): Add test for get_task_dataset_minus_support for
1.1.0,multitask case with missing data...
1.1.0,Join information for all tasks.
1.1.0,TODO(rbharath): Find a way to get rid of this import?
1.1.0,Extract model info
1.1.0,Get graph topology for x
1.1.0,Building outputs
1.1.0,Set epsilon
1.1.0,Initialize
1.1.0,"Path to save checkpoint files, which matches the"
1.1.0,replicated supervisor's default path.
1.1.0,Create target inputs
1.1.0,Get train function
1.1.0,TODO(rbharath): I believe this is total amount of data
1.1.0,Get graph information
1.1.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.1.0,the number of labeled data points in target_i. This is to normalize each task
1.1.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.1.0,Get other optimizer information
1.1.0,TODO(rbharath): Figure out how to handle phase appropriately
1.1.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.1.0,"tensors of shape (batch_size,)"
1.1.0,It's ok to divide by just the batch_size rather than the number of nonzero
1.1.0,examples (effect averages out)
1.1.0,Perform the optimization
1.1.0,TODO(rbharath): Disabling saving for now to try to debug.
1.1.0,run eval data through the model
1.1.0,"Shape (n_samples, n_tasks)"
1.1.0,TODO(rbharath): Find a way to get rid of this import?
1.1.0,Obtain appropriate loss function
1.1.0,Extract model info
1.1.0,Get graph topology for x
1.1.0,############################################################ DEBUG
1.1.0,self.feat_dim = self.model.get_num_output_features()
1.1.0,############################################################ DEBUG
1.1.0,Raw logit outputs
1.1.0,Set epsilon
1.1.0,Initialize
1.1.0,"Path to save checkpoint files, which matches the"
1.1.0,replicated supervisor's default path.
1.1.0,Create target inputs
1.1.0,############################################################### DEBUG
1.1.0,"print(""multitask classifier"")"
1.1.0,"print(""feat"")"
1.1.0,print(feat)
1.1.0,############################################################### DEBUG
1.1.0,Get train function
1.1.0,TODO(rbharath): I believe this is total amount of data
1.1.0,Get graph information
1.1.0,"TODO (hraut->rhbarath): num_datapoints should be a vector, with ith element being"
1.1.0,the number of labeled data points in target_i. This is to normalize each task
1.1.0,num_dat_dict = {self.num_datapoints_placeholder : self.}
1.1.0,Get other optimizer information
1.1.0,TODO(rbharath): Figure out how to handle phase appropriately
1.1.0,"label_placeholder of shape (batch_size, n_tasks). Split into n_tasks"
1.1.0,"tensors of shape (batch_size,)"
1.1.0,Convert the labels into one-hot vector encodings.
1.1.0,Since we use tf.nn.softmax_cross_entropy_with_logits note that we pass in
1.1.0,un-softmaxed logits rather than softmax outputs.
1.1.0,It's ok to divide by just the batch_size rather than the number of nonzero
1.1.0,examples (effect averages out)
1.1.0,Perform the optimization
1.1.0,TODO(rbharath): Disabling saving for now to try to debug.
1.1.0,run eval data through the model
1.1.0,"Shape (n_samples, n_tasks)"
1.1.0,run eval data through the model
1.1.0,self.n_atoms = n_atoms
1.1.0,Define the list of tensors to be used as topology
1.1.0,Merge mol conv objects
1.1.0,Generate dicts
1.1.0,self.n_atoms = n_atoms
1.1.0,Define the list of tensors to be used as topology
1.1.0,Extract atom numbers
1.1.0,Generate dicts
1.1.0,molecule * atom(graph) => step => features
1.1.0,molecule * atom(graph) => step
1.1.0,molecule * atom(graph) => step
1.1.0,Define the list of tensors to be used as topology
1.1.0,calculation orders for a batch of molecules
1.1.0,padding atom features vector of each molecule with 0
1.1.0,self.n_atoms = n_atoms
1.1.0,Define the list of tensors to be used as topology
1.1.0,Extract atom numbers
1.1.0,Generate dicts
1.1.0,self.n_atoms = n_atoms
1.1.0,Define the list of tensors to be used as topology
1.1.0,Extract atom numbers
1.1.0,number of atoms in each molecule
1.1.0,index of pair features
1.1.0,number of pairs for each atom
1.1.0,atom features
1.1.0,pair features
1.1.0,Generate dicts
1.1.0,Associate each atom with cell it belongs to. O(N*n_cells)
1.1.0,"Shape (n_cells, k)"
1.1.0,"Shape (N, 1)"
1.1.0,Associate each cell with its neighbor cells. Assumes periodic boundary
1.1.0,"conditions, so does wrapround. O(constant)"
1.1.0,"Shape (n_cells, 26)"
1.1.0,"Shape (N, 26)"
1.1.0,"coords of shape (N, ndim)"
1.1.0,"Shape (N, 26, k, ndim)"
1.1.0,"Shape (N, 26, k)"
1.1.0,"Shape (N, 26, k)"
1.1.0,"Shape (N, 26, k, ndim)"
1.1.0,"For smaller systems especially, the periodic boundary conditions can"
1.1.0,result in neighboring cells being seen multiple times. Maybe use tf.unique to
1.1.0,make sure duplicate neighbors are ignored?
1.1.0,TODO(rbharath): How does distance need to be modified here to
1.1.0,account for periodic boundary conditions?
1.1.0,"Shape (N, 26, k)"
1.1.0,"Shape (N, 26*k)"
1.1.0,TODO(rbharath): This will cause an issue with duplicates!
1.1.0,"Shape (N, M)"
1.1.0,"N elts of size (M,) each"
1.1.0,"Shape (N, 26*k)"
1.1.0,"N elts of size (26*k,) each"
1.1.0,"N elts of size (M,) each"
1.1.0,"Shape (N, M)"
1.1.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.1.0,"N tensors of shape (n_cells, 1)"
1.1.0,"Shape (N*n_cells, 1) after tile"
1.1.0,"List of N tensors of shape (n_cells, 1)"
1.1.0,Lists of length N
1.1.0,Lists of length n_cells
1.1.0,Get indices of k atoms closest to each cell point
1.1.0,TODO(rbharath): tf.stack for tf 1.0
1.1.0,"Tensor of shape (n_cells, k, ndim)"
1.1.0,atoms_in_cells = tf.stack(atoms_in_cells)
1.1.0,"Tensor of shape (26, k, ndim)"
1.1.0,"Reshape to (26*k, ndim)"
1.1.0,"Subtract out atom vector. Still of shape (26*k, ndim) due to broadcast."
1.1.0,"Dists of shape (26*k, 1)"
1.1.0,"Of shape (k, ndim)"
1.1.0,"Tile both cells and coords to form arrays of size (n_cells*N, ndim)"
1.1.0,TODO(rbharath): Change this for tf 1.0
1.1.0,"n_cells tensors of shape (N, 1)"
1.1.0,"Shape (N*n_cells, 1) after tile"
1.1.0,"List of n_cells tensors of shape (N, 1)"
1.1.0,Lists of length n_cells
1.1.0,Lists of length n_cells
1.1.0,Get indices of k atoms closest to each cell point
1.1.0,"n_cells tensors of shape (k, ndim)"
1.1.0,"Tensor of shape (n_cells, k)"
1.1.0,TODO(rbharath):
1.1.0,- Need to find neighbors of the cells (+/- 1 in every dimension).
1.1.0,- Need to group closest atoms amongst cell neighbors
1.1.0,- Need to do another top_k to find indices of closest neighbors.
1.1.0,- Return N lists corresponding to neighbors for every atom.
1.1.0,TODO(rbharath): Do we need to handle periodic boundary conditions
1.1.0,TODO(rbharath): This doesn't handle boundaries well. We hard-code
1.1.0,"looking for 26 neighbors, which isn't right for boundary cells in"
1.1.0,the cube.
1.1.0,Number of neighbors of central cube in 3-space is
1.1.0,3^2 (top-face) + 3^2 (bottom-face) + (3^2-1) (middle-band)
1.1.0,TODO(rbharath)
1.1.0,n_cells = int(cells.get_shape()[0])
1.1.0,"Tile cells to form arrays of size (n_cells*n_cells, ndim)"
1.1.0,"Two tilings (a, b, c, a, b, c, ...) vs. (a, a, a, b, b, b, etc.)"
1.1.0,"Tile (a, a, a, b, b, b, etc.)"
1.1.0,"Tile (a, b, c, a, b, c, ...)"
1.1.0,"Lists of n_cells tensors of shape (N, 1)"
1.1.0,Lists of length n_cells
1.1.0,Lists of length n_cells
1.1.0,Get indices of k atoms closest to each cell point
1.1.0,"n_cells tensors of shape (26,)"
1.1.0,TODO(rbharath): Make this handle minibatches
1.1.0,"Shape (N_protein+N_ligand, 3)"
1.1.0,"Shape (N_protein+N_ligand,)"
1.1.0,"Shape (N_protein+N_ligand, M)"
1.1.0,"Shape (N_protein+N_ligand,)"
1.1.0,"Shape (N_protein+N_ligand, 3)"
1.1.0,"Shape (N_protein+N_ligand,)"
1.1.0,"Shape (N_protein+N_ligand, M)"
1.1.0,"Shape (N_protein+N_ligand, M, 3)"
1.1.0,"Shape (N_protein+N_ligand, M)"
1.1.0,"Shape (N_protein+N_ligand, M, 3)"
1.1.0,"Shape (N_protein+N_ligand, M)"
1.1.0,TODO(rbharath): Need to subtract out Van-der-Waals radii from dists
1.1.0,"Shape (N_protein+N_ligand, M)"
1.1.0,"Shape (N_protein+N_ligand, M)"
1.1.0,TODO(rbharath): Use RDKit to compute number of rotatable bonds in ligand.
1.1.0,TODO(rbharath): Autodock Vina only uses protein-ligand interactions in
1.1.0,computing free-energy. This implementation currently uses all interaction
1.1.0,terms. Not sure if this makes a difference.
1.1.0,"Shape (N_protein+N_ligand, M)"
1.1.0,Shape () -- scalar
1.1.0,# Gather Projection
1.1.0,"graph_model.add(dc.nn.Dense(128, activation='relu'))"
1.1.0,There should be 8 layers in graph_model
1.1.0,assert len(graph_model.layers) == 6
1.1.0,Add layers
1.1.0,Need to add batch-norm separately to test/support due to differing
1.1.0,shapes.
1.1.0,Apply an attention lstm layer
1.1.0,Gather Projection
1.1.0,Add layers
1.1.0,Need to add batch-norm separately to test/support due to differing
1.1.0,shapes.
1.1.0,Apply an attention lstm layer
1.1.0,Gather Projection
1.1.0,Degrees from 1 to max_deg inclusive
1.1.0,TODO(rbharath): Should this be 0 to max_deg inclusive?
1.1.0,"Should have shape (?, deg)"
1.1.0,"Shape of atom_features should be (?, n_feat)"
1.1.0,"Shape of deg_slice placeholder should be (max_deg+1-min_deg, 2)"
1.1.0,TODO(rbharath): Check that this operation is differentiable.
1.1.0,The number of cells which we should theoretically have
1.1.0,The number of cells which we should theoretically have
1.1.0,"Each atom neighbors tensor should be (k, ndim) shaped."
1.1.0,The number of cells which we should theoretically have
1.1.0,TODO(rbharath): The test below only checks that shapes work out.
1.1.0,Need to do a correctness implementation vs. a simple CPU impl.
1.1.0,The number of cells which we should theoretically have
1.1.0,TODO(rbharath): The test below only checks that shapes work out.
1.1.0,Need to do a correctness implementation vs. a simple CPU impl.
1.1.0,The number of cells which we should theoretically have
1.1.0,TODO(rbharath): The test below only checks that shapes work out.
1.1.0,Need to do a correctness implementation vs. a simple CPU impl.
1.1.0,TODO(rbharath): Commenting this out due to weird segfaults
1.1.0,def test_vina_generate_conformers(self):
1.1.0,"""""""Test that Vina Model can generate conformers"""""""
1.1.0,data_dir = os.path.dirname(os.path.realpath(__file__))
1.1.0,"protein_file = os.path.join(data_dir, ""1jld_protein.pdb"")"
1.1.0,"ligand_file = os.path.join(data_dir, ""1jld_ligand.pdb"")"
1.1.0,max_protein_atoms = 3500
1.1.0,max_ligand_atoms = 100
1.1.0,"print(""Loading protein file"")"
1.1.0,"protein_xyz, protein_mol = rdkit_util.load_molecule(protein_file)"
1.1.0,protein_Z = pad_array(
1.1.0,"np.array([atom.GetAtomicNum() for atom in protein_mol.GetAtoms()]),"
1.1.0,max_protein_atoms)
1.1.0,"print(""Loading ligand file"")"
1.1.0,"ligand_xyz, ligand_mol = rdkit_util.load_molecule(ligand_file)"
1.1.0,ligand_Z = pad_array(
1.1.0,"np.array([atom.GetAtomicNum() for atom in ligand_mol.GetAtoms()]),"
1.1.0,max_ligand_atoms)
1.1.0,-*- coding: utf-8 -*-
1.1.0,Assigning featurizer if not user defined
1.1.0,loading datasets
1.1.0,Assembling train and valid datasets
1.1.0,!/usr/bin/env python2
1.1.0,-*- coding: utf-8 -*-
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow MultiTaskDNN model
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow robust MultiTaskDNN model
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow logistic regression model
1.1.0,Loading hyper parameters
1.1.0,Transform fingerprints to IRV features
1.1.0,Building tensorflow IRV model
1.1.0,Loading hyper parameters
1.1.0,Gather Projection
1.1.0,Loading hyper parameters
1.1.0,Loading hyper parameters
1.1.0,Building scikit random forest model
1.1.0,Loading hyper parameters
1.1.0,Building xgboost classification model
1.1.0,Loading hyper parameters
1.1.0,Building tensorflow MultiTaskDNN model
1.1.0,Loading hyper parameters
1.1.0,Initialize model folder
1.1.0,Loading hyper parameters
1.1.0,Gather Projection
1.1.0,Loading hyper parameters
1.1.0,Loading hyper parameters
1.1.0,Loading hyper parameters
1.1.0,Building scikit random forest model
1.1.0,Loading hyper parameters
1.1.0,Building xgboost classification model
1.1.0,Loading hyperparameters
1.1.0,num positive/negative ligands
1.1.0,Set batch sizes for network
1.1.0,Model structure
1.1.0,Traning settings
1.1.0,Fit trained model
1.1.0,Evaluating low data model
1.1.0,-*- coding: utf-8 -*-
1.1.0,Assigning featurizer if not user defined
1.1.0,loading datasets
1.1.0,
1.1.0,Note by @XericZephyr. Reason why I spun off this function:
1.1.0,1. Some model needs dataset information.
1.1.0,2. It offers us possibility to **cache** the dataset
1.1.0,"if the featurizer runs very slow, e.g., GraphConv."
1.1.0,2+. The cache can even happen at Travis CI to accelerate
1.1.0,CI testing.
1.1.0,
1.1.0,loading datasets
1.1.0,!/usr/bin/env python2
1.1.0,-*- coding: utf-8 -*-
1.1.0,Featurize qm9 dataset
1.1.0,transformers = [
1.1.0,"deepchem.trans.LogTransformer(transform_X=True),"
1.1.0,"deepchem.trans.NormalizationTransformer(transform_y=True,"
1.1.0,dataset=train_dataset)]
1.1.0,Set shard size low to avoid memory problems.
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Set some global variables up top
1.1.0,Featurize KAGGLE dataset
1.1.0,############################################################# TIMING
1.1.0,############################################################# TIMING
1.1.0,Featurize qm7 dataset
1.1.0,Featurize clintox dataset
1.1.0,Transform clintox dataset
1.1.0,Split clintox dataset
1.1.0,Featurize bbb dataset
1.1.0,Initialize transformers
1.1.0,Load nci dataset
1.1.0,Featurize nci dataset
1.1.0,Initialize transformers
1.1.0,Featurize HOPV dataset
1.1.0,Initialize transformers
1.1.0,Featurize PPB dataset
1.1.0,Initialize transformers
1.1.0,Load MUV dataset
1.1.0,Featurize MUV dataset
1.1.0,Initialize transformers
1.1.0,Featurize clearance dataset
1.1.0,Initialize transformers
1.1.0,Featurize TOXCAST dataset
1.1.0,Initialize transformers
1.1.0,Featurize bace dataset
1.1.0,Initialize transformers
1.1.0,Featurize bace dataset
1.1.0,Initialize transformers
1.1.0,Featurize Tox21 dataset
1.1.0,Initialize transformers
1.1.0,Featurize ChEMBL dataset
1.1.0,Initialize transformers
1.1.0,Featurize hiv dataset
1.1.0,Initialize transformers
1.1.0,Featurize SIDER dataset
1.1.0,Initialize transformers
1.1.0,Featurize SAMPL dataset
1.1.0,Initialize transformers
1.1.0,Featurize Delaney dataset
1.1.0,Initialize transformers
1.1.0,Featurize PCBA dataset
1.1.0,Initialize transformers
1.1.0,Featurize Lipophilicity dataset
1.1.0,Initialize transformers
1.1.0,TODO(rbharath): This function is complicated and monolithic. Is there a nice
1.1.0,way to refactor this?
1.1.0,arbitrarily return last model
1.1.0,Define train dataset
1.1.0,Define validation dataset
1.1.0,TODO (Bowen): make this function less memory intensive
1.1.0,set 1st column as the column index of dataframe
1.1.0,merge descriptor and activities dataframe into output dataframe based on
1.1.0,"the molecule name, which is the index for both dataframes (but named"
1.1.0,differently). Default merge is inner merge
1.1.0,need to manually set dataframe indexname after merge based on index
1.1.0,from deepchem.scripts.dock_dude import *
1.1.0,from ipyparallel import Client
1.1.0,rc = Client()
1.1.0,dview = rc[:]
1.1.0,"prepare_ligands_and_dock_ligands_to_receptors(""/home/enf/datasets/all"", ""/home/enf/deep-docking/shallow/dude_docked"", dview)"
1.1.0,
1.1.0,"If mol_id is not set, then use isomeric smiles as unique identifier"
1.1.0,iterator = data_df.iterrows()
1.1.0,TODO(rbharath): BROKEN!
1.1.0,Trim unwanted indexing fields
1.1.0,Connect to running ipython server
1.1.0,Copyright 2016 The TensorFlow Authors. All Rights Reserved.
1.1.0,
1.1.0,"Licensed under the Apache License, Version 2.0 (the ""License"");"
1.1.0,you may not use this file except in compliance with the License.
1.1.0,You may obtain a copy of the License at
1.1.0,
1.1.0,http://www.apache.org/licenses/LICENSE-2.0
1.1.0,
1.1.0,"Unless required by applicable law or agreed to in writing, software"
1.1.0,"distributed under the License is distributed on an ""AS IS"" BASIS,"
1.1.0,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
1.1.0,See the License for the specific language governing permissions and
1.1.0,limitations under the License.
1.1.0,==============================================================================
1.1.0,Maps from a function name to a dictionary that describes how to
1.1.0,map from an old argument keyword to the new argument keyword.
1.1.0,Mapping from function to the new name of the function
1.1.0,Functions that were reordered should be changed to the new keyword args
1.1.0,"for safety, if positional arguments are used. If you have reversed the"
1.1.0,"positional arguments yourself, this could do the wrong thing."
1.1.0,Specially handled functions.
1.1.0,TODO(aselle): Could check for a literal list of bools and try to convert
1.1.0,them to indices.
1.1.0,all edits are lists of chars
1.1.0,Iterate of each line
1.1.0,sort by column so that edits are processed in order in order to make
1.1.0,indexing adjustments cumulative for changes that change the string
1.1.0,length
1.1.0,"Extract each line to a list of characters, because mutable lists"
1.1.0,"are editable, unlike immutable strings."
1.1.0,Record a description of the change
1.1.0,Make underscore buffers for underlining where in the line the edit was
1.1.0,Iterate for each edit
1.1.0,"Create effective start, end by accounting for change in length due"
1.1.0,to previous edits
1.1.0,Make sure the edit is changing what it should be changing
1.1.0,Make the edit
1.1.0,Create the underline highlighting of the before and after
1.1.0,Keep track of how to generate effective ranges
1.1.0,Finish the report comment
1.1.0,"Strangely, ast.ListComp returns the col_offset of the first token"
1.1.0,after the '[' token which appears to be a bug. Workaround by
1.1.0,explicitly finding the real start of the list comprehension.
1.1.0,loop over lines
1.1.0,Reverse the text to and regular expression search for whitespace
1.1.0,First find if a [ can be found with only whitespace between it and
1.1.0,col.
1.1.0,TODO(aselle):
1.1.0,"this is poor comment detection, but it is good enough for"
1.1.0,cases where the comment does not contain string literal starting/
1.1.0,ending characters. If ast gave us start and end locations of the
1.1.0,"ast nodes rather than just start, we could use string literal"
1.1.0,node ranges to filter out spurious #'s that appear in string
1.1.0,literals.
1.1.0,"Most other nodes return proper locations (with notably does not), but"
1.1.0,it is not possible to use that in an argument.
1.1.0,"Find a simple attribute name path e.g. ""tf.foo.bar"""
1.1.0,Make sure the func is marked as being part of a call
1.1.0,Call special handlers
1.1.0,Examine any non-keyword argument and make it into a keyword argument
1.1.0,if reordering required.
1.1.0,Examine each keyword argument and convert it to the final renamed form
1.1.0,TODO(aselle): We should scan backward to find the start of the
1.1.0,keyword key. Unfortunately ast does not give you the location of
1.1.0,"keyword keys, so we are forced to infer it from the keyword arg"
1.1.0,value.
1.1.0,"Write to a temporary file, just in case we are doing an implace modify."
1.1.0,Broad exceptions are required here because ast throws whatever it wants.
1.1.0,pylint: disable=broad-except
1.1.0,pylint: enable=broad-except
1.1.0,make sure output directory doesn't exist
1.1.0,make sure output directory does not overlap with root_directory
1.1.0,Collect list of files to process (we do this to correctly handle if the
1.1.0,user puts the output directory in some sub directory of the input dir)
1.1.0,import os
1.1.0,"from deepchem.utils.save import load_from_disk, save_to_disk"
1.1.0,from deepchem.featurizers.fingerprints import CircularFingerprint
1.1.0,from deepchem.featurizers.basic import RDKitDescriptors
1.1.0,from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer
1.1.0,from deepchem.featurizers.grid_featurizer import GridFeaturizer
1.1.0,from deepchem.featurizers.featurize import DataLoader
1.1.0,
1.1.0,"dataset_file = ""../../../datasets/pdbbind_full_df.pkl.gz"""
1.1.0,"print(""About to load dataset form disk."")"
1.1.0,dataset = load_from_disk(dataset_file)
1.1.0,"print(""Loaded dataset."")"
1.1.0,
1.1.0,grid_featurizer = GridFeaturizer(
1.1.0,"voxel_width=16.0, feature_types=""voxel_combined"","
1.1.0,"voxel_feature_types=[""ecfp"", ""splif"", ""hbond"", ""pi_stack"", ""cation_pi"","
1.1.0,"""salt_bridge""], ecfp_power=9, splif_power=9,"
1.1.0,"parallel=True, flatten=True)"
1.1.0,featurizers = [CircularFingerprint(size=1024)]
1.1.0,"featurizers += [grid_featurizer, NNScoreComplexFeaturizer()]"
1.1.0,
1.1.0,#Make a directory in which to store the featurized complexes.
1.1.0,"base_dir = ""../../../grid_nnscore_circular_features"""
1.1.0,if not os.path.exists(base_dir):
1.1.0,os.makedirs(base_dir)
1.1.0,"data_dir = os.path.join(base_dir, ""data"")"
1.1.0,if not os.path.exists(data_dir):
1.1.0,os.makedirs(data_dir)
1.1.0,
1.1.0,"featurized_samples_file = os.path.join(data_dir, ""featurized_samples.joblib"")"
1.1.0,
1.1.0,"feature_dir = os.path.join(base_dir, ""features"")"
1.1.0,if not os.path.exists(feature_dir):
1.1.0,os.makedirs(feature_dir)
1.1.0,
1.1.0,"samples_dir = os.path.join(base_dir, ""samples"")"
1.1.0,if not os.path.exists(samples_dir):
1.1.0,os.makedirs(samples_dir)
1.1.0,
1.1.0,
1.1.0,
1.1.0,featurizers = compound_featurizers + complex_featurizers
1.1.0,"featurizer = DataLoader(tasks=[""label""],"
1.1.0,"smiles_field=""smiles"","
1.1.0,"protein_pdb_field=""protein_pdb"","
1.1.0,"ligand_pdb_field=""ligand_pdb"","
1.1.0,"compound_featurizers=compound_featurizers,"
1.1.0,"complex_featurizers=complex_featurizers,"
1.1.0,"id_field=""complex_id"","
1.1.0,verbose=False)
1.1.0,from ipyparallel import Client
1.1.0,c = Client()
1.1.0,"print(""c.ids"")"
1.1.0,print(c.ids)
1.1.0,dview = c[:]
1.1.0,"featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,"
1.1.0,"worker_pool=dview, shard_size=1024)"
1.1.0,
1.1.0,"save_to_disk(featurized_samples, featurized_samples_file)"
1.1.0,"print(""Preparing ligand %s"" % mol_name)"
0.0.4,-*- coding: utf-8 -*-
0.0.4,
0.0.4,"deepchem documentation build configuration file, created by"
0.0.4,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
0.0.4,
0.0.4,This file is execfile()d with the current directory set to its
0.0.4,containing dir.
0.0.4,
0.0.4,Note that not all possible configuration values are present in this
0.0.4,autogenerated file.
0.0.4,
0.0.4,All configuration values have a default; values that are commented out
0.0.4,serve to show the default.
0.0.4,"If extensions (or modules to document with autodoc) are in another directory,"
0.0.4,add these directories to sys.path here. If the directory is relative to the
0.0.4,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.0.4,"sys.path.insert(0, os.path.abspath('.'))"
0.0.4,-- General configuration ------------------------------------------------
0.0.4,"If your documentation needs a minimal Sphinx version, state it here."
0.0.4,needs_sphinx = '1.0'
0.0.4,"Add any Sphinx extension module names here, as strings. They can be"
0.0.4,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.0.4,ones.
0.0.4,"Add any paths that contain templates here, relative to this directory."
0.0.4,The suffix(es) of source filenames.
0.0.4,You can specify multiple suffix as a list of string:
0.0.4,"source_suffix = ['.rst', '.md']"
0.0.4,The encoding of source files.
0.0.4,source_encoding = 'utf-8-sig'
0.0.4,The master toctree document.
0.0.4,General information about the project.
0.0.4,"The version info for the project you're documenting, acts as replacement for"
0.0.4,"|version| and |release|, also used in various other places throughout the"
0.0.4,built documents.
0.0.4,
0.0.4,The short X.Y version.
0.0.4,"The full version, including alpha/beta/rc tags."
0.0.4,The language for content autogenerated by Sphinx. Refer to documentation
0.0.4,for a list of supported languages.
0.0.4,
0.0.4,This is also used if you do content translation via gettext catalogs.
0.0.4,"Usually you set ""language"" from the command line for these cases."
0.0.4,"There are two options for replacing |today|: either, you set today to some"
0.0.4,"non-false value, then it is used:"
0.0.4,today = ''
0.0.4,"Else, today_fmt is used as the format for a strftime call."
0.0.4,"today_fmt = '%B %d, %Y'"
0.0.4,"List of patterns, relative to source directory, that match files and"
0.0.4,directories to ignore when looking for source files.
0.0.4,The reST default role (used for this markup: `text`) to use for all
0.0.4,documents.
0.0.4,default_role = None
0.0.4,"If true, '()' will be appended to :func: etc. cross-reference text."
0.0.4,add_function_parentheses = True
0.0.4,"If true, the current module name will be prepended to all description"
0.0.4,unit titles (such as .. function::).
0.0.4,add_module_names = True
0.0.4,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.0.4,output. They are ignored by default.
0.0.4,show_authors = False
0.0.4,The name of the Pygments (syntax highlighting) style to use.
0.0.4,A list of ignored prefixes for module index sorting.
0.0.4,modindex_common_prefix = []
0.0.4,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.0.4,keep_warnings = False
0.0.4,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.0.4,-- Options for HTML output ----------------------------------------------
0.0.4,The theme to use for HTML and HTML Help pages.  See the documentation for
0.0.4,a list of builtin themes.
0.0.4,Theme options are theme-specific and customize the look and feel of a theme
0.0.4,"further.  For a list of options available for each theme, see the"
0.0.4,documentation.
0.0.4,html_theme_options = {}
0.0.4,"Add any paths that contain custom themes here, relative to this directory."
0.0.4,html_theme_path = []
0.0.4,"The name for this set of Sphinx documents.  If None, it defaults to"
0.0.4,"""<project> v<release> documentation""."
0.0.4,html_title = None
0.0.4,A shorter title for the navigation bar.  Default is the same as html_title.
0.0.4,html_short_title = None
0.0.4,The name of an image file (relative to this directory) to place at the top
0.0.4,of the sidebar.
0.0.4,html_logo = None
0.0.4,The name of an image file (within the static path) to use as favicon of the
0.0.4,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.0.4,pixels large.
0.0.4,html_favicon = None
0.0.4,"Add any paths that contain custom static files (such as style sheets) here,"
0.0.4,"relative to this directory. They are copied after the builtin static files,"
0.0.4,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.0.4,Add any extra paths that contain custom files (such as robots.txt or
0.0.4,".htaccess) here, relative to this directory. These files are copied"
0.0.4,directly to the root of the documentation.
0.0.4,html_extra_path = []
0.0.4,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.0.4,using the given strftime format.
0.0.4,"html_last_updated_fmt = '%b %d, %Y'"
0.0.4,"If true, SmartyPants will be used to convert quotes and dashes to"
0.0.4,typographically correct entities.
0.0.4,html_use_smartypants = True
0.0.4,"Custom sidebar templates, maps document names to template names."
0.0.4,html_sidebars = {}
0.0.4,"Additional templates that should be rendered to pages, maps page names to"
0.0.4,template names.
0.0.4,html_additional_pages = {}
0.0.4,"If false, no module index is generated."
0.0.4,html_domain_indices = True
0.0.4,"If false, no index is generated."
0.0.4,html_use_index = True
0.0.4,"If true, the index is split into individual pages for each letter."
0.0.4,html_split_index = False
0.0.4,"If true, links to the reST sources are added to the pages."
0.0.4,html_show_sourcelink = True
0.0.4,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.0.4,html_show_sphinx = True
0.0.4,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.0.4,html_show_copyright = True
0.0.4,"If true, an OpenSearch description file will be output, and all pages will"
0.0.4,contain a <link> tag referring to it.  The value of this option must be the
0.0.4,base URL from which the finished HTML is served.
0.0.4,html_use_opensearch = ''
0.0.4,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.0.4,html_file_suffix = None
0.0.4,Language to be used for generating the HTML full-text search index.
0.0.4,Sphinx supports the following languages:
0.0.4,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
0.0.4,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
0.0.4,html_search_language = 'en'
0.0.4,"A dictionary with options for the search language support, empty by default."
0.0.4,Now only 'ja' uses this config value
0.0.4,html_search_options = {'type': 'default'}
0.0.4,The name of a javascript file (relative to the configuration directory) that
0.0.4,"implements a search results scorer. If empty, the default will be used."
0.0.4,html_search_scorer = 'scorer.js'
0.0.4,Output file base name for HTML help builder.
0.0.4,-- Options for LaTeX output ---------------------------------------------
0.0.4,The paper size ('letterpaper' or 'a4paper').
0.0.4,"'papersize': 'letterpaper',"
0.0.4,"The font size ('10pt', '11pt' or '12pt')."
0.0.4,"'pointsize': '10pt',"
0.0.4,Additional stuff for the LaTeX preamble.
0.0.4,"'preamble': '',"
0.0.4,Latex figure (float) alignment
0.0.4,"'figure_align': 'htbp',"
0.0.4,Grouping the document tree into LaTeX files. List of tuples
0.0.4,"(source start file, target name, title,"
0.0.4,"author, documentclass [howto, manual, or own class])."
0.0.4,The name of an image file (relative to this directory) to place at the top of
0.0.4,the title page.
0.0.4,latex_logo = None
0.0.4,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.0.4,not chapters.
0.0.4,latex_use_parts = False
0.0.4,"If true, show page references after internal links."
0.0.4,latex_show_pagerefs = False
0.0.4,"If true, show URL addresses after external links."
0.0.4,latex_show_urls = False
0.0.4,Documents to append as an appendix to all manuals.
0.0.4,latex_appendices = []
0.0.4,"If false, no module index is generated."
0.0.4,latex_domain_indices = True
0.0.4,-- Options for manual page output ---------------------------------------
0.0.4,One entry per manual page. List of tuples
0.0.4,"(source start file, name, description, authors, manual section)."
0.0.4,"If true, show URL addresses after external links."
0.0.4,man_show_urls = False
0.0.4,-- Options for Texinfo output -------------------------------------------
0.0.4,Grouping the document tree into Texinfo files. List of tuples
0.0.4,"(source start file, target name, title, author,"
0.0.4,"dir menu entry, description, category)"
0.0.4,Documents to append as an appendix to all manuals.
0.0.4,texinfo_appendices = []
0.0.4,"If false, no module index is generated."
0.0.4,texinfo_domain_indices = True
0.0.4,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.0.4,texinfo_show_urls = 'footnote'
0.0.4,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.0.4,texinfo_no_detailmenu = False
0.0.4,Example configuration for intersphinx: refer to the Python standard library.
0.0.4,lines in the label file have format
0.0.4,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
0.0.4,"print line[0], line[3]"
0.0.4,TODO(rbharath): Use standard joblib once old-data has been regenerated.
0.0.4,import joblib
0.0.4,First line of user-specified CSV *must* be header.
0.0.4,working-with-3d-molecules
0.0.4,initial embedding
0.0.4,minimization and pruning
0.0.4,always keep lowest-energy conformer
0.0.4,discard conformers after max_conformers is reached
0.0.4,get RMSD to selected conformers
0.0.4,discard conformers within the RMSD threshold
0.0.4,create a new molecule to hold the chosen conformers
0.0.4,this ensures proper conformer IDs and energy-based ordering
0.0.4,TODO(rbharath): The semantics of this class are very difficult to debug.
0.0.4,"Multiple transformations of the data are performed on disk, and computations"
0.0.4,of mean/std are spread across multiple functions for efficiency. Some
0.0.4,refactoring needs to happen here.
0.0.4,TODO(rbharath): Still a bit of information leakage.
0.0.4,TODO(rbharath): FeaturizedSamples should not be responsible for
0.0.4,"X-transform, X_sums, etc. Move that stuff over to Dataset."
0.0.4,"input/output transforms not specified yet, so"
0.0.4,"self.transforms = (input_transforms, output_transforms) =>"
0.0.4,TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
0.0.4,"called without calling transform(), it will explode. Maybe have a separate"
0.0.4,initialization function to avoid this problem.
0.0.4,Store input_transforms/output_transforms so the dataset remembers its state.
0.0.4,TODO(rbharath): These lines are puzzling. Better way to avoid storage
0.0.4,duplication here?
0.0.4,Turns NaNs to zeros
0.0.4,"The following are all associated with Dataset, but are separate functions to"
0.0.4,make it easy to use multiprocessing.
0.0.4,TODO(rbharath): This is a hack. clean up.
0.0.4,TODO(rbharath): Should X be saved to out_X_transformed as well? Since
0.0.4,itershards expects to loop over X-transformed? (Ditto for y/w)
0.0.4,perform common train/test split across all tasks
0.0.4,Set missing data to have weight zero
0.0.4,Note that X_n is a list of floats
0.0.4,"Note y_n is a list of arrays of shape (n_tasks,)"
0.0.4,TODO(rbharath): This is a hack based on fact that multi-tasktype models
0.0.4,aren't supported.
0.0.4,"Sometimes all samples have zero weight. In this case, continue."
0.0.4,We need to import models so they can be created by model_builder
0.0.4,Featurize input
0.0.4,Transform data into arrays for ML
0.0.4,Split into train/test
0.0.4,Transforming train/test data
0.0.4,Fit model
0.0.4,Eval model on train
0.0.4,TODO(rbharath): There should be some automatic check to ensure that all
0.0.4,required model_params are specified.
0.0.4,Featurize input
0.0.4,Transform data into arrays for ML
0.0.4,Split into train/test
0.0.4,Transforming train/test data
0.0.4,Fit model
0.0.4,Eval model on train
0.0.4,Moving imports to be local to avoid isnstall issues with
0.0.4,"Convolution3D, which is not yet part of keras proper."
0.0.4,number of convolutional filters to use at each layer
0.0.4,level of pooling to perform at each layer (POOL x POOL)
0.0.4,level of convolution to perform at each layer (CONV x CONV)
0.0.4,"TODO(rbharath): If we change away from axis-size 32, this code will break."
0.0.4,Eventually figure out a more general rule that works for all axis sizes.
0.0.4,Note that keras requires the model architecture and weights to be stored
0.0.4,separately. A json file is generated that specifies the model architecture.
0.0.4,The weights will be stored in an h5 file. The pkl.gz file with store the
0.0.4,target name.
0.0.4,Save architecture
0.0.4,Add eps weight to avoid minibatches with zero weight (causes theano to crash).
0.0.4,"Class probabilities are predicted for classification outputs. Instead,"
0.0.4,output the most likely class.
0.0.4,TODO(rbharath): This does not work with very large datasets! sklearn does
0.0.4,"support partial_fit, but only for some models. Might make sense to make"
0.0.4,PartialSklearnModel subclass at some point to support large data models.
0.0.4,List of registered models
0.0.4,TODO(rbharath/enf): We need a structured way to deal with potential GPU
0.0.4,memory overflows.
0.0.4,TODO(rbharath): The structure of the produced df might be
0.0.4,complicated. Better way to model?
0.0.4,TODO(rbharath): This feels like a total hack. Is there a structured way
0.0.4,to deal with this instead?
0.0.4,# The following notice is copied from the original NNScore file.
0.0.4,NNScore 2.01 is released under the GNU General Public License (see
0.0.4,http://www.gnu.org/licenses/gpl.html).
0.0.4,"If you have any questions, comments, or suggestions, please don't"
0.0.4,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.4,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.4,HERE].
0.0.4,ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
0.0.4,"This is just a scaling factor, so it's set so as to keep the network"
0.0.4,inputs roughly contained in 0-1
0.0.4,"O-H distance is 0.96 A, N-H is 1.01 A. See"
0.0.4,http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
0.0.4,"If atoms are < 2.5 A apart, we count it as a close contact"
0.0.4,"If receptor and ligand atoms are > 4 A apart, we consider them"
0.0.4,unable to interact with simple electrostatics.
0.0.4,"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
0.0.4,"distance of 7.5 A is good cutoff. This seems really big to me,"
0.0.4,except that pi-pi interactions (parallel) are actually usually
0.0.4,off centered. Interesting paper.  Note that adenine and
0.0.4,"tryptophan count as two aromatic rings. So, for example, an"
0.0.4,"interaction between these two, if positioned correctly, could"
0.0.4,count for 4 pi-pi interactions.
0.0.4,Cation-pi interaction cutoff based on
0.0.4,"""Cation-pi interactions in structural biology."""
0.0.4,4  is good cutoff for salt bridges according to
0.0.4,"""Close-Range Electrostatic Interactions in Proteins"","
0.0.4,"but looking at complexes, I decided to go with 5.5 A"
0.0.4,This is perhaps controversial. I noticed that often a pi-cation
0.0.4,"interaction or other pi interaction was only slightly off, but"
0.0.4,"looking at the structure, it was clearly supposed to be a pi-cation"
0.0.4,interaction. I've decided then to artificially expand the radius of
0.0.4,"each pi ring. Think of this as adding in a VDW radius, or"
0.0.4,"accounting for poor crystal-structure resolution, or whatever you"
0.0.4,want to justify it.
0.0.4,note that dictionaries (hashtables) are passed by reference in python
0.0.4,Now see if there's hydrophobic contacts (C-C contacts)
0.0.4,to convert into J/mol; might be nice to double check this
0.0.4,TODO(bramsundar): What are units of
0.0.4,ligand_charge/receptor_charge?
0.0.4,"so they're more or less perpendicular, it's probably a"
0.0.4,"pi-edge interaction having looked at many structures, I"
0.0.4,noticed the algorithm was identifying T-pi reactions
0.0.4,"when the two rings were in fact quite distant, often"
0.0.4,"with other atoms in between. Eye-balling it, requiring"
0.0.4,that at their closest they be at least 5 A apart seems
0.0.4,to separate the good T's from the bad
0.0.4,"so at their closest points, the two rings come within"
0.0.4,5 A of each other.
0.0.4,"okay, is the ligand pi pointing into the receptor"
0.0.4,"pi, or the other way around?  first, project the"
0.0.4,center of the ligand pi onto the plane of the
0.0.4,"receptor pi, and vs. versa"
0.0.4,"This could be directional somehow, like a hydrogen"
0.0.4,bond.
0.0.4,"now, if it's a true pi-T interaction, this projected"
0.0.4,point should fall within the ring whose plane it's
0.0.4,been projected into.
0.0.4,so it is in the ring on the projected plane.
0.0.4,since it could be interacting with a cofactor or something
0.0.4,Now see if there's some sort of hydrogen bond between
0.0.4,"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
0.0.4,H_BOND_ANGLE.
0.0.4,TODO(rbharath): This is a horrible inner-loop search. Can
0.0.4,this be made more efficient?
0.0.4,Make sure to set comment (used below)
0.0.4,Make sure to set comment (used below)
0.0.4,"print ""nearby hydrogens: "" + str(hydrogens)"
0.0.4,now we need to check the angles
0.0.4,"TODO(rbharath): Rather than using this heuristic, it seems like"
0.0.4,it might be better to just report the angle in the feature
0.0.4,vector...
0.0.4,"so there could be some pi-pi interactions.  Now, let's"
0.0.4,check for stacking interactions. Are the two pi's roughly
0.0.4,parallel?
0.0.4,"so they're more or less parallel, it's probably pi-pi"
0.0.4,"stacking now, since pi-pi are not usually right on"
0.0.4,top of each other. They're often staggered. So I don't
0.0.4,want to just look at the centers of the rings and
0.0.4,compare. Let's look at each of the atoms.  do atom of
0.0.4,"the atoms of one ring, when projected onto the plane of"
0.0.4,"the other, fall within that other ring?"
0.0.4,start by assuming it's not a pi-pi stacking interaction
0.0.4,project the ligand atom onto the plane of the receptor ring
0.0.4,TODO(rbharath): This if-else is confusing.
0.0.4,project the ligand atom onto the plane of the receptor ring
0.0.4,since it could be interacting with a cofactor or something
0.0.4,project the charged onto the plane of the aromatic
0.0.4,since it could be interacting with a cofactor or something
0.0.4,now it's the ligand that has the aromatic group
0.0.4,since it could be interacting with a cofactor or something
0.0.4,so they have oppositve charges
0.0.4,TODO(rbharath): What is atom type A here?
0.0.4,Load receptor and ligand from file.
0.0.4,## OPEN TEMPDIR
0.0.4,## CLOSE TEMPDIR
0.0.4,NNScore 2.01 is released under the GNU General Public License (see
0.0.4,http://www.gnu.org/licenses/gpl.html).
0.0.4,"If you have any questions, comments, or suggestions, please don't"
0.0.4,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.4,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.4,HERE].
0.0.4,"AddHydrogens(polaronly, correctForPH, pH)"
0.0.4,a*x + b*y + c*z = dI think that
0.0.4,"self.x, self.y, self.z = x, y, z"
0.0.4,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
0.0.4,TODO(bramsundar): Should this be __copy__?
0.0.4,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
0.0.4,"return np.array([self.x, self.y, self.z])"
0.0.4,TODO(rbharath): Should this be an atom function?
0.0.4,"This line is necessary for babel to work, though many PDBs in"
0.0.4,the PDB would have this line commented out
0.0.4,now atom type (for pdbqt)
0.0.4,"If atomtype is not specified, but atomname is, set atomtype to the"
0.0.4,"first letter of atomname. This heuristic suffices for proteins,"
0.0.4,since no two-letter elements appear in standard amino acids.
0.0.4,Any number needs to be removed from the element name
0.0.4,"this only uses the rightmost three characters, essentially"
0.0.4,removing unique rotamer identification
0.0.4,"The normal vector to plane is n = [a, b, c]"
0.0.4,We first shift by basepoint (a point on given plane) to make math
0.0.4,simpler. basepoint is given by d/||n||^2 * n
0.0.4,The perpendicular component of diff to plane is
0.0.4,(n^T diff / ||n||^2) * n
0.0.4,generate SMILES for fragments
0.0.4,import all Featurizer subclasses so __subclasses__ will work
0.0.4,these have to be local imports to avoid circular imports
0.0.4,get output from engines
0.0.4,get the maximum number of conformers
0.0.4,construct the new container
0.0.4,- first axis = # mols
0.0.4,- second axis = max # conformers
0.0.4,- remaining axes = determined by feature shape
0.0.4,fill in the container
0.0.4,"If gzipped, need to compute extension again"
0.0.4,"If CSV input, assume that first row contains labels"
0.0.4,Skip labels
0.0.4,"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
0.0.4,raw_df = pd.DataFrame.from_records(processed_rows)
0.0.4,"pandas rows are tuples (row_num, row_data)"
0.0.4,The standard columns for featurized data.
0.0.4,"compounds_df is not altered by any method after initialization, so it's"
0.0.4,safe to keep a copy in memory and on disk.
0.0.4,TODO(rbharath): Might this be inefficient?
0.0.4,Sort from largest to smallest scaffold sets
0.0.4,list-of-available-descriptors.
0.0.4,NNScore 2.01 is released under the GNU General Public License (see
0.0.4,http://www.gnu.org/licenses/gpl.html).
0.0.4,"If you have any questions, comments, or suggestions, please don't"
0.0.4,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.4,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.4,HERE].
0.0.4,Remove rings of length 0
0.0.4,"To remove duplicate entries, we convert rings from a list to set, and"
0.0.4,then back to a list again. There's a snafu since each ring in rings is
0.0.4,itself a list (and lists are unhashable in python). To circumvent this
0.0.4,"issue, we convert each ring into a string (after sorting). For example,"
0.0.4,"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
0.0.4,"original lists, we use ast.literal_eval."
0.0.4,Use dictionary to maintain state about which rings are supersets.
0.0.4,All distances are in Angstroms. Duplicate pairs not specified. For
0.0.4,"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
0.0.4,"This one not from source sited above. Not sure where it's from, but"
0.0.4,"it wouldn't ever be used in the current context (""AutoGrow"")"
0.0.4,estimate based on eye balling Handbook of Chemistry and Physics
0.0.4,Reset internal state
0.0.4,Now load the file into a list
0.0.4,"Load atom data (coordinates, etc.)"
0.0.4,this string unique identifies each atom
0.0.4,so each atom can only be loaded once. No rotamers.
0.0.4,So you're actually reindexing everything here.
0.0.4,### TODO(rbharath): Disabling loading of non
0.0.4,Check that the range is nonempty.
0.0.4,"just so no PDB is empty, VMD will load them all"
0.0.4,write coordinates
0.0.4,first get available index
0.0.4,now add atom
0.0.4,Add to non-protein list
0.0.4,Functions to determine the bond connectivity based on distance
0.0.4,==============================================================
0.0.4,Functions to identify positive charges
0.0.4,======================================
0.0.4,Metallic atoms are assumed to be cations.
0.0.4,Get all the quartenary amines on non-protein residues (these are the
0.0.4,only non-protein groups that will be identified as positively
0.0.4,charged). Note that nitrogen has only 5 valence electrons (out of 8
0.0.4,"for a full shell), so any nitrogen with four bonds must be positively"
0.0.4,charged (think NH4+).
0.0.4,"a quartenary amine, so it's easy"
0.0.4,so the indices stored is just the index of the nitrogen and any
0.0.4,attached atoms
0.0.4,"maybe you only have two hydrogens added, but they're sp3 hybridized."
0.0.4,"Just count this as a quartenary amine, since I think the positive"
0.0.4,charge would be stabilized. This situation can arise with
0.0.4,lone-pair electron nitrogen compounds like pyrrolidine
0.0.4,(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
0.0.4,Test that the angles approximately match the tetrahedral 109
0.0.4,degrees
0.0.4,so indexes added are the nitrogen and any attached atoms.
0.0.4,let's check for a phosphate or anything where a phosphorus is bound
0.0.4,"to two oxygens, where both oxygens are bound to only one heavy atom"
0.0.4,(the phosphorus). I think this will get several phosphorus
0.0.4,substances.
0.0.4,now count the number of oxygens bound only to the phosphorus
0.0.4,"let's check for guanidino-like groups (actually H2N-C-NH2,"
0.0.4,where not CN3.)
0.0.4,if the carbon has only three atoms connected to it
0.0.4,"if true, carbon is connected to at least two nitrogens now,"
0.0.4,so we need to count the number of nitrogens that are only
0.0.4,connected to one heavy atom (the carbon)
0.0.4,Index of atom that connects this charged group to
0.0.4,"the rest of the molecule, ultimately to make sure"
0.0.4,it's sp3 hybridized. Remains -1 if no such atom exists.
0.0.4,TODO(rbharath): Is picking the first non-nitrogen atom
0.0.4,correct here?
0.0.4,Handle case of guanidinium cation
0.0.4,so there are at two nitrogens that are only
0.0.4,connected to the carbon (and probably some
0.0.4,hydrogens)
0.0.4,now you need to make sure connector_ind atom is sp3 hybridized
0.0.4,"There are only two ""guanidino"" nitrogens. Assume the"
0.0.4,negative charge is spread equally between the two.
0.0.4,a carboxylate carbon will have three items connected to it.
0.0.4,a carboxylate will have two oxygens connected to
0.0.4,"it. Now, each of the oxygens should be connected"
0.0.4,to only one heavy atom (so if it's connected to a
0.0.4,"hydrogen, that's okay)"
0.0.4,so it's a carboxylate! Add a negative charge.
0.0.4,Assume negative charge is centered between the two
0.0.4,oxygens.
0.0.4,let's check for a sulfonate or anything where a sulfur is
0.0.4,bound to at least three oxygens and at least three are
0.0.4,bound to only the sulfur (or the sulfur and a hydrogen).
0.0.4,the sulfur is bound to at least three oxygens now
0.0.4,count the number of oxygens that are only bound to the
0.0.4,sulfur
0.0.4,so there are at least three oxygens that are only
0.0.4,bound to the sulfur
0.0.4,Group atoms in the same residue together
0.0.4,Assign each atom a residue key.
0.0.4,Handle edge case of last residue.
0.0.4,Select those atoms which are part of the charged group.
0.0.4,Functions to identify aromatic rings
0.0.4,====================================
0.0.4,first identify the center point
0.0.4,now get the plane that defines this ring. Recall that there are
0.0.4,atleast 3-points in indices_of_ring by ValueError above.
0.0.4,# formula for plane will be ax + by + cz = d
0.0.4,"first, let's see if the last atom in this ring is a carbon"
0.0.4,connected to four atoms. That would be a quick way of
0.0.4,telling this is not an aromatic ring
0.0.4,now check the dihedral between the ring atoms to see if
0.0.4,it's flat
0.0.4,"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.4,ring[ind+3] range of this function is -pi to pi
0.0.4,now check the dihedral between the ring atoms and an atom
0.0.4,connected to the current atom to see if that's flat too.
0.0.4,"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.4,"ring[ind+3], range of this function is -pi to pi"
0.0.4,Get all the rings containing each of the atoms in the ligand
0.0.4,Aromatic rings are of length 5 or 6
0.0.4,"Due to data errors in PDB files, there are cases in which"
0.0.4,non-protein atoms are bonded to protein atoms. Manually remove these
0.0.4,"cases, by testing that ring atom indices are a subset of non-protein"
0.0.4,ring indices.
0.0.4,Aromatic rings are flat
0.0.4,Aromatic rings are of length <= 6
0.0.4,At least 3 indices are required to identify the aromatic plane.
0.0.4,if self.get_aromatic_marker(indices_of_ring) is None:
0.0.4,"raise ValueError(""None at %s for %s"" % (key,"
0.0.4,str(indices_of_ring)))
0.0.4,Tryptophan has two aromatic rings.
0.0.4,Functions to assign secondary structure to protein residues
0.0.4,===========================================================
0.0.4,"first, we need to know what residues are available"
0.0.4,print self.get_residues()
0.0.4,TODO(rbharath): Why magic number 8?
0.0.4,now make sure the first four all have the same resid and
0.0.4,the last four all have the same resid
0.0.4,TODO(rbharath): Ugly code right here...
0.0.4,Now give easier to use names to the atoms
0.0.4,Now compute the phi and psi dihedral angles
0.0.4,Now use those angles to determine if it's alpha or beta
0.0.4,A residue of index i is only going to be in an alpha helix
0.0.4,its CA is within 6 A of the CA of the residue i + 3
0.0.4,so it's in an alpha helix
0.0.4,so now compare that CA to all the other CA's
0.0.4,so it's also in an alpha helix
0.0.4,so this CA atom is one of the ones the first atom
0.0.4,might hydrogen bond with
0.0.4,so these two CA atoms are close enough together
0.0.4,that their residues are probably hydrogen bonded
0.0.4,Alpha helices are only alpha helices if they span at least 4
0.0.4,residues (to wrap around and hydrogen bond). I'm going to
0.0.4,"require them to span at least 5 residues, based on"
0.0.4,examination of many structures.
0.0.4,now go through each of the BETA CA atoms. A residue is only
0.0.4,going to be called a beta sheet if CA atom is within 6.0 A
0.0.4,"of another CA beta, same chain, but index difference > 2."
0.0.4,so it's in a beta sheet
0.0.4,so not comparing an atom to itself
0.0.4,so you're comparing it only to other BETA-sheet atoms
0.0.4,so require them to be on the same chain. needed to
0.0.4,indices can be fairly compared
0.0.4,so the two residues are not simply adjacent to each
0.0.4,other on the chain
0.0.4,so these to atoms are close to each other
0.0.4,Now some more post-processing needs to be done. Do this
0.0.4,again to clear up mess that may have just been created
0.0.4,"(single residue beta strand, for example)"
0.0.4,Beta sheets are usually at least 3 residues long
0.0.4,so they are sequential
0.0.4,Now update each of the atoms with this structural information
0.0.4,Use this list to perform sanity checks on alpha-helix and beta-sheet
0.0.4,labels.
0.0.4,check for separate count and SMILES entries for each fragment
0.0.4,## 3zso comes from PDBBind-CN
0.0.4,The ligand is also specified by pdbbind
0.0.4,"Currently, just verifies that nothing crashes."
0.0.4,## 3zp9 comes from PDBBind-CN
0.0.4,The ligand is also specified by pdbbind
0.0.4,## 3bwf comes from PDBBind-CN
0.0.4,The ligand is also specified by pdbbind
0.0.4,"The keys of these dicts are pairs of atomtypes, but the keys are"
0.0.4,"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
0.0.4,"atom types, there are N*(N+1)/2 unique pairs."
0.0.4,TODO(rbharath): Charges are not computed correctly for certain
0.0.4,ligands! (see 2y2h_ligand). Understand why this happens.
0.0.4,assert np.count_nonzero(np.array(electrostatics.values())) > 0
0.0.4,print counts
0.0.4,1zea is the only example that has any pi-stacking.
0.0.4,Lengths:
0.0.4,ligand_receptor_close_contacts: N*(N+1)/2
0.0.4,ligand_receptor_contacts: N*(N+1)/2
0.0.4,ligand_receptor_electrostatics: N*(N+1)/2
0.0.4,ligand_atom_counts: N
0.0.4,hbonds: 12
0.0.4,hydrophobics: 6
0.0.4,stacking: 3
0.0.4,pi_cation: 6
0.0.4,t_shaped: 3
0.0.4,active_site_flexibility: 6
0.0.4,salt_bridges: 3
0.0.4,rotatable_boonds_count: 1
0.0.4,We need to import models so they can be created by model_builder
0.0.3,-*- coding: utf-8 -*-
0.0.3,
0.0.3,"deepchem documentation build configuration file, created by"
0.0.3,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
0.0.3,
0.0.3,This file is execfile()d with the current directory set to its
0.0.3,containing dir.
0.0.3,
0.0.3,Note that not all possible configuration values are present in this
0.0.3,autogenerated file.
0.0.3,
0.0.3,All configuration values have a default; values that are commented out
0.0.3,serve to show the default.
0.0.3,"If extensions (or modules to document with autodoc) are in another directory,"
0.0.3,add these directories to sys.path here. If the directory is relative to the
0.0.3,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.0.3,"sys.path.insert(0, os.path.abspath('.'))"
0.0.3,-- General configuration ------------------------------------------------
0.0.3,"If your documentation needs a minimal Sphinx version, state it here."
0.0.3,needs_sphinx = '1.0'
0.0.3,"Add any Sphinx extension module names here, as strings. They can be"
0.0.3,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.0.3,ones.
0.0.3,"Add any paths that contain templates here, relative to this directory."
0.0.3,The suffix(es) of source filenames.
0.0.3,You can specify multiple suffix as a list of string:
0.0.3,"source_suffix = ['.rst', '.md']"
0.0.3,The encoding of source files.
0.0.3,source_encoding = 'utf-8-sig'
0.0.3,The master toctree document.
0.0.3,General information about the project.
0.0.3,"The version info for the project you're documenting, acts as replacement for"
0.0.3,"|version| and |release|, also used in various other places throughout the"
0.0.3,built documents.
0.0.3,
0.0.3,The short X.Y version.
0.0.3,"The full version, including alpha/beta/rc tags."
0.0.3,The language for content autogenerated by Sphinx. Refer to documentation
0.0.3,for a list of supported languages.
0.0.3,
0.0.3,This is also used if you do content translation via gettext catalogs.
0.0.3,"Usually you set ""language"" from the command line for these cases."
0.0.3,"There are two options for replacing |today|: either, you set today to some"
0.0.3,"non-false value, then it is used:"
0.0.3,today = ''
0.0.3,"Else, today_fmt is used as the format for a strftime call."
0.0.3,"today_fmt = '%B %d, %Y'"
0.0.3,"List of patterns, relative to source directory, that match files and"
0.0.3,directories to ignore when looking for source files.
0.0.3,The reST default role (used for this markup: `text`) to use for all
0.0.3,documents.
0.0.3,default_role = None
0.0.3,"If true, '()' will be appended to :func: etc. cross-reference text."
0.0.3,add_function_parentheses = True
0.0.3,"If true, the current module name will be prepended to all description"
0.0.3,unit titles (such as .. function::).
0.0.3,add_module_names = True
0.0.3,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.0.3,output. They are ignored by default.
0.0.3,show_authors = False
0.0.3,The name of the Pygments (syntax highlighting) style to use.
0.0.3,A list of ignored prefixes for module index sorting.
0.0.3,modindex_common_prefix = []
0.0.3,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.0.3,keep_warnings = False
0.0.3,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.0.3,-- Options for HTML output ----------------------------------------------
0.0.3,The theme to use for HTML and HTML Help pages.  See the documentation for
0.0.3,a list of builtin themes.
0.0.3,Theme options are theme-specific and customize the look and feel of a theme
0.0.3,"further.  For a list of options available for each theme, see the"
0.0.3,documentation.
0.0.3,html_theme_options = {}
0.0.3,"Add any paths that contain custom themes here, relative to this directory."
0.0.3,html_theme_path = []
0.0.3,"The name for this set of Sphinx documents.  If None, it defaults to"
0.0.3,"""<project> v<release> documentation""."
0.0.3,html_title = None
0.0.3,A shorter title for the navigation bar.  Default is the same as html_title.
0.0.3,html_short_title = None
0.0.3,The name of an image file (relative to this directory) to place at the top
0.0.3,of the sidebar.
0.0.3,html_logo = None
0.0.3,The name of an image file (within the static path) to use as favicon of the
0.0.3,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.0.3,pixels large.
0.0.3,html_favicon = None
0.0.3,"Add any paths that contain custom static files (such as style sheets) here,"
0.0.3,"relative to this directory. They are copied after the builtin static files,"
0.0.3,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.0.3,Add any extra paths that contain custom files (such as robots.txt or
0.0.3,".htaccess) here, relative to this directory. These files are copied"
0.0.3,directly to the root of the documentation.
0.0.3,html_extra_path = []
0.0.3,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.0.3,using the given strftime format.
0.0.3,"html_last_updated_fmt = '%b %d, %Y'"
0.0.3,"If true, SmartyPants will be used to convert quotes and dashes to"
0.0.3,typographically correct entities.
0.0.3,html_use_smartypants = True
0.0.3,"Custom sidebar templates, maps document names to template names."
0.0.3,html_sidebars = {}
0.0.3,"Additional templates that should be rendered to pages, maps page names to"
0.0.3,template names.
0.0.3,html_additional_pages = {}
0.0.3,"If false, no module index is generated."
0.0.3,html_domain_indices = True
0.0.3,"If false, no index is generated."
0.0.3,html_use_index = True
0.0.3,"If true, the index is split into individual pages for each letter."
0.0.3,html_split_index = False
0.0.3,"If true, links to the reST sources are added to the pages."
0.0.3,html_show_sourcelink = True
0.0.3,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.0.3,html_show_sphinx = True
0.0.3,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.0.3,html_show_copyright = True
0.0.3,"If true, an OpenSearch description file will be output, and all pages will"
0.0.3,contain a <link> tag referring to it.  The value of this option must be the
0.0.3,base URL from which the finished HTML is served.
0.0.3,html_use_opensearch = ''
0.0.3,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.0.3,html_file_suffix = None
0.0.3,Language to be used for generating the HTML full-text search index.
0.0.3,Sphinx supports the following languages:
0.0.3,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
0.0.3,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
0.0.3,html_search_language = 'en'
0.0.3,"A dictionary with options for the search language support, empty by default."
0.0.3,Now only 'ja' uses this config value
0.0.3,html_search_options = {'type': 'default'}
0.0.3,The name of a javascript file (relative to the configuration directory) that
0.0.3,"implements a search results scorer. If empty, the default will be used."
0.0.3,html_search_scorer = 'scorer.js'
0.0.3,Output file base name for HTML help builder.
0.0.3,-- Options for LaTeX output ---------------------------------------------
0.0.3,The paper size ('letterpaper' or 'a4paper').
0.0.3,"'papersize': 'letterpaper',"
0.0.3,"The font size ('10pt', '11pt' or '12pt')."
0.0.3,"'pointsize': '10pt',"
0.0.3,Additional stuff for the LaTeX preamble.
0.0.3,"'preamble': '',"
0.0.3,Latex figure (float) alignment
0.0.3,"'figure_align': 'htbp',"
0.0.3,Grouping the document tree into LaTeX files. List of tuples
0.0.3,"(source start file, target name, title,"
0.0.3,"author, documentclass [howto, manual, or own class])."
0.0.3,The name of an image file (relative to this directory) to place at the top of
0.0.3,the title page.
0.0.3,latex_logo = None
0.0.3,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.0.3,not chapters.
0.0.3,latex_use_parts = False
0.0.3,"If true, show page references after internal links."
0.0.3,latex_show_pagerefs = False
0.0.3,"If true, show URL addresses after external links."
0.0.3,latex_show_urls = False
0.0.3,Documents to append as an appendix to all manuals.
0.0.3,latex_appendices = []
0.0.3,"If false, no module index is generated."
0.0.3,latex_domain_indices = True
0.0.3,-- Options for manual page output ---------------------------------------
0.0.3,One entry per manual page. List of tuples
0.0.3,"(source start file, name, description, authors, manual section)."
0.0.3,"If true, show URL addresses after external links."
0.0.3,man_show_urls = False
0.0.3,-- Options for Texinfo output -------------------------------------------
0.0.3,Grouping the document tree into Texinfo files. List of tuples
0.0.3,"(source start file, target name, title, author,"
0.0.3,"dir menu entry, description, category)"
0.0.3,Documents to append as an appendix to all manuals.
0.0.3,texinfo_appendices = []
0.0.3,"If false, no module index is generated."
0.0.3,texinfo_domain_indices = True
0.0.3,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.0.3,texinfo_show_urls = 'footnote'
0.0.3,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.0.3,texinfo_no_detailmenu = False
0.0.3,Example configuration for intersphinx: refer to the Python standard library.
0.0.3,lines in the label file have format
0.0.3,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
0.0.3,"print line[0], line[3]"
0.0.3,TODO(rbharath): Use standard joblib once old-data has been regenerated.
0.0.3,import joblib
0.0.3,First line of user-specified CSV *must* be header.
0.0.3,working-with-3d-molecules
0.0.3,initial embedding
0.0.3,minimization and pruning
0.0.3,always keep lowest-energy conformer
0.0.3,discard conformers after max_conformers is reached
0.0.3,get RMSD to selected conformers
0.0.3,discard conformers within the RMSD threshold
0.0.3,create a new molecule to hold the chosen conformers
0.0.3,this ensures proper conformer IDs and energy-based ordering
0.0.3,TODO(rbharath): The semantics of this class are very difficult to debug.
0.0.3,"Multiple transformations of the data are performed on disk, and computations"
0.0.3,of mean/std are spread across multiple functions for efficiency. Some
0.0.3,refactoring needs to happen here.
0.0.3,TODO(rbharath): Still a bit of information leakage.
0.0.3,TODO(rbharath): FeaturizedSamples should not be responsible for
0.0.3,"X-transform, X_sums, etc. Move that stuff over to Dataset."
0.0.3,"input/output transforms not specified yet, so"
0.0.3,"self.transforms = (input_transforms, output_transforms) =>"
0.0.3,TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
0.0.3,"called without calling transform(), it will explode. Maybe have a separate"
0.0.3,initialization function to avoid this problem.
0.0.3,Store input_transforms/output_transforms so the dataset remembers its state.
0.0.3,TODO(rbharath): These lines are puzzling. Better way to avoid storage
0.0.3,duplication here?
0.0.3,Turns NaNs to zeros
0.0.3,"The following are all associated with Dataset, but are separate functions to"
0.0.3,make it easy to use multiprocessing.
0.0.3,TODO(rbharath): This is a hack. clean up.
0.0.3,TODO(rbharath): Should X be saved to out_X_transformed as well? Since
0.0.3,itershards expects to loop over X-transformed? (Ditto for y/w)
0.0.3,perform common train/test split across all tasks
0.0.3,Set missing data to have weight zero
0.0.3,Note that X_n is a list of floats
0.0.3,"Note y_n is a list of arrays of shape (n_tasks,)"
0.0.3,TODO(rbharath): This is a hack based on fact that multi-tasktype models
0.0.3,aren't supported.
0.0.3,"Sometimes all samples have zero weight. In this case, continue."
0.0.3,We need to import models so they can be created by model_builder
0.0.3,Featurize input
0.0.3,Transform data into arrays for ML
0.0.3,Split into train/test
0.0.3,Transforming train/test data
0.0.3,Fit model
0.0.3,Eval model on train
0.0.3,TODO(rbharath): There should be some automatic check to ensure that all
0.0.3,required model_params are specified.
0.0.3,Featurize input
0.0.3,Transform data into arrays for ML
0.0.3,Split into train/test
0.0.3,Transforming train/test data
0.0.3,Fit model
0.0.3,Eval model on train
0.0.3,Moving imports to be local to avoid isnstall issues with
0.0.3,"Convolution3D, which is not yet part of keras proper."
0.0.3,number of convolutional filters to use at each layer
0.0.3,level of pooling to perform at each layer (POOL x POOL)
0.0.3,level of convolution to perform at each layer (CONV x CONV)
0.0.3,"TODO(rbharath): If we change away from axis-size 32, this code will break."
0.0.3,Eventually figure out a more general rule that works for all axis sizes.
0.0.3,Note that keras requires the model architecture and weights to be stored
0.0.3,separately. A json file is generated that specifies the model architecture.
0.0.3,The weights will be stored in an h5 file. The pkl.gz file with store the
0.0.3,target name.
0.0.3,Save architecture
0.0.3,Add eps weight to avoid minibatches with zero weight (causes theano to crash).
0.0.3,"Class probabilities are predicted for classification outputs. Instead,"
0.0.3,output the most likely class.
0.0.3,TODO(rbharath): This does not work with very large datasets! sklearn does
0.0.3,"support partial_fit, but only for some models. Might make sense to make"
0.0.3,PartialSklearnModel subclass at some point to support large data models.
0.0.3,List of registered models
0.0.3,TODO(rbharath/enf): We need a structured way to deal with potential GPU
0.0.3,memory overflows.
0.0.3,TODO(rbharath): The structure of the produced df might be
0.0.3,complicated. Better way to model?
0.0.3,TODO(rbharath): This feels like a total hack. Is there a structured way
0.0.3,to deal with this instead?
0.0.3,# The following notice is copied from the original NNScore file.
0.0.3,NNScore 2.01 is released under the GNU General Public License (see
0.0.3,http://www.gnu.org/licenses/gpl.html).
0.0.3,"If you have any questions, comments, or suggestions, please don't"
0.0.3,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.3,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.3,HERE].
0.0.3,ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
0.0.3,"This is just a scaling factor, so it's set so as to keep the network"
0.0.3,inputs roughly contained in 0-1
0.0.3,"O-H distance is 0.96 A, N-H is 1.01 A. See"
0.0.3,http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
0.0.3,"If atoms are < 2.5 A apart, we count it as a close contact"
0.0.3,"If receptor and ligand atoms are > 4 A apart, we consider them"
0.0.3,unable to interact with simple electrostatics.
0.0.3,"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
0.0.3,"distance of 7.5 A is good cutoff. This seems really big to me,"
0.0.3,except that pi-pi interactions (parallel) are actually usually
0.0.3,off centered. Interesting paper.  Note that adenine and
0.0.3,"tryptophan count as two aromatic rings. So, for example, an"
0.0.3,"interaction between these two, if positioned correctly, could"
0.0.3,count for 4 pi-pi interactions.
0.0.3,Cation-pi interaction cutoff based on
0.0.3,"""Cation-pi interactions in structural biology."""
0.0.3,4  is good cutoff for salt bridges according to
0.0.3,"""Close-Range Electrostatic Interactions in Proteins"","
0.0.3,"but looking at complexes, I decided to go with 5.5 A"
0.0.3,This is perhaps controversial. I noticed that often a pi-cation
0.0.3,"interaction or other pi interaction was only slightly off, but"
0.0.3,"looking at the structure, it was clearly supposed to be a pi-cation"
0.0.3,interaction. I've decided then to artificially expand the radius of
0.0.3,"each pi ring. Think of this as adding in a VDW radius, or"
0.0.3,"accounting for poor crystal-structure resolution, or whatever you"
0.0.3,want to justify it.
0.0.3,note that dictionaries (hashtables) are passed by reference in python
0.0.3,Now see if there's hydrophobic contacts (C-C contacts)
0.0.3,to convert into J/mol; might be nice to double check this
0.0.3,TODO(bramsundar): What are units of
0.0.3,ligand_charge/receptor_charge?
0.0.3,"so they're more or less perpendicular, it's probably a"
0.0.3,"pi-edge interaction having looked at many structures, I"
0.0.3,noticed the algorithm was identifying T-pi reactions
0.0.3,"when the two rings were in fact quite distant, often"
0.0.3,"with other atoms in between. Eye-balling it, requiring"
0.0.3,that at their closest they be at least 5 A apart seems
0.0.3,to separate the good T's from the bad
0.0.3,"so at their closest points, the two rings come within"
0.0.3,5 A of each other.
0.0.3,"okay, is the ligand pi pointing into the receptor"
0.0.3,"pi, or the other way around?  first, project the"
0.0.3,center of the ligand pi onto the plane of the
0.0.3,"receptor pi, and vs. versa"
0.0.3,"This could be directional somehow, like a hydrogen"
0.0.3,bond.
0.0.3,"now, if it's a true pi-T interaction, this projected"
0.0.3,point should fall within the ring whose plane it's
0.0.3,been projected into.
0.0.3,so it is in the ring on the projected plane.
0.0.3,since it could be interacting with a cofactor or something
0.0.3,Now see if there's some sort of hydrogen bond between
0.0.3,"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
0.0.3,H_BOND_ANGLE.
0.0.3,TODO(rbharath): This is a horrible inner-loop search. Can
0.0.3,this be made more efficient?
0.0.3,Make sure to set comment (used below)
0.0.3,Make sure to set comment (used below)
0.0.3,"print ""nearby hydrogens: "" + str(hydrogens)"
0.0.3,now we need to check the angles
0.0.3,"TODO(rbharath): Rather than using this heuristic, it seems like"
0.0.3,it might be better to just report the angle in the feature
0.0.3,vector...
0.0.3,"so there could be some pi-pi interactions.  Now, let's"
0.0.3,check for stacking interactions. Are the two pi's roughly
0.0.3,parallel?
0.0.3,"so they're more or less parallel, it's probably pi-pi"
0.0.3,"stacking now, since pi-pi are not usually right on"
0.0.3,top of each other. They're often staggered. So I don't
0.0.3,want to just look at the centers of the rings and
0.0.3,compare. Let's look at each of the atoms.  do atom of
0.0.3,"the atoms of one ring, when projected onto the plane of"
0.0.3,"the other, fall within that other ring?"
0.0.3,start by assuming it's not a pi-pi stacking interaction
0.0.3,project the ligand atom onto the plane of the receptor ring
0.0.3,TODO(rbharath): This if-else is confusing.
0.0.3,project the ligand atom onto the plane of the receptor ring
0.0.3,since it could be interacting with a cofactor or something
0.0.3,project the charged onto the plane of the aromatic
0.0.3,since it could be interacting with a cofactor or something
0.0.3,now it's the ligand that has the aromatic group
0.0.3,since it could be interacting with a cofactor or something
0.0.3,so they have oppositve charges
0.0.3,TODO(rbharath): What is atom type A here?
0.0.3,Load receptor and ligand from file.
0.0.3,## OPEN TEMPDIR
0.0.3,## CLOSE TEMPDIR
0.0.3,NNScore 2.01 is released under the GNU General Public License (see
0.0.3,http://www.gnu.org/licenses/gpl.html).
0.0.3,"If you have any questions, comments, or suggestions, please don't"
0.0.3,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.3,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.3,HERE].
0.0.3,"AddHydrogens(polaronly, correctForPH, pH)"
0.0.3,a*x + b*y + c*z = dI think that
0.0.3,"self.x, self.y, self.z = x, y, z"
0.0.3,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
0.0.3,TODO(bramsundar): Should this be __copy__?
0.0.3,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
0.0.3,"return np.array([self.x, self.y, self.z])"
0.0.3,TODO(rbharath): Should this be an atom function?
0.0.3,"This line is necessary for babel to work, though many PDBs in"
0.0.3,the PDB would have this line commented out
0.0.3,now atom type (for pdbqt)
0.0.3,"If atomtype is not specified, but atomname is, set atomtype to the"
0.0.3,"first letter of atomname. This heuristic suffices for proteins,"
0.0.3,since no two-letter elements appear in standard amino acids.
0.0.3,Any number needs to be removed from the element name
0.0.3,"this only uses the rightmost three characters, essentially"
0.0.3,removing unique rotamer identification
0.0.3,"The normal vector to plane is n = [a, b, c]"
0.0.3,We first shift by basepoint (a point on given plane) to make math
0.0.3,simpler. basepoint is given by d/||n||^2 * n
0.0.3,The perpendicular component of diff to plane is
0.0.3,(n^T diff / ||n||^2) * n
0.0.3,generate SMILES for fragments
0.0.3,import all Featurizer subclasses so __subclasses__ will work
0.0.3,these have to be local imports to avoid circular imports
0.0.3,get output from engines
0.0.3,get the maximum number of conformers
0.0.3,construct the new container
0.0.3,- first axis = # mols
0.0.3,- second axis = max # conformers
0.0.3,- remaining axes = determined by feature shape
0.0.3,fill in the container
0.0.3,"If gzipped, need to compute extension again"
0.0.3,"If CSV input, assume that first row contains labels"
0.0.3,Skip labels
0.0.3,"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
0.0.3,raw_df = pd.DataFrame.from_records(processed_rows)
0.0.3,"pandas rows are tuples (row_num, row_data)"
0.0.3,The standard columns for featurized data.
0.0.3,"compounds_df is not altered by any method after initialization, so it's"
0.0.3,safe to keep a copy in memory and on disk.
0.0.3,TODO(rbharath): Might this be inefficient?
0.0.3,Sort from largest to smallest scaffold sets
0.0.3,list-of-available-descriptors.
0.0.3,NNScore 2.01 is released under the GNU General Public License (see
0.0.3,http://www.gnu.org/licenses/gpl.html).
0.0.3,"If you have any questions, comments, or suggestions, please don't"
0.0.3,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.3,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.3,HERE].
0.0.3,Remove rings of length 0
0.0.3,"To remove duplicate entries, we convert rings from a list to set, and"
0.0.3,then back to a list again. There's a snafu since each ring in rings is
0.0.3,itself a list (and lists are unhashable in python). To circumvent this
0.0.3,"issue, we convert each ring into a string (after sorting). For example,"
0.0.3,"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
0.0.3,"original lists, we use ast.literal_eval."
0.0.3,Use dictionary to maintain state about which rings are supersets.
0.0.3,All distances are in Angstroms. Duplicate pairs not specified. For
0.0.3,"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
0.0.3,"This one not from source sited above. Not sure where it's from, but"
0.0.3,"it wouldn't ever be used in the current context (""AutoGrow"")"
0.0.3,estimate based on eye balling Handbook of Chemistry and Physics
0.0.3,Reset internal state
0.0.3,Now load the file into a list
0.0.3,"Load atom data (coordinates, etc.)"
0.0.3,this string unique identifies each atom
0.0.3,so each atom can only be loaded once. No rotamers.
0.0.3,So you're actually reindexing everything here.
0.0.3,### TODO(rbharath): Disabling loading of non
0.0.3,Check that the range is nonempty.
0.0.3,"just so no PDB is empty, VMD will load them all"
0.0.3,write coordinates
0.0.3,first get available index
0.0.3,now add atom
0.0.3,Add to non-protein list
0.0.3,Functions to determine the bond connectivity based on distance
0.0.3,==============================================================
0.0.3,Functions to identify positive charges
0.0.3,======================================
0.0.3,Metallic atoms are assumed to be cations.
0.0.3,Get all the quartenary amines on non-protein residues (these are the
0.0.3,only non-protein groups that will be identified as positively
0.0.3,charged). Note that nitrogen has only 5 valence electrons (out of 8
0.0.3,"for a full shell), so any nitrogen with four bonds must be positively"
0.0.3,charged (think NH4+).
0.0.3,"a quartenary amine, so it's easy"
0.0.3,so the indices stored is just the index of the nitrogen and any
0.0.3,attached atoms
0.0.3,"maybe you only have two hydrogens added, but they're sp3 hybridized."
0.0.3,"Just count this as a quartenary amine, since I think the positive"
0.0.3,charge would be stabilized. This situation can arise with
0.0.3,lone-pair electron nitrogen compounds like pyrrolidine
0.0.3,(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
0.0.3,Test that the angles approximately match the tetrahedral 109
0.0.3,degrees
0.0.3,so indexes added are the nitrogen and any attached atoms.
0.0.3,let's check for a phosphate or anything where a phosphorus is bound
0.0.3,"to two oxygens, where both oxygens are bound to only one heavy atom"
0.0.3,(the phosphorus). I think this will get several phosphorus
0.0.3,substances.
0.0.3,now count the number of oxygens bound only to the phosphorus
0.0.3,"let's check for guanidino-like groups (actually H2N-C-NH2,"
0.0.3,where not CN3.)
0.0.3,if the carbon has only three atoms connected to it
0.0.3,"if true, carbon is connected to at least two nitrogens now,"
0.0.3,so we need to count the number of nitrogens that are only
0.0.3,connected to one heavy atom (the carbon)
0.0.3,Index of atom that connects this charged group to
0.0.3,"the rest of the molecule, ultimately to make sure"
0.0.3,it's sp3 hybridized. Remains -1 if no such atom exists.
0.0.3,TODO(rbharath): Is picking the first non-nitrogen atom
0.0.3,correct here?
0.0.3,Handle case of guanidinium cation
0.0.3,so there are at two nitrogens that are only
0.0.3,connected to the carbon (and probably some
0.0.3,hydrogens)
0.0.3,now you need to make sure connector_ind atom is sp3 hybridized
0.0.3,"There are only two ""guanidino"" nitrogens. Assume the"
0.0.3,negative charge is spread equally between the two.
0.0.3,a carboxylate carbon will have three items connected to it.
0.0.3,a carboxylate will have two oxygens connected to
0.0.3,"it. Now, each of the oxygens should be connected"
0.0.3,to only one heavy atom (so if it's connected to a
0.0.3,"hydrogen, that's okay)"
0.0.3,so it's a carboxylate! Add a negative charge.
0.0.3,Assume negative charge is centered between the two
0.0.3,oxygens.
0.0.3,let's check for a sulfonate or anything where a sulfur is
0.0.3,bound to at least three oxygens and at least three are
0.0.3,bound to only the sulfur (or the sulfur and a hydrogen).
0.0.3,the sulfur is bound to at least three oxygens now
0.0.3,count the number of oxygens that are only bound to the
0.0.3,sulfur
0.0.3,so there are at least three oxygens that are only
0.0.3,bound to the sulfur
0.0.3,Group atoms in the same residue together
0.0.3,Assign each atom a residue key.
0.0.3,Handle edge case of last residue.
0.0.3,Select those atoms which are part of the charged group.
0.0.3,Functions to identify aromatic rings
0.0.3,====================================
0.0.3,first identify the center point
0.0.3,now get the plane that defines this ring. Recall that there are
0.0.3,atleast 3-points in indices_of_ring by ValueError above.
0.0.3,# formula for plane will be ax + by + cz = d
0.0.3,"first, let's see if the last atom in this ring is a carbon"
0.0.3,connected to four atoms. That would be a quick way of
0.0.3,telling this is not an aromatic ring
0.0.3,now check the dihedral between the ring atoms to see if
0.0.3,it's flat
0.0.3,"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.3,ring[ind+3] range of this function is -pi to pi
0.0.3,now check the dihedral between the ring atoms and an atom
0.0.3,connected to the current atom to see if that's flat too.
0.0.3,"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.3,"ring[ind+3], range of this function is -pi to pi"
0.0.3,Get all the rings containing each of the atoms in the ligand
0.0.3,Aromatic rings are of length 5 or 6
0.0.3,"Due to data errors in PDB files, there are cases in which"
0.0.3,non-protein atoms are bonded to protein atoms. Manually remove these
0.0.3,"cases, by testing that ring atom indices are a subset of non-protein"
0.0.3,ring indices.
0.0.3,Aromatic rings are flat
0.0.3,Aromatic rings are of length <= 6
0.0.3,At least 3 indices are required to identify the aromatic plane.
0.0.3,if self.get_aromatic_marker(indices_of_ring) is None:
0.0.3,"raise ValueError(""None at %s for %s"" % (key,"
0.0.3,str(indices_of_ring)))
0.0.3,Tryptophan has two aromatic rings.
0.0.3,Functions to assign secondary structure to protein residues
0.0.3,===========================================================
0.0.3,"first, we need to know what residues are available"
0.0.3,print self.get_residues()
0.0.3,TODO(rbharath): Why magic number 8?
0.0.3,now make sure the first four all have the same resid and
0.0.3,the last four all have the same resid
0.0.3,TODO(rbharath): Ugly code right here...
0.0.3,Now give easier to use names to the atoms
0.0.3,Now compute the phi and psi dihedral angles
0.0.3,Now use those angles to determine if it's alpha or beta
0.0.3,A residue of index i is only going to be in an alpha helix
0.0.3,its CA is within 6 A of the CA of the residue i + 3
0.0.3,so it's in an alpha helix
0.0.3,so now compare that CA to all the other CA's
0.0.3,so it's also in an alpha helix
0.0.3,so this CA atom is one of the ones the first atom
0.0.3,might hydrogen bond with
0.0.3,so these two CA atoms are close enough together
0.0.3,that their residues are probably hydrogen bonded
0.0.3,Alpha helices are only alpha helices if they span at least 4
0.0.3,residues (to wrap around and hydrogen bond). I'm going to
0.0.3,"require them to span at least 5 residues, based on"
0.0.3,examination of many structures.
0.0.3,now go through each of the BETA CA atoms. A residue is only
0.0.3,going to be called a beta sheet if CA atom is within 6.0 A
0.0.3,"of another CA beta, same chain, but index difference > 2."
0.0.3,so it's in a beta sheet
0.0.3,so not comparing an atom to itself
0.0.3,so you're comparing it only to other BETA-sheet atoms
0.0.3,so require them to be on the same chain. needed to
0.0.3,indices can be fairly compared
0.0.3,so the two residues are not simply adjacent to each
0.0.3,other on the chain
0.0.3,so these to atoms are close to each other
0.0.3,Now some more post-processing needs to be done. Do this
0.0.3,again to clear up mess that may have just been created
0.0.3,"(single residue beta strand, for example)"
0.0.3,Beta sheets are usually at least 3 residues long
0.0.3,so they are sequential
0.0.3,Now update each of the atoms with this structural information
0.0.3,Use this list to perform sanity checks on alpha-helix and beta-sheet
0.0.3,labels.
0.0.3,check for separate count and SMILES entries for each fragment
0.0.3,## 3zso comes from PDBBind-CN
0.0.3,The ligand is also specified by pdbbind
0.0.3,"Currently, just verifies that nothing crashes."
0.0.3,## 3zp9 comes from PDBBind-CN
0.0.3,The ligand is also specified by pdbbind
0.0.3,## 3bwf comes from PDBBind-CN
0.0.3,The ligand is also specified by pdbbind
0.0.3,"The keys of these dicts are pairs of atomtypes, but the keys are"
0.0.3,"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
0.0.3,"atom types, there are N*(N+1)/2 unique pairs."
0.0.3,TODO(rbharath): Charges are not computed correctly for certain
0.0.3,ligands! (see 2y2h_ligand). Understand why this happens.
0.0.3,assert np.count_nonzero(np.array(electrostatics.values())) > 0
0.0.3,print counts
0.0.3,1zea is the only example that has any pi-stacking.
0.0.3,Lengths:
0.0.3,ligand_receptor_close_contacts: N*(N+1)/2
0.0.3,ligand_receptor_contacts: N*(N+1)/2
0.0.3,ligand_receptor_electrostatics: N*(N+1)/2
0.0.3,ligand_atom_counts: N
0.0.3,hbonds: 12
0.0.3,hydrophobics: 6
0.0.3,stacking: 3
0.0.3,pi_cation: 6
0.0.3,t_shaped: 3
0.0.3,active_site_flexibility: 6
0.0.3,salt_bridges: 3
0.0.3,rotatable_boonds_count: 1
0.0.3,We need to import models so they can be created by model_builder
0.0.2,-*- coding: utf-8 -*-
0.0.2,
0.0.2,"deepchem documentation build configuration file, created by"
0.0.2,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
0.0.2,
0.0.2,This file is execfile()d with the current directory set to its
0.0.2,containing dir.
0.0.2,
0.0.2,Note that not all possible configuration values are present in this
0.0.2,autogenerated file.
0.0.2,
0.0.2,All configuration values have a default; values that are commented out
0.0.2,serve to show the default.
0.0.2,"If extensions (or modules to document with autodoc) are in another directory,"
0.0.2,add these directories to sys.path here. If the directory is relative to the
0.0.2,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.0.2,"sys.path.insert(0, os.path.abspath('.'))"
0.0.2,-- General configuration ------------------------------------------------
0.0.2,"If your documentation needs a minimal Sphinx version, state it here."
0.0.2,needs_sphinx = '1.0'
0.0.2,"Add any Sphinx extension module names here, as strings. They can be"
0.0.2,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.0.2,ones.
0.0.2,"Add any paths that contain templates here, relative to this directory."
0.0.2,The suffix(es) of source filenames.
0.0.2,You can specify multiple suffix as a list of string:
0.0.2,"source_suffix = ['.rst', '.md']"
0.0.2,The encoding of source files.
0.0.2,source_encoding = 'utf-8-sig'
0.0.2,The master toctree document.
0.0.2,General information about the project.
0.0.2,"The version info for the project you're documenting, acts as replacement for"
0.0.2,"|version| and |release|, also used in various other places throughout the"
0.0.2,built documents.
0.0.2,
0.0.2,The short X.Y version.
0.0.2,"The full version, including alpha/beta/rc tags."
0.0.2,The language for content autogenerated by Sphinx. Refer to documentation
0.0.2,for a list of supported languages.
0.0.2,
0.0.2,This is also used if you do content translation via gettext catalogs.
0.0.2,"Usually you set ""language"" from the command line for these cases."
0.0.2,"There are two options for replacing |today|: either, you set today to some"
0.0.2,"non-false value, then it is used:"
0.0.2,today = ''
0.0.2,"Else, today_fmt is used as the format for a strftime call."
0.0.2,"today_fmt = '%B %d, %Y'"
0.0.2,"List of patterns, relative to source directory, that match files and"
0.0.2,directories to ignore when looking for source files.
0.0.2,The reST default role (used for this markup: `text`) to use for all
0.0.2,documents.
0.0.2,default_role = None
0.0.2,"If true, '()' will be appended to :func: etc. cross-reference text."
0.0.2,add_function_parentheses = True
0.0.2,"If true, the current module name will be prepended to all description"
0.0.2,unit titles (such as .. function::).
0.0.2,add_module_names = True
0.0.2,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.0.2,output. They are ignored by default.
0.0.2,show_authors = False
0.0.2,The name of the Pygments (syntax highlighting) style to use.
0.0.2,A list of ignored prefixes for module index sorting.
0.0.2,modindex_common_prefix = []
0.0.2,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.0.2,keep_warnings = False
0.0.2,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.0.2,-- Options for HTML output ----------------------------------------------
0.0.2,The theme to use for HTML and HTML Help pages.  See the documentation for
0.0.2,a list of builtin themes.
0.0.2,Theme options are theme-specific and customize the look and feel of a theme
0.0.2,"further.  For a list of options available for each theme, see the"
0.0.2,documentation.
0.0.2,html_theme_options = {}
0.0.2,"Add any paths that contain custom themes here, relative to this directory."
0.0.2,html_theme_path = []
0.0.2,"The name for this set of Sphinx documents.  If None, it defaults to"
0.0.2,"""<project> v<release> documentation""."
0.0.2,html_title = None
0.0.2,A shorter title for the navigation bar.  Default is the same as html_title.
0.0.2,html_short_title = None
0.0.2,The name of an image file (relative to this directory) to place at the top
0.0.2,of the sidebar.
0.0.2,html_logo = None
0.0.2,The name of an image file (within the static path) to use as favicon of the
0.0.2,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.0.2,pixels large.
0.0.2,html_favicon = None
0.0.2,"Add any paths that contain custom static files (such as style sheets) here,"
0.0.2,"relative to this directory. They are copied after the builtin static files,"
0.0.2,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.0.2,Add any extra paths that contain custom files (such as robots.txt or
0.0.2,".htaccess) here, relative to this directory. These files are copied"
0.0.2,directly to the root of the documentation.
0.0.2,html_extra_path = []
0.0.2,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.0.2,using the given strftime format.
0.0.2,"html_last_updated_fmt = '%b %d, %Y'"
0.0.2,"If true, SmartyPants will be used to convert quotes and dashes to"
0.0.2,typographically correct entities.
0.0.2,html_use_smartypants = True
0.0.2,"Custom sidebar templates, maps document names to template names."
0.0.2,html_sidebars = {}
0.0.2,"Additional templates that should be rendered to pages, maps page names to"
0.0.2,template names.
0.0.2,html_additional_pages = {}
0.0.2,"If false, no module index is generated."
0.0.2,html_domain_indices = True
0.0.2,"If false, no index is generated."
0.0.2,html_use_index = True
0.0.2,"If true, the index is split into individual pages for each letter."
0.0.2,html_split_index = False
0.0.2,"If true, links to the reST sources are added to the pages."
0.0.2,html_show_sourcelink = True
0.0.2,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.0.2,html_show_sphinx = True
0.0.2,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.0.2,html_show_copyright = True
0.0.2,"If true, an OpenSearch description file will be output, and all pages will"
0.0.2,contain a <link> tag referring to it.  The value of this option must be the
0.0.2,base URL from which the finished HTML is served.
0.0.2,html_use_opensearch = ''
0.0.2,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.0.2,html_file_suffix = None
0.0.2,Language to be used for generating the HTML full-text search index.
0.0.2,Sphinx supports the following languages:
0.0.2,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
0.0.2,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
0.0.2,html_search_language = 'en'
0.0.2,"A dictionary with options for the search language support, empty by default."
0.0.2,Now only 'ja' uses this config value
0.0.2,html_search_options = {'type': 'default'}
0.0.2,The name of a javascript file (relative to the configuration directory) that
0.0.2,"implements a search results scorer. If empty, the default will be used."
0.0.2,html_search_scorer = 'scorer.js'
0.0.2,Output file base name for HTML help builder.
0.0.2,-- Options for LaTeX output ---------------------------------------------
0.0.2,The paper size ('letterpaper' or 'a4paper').
0.0.2,"'papersize': 'letterpaper',"
0.0.2,"The font size ('10pt', '11pt' or '12pt')."
0.0.2,"'pointsize': '10pt',"
0.0.2,Additional stuff for the LaTeX preamble.
0.0.2,"'preamble': '',"
0.0.2,Latex figure (float) alignment
0.0.2,"'figure_align': 'htbp',"
0.0.2,Grouping the document tree into LaTeX files. List of tuples
0.0.2,"(source start file, target name, title,"
0.0.2,"author, documentclass [howto, manual, or own class])."
0.0.2,The name of an image file (relative to this directory) to place at the top of
0.0.2,the title page.
0.0.2,latex_logo = None
0.0.2,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.0.2,not chapters.
0.0.2,latex_use_parts = False
0.0.2,"If true, show page references after internal links."
0.0.2,latex_show_pagerefs = False
0.0.2,"If true, show URL addresses after external links."
0.0.2,latex_show_urls = False
0.0.2,Documents to append as an appendix to all manuals.
0.0.2,latex_appendices = []
0.0.2,"If false, no module index is generated."
0.0.2,latex_domain_indices = True
0.0.2,-- Options for manual page output ---------------------------------------
0.0.2,One entry per manual page. List of tuples
0.0.2,"(source start file, name, description, authors, manual section)."
0.0.2,"If true, show URL addresses after external links."
0.0.2,man_show_urls = False
0.0.2,-- Options for Texinfo output -------------------------------------------
0.0.2,Grouping the document tree into Texinfo files. List of tuples
0.0.2,"(source start file, target name, title, author,"
0.0.2,"dir menu entry, description, category)"
0.0.2,Documents to append as an appendix to all manuals.
0.0.2,texinfo_appendices = []
0.0.2,"If false, no module index is generated."
0.0.2,texinfo_domain_indices = True
0.0.2,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.0.2,texinfo_show_urls = 'footnote'
0.0.2,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.0.2,texinfo_no_detailmenu = False
0.0.2,Example configuration for intersphinx: refer to the Python standard library.
0.0.2,lines in the label file have format
0.0.2,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
0.0.2,"print line[0], line[3]"
0.0.2,TODO(rbharath): Use standard joblib once old-data has been regenerated.
0.0.2,import joblib
0.0.2,First line of user-specified CSV *must* be header.
0.0.2,working-with-3d-molecules
0.0.2,initial embedding
0.0.2,minimization and pruning
0.0.2,always keep lowest-energy conformer
0.0.2,discard conformers after max_conformers is reached
0.0.2,get RMSD to selected conformers
0.0.2,discard conformers within the RMSD threshold
0.0.2,create a new molecule to hold the chosen conformers
0.0.2,this ensures proper conformer IDs and energy-based ordering
0.0.2,TODO(rbharath): The semantics of this class are very difficult to debug.
0.0.2,"Multiple transformations of the data are performed on disk, and computations"
0.0.2,of mean/std are spread across multiple functions for efficiency. Some
0.0.2,refactoring needs to happen here.
0.0.2,TODO(rbharath): Still a bit of information leakage.
0.0.2,TODO(rbharath): FeaturizedSamples should not be responsible for
0.0.2,"X-transform, X_sums, etc. Move that stuff over to Dataset."
0.0.2,"input/output transforms not specified yet, so"
0.0.2,"self.transforms = (input_transforms, output_transforms) =>"
0.0.2,TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
0.0.2,"called without calling transform(), it will explode. Maybe have a separate"
0.0.2,initialization function to avoid this problem.
0.0.2,Store input_transforms/output_transforms so the dataset remembers its state.
0.0.2,TODO(rbharath): These lines are puzzling. Better way to avoid storage
0.0.2,duplication here?
0.0.2,Turns NaNs to zeros
0.0.2,"The following are all associated with Dataset, but are separate functions to"
0.0.2,make it easy to use multiprocessing.
0.0.2,TODO(rbharath): This is a hack. clean up.
0.0.2,TODO(rbharath): Should X be saved to out_X_transformed as well? Since
0.0.2,itershards expects to loop over X-transformed? (Ditto for y/w)
0.0.2,perform common train/test split across all tasks
0.0.2,Set missing data to have weight zero
0.0.2,Note that X_n is a list of floats
0.0.2,"Note y_n is a list of arrays of shape (n_tasks,)"
0.0.2,TODO(rbharath): This is a hack based on fact that multi-tasktype models
0.0.2,aren't supported.
0.0.2,"Sometimes all samples have zero weight. In this case, continue."
0.0.2,We need to import models so they can be created by model_builder
0.0.2,Featurize input
0.0.2,Transform data into arrays for ML
0.0.2,Split into train/test
0.0.2,Transforming train/test data
0.0.2,Fit model
0.0.2,Eval model on train
0.0.2,TODO(rbharath): There should be some automatic check to ensure that all
0.0.2,required model_params are specified.
0.0.2,Featurize input
0.0.2,Transform data into arrays for ML
0.0.2,Split into train/test
0.0.2,Transforming train/test data
0.0.2,Fit model
0.0.2,Eval model on train
0.0.2,Moving imports to be local to avoid isnstall issues with
0.0.2,"Convolution3D, which is not yet part of keras proper."
0.0.2,number of convolutional filters to use at each layer
0.0.2,level of pooling to perform at each layer (POOL x POOL)
0.0.2,level of convolution to perform at each layer (CONV x CONV)
0.0.2,"TODO(rbharath): If we change away from axis-size 32, this code will break."
0.0.2,Eventually figure out a more general rule that works for all axis sizes.
0.0.2,Note that keras requires the model architecture and weights to be stored
0.0.2,separately. A json file is generated that specifies the model architecture.
0.0.2,The weights will be stored in an h5 file. The pkl.gz file with store the
0.0.2,target name.
0.0.2,Save architecture
0.0.2,Add eps weight to avoid minibatches with zero weight (causes theano to crash).
0.0.2,"Class probabilities are predicted for classification outputs. Instead,"
0.0.2,output the most likely class.
0.0.2,TODO(rbharath): This does not work with very large datasets! sklearn does
0.0.2,"support partial_fit, but only for some models. Might make sense to make"
0.0.2,PartialSklearnModel subclass at some point to support large data models.
0.0.2,List of registered models
0.0.2,TODO(rbharath/enf): We need a structured way to deal with potential GPU
0.0.2,memory overflows.
0.0.2,TODO(rbharath): The structure of the produced df might be
0.0.2,complicated. Better way to model?
0.0.2,TODO(rbharath): This feels like a total hack. Is there a structured way
0.0.2,to deal with this instead?
0.0.2,# The following notice is copied from the original NNScore file.
0.0.2,NNScore 2.01 is released under the GNU General Public License (see
0.0.2,http://www.gnu.org/licenses/gpl.html).
0.0.2,"If you have any questions, comments, or suggestions, please don't"
0.0.2,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.2,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.2,HERE].
0.0.2,ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
0.0.2,"This is just a scaling factor, so it's set so as to keep the network"
0.0.2,inputs roughly contained in 0-1
0.0.2,"O-H distance is 0.96 A, N-H is 1.01 A. See"
0.0.2,http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
0.0.2,"If atoms are < 2.5 A apart, we count it as a close contact"
0.0.2,"If receptor and ligand atoms are > 4 A apart, we consider them"
0.0.2,unable to interact with simple electrostatics.
0.0.2,"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
0.0.2,"distance of 7.5 A is good cutoff. This seems really big to me,"
0.0.2,except that pi-pi interactions (parallel) are actually usually
0.0.2,off centered. Interesting paper.  Note that adenine and
0.0.2,"tryptophan count as two aromatic rings. So, for example, an"
0.0.2,"interaction between these two, if positioned correctly, could"
0.0.2,count for 4 pi-pi interactions.
0.0.2,Cation-pi interaction cutoff based on
0.0.2,"""Cation-pi interactions in structural biology."""
0.0.2,4  is good cutoff for salt bridges according to
0.0.2,"""Close-Range Electrostatic Interactions in Proteins"","
0.0.2,"but looking at complexes, I decided to go with 5.5 A"
0.0.2,This is perhaps controversial. I noticed that often a pi-cation
0.0.2,"interaction or other pi interaction was only slightly off, but"
0.0.2,"looking at the structure, it was clearly supposed to be a pi-cation"
0.0.2,interaction. I've decided then to artificially expand the radius of
0.0.2,"each pi ring. Think of this as adding in a VDW radius, or"
0.0.2,"accounting for poor crystal-structure resolution, or whatever you"
0.0.2,want to justify it.
0.0.2,note that dictionaries (hashtables) are passed by reference in python
0.0.2,Now see if there's hydrophobic contacts (C-C contacts)
0.0.2,to convert into J/mol; might be nice to double check this
0.0.2,TODO(bramsundar): What are units of
0.0.2,ligand_charge/receptor_charge?
0.0.2,"so they're more or less perpendicular, it's probably a"
0.0.2,"pi-edge interaction having looked at many structures, I"
0.0.2,noticed the algorithm was identifying T-pi reactions
0.0.2,"when the two rings were in fact quite distant, often"
0.0.2,"with other atoms in between. Eye-balling it, requiring"
0.0.2,that at their closest they be at least 5 A apart seems
0.0.2,to separate the good T's from the bad
0.0.2,"so at their closest points, the two rings come within"
0.0.2,5 A of each other.
0.0.2,"okay, is the ligand pi pointing into the receptor"
0.0.2,"pi, or the other way around?  first, project the"
0.0.2,center of the ligand pi onto the plane of the
0.0.2,"receptor pi, and vs. versa"
0.0.2,"This could be directional somehow, like a hydrogen"
0.0.2,bond.
0.0.2,"now, if it's a true pi-T interaction, this projected"
0.0.2,point should fall within the ring whose plane it's
0.0.2,been projected into.
0.0.2,so it is in the ring on the projected plane.
0.0.2,since it could be interacting with a cofactor or something
0.0.2,Now see if there's some sort of hydrogen bond between
0.0.2,"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
0.0.2,H_BOND_ANGLE.
0.0.2,TODO(rbharath): This is a horrible inner-loop search. Can
0.0.2,this be made more efficient?
0.0.2,Make sure to set comment (used below)
0.0.2,Make sure to set comment (used below)
0.0.2,"print ""nearby hydrogens: "" + str(hydrogens)"
0.0.2,now we need to check the angles
0.0.2,"TODO(rbharath): Rather than using this heuristic, it seems like"
0.0.2,it might be better to just report the angle in the feature
0.0.2,vector...
0.0.2,"so there could be some pi-pi interactions.  Now, let's"
0.0.2,check for stacking interactions. Are the two pi's roughly
0.0.2,parallel?
0.0.2,"so they're more or less parallel, it's probably pi-pi"
0.0.2,"stacking now, since pi-pi are not usually right on"
0.0.2,top of each other. They're often staggered. So I don't
0.0.2,want to just look at the centers of the rings and
0.0.2,compare. Let's look at each of the atoms.  do atom of
0.0.2,"the atoms of one ring, when projected onto the plane of"
0.0.2,"the other, fall within that other ring?"
0.0.2,start by assuming it's not a pi-pi stacking interaction
0.0.2,project the ligand atom onto the plane of the receptor ring
0.0.2,TODO(rbharath): This if-else is confusing.
0.0.2,project the ligand atom onto the plane of the receptor ring
0.0.2,since it could be interacting with a cofactor or something
0.0.2,project the charged onto the plane of the aromatic
0.0.2,since it could be interacting with a cofactor or something
0.0.2,now it's the ligand that has the aromatic group
0.0.2,since it could be interacting with a cofactor or something
0.0.2,so they have oppositve charges
0.0.2,TODO(rbharath): What is atom type A here?
0.0.2,Load receptor and ligand from file.
0.0.2,## OPEN TEMPDIR
0.0.2,## CLOSE TEMPDIR
0.0.2,NNScore 2.01 is released under the GNU General Public License (see
0.0.2,http://www.gnu.org/licenses/gpl.html).
0.0.2,"If you have any questions, comments, or suggestions, please don't"
0.0.2,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.2,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.2,HERE].
0.0.2,"AddHydrogens(polaronly, correctForPH, pH)"
0.0.2,a*x + b*y + c*z = dI think that
0.0.2,"self.x, self.y, self.z = x, y, z"
0.0.2,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
0.0.2,TODO(bramsundar): Should this be __copy__?
0.0.2,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
0.0.2,"return np.array([self.x, self.y, self.z])"
0.0.2,TODO(rbharath): Should this be an atom function?
0.0.2,"This line is necessary for babel to work, though many PDBs in"
0.0.2,the PDB would have this line commented out
0.0.2,now atom type (for pdbqt)
0.0.2,"If atomtype is not specified, but atomname is, set atomtype to the"
0.0.2,"first letter of atomname. This heuristic suffices for proteins,"
0.0.2,since no two-letter elements appear in standard amino acids.
0.0.2,Any number needs to be removed from the element name
0.0.2,"this only uses the rightmost three characters, essentially"
0.0.2,removing unique rotamer identification
0.0.2,"The normal vector to plane is n = [a, b, c]"
0.0.2,We first shift by basepoint (a point on given plane) to make math
0.0.2,simpler. basepoint is given by d/||n||^2 * n
0.0.2,The perpendicular component of diff to plane is
0.0.2,(n^T diff / ||n||^2) * n
0.0.2,generate SMILES for fragments
0.0.2,import all Featurizer subclasses so __subclasses__ will work
0.0.2,these have to be local imports to avoid circular imports
0.0.2,get output from engines
0.0.2,get the maximum number of conformers
0.0.2,construct the new container
0.0.2,- first axis = # mols
0.0.2,- second axis = max # conformers
0.0.2,- remaining axes = determined by feature shape
0.0.2,fill in the container
0.0.2,"If gzipped, need to compute extension again"
0.0.2,"If CSV input, assume that first row contains labels"
0.0.2,Skip labels
0.0.2,"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
0.0.2,raw_df = pd.DataFrame.from_records(processed_rows)
0.0.2,"pandas rows are tuples (row_num, row_data)"
0.0.2,The standard columns for featurized data.
0.0.2,"compounds_df is not altered by any method after initialization, so it's"
0.0.2,safe to keep a copy in memory and on disk.
0.0.2,TODO(rbharath): Might this be inefficient?
0.0.2,Sort from largest to smallest scaffold sets
0.0.2,list-of-available-descriptors.
0.0.2,NNScore 2.01 is released under the GNU General Public License (see
0.0.2,http://www.gnu.org/licenses/gpl.html).
0.0.2,"If you have any questions, comments, or suggestions, please don't"
0.0.2,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.2,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.2,HERE].
0.0.2,Remove rings of length 0
0.0.2,"To remove duplicate entries, we convert rings from a list to set, and"
0.0.2,then back to a list again. There's a snafu since each ring in rings is
0.0.2,itself a list (and lists are unhashable in python). To circumvent this
0.0.2,"issue, we convert each ring into a string (after sorting). For example,"
0.0.2,"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
0.0.2,"original lists, we use ast.literal_eval."
0.0.2,Use dictionary to maintain state about which rings are supersets.
0.0.2,All distances are in Angstroms. Duplicate pairs not specified. For
0.0.2,"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
0.0.2,"This one not from source sited above. Not sure where it's from, but"
0.0.2,"it wouldn't ever be used in the current context (""AutoGrow"")"
0.0.2,estimate based on eye balling Handbook of Chemistry and Physics
0.0.2,Reset internal state
0.0.2,Now load the file into a list
0.0.2,"Load atom data (coordinates, etc.)"
0.0.2,this string unique identifies each atom
0.0.2,so each atom can only be loaded once. No rotamers.
0.0.2,So you're actually reindexing everything here.
0.0.2,### TODO(rbharath): Disabling loading of non
0.0.2,Check that the range is nonempty.
0.0.2,"just so no PDB is empty, VMD will load them all"
0.0.2,write coordinates
0.0.2,first get available index
0.0.2,now add atom
0.0.2,Add to non-protein list
0.0.2,Functions to determine the bond connectivity based on distance
0.0.2,==============================================================
0.0.2,Functions to identify positive charges
0.0.2,======================================
0.0.2,Metallic atoms are assumed to be cations.
0.0.2,Get all the quartenary amines on non-protein residues (these are the
0.0.2,only non-protein groups that will be identified as positively
0.0.2,charged). Note that nitrogen has only 5 valence electrons (out of 8
0.0.2,"for a full shell), so any nitrogen with four bonds must be positively"
0.0.2,charged (think NH4+).
0.0.2,"a quartenary amine, so it's easy"
0.0.2,so the indices stored is just the index of the nitrogen and any
0.0.2,attached atoms
0.0.2,"maybe you only have two hydrogens added, but they're sp3 hybridized."
0.0.2,"Just count this as a quartenary amine, since I think the positive"
0.0.2,charge would be stabilized. This situation can arise with
0.0.2,lone-pair electron nitrogen compounds like pyrrolidine
0.0.2,(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
0.0.2,Test that the angles approximately match the tetrahedral 109
0.0.2,degrees
0.0.2,so indexes added are the nitrogen and any attached atoms.
0.0.2,let's check for a phosphate or anything where a phosphorus is bound
0.0.2,"to two oxygens, where both oxygens are bound to only one heavy atom"
0.0.2,(the phosphorus). I think this will get several phosphorus
0.0.2,substances.
0.0.2,now count the number of oxygens bound only to the phosphorus
0.0.2,"let's check for guanidino-like groups (actually H2N-C-NH2,"
0.0.2,where not CN3.)
0.0.2,if the carbon has only three atoms connected to it
0.0.2,"if true, carbon is connected to at least two nitrogens now,"
0.0.2,so we need to count the number of nitrogens that are only
0.0.2,connected to one heavy atom (the carbon)
0.0.2,Index of atom that connects this charged group to
0.0.2,"the rest of the molecule, ultimately to make sure"
0.0.2,it's sp3 hybridized. Remains -1 if no such atom exists.
0.0.2,TODO(rbharath): Is picking the first non-nitrogen atom
0.0.2,correct here?
0.0.2,Handle case of guanidinium cation
0.0.2,so there are at two nitrogens that are only
0.0.2,connected to the carbon (and probably some
0.0.2,hydrogens)
0.0.2,now you need to make sure connector_ind atom is sp3 hybridized
0.0.2,"There are only two ""guanidino"" nitrogens. Assume the"
0.0.2,negative charge is spread equally between the two.
0.0.2,a carboxylate carbon will have three items connected to it.
0.0.2,a carboxylate will have two oxygens connected to
0.0.2,"it. Now, each of the oxygens should be connected"
0.0.2,to only one heavy atom (so if it's connected to a
0.0.2,"hydrogen, that's okay)"
0.0.2,so it's a carboxylate! Add a negative charge.
0.0.2,Assume negative charge is centered between the two
0.0.2,oxygens.
0.0.2,let's check for a sulfonate or anything where a sulfur is
0.0.2,bound to at least three oxygens and at least three are
0.0.2,bound to only the sulfur (or the sulfur and a hydrogen).
0.0.2,the sulfur is bound to at least three oxygens now
0.0.2,count the number of oxygens that are only bound to the
0.0.2,sulfur
0.0.2,so there are at least three oxygens that are only
0.0.2,bound to the sulfur
0.0.2,Group atoms in the same residue together
0.0.2,Assign each atom a residue key.
0.0.2,Handle edge case of last residue.
0.0.2,Select those atoms which are part of the charged group.
0.0.2,Functions to identify aromatic rings
0.0.2,====================================
0.0.2,first identify the center point
0.0.2,now get the plane that defines this ring. Recall that there are
0.0.2,atleast 3-points in indices_of_ring by ValueError above.
0.0.2,# formula for plane will be ax + by + cz = d
0.0.2,"first, let's see if the last atom in this ring is a carbon"
0.0.2,connected to four atoms. That would be a quick way of
0.0.2,telling this is not an aromatic ring
0.0.2,now check the dihedral between the ring atoms to see if
0.0.2,it's flat
0.0.2,"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.2,ring[ind+3] range of this function is -pi to pi
0.0.2,now check the dihedral between the ring atoms and an atom
0.0.2,connected to the current atom to see if that's flat too.
0.0.2,"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.2,"ring[ind+3], range of this function is -pi to pi"
0.0.2,Get all the rings containing each of the atoms in the ligand
0.0.2,Aromatic rings are of length 5 or 6
0.0.2,"Due to data errors in PDB files, there are cases in which"
0.0.2,non-protein atoms are bonded to protein atoms. Manually remove these
0.0.2,"cases, by testing that ring atom indices are a subset of non-protein"
0.0.2,ring indices.
0.0.2,Aromatic rings are flat
0.0.2,Aromatic rings are of length <= 6
0.0.2,At least 3 indices are required to identify the aromatic plane.
0.0.2,if self.get_aromatic_marker(indices_of_ring) is None:
0.0.2,"raise ValueError(""None at %s for %s"" % (key,"
0.0.2,str(indices_of_ring)))
0.0.2,Tryptophan has two aromatic rings.
0.0.2,Functions to assign secondary structure to protein residues
0.0.2,===========================================================
0.0.2,"first, we need to know what residues are available"
0.0.2,print self.get_residues()
0.0.2,TODO(rbharath): Why magic number 8?
0.0.2,now make sure the first four all have the same resid and
0.0.2,the last four all have the same resid
0.0.2,TODO(rbharath): Ugly code right here...
0.0.2,Now give easier to use names to the atoms
0.0.2,Now compute the phi and psi dihedral angles
0.0.2,Now use those angles to determine if it's alpha or beta
0.0.2,A residue of index i is only going to be in an alpha helix
0.0.2,its CA is within 6 A of the CA of the residue i + 3
0.0.2,so it's in an alpha helix
0.0.2,so now compare that CA to all the other CA's
0.0.2,so it's also in an alpha helix
0.0.2,so this CA atom is one of the ones the first atom
0.0.2,might hydrogen bond with
0.0.2,so these two CA atoms are close enough together
0.0.2,that their residues are probably hydrogen bonded
0.0.2,Alpha helices are only alpha helices if they span at least 4
0.0.2,residues (to wrap around and hydrogen bond). I'm going to
0.0.2,"require them to span at least 5 residues, based on"
0.0.2,examination of many structures.
0.0.2,now go through each of the BETA CA atoms. A residue is only
0.0.2,going to be called a beta sheet if CA atom is within 6.0 A
0.0.2,"of another CA beta, same chain, but index difference > 2."
0.0.2,so it's in a beta sheet
0.0.2,so not comparing an atom to itself
0.0.2,so you're comparing it only to other BETA-sheet atoms
0.0.2,so require them to be on the same chain. needed to
0.0.2,indices can be fairly compared
0.0.2,so the two residues are not simply adjacent to each
0.0.2,other on the chain
0.0.2,so these to atoms are close to each other
0.0.2,Now some more post-processing needs to be done. Do this
0.0.2,again to clear up mess that may have just been created
0.0.2,"(single residue beta strand, for example)"
0.0.2,Beta sheets are usually at least 3 residues long
0.0.2,so they are sequential
0.0.2,Now update each of the atoms with this structural information
0.0.2,Use this list to perform sanity checks on alpha-helix and beta-sheet
0.0.2,labels.
0.0.2,check for separate count and SMILES entries for each fragment
0.0.2,## 3zso comes from PDBBind-CN
0.0.2,The ligand is also specified by pdbbind
0.0.2,"Currently, just verifies that nothing crashes."
0.0.2,## 3zp9 comes from PDBBind-CN
0.0.2,The ligand is also specified by pdbbind
0.0.2,## 3bwf comes from PDBBind-CN
0.0.2,The ligand is also specified by pdbbind
0.0.2,"The keys of these dicts are pairs of atomtypes, but the keys are"
0.0.2,"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
0.0.2,"atom types, there are N*(N+1)/2 unique pairs."
0.0.2,TODO(rbharath): Charges are not computed correctly for certain
0.0.2,ligands! (see 2y2h_ligand). Understand why this happens.
0.0.2,assert np.count_nonzero(np.array(electrostatics.values())) > 0
0.0.2,print counts
0.0.2,1zea is the only example that has any pi-stacking.
0.0.2,Lengths:
0.0.2,ligand_receptor_close_contacts: N*(N+1)/2
0.0.2,ligand_receptor_contacts: N*(N+1)/2
0.0.2,ligand_receptor_electrostatics: N*(N+1)/2
0.0.2,ligand_atom_counts: N
0.0.2,hbonds: 12
0.0.2,hydrophobics: 6
0.0.2,stacking: 3
0.0.2,pi_cation: 6
0.0.2,t_shaped: 3
0.0.2,active_site_flexibility: 6
0.0.2,salt_bridges: 3
0.0.2,rotatable_boonds_count: 1
0.0.2,We need to import models so they can be created by model_builder
0.0.1,-*- coding: utf-8 -*-
0.0.1,
0.0.1,"deepchem documentation build configuration file, created by"
0.0.1,sphinx-quickstart on Tue Jan 19 17:37:50 2016.
0.0.1,
0.0.1,This file is execfile()d with the current directory set to its
0.0.1,containing dir.
0.0.1,
0.0.1,Note that not all possible configuration values are present in this
0.0.1,autogenerated file.
0.0.1,
0.0.1,All configuration values have a default; values that are commented out
0.0.1,serve to show the default.
0.0.1,"If extensions (or modules to document with autodoc) are in another directory,"
0.0.1,add these directories to sys.path here. If the directory is relative to the
0.0.1,"documentation root, use os.path.abspath to make it absolute, like shown here."
0.0.1,"sys.path.insert(0, os.path.abspath('.'))"
0.0.1,-- General configuration ------------------------------------------------
0.0.1,"If your documentation needs a minimal Sphinx version, state it here."
0.0.1,needs_sphinx = '1.0'
0.0.1,"Add any Sphinx extension module names here, as strings. They can be"
0.0.1,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
0.0.1,ones.
0.0.1,"Add any paths that contain templates here, relative to this directory."
0.0.1,The suffix(es) of source filenames.
0.0.1,You can specify multiple suffix as a list of string:
0.0.1,"source_suffix = ['.rst', '.md']"
0.0.1,The encoding of source files.
0.0.1,source_encoding = 'utf-8-sig'
0.0.1,The master toctree document.
0.0.1,General information about the project.
0.0.1,"The version info for the project you're documenting, acts as replacement for"
0.0.1,"|version| and |release|, also used in various other places throughout the"
0.0.1,built documents.
0.0.1,
0.0.1,The short X.Y version.
0.0.1,"The full version, including alpha/beta/rc tags."
0.0.1,The language for content autogenerated by Sphinx. Refer to documentation
0.0.1,for a list of supported languages.
0.0.1,
0.0.1,This is also used if you do content translation via gettext catalogs.
0.0.1,"Usually you set ""language"" from the command line for these cases."
0.0.1,"There are two options for replacing |today|: either, you set today to some"
0.0.1,"non-false value, then it is used:"
0.0.1,today = ''
0.0.1,"Else, today_fmt is used as the format for a strftime call."
0.0.1,"today_fmt = '%B %d, %Y'"
0.0.1,"List of patterns, relative to source directory, that match files and"
0.0.1,directories to ignore when looking for source files.
0.0.1,The reST default role (used for this markup: `text`) to use for all
0.0.1,documents.
0.0.1,default_role = None
0.0.1,"If true, '()' will be appended to :func: etc. cross-reference text."
0.0.1,add_function_parentheses = True
0.0.1,"If true, the current module name will be prepended to all description"
0.0.1,unit titles (such as .. function::).
0.0.1,add_module_names = True
0.0.1,"If true, sectionauthor and moduleauthor directives will be shown in the"
0.0.1,output. They are ignored by default.
0.0.1,show_authors = False
0.0.1,The name of the Pygments (syntax highlighting) style to use.
0.0.1,A list of ignored prefixes for module index sorting.
0.0.1,modindex_common_prefix = []
0.0.1,"If true, keep warnings as ""system message"" paragraphs in the built documents."
0.0.1,keep_warnings = False
0.0.1,"If true, `todo` and `todoList` produce output, else they produce nothing."
0.0.1,-- Options for HTML output ----------------------------------------------
0.0.1,The theme to use for HTML and HTML Help pages.  See the documentation for
0.0.1,a list of builtin themes.
0.0.1,Theme options are theme-specific and customize the look and feel of a theme
0.0.1,"further.  For a list of options available for each theme, see the"
0.0.1,documentation.
0.0.1,html_theme_options = {}
0.0.1,"Add any paths that contain custom themes here, relative to this directory."
0.0.1,html_theme_path = []
0.0.1,"The name for this set of Sphinx documents.  If None, it defaults to"
0.0.1,"""<project> v<release> documentation""."
0.0.1,html_title = None
0.0.1,A shorter title for the navigation bar.  Default is the same as html_title.
0.0.1,html_short_title = None
0.0.1,The name of an image file (relative to this directory) to place at the top
0.0.1,of the sidebar.
0.0.1,html_logo = None
0.0.1,The name of an image file (within the static path) to use as favicon of the
0.0.1,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
0.0.1,pixels large.
0.0.1,html_favicon = None
0.0.1,"Add any paths that contain custom static files (such as style sheets) here,"
0.0.1,"relative to this directory. They are copied after the builtin static files,"
0.0.1,"so a file named ""default.css"" will overwrite the builtin ""default.css""."
0.0.1,Add any extra paths that contain custom files (such as robots.txt or
0.0.1,".htaccess) here, relative to this directory. These files are copied"
0.0.1,directly to the root of the documentation.
0.0.1,html_extra_path = []
0.0.1,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,"
0.0.1,using the given strftime format.
0.0.1,"html_last_updated_fmt = '%b %d, %Y'"
0.0.1,"If true, SmartyPants will be used to convert quotes and dashes to"
0.0.1,typographically correct entities.
0.0.1,html_use_smartypants = True
0.0.1,"Custom sidebar templates, maps document names to template names."
0.0.1,html_sidebars = {}
0.0.1,"Additional templates that should be rendered to pages, maps page names to"
0.0.1,template names.
0.0.1,html_additional_pages = {}
0.0.1,"If false, no module index is generated."
0.0.1,html_domain_indices = True
0.0.1,"If false, no index is generated."
0.0.1,html_use_index = True
0.0.1,"If true, the index is split into individual pages for each letter."
0.0.1,html_split_index = False
0.0.1,"If true, links to the reST sources are added to the pages."
0.0.1,html_show_sourcelink = True
0.0.1,"If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True."
0.0.1,html_show_sphinx = True
0.0.1,"If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True."
0.0.1,html_show_copyright = True
0.0.1,"If true, an OpenSearch description file will be output, and all pages will"
0.0.1,contain a <link> tag referring to it.  The value of this option must be the
0.0.1,base URL from which the finished HTML is served.
0.0.1,html_use_opensearch = ''
0.0.1,"This is the file name suffix for HTML files (e.g. "".xhtml"")."
0.0.1,html_file_suffix = None
0.0.1,Language to be used for generating the HTML full-text search index.
0.0.1,Sphinx supports the following languages:
0.0.1,"'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'"
0.0.1,"'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr'"
0.0.1,html_search_language = 'en'
0.0.1,"A dictionary with options for the search language support, empty by default."
0.0.1,Now only 'ja' uses this config value
0.0.1,html_search_options = {'type': 'default'}
0.0.1,The name of a javascript file (relative to the configuration directory) that
0.0.1,"implements a search results scorer. If empty, the default will be used."
0.0.1,html_search_scorer = 'scorer.js'
0.0.1,Output file base name for HTML help builder.
0.0.1,-- Options for LaTeX output ---------------------------------------------
0.0.1,The paper size ('letterpaper' or 'a4paper').
0.0.1,"'papersize': 'letterpaper',"
0.0.1,"The font size ('10pt', '11pt' or '12pt')."
0.0.1,"'pointsize': '10pt',"
0.0.1,Additional stuff for the LaTeX preamble.
0.0.1,"'preamble': '',"
0.0.1,Latex figure (float) alignment
0.0.1,"'figure_align': 'htbp',"
0.0.1,Grouping the document tree into LaTeX files. List of tuples
0.0.1,"(source start file, target name, title,"
0.0.1,"author, documentclass [howto, manual, or own class])."
0.0.1,The name of an image file (relative to this directory) to place at the top of
0.0.1,the title page.
0.0.1,latex_logo = None
0.0.1,"For ""manual"" documents, if this is true, then toplevel headings are parts,"
0.0.1,not chapters.
0.0.1,latex_use_parts = False
0.0.1,"If true, show page references after internal links."
0.0.1,latex_show_pagerefs = False
0.0.1,"If true, show URL addresses after external links."
0.0.1,latex_show_urls = False
0.0.1,Documents to append as an appendix to all manuals.
0.0.1,latex_appendices = []
0.0.1,"If false, no module index is generated."
0.0.1,latex_domain_indices = True
0.0.1,-- Options for manual page output ---------------------------------------
0.0.1,One entry per manual page. List of tuples
0.0.1,"(source start file, name, description, authors, manual section)."
0.0.1,"If true, show URL addresses after external links."
0.0.1,man_show_urls = False
0.0.1,-- Options for Texinfo output -------------------------------------------
0.0.1,Grouping the document tree into Texinfo files. List of tuples
0.0.1,"(source start file, target name, title, author,"
0.0.1,"dir menu entry, description, category)"
0.0.1,Documents to append as an appendix to all manuals.
0.0.1,texinfo_appendices = []
0.0.1,"If false, no module index is generated."
0.0.1,texinfo_domain_indices = True
0.0.1,"How to display URL addresses: 'footnote', 'no', or 'inline'."
0.0.1,texinfo_show_urls = 'footnote'
0.0.1,"If true, do not generate a @detailmenu in the ""Top"" node's menu."
0.0.1,texinfo_no_detailmenu = False
0.0.1,Example configuration for intersphinx: refer to the Python standard library.
0.0.1,lines in the label file have format
0.0.1,PDB-code Resolution Release-Year -logKd Kd reference ligand-name
0.0.1,"print line[0], line[3]"
0.0.1,TODO(rbharath): Use standard joblib once old-data has been regenerated.
0.0.1,import joblib
0.0.1,First line of user-specified CSV *must* be header.
0.0.1,working-with-3d-molecules
0.0.1,initial embedding
0.0.1,minimization and pruning
0.0.1,always keep lowest-energy conformer
0.0.1,discard conformers after max_conformers is reached
0.0.1,get RMSD to selected conformers
0.0.1,discard conformers within the RMSD threshold
0.0.1,create a new molecule to hold the chosen conformers
0.0.1,this ensures proper conformer IDs and energy-based ordering
0.0.1,TODO(rbharath): The semantics of this class are very difficult to debug.
0.0.1,"Multiple transformations of the data are performed on disk, and computations"
0.0.1,of mean/std are spread across multiple functions for efficiency. Some
0.0.1,refactoring needs to happen here.
0.0.1,TODO(rbharath): Still a bit of information leakage.
0.0.1,TODO(rbharath): FeaturizedSamples should not be responsible for
0.0.1,"X-transform, X_sums, etc. Move that stuff over to Dataset."
0.0.1,"input/output transforms not specified yet, so"
0.0.1,"self.transforms = (input_transforms, output_transforms) =>"
0.0.1,TODO(rbharath): There is a dangerous mixup in semantics. If itershards() is
0.0.1,"called without calling transform(), it will explode. Maybe have a separate"
0.0.1,initialization function to avoid this problem.
0.0.1,Store input_transforms/output_transforms so the dataset remembers its state.
0.0.1,TODO(rbharath): These lines are puzzling. Better way to avoid storage
0.0.1,duplication here?
0.0.1,Turns NaNs to zeros
0.0.1,"The following are all associated with Dataset, but are separate functions to"
0.0.1,make it easy to use multiprocessing.
0.0.1,TODO(rbharath): This is a hack. clean up.
0.0.1,TODO(rbharath): Should X be saved to out_X_transformed as well? Since
0.0.1,itershards expects to loop over X-transformed? (Ditto for y/w)
0.0.1,perform common train/test split across all tasks
0.0.1,Set missing data to have weight zero
0.0.1,Note that X_n is a list of floats
0.0.1,"Note y_n is a list of arrays of shape (n_tasks,)"
0.0.1,TODO(rbharath): This is a hack based on fact that multi-tasktype models
0.0.1,aren't supported.
0.0.1,"Sometimes all samples have zero weight. In this case, continue."
0.0.1,We need to import models so they can be created by model_builder
0.0.1,Featurize input
0.0.1,Transform data into arrays for ML
0.0.1,Split into train/test
0.0.1,Transforming train/test data
0.0.1,Fit model
0.0.1,Eval model on train
0.0.1,TODO(rbharath): There should be some automatic check to ensure that all
0.0.1,required model_params are specified.
0.0.1,Featurize input
0.0.1,Transform data into arrays for ML
0.0.1,Split into train/test
0.0.1,Transforming train/test data
0.0.1,Fit model
0.0.1,Eval model on train
0.0.1,Moving imports to be local to avoid isnstall issues with
0.0.1,"Convolution3D, which is not yet part of keras proper."
0.0.1,number of convolutional filters to use at each layer
0.0.1,level of pooling to perform at each layer (POOL x POOL)
0.0.1,level of convolution to perform at each layer (CONV x CONV)
0.0.1,"TODO(rbharath): If we change away from axis-size 32, this code will break."
0.0.1,Eventually figure out a more general rule that works for all axis sizes.
0.0.1,Note that keras requires the model architecture and weights to be stored
0.0.1,separately. A json file is generated that specifies the model architecture.
0.0.1,The weights will be stored in an h5 file. The pkl.gz file with store the
0.0.1,target name.
0.0.1,Save architecture
0.0.1,Add eps weight to avoid minibatches with zero weight (causes theano to crash).
0.0.1,"Class probabilities are predicted for classification outputs. Instead,"
0.0.1,output the most likely class.
0.0.1,TODO(rbharath): This does not work with very large datasets! sklearn does
0.0.1,"support partial_fit, but only for some models. Might make sense to make"
0.0.1,PartialSklearnModel subclass at some point to support large data models.
0.0.1,List of registered models
0.0.1,TODO(rbharath/enf): We need a structured way to deal with potential GPU
0.0.1,memory overflows.
0.0.1,TODO(rbharath): The structure of the produced df might be
0.0.1,complicated. Better way to model?
0.0.1,TODO(rbharath): This feels like a total hack. Is there a structured way
0.0.1,to deal with this instead?
0.0.1,# The following notice is copied from the original NNScore file.
0.0.1,NNScore 2.01 is released under the GNU General Public License (see
0.0.1,http://www.gnu.org/licenses/gpl.html).
0.0.1,"If you have any questions, comments, or suggestions, please don't"
0.0.1,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.1,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.1,HERE].
0.0.1,ELECTROSTATIC_JOULE_PER_MOL = 138.94238460104697e4 # units?
0.0.1,"This is just a scaling factor, so it's set so as to keep the network"
0.0.1,inputs roughly contained in 0-1
0.0.1,"O-H distance is 0.96 A, N-H is 1.01 A. See"
0.0.1,http://www.science.uwaterloo.ca/~cchieh/cact/c120/bondel.html
0.0.1,"If atoms are < 2.5 A apart, we count it as a close contact"
0.0.1,"If receptor and ligand atoms are > 4 A apart, we consider them"
0.0.1,unable to interact with simple electrostatics.
0.0.1,"""PI-Stacking Interactions ALIVE AND WELL IN PROTEINS"" says"
0.0.1,"distance of 7.5 A is good cutoff. This seems really big to me,"
0.0.1,except that pi-pi interactions (parallel) are actually usually
0.0.1,off centered. Interesting paper.  Note that adenine and
0.0.1,"tryptophan count as two aromatic rings. So, for example, an"
0.0.1,"interaction between these two, if positioned correctly, could"
0.0.1,count for 4 pi-pi interactions.
0.0.1,Cation-pi interaction cutoff based on
0.0.1,"""Cation-pi interactions in structural biology."""
0.0.1,4  is good cutoff for salt bridges according to
0.0.1,"""Close-Range Electrostatic Interactions in Proteins"","
0.0.1,"but looking at complexes, I decided to go with 5.5 A"
0.0.1,This is perhaps controversial. I noticed that often a pi-cation
0.0.1,"interaction or other pi interaction was only slightly off, but"
0.0.1,"looking at the structure, it was clearly supposed to be a pi-cation"
0.0.1,interaction. I've decided then to artificially expand the radius of
0.0.1,"each pi ring. Think of this as adding in a VDW radius, or"
0.0.1,"accounting for poor crystal-structure resolution, or whatever you"
0.0.1,want to justify it.
0.0.1,note that dictionaries (hashtables) are passed by reference in python
0.0.1,Now see if there's hydrophobic contacts (C-C contacts)
0.0.1,to convert into J/mol; might be nice to double check this
0.0.1,TODO(bramsundar): What are units of
0.0.1,ligand_charge/receptor_charge?
0.0.1,"so they're more or less perpendicular, it's probably a"
0.0.1,"pi-edge interaction having looked at many structures, I"
0.0.1,noticed the algorithm was identifying T-pi reactions
0.0.1,"when the two rings were in fact quite distant, often"
0.0.1,"with other atoms in between. Eye-balling it, requiring"
0.0.1,that at their closest they be at least 5 A apart seems
0.0.1,to separate the good T's from the bad
0.0.1,"so at their closest points, the two rings come within"
0.0.1,5 A of each other.
0.0.1,"okay, is the ligand pi pointing into the receptor"
0.0.1,"pi, or the other way around?  first, project the"
0.0.1,center of the ligand pi onto the plane of the
0.0.1,"receptor pi, and vs. versa"
0.0.1,"This could be directional somehow, like a hydrogen"
0.0.1,bond.
0.0.1,"now, if it's a true pi-T interaction, this projected"
0.0.1,point should fall within the ring whose plane it's
0.0.1,been projected into.
0.0.1,so it is in the ring on the projected plane.
0.0.1,since it could be interacting with a cofactor or something
0.0.1,Now see if there's some sort of hydrogen bond between
0.0.1,"these two atoms. distance cutoff = H_BOND_DIST, angle cutoff ="
0.0.1,H_BOND_ANGLE.
0.0.1,TODO(rbharath): This is a horrible inner-loop search. Can
0.0.1,this be made more efficient?
0.0.1,Make sure to set comment (used below)
0.0.1,Make sure to set comment (used below)
0.0.1,"print ""nearby hydrogens: "" + str(hydrogens)"
0.0.1,now we need to check the angles
0.0.1,"TODO(rbharath): Rather than using this heuristic, it seems like"
0.0.1,it might be better to just report the angle in the feature
0.0.1,vector...
0.0.1,"so there could be some pi-pi interactions.  Now, let's"
0.0.1,check for stacking interactions. Are the two pi's roughly
0.0.1,parallel?
0.0.1,"so they're more or less parallel, it's probably pi-pi"
0.0.1,"stacking now, since pi-pi are not usually right on"
0.0.1,top of each other. They're often staggered. So I don't
0.0.1,want to just look at the centers of the rings and
0.0.1,compare. Let's look at each of the atoms.  do atom of
0.0.1,"the atoms of one ring, when projected onto the plane of"
0.0.1,"the other, fall within that other ring?"
0.0.1,start by assuming it's not a pi-pi stacking interaction
0.0.1,project the ligand atom onto the plane of the receptor ring
0.0.1,TODO(rbharath): This if-else is confusing.
0.0.1,project the ligand atom onto the plane of the receptor ring
0.0.1,since it could be interacting with a cofactor or something
0.0.1,project the charged onto the plane of the aromatic
0.0.1,since it could be interacting with a cofactor or something
0.0.1,now it's the ligand that has the aromatic group
0.0.1,since it could be interacting with a cofactor or something
0.0.1,so they have oppositve charges
0.0.1,TODO(rbharath): What is atom type A here?
0.0.1,Load receptor and ligand from file.
0.0.1,## OPEN TEMPDIR
0.0.1,## CLOSE TEMPDIR
0.0.1,NNScore 2.01 is released under the GNU General Public License (see
0.0.1,http://www.gnu.org/licenses/gpl.html).
0.0.1,"If you have any questions, comments, or suggestions, please don't"
0.0.1,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.1,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.1,HERE].
0.0.1,"AddHydrogens(polaronly, correctForPH, pH)"
0.0.1,a*x + b*y + c*z = dI think that
0.0.1,"self.x, self.y, self.z = x, y, z"
0.0.1,"self.x, self.y, self.z = coords[0], coords[1], coords[2]"
0.0.1,TODO(bramsundar): Should this be __copy__?
0.0.1,"return self.dist_to(Point(coords=np.array([0, 0, 0])))"
0.0.1,"return np.array([self.x, self.y, self.z])"
0.0.1,TODO(rbharath): Should this be an atom function?
0.0.1,"This line is necessary for babel to work, though many PDBs in"
0.0.1,the PDB would have this line commented out
0.0.1,now atom type (for pdbqt)
0.0.1,"If atomtype is not specified, but atomname is, set atomtype to the"
0.0.1,"first letter of atomname. This heuristic suffices for proteins,"
0.0.1,since no two-letter elements appear in standard amino acids.
0.0.1,Any number needs to be removed from the element name
0.0.1,"this only uses the rightmost three characters, essentially"
0.0.1,removing unique rotamer identification
0.0.1,"The normal vector to plane is n = [a, b, c]"
0.0.1,We first shift by basepoint (a point on given plane) to make math
0.0.1,simpler. basepoint is given by d/||n||^2 * n
0.0.1,The perpendicular component of diff to plane is
0.0.1,(n^T diff / ||n||^2) * n
0.0.1,generate SMILES for fragments
0.0.1,import all Featurizer subclasses so __subclasses__ will work
0.0.1,these have to be local imports to avoid circular imports
0.0.1,get output from engines
0.0.1,get the maximum number of conformers
0.0.1,construct the new container
0.0.1,- first axis = # mols
0.0.1,- second axis = max # conformers
0.0.1,- remaining axes = determined by feature shape
0.0.1,fill in the container
0.0.1,"If gzipped, need to compute extension again"
0.0.1,"If CSV input, assume that first row contains labels"
0.0.1,Skip labels
0.0.1,"processed_rows = raw_df.apply(process_raw_sample_helper_partial, axis=1)"
0.0.1,raw_df = pd.DataFrame.from_records(processed_rows)
0.0.1,"pandas rows are tuples (row_num, row_data)"
0.0.1,The standard columns for featurized data.
0.0.1,"compounds_df is not altered by any method after initialization, so it's"
0.0.1,safe to keep a copy in memory and on disk.
0.0.1,TODO(rbharath): Might this be inefficient?
0.0.1,Sort from largest to smallest scaffold sets
0.0.1,list-of-available-descriptors.
0.0.1,NNScore 2.01 is released under the GNU General Public License (see
0.0.1,http://www.gnu.org/licenses/gpl.html).
0.0.1,"If you have any questions, comments, or suggestions, please don't"
0.0.1,"hesitate to contact me, Jacob Durrant, at jdurrant [at] ucsd [dot]"
0.0.1,"edu. If you use NNScore 2.01 in your work, please cite [REFERENCE"
0.0.1,HERE].
0.0.1,Remove rings of length 0
0.0.1,"To remove duplicate entries, we convert rings from a list to set, and"
0.0.1,then back to a list again. There's a snafu since each ring in rings is
0.0.1,itself a list (and lists are unhashable in python). To circumvent this
0.0.1,"issue, we convert each ring into a string (after sorting). For example,"
0.0.1,"[2, 1] maps to '[1, 2]'. These strings are hashable. To recover the"
0.0.1,"original lists, we use ast.literal_eval."
0.0.1,Use dictionary to maintain state about which rings are supersets.
0.0.1,All distances are in Angstroms. Duplicate pairs not specified. For
0.0.1,"example, to find distance (""H"", ""C""), the lookup key is (""C"", ""H"")"
0.0.1,"This one not from source sited above. Not sure where it's from, but"
0.0.1,"it wouldn't ever be used in the current context (""AutoGrow"")"
0.0.1,estimate based on eye balling Handbook of Chemistry and Physics
0.0.1,Reset internal state
0.0.1,Now load the file into a list
0.0.1,"Load atom data (coordinates, etc.)"
0.0.1,this string unique identifies each atom
0.0.1,so each atom can only be loaded once. No rotamers.
0.0.1,So you're actually reindexing everything here.
0.0.1,### TODO(rbharath): Disabling loading of non
0.0.1,Check that the range is nonempty.
0.0.1,"just so no PDB is empty, VMD will load them all"
0.0.1,write coordinates
0.0.1,first get available index
0.0.1,now add atom
0.0.1,Add to non-protein list
0.0.1,Functions to determine the bond connectivity based on distance
0.0.1,==============================================================
0.0.1,Functions to identify positive charges
0.0.1,======================================
0.0.1,Metallic atoms are assumed to be cations.
0.0.1,Get all the quartenary amines on non-protein residues (these are the
0.0.1,only non-protein groups that will be identified as positively
0.0.1,charged). Note that nitrogen has only 5 valence electrons (out of 8
0.0.1,"for a full shell), so any nitrogen with four bonds must be positively"
0.0.1,charged (think NH4+).
0.0.1,"a quartenary amine, so it's easy"
0.0.1,so the indices stored is just the index of the nitrogen and any
0.0.1,attached atoms
0.0.1,"maybe you only have two hydrogens added, but they're sp3 hybridized."
0.0.1,"Just count this as a quartenary amine, since I think the positive"
0.0.1,charge would be stabilized. This situation can arise with
0.0.1,lone-pair electron nitrogen compounds like pyrrolidine
0.0.1,(http://www.chem.ucla.edu/harding/tutorials/lone_pair.pdf)
0.0.1,Test that the angles approximately match the tetrahedral 109
0.0.1,degrees
0.0.1,so indexes added are the nitrogen and any attached atoms.
0.0.1,let's check for a phosphate or anything where a phosphorus is bound
0.0.1,"to two oxygens, where both oxygens are bound to only one heavy atom"
0.0.1,(the phosphorus). I think this will get several phosphorus
0.0.1,substances.
0.0.1,now count the number of oxygens bound only to the phosphorus
0.0.1,"let's check for guanidino-like groups (actually H2N-C-NH2,"
0.0.1,where not CN3.)
0.0.1,if the carbon has only three atoms connected to it
0.0.1,"if true, carbon is connected to at least two nitrogens now,"
0.0.1,so we need to count the number of nitrogens that are only
0.0.1,connected to one heavy atom (the carbon)
0.0.1,Index of atom that connects this charged group to
0.0.1,"the rest of the molecule, ultimately to make sure"
0.0.1,it's sp3 hybridized. Remains -1 if no such atom exists.
0.0.1,TODO(rbharath): Is picking the first non-nitrogen atom
0.0.1,correct here?
0.0.1,Handle case of guanidinium cation
0.0.1,so there are at two nitrogens that are only
0.0.1,connected to the carbon (and probably some
0.0.1,hydrogens)
0.0.1,now you need to make sure connector_ind atom is sp3 hybridized
0.0.1,"There are only two ""guanidino"" nitrogens. Assume the"
0.0.1,negative charge is spread equally between the two.
0.0.1,a carboxylate carbon will have three items connected to it.
0.0.1,a carboxylate will have two oxygens connected to
0.0.1,"it. Now, each of the oxygens should be connected"
0.0.1,to only one heavy atom (so if it's connected to a
0.0.1,"hydrogen, that's okay)"
0.0.1,so it's a carboxylate! Add a negative charge.
0.0.1,Assume negative charge is centered between the two
0.0.1,oxygens.
0.0.1,let's check for a sulfonate or anything where a sulfur is
0.0.1,bound to at least three oxygens and at least three are
0.0.1,bound to only the sulfur (or the sulfur and a hydrogen).
0.0.1,the sulfur is bound to at least three oxygens now
0.0.1,count the number of oxygens that are only bound to the
0.0.1,sulfur
0.0.1,so there are at least three oxygens that are only
0.0.1,bound to the sulfur
0.0.1,Group atoms in the same residue together
0.0.1,Assign each atom a residue key.
0.0.1,Handle edge case of last residue.
0.0.1,Select those atoms which are part of the charged group.
0.0.1,Functions to identify aromatic rings
0.0.1,====================================
0.0.1,first identify the center point
0.0.1,now get the plane that defines this ring. Recall that there are
0.0.1,atleast 3-points in indices_of_ring by ValueError above.
0.0.1,# formula for plane will be ax + by + cz = d
0.0.1,"first, let's see if the last atom in this ring is a carbon"
0.0.1,connected to four atoms. That would be a quick way of
0.0.1,telling this is not an aromatic ring
0.0.1,now check the dihedral between the ring atoms to see if
0.0.1,it's flat
0.0.1,"15 degrees is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.1,ring[ind+3] range of this function is -pi to pi
0.0.1,now check the dihedral between the ring atoms and an atom
0.0.1,connected to the current atom to see if that's flat too.
0.0.1,"15 degress is the cutoff, ring[ind], ring[ind+1], ring[ind+2],"
0.0.1,"ring[ind+3], range of this function is -pi to pi"
0.0.1,Get all the rings containing each of the atoms in the ligand
0.0.1,Aromatic rings are of length 5 or 6
0.0.1,"Due to data errors in PDB files, there are cases in which"
0.0.1,non-protein atoms are bonded to protein atoms. Manually remove these
0.0.1,"cases, by testing that ring atom indices are a subset of non-protein"
0.0.1,ring indices.
0.0.1,Aromatic rings are flat
0.0.1,Aromatic rings are of length <= 6
0.0.1,At least 3 indices are required to identify the aromatic plane.
0.0.1,if self.get_aromatic_marker(indices_of_ring) is None:
0.0.1,"raise ValueError(""None at %s for %s"" % (key,"
0.0.1,str(indices_of_ring)))
0.0.1,Tryptophan has two aromatic rings.
0.0.1,Functions to assign secondary structure to protein residues
0.0.1,===========================================================
0.0.1,"first, we need to know what residues are available"
0.0.1,print self.get_residues()
0.0.1,TODO(rbharath): Why magic number 8?
0.0.1,now make sure the first four all have the same resid and
0.0.1,the last four all have the same resid
0.0.1,TODO(rbharath): Ugly code right here...
0.0.1,Now give easier to use names to the atoms
0.0.1,Now compute the phi and psi dihedral angles
0.0.1,Now use those angles to determine if it's alpha or beta
0.0.1,A residue of index i is only going to be in an alpha helix
0.0.1,its CA is within 6 A of the CA of the residue i + 3
0.0.1,so it's in an alpha helix
0.0.1,so now compare that CA to all the other CA's
0.0.1,so it's also in an alpha helix
0.0.1,so this CA atom is one of the ones the first atom
0.0.1,might hydrogen bond with
0.0.1,so these two CA atoms are close enough together
0.0.1,that their residues are probably hydrogen bonded
0.0.1,Alpha helices are only alpha helices if they span at least 4
0.0.1,residues (to wrap around and hydrogen bond). I'm going to
0.0.1,"require them to span at least 5 residues, based on"
0.0.1,examination of many structures.
0.0.1,now go through each of the BETA CA atoms. A residue is only
0.0.1,going to be called a beta sheet if CA atom is within 6.0 A
0.0.1,"of another CA beta, same chain, but index difference > 2."
0.0.1,so it's in a beta sheet
0.0.1,so not comparing an atom to itself
0.0.1,so you're comparing it only to other BETA-sheet atoms
0.0.1,so require them to be on the same chain. needed to
0.0.1,indices can be fairly compared
0.0.1,so the two residues are not simply adjacent to each
0.0.1,other on the chain
0.0.1,so these to atoms are close to each other
0.0.1,Now some more post-processing needs to be done. Do this
0.0.1,again to clear up mess that may have just been created
0.0.1,"(single residue beta strand, for example)"
0.0.1,Beta sheets are usually at least 3 residues long
0.0.1,so they are sequential
0.0.1,Now update each of the atoms with this structural information
0.0.1,Use this list to perform sanity checks on alpha-helix and beta-sheet
0.0.1,labels.
0.0.1,check for separate count and SMILES entries for each fragment
0.0.1,## 3zso comes from PDBBind-CN
0.0.1,The ligand is also specified by pdbbind
0.0.1,"Currently, just verifies that nothing crashes."
0.0.1,## 3zp9 comes from PDBBind-CN
0.0.1,The ligand is also specified by pdbbind
0.0.1,## 3bwf comes from PDBBind-CN
0.0.1,The ligand is also specified by pdbbind
0.0.1,"The keys of these dicts are pairs of atomtypes, but the keys are"
0.0.1,"sorted so that (""C"", ""O"") is always written as ""C_O"". Thus, for N"
0.0.1,"atom types, there are N*(N+1)/2 unique pairs."
0.0.1,TODO(rbharath): Charges are not computed correctly for certain
0.0.1,ligands! (see 2y2h_ligand). Understand why this happens.
0.0.1,assert np.count_nonzero(np.array(electrostatics.values())) > 0
0.0.1,print counts
0.0.1,1zea is the only example that has any pi-stacking.
0.0.1,Lengths:
0.0.1,ligand_receptor_close_contacts: N*(N+1)/2
0.0.1,ligand_receptor_contacts: N*(N+1)/2
0.0.1,ligand_receptor_electrostatics: N*(N+1)/2
0.0.1,ligand_atom_counts: N
0.0.1,hbonds: 12
0.0.1,hydrophobics: 6
0.0.1,stacking: 3
0.0.1,pi_cation: 6
0.0.1,t_shaped: 3
0.0.1,active_site_flexibility: 6
0.0.1,salt_bridges: 3
0.0.1,rotatable_boonds_count: 1
0.0.1,We need to import models so they can be created by model_builder
