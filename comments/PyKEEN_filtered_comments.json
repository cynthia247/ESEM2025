{
  "v1.11.0": [
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "autodoc_member_order = 'bysource'",
    "autodoc_preserve_defaults = True",
    "we start by creating the representation for those entities where we have pre-trained features",
    "here we simulate this for a set of Asian countries",
    "Next, we directly create representations for the remaining ones using the backfill representation.",
    "To do this, we need to create an iterable (e.g., a set) of all of the entity IDs that are in the base",
    "representation. Then, the assignments to the base representation and an auxillary representation are",
    "automatically generated for the base class.",
    "We assume that we do not have any pre-trained information for relations here for simplicity and train",
    "them from scratch.",
    "The combined representation can now be used as any other representation, e.g., to train a model with",
    "distmult interaction:",
    "Step 1: Get triples",
    "Step 2: Configure the model",
    "Step 3: Configure the loop",
    "Step 4: Train",
    "Step 5: Evaluate the model",
    "create one checkpoint every 10 epochs",
    "create checkpoints every 5 epochs, and at epoch 7",
    "create one checkpoint every 10 epochs",
    "create checkpoints at epoch 1, 7, and 10",
    "create a default result tracker (or use a proper one)",
    "in this example, we just use the training loss",
    "Important: use the same result tracker instance as in the checkpoint callback",
    "%%",
    "As an example, we will use a small dataset that comes with entity and relation labels.",
    "This dataset provides entity names.",
    "%%",
    "Direct access to the mapping, here for entities.",
    "a mapping from labels/strings to the Ids",
    "the inverse mapping",
    "%%",
    "The labeling object also offers convenience methods for converting ids in different formats to strings",
    "%%",
    "The triples factory exposes utility methods to normalize to ids",
    "TODO: we should move that to the labeling",
    "%%",
    "Get tensor of entity identifiers",
    "%%",
    "train a model",
    "access entity and relation representations",
    "%%",
    "TransE has one representation for entities and one for relations",
    "both are simple embedding matrices",
    "%%",
    "get representations for all entities/relations",
    "%%",
    "this corresponds to explicitly passing indices=None",
    "%%",
    "%%",
    "detach tensor, move to cpu, and convert to numpy",
    "Get a training dataset",
    "The following applies to most packaged datasets,",
    "although the dataset class itself makes `validation' optional.",
    "Pick a model",
    "Pick an optimizer from PyTorch",
    "Pick a training approach (sLCWA or LCWA)",
    "Train like Cristiano Ronaldo",
    "Pick an evaluator",
    "Evaluate",
    "print(results)",
    "get a dataset",
    "Pick a model",
    "Pick a training approach (sLCWA or LCWA)",
    "Train like Cristiano Ronaldo",
    "NEW: validation evaluation callback",
    "Pick an evaluation loop (NEW)",
    "Evaluate",
    "print(results)",
    "check probability distribution",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "ignore abstract classes",
    "verify that all classes have the hpo_default dictionary",
    "verify that we can bind the keys to the __init__'s signature",
    "note: this is only of limited use since many have **kwargs which",
    "raise an OOM error whenever the batch size is larger than 1",
    "TODO: predict_target",
    "docstr-coverage: inherited",
    "check if within 0.5 std of observed",
    "test error is raised",
    "there is an extra test for this case",
    "docstr-coverage: inherited",
    "same size tensors",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Tests that exception will be thrown when more than or less than two tensors are passed",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "ensure each node participates in at least one edge",
    "check type and shape",
    "number of colors is monotonically increasing",
    "ensure each node participates in at least one edge",
    "normalize",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    "negative number",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "See https://github.com/pykeen/pykeen/pull/883",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising",
    "wrong ndim",
    "wrong last dim",
    "wrong dtype: float",
    "wrong dtype: complex",
    "correct",
    ">>> positional argument",
    "mapped_triples",
    "triples factory",
    "labeled triples + factory",
    "single labeled triple",
    "multiple labeled triples as list",
    "multiple labeled triples as array",
    ">>> keyword only",
    "fixme: find reason / enforce single-thread",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "simulate creating a new triples factory with shared set of relations by shuffling",
    "quaternion have four components",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "cf. https://github.com/pykeen/pykeen/issues/1118",
    "save a reference to the old init *before* mocking",
    "run a small pipline",
    "use sampled training loop ...",
    "... without explicitly selecting a negative sampler ...",
    "... but providing custom kwargs",
    "other parameters for fast test",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    "check for invalid keys",
    "check that each parameter without a default occurs",
    "try to instantiate loss for some configurations in the HPO search space",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    "the relative tolerance for checking close results, cf. torch.allclose",
    "the absolute tolerance for checking close results, cf. torch.allclose",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "allclose checks: | input - other | < atol + rtol * |other|",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "verify that the regularizer is stored for both, entity and relation representations",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "regularization term should be zero",
    "updated should be set to false",
    "call method",
    "generate random tensors",
    "generate inputs",
    "call update",
    "check shape",
    "check result",
    "generate single random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "update term",
    "check that the expected term is returned",
    "check that the regularizer is now reset",
    "create another instance with apply_only_once enabled",
    "test initial state",
    "after first update, should change the term",
    "after second update, no change should happen",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "TODO: look into score_r for inverse relations",
    "clear buffers for message passing models",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    "this implicitly tests extra_repr / iter_extra_repr",
    "select random indices",
    "forward pass with full graph",
    "forward pass with restricted graph",
    "verify the results are similar",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": the input dimension",
    ": the output dimension",
    ": the number of entities",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    ": the interaction to use for testing a model",
    "initializers *may* work in-place => clone",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    "inferred from triples factory",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "generate random weights such that sum = n",
    "for sanity checking: give the largest weight to best rank => should improve",
    "generate two versions",
    "1. repeat each rank/candidate pair a random number of times",
    "2. do not repeat, but assign a corresponding weight",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "generate random triples factories",
    "generate random alignment",
    "add label information if necessary",
    "prepare alignment data frame",
    "call",
    "check",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "verify that the input is valid",
    "combine",
    "verify shape",
    "to be initialized in subclass",
    "no column has been removed",
    "all old columns are unmodified",
    "new columns are boolean",
    "no columns have been added",
    "check subset relation",
    "check for unique values",
    "check for subset property",
    "maybe additional checks",
    "TODO: this could be shared with the model tests",
    "fixme: CompGCN leads to an autograd runtime error...",
    "models.CompGCN: dict(embedding_dim=EMBEDDING_DIM),",
    "FixedModel: dict(embedding_dim=EMBEDDING_DIM),",
    "test combinations of models with training loops",
    "some models require inverse relations",
    "some model require access to the training triples",
    "inductive models require an inductive mode to be set, and an inference factory to be passed",
    "fake an inference factory",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "sum over the softmax dim as weights sum up to 1",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "abstract classes",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "prediction post-processing",
    "mock prediction data frame",
    "score consumers",
    "use a small model, since operation is expensive",
    "all scores, automatic batch size",
    "top 3 scores",
    "top 3 scores, fixed batch size, head scoring",
    "all scores, relation scoring",
    "all scores, relation scoring",
    "model with inverse relations",
    "check type",
    "check shape",
    "check ID ranges",
    "mapped triples, automatic batch size selection, no factory",
    "mapped triples, fixed batch size, no factory",
    "labeled triples with factory",
    "labeled triples as list",
    "single labeled triple",
    "model with inverse relations",
    "ID-based, no factory",
    "string-based + factory",
    "mixed + factory",
    "no restriction, no factory",
    "no restriction, factory",
    "id restriction, no factory ...",
    "id restriction with factory",
    "comment: we only use id-based input, since the normalization has already been tested",
    "create model",
    "id-based head/relation/tail prediction, no restriction",
    "restriction by list of ints",
    "tail prediction",
    "try accessing each element",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "check case",
    "apply restriction",
    "check monotonicity (in counts)",
    "check factories",
    "clear",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "extra-relation",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "additional",
    "verify shape",
    "verify dtype",
    "verify number of entities/relations",
    "verify offsets",
    "create old, new pairs",
    "simulate merging ids",
    "only a single pair",
    "apply",
    "every key is contained",
    "value range",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "TODO: we could move this part into the interaction module itself",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u^T act(h^T W t + V [h; t] + b)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "hidden state:",
    "1. \"h^T W t\"",
    "2. \"V [h; t]\"",
    "3. \"+ b\"",
    "activation",
    "projection",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "we calculate the scores using the hard-coded formula, instead of utilizing table + einsum",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize rotations to unit modulus",
    "check for unit modulus",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "message_dim must be divisible by num_heads",
    "generate test data (with fixed seed for reproducibility)",
    "get result using argpartition",
    "check shape",
    "check type",
    "check value range",
    "check equality with argsort",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "complex tensor",
    "check value range",
    "check modulus == 1",
    "quaternion needs shape to end on 4",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "value range",
    "highest degree node has largest value",
    "Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf",
    "create triples with a dummy relation type 0",
    "0: green: 2, 3, 7, 8",
    "1: red: 1, 4, 6, 9",
    "2: blue: 0, 5",
    "the example includes the first power",
    "requires at least one complex tensor as input",
    "check type",
    "check size",
    "check value range",
    "inferred from triples factory",
    "inferred from assignment",
    "the representation module infers the max_id from the provided labels",
    "the following entity does not have an image -> will have to use backfill",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "max_id is inferred from assignment",
    "create random assignment",
    "update kwargs",
    "empty bases",
    "inconsistent base shapes",
    "invalid base id",
    "invalid local index",
    "docstr-coverage: inherited",
    "allocate result",
    "prepare distributions",
    "ensure positivity for variance",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "TODO: more tests",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "x_i ~ N(mu_i, 1)",
    "closed-form solution",
    "sampled confidence interval",
    "check that closed-form is in confidence interval of sampled",
    "positive values only",
    "positive and negative values",
    "Check for correct class",
    "check correct num_entities",
    "check type",
    "check length",
    "check type",
    "check length",
    "check confidence positivity",
    "Check for correct class",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "initialize",
    "update with batches",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": synonyms of this loss",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "docstr-coverage: inherited",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "0. default",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "docstr-coverage: inherited",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "docstr-coverage: inherited",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "docstr-coverage: inherited",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "docstr-coverage: inherited",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "make sure labels form a proper probability distribution",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "subtract margin from positive scores",
    "divide by temperature",
    "docstr-coverage: inherited",
    "subtract margin from positive scores",
    "normalize positive score shape",
    "divide by temperature",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "we pass *all* scores as negatives, but set the weight of positives to zero",
    "this allows keeping a dense shape",
    "docstr-coverage: inherited",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)",
    "note: this is a reduction along the softmax dim; since the weights are already normalized",
    "to sum to one, we want a sum reduction here, instead of using the self._reduction",
    "docstr-coverage: inherited",
    "Sanity check",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative loss part",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    ": A resolver for loss modules",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "docstr-coverage: inherited",
    ": A resolver for relation inverter protocols",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "upgrade to sequence",
    "broadcast",
    "normalize ids: -> ids.shape: (batch_size, num_ids)",
    "normalize batch -> batch.shape: (batch_size, 1, 3)",
    "allocate memory",
    "copy ids",
    "reshape",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "note: this is a hack, and should be fixed up-stream by making NodePiece",
    "use proper complex embeddings for rotate interaction; however, we also have representations",
    "that perform message passing, and we would need to propagate the base representation's complexity through it",
    "ensure pathlib",
    "cf. https://stackoverflow.com/a/1176023",
    "check validity",
    "path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "resolve path to make sure it is an absolute path",
    "ensure directory exists",
    "message passing: collect colors of neighbors",
    "dense colors: shape: (n, c)",
    "adj:          shape: (n, n)",
    "values need to be float, since torch.sparse.mm does not support integer dtypes",
    "size: will be correctly inferred",
    "concat with old colors",
    "hash",
    "create random indicator functions of low dimensionality",
    "collect neighbors' colors",
    "round to avoid numerical effects",
    "hash first",
    "concat with old colors",
    "re-hash",
    "only keep connectivity, but remove multiplicity",
    "note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,",
    "this is rather useless",
    "initial: degree",
    "note: we calculate this separately, since we can use a more efficient implementation for the first step",
    "hash",
    "determine small integer type for dense count array",
    "convergence check",
    "each node has a unique color",
    "the number of colors did not improve in the last iteration",
    "cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports",
    "docstr-coverage: excused `wrapped`",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "docstr-coverage: inherited",
    "no need to compute anything",
    "docstr-coverage: inherited",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "regularizer-specific parameters",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "orthogonality soft constraint: cosine similarity at most epsilon",
    "The normalization factor to balance individual regularizers' contribution.",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A resolver for regularizers",
    "high-level",
    "Low-Level",
    "cf. https://github.com/python/mypy/issues/5374",
    ": the dataframe; has to have a column named \"score\"",
    ": an optional factory to use for labeling",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the prediction target",
    ": the other column's fixed IDs",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the ID-based triples, shape: (n, 3)",
    ": the scores",
    "3-tuple for return",
    "extract label information, if possible",
    "no restriction",
    "restriction is a tensor",
    "restriction is a sequence of integers or strings",
    "now, restriction is a sequence of integers",
    "if explicit ids have been given, and label information is available, extract list of labels",
    "exactly one of them is None",
    "create input batch",
    "note type alias annotation required,",
    "cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases",
    "batch, TODO: ids?",
    "docstr-coverage: inherited",
    "initialize buffer on device",
    "docstr-coverage: inherited",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "determine corresponding indices",
    "batch_id, score_id = divmod(top_indices, num_scores)",
    "combine to top triples",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "docstr-coverage: inherited",
    "TODO: variable targets across batches/samples?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "(?, r, t) => r.stride > t.stride",
    "(h, ?, t) => h.stride > t.stride",
    "(h, r, ?) => h.stride > r.stride",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "train model; note: needs larger number of epochs to do something useful ;-)",
    "create prediction dataset, where the head entities is from a set of European countries,",
    "and the relations are connected to tourism",
    "calculate all scores for this restricted set, and keep k=3 largest",
    "add labels",
    ": the choices for the first and second component of the input batch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "calculate batch scores onces",
    "consume by all consumers",
    "TODO: Support partial dataset",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "exactly one of them is None",
    "",
    "TODO: add support for (automatic) slicing",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "get input & target",
    "get label-to-id mapping and prediction targets",
    "get scores",
    "note: maybe we want to expose these scores, too?",
    "create raw dataframe",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "normalize input",
    "calculate scores (with automatic memory optimization)",
    "determine fully qualified name",
    "shorten to main module",
    "verify that short name can be imported from the abbreviated reference",
    "get docdata and extract name & citation",
    "fallback for name: capitalized class name without base suffix",
    "extract citation information and warn about lack thereof",
    "compose reference",
    "cf. https://github.com/python/mypy/issues/5374",
    "\"Closed-Form Expectation\",",
    "\"Closed-Form Variance\",",
    "\"\u2713\" if metric.closed_expectation else \"\",",
    "\"\u2713\" if metric.closed_variance else \"\",",
    "Add HPO command",
    "Add NodePiece tokenization command",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    "entity alignment sides",
    "utils",
    "some PyTorch functions to not properly propagate types (e.g., float() does not return FloatTensor but Tensor)",
    "however it is still useful to distinguish float tensors from long ones",
    "to make the switch easier once PyTorch improves typing, we use a global type alias inside PyKEEN.",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "entity alignment",
    "input normalization",
    "note: the base class does not have any parameters",
    "Heuristic for default value",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: the only parameters are inside the relation representation module, which has its own reset_parameters",
    "docstr-coverage: inherited",
    "TODO: can we change the dimension order to make this contiguous?",
    "docstr-coverage: inherited",
    "normalize num blocks",
    "determine necessary padding",
    "determine block sizes",
    "(R, nb, bsi, bso)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(n, di) -> (n, nb, bsi)",
    "(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)",
    "(R, n, nb, bso) -> (R * n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "(n, R * n), (R * n, do) -> (n, do)",
    "remove padding if necessary",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(R * n, n), (n, di) -> (R * n, di)",
    "(R * n, di) -> (R, n, nb, bsi)",
    "(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)",
    "(n, nb, bso) -> (n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "remove padding if necessary",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "TODO: we could cache the stacked adjacency matrices",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "input validation",
    "has to be imported now to avoid cyclic imports",
    "has to be assigned after call to nn.Module init",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "heuristic",
    "normalize *before* repeating",
    "repeat if necessary",
    "regularize *after* repeating",
    "dropout & regularizer will appear automatically, since it is a nn.Module",
    "has to be imported here to avoid cyclic import",
    "docstr-coverage: inherited",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "docstr-coverage: inherited",
    "initialize weights in-place",
    "docstr-coverage: inherited",
    "apply constraints in-place",
    "fixme: work-around until nn.Embedding supports complex",
    "docstr-coverage: inherited",
    "fixme: work-around until nn.Embedding supports complex",
    "verify that contiguity is preserved",
    "create low-rank approximation object",
    "get base representations, shape: (n, *ds)",
    "calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))",
    "overwrite bases and weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    ": Constrainers",
    ":",
    ": - :func:`torch.nn.functional.normalize`",
    ": - :func:`complex_normalize`",
    ": - :func:`torch.clamp`",
    ": - :func:`clamp_norm`",
    ": Normalizers, which has by default:",
    ":",
    ": - :func:`torch.nn.functional.normalize`",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "TODO: might not be true for all compositions",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "docstr-coverage: inherited",
    "check max_id",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "docstr-coverage: inherited",
    ": the base representations",
    ": the combination module",
    "input normalization",
    "has to be imported here to avoid cyclic import",
    "create base representations",
    "verify same ID range",
    "note: we could also relax the requiremen, and set max_id = min(max_ids)",
    "shape inference",
    "assign base representations *after* super init",
    "docstr-coverage: inherited",
    "delegate to super class",
    "docstr-coverage: inherited",
    "Generate graph dataset from the Monarch Disease Ontology (MONDO)",
    ": the assignment from global ID to (representation, local id), shape: (max_id, 2)",
    "import here to avoid cyclic import",
    "instantiate base representations if necessary",
    "there needs to be at least one base",
    "while possible, this might be unintended",
    "extract shape",
    "check for invalid base ids",
    "check for invalid local indices",
    "assign modules / buffers *after* super init",
    "docstr-coverage: inherited",
    "flatten assignment to ease construction of inverse indices",
    "we group indices by the representation which provides them",
    "thus, we need an inverse to restore the correct order",
    "get representations",
    "update inverse indices",
    "invert flattening",
    "import here to avoid cyclic import",
    "comment: not all representations support passing a shape parameter",
    "create assignment",
    "base",
    "other",
    "import here to avoid cyclic import",
    "infer shape",
    "infer max_id",
    "docstr-coverage: inherited",
    "TODO: can be a combined representations, with appropriate tensor-train combination",
    ": shape: (max_id, num_cores)",
    ": the bases, length: num_cores, with compatible shapes",
    "check shape",
    "check value range",
    "do not increase counter i, since the dimension is shared with the following term",
    "i += 1",
    "ids //= m_i",
    "import here to avoid cyclic import",
    "normalize ranks",
    "determine M_k, N_k",
    "TODO: allow to pass them from outside?",
    "normalize assignment",
    "determine shapes and einsum equation",
    "create base representations",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "abstract",
    "concrete classes",
    ": A resolver for PyG message passing layers",
    "default flow",
    ": the message passing layers",
    ": the flow direction of messages across layers",
    ": the edge index, shape: (2, num_edges)",
    "fail if dependencies are missing",
    "avoid cyclic import",
    "the base representations, e.g., entity embeddings or features",
    "verify max_id",
    "verify shape",
    "assign sub-module *after* super call",
    "initialize layers",
    "normalize activation",
    "check consistency",
    "buffer edge index for message passing",
    "TODO: inductiveness; we need to",
    "* replace edge_index",
    "* replace base representations",
    "* keep layers & activations",
    "docstr-coverage: inherited",
    "we can restrict the message passing to the k-hop neighborhood of the desired indices;",
    "this does only make sense if we do not request *all* indices",
    "k_hop_subgraph returns:",
    "(1) the nodes involved in the subgraph",
    "(2) the filtered edge_index connectivity",
    "(3) the mapping from node indices in node_idx to their new location, and",
    "(4) the edge mask indicating which edges were preserved",
    "we only need the base representations for the neighbor indices",
    "get *all* base representations",
    "use *all* edges",
    "perform message passing",
    "select desired indices",
    "docstr-coverage: inherited",
    ": the edge type, shape: (num_edges,)",
    "register an additional buffer for the categorical edge type",
    "docstr-coverage: inherited",
    ": the relation representations used to obtain initial edge features",
    "avoid cyclic import",
    "docstr-coverage: inherited",
    "get initial relation representations",
    "select edge attributes from relation representations according to relation type",
    "perform message passing",
    "apply relation transformation, if necessary",
    "Classes",
    "Resolver",
    "backwards compatibility",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "note: we explicitly need to provide a relation initializer here,",
    "since ERMLPE shares initializers between entities and relations by default",
    "must be cloned if we want to do backprop",
    "the color initializer",
    "variants for the edge index",
    "additional parameters for iter_weisfeiler_lehman",
    "normalize shape",
    "get coloring",
    "make color initializer",
    "initialize color representations",
    "note: this could be a representation?",
    "init entity representations according to the color",
    "create random walk matrix",
    "TODO replace iter_matrix_power and safe_diagonal with torch_ppr functions?",
    "stack diagonal entries of powers of rw",
    ": A resolver for initializers, including both elements of :mod:`torch.nn.init` and",
    ": custom additions in :mod:`pykeen.nn.init`",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "backward compatibility with RGCN",
    "docstr-coverage: inherited",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    ": A resolver for R-GCN edge weighting implementations",
    "Caches",
    "if the sparsity becomes too low, convert to a dense matrix",
    "note: this heuristic is based on the memory consumption,",
    "for a sparse matrix, we store 3 values per nnz (row index, column index, value)",
    "performance-wise, it likely makes sense to switch even earlier",
    "`torch.sparse.mm` can also deal with dense 2nd argument",
    "note: torch.sparse.mm only works for COO matrices;",
    "@ only works for CSR matrices",
    "convert to COO, if necessary",
    "we need to use indices here, since there may be zero diagonal entries",
    "darglint does not like",
    "raise cls(shape=shape, reference=reference)",
    "Normalize relation embeddings",
    "1 * ? = ?; ? * 1 = ?",
    "i**2 = j**2 = k**2 = -1",
    "i * j = k; i * k = -j",
    "j * i = -k, j * k = i",
    "k * i = j; k * j = -i",
    "noqa: DAR401",
    "docstr-coverage: inherited",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "docstr-coverage: inherited",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    ": A resolver for similarities for :class:`pykeen.nn.modules.KG2EInteraction`",
    "REPRESENTATION",
    "base",
    "concrete",
    "INITIALIZER",
    "INTERACTIONS",
    "Adapter classes",
    "Concrete Classes",
    "combinations",
    "TODO: split file into multiple smaller ones?",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    "docstr-coverage:excused `overload`",
    "docstr-coverage:excused `overload`",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for relation representations",
    "if the interaction function's head parameter should only receive a subset of entity representations",
    "if the interaction function's tail parameter should only receive a subset of entity representations",
    "TODO: does not seem to be used",
    ": the interaction's value range (for unrestricted input)",
    "TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2",
    "TODO: annotate properties, e.g., symmetry, and use them for testing?",
    "TODO: annotate complexity?",
    ": whether the interaction is defined on complex input",
    ": The functional interaction form",
    "docstr-coverage: inherited",
    "TODO: we only allow single-tensor representations here, but could easily generalize",
    "docstr-coverage: inherited",
    "TODO: implement the unbalanced variant from the paper: f(h, r, t) = (h + r)^T t",
    "TODO: update class docstring",
    "TODO: give this a better name?",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Height and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    ": the embedding dimension",
    ": the number of input channels of the convolution",
    ": the embedding \"image\" height",
    ": the embedding \"image\" width",
    ": the number of output channels of the convolution",
    ": the convolution kernel height",
    ": the convolution kernel width",
    "resolve image shape",
    "Store initial input for error message",
    "infer open dimensions from the remainder",
    "resolve kernel size defaults",
    "vector & scalar offset",
    "the offset is only used for tails",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "The interaction model",
    "docstr-coverage: inherited",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "cat into shape (..., 1, d, 3)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "normalize hidden_dim",
    "shortcut for same shape",
    "split weight into head-/relation-/tail-specific sub-matrices",
    "docstr-coverage: inherited",
    "Initialize biases with zero",
    "In the original formulation,",
    "repeat if necessary, and concat head and relation, (*batch_dims, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "project to relation specific subspace",
    "ensure constraints",
    "TODO: update docstring",
    "TODO: give this a better name?",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "docstr-coverage: inherited",
    "projections",
    "default core tensor initialization",
    "cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "normalize initializer",
    "normalize relation dimension",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "docstr-coverage: inherited",
    "instantiate here to make module easily serializable",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "x_2 contraction",
    "x_1 contraction",
    "shapes",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "there are separate biases for entities in head and tail position",
    "docstr-coverage: inherited",
    "with k=4",
    "TODO: this sign is in the official code, too, but why do we need it?",
    "note: this is a fused kernel for computing the Hamilton product and the inner product at once",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: expose initialization?",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "relation box head; relation box tail",
    "head position and bump",
    "tail position and bump",
    "head score",
    "head box score",
    "tail box score",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "input normalization",
    "Core tensor",
    "docstr-coverage: inherited",
    "initialize core tensor",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer to (2, prod(batch_dims), dim)",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output along sequence dimension, (prod(batch_dims), dim)",
    "output shape: (prod(batch_dims), dim)",
    "reshape",
    "r_head, r_mid, r_tail",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "type alias for AutoSF block description",
    "head_index, relation_index, tail_index, sign",
    ": a description of the block structure",
    "convert to tuple",
    "infer the number of entity and relation representations",
    "verify coefficients",
    "dynamic entity / relation shapes",
    "docstr-coverage: inherited",
    "r_head, r_bias, r_tail",
    ": A resolver for stateful interaction functions",
    "Concrete classes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "input normalization",
    "instantiate separate combinations",
    "docstr-coverage: inherited",
    "split complex; repeat real",
    "separately combine real and imaginary parts",
    "combine",
    "docstr-coverage: inherited",
    "symbolic output to avoid dtype issue",
    "we only need to consider real part here",
    "the gate",
    "the combination",
    "docstr-coverage: inherited",
    "Base",
    "Concrete",
    "Resolver",
    ": The stateless function that gets composed",
    "docstr-coverage: inherited",
    "NOTE: wrapping torch.sub and torch.mul since their docstrings cause an issue...",
    ": Subtracts with :func:`torch.sub`",
    ": Multiplies with :func:`torch.mul`",
    ": A path to an image file or a tensor representation of the image",
    ": A sequence of image hints",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "infer shape",
    "docstr-coverage: inherited",
    "TODO extract out a shared base class if we ever get a second image source",
    "we can have multiple images per entity -> collect image URLs per image",
    "entity ID",
    "relation ID",
    "image URL",
    "check whether images are still missing",
    "select on image url per image in a reproducible way",
    "traverse relations in order of preference",
    "now there is an image available -> select reproducible by URL sorting",
    "did not break -> no image",
    "abstract",
    "concrete",
    "docstr-coverage: inherited",
    "tokenize",
    "pad",
    "get character embeddings",
    "pool",
    "docstr-coverage: inherited",
    ": A resolver for text encoders. By default, can use 'characterembedding'",
    ": for :class:`CharacterEmbeddingTextEncoder` or 'transformer' for",
    ": :class:`TransformerTextEncoder`.",
    "Concrete classes",
    "docstr-coverage: inherited",
    "This import doesn't need a wrapper since it's a transitive",
    "requirement of PyOBO",
    ": Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing",
    "cf. https://meta.wikimedia.org/wiki/User-Agent_policy",
    "cf. https://wikitech.wikimedia.org/wiki/Robot_policy",
    "break into smaller requests",
    "try to load cached first",
    "determine missing entries",
    "retrieve information via SPARQL",
    "save entries",
    "fill missing descriptions",
    "for mypy",
    "get labels & descriptions",
    "compose labels",
    ": A resolver for text caches",
    "Text Cache",
    "Text Encoder",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "docstr-coverage: inherited",
    "sort by decreasing degree",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "sort by decreasing page rank",
    "docstr-coverage: inherited",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A resolver for NodePiece anchor selectors",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": A list with ratios per representation in their creation order,",
    ": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`",
    ": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "Create an MLP for string aggregation",
    "note: the token representations' shape includes the number of tokens as leading dim",
    "unique hashes per representation",
    "unique hashes if we concatenate all representations together",
    "TODO: vectorization?",
    "remove self-loops",
    "add inverse edges and remove duplicates",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "TODO: expose num_anchors?",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "To prevent possible segfaults in the METIS C code, METIS expects a graph",
    "(1) without self-loops; (2) with inverse edges added; (3) with unique edges only",
    "https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18",
    "select independently per partition",
    "select adjacency part;",
    "note: the indices will automatically be in [0, ..., high - low), since they are *local* indices",
    "offset",
    "the -1 comes from the shared padding token",
    "note: permutation will be later on reverted",
    "add back 1 for the shared padding token",
    "TODO: check if perm is used correctly",
    "verify pool",
    "docstr-coverage: inherited",
    "choose first num_tokens",
    "TODO: vectorization?",
    ": A resolver for NodePiece tokenizers",
    "heuristic",
    "heuristic",
    "calculate configuration digest",
    "create anchor selection instance",
    "select anchors",
    "anchor search (=anchor assignment?)",
    "assign anchors",
    "save",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "ensure parent directory exists",
    "save via torch.save",
    "docstr-coverage: inherited",
    "TODO: since we save a contiguous array of (num_entities, num_anchors),",
    "it would be more efficient to not convert to a mapping, but directly select from the tensor",
    ": A resolver for NodePiece precomputed tokenizer loaders",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "Data containers",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "this array contains the indices of the k closest anchors nodes, but without guarantee that they are sorted",
    "now we want to sort these top-k entries, (O(k log k)) (and only those)",
    "docstr-coverage: inherited",
    "convert to adjacency matrix",
    "convert to scipy sparse csr",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "TODO: padding for unreachable?",
    "docstr-coverage: inherited",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "anchor nodes have themselves as a starting found anchor",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output that track the distance to each found anchor",
    "dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)",
    "initial anchors are 0-hop away from themselves",
    "propagate one hop",
    "TODO the float() trick for GPU result stability until the torch_sparse issue is resolved",
    "https://github.com/rusty1s/pytorch_sparse/issues/243",
    "convergence check",
    "newly reached is a mask that points to newly discovered anchors at this particular step",
    "implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)",
    "in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "update the value in the pool by the current hop value (we start from 0, so +1 be default)",
    "stop once we have enough",
    "sort the pool by nearest to farthest anchors",
    "values with distance 255 (or max for unsigned int8 type) are padding tokens",
    "since the output is sorted, no need for random sampling, we just take top-k nearest",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "select k anchors with largest ppr, shape: (batch_size, k)",
    "prepare adjacency matrix only once",
    "prepare result",
    "progress bar?",
    "batch-wise computation of PPR",
    "run page-rank calculation, shape: (batch_size, n)",
    "select PPR values for the anchors, shape: (batch_size, num_anchors)",
    ": A resolver for NodePiece anchor searchers",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "should be exactly 1.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2",
    "use V[x] = E[x^2] - E[x]^2",
    "group by same weight -> compute H_w(n) for multiple n at once",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective",
    "cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: should we return the sum of weights?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    ": whether the metric supports weights",
    ": whether there is a closed-form solution of the expectation",
    ": whether there is a closed-form solution of the variance",
    "normalize weights",
    "calculate weighted harmonic mean",
    "calculate cdf",
    "determine value at p=0.5",
    "special case for exactly 0.5",
    "see also: https://cran.r-project.org/web/packages/metrica/vignettes/available_metrics_classification.html",
    "todo: do we need numpy support?",
    "TODO: re-consider threshold",
    "noqa:DAR202",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: can we directly include sklearn's docstring here?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "noqa: DAR202",
    "todo: it would make sense to have a separate evaluator which constructs the confusion matrix only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "todo: https://en.wikipedia.org/wiki/Diagnostic_odds_ratio#Confidence_interval",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "todo: improve doc",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A resolver for classification metrics",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "todo: read from config instead",
    "Score with original triples",
    "Score with inverse triples",
    "noqa:DAR101",
    "noqa:DAR401",
    "Create directory in which all experimental artifacts are saved",
    "noqa:DAR101",
    "clip for node piece configurations",
    "\"pykeen experiments reproduce\" expects \"model reference dataset\"",
    "TODO: take care that triples aren't removed that are the only ones with any given entity",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "ensure that each entity & relation occurs at least once",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "save literal-to-id mapping",
    "save numeric literals",
    "load literal-to-id",
    "load literals",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    ": the number of unique entities",
    ": the number of relations (maybe including \"artificial\" inverse relations)",
    ": whether to create inverse triples",
    ": the number of real relations, i.e., without artificial inverses",
    "ensure torch.Tensor",
    "input validation",
    "always store as torch.long, i.e., torch's default integer dtype",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "docstr-coverage: inherited",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "note: num_relations will be doubled again when instantiating with create_inverse_triples=True",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "docstr-coverage: inherited",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "Generate a word cloud image",
    "docstr-coverage: inherited",
    "vectorized label lookup",
    "Re-order columns",
    "docstr-coverage: inherited",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "ID-based triples",
    "labeled triples",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "triples factory",
    "all keyword-based options have been none",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "only labeled triples are remaining",
    "Cleaners",
    "Splitters",
    "Utils",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "docstr-coverage: inherited",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "TODO: this could easily be extended to select a batch of triples",
    "-> speeds up the process at the cost of slightly larger movements",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "docstr-coverage: inherited",
    ": A resolver for triple cleaners",
    "docstr-coverage: inherited",
    "Make sure that the first element has all the right stuff in it",
    "docstr-coverage: inherited",
    ": A resolver for triple splitters",
    "backwards compatibility",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "noqa:DAR202",
    "noqa:DAR401",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "docstr-coverage: inherited",
    "convert to csr for fast row slicing",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "note: this seems to be a pretty unsafe method to derive __init__ kwargs...",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "the exit stack ensure that we clean up temporary files when an error occurs",
    "When using early stopping models have to be saved separately at the best epoch, since the training",
    "loop will due to the patience continue to train after the best epoch and thus alter the model",
    "note: NamedTemporaryFile does not seem to work",
    "Create a path",
    "Ensure the release of memory",
    "Clear optimizer",
    "Accumulate loss over epoch",
    "Flag to check when to quit the size probing",
    "apply callbacks before starting with batch",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "For testing purposes we're only interested in processing one batch",
    "note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest",
    "in practice, this should have a minor effect, since typically batch_size << num_instances",
    "TODO: is this necessary?",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "-> the temporay file has to be created outside, which we assert here",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "optimizer callbacks",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "docstr-coverage: inherited",
    "disable automatic batching",
    "docstr-coverage: inherited",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Slicing is not possible for sLCWA",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "lazy init",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    "TODO: we may want to separate TrainingCallback from pre-step callbacks in the future",
    "docstr-coverage: inherited",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "note: we want to run this step during size probing to cleanup any remaining grads",
    "docstr-coverage: inherited",
    "pre-step callbacks",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "note: we want to apply this during size probing to properly account for the memory necessary for e.g.,",
    "regularization",
    "docstr-coverage: inherited",
    "do not share optimal parameters across different training loops",
    "todo: create dataset only once",
    "no sub-batching (for evaluation, we can just reduce batch size without any effect)",
    "this is handled by the AMO wrapper",
    "no backward passes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "set to evaluation mode",
    "determine maximum batch size",
    "TODO: this should be num_instances rather than num_triples",
    "note: slicing is only effective for LCWA training",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "use 1-based epochs",
    "save checkpoint",
    "None corresponds to no clean-up",
    "add newly saved checkpoint to the store",
    "delete checkpoints which we do not want to keep",
    ": A resolver for training callbacks",
    "",
    ": A resolver for training loops",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Split batch components",
    "Send batch to device",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "unpack",
    "Send batch to device",
    "head prediction",
    "TODO: exploit sparsity",
    "note: this is different to what we do for LCWA, where we collect *all* training entities",
    "for which the combination is true",
    "tail prediction",
    "TODO: exploit sparsity",
    "regularization",
    "docstr-coverage: inherited",
    "TODO?",
    "To make MyPy happy",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    ": The path to the weights of the best model",
    ": Whether to delete the file with the best model weights after termination",
    ": note: the weights will be re-loaded into the model before",
    ": Whether to use a tqdm progress bar for evaluation",
    ": Keyword arguments for the tqdm progress bar",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "for mypy",
    "Evaluate",
    "Only perform time-consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "Utils",
    ": A resolver for stoppers",
    "dataset",
    "model",
    "stored outside of the training loop / optimizer to give access to auto-tuning from Lightning",
    "optimizer",
    "TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA",
    "docstr-coverage: inherited",
    "call post_parameter_update",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "TODO:",
    "shuffle=shuffle,",
    "drop_last=drop_last,",
    "sampler=sampler,",
    "shuffle=shuffle,",
    "disable automatic batching in data loader",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    ": A resolver for PyTorch Lightning training modules",
    "note: since this file is executed via __main__, its module name is replaced by __name__",
    "hence, the two classes' fully qualified names start with \"_\" and are considered private",
    "cf. https://github.com/cthoyt/class-resolver/issues/39",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "mixed precision training",
    "docstr-coverage: inherited",
    "side?.metric",
    "individual side",
    "Because the order of the values of a dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "combined",
    "docstr-coverage: inherited",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    "TODO: unused?",
    ": the scores of the true choice, shape: (*bs), dtype: float",
    ": the number of scores which were larger than the true score, shape: (*bs), dtype: long",
    ": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long",
    ": the total number of compared scores, shape: (*bs), dtype: long",
    "TODO: maybe move into separate module?",
    "actual type: nested dictionary with string keys",
    "assert isinstance(one_key, NamedTuple)",
    "TODO: should we enforce this?",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Prepare for result filtering",
    "Ensure evaluation mode",
    "Send model & tensors to device",
    "no batch size -> automatic memory optimization",
    "no slice size -> automatic memory optimization",
    "Show progressbar",
    "note: we provide the *maximum* batch and slice size here; it is reduced if necessary",
    "kwargs",
    "if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)",
    "otherwise, i.e., without inverse triples, we also need score_h",
    "if relations are to be predicted, we need to slice score_r",
    "raise an error, if any of the required methods cannot slice",
    "we ignore keys which clearly do not have an effect on the memory consumptions",
    "we ignore batch_size and slice_size as those are optimized",
    "we use mapped_triples' shape instead",
    "we want to separate optimize for each evaluator instance",
    "todo: maybe we want to have some more keys outside of kwargs for hashing / have more visibility about",
    "what is passed around",
    "clear evaluator and reset progress bar (necessary for size-probing / evaluation fallback)",
    "batch-wise processing",
    "the relation_filter can be re-used (per batch) when we evaluate head *and* tail predictions",
    "(which is the standard setting), cf. create_sparse_positive_filter_",
    "update progress bar with actual batch size",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optionally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "avoid cyclic imports",
    "normalize keys",
    "TODO: find a better way to handle this",
    "Evaluation loops",
    "Evaluation datasets",
    "batch",
    "tqdm",
    "data loader",
    "set upper limit of batch size for automatic memory optimization",
    "set model to evaluation mode",
    "delegate to AMO wrapper",
    "The key-id for each triple, shape: (num_triples,)",
    ": the number of targets for each key, shape: (num_unique_keys + 1,)",
    ": the concatenation of unique targets for each key (use bounds to select appropriate sub-array)",
    "input verification",
    "group key = everything except the prediction target",
    "initialize data structure",
    "group by key",
    "convert lists to arrays",
    "instantiate",
    "return indices corresponding to the `item`-th triple",
    "input normalization",
    "prepare filter indices if required",
    "sorted by target -> most of the batches only have a single target",
    "group by target",
    "stack groups into a single tensor",
    "avoid cyclic imports",
    "TODO: it would be better to allow separate batch sizes for entity/relation prediction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is",
    "not shuffled, and contains evaluation ranking tasks sorted by target",
    "TODO: in theory, we could make a single score calculation for e.g.,",
    "{(h, r, t1), (h, r, t1), ..., (h, r, tk)}",
    "predict scores for all candidates",
    "filter scores",
    "extract true scores",
    "replace by nan",
    "rewrite true scores",
    "create dense positive masks",
    "TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around",
    "delegate processing of scores to the evaluator",
    ": A resolver for evaluators",
    ": A resolver for metric results",
    "docstr-coverage: inherited",
    "delay declaration",
    "note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation",
    "metric. we do want to support user-selected metrics on arbitrary datasets instead",
    "this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}",
    "check targets",
    "filter supported metrics",
    "prepare input format, cf. `evaluator.expected_input``",
    "y_pred_pos: shape: (num_edge,)",
    "y_pred_neg: shape: (num_edge, num_nodes_neg)",
    "move tensor to device",
    "iterate over prediction targets",
    "cf. https://github.com/snap-stanford/ogb/pull/357",
    "combine to input dictionary",
    "delegate to OGB evaluator",
    "post-processing",
    "normalize name",
    "OGB does not aggregate values across triples",
    "todo: maybe we can merge this code with the AMO code of the base evaluator?",
    "pre-allocate",
    "TODO: maybe we want to collect scores on CPU / add an option?",
    "iterate over batches",
    "combine ids, shape: (batch_size, num_negatives + 1)",
    "get scores, shape: (batch_size, num_negatives + 1)",
    "store positive and negative scores",
    "flatten dictionaries",
    "individual side",
    "combined",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "ensure that rank-opt <= rank-pess",
    "assert that rank-real = (opt + pess)/2",
    "fixme: the annotation of ClassResolver.__iter__ seems to be broken (X instead of Type[X])",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "docstr-coverage: inherited",
    "TODO: do not require to compute all scores beforehand",
    "cf. Model.score_t(ts=...)",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "combine key batches",
    "calculate key frequency",
    "weight = inverse frequency",
    "broadcast to samples",
    "docstr-coverage: inherited",
    "store keys for calculating macro weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute macro weights",
    "note: we wrap the array into a list to be able to re-use _iter_ranks",
    "calculate weighted metrics",
    "Clear buffers",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "docstr-coverage: inherited",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "TODO: support \"broadcasting\" representation regularizers?",
    "e.g. re-use the same regularizer for everything; or",
    "pass a dictionary with keys \"entity\"/\"relation\";",
    "values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "note, triples_factory is required instead of just using self.num_entities",
    "and self.num_relations for the inductive case when this is different",
    "instantiate regularizer",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "batch normalization modules use batch statistics in training mode",
    "-> different batch divisions lead to different results",
    "docstr-coverage: inherited",
    "normalize before checking",
    "slice early to allow lazy computation of target representations",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "normalize before checking",
    "slice early to allow lazy computation of target representations",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "normalize before checking",
    "slice early to allow lazy computation of target representations",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "normalization",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": the number of entities",
    ": the number of relations",
    ": whether to use inverse relations",
    ": utility for generating inverse relations",
    ": When predict_with_sigmoid is set to True, the sigmoid function is",
    ": applied to the logits during evaluation and also for predictions",
    ": after training, but has no effect on the training.",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Meta Models",
    "Utils",
    ": A resolver for knowledge graph embedding models",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "always create representations for normal and inverse relations and padding",
    "note: we need to share the aggregation across representations, since the aggregation may have",
    "trainable parameters",
    "note: we cannot ensure the mapping also matches...",
    "get relation representations",
    "get combination",
    "get token representations",
    "relation representations are shared",
    "share combination weights",
    ": a mapping from inductive mode to corresponding entity representations",
    ": note: there may be duplicate values, if entity representations are shared between validation and testing",
    "inductive factories",
    "entity representation kwargs may contain a triples factory, which needs to be replaced",
    "entity_representations_kwargs.pop(\"triples_factory\", None)",
    "note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere",
    "shared",
    "non-shared",
    "note: \"training\" is an attribute of nn.Module -> need to rename to avoid name collision",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the indexed filter triples, i.e., sparse masks",
    "avoid cyclic imports",
    "create base model",
    "assign *after* nn.Module.__init__",
    "save constants",
    "index triples",
    "initialize base model's parameters",
    "get masks, shape: (batch_size, num_entities/num_relations)",
    "combine masks",
    "note: * is an elementwise and, and + and elementwise or",
    "get non-zero entries",
    "set scores for fill value for every non-occuring entry",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "NodePiece",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "entity embedding",
    "ConvE uses one bias for each entity",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the default loss function is the self-adversarial negative sampling loss",
    ": The default parameters for the default loss function class",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "note: DistMult only regularizes the relation embeddings;",
    "entity embeddings are hard constrained instead",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "note: this parameter is not named \"entity_regularizer\" for compatability with the",
    "regularizer-specific HPO code",
    "translation vector in hyperplane",
    "normal vector of hyperplane",
    "normalise the normal vectors to unit l2 length",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "thus, we need to use a weight regularizer instead of having an Embedding regularizer,",
    "which only regularizes the weights used in a batch",
    "note: the following is already the default",
    "default_regularizer=self.regularizer_default,",
    "default_regularizer_kwargs=self.regularizer_default_kwargs,",
    ": The default strategy for optimizing the model's hyper-parameters",
    "interaction function kwargs",
    "entity embedding",
    "relation embedding",
    "relation projection",
    "TODO: Initialize from TransE",
    "relation embedding",
    "relation projection",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "TODO: what about using the default regularizer?",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "the individual combination for real/complex parts",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "no activation",
    ": the interaction class (for generating the overview table)",
    "added by ERModel",
    "max_id=triples_factory.num_entities,",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": The WANDB run",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Base classes",
    "Concrete classes",
    "Utilities",
    ": A resolver for result trackers",
    "always add a Python result tracker for storing the configuration",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "docstr-coverage: inherited",
    ": The column names",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "store set of triples",
    "docstr-coverage: inherited",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    ": A resolver for mapping filterers",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "docstr-coverage: inherited",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "Utils",
    ": A resolver for negative samplers",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "docstr-coverage: inherited",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "docstr-coverage: inherited",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "file names for storing results",
    "TODO: rename param?",
    "always save results as json file",
    "save other components only if requested (which they are, by default)",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train",
    "TODO: allow empty validation / testing",
    "evaluation restriction to a subset of entities/relations",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "8. Evaluation",
    "7. Training (ronaldo style)",
    "Misc",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Misc",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "Start tracking",
    "cf. also https://github.com/pykeen/pykeen/issues/1071",
    "TODO: use a class-resolver?",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters,",
    ": based on :class:`torch.optim.lr_scheduler.LRScheduler`",
    "note: for some reason, mypy does not properly recognize the tuple[T1, T2, T3] notation,",
    "but rather uses tuple[T1 | T2 | T3, ...]",
    "dataset",
    "create inverse triples",
    "models, losses",
    "regularizers",
    "optimizers, training loops",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "paths need to be encoded as strings to make them JSON-serializable",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "convert class to string to use caching",
    "Assume it's a file path",
    "note: we only need to set the create_inverse_triples in the training factory.",
    "normalize dataset kwargs",
    "enable passing force option via dataset_kwargs",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "Type annotation for split types",
    "type variables for dictionaries of preprocessed data loaded through torch.load",
    ": The name of the dataset to download",
    "docstr-coverage: inherited",
    "label mapping is in dataset.root/mapping",
    "docstr-coverage: inherited",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "dtype: numpy.int64, shape: (m,)",
    "dtype: numpy.int64, shape: (n, k)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "noqa: D102",
    "docstr-coverage: inherited",
    ": the node types",
    "shape: (n,)",
    "dtype: numpy.int64, shape: (n, k)",
    "disease: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).",
    "drug: STITCH ID (http://stitch.embl.de/).",
    "function: Gene Ontology ID (http://geneontology.org/).",
    "protein: Proteins: Entrez Gene ID (https://www.genenames.org/).",
    "side effects: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).",
    "todo(@cthoyt): proper prefixing?",
    "docstr-coverage: inherited",
    "entity mappings are separate for each node type -> combine",
    "convert entity_name to categorical for fast joins",
    "we need the entity dataframe for fast re-mapping later on",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compose temporary df",
    "add extra column with old index to revert sort order change by merge",
    "convert to categorical dtype",
    "join with entity mapping",
    "revert change in order",
    "select global ID",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "TODO: maybe merge into analyze / make sub-command",
    "only save full data",
    "Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)",
    "Plot: difference between mean head and tail degree",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "Normalize path",
    "Base classes",
    "Utilities",
    "note: this needs `O(old_max_id)` memory.",
    "note: this is quite similar to pykeen.triples.triples_factory._map_triples_elements_to_ids",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    "early termination for simple case",
    "restrict triples factories (without modifying the entity to id mapping)",
    "collapse entity and relation ids",
    "help mypy",
    "update factories",
    "also update testing and validation",
    "update metadata",
    "note:",
    "- we convert to list to make sure that the metadata is JSON-serializable",
    "- we sort because the order does not matter for the functionality of this method",
    "compose restricted dataset",
    "docstr-coverage: inherited",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "TODO: use class-resolver normalize?",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    ": A resolver for datasets",
    "ZENODO_URL = \"https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip\"",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "Base class",
    "Mid-level classes",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "generate subfolders 'training' and  'inference'",
    "TODO: use class-resolver normalize?",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "Base class",
    "Mid-level classes",
    "Datasets",
    ": A resolver for inductive datasets",
    "graph pairs",
    "graph sizes",
    "graph versions",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "ensure zip file is present",
    "save relative paths beforehand so they are present for loading",
    "delegate to super class",
    "docstr-coverage: inherited",
    "left side has files ending with 1, right side with 2",
    "docstr-coverage: inherited",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "input validation",
    "store *before* calling super to have it available when loading the graphs",
    "ensure zip file is present",
    "shared directory for multiple datasets.",
    "docstr-coverage: inherited",
    "create triples factory",
    "docstr-coverage: inherited",
    "load mappings for both sides",
    "load triple alignments",
    "extract entity alignments",
    "(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2",
    "TODO: support ID-only graphs",
    "load both graphs",
    "load alignment",
    "drop duplicates",
    "combine",
    "store for repr",
    "split",
    "create inverse triples only for training",
    "docstr-coverage: inherited",
    "base",
    "concrete",
    "Abstract class",
    "Concrete classes",
    "Data Structures",
    "a buffer for the triples",
    "the offsets",
    "normalization",
    "append shifted mapped triples",
    "update offsets",
    "merge labels with same ID",
    "for mypy",
    "reconstruct label-to-id",
    "optional",
    "merge entity mapping",
    "merge relation mapping",
    "convert labels to IDs",
    "map labels, using -1 as fill-value for invalid labels",
    "we cannot drop them here, since the two columns need to stay aligned",
    "filter alignment",
    "map alignment from old IDs to new IDs",
    "determine swapping partner",
    "only keep triples where we have a swapping partner",
    "replace by swapping partner",
    ": the merged id-based triples, shape: (n, 3)",
    ": the updated alignment, shape: (2, m)",
    ": additional keyword-based parameters for adjusting label-to-id mappings",
    "concatenate triples",
    "filter alignment and translate to IDs",
    "process",
    "TODO: restrict to only using training alignments?",
    "merge mappings",
    "TODO: unreachable code",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "add swap triples",
    "e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)",
    "create dense entity remapping for swap",
    "add swapped triples",
    "swap head",
    "swap tail",
    ": the name of the additional alignment relation",
    "docstr-coverage: inherited",
    "add alignment triples with extra relation",
    "docstr-coverage: inherited",
    "determine connected components regarding the same-as relation (i.e., applies transitivity)",
    "apply id mapping",
    "ensure consecutive IDs",
    "only use training alignments?",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "a fixed checkpoint_name leads avoid collision across trials",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "noqa: DAR101",
    ": The checkpoint frequency",
    ": the result tracker which receives updates on metrics",
    ": since the same tracker instance needs to receive results from the training loop, we do require a pre-instantiated",
    ": one rather than offering to provide hints, too",
    ": the metric selection",
    "note: internal detail",
    ": a resolver for checkpoint schedules",
    "determine when checkpoints are written",
    "simulate cleanup",
    "TODO: for some reason, this field is missing in the documentation",
    ": the normalized metric name (as seen by the result tracker)",
    ": the metric prefix; if None, do not check prefix",
    ": whether to maximize or minimize the metric",
    "docstr-coverage: inherited",
    "prefix filter",
    "metric filter",
    ": the number of checkpoints to keep",
    "convert to set for better lookup speed",
    "the set operation should be a nop of sets",
    ": the result tracker which receives updates on metrics",
    ": since the same tracker instance needs to receive results from the training loop, we do require a pre-instantiated",
    ": one rather than offering to provide hints, too",
    ": the metric selection",
    "note: internal detail",
    ": a resolver for checkpoint keepers",
    "TODO: without label to id mapping a model might be pretty use-less",
    "TODO: it would be nice to get a configuration to re-construct the model",
    "save model's weights to a file",
    "load weights again",
    "update the model",
    "%%",
    "%% [markdown]",
    "## Training a model with PyKEEN",
    "%%",
    "this will log a metric with name \"validation.loss\" to the configured result tracker",
    "%% [markdown]",
    "## Evaluation with seaborn",
    "%%"
  ],
  "v1.10.2": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "See discussion for adding huggingface intersphinx docs at",
    "https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776",
    "autodoc_member_order = 'bysource'",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "ignore abstract classes",
    "verify that all classes have the hpo_default dictionary",
    "verify that we can bind the keys to the __init__'s signature",
    "note: this is only of limited use since many have **kwargs which",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "check if within 0.5 std of observed",
    "test error is raised",
    "there is an extra test for this case",
    "docstr-coverage: inherited",
    "same size tensors",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Tests that exception will be thrown when more than or less than two tensors are passed",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "ensure each node participates in at least one edge",
    "check type and shape",
    "number of colors is monotonically increasing",
    "ensure each node participates in at least one edge",
    "normalize",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    "negative number",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "See https://github.com/pykeen/pykeen/pull/883",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising",
    "wrong ndim",
    "wrong last dim",
    "wrong dtype: float",
    "wrong dtype: complex",
    "correct",
    ">>> positional argument",
    "mapped_triples",
    "triples factory",
    "labeled triples + factory",
    "single labeled triple",
    "multiple labeled triples as list",
    "multiple labeled triples as array",
    ">>> keyword only",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "fixme: find reason / enforce single-thread",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "simulate creating a new triples factory with shared set of relations by shuffling",
    "quaternion have four components",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "cf. https://github.com/pykeen/pykeen/issues/1118",
    "save a reference to the old init *before* mocking",
    "run a small pipline",
    "use sampled training loop ...",
    "... without explicitly selecting a negative sampler ...",
    "... but providing custom kwargs",
    "other parameters for fast test",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    "check for invalid keys",
    "check that each parameter without a default occurs",
    "try to instantiate loss for some configurations in the HPO search space",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    "the relative tolerance for checking close results, cf. torch.allclose",
    "the absolute tolerance for checking close results, cf. torch.allclose",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "allclose checks: | input - other | < atol + rtol * |other|",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "verify that the regularizer is stored for both, entity and relation representations",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "regularization term should be zero",
    "updated should be set to false",
    "call method",
    "generate random tensors",
    "generate inputs",
    "call update",
    "check shape",
    "check result",
    "generate single random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "update term",
    "check that the expected term is returned",
    "check that the regularizer is now reset",
    "create another instance with apply_only_once enabled",
    "test initial state",
    "after first update, should change the term",
    "after second update, no change should happen",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "TODO: look into score_r for inverse relations",
    "clear buffers for message passing models",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    "this implicitly tests extra_repr / iter_extra_repr",
    "select random indices",
    "forward pass with full graph",
    "forward pass with restricted graph",
    "verify the results are similar",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": the input dimension",
    ": the output dimension",
    ": the number of entities",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    ": the interaction to use for testing a model",
    "initializers *may* work in-place => clone",
    "actual number may be different...",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    "inferred from triples factory",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "generate random weights such that sum = n",
    "for sanity checking: give the largest weight to best rank => should improve",
    "generate two versions",
    "1. repeat each rank/candidate pair a random number of times",
    "2. do not repeat, but assign a corresponding weight",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "generate random triples factories",
    "generate random alignment",
    "add label information if necessary",
    "prepare alignment data frame",
    "call",
    "check",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "verify that the input is valid",
    "combine",
    "verify shape",
    "to be initialized in subclass",
    "no column has been removed",
    "all old columns are unmodified",
    "new columns are boolean",
    "no columns have been added",
    "check subset relation",
    "-*- coding: utf-8 -*-",
    "TODO: this could be shared with the model tests",
    "fixme: CompGCN leads to an autograd runtime error...",
    "models.CompGCN: dict(embedding_dim=EMBEDDING_DIM),",
    "FixedModel: dict(embedding_dim=EMBEDDING_DIM),",
    "test combinations of models with training loops",
    "some models require inverse relations",
    "some model require access to the training triples",
    "inductive models require an inductive mode to be set, and an inference factory to be passed",
    "fake an inference factory",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "sum over the softmax dim as weights sum up to 1",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "abstract classes",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "prediction post-processing",
    "mock prediction data frame",
    "score consumers",
    "use a small model, since operation is expensive",
    "all scores, automatic batch size",
    "top 3 scores",
    "top 3 scores, fixed batch size, head scoring",
    "all scores, relation scoring",
    "all scores, relation scoring",
    "model with inverse relations",
    "check type",
    "check shape",
    "check ID ranges",
    "mapped triples, automatic batch size selection, no factory",
    "mapped triples, fixed batch size, no factory",
    "labeled triples with factory",
    "labeled triples as list",
    "single labeled triple",
    "model with inverse relations",
    "ID-based, no factory",
    "string-based + factory",
    "mixed + factory",
    "no restriction, no factory",
    "no restriction, factory",
    "id restriction, no factory ...",
    "id restriction with factory",
    "comment: we only use id-based input, since the normalization has already been tested",
    "create model",
    "id-based head/relation/tail prediction, no restriction",
    "restriction by list of ints",
    "tail prediction",
    "try accessing each element",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "clear",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "extra-relation",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "additional",
    "verify shape",
    "verify dtype",
    "verify number of entities/relations",
    "verify offsets",
    "create old, new pairs",
    "simulate merging ids",
    "only a single pair",
    "apply",
    "every key is contained",
    "value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: we could move this part into the interaction module itself",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "Rotate (=Hamilton product in quaternion space).",
    "we calculate the scores using the hard-coded formula, instead of utilizing table + einsum",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize rotations to unit modulus",
    "check for unit modulus",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "check type",
    "check size",
    "check value range",
    "-*- coding: utf-8 -*-",
    "message_dim must be divisible by num_heads",
    "generate test data (with fixed seed for reproducibility)",
    "get result using argpartition",
    "check shape",
    "check type",
    "check value range",
    "check equality with argsort",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "-*- coding: utf-8 -*-",
    "complex tensor",
    "check value range",
    "check modulus == 1",
    "quaternion needs shape to end on 4",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "value range",
    "highest degree node has largest value",
    "Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf",
    "create triples with a dummy relation type 0",
    "0: green: 2, 3, 7, 8",
    "1: red: 1, 4, 6, 9",
    "2: blue: 0, 5",
    "the example includes the first power",
    "requires at least one complex tensor as input",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "inferred from triples factory",
    "inferred from assignment",
    "the representation module infers the max_id from the provided labels",
    "the following entity does not have an image -> will have to use backfill",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "max_id is inferred from assignment",
    "create random assignment",
    "update kwargs",
    "empty bases",
    "inconsistent base shapes",
    "invalid base id",
    "invalid local index",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "TODO this is the only place this function is used.",
    "Is there an alternative so we can remove it?",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "TODO: more tests",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "x_i ~ N(mu_i, 1)",
    "closed-form solution",
    "sampled confidence interval",
    "check that closed-form is in confidence interval of sampled",
    "positive values only",
    "positive and negative values",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "check correct num_entities",
    "check type",
    "check length",
    "check type",
    "check length",
    "check confidence positivity",
    "Check for correct class",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "-*- coding: utf-8 -*-",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "initialize",
    "update with batches",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": synonyms of this loss",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "docstr-coverage: inherited",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "0. default",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "docstr-coverage: inherited",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "docstr-coverage: inherited",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "docstr-coverage: inherited",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "docstr-coverage: inherited",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "make sure labels form a proper probability distribution",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "subtract margin from positive scores",
    "divide by temperature",
    "docstr-coverage: inherited",
    "subtract margin from positive scores",
    "normalize positive score shape",
    "divide by temperature",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "we pass *all* scores as negatives, but set the weight of positives to zero",
    "this allows keeping a dense shape",
    "docstr-coverage: inherited",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)",
    "note: this is a reduction along the softmax dim; since the weights are already normalized",
    "to sum to one, we want a sum reduction here, instead of using the self._reduction",
    "docstr-coverage: inherited",
    "Sanity check",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative loss part",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "TODO delete when deleting _normalize_dim (below)",
    "TODO delete when deleting convert_to_canonical_shape (below)",
    "TODO delete? See note in test_sim.py on its only usage",
    "upgrade to sequence",
    "broadcast",
    "normalize ids: -> ids.shape: (batch_size, num_ids)",
    "normalize batch -> batch.shape: (batch_size, 1, 3)",
    "allocate memory",
    "copy ids",
    "reshape",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "note: this is a hack, and should be fixed up-stream by making NodePiece",
    "use proper complex embeddings for rotate interaction; however, we also have representations",
    "that perform message passing, and we would need to propagate the base representation's complexity through it",
    "ensure pathlib",
    "cf. https://stackoverflow.com/a/1176023",
    "check validity",
    "path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "resolve path to make sure it is an absolute path",
    "ensure directory exists",
    "message passing: collect colors of neighbors",
    "dense colors: shape: (n, c)",
    "adj:          shape: (n, n)",
    "values need to be float, since torch.sparse.mm does not support integer dtypes",
    "size: will be correctly inferred",
    "concat with old colors",
    "hash",
    "create random indicator functions of low dimensionality",
    "collect neighbors' colors",
    "round to avoid numerical effects",
    "hash first",
    "concat with old colors",
    "re-hash",
    "only keep connectivity, but remove multiplicity",
    "note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,",
    "this is rather useless",
    "initial: degree",
    "note: we calculate this separately, since we can use a more efficient implementation for the first step",
    "hash",
    "determine small integer type for dense count array",
    "convergence check",
    "each node has a unique color",
    "the number of colors did not improve in the last iteration",
    "cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "docstr-coverage: inherited",
    "no need to compute anything",
    "docstr-coverage: inherited",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "regularizer-specific parameters",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "orthogonality soft constraint: cosine similarity at most epsilon",
    "The normalization factor to balance individual regularizers' contribution.",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "high-level",
    "Low-Level",
    "cf. https://github.com/python/mypy/issues/5374",
    ": the dataframe; has to have a column named \"score\"",
    ": an optional factory to use for labeling",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the prediction target",
    ": the other column's fixed IDs",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the ID-based triples, shape: (n, 3)",
    ": the scores",
    "3-tuple for return",
    "extract label information, if possible",
    "no restriction",
    "restriction is a tensor",
    "restriction is a sequence of integers or strings",
    "now, restriction is a sequence of integers",
    "if explicit ids have been given, and label information is available, extract list of labels",
    "exactly one of them is None",
    "create input batch",
    "note type alias annotation required,",
    "cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases",
    "batch, TODO: ids?",
    "docstr-coverage: inherited",
    "initialize buffer on device",
    "docstr-coverage: inherited",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "determine corresponding indices",
    "batch_id, score_id = divmod(top_indices, num_scores)",
    "combine to top triples",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "docstr-coverage: inherited",
    "TODO: variable targets across batches/samples?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "(?, r, t) => r.stride > t.stride",
    "(h, ?, t) => h.stride > t.stride",
    "(h, r, ?) => h.stride > r.stride",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "train model; note: needs larger number of epochs to do something useful ;-)",
    "create prediction dataset, where the head entities is from a set of European countries,",
    "and the relations are connected to tourism",
    "calculate all scores for this restricted set, and keep k=3 largest",
    "add labels",
    ": the choices for the first and second component of the input batch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "calculate batch scores onces",
    "consume by all consumers",
    "TODO: Support partial dataset",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "exactly one of them is None",
    "",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "get input & target",
    "get label-to-id mapping and prediction targets",
    "get scores",
    "note: maybe we want to expose these scores, too?",
    "create raw dataframe",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "normalize input",
    "calculate scores (with automatic memory optimization)",
    "-*- coding: utf-8 -*-",
    "determine fully qualified name",
    "shorten to main module",
    "verify that short name can be imported from the abbreviated reference",
    "get docdata and extract name & citation",
    "fallback for name: capitalized class name without base suffix",
    "extract citation information and warn about lack thereof",
    "compose reference",
    "cf. https://github.com/python/mypy/issues/5374",
    "\"Closed-Form Expectation\",",
    "\"Closed-Form Variance\",",
    "\"\u2713\" if metric.closed_expectation else \"\",",
    "\"\u2713\" if metric.closed_variance else \"\",",
    "Add HPO command",
    "Add NodePiece tokenization command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    "entity alignment sides",
    "utils",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "entity alignment",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "infer shape",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "input normalization",
    "note: the base class does not have any parameters",
    "Heuristic for default value",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: the only parameters are inside the relation representation module, which has its own reset_parameters",
    "docstr-coverage: inherited",
    "TODO: can we change the dimension order to make this contiguous?",
    "docstr-coverage: inherited",
    "normalize num blocks",
    "determine necessary padding",
    "determine block sizes",
    "(R, nb, bsi, bso)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(n, di) -> (n, nb, bsi)",
    "(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)",
    "(R, n, nb, bso) -> (R * n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "(n, R * n), (R * n, do) -> (n, do)",
    "remove padding if necessary",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(R * n, n), (n, di) -> (R * n, di)",
    "(R * n, di) -> (R, n, nb, bsi)",
    "(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)",
    "(n, nb, bso) -> (n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "remove padding if necessary",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "TODO: we could cache the stacked adjacency matrices",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "input validation",
    "has to be imported now to avoid cyclic imports",
    "has to be assigned after call to nn.Module init",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "heuristic",
    "normalize *before* repeating",
    "repeat if necessary",
    "regularize *after* repeating",
    "dropout & regularizer will appear automatically, since it is a nn.Module",
    "has to be imported here to avoid cyclic import",
    "docstr-coverage: inherited",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "docstr-coverage: inherited",
    "initialize weights in-place",
    "docstr-coverage: inherited",
    "apply constraints in-place",
    "fixme: work-around until nn.Embedding supports complex",
    "docstr-coverage: inherited",
    "fixme: work-around until nn.Embedding supports complex",
    "verify that contiguity is preserved",
    "create low-rank approximation object",
    "get base representations, shape: (n, *ds)",
    "calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))",
    "overwrite bases and weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "TODO: might not be true for all compositions",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "docstr-coverage: inherited",
    "check max_id",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "docstr-coverage: inherited",
    ": the base representations",
    ": the combination module",
    "input normalization",
    "has to be imported here to avoid cyclic import",
    "create base representations",
    "verify same ID range",
    "note: we could also relax the requiremen, and set max_id = min(max_ids)",
    "shape inference",
    "assign base representations *after* super init",
    "docstr-coverage: inherited",
    "delegate to super class",
    "docstr-coverage: inherited",
    "Generate graph dataset from the Monarch Disease Ontology (MONDO)",
    ": the assignment from global ID to (representation, local id), shape: (max_id, 2)",
    "import here to avoid cyclic import",
    "instantiate base representations if necessary",
    "there needs to be at least one base",
    "while possible, this might be unintended",
    "extract shape",
    "check for invalid base ids",
    "check for invalid local indices",
    "assign modules / buffers *after* super init",
    "docstr-coverage: inherited",
    "flatten assignment to ease construction of inverse indices",
    "we group indices by the representation which provides them",
    "thus, we need an inverse to restore the correct order",
    "get representations",
    "update inverse indices",
    "invert flattening",
    "import here to avoid cyclic import",
    "comment: not all representations support passing a shape parameter",
    "create assignment",
    "base",
    "other",
    "import here to avoid cyclic import",
    "infer shape",
    "infer max_id",
    "docstr-coverage: inherited",
    "TODO: can be a combined representations, with appropriate tensor-train combination",
    ": shape: (max_id, num_cores)",
    ": the bases, length: num_cores, with compatible shapes",
    "check shape",
    "check value range",
    "do not increase counter i, since the dimension is shared with the following term",
    "i += 1",
    "ids //= m_i",
    "import here to avoid cyclic import",
    "normalize ranks",
    "determine M_k, N_k",
    "TODO: allow to pass them from outside?",
    "normalize assignment",
    "determine shapes and einsum equation",
    "create base representations",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "abstract",
    "concrete classes",
    "default flow",
    ": the message passing layers",
    ": the flow direction of messages across layers",
    ": the edge index, shape: (2, num_edges)",
    "fail if dependencies are missing",
    "avoid cyclic import",
    "the base representations, e.g., entity embeddings or features",
    "verify max_id",
    "verify shape",
    "assign sub-module *after* super call",
    "initialize layers",
    "normalize activation",
    "check consistency",
    "buffer edge index for message passing",
    "TODO: inductiveness; we need to",
    "* replace edge_index",
    "* replace base representations",
    "* keep layers & activations",
    "docstr-coverage: inherited",
    "we can restrict the message passing to the k-hop neighborhood of the desired indices;",
    "this does only make sense if we do not request *all* indices",
    "k_hop_subgraph returns:",
    "(1) the nodes involved in the subgraph",
    "(2) the filtered edge_index connectivity",
    "(3) the mapping from node indices in node_idx to their new location, and",
    "(4) the edge mask indicating which edges were preserved",
    "we only need the base representations for the neighbor indices",
    "get *all* base representations",
    "use *all* edges",
    "perform message passing",
    "select desired indices",
    "docstr-coverage: inherited",
    ": the edge type, shape: (num_edges,)",
    "register an additional buffer for the categorical edge type",
    "docstr-coverage: inherited",
    ": the relation representations used to obtain initial edge features",
    "avoid cyclic import",
    "docstr-coverage: inherited",
    "get initial relation representations",
    "select edge attributes from relation representations according to relation type",
    "perform message passing",
    "apply relation transformation, if necessary",
    "-*- coding: utf-8 -*-",
    "Classes",
    "Resolver",
    "backwards compatibility",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "must be cloned if we want to do backprop",
    "the color initializer",
    "variants for the edge index",
    "additional parameters for iter_weisfeiler_lehman",
    "normalize shape",
    "get coloring",
    "make color initializer",
    "initialize color representations",
    "note: this could be a representation?",
    "init entity representations according to the color",
    "create random walk matrix",
    "stack diagonal entries of powers of rw",
    "abstract",
    "concrete",
    "docstr-coverage: inherited",
    "tokenize",
    "pad",
    "get character embeddings",
    "pool",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "backward compatibility with RGCN",
    "docstr-coverage: inherited",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    "-*- coding: utf-8 -*-",
    "Caches",
    "if the sparsity becomes too low, convert to a dense matrix",
    "note: this heuristic is based on the memory consumption,",
    "for a sparse matrix, we store 3 values per nnz (row index, column index, value)",
    "performance-wise, it likely makes sense to switch even earlier",
    "`torch.sparse.mm` can also deal with dense 2nd argument",
    "note: torch.sparse.mm only works for COO matrices;",
    "@ only works for CSR matrices",
    "convert to COO, if necessary",
    "we need to use indices here, since there may be zero diagonal entries",
    "docstr-coverage: inherited",
    ": Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing",
    "cf. https://meta.wikimedia.org/wiki/User-Agent_policy",
    "cf. https://wikitech.wikimedia.org/wiki/Robot_policy",
    "break into smaller requests",
    "try to load cached first",
    "determine missing entries",
    "retrieve information via SPARQL",
    "save entries",
    "fill missing descriptions",
    "for mypy",
    "get labels & descriptions",
    "compose labels",
    "we can have multiple images per entity -> collect image URLs per image",
    "entity ID",
    "relation ID",
    "image URL",
    "check whether images are still missing",
    "select on image url per image in a reproducible way",
    "traverse relations in order of preference",
    "now there is an image available -> select reproducible by URL sorting",
    "did not break -> no image",
    "This import doesn't need a wrapper since it's a transitive",
    "requirement of PyOBO",
    "darglint does not like",
    "raise cls(shape=shape, reference=reference)",
    "1 * ? = ?; ? * 1 = ?",
    "i**2 = j**2 = k**2 = -1",
    "i * j = k; i * k = -j",
    "j * i = -k, j * k = i",
    "k * i = j; k * j = -i",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "-*- coding: utf-8 -*-",
    "REPRESENTATION",
    "base",
    "concrete",
    "INITIALIZER",
    "INTERACTIONS",
    "Adapter classes",
    "Concrete Classes",
    "combinations",
    "-*- coding: utf-8 -*-",
    "TODO: split file into multiple smaller ones?",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different.",
    ": Otherwise, the entity_shape is used for head & tail entities",
    ": The symbolic shapes for relation representations",
    "if the interaction function's head parameter should only receive a subset of entity representations",
    "if the interaction function's tail parameter should only receive a subset of entity representations",
    ": the interaction's value range (for unrestricted input)",
    "TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2",
    "TODO: annotate properties, e.g., symmetry, and use them for testing?",
    "TODO: annotate complexity?",
    ": whether the interaction is defined on complex input",
    "TODO: cannot cover dynamic shapes, e.g., AutoSF",
    "TODO: we could change that to slicing along multiple dimensions, if necessary",
    ": The functional interaction form",
    "docstr-coverage: inherited",
    "TODO: we only allow single-tensor representations here, but could easily generalize",
    "docstr-coverage: inherited",
    "TODO: update class docstring",
    "TODO: give this a better name?",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    "vector & scalar offset",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "normalize kernel height",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The interaction model",
    "docstr-coverage: inherited",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "docstr-coverage: inherited",
    "normalize hidden_dim",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Initialize biases with zero",
    "In the original formulation,",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: update docstring",
    "TODO: give this a better name?",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "default core tensor initialization",
    "cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "normalize initializer",
    "normalize relation dimension",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "docstr-coverage: inherited",
    "instantiate here to make module easily serializable",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "there are separate biases for entities in head and tail position",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "with k=4",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "docstr-coverage: inherited",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "input normalization",
    "Core tensor",
    "docstr-coverage: inherited",
    "initialize core tensor",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "r_head, r_mid, r_tail",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "type alias for AutoSF block description",
    "head_index, relation_index, tail_index, sign",
    ": a description of the block structure",
    "convert to tuple",
    "infer the number of entity and relation representations",
    "verify coefficients",
    "dynamic entity / relation shapes",
    "docstr-coverage: inherited",
    "r_head, r_bias, r_tail",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: excused `wrapped`",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "cat into shape (..., 1, d, 3)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "shortcut for same shape",
    "split weight into head-/relation-/tail-specific sub-matrices",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace",
    "ensure constraints",
    "x_1 contraction",
    "x_2 contraction",
    "TODO: this sign is in the official code, too, but why do we need it?",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "head",
    "relation",
    "tail",
    "extension: negative (power) norm",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "input normalization",
    "instantiate separate combinations",
    "docstr-coverage: inherited",
    "split complex; repeat real",
    "separately combine real and imaginary parts",
    "combine",
    "docstr-coverage: inherited",
    "symbolic output to avoid dtype issue",
    "we only need to consider real part here",
    "the gate",
    "the combination",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "docstr-coverage: inherited",
    "sort by decreasing degree",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "sort by decreasing page rank",
    "docstr-coverage: inherited",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": A list with ratios per representation in their creation order,",
    ": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`",
    ": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "Create an MLP for string aggregation",
    "note: the token representations' shape includes the number of tokens as leading dim",
    "unique hashes per representation",
    "unique hashes if we concatenate all representations together",
    "TODO: vectorization?",
    "remove self-loops",
    "add inverse edges and remove duplicates",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "TODO: expose num_anchors?",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "To prevent possible segfaults in the METIS C code, METIS expects a graph",
    "(1) without self-loops; (2) with inverse edges added; (3) with unique edges only",
    "https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18",
    "select independently per partition",
    "select adjacency part;",
    "note: the indices will automatically be in [0, ..., high - low), since they are *local* indices",
    "offset",
    "the -1 comes from the shared padding token",
    "note: permutation will be later on reverted",
    "add back 1 for the shared padding token",
    "TODO: check if perm is used correctly",
    "verify pool",
    "docstr-coverage: inherited",
    "choose first num_tokens",
    "TODO: vectorization?",
    "heuristic",
    "heuristic",
    "calculate configuration digest",
    "create anchor selection instance",
    "select anchors",
    "anchor search (=anchor assignment?)",
    "assign anchors",
    "save",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "ensure parent directory exists",
    "save via torch.save",
    "docstr-coverage: inherited",
    "TODO: since we save a contiguous array of (num_entities, num_anchors),",
    "it would be more efficient to not convert to a mapping, but directly select from the tensor",
    "-*- coding: utf-8 -*-",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "Data containers",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "this array contains the indices of the k closest anchors nodes, but without guarantee that they are sorted",
    "now we want to sort these top-k entries, (O(k log k)) (and only those)",
    "docstr-coverage: inherited",
    "convert to adjacency matrix",
    "convert to scipy sparse csr",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "TODO: padding for unreachable?",
    "docstr-coverage: inherited",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "anchor nodes have themselves as a starting found anchor",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output that track the distance to each found anchor",
    "dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)",
    "initial anchors are 0-hop away from themselves",
    "propagate one hop",
    "TODO the float() trick for GPU result stability until the torch_sparse issue is resolved",
    "https://github.com/rusty1s/pytorch_sparse/issues/243",
    "convergence check",
    "newly reached is a mask that points to newly discovered anchors at this particular step",
    "implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)",
    "in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "update the value in the pool by the current hop value (we start from 0, so +1 be default)",
    "stop once we have enough",
    "sort the pool by nearest to farthest anchors",
    "values with distance 255 (or max for unsigned int8 type) are padding tokens",
    "since the output is sorted, no need for random sampling, we just take top-k nearest",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "select k anchors with largest ppr, shape: (batch_size, k)",
    "prepare adjacency matrix only once",
    "prepare result",
    "progress bar?",
    "batch-wise computation of PPR",
    "run page-rank calculation, shape: (batch_size, n)",
    "select PPR values for the anchors, shape: (batch_size, num_anchors)",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "should be exactly 1.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2",
    "use V[x] = E[x^2] - E[x]^2",
    "group by same weight -> compute H_w(n) for multiple n at once",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective",
    "cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: should we return the sum of weights?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "-*- coding: utf-8 -*-",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    ": whether the metric supports weights",
    ": whether there is a closed-form solution of the expectation",
    ": whether there is a closed-form solution of the variance",
    "normalize weights",
    "calculate weighted harmonic mean",
    "calculate cdf",
    "determine value at p=0.5",
    "special case for exactly 0.5",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "see also: https://cran.r-project.org/web/packages/metrica/vignettes/available_metrics_classification.html",
    "todo: do we need numpy support?",
    "TODO: re-consider threshold",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: can we directly include sklearn's docstring here?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "todo: it would make sense to have a separate evaluator which constructs the confusion matrix only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "todo: https://en.wikipedia.org/wiki/Diagnostic_odds_ratio#Confidence_interval",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "todo: improve doc",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "todo: read from config instead",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "noqa:DAR101",
    "noqa:DAR401",
    "Create directory in which all experimental artifacts are saved",
    "noqa:DAR101",
    "clip for node piece configurations",
    "\"pykeen experiments reproduce\" expects \"model reference dataset\"",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: take care that triples aren't removed that are the only ones with any given entity",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "save literal-to-id mapping",
    "save numeric literals",
    "load literal-to-id",
    "load literals",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    ": the number of unique entities",
    ": the number of relations (maybe including \"artificial\" inverse relations)",
    ": whether to create inverse triples",
    ": the number of real relations, i.e., without artificial inverses",
    "ensure torch.Tensor",
    "input validation",
    "always store as torch.long, i.e., torch's default integer dtype",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "docstr-coverage: inherited",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "note: num_relations will be doubled again when instantiating with create_inverse_triples=True",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "docstr-coverage: inherited",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "Generate a word cloud image",
    "docstr-coverage: inherited",
    "vectorized label lookup",
    "Re-order columns",
    "docstr-coverage: inherited",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "ID-based triples",
    "labeled triples",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "triples factory",
    "all keyword-based options have been none",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "only labeled triples are remaining",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "docstr-coverage: inherited",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Make sure that the first element has all the right stuff in it",
    "docstr-coverage: inherited",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "noqa:DAR202",
    "noqa:DAR401",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "docstr-coverage: inherited",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "docstr-coverage: inherited",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "the exit stack ensure that we clean up temporary files when an error occurs",
    "When using early stopping models have to be saved separately at the best epoch, since the training",
    "loop will due to the patience continue to train after the best epoch and thus alter the model",
    "note: NamedTemporaryFile does not seem to work",
    "Create a path",
    "Ensure the release of memory",
    "Clear optimizer",
    "Accumulate loss over epoch",
    "Flag to check when to quit the size probing",
    "apply callbacks before starting with batch",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "For testing purposes we're only interested in processing one batch",
    "note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest",
    "in practice, this should have a minor effect, since typically batch_size << num_instances",
    "TODO: is this necessary?",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "-> the temporay file has to be created outside, which we assert here",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "optimizer callbacks",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "disable automatic batching",
    "docstr-coverage: inherited",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "lazy init",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    "TODO: we may want to separate TrainingCallback from pre-step callbacks in the future",
    "docstr-coverage: inherited",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "note: we want to run this step during size probing to cleanup any remaining grads",
    "docstr-coverage: inherited",
    "pre-step callbacks",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "note: we want to apply this during size probing to properly account for the memory necessary for e.g.,",
    "regularization",
    "docstr-coverage: inherited",
    "do not share optimal parameters across different training loops",
    "todo: create dataset only once",
    "no sub-batching (for evaluation, we can just reduce batch size without any effect)",
    "this is handled by the AMO wrapper",
    "no backward passes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "set to evaluation mode",
    "determine maximum batch size",
    "try to avoid OOM kills on cpu for large datasets",
    "TODO: this should be num_instances rather than num_triples; also for cpu, we may want to reduce this",
    "note: slicing is only effective for LCWA training",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Split batch components",
    "Send batch to device",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "unpack",
    "Send batch to device",
    "head prediction",
    "TODO: exploit sparsity",
    "note: this is different to what we do for LCWA, where we collect *all* training entities",
    "for which the combination is true",
    "tail prediction",
    "TODO: exploit sparsity",
    "regularization",
    "docstr-coverage: inherited",
    "TODO?",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    ": the path to the weights of the best model",
    ": whether to delete the file with the best model weights after termination",
    ": note: the weights will be re-loaded into the model before",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "for mypy",
    "Evaluate",
    "Only perform time-consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "dataset",
    "model",
    "stored outside of the training loop / optimizer to give access to auto-tuning from Lightning",
    "optimizer",
    "TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA",
    "docstr-coverage: inherited",
    "call post_parameter_update",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "TODO:",
    "shuffle=shuffle,",
    "drop_last=drop_last,",
    "sampler=sampler,",
    "shuffle=shuffle,",
    "disable automatic batching in data loader",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "note: since this file is executed via __main__, its module name is replaced by __name__",
    "hence, the two classes' fully qualified names start with \"_\" and are considered private",
    "cf. https://github.com/cthoyt/class-resolver/issues/39",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "mixed precision training",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "side?.metric",
    "individual side",
    "Because the order of the values of a dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "combined",
    "docstr-coverage: inherited",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    ": the scores of the true choice, shape: (*bs), dtype: float",
    ": the number of scores which were larger than the true score, shape: (*bs), dtype: long",
    ": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long",
    ": the total number of compared scores, shape: (*bs), dtype: long",
    "-*- coding: utf-8 -*-",
    "TODO: maybe move into separate module?",
    "actual type: nested dictionary with string keys",
    "assert isinstance(one_key, NamedTuple)",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Prepare for result filtering",
    "Ensure evaluation mode",
    "Send model & tensors to device",
    "no batch size -> automatic memory optimization",
    "no slice size -> automatic memory optimization",
    "Show progressbar",
    "note: we provide the *maximum* batch and slice size here; it is reduced if necessary",
    "kwargs",
    "if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)",
    "otherwise, i.e., without inverse triples, we also need score_h",
    "if relations are to be predicted, we need to slice score_r",
    "raise an error, if any of the required methods cannot slice",
    "todo: maybe we want to have some more keys outside of kwargs for hashing / have more visibility about",
    "what is passed around",
    "clear evaluator and reset progress bar (necessary for size-probing / evaluation fallback)",
    "batch-wise processing",
    "the relation_filter can be re-used (per batch) when we evaluate head *and* tail predictions",
    "(which is the standard setting), cf. create_sparse_positive_filter_",
    "update progress bar with actual batch size",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optionally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "avoid cyclic imports",
    "normalize keys",
    "TODO: find a better way to handle this",
    "-*- coding: utf-8 -*-",
    "Evaluation loops",
    "Evaluation datasets",
    "batch",
    "tqdm",
    "data loader",
    "set upper limit of batch size for automatic memory optimization",
    "set model to evaluation mode",
    "delegate to AMO wrapper",
    "The key-id for each triple, shape: (num_triples,)",
    ": the number of targets for each key, shape: (num_unique_keys + 1,)",
    ": the concatenation of unique targets for each key (use bounds to select appropriate sub-array)",
    "input verification",
    "group key = everything except the prediction target",
    "initialize data structure",
    "group by key",
    "convert lists to arrays",
    "instantiate",
    "return indices corresponding to the `item`-th triple",
    "input normalization",
    "prepare filter indices if required",
    "sorted by target -> most of the batches only have a single target",
    "group by target",
    "stack groups into a single tensor",
    "avoid cyclic imports",
    "TODO: it would be better to allow separate batch sizes for entity/relation prediction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is",
    "not shuffled, and contains evaluation ranking tasks sorted by target",
    "TODO: in theory, we could make a single score calculation for e.g.,",
    "{(h, r, t1), (h, r, t1), ..., (h, r, tk)}",
    "predict scores for all candidates",
    "filter scores",
    "extract true scores",
    "replace by nan",
    "rewrite true scores",
    "create dense positive masks",
    "TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around",
    "delegate processing of scores to the evaluator",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "delay declaration",
    "note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation",
    "metric. we do want to support user-selected metrics on arbitrary datasets instead",
    "this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}",
    "check targets",
    "filter supported metrics",
    "prepare input format, cf. `evaluator.expected_input``",
    "y_pred_pos: shape: (num_edge,)",
    "y_pred_neg: shape: (num_edge, num_nodes_neg)",
    "move tensor to device",
    "iterate over prediction targets",
    "cf. https://github.com/snap-stanford/ogb/pull/357",
    "combine to input dictionary",
    "delegate to OGB evaluator",
    "post-processing",
    "normalize name",
    "OGB does not aggregate values across triples",
    "todo: maybe we can merge this code with the AMO code of the base evaluator?",
    "pre-allocate",
    "TODO: maybe we want to collect scores on CPU / add an option?",
    "iterate over batches",
    "combine ids, shape: (batch_size, num_negatives + 1)",
    "get scores, shape: (batch_size, num_negatives + 1)",
    "store positive and negative scores",
    "-*- coding: utf-8 -*-",
    "flatten dictionaries",
    "individual side",
    "combined",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "ensure that rank-opt <= rank-pess",
    "assert that rank-real = (opt + pess)/2",
    "fixme: the annotation of ClassResolver.__iter__ seems to be broken (X instead of Type[X])",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "docstr-coverage: inherited",
    "TODO: do not require to compute all scores beforehand",
    "cf. Model.score_t(ts=...)",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "combine key batches",
    "calculate key frequency",
    "weight = inverse frequency",
    "broadcast to samples",
    "docstr-coverage: inherited",
    "store keys for calculating macro weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute macro weights",
    "note: we wrap the array into a list to be able to re-use _iter_ranks",
    "calculate weighted metrics",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "docstr-coverage: inherited",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "TODO: support \"broadcasting\" representation regularizers?",
    "e.g. re-use the same regularizer for everything; or",
    "pass a dictionary with keys \"entity\"/\"relation\";",
    "values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "note, triples_factory is required instead of just using self.num_entities",
    "and self.num_relations for the inductive case when this is different",
    "instantiate regularizer",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "batch normalization modules use batch statistics in training mode",
    "-> different batch divisions lead to different results",
    "docstr-coverage: inherited",
    "normalize before checking",
    "slice early to allow lazy computation of target representations",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "normalize before checking",
    "slice early to allow lazy computation of target representations",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "normalize before checking",
    "slice early to allow lazy computation of target representations",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": the number of entities",
    ": the number of relations",
    ": whether to use inverse relations",
    ": utility for generating inverse relations",
    ": When predict_with_sigmoid is set to True, the sigmoid function is",
    ": applied to the logits during evaluation and also for predictions",
    ": after training, but has no effect on the training.",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Meta Models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "note: we need to share the aggregation across representations, since the aggregation may have",
    "trainable parameters",
    "note: we cannot ensure the mapping also matches...",
    "get relation representations",
    "get combination",
    "get token representations",
    "relation representations are shared",
    "share combination weights",
    ": a mapping from inductive mode to corresponding entity representations",
    ": note: there may be duplicate values, if entity representations are shared between validation and testing",
    "inductive factories",
    "entity representation kwargs may contain a triples factory, which needs to be replaced",
    "entity_representations_kwargs.pop(\"triples_factory\", None)",
    "note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere",
    "shared",
    "non-shared",
    "note: \"training\" is an attribute of nn.Module -> need to rename to avoid name collision",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the indexed filter triples, i.e., sparse masks",
    "avoid cyclic imports",
    "create base model",
    "assign *after* nn.Module.__init__",
    "save constants",
    "index triples",
    "initialize base model's parameters",
    "get masks, shape: (batch_size, num_entities/num_relations)",
    "combine masks",
    "note: * is an elementwise and, and + and elementwise or",
    "get non-zero entries",
    "set scores for fill value for every non-occuring entry",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "NodePiece",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "entity embedding",
    "ConvE uses one bias for each entity",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the default loss function is the self-adversarial negative sampling loss",
    ": The default parameters for the default loss function class",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "note: DistMult only regularizes the relation embeddings;",
    "entity embeddings are hard constrained instead",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "note: this parameter is not named \"entity_regularizer\" for compatability with the",
    "regularizer-specific HPO code",
    "translation vector in hyperplane",
    "normal vector of hyperplane",
    "normalise the normal vectors to unit l2 length",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "thus, we need to use a weight regularizer instead of having an Embedding regularizer,",
    "which only regularizes the weights used in a batch",
    "note: the following is already the default",
    "default_regularizer=self.regularizer_default,",
    "default_regularizer_kwargs=self.regularizer_default_kwargs,",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "relation embedding",
    "relation projection",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "input normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "the individual combination for real/complex parts",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "no activation",
    "-*- coding: utf-8 -*-",
    ": the interaction class (for generating the overview table)",
    "added by ERModel",
    "max_id=triples_factory.num_entities,",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "docstr-coverage: inherited",
    ": The column names",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    "docstr-coverage: inherited",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "docstr-coverage: inherited",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "docstr-coverage: inherited",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "docstr-coverage: inherited",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "file names for storing results",
    "TODO: rename param?",
    "always save results as json file",
    "save other components only if requested (which they are, by default)",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train",
    "TODO: allow empty validation / testing",
    "evaluation restriction to a subset of entities/relations",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "8. Evaluation",
    "7. Training (ronaldo style)",
    "Misc",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Misc",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "Start tracking",
    "-*- coding: utf-8 -*-",
    "cf. also https://github.com/pykeen/pykeen/issues/1071",
    "TODO: use a class-resolver?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "convert class to string to use caching",
    "Assume it's a file path",
    "note: we only need to set the create_inverse_triples in the training factory.",
    "normalize dataset kwargs",
    "enable passing force option via dataset_kwargs",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Type annotation for split types",
    "type variables for dictionaries of preprocessed data loaded through torch.load",
    ": The name of the dataset to download",
    "docstr-coverage: inherited",
    "label mapping is in dataset.root/mapping",
    "docstr-coverage: inherited",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "dtype: numpy.int64, shape: (m,)",
    "dtype: numpy.int64, shape: (n, k)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "noqa: D102",
    "docstr-coverage: inherited",
    ": the node types",
    "shape: (n,)",
    "dtype: numpy.int64, shape: (n, k)",
    "disease: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).",
    "drug: STITCH ID (http://stitch.embl.de/).",
    "function: Gene Ontology ID (http://geneontology.org/).",
    "protein: Proteins: Entrez Gene ID (https://www.genenames.org/).",
    "side effects: UMLS CUI (https://www.nlm.nih.gov/research/umls/index.html).",
    "todo(@cthoyt): proper prefixing?",
    "docstr-coverage: inherited",
    "entity mappings are separate for each node type -> combine",
    "convert entity_name to categorical for fast joins",
    "we need the entity dataframe for fast re-mapping later on",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compose temporary df",
    "add extra column with old index to revert sort order change by merge",
    "convert to categorical dtype",
    "join with entity mapping",
    "revert change in order",
    "select global ID",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "TODO: maybe merge into analyze / make sub-command",
    "only save full data",
    "Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)",
    "Plot: difference between mean head and tail degree",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Utilities",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    "docstr-coverage: inherited",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "TODO: use class-resolver normalize?",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ZENODO_URL = \"https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip\"",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "generate subfolders 'training' and  'inference'",
    "TODO: use class-resolver normalize?",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    "Datasets",
    "-*- coding: utf-8 -*-",
    "graph pairs",
    "graph sizes",
    "graph versions",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "ensure zip file is present",
    "save relative paths beforehand so they are present for loading",
    "delegate to super class",
    "docstr-coverage: inherited",
    "left side has files ending with 1, right side with 2",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "input validation",
    "store *before* calling super to have it available when loading the graphs",
    "ensure zip file is present",
    "shared directory for multiple datasets.",
    "docstr-coverage: inherited",
    "create triples factory",
    "docstr-coverage: inherited",
    "load mappings for both sides",
    "load triple alignments",
    "extract entity alignments",
    "(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2",
    "TODO: support ID-only graphs",
    "load both graphs",
    "load alignment",
    "drop duplicates",
    "combine",
    "store for repr",
    "split",
    "create inverse triples only for training",
    "docstr-coverage: inherited",
    "base",
    "concrete",
    "Abstract class",
    "Concrete classes",
    "Data Structures",
    "a buffer for the triples",
    "the offsets",
    "normalization",
    "append shifted mapped triples",
    "update offsets",
    "merge labels with same ID",
    "for mypy",
    "reconstruct label-to-id",
    "optional",
    "merge entity mapping",
    "merge relation mapping",
    "convert labels to IDs",
    "map labels, using -1 as fill-value for invalid labels",
    "we cannot drop them here, since the two columns need to stay aligned",
    "filter alignment",
    "map alignment from old IDs to new IDs",
    "determine swapping partner",
    "only keep triples where we have a swapping partner",
    "replace by swapping partner",
    ": the merged id-based triples, shape: (n, 3)",
    ": the updated alignment, shape: (2, m)",
    ": additional keyword-based parameters for adjusting label-to-id mappings",
    "concatenate triples",
    "filter alignment and translate to IDs",
    "process",
    "TODO: restrict to only using training alignments?",
    "merge mappings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "add swap triples",
    "e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)",
    "create dense entity remapping for swap",
    "add swapped triples",
    "swap head",
    "swap tail",
    ": the name of the additional alignment relation",
    "docstr-coverage: inherited",
    "add alignment triples with extra relation",
    "docstr-coverage: inherited",
    "determine connected components regarding the same-as relation (i.e., applies transitivity)",
    "apply id mapping",
    "ensure consecutive IDs",
    "only use training alignments?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "a fixed checkpoint_name leads avoid collision across trials",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "noqa: DAR101",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "%%",
    "%% [markdown]",
    "## Training a model with PyKEEN",
    "%%",
    "this will log a metric with name \"validation.loss\" to the configured result tracker",
    "%% [markdown]",
    "## Evaluation with seaborn",
    "%%",
    "%%"
  ],
  "v1.10.1": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "See discussion for adding huggingface intersphinx docs at",
    "https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776",
    "autodoc_member_order = 'bysource'",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "check if within 0.5 std of observed",
    "test error is raised",
    "there is an extra test for this case",
    "docstr-coverage: inherited",
    "same size tensors",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Tests that exception will be thrown when more than or less than two tensors are passed",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "ensure each node participates in at least one edge",
    "check type and shape",
    "number of colors is monotonically increasing",
    "ensure each node participates in at least one edge",
    "normalize",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "See https://github.com/pykeen/pykeen/pull/883",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising",
    "wrong ndim",
    "wrong last dim",
    "wrong dtype: float",
    "wrong dtype: complex",
    "correct",
    ">>> positional argument",
    "mapped_triples",
    "triples factory",
    "labeled triples + factory",
    "single labeled triple",
    "multiple labeled triples as list",
    "multiple labeled triples as array",
    ">>> keyword only",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "quaternion have four components",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "cf. https://github.com/pykeen/pykeen/issues/1118",
    "save a reference to the old init *before* mocking",
    "run a small pipline",
    "use sampled training loop ...",
    "... without explicitly selecting a negative sampler ...",
    "... but providing custom kwargs",
    "other parameters for fast test",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    "the relative tolerance for checking close results, cf. torch.allclose",
    "the absolute tolerance for checking close results, cf. torch.allclose",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "allclose checks: | input - other | < atol + rtol * |other|",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "verify that the regularizer is stored for both, entity and relation representations",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "regularization term should be zero",
    "updated should be set to false",
    "call method",
    "generate random tensors",
    "generate inputs",
    "call update",
    "check shape",
    "check result",
    "generate single random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "update term",
    "check that the expected term is returned",
    "check that the regularizer is now reset",
    "create another instance with apply_only_once enabled",
    "test initial state",
    "after first update, should change the term",
    "after second update, no change should happen",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "TODO: look into score_r for inverse relations",
    "clear buffers for message passing models",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    "this implicitly tests extra_repr / iter_extra_repr",
    "select random indices",
    "forward pass with full graph",
    "forward pass with restricted graph",
    "verify the results are similar",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": the input dimension",
    ": the output dimension",
    ": the number of entities",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    ": the interaction to use for testing a model",
    "initializers *may* work in-place => clone",
    "actual number may be different...",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    "inferred from triples factory",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "generate random weights such that sum = n",
    "for sanity checking: give the largest weight to best rank => should improve",
    "generate two versions",
    "1. repeat each rank/candidate pair a random number of times",
    "2. do not repeat, but assign a corresponding weight",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "generate random triples factories",
    "generate random alignment",
    "add label information if necessary",
    "prepare alignment data frame",
    "call",
    "check",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "verify that the input is valid",
    "combine",
    "verify shape",
    "to be initialized in subclass",
    "no column has been removed",
    "all old columns are unmodified",
    "new columns are boolean",
    "no columns have been added",
    "check subset relation",
    "-*- coding: utf-8 -*-",
    "TODO: this could be shared with the model tests",
    "FixedModel: dict(embedding_dim=EMBEDDING_DIM),",
    "test combinations of models with training loops",
    "some models require inverse relations",
    "some model require access to the training triples",
    "inductive models require an inductive mode to be set, and an inference factory to be passed",
    "fake an inference factory",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "sum over the softmax dim as weights sum up to 1",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "abstract classes",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "prediction post-processing",
    "mock prediction data frame",
    "score consumers",
    "use a small model, since operation is expensive",
    "all scores, automatic batch size",
    "top 3 scores",
    "top 3 scores, fixed batch size, head scoring",
    "all scores, relation scoring",
    "all scores, relation scoring",
    "model with inverse relations",
    "check type",
    "check shape",
    "check ID ranges",
    "mapped triples, automatic batch size selection, no factory",
    "mapped triples, fixed batch size, no factory",
    "labeled triples with factory",
    "labeled triples as list",
    "single labeled triple",
    "model with inverse relations",
    "ID-based, no factory",
    "string-based + factory",
    "mixed + factory",
    "no restriction, no factory",
    "no restriction, factory",
    "id restriction, no factory ...",
    "id restriction with factory",
    "comment: we only use id-based input, since the normalization has already been tested",
    "create model",
    "id-based head/relation/tail prediction, no restriction",
    "restriction by list of ints",
    "tail prediction",
    "try accessing each element",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "clear",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "extra-relation",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "additional",
    "verify shape",
    "verify dtype",
    "verify number of entities/relations",
    "verify offsets",
    "create old, new pairs",
    "simulate merging ids",
    "only a single pair",
    "apply",
    "every key is contained",
    "value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: we could move this part into the interaction module itself",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "Rotate (=Hamilton product in quaternion space).",
    "we calculate the scores using the hard-coded formula, instead of utilizing table + einsum",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize rotations to unit modulus",
    "check for unit modulus",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "check type",
    "check size",
    "check value range",
    "-*- coding: utf-8 -*-",
    "message_dim must be divisible by num_heads",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "-*- coding: utf-8 -*-",
    "complex tensor",
    "check value range",
    "check modulus == 1",
    "quaternion needs shape to end on 4",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "value range",
    "highest degree node has largest value",
    "Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf",
    "create triples with a dummy relation type 0",
    "0: green: 2, 3, 7, 8",
    "1: red: 1, 4, 6, 9",
    "2: blue: 0, 5",
    "the example includes the first power",
    "requires at least one complex tensor as input",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "inferred from triples factory",
    "inferred from assignment",
    "the representation module infers the max_id from the provided labels",
    "the following entity does not have an image -> will have to use backfill",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "max_id is inferred from assignment",
    "create random assignment",
    "update kwargs",
    "empty bases",
    "inconsistent base shapes",
    "invalid base id",
    "invalid local index",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "TODO this is the only place this function is used.",
    "Is there an alternative so we can remove it?",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "x_i ~ N(mu_i, 1)",
    "closed-form solution",
    "sampled confidence interval",
    "check that closed-form is in confidence interval of sampled",
    "positive values only",
    "positive and negative values",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "check correct num_entities",
    "check type",
    "check length",
    "check type",
    "check length",
    "check confidence positivity",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "-*- coding: utf-8 -*-",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "initialize",
    "update with batches",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": synonyms of this loss",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "docstr-coverage: inherited",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "docstr-coverage: inherited",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "docstr-coverage: inherited",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "docstr-coverage: inherited",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "docstr-coverage: inherited",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "make sure labels form a proper probability distribution",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "subtract margin from positive scores",
    "divide by temperature",
    "docstr-coverage: inherited",
    "subtract margin from positive scores",
    "normalize positive score shape",
    "divide by temperature",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "we pass *all* scores as negatives, but set the weight of positives to zero",
    "this allows keeping a dense shape",
    "docstr-coverage: inherited",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)",
    "note: this is a reduction along the softmax dim; since the weights are already normalized",
    "to sum to one, we want a sum reduction here, instead of using the self._reduction",
    "docstr-coverage: inherited",
    "Sanity check",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative loss part",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "TODO delete when deleting _normalize_dim (below)",
    "TODO delete when deleting convert_to_canonical_shape (below)",
    "TODO delete? See note in test_sim.py on its only usage",
    "upgrade to sequence",
    "broadcast",
    "normalize ids: -> ids.shape: (batch_size, num_ids)",
    "normalize batch -> batch.shape: (batch_size, 1, 3)",
    "allocate memory",
    "copy ids",
    "reshape",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "ensure pathlib",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "cf. https://stackoverflow.com/a/1176023",
    "check validity",
    "path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "resolve path to make sure it is an absolute path",
    "ensure directory exists",
    "message passing: collect colors of neighbors",
    "dense colors: shape: (n, c)",
    "adj:          shape: (n, n)",
    "values need to be float, since torch.sparse.mm does not support integer dtypes",
    "size: will be correctly inferred",
    "concat with old colors",
    "hash",
    "create random indicator functions of low dimensionality",
    "collect neighbors' colors",
    "round to avoid numerical effects",
    "hash first",
    "concat with old colors",
    "re-hash",
    "only keep connectivity, but remove multiplicity",
    "note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,",
    "this is rather useless",
    "initial: degree",
    "note: we calculate this separately, since we can use a more efficient implementation for the first step",
    "hash",
    "determine small integer type for dense count array",
    "convergence check",
    "each node has a unique color",
    "the number of colors did not improve in the last iteration",
    "cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "docstr-coverage: inherited",
    "no need to compute anything",
    "docstr-coverage: inherited",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "regularizer-specific parameters",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "orthogonality soft constraint: cosine similarity at most epsilon",
    "The normalization factor to balance individual regularizers' contribution.",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "high-level",
    "Low-Level",
    "cf. https://github.com/python/mypy/issues/5374",
    ": the dataframe; has to have a column named \"score\"",
    ": an optional factory to use for labeling",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the prediction target",
    ": the other column's fixed IDs",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the ID-based triples, shape: (n, 3)",
    ": the scores",
    "3-tuple for return",
    "extract label information, if possible",
    "no restriction",
    "restriction is a tensor",
    "restriction is a sequence of integers or strings",
    "now, restriction is a sequence of integers",
    "if explicit ids have been given, and label information is available, extract list of labels",
    "exactly one of them is None",
    "create input batch",
    "note type alias annotation required,",
    "cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases",
    "batch, TODO: ids?",
    "docstr-coverage: inherited",
    "initialize buffer on device",
    "docstr-coverage: inherited",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "determine corresponding indices",
    "batch_id, score_id = divmod(top_indices, num_scores)",
    "combine to top triples",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "docstr-coverage: inherited",
    "TODO: variable targets across batches/samples?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "(?, r, t) => r.stride > t.stride",
    "(h, ?, t) => h.stride > t.stride",
    "(h, r, ?) => h.stride > r.stride",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "train model; note: needs larger number of epochs to do something useful ;-)",
    "create prediction dataset, where the head entities is from a set of European countries,",
    "and the relations are connected to tourism",
    "calculate all scores for this restricted set, and keep k=3 largest",
    "add labels",
    ": the choices for the first and second component of the input batch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "calculate batch scores onces",
    "consume by all consumers",
    "TODO: Support partial dataset",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "exactly one of them is None",
    "",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "get input & target",
    "get label-to-id mapping and prediction targets",
    "get scores",
    "create raw dataframe",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "normalize input",
    "calculate scores (with automatic memory optimization)",
    "-*- coding: utf-8 -*-",
    "\"Closed-Form Expectation\",",
    "\"Closed-Form Variance\",",
    "\"\u2713\" if metric.closed_expectation else \"\",",
    "\"\u2713\" if metric.closed_variance else \"\",",
    "Add HPO command",
    "Add NodePiece tokenization command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    "entity alignment sides",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "entity alignment",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "infer shape",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "input normalization",
    "note: the base class does not have any parameters",
    "Heuristic for default value",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: the only parameters are inside the relation representation module, which has its own reset_parameters",
    "docstr-coverage: inherited",
    "TODO: can we change the dimension order to make this contiguous?",
    "docstr-coverage: inherited",
    "normalize num blocks",
    "determine necessary padding",
    "determine block sizes",
    "(R, nb, bsi, bso)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(n, di) -> (n, nb, bsi)",
    "(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)",
    "(R, n, nb, bso) -> (R * n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "(n, R * n), (R * n, do) -> (n, do)",
    "remove padding if necessary",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(R * n, n), (n, di) -> (R * n, di)",
    "(R * n, di) -> (R, n, nb, bsi)",
    "(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)",
    "(n, nb, bso) -> (n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "remove padding if necessary",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "TODO: we could cache the stacked adjacency matrices",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "input validation",
    "has to be imported now to avoid cyclic imports",
    "has to be assigned after call to nn.Module init",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "heuristic",
    "normalize *before* repeating",
    "repeat if necessary",
    "regularize *after* repeating",
    "dropout & regularizer will appear automatically, since it is a nn.Module",
    "has to be imported here to avoid cyclic import",
    "docstr-coverage: inherited",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "docstr-coverage: inherited",
    "initialize weights in-place",
    "docstr-coverage: inherited",
    "apply constraints in-place",
    "fixme: work-around until nn.Embedding supports complex",
    "docstr-coverage: inherited",
    "fixme: work-around until nn.Embedding supports complex",
    "verify that contiguity is preserved",
    "create low-rank approximation object",
    "get base representations, shape: (n, *ds)",
    "calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))",
    "overwrite bases and weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "TODO: might not be true for all compositions",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "docstr-coverage: inherited",
    "check max_id",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "docstr-coverage: inherited",
    ": the base representations",
    ": the combination module",
    "input normalization",
    "has to be imported here to avoid cyclic import",
    "create base representations",
    "verify same ID range",
    "note: we could also relax the requiremen, and set max_id = min(max_ids)",
    "shape inference",
    "assign base representations *after* super init",
    "docstr-coverage: inherited",
    "delegate to super class",
    "Generate graph dataset from the Monarch Disease Ontology (MONDO)",
    ": the assignment from global ID to (representation, local id), shape: (max_id, 2)",
    "import here to avoid cyclic import",
    "instantiate base representations if necessary",
    "there needs to be at least one base",
    "while possible, this might be unintended",
    "extract shape",
    "check for invalid base ids",
    "check for invalid local indices",
    "assign modules / buffers *after* super init",
    "docstr-coverage: inherited",
    "flatten assignment to ease construction of inverse indices",
    "we group indices by the representation which provides them",
    "thus, we need an inverse to restore the correct order",
    "get representations",
    "update inverse indices",
    "invert flattening",
    "import here to avoid cyclic import",
    "comment: not all representations support passing a shape parameter",
    "create assignment",
    "base",
    "other",
    "import here to avoid cyclic import",
    "infer shape",
    "infer max_id",
    "docstr-coverage: inherited",
    "TODO: can be a combined representations, with appropriate tensor-train combination",
    ": shape: (max_id, num_cores)",
    ": the bases, length: num_cores, with compatible shapes",
    "check shape",
    "check value range",
    "do not increase counter i, since the dimension is shared with the following term",
    "i += 1",
    "ids //= m_i",
    "import here to avoid cyclic import",
    "normalize ranks",
    "determine M_k, N_k",
    "TODO: allow to pass them from outside?",
    "normalize assignment",
    "determine shapes and einsum equation",
    "create base representations",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "abstract",
    "concrete classes",
    "default flow",
    ": the message passing layers",
    ": the flow direction of messages across layers",
    ": the edge index, shape: (2, num_edges)",
    "fail if dependencies are missing",
    "avoid cyclic import",
    "the base representations, e.g., entity embeddings or features",
    "verify max_id",
    "verify shape",
    "assign sub-module *after* super call",
    "initialize layers",
    "normalize activation",
    "check consistency",
    "buffer edge index for message passing",
    "TODO: inductiveness; we need to",
    "* replace edge_index",
    "* replace base representations",
    "* keep layers & activations",
    "docstr-coverage: inherited",
    "we can restrict the message passing to the k-hop neighborhood of the desired indices;",
    "this does only make sense if we do not request *all* indices",
    "k_hop_subgraph returns:",
    "(1) the nodes involved in the subgraph",
    "(2) the filtered edge_index connectivity",
    "(3) the mapping from node indices in node_idx to their new location, and",
    "(4) the edge mask indicating which edges were preserved",
    "we only need the base representations for the neighbor indices",
    "get *all* base representations",
    "use *all* edges",
    "perform message passing",
    "select desired indices",
    "docstr-coverage: inherited",
    ": the edge type, shape: (num_edges,)",
    "register an additional buffer for the categorical edge type",
    "docstr-coverage: inherited",
    ": the relation representations used to obtain initial edge features",
    "avoid cyclic import",
    "docstr-coverage: inherited",
    "get initial relation representations",
    "select edge attributes from relation representations according to relation type",
    "perform message passing",
    "apply relation transformation, if necessary",
    "-*- coding: utf-8 -*-",
    "Classes",
    "Resolver",
    "backwards compatibility",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "must be cloned if we want to do backprop",
    "the color initializer",
    "variants for the edge index",
    "additional parameters for iter_weisfeiler_lehman",
    "normalize shape",
    "get coloring",
    "make color initializer",
    "initialize color representations",
    "note: this could be a representation?",
    "init entity representations according to the color",
    "create random walk matrix",
    "stack diagonal entries of powers of rw",
    "abstract",
    "concrete",
    "docstr-coverage: inherited",
    "tokenize",
    "pad",
    "get character embeddings",
    "pool",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "backward compatibility with RGCN",
    "docstr-coverage: inherited",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    "-*- coding: utf-8 -*-",
    "Caches",
    "if the sparsity becomes too low, convert to a dense matrix",
    "note: this heuristic is based on the memory consumption,",
    "for a sparse matrix, we store 3 values per nnz (row index, column index, value)",
    "performance-wise, it likely makes sense to switch even earlier",
    "`torch.sparse.mm` can also deal with dense 2nd argument",
    "note: torch.sparse.mm only works for COO matrices;",
    "@ only works for CSR matrices",
    "convert to COO, if necessary",
    "we need to use indices here, since there may be zero diagonal entries",
    ": Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing",
    "cf. https://meta.wikimedia.org/wiki/User-Agent_policy",
    "cf. https://wikitech.wikimedia.org/wiki/Robot_policy",
    "break into smaller requests",
    "try to load cached first",
    "determine missing entries",
    "retrieve information via SPARQL",
    "save entries",
    "fill missing descriptions",
    "for mypy",
    "get labels & descriptions",
    "compose labels",
    "we can have multiple images per entity -> collect image URLs per image",
    "entity ID",
    "relation ID",
    "image URL",
    "check whether images are still missing",
    "select on image url per image in a reproducible way",
    "traverse relations in order of preference",
    "now there is an image available -> select reproducible by URL sorting",
    "did not break -> no image",
    "This import doesn't need a wrapper since it's a transitive",
    "requirement of PyOBO",
    "darglint does not like",
    "raise cls(shape=shape, reference=reference)",
    "1 * ? = ?; ? * 1 = ?",
    "i**2 = j**2 = k**2 = -1",
    "i * j = k; i * k = -j",
    "j * i = -k, j * k = i",
    "k * i = j; k * j = -i",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "-*- coding: utf-8 -*-",
    "REPRESENTATION",
    "base",
    "concrete",
    "INITIALIZER",
    "INTERACTIONS",
    "Adapter classes",
    "Concrete Classes",
    "combinations",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different.",
    ": Otherwise, the entity_shape is used for head & tail entities",
    ": The symbolic shapes for relation representations",
    "if the interaction function's head parameter should only receive a subset of entity representations",
    "if the interaction function's tail parameter should only receive a subset of entity representations",
    ": the interaction's value range (for unrestricted input)",
    "TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2",
    "TODO: annotate properties, e.g., symmetry, and use them for testing?",
    "TODO: annotate complexity?",
    "TODO: cannot cover dynamic shapes, e.g., AutoSF",
    "TODO: we could change that to slicing along multiple dimensions, if necessary",
    ": The functional interaction form",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: update class docstring",
    "TODO: give this a better name?",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    "vector & scalar offset",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "normalize kernel height",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The interaction model",
    "docstr-coverage: inherited",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "docstr-coverage: inherited",
    "normalize hidden_dim",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Initialize biases with zero",
    "In the original formulation,",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "default core tensor initialization",
    "cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "normalize initializer",
    "normalize relation dimension",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "docstr-coverage: inherited",
    "instantiate here to make module easily serializable",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "there are separate biases for entities in head and tail position",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "with k=4",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "docstr-coverage: inherited",
    "input normalization",
    "Core tensor",
    "docstr-coverage: inherited",
    "initialize core tensor",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "r_head, r_mid, r_tail",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "type alias for AutoSF block description",
    "head_index, relation_index, tail_index, sign",
    ": a description of the block structure",
    "convert to tuple",
    "infer the number of entity and relation representations",
    "verify coefficients",
    "dynamic entity / relation shapes",
    "docstr-coverage: inherited",
    "r_head, r_bias, r_tail",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: excused `wrapped`",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "cat into shape (..., 1, d, 3)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "shortcut for same shape",
    "split weight into head-/relation-/tail-specific sub-matrices",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace",
    "ensure constraints",
    "x_1 contraction",
    "x_2 contraction",
    "TODO: this sign is in the official code, too, but why do we need it?",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "head",
    "relation",
    "tail",
    "extension: negative (power) norm",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "input normalization",
    "instantiate separate combinations",
    "docstr-coverage: inherited",
    "split complex; repeat real",
    "separately combine real and imaginary parts",
    "combine",
    "docstr-coverage: inherited",
    "symbolic output to avoid dtype issue",
    "we only need to consider real part here",
    "the gate",
    "the combination",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "docstr-coverage: inherited",
    "sort by decreasing degree",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "sort by decreasing page rank",
    "docstr-coverage: inherited",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": A list with ratios per representation in their creation order,",
    ": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`",
    ": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "Create an MLP for string aggregation",
    "note: the token representations' shape includes the number of tokens as leading dim",
    "unique hashes per representation",
    "unique hashes if we concatenate all representations together",
    "TODO: vectorization?",
    "remove self-loops",
    "add inverse edges and remove duplicates",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "TODO: expose num_anchors?",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "To prevent possible segfaults in the METIS C code, METIS expects a graph",
    "(1) without self-loops; (2) with inverse edges added; (3) with unique edges only",
    "https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18",
    "select independently per partition",
    "select adjacency part;",
    "note: the indices will automatically be in [0, ..., high - low), since they are *local* indices",
    "offset",
    "the -1 comes from the shared padding token",
    "note: permutation will be later on reverted",
    "add back 1 for the shared padding token",
    "TODO: check if perm is used correctly",
    "verify pool",
    "docstr-coverage: inherited",
    "choose first num_tokens",
    "TODO: vectorization?",
    "heuristic",
    "heuristic",
    "calculate configuration digest",
    "create anchor selection instance",
    "select anchors",
    "anchor search (=anchor assignment?)",
    "assign anchors",
    "save",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "ensure parent directory exists",
    "save via torch.save",
    "docstr-coverage: inherited",
    "TODO: since we save a contiguous array of (num_entities, num_anchors),",
    "it would be more efficient to not convert to a mapping, but directly select from the tensor",
    "-*- coding: utf-8 -*-",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "Data containers",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "convert to adjacency matrix",
    "convert to scipy sparse csr",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "TODO: padding for unreachable?",
    "select anchor IDs with smallest distance",
    "docstr-coverage: inherited",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "anchor nodes have themselves as a starting found anchor",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output that track the distance to each found anchor",
    "dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)",
    "initial anchors are 0-hop away from themselves",
    "propagate one hop",
    "TODO the float() trick for GPU result stability until the torch_sparse issue is resolved",
    "https://github.com/rusty1s/pytorch_sparse/issues/243",
    "convergence check",
    "newly reached is a mask that points to newly discovered anchors at this particular step",
    "implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)",
    "in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "update the value in the pool by the current hop value (we start from 0, so +1 be default)",
    "stop once we have enough",
    "sort the pool by nearest to farthest anchors",
    "values with distance 255 (or max for unsigned int8 type) are padding tokens",
    "since the output is sorted, no need for random sampling, we just take top-k nearest",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "select k anchors with largest ppr, shape: (batch_size, k)",
    "prepare adjacency matrix only once",
    "prepare result",
    "progress bar?",
    "batch-wise computation of PPR",
    "run page-rank calculation, shape: (batch_size, n)",
    "select PPR values for the anchors, shape: (batch_size, num_anchors)",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "should be exactly 1.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2",
    "use V[x] = E[x^2] - E[x]^2",
    "group by same weight -> compute H_w(n) for multiple n at once",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective",
    "cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: should we return the sum of weights?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "-*- coding: utf-8 -*-",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    ": whether the metric supports weights",
    ": whether there is a closed-form solution of the expectation",
    ": whether there is a closed-form solution of the variance",
    "normalize weights",
    "calculate weighted harmonic mean",
    "calculate cdf",
    "determine value at p=0.5",
    "special case for exactly 0.5",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A description of the metric",
    ": The function that runs the metric",
    "docstr-coverage: inherited",
    ": Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics",
    ": This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`",
    "TODO there's something wrong with this, so add it later",
    "classifier_annotator.higher(",
    "rmc.pr_auc_score,",
    "name=\"AUC-PR\",",
    "description=\"Area Under the Precision-Recall Curve\",",
    "link=\"https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score\",",
    ")",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "noqa:DAR101",
    "noqa:DAR401",
    "Create directory in which all experimental artifacts are saved",
    "noqa:DAR101",
    "clip for node piece configurations",
    "\"pykeen experiments reproduce\" expects \"model reference dataset\"",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: take care that triples aren't removed that are the only ones with any given entity",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "save literal-to-id mapping",
    "save numeric literals",
    "load literal-to-id",
    "load literals",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    ": the number of unique entities",
    ": the number of relations (maybe including \"artificial\" inverse relations)",
    ": whether to create inverse triples",
    ": the number of real relations, i.e., without artificial inverses",
    "ensure torch.Tensor",
    "input validation",
    "always store as torch.long, i.e., torch's default integer dtype",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "docstr-coverage: inherited",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "note: num_relations will be doubled again when instantiating with create_inverse_triples=True",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "docstr-coverage: inherited",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "Generate a word cloud image",
    "docstr-coverage: inherited",
    "vectorized label lookup",
    "Re-order columns",
    "docstr-coverage: inherited",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "ID-based triples",
    "labeled triples",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "triples factory",
    "all keyword-based options have been none",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "only labeled triples are remaining",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "docstr-coverage: inherited",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Make sure that the first element has all the right stuff in it",
    "docstr-coverage: inherited",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "noqa:DAR202",
    "noqa:DAR401",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "docstr-coverage: inherited",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "docstr-coverage: inherited",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest",
    "in practice, this should have a minor effect, since typically batch_size << num_instances",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "disable automatic batching",
    "docstr-coverage: inherited",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "lazy init",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Split batch components",
    "Send batch to device",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "unpack",
    "Send batch to device",
    "head prediction",
    "TODO: exploit sparsity",
    "note: this is different to what we do for LCWA, where we collect *all* training entities",
    "for which the combination is true",
    "tail prediction",
    "TODO: exploit sparsity",
    "regularization",
    "docstr-coverage: inherited",
    "TODO?",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    ": the path to the weights of the best model",
    ": whether to delete the file with the best model weights after termination",
    ": note: the weights will be re-loaded into the model before",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "for mypy",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "dataset",
    "model",
    "stored outside of the training loop / optimizer to give access to auto-tuning from Lightning",
    "optimizer",
    "TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA",
    "docstr-coverage: inherited",
    "call post_parameter_update",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "TODO:",
    "shuffle=shuffle,",
    "drop_last=drop_last,",
    "sampler=sampler,",
    "shuffle=shuffle,",
    "disable automatic batching in data loader",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "note: since this file is executed via __main__, its module name is replaced by __name__",
    "hence, the two classes' fully qualified names start with \"_\" and are considered private",
    "cf. https://github.com/cthoyt/class-resolver/issues/39",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "mixed precision training",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    ": The metric key",
    ": Side of the metric, or \"both\"",
    ": The rank type",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "normalize keys",
    "TODO: this can only normalize rank-based metrics!",
    "TODO: find a better way to handle this",
    "-*- coding: utf-8 -*-",
    "TODO: fix this upstream / make metric.score comply to signature",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Because the order of the values of an dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    ": the scores of the true choice, shape: (*bs), dtype: float",
    ": the number of scores which were larger than the true score, shape: (*bs), dtype: long",
    ": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long",
    ": the total number of compared scores, shape: (*bs), dtype: long",
    "-*- coding: utf-8 -*-",
    "TODO remove this, it makes code much harder to reason about",
    "add mode parameter",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "do not display progress bar while searching",
    "start by searching for batch_size",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "we do not need to repeat time-consuming checks",
    "infer start value",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r), (h, t) or (r, t)) scored on all",
    "entities/relations, must have failed to start slice_size search, we start with trying half the",
    "entities/relations.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)",
    "otherwise, i.e., without inverse triples, we also need score_h",
    "if relations are to be predicted, we need to slice score_r",
    "raise an error, if any of the required methods cannot slice",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Send to device",
    "Ensure evaluation mode",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optionally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "-*- coding: utf-8 -*-",
    "Evaluation loops",
    "Evaluation datasets",
    ": the MemoryUtilizationMaximizer instance for :func:`_evaluate`.",
    "batch",
    "tqdm",
    "data loader",
    "set upper limit of batch size for automatic memory optimization",
    "set model to evaluation mode",
    "delegate to AMO wrapper",
    "The key-id for each triple, shape: (num_triples,)",
    ": the number of targets for each key, shape: (num_unique_keys + 1,)",
    ": the concatenation of unique targets for each key (use bounds to select appropriate sub-array)",
    "input verification",
    "group key = everything except the prediction target",
    "initialize data structure",
    "group by key",
    "convert lists to arrays",
    "instantiate",
    "return indices corresponding to the `item`-th triple",
    "input normalization",
    "prepare filter indices if required",
    "sorted by target -> most of the batches only have a single target",
    "group by target",
    "stack groups into a single tensor",
    "avoid cyclic imports",
    "TODO: it would be better to allow separate batch sizes for entity/relation prediction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is",
    "not shuffled, and contains evaluation ranking tasks sorted by target",
    "TODO: in theory, we could make a single score calculation for e.g.,",
    "{(h, r, t1), (h, r, t1), ..., (h, r, tk)}",
    "predict scores for all candidates",
    "filter scores",
    "extract true scores",
    "replace by nan",
    "rewrite true scores",
    "create dense positive masks",
    "TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around",
    "delegate processing of scores to the evaluator",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation",
    "metric. we do want to support user-selected metrics on arbitrary datasets instead",
    "this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}",
    "filter supported metrics",
    "prepare input format, cf. `evaluator.expected_input``",
    "y_pred_pos: shape: (num_edge,)",
    "y_pred_neg: shape: (num_edge, num_nodes_neg)",
    "iterate over prediction targets",
    "pre-allocate",
    "TODO: maybe we want to collect scores on CPU / add an option?",
    "iterate over batches",
    "combine ids, shape: (batch_size, num_negatives + 1)",
    "get scores, shape: (batch_size, num_negatives + 1)",
    "store positive and negative scores",
    "cf. https://github.com/snap-stanford/ogb/pull/357",
    "combine to input dictionary",
    "delegate to OGB evaluator",
    "post-processing",
    "normalize name",
    "OGB does not aggregate values across triples",
    "-*- coding: utf-8 -*-",
    "flatten dictionaries",
    "individual side",
    "combined",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "docstr-coverage: inherited",
    "TODO: do not require to compute all scores beforehand",
    "cf. Model.score_t(ts=...)",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "combine key batches",
    "calculate key frequency",
    "weight = inverse frequency",
    "broadcast to samples",
    "docstr-coverage: inherited",
    "store keys for calculating macro weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute macro weights",
    "note: we wrap the array into a list to be able to re-use _iter_ranks",
    "calculate weighted metrics",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "docstr-coverage: inherited",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "TODO: support \"broadcasting\" representation regularizers?",
    "e.g. re-use the same regularizer for everything; or",
    "pass a dictionary with keys \"entity\"/\"relation\";",
    "values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "note, triples_factory is required instead of just using self.num_entities",
    "and self.num_relations for the inductive case when this is different",
    "instantiate regularizer",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "batch normalization modules use batch statistics in training mode",
    "-> different batch divisions lead to different results",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": the number of entities",
    ": the number of relations",
    ": whether to use inverse relations",
    ": utility for generating inverse relations",
    ": When predict_with_sigmoid is set to True, the sigmoid function is",
    ": applied to the logits during evaluation and also for predictions",
    ": after training, but has no effect on the training.",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Meta Models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "note: we need to share the aggregation across representations, since the aggregation may have",
    "trainable parameters",
    ": a mapping from inductive mode to corresponding entity representations",
    ": note: there may be duplicate values, if entity representations are shared between validation and testing",
    "inductive factories",
    "entity representation kwargs may contain a triples factory, which needs to be replaced",
    "entity_representations_kwargs.pop(\"triples_factory\", None)",
    "note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere",
    "shared",
    "non-shared",
    "note: \"training\" is an attribute of nn.Module -> need to rename to avoid name collision",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the indexed filter triples, i.e., sparse masks",
    "avoid cyclic imports",
    "create base model",
    "assign *after* nn.Module.__init__",
    "save constants",
    "index triples",
    "initialize base model's parameters",
    "get masks, shape: (batch_size, num_entities/num_relations)",
    "combine masks",
    "note: * is an elementwise and, and + and elementwise or",
    "get non-zero entries",
    "set scores for fill value for every non-occuring entry",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "NodePiece",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "entity embedding",
    "ConvE uses one bias for each entity",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the default loss function is the self-adversarial negative sampling loss",
    ": The default parameters for the default loss function class",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "note: DistMult only regularizes the relation embeddings;",
    "entity embeddings are hard constrained instead",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "translation vector in hyperplane",
    "normal vector of hyperplane",
    "normalise the normal vectors to unit l2 length",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "thus, we need to use a weight regularizer instead of having an Embedding regularizer,",
    "which only regularizes the weights used in a batch",
    "note: the following is already the default",
    "default_regularizer=self.regularizer_default,",
    "default_regularizer_kwargs=self.regularizer_default_kwargs,",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "relation embedding",
    "relation projection",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "input normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "the individual combination for real/complex parts",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "no activation",
    "-*- coding: utf-8 -*-",
    ": the interaction class (for generating the overview table)",
    "added by ERModel",
    "max_id=triples_factory.num_entities,",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "docstr-coverage: inherited",
    ": The column names",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    "docstr-coverage: inherited",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "docstr-coverage: inherited",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "docstr-coverage: inherited",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "docstr-coverage: inherited",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "file names for storing results",
    "TODO: rename param?",
    "always save results as json file",
    "save other components only if requested (which they are, by default)",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train",
    "TODO: allow empty validation / testing",
    "evaluation restriction to a subset of entities/relations",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "8. Evaluation",
    "7. Training (ronaldo style)",
    "Misc",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Misc",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "Start tracking",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "cf. also https://github.com/pykeen/pykeen/issues/1071",
    "TODO: use a class-resolver?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "convert class to string to use caching",
    "Assume it's a file path",
    "note: we only need to set the create_inverse_triples in the training factory.",
    "normalize dataset kwargs",
    "enable passing force option via dataset_kwargs",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "TODO: maybe merge into analyze / make sub-command",
    "only save full data",
    "Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)",
    "Plot: difference between mean head and tail degree",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Utilities",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    "docstr-coverage: inherited",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "TODO: use class-resolver normalize?",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ZENODO_URL = \"https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip\"",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "generate subfolders 'training' and  'inference'",
    "TODO: use class-resolver normalize?",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    "Datasets",
    "-*- coding: utf-8 -*-",
    "graph pairs",
    "graph sizes",
    "graph versions",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "ensure zip file is present",
    "save relative paths beforehand so they are present for loading",
    "delegate to super class",
    "docstr-coverage: inherited",
    "left side has files ending with 1, right side with 2",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "input validation",
    "store *before* calling super to have it available when loading the graphs",
    "ensure zip file is present",
    "shared directory for multiple datasets.",
    "docstr-coverage: inherited",
    "create triples factory",
    "docstr-coverage: inherited",
    "load mappings for both sides",
    "load triple alignments",
    "extract entity alignments",
    "(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2",
    "TODO: support ID-only graphs",
    "load both graphs",
    "load alignment",
    "drop duplicates",
    "combine",
    "store for repr",
    "split",
    "create inverse triples only for training",
    "docstr-coverage: inherited",
    "base",
    "concrete",
    "Abstract class",
    "Concrete classes",
    "Data Structures",
    "a buffer for the triples",
    "the offsets",
    "normalization",
    "append shifted mapped triples",
    "update offsets",
    "merge labels with same ID",
    "for mypy",
    "reconstruct label-to-id",
    "optional",
    "merge entity mapping",
    "merge relation mapping",
    "convert labels to IDs",
    "map labels, using -1 as fill-value for invalid labels",
    "we cannot drop them here, since the two columns need to stay aligned",
    "filter alignment",
    "map alignment from old IDs to new IDs",
    "determine swapping partner",
    "only keep triples where we have a swapping partner",
    "replace by swapping partner",
    ": the merged id-based triples, shape: (n, 3)",
    ": the updated alignment, shape: (2, m)",
    ": additional keyword-based parameters for adjusting label-to-id mappings",
    "concatenate triples",
    "filter alignment and translate to IDs",
    "process",
    "TODO: restrict to only using training alignments?",
    "merge mappings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "add swap triples",
    "e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)",
    "create dense entity remapping for swap",
    "add swapped triples",
    "swap head",
    "swap tail",
    ": the name of the additional alignment relation",
    "docstr-coverage: inherited",
    "add alignment triples with extra relation",
    "docstr-coverage: inherited",
    "determine connected components regarding the same-as relation (i.e., applies transitivity)",
    "apply id mapping",
    "ensure consecutive IDs",
    "only use training alignments?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "noqa: DAR101",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.10.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "See discussion for adding huggingface intersphinx docs at",
    "https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776",
    "autodoc_member_order = 'bysource'",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "check if within 0.5 std of observed",
    "test error is raised",
    "there is an extra test for this case",
    "docstr-coverage: inherited",
    "same size tensors",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Tests that exception will be thrown when more than or less than two tensors are passed",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "ensure each node participates in at least one edge",
    "check type and shape",
    "number of colors is monotonically increasing",
    "ensure each node participates in at least one edge",
    "normalize",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "See https://github.com/pykeen/pykeen/pull/883",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising",
    "wrong ndim",
    "wrong last dim",
    "wrong dtype: float",
    "wrong dtype: complex",
    "correct",
    ">>> positional argument",
    "mapped_triples",
    "triples factory",
    "labeled triples + factory",
    "single labeled triple",
    "multiple labeled triples as list",
    "multiple labeled triples as array",
    ">>> keyword only",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "quaternion have four components",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "cf. https://github.com/pykeen/pykeen/issues/1118",
    "save a reference to the old init *before* mocking",
    "run a small pipline",
    "use sampled training loop ...",
    "... without explicitly selecting a negative sampler ...",
    "... but providing custom kwargs",
    "other parameters for fast test",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    "the relative tolerance for checking close results, cf. torch.allclose",
    "the absolute tolerance for checking close results, cf. torch.allclose",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "allclose checks: | input - other | < atol + rtol * |other|",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "verify that the regularizer is stored for both, entity and relation representations",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "regularization term should be zero",
    "updated should be set to false",
    "call method",
    "generate random tensors",
    "generate inputs",
    "call update",
    "check shape",
    "check result",
    "generate single random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "update term",
    "check that the expected term is returned",
    "check that the regularizer is now reset",
    "create another instance with apply_only_once enabled",
    "test initial state",
    "after first update, should change the term",
    "after second update, no change should happen",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "TODO: look into score_r for inverse relations",
    "clear buffers for message passing models",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    "this implicitly tests extra_repr / iter_extra_repr",
    "select random indices",
    "forward pass with full graph",
    "forward pass with restricted graph",
    "verify the results are similar",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": the input dimension",
    ": the output dimension",
    ": the number of entities",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    ": the interaction to use for testing a model",
    "initializers *may* work in-place => clone",
    "actual number may be different...",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    "inferred from triples factory",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "generate random weights such that sum = n",
    "for sanity checking: give the largest weight to best rank => should improve",
    "generate two versions",
    "1. repeat each rank/candidate pair a random number of times",
    "2. do not repeat, but assign a corresponding weight",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "generate random triples factories",
    "generate random alignment",
    "add label information if necessary",
    "prepare alignment data frame",
    "call",
    "check",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "verify that the input is valid",
    "combine",
    "verify shape",
    "to be initialized in subclass",
    "no column has been removed",
    "all old columns are unmodified",
    "new columns are boolean",
    "no columns have been added",
    "check subset relation",
    "-*- coding: utf-8 -*-",
    "TODO: this could be shared with the model tests",
    "FixedModel: dict(embedding_dim=EMBEDDING_DIM),",
    "test combinations of models with training loops",
    "some models require inverse relations",
    "some model require access to the training triples",
    "inductive models require an inductive mode to be set, and an inference factory to be passed",
    "fake an inference factory",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "sum over the softmax dim as weights sum up to 1",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "abstract classes",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "prediction post-processing",
    "mock prediction data frame",
    "score consumers",
    "use a small model, since operation is expensive",
    "all scores, automatic batch size",
    "top 3 scores",
    "top 3 scores, fixed batch size, head scoring",
    "all scores, relation scoring",
    "all scores, relation scoring",
    "model with inverse relations",
    "check type",
    "check shape",
    "check ID ranges",
    "mapped triples, automatic batch size selection, no factory",
    "mapped triples, fixed batch size, no factory",
    "labeled triples with factory",
    "labeled triples as list",
    "single labeled triple",
    "model with inverse relations",
    "ID-based, no factory",
    "string-based + factory",
    "mixed + factory",
    "no restriction, no factory",
    "no restriction, factory",
    "id restriction, no factory ...",
    "id restriction with factory",
    "comment: we only use id-based input, since the normalization has already been tested",
    "create model",
    "id-based head/relation/tail prediction, no restriction",
    "restriction by list of ints",
    "tail prediction",
    "try accessing each element",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "clear",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "extra-relation",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "additional",
    "verify shape",
    "verify dtype",
    "verify number of entities/relations",
    "verify offsets",
    "create old, new pairs",
    "simulate merging ids",
    "only a single pair",
    "apply",
    "every key is contained",
    "value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: we could move this part into the interaction module itself",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "Rotate (=Hamilton product in quaternion space).",
    "we calculate the scores using the hard-coded formula, instead of utilizing table + einsum",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize rotations to unit modulus",
    "check for unit modulus",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "check type",
    "check size",
    "check value range",
    "-*- coding: utf-8 -*-",
    "message_dim must be divisible by num_heads",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "-*- coding: utf-8 -*-",
    "complex tensor",
    "check value range",
    "check modulus == 1",
    "quaternion needs shape to end on 4",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "value range",
    "highest degree node has largest value",
    "Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf",
    "create triples with a dummy relation type 0",
    "0: green: 2, 3, 7, 8",
    "1: red: 1, 4, 6, 9",
    "2: blue: 0, 5",
    "the example includes the first power",
    "requires at least one complex tensor as input",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "inferred from triples factory",
    "inferred from assignment",
    "the representation module infers the max_id from the provided labels",
    "the following entity does not have an image -> will have to use backfill",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "max_id is inferred from assignment",
    "create random assignment",
    "update kwargs",
    "empty bases",
    "inconsistent base shapes",
    "invalid base id",
    "invalid local index",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "TODO this is the only place this function is used.",
    "Is there an alternative so we can remove it?",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "x_i ~ N(mu_i, 1)",
    "closed-form solution",
    "sampled confidence interval",
    "check that closed-form is in confidence interval of sampled",
    "positive values only",
    "positive and negative values",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "check correct num_entities",
    "check type",
    "check length",
    "check type",
    "check length",
    "check confidence positivity",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "-*- coding: utf-8 -*-",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "initialize",
    "update with batches",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": synonyms of this loss",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "docstr-coverage: inherited",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "docstr-coverage: inherited",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "docstr-coverage: inherited",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "docstr-coverage: inherited",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "docstr-coverage: inherited",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "make sure labels form a proper probability distribution",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "subtract margin from positive scores",
    "divide by temperature",
    "docstr-coverage: inherited",
    "subtract margin from positive scores",
    "normalize positive score shape",
    "divide by temperature",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "we pass *all* scores as negatives, but set the weight of positives to zero",
    "this allows keeping a dense shape",
    "docstr-coverage: inherited",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)",
    "note: this is a reduction along the softmax dim; since the weights are already normalized",
    "to sum to one, we want a sum reduction here, instead of using the self._reduction",
    "docstr-coverage: inherited",
    "Sanity check",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative loss part",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "TODO delete when deleting _normalize_dim (below)",
    "TODO delete when deleting convert_to_canonical_shape (below)",
    "TODO delete? See note in test_sim.py on its only usage",
    "upgrade to sequence",
    "broadcast",
    "normalize ids: -> ids.shape: (batch_size, num_ids)",
    "normalize batch -> batch.shape: (batch_size, 1, 3)",
    "allocate memory",
    "copy ids",
    "reshape",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "ensure pathlib",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "cf. https://stackoverflow.com/a/1176023",
    "check validity",
    "path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "resolve path to make sure it is an absolute path",
    "ensure directory exists",
    "message passing: collect colors of neighbors",
    "dense colors: shape: (n, c)",
    "adj:          shape: (n, n)",
    "values need to be float, since torch.sparse.mm does not support integer dtypes",
    "size: will be correctly inferred",
    "concat with old colors",
    "hash",
    "create random indicator functions of low dimensionality",
    "collect neighbors' colors",
    "round to avoid numerical effects",
    "hash first",
    "concat with old colors",
    "re-hash",
    "only keep connectivity, but remove multiplicity",
    "note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,",
    "this is rather useless",
    "initial: degree",
    "note: we calculate this separately, since we can use a more efficient implementation for the first step",
    "hash",
    "determine small integer type for dense count array",
    "convergence check",
    "each node has a unique color",
    "the number of colors did not improve in the last iteration",
    "cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "docstr-coverage: inherited",
    "no need to compute anything",
    "docstr-coverage: inherited",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "regularizer-specific parameters",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "orthogonality soft constraint: cosine similarity at most epsilon",
    "The normalization factor to balance individual regularizers' contribution.",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "high-level",
    "Low-Level",
    "cf. https://github.com/python/mypy/issues/5374",
    ": the dataframe; has to have a column named \"score\"",
    ": an optional factory to use for labeling",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the prediction target",
    ": the other column's fixed IDs",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": the ID-based triples, shape: (n, 3)",
    ": the scores",
    "3-tuple for return",
    "extract label information, if possible",
    "no restriction",
    "restriction is a tensor",
    "restriction is a sequence of integers or strings",
    "now, restriction is a sequence of integers",
    "if explicit ids have been given, and label information is available, extract list of labels",
    "exactly one of them is None",
    "create input batch",
    "note type alias annotation required,",
    "cf. https://mypy.readthedocs.io/en/stable/common_issues.html#variables-vs-type-aliases",
    "batch, TODO: ids?",
    "docstr-coverage: inherited",
    "initialize buffer on device",
    "docstr-coverage: inherited",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "determine corresponding indices",
    "batch_id, score_id = divmod(top_indices, num_scores)",
    "combine to top triples",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "docstr-coverage: inherited",
    "TODO: variable targets across batches/samples?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "(?, r, t) => r.stride > t.stride",
    "(h, ?, t) => h.stride > t.stride",
    "(h, r, ?) => h.stride > r.stride",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "train model; note: needs larger number of epochs to do something useful ;-)",
    "create prediction dataset, where the head entities is from a set of European countries,",
    "and the relations are connected to tourism",
    "calculate all scores for this restricted set, and keep k=3 largest",
    "add labels",
    ": the choices for the first and second component of the input batch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "calculate batch scores onces",
    "consume by all consumers",
    "TODO: Support partial dataset",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "exactly one of them is None",
    "",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "get input & target",
    "get label-to-id mapping and prediction targets",
    "get scores",
    "create raw dataframe",
    "note: the models' predict method takes care of setting the model to evaluation mode",
    "normalize input",
    "calculate scores (with automatic memory optimization)",
    "-*- coding: utf-8 -*-",
    "\"Closed-Form Expectation\",",
    "\"Closed-Form Variance\",",
    "\"\u2713\" if metric.closed_expectation else \"\",",
    "\"\u2713\" if metric.closed_variance else \"\",",
    "Add HPO command",
    "Add NodePiece tokenization command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    "entity alignment sides",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "entity alignment",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "infer shape",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "input normalization",
    "note: the base class does not have any parameters",
    "Heuristic for default value",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: the only parameters are inside the relation representation module, which has its own reset_parameters",
    "docstr-coverage: inherited",
    "TODO: can we change the dimension order to make this contiguous?",
    "docstr-coverage: inherited",
    "normalize num blocks",
    "determine necessary padding",
    "determine block sizes",
    "(R, nb, bsi, bso)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(n, di) -> (n, nb, bsi)",
    "(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)",
    "(R, n, nb, bso) -> (R * n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "(n, R * n), (R * n, do) -> (n, do)",
    "remove padding if necessary",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(R * n, n), (n, di) -> (R * n, di)",
    "(R * n, di) -> (R, n, nb, bsi)",
    "(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)",
    "(n, nb, bso) -> (n, do)",
    "note: depending on the contracting order, the output may supporting viewing, or not",
    "remove padding if necessary",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "TODO: we could cache the stacked adjacency matrices",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "input validation",
    "has to be imported now to avoid cyclic imports",
    "has to be assigned after call to nn.Module init",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "heuristic",
    "normalize *before* repeating",
    "repeat if necessary",
    "regularize *after* repeating",
    "dropout & regularizer will appear automatically, since it is a nn.Module",
    "has to be imported here to avoid cyclic import",
    "docstr-coverage: inherited",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "docstr-coverage: inherited",
    "initialize weights in-place",
    "docstr-coverage: inherited",
    "apply constraints in-place",
    "fixme: work-around until nn.Embedding supports complex",
    "docstr-coverage: inherited",
    "fixme: work-around until nn.Embedding supports complex",
    "verify that contiguity is preserved",
    "create low-rank approximation object",
    "get base representations, shape: (n, *ds)",
    "calculate SVD, U.shape: (n, k), s.shape: (k,), u.shape: (k, prod(ds))",
    "overwrite bases and weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "TODO: might not be true for all compositions",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "docstr-coverage: inherited",
    "check max_id",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "docstr-coverage: inherited",
    ": the base representations",
    ": the combination module",
    "input normalization",
    "has to be imported here to avoid cyclic import",
    "create base representations",
    "verify same ID range",
    "note: we could also relax the requiremen, and set max_id = min(max_ids)",
    "shape inference",
    "assign base representations *after* super init",
    "docstr-coverage: inherited",
    "delegate to super class",
    "Generate graph dataset from the Monarch Disease Ontology (MONDO)",
    ": the assignment from global ID to (representation, local id), shape: (max_id, 2)",
    "import here to avoid cyclic import",
    "instantiate base representations if necessary",
    "there needs to be at least one base",
    "while possible, this might be unintended",
    "extract shape",
    "check for invalid base ids",
    "check for invalid local indices",
    "assign modules / buffers *after* super init",
    "docstr-coverage: inherited",
    "flatten assignment to ease construction of inverse indices",
    "we group indices by the representation which provides them",
    "thus, we need an inverse to restore the correct order",
    "get representations",
    "update inverse indices",
    "invert flattening",
    "import here to avoid cyclic import",
    "comment: not all representations support passing a shape parameter",
    "create assignment",
    "base",
    "other",
    "import here to avoid cyclic import",
    "infer shape",
    "infer max_id",
    "docstr-coverage: inherited",
    "TODO: can be a combined representations, with appropriate tensor-train combination",
    ": shape: (max_id, num_cores)",
    ": the bases, length: num_cores, with compatible shapes",
    "check shape",
    "check value range",
    "do not increase counter i, since the dimension is shared with the following term",
    "i += 1",
    "ids //= m_i",
    "import here to avoid cyclic import",
    "normalize ranks",
    "determine M_k, N_k",
    "TODO: allow to pass them from outside?",
    "normalize assignment",
    "determine shapes and einsum equation",
    "create base representations",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "abstract",
    "concrete classes",
    "default flow",
    ": the message passing layers",
    ": the flow direction of messages across layers",
    ": the edge index, shape: (2, num_edges)",
    "fail if dependencies are missing",
    "avoid cyclic import",
    "the base representations, e.g., entity embeddings or features",
    "verify max_id",
    "verify shape",
    "assign sub-module *after* super call",
    "initialize layers",
    "normalize activation",
    "check consistency",
    "buffer edge index for message passing",
    "TODO: inductiveness; we need to",
    "* replace edge_index",
    "* replace base representations",
    "* keep layers & activations",
    "docstr-coverage: inherited",
    "we can restrict the message passing to the k-hop neighborhood of the desired indices;",
    "this does only make sense if we do not request *all* indices",
    "k_hop_subgraph returns:",
    "(1) the nodes involved in the subgraph",
    "(2) the filtered edge_index connectivity",
    "(3) the mapping from node indices in node_idx to their new location, and",
    "(4) the edge mask indicating which edges were preserved",
    "we only need the base representations for the neighbor indices",
    "get *all* base representations",
    "use *all* edges",
    "perform message passing",
    "select desired indices",
    "docstr-coverage: inherited",
    ": the edge type, shape: (num_edges,)",
    "register an additional buffer for the categorical edge type",
    "docstr-coverage: inherited",
    ": the relation representations used to obtain initial edge features",
    "avoid cyclic import",
    "docstr-coverage: inherited",
    "get initial relation representations",
    "select edge attributes from relation representations according to relation type",
    "perform message passing",
    "apply relation transformation, if necessary",
    "-*- coding: utf-8 -*-",
    "Classes",
    "Resolver",
    "backwards compatibility",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "must be cloned if we want to do backprop",
    "the color initializer",
    "variants for the edge index",
    "additional parameters for iter_weisfeiler_lehman",
    "normalize shape",
    "get coloring",
    "make color initializer",
    "initialize color representations",
    "note: this could be a representation?",
    "init entity representations according to the color",
    "create random walk matrix",
    "stack diagonal entries of powers of rw",
    "abstract",
    "concrete",
    "docstr-coverage: inherited",
    "tokenize",
    "pad",
    "get character embeddings",
    "pool",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "backward compatibility with RGCN",
    "docstr-coverage: inherited",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    "-*- coding: utf-8 -*-",
    "Caches",
    "if the sparsity becomes too low, convert to a dense matrix",
    "note: this heuristic is based on the memory consumption,",
    "for a sparse matrix, we store 3 values per nnz (row index, column index, value)",
    "performance-wise, it likely makes sense to switch even earlier",
    "`torch.sparse.mm` can also deal with dense 2nd argument",
    "note: torch.sparse.mm only works for COO matrices;",
    "@ only works for CSR matrices",
    "convert to COO, if necessary",
    "we need to use indices here, since there may be zero diagonal entries",
    ": Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing",
    "cf. https://meta.wikimedia.org/wiki/User-Agent_policy",
    "cf. https://wikitech.wikimedia.org/wiki/Robot_policy",
    "break into smaller requests",
    "try to load cached first",
    "determine missing entries",
    "retrieve information via SPARQL",
    "save entries",
    "fill missing descriptions",
    "for mypy",
    "get labels & descriptions",
    "compose labels",
    "we can have multiple images per entity -> collect image URLs per image",
    "entity ID",
    "relation ID",
    "image URL",
    "check whether images are still missing",
    "select on image url per image in a reproducible way",
    "traverse relations in order of preference",
    "now there is an image available -> select reproducible by URL sorting",
    "did not break -> no image",
    "This import doesn't need a wrapper since it's a transitive",
    "requirement of PyOBO",
    "darglint does not like",
    "raise cls(shape=shape, reference=reference)",
    "1 * ? = ?; ? * 1 = ?",
    "i**2 = j**2 = k**2 = -1",
    "i * j = k; i * k = -j",
    "j * i = -k, j * k = i",
    "k * i = j; k * j = -i",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "-*- coding: utf-8 -*-",
    "REPRESENTATION",
    "base",
    "concrete",
    "INITIALIZER",
    "INTERACTIONS",
    "Adapter classes",
    "Concrete Classes",
    "combinations",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different.",
    ": Otherwise, the entity_shape is used for head & tail entities",
    ": The symbolic shapes for relation representations",
    "if the interaction function's head parameter should only receive a subset of entity representations",
    "if the interaction function's tail parameter should only receive a subset of entity representations",
    ": the interaction's value range (for unrestricted input)",
    "TODO: annotate modelling capabilities? cf., e.g., https://arxiv.org/abs/1902.10197, Table 2",
    "TODO: annotate properties, e.g., symmetry, and use them for testing?",
    "TODO: annotate complexity?",
    "TODO: cannot cover dynamic shapes, e.g., AutoSF",
    "TODO: we could change that to slicing along multiple dimensions, if necessary",
    ": The functional interaction form",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: update class docstring",
    "TODO: give this a better name?",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    "vector & scalar offset",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "normalize kernel height",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The interaction model",
    "docstr-coverage: inherited",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "docstr-coverage: inherited",
    "normalize hidden_dim",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Initialize biases with zero",
    "In the original formulation,",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "default core tensor initialization",
    "cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "normalize initializer",
    "normalize relation dimension",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "docstr-coverage: inherited",
    "instantiate here to make module easily serializable",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "there are separate biases for entities in head and tail position",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "with k=4",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "docstr-coverage: inherited",
    "input normalization",
    "Core tensor",
    "docstr-coverage: inherited",
    "initialize core tensor",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "r_head, r_mid, r_tail",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "type alias for AutoSF block description",
    "head_index, relation_index, tail_index, sign",
    ": a description of the block structure",
    "convert to tuple",
    "infer the number of entity and relation representations",
    "verify coefficients",
    "dynamic entity / relation shapes",
    "docstr-coverage: inherited",
    "r_head, r_bias, r_tail",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: excused `wrapped`",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "cat into shape (..., 1, d, 3)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "shortcut for same shape",
    "split weight into head-/relation-/tail-specific sub-matrices",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace",
    "ensure constraints",
    "x_1 contraction",
    "x_2 contraction",
    "TODO: this sign is in the official code, too, but why do we need it?",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "head",
    "relation",
    "tail",
    "extension: negative (power) norm",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "input normalization",
    "instantiate separate combinations",
    "docstr-coverage: inherited",
    "split complex; repeat real",
    "separately combine real and imaginary parts",
    "combine",
    "docstr-coverage: inherited",
    "symbolic output to avoid dtype issue",
    "we only need to consider real part here",
    "the gate",
    "the combination",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "docstr-coverage: inherited",
    "sort by decreasing degree",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "sort by decreasing page rank",
    "docstr-coverage: inherited",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": A list with ratios per representation in their creation order,",
    ": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`",
    ": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "Create an MLP for string aggregation",
    "note: the token representations' shape includes the number of tokens as leading dim",
    "unique hashes per representation",
    "unique hashes if we concatenate all representations together",
    "TODO: vectorization?",
    "remove self-loops",
    "add inverse edges and remove duplicates",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "TODO: expose num_anchors?",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "To prevent possible segfaults in the METIS C code, METIS expects a graph",
    "(1) without self-loops; (2) with inverse edges added; (3) with unique edges only",
    "https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18",
    "select independently per partition",
    "select adjacency part;",
    "note: the indices will automatically be in [0, ..., high - low), since they are *local* indices",
    "offset",
    "the -1 comes from the shared padding token",
    "note: permutation will be later on reverted",
    "add back 1 for the shared padding token",
    "TODO: check if perm is used correctly",
    "verify pool",
    "docstr-coverage: inherited",
    "choose first num_tokens",
    "TODO: vectorization?",
    "heuristic",
    "heuristic",
    "calculate configuration digest",
    "create anchor selection instance",
    "select anchors",
    "anchor search (=anchor assignment?)",
    "assign anchors",
    "save",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "ensure parent directory exists",
    "save via torch.save",
    "docstr-coverage: inherited",
    "TODO: since we save a contiguous array of (num_entities, num_anchors),",
    "it would be more efficient to not convert to a mapping, but directly select from the tensor",
    "-*- coding: utf-8 -*-",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "Data containers",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "convert to adjacency matrix",
    "convert to scipy sparse csr",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "TODO: padding for unreachable?",
    "select anchor IDs with smallest distance",
    "docstr-coverage: inherited",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "anchor nodes have themselves as a starting found anchor",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output that track the distance to each found anchor",
    "dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)",
    "initial anchors are 0-hop away from themselves",
    "propagate one hop",
    "TODO the float() trick for GPU result stability until the torch_sparse issue is resolved",
    "https://github.com/rusty1s/pytorch_sparse/issues/243",
    "convergence check",
    "newly reached is a mask that points to newly discovered anchors at this particular step",
    "implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)",
    "in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "update the value in the pool by the current hop value (we start from 0, so +1 be default)",
    "stop once we have enough",
    "sort the pool by nearest to farthest anchors",
    "values with distance 255 (or max for unsigned int8 type) are padding tokens",
    "since the output is sorted, no need for random sampling, we just take top-k nearest",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "select k anchors with largest ppr, shape: (batch_size, k)",
    "prepare adjacency matrix only once",
    "prepare result",
    "progress bar?",
    "batch-wise computation of PPR",
    "run page-rank calculation, shape: (batch_size, n)",
    "select PPR values for the anchors, shape: (batch_size, num_anchors)",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "should be exactly 1.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2",
    "use V[x] = E[x^2] - E[x]^2",
    "group by same weight -> compute H_w(n) for multiple n at once",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective",
    "cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: should we return the sum of weights?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "-*- coding: utf-8 -*-",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    ": whether the metric supports weights",
    ": whether there is a closed-form solution of the expectation",
    ": whether there is a closed-form solution of the variance",
    "normalize weights",
    "calculate weighted harmonic mean",
    "calculate cdf",
    "determine value at p=0.5",
    "special case for exactly 0.5",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A description of the metric",
    ": The function that runs the metric",
    "docstr-coverage: inherited",
    ": Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics",
    ": This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`",
    "TODO there's something wrong with this, so add it later",
    "classifier_annotator.higher(",
    "rmc.pr_auc_score,",
    "name=\"AUC-PR\",",
    "description=\"Area Under the Precision-Recall Curve\",",
    "link=\"https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score\",",
    ")",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "noqa:DAR101",
    "noqa:DAR401",
    "Create directory in which all experimental artifacts are saved",
    "noqa:DAR101",
    "clip for node piece configurations",
    "\"pykeen experiments reproduce\" expects \"model reference dataset\"",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: take care that triples aren't removed that are the only ones with any given entity",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "save literal-to-id mapping",
    "save numeric literals",
    "load literal-to-id",
    "load literals",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    ": the number of unique entities",
    ": the number of relations (maybe including \"artificial\" inverse relations)",
    ": whether to create inverse triples",
    ": the number of real relations, i.e., without artificial inverses",
    "ensure torch.Tensor",
    "input validation",
    "always store as torch.long, i.e., torch's default integer dtype",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "docstr-coverage: inherited",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "note: num_relations will be doubled again when instantiating with create_inverse_triples=True",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "docstr-coverage: inherited",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "Generate a word cloud image",
    "docstr-coverage: inherited",
    "vectorized label lookup",
    "Re-order columns",
    "docstr-coverage: inherited",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "ID-based triples",
    "labeled triples",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "triples factory",
    "all keyword-based options have been none",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "delegate to keyword-based get_mapped_triples to re-use optional validation logic",
    "only labeled triples are remaining",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "docstr-coverage: inherited",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Make sure that the first element has all the right stuff in it",
    "docstr-coverage: inherited",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "noqa:DAR202",
    "noqa:DAR401",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "docstr-coverage: inherited",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "docstr-coverage: inherited",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest",
    "in practice, this should have a minor effect, since typically batch_size << num_instances",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "disable automatic batching",
    "docstr-coverage: inherited",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "lazy init",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Split batch components",
    "Send batch to device",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "note: we use Tuple[Tensor] here, so we can re-use TensorDataset instead of having to create a custom one",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "unpack",
    "Send batch to device",
    "head prediction",
    "TODO: exploit sparsity",
    "note: this is different to what we do for LCWA, where we collect *all* training entities",
    "for which the combination is true",
    "tail prediction",
    "TODO: exploit sparsity",
    "regularization",
    "docstr-coverage: inherited",
    "TODO?",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    ": the path to the weights of the best model",
    ": whether to delete the file with the best model weights after termination",
    ": note: the weights will be re-loaded into the model before",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "for mypy",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "dataset",
    "model",
    "stored outside of the training loop / optimizer to give access to auto-tuning from Lightning",
    "optimizer",
    "TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA",
    "docstr-coverage: inherited",
    "call post_parameter_update",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "TODO:",
    "shuffle=shuffle,",
    "drop_last=drop_last,",
    "sampler=sampler,",
    "shuffle=shuffle,",
    "disable automatic batching in data loader",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "note: since this file is executed via __main__, its module name is replaced by __name__",
    "hence, the two classes' fully qualified names start with \"_\" and are considered private",
    "cf. https://github.com/cthoyt/class-resolver/issues/39",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "mixed precision training",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    ": The metric key",
    ": Side of the metric, or \"both\"",
    ": The rank type",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "normalize keys",
    "TODO: this can only normalize rank-based metrics!",
    "TODO: find a better way to handle this",
    "-*- coding: utf-8 -*-",
    "TODO: fix this upstream / make metric.score comply to signature",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Because the order of the values of an dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    ": the scores of the true choice, shape: (*bs), dtype: float",
    ": the number of scores which were larger than the true score, shape: (*bs), dtype: long",
    ": the number of scores which were not smaller than the true score, shape: (*bs), dtype: long",
    ": the total number of compared scores, shape: (*bs), dtype: long",
    "-*- coding: utf-8 -*-",
    "TODO remove this, it makes code much harder to reason about",
    "add mode parameter",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "do not display progress bar while searching",
    "start by searching for batch_size",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "we do not need to repeat time-consuming checks",
    "infer start value",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r), (h, t) or (r, t)) scored on all",
    "entities/relations, must have failed to start slice_size search, we start with trying half the",
    "entities/relations.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)",
    "otherwise, i.e., without inverse triples, we also need score_h",
    "if relations are to be predicted, we need to slice score_r",
    "raise an error, if any of the required methods cannot slice",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Send to device",
    "Ensure evaluation mode",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optionally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "-*- coding: utf-8 -*-",
    "Evaluation loops",
    "Evaluation datasets",
    ": the MemoryUtilizationMaximizer instance for :func:`_evaluate`.",
    "batch",
    "tqdm",
    "data loader",
    "set upper limit of batch size for automatic memory optimization",
    "set model to evaluation mode",
    "delegate to AMO wrapper",
    "The key-id for each triple, shape: (num_triples,)",
    ": the number of targets for each key, shape: (num_unique_keys + 1,)",
    ": the concatenation of unique targets for each key (use bounds to select appropriate sub-array)",
    "input verification",
    "group key = everything except the prediction target",
    "initialize data structure",
    "group by key",
    "convert lists to arrays",
    "instantiate",
    "return indices corresponding to the `item`-th triple",
    "input normalization",
    "prepare filter indices if required",
    "sorted by target -> most of the batches only have a single target",
    "group by target",
    "stack groups into a single tensor",
    "avoid cyclic imports",
    "TODO: it would be better to allow separate batch sizes for entity/relation prediction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is",
    "not shuffled, and contains evaluation ranking tasks sorted by target",
    "TODO: in theory, we could make a single score calculation for e.g.,",
    "{(h, r, t1), (h, r, t1), ..., (h, r, tk)}",
    "predict scores for all candidates",
    "filter scores",
    "extract true scores",
    "replace by nan",
    "rewrite true scores",
    "create dense positive masks",
    "TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around",
    "delegate processing of scores to the evaluator",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "note: OGB's evaluator needs a dataset name as input, and uses it to lookup the standard evaluation",
    "metric. we do want to support user-selected metrics on arbitrary datasets instead",
    "this setting is equivalent to the WikiKG2 setting, and will calculate MRR *and* H@k for k in {1, 3, 10}",
    "filter supported metrics",
    "prepare input format, cf. `evaluator.expected_input``",
    "y_pred_pos: shape: (num_edge,)",
    "y_pred_neg: shape: (num_edge, num_nodes_neg)",
    "iterate over prediction targets",
    "pre-allocate",
    "TODO: maybe we want to collect scores on CPU / add an option?",
    "iterate over batches",
    "combine ids, shape: (batch_size, num_negatives + 1)",
    "get scores, shape: (batch_size, num_negatives + 1)",
    "store positive and negative scores",
    "cf. https://github.com/snap-stanford/ogb/pull/357",
    "combine to input dictionary",
    "delegate to OGB evaluator",
    "post-processing",
    "normalize name",
    "OGB does not aggregate values across triples",
    "-*- coding: utf-8 -*-",
    "flatten dictionaries",
    "individual side",
    "combined",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "docstr-coverage: inherited",
    "TODO: do not require to compute all scores beforehand",
    "cf. Model.score_t(ts=...)",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "combine key batches",
    "calculate key frequency",
    "weight = inverse frequency",
    "broadcast to samples",
    "docstr-coverage: inherited",
    "store keys for calculating macro weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute macro weights",
    "note: we wrap the array into a list to be able to re-use _iter_ranks",
    "calculate weighted metrics",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "docstr-coverage: inherited",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "TODO: support \"broadcasting\" representation regularizers?",
    "e.g. re-use the same regularizer for everything; or",
    "pass a dictionary with keys \"entity\"/\"relation\";",
    "values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "note, triples_factory is required instead of just using self.num_entities",
    "and self.num_relations for the inductive case when this is different",
    "instantiate regularizer",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "batch normalization modules use batch statistics in training mode",
    "-> different batch divisions lead to different results",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": the number of entities",
    ": the number of relations",
    ": whether to use inverse relations",
    ": utility for generating inverse relations",
    ": When predict_with_sigmoid is set to True, the sigmoid function is",
    ": applied to the logits during evaluation and also for predictions",
    ": after training, but has no effect on the training.",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Meta Models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "note: we need to share the aggregation across representations, since the aggregation may have",
    "trainable parameters",
    ": a mapping from inductive mode to corresponding entity representations",
    ": note: there may be duplicate values, if entity representations are shared between validation and testing",
    "inductive factories",
    "entity representation kwargs may contain a triples factory, which needs to be replaced",
    "entity_representations_kwargs.pop(\"triples_factory\", None)",
    "note: this is *not* a nn.ModuleDict; the modules have to be registered elsewhere",
    "shared",
    "non-shared",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the indexed filter triples, i.e., sparse masks",
    "avoid cyclic imports",
    "create base model",
    "assign *after* nn.Module.__init__",
    "save constants",
    "index triples",
    "initialize base model's parameters",
    "get masks, shape: (batch_size, num_entities/num_relations)",
    "combine masks",
    "note: * is an elementwise and, and + and elementwise or",
    "get non-zero entries",
    "set scores for fill value for every non-occuring entry",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "NodePiece",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "entity embedding",
    "ConvE uses one bias for each entity",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the default loss function is the self-adversarial negative sampling loss",
    ": The default parameters for the default loss function class",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "note: DistMult only regularizes the relation embeddings;",
    "entity embeddings are hard constrained instead",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "translation vector in hyperplane",
    "normal vector of hyperplane",
    "normalise the normal vectors to unit l2 length",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "thus, we need to use a weight regularizer instead of having an Embedding regularizer,",
    "which only regularizes the weights used in a batch",
    "note: the following is already the default",
    "default_regularizer=self.regularizer_default,",
    "default_regularizer_kwargs=self.regularizer_default_kwargs,",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "relation embedding",
    "relation projection",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "input normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "the individual combination for real/complex parts",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "no activation",
    "-*- coding: utf-8 -*-",
    ": the interaction class (for generating the overview table)",
    "added by ERModel",
    "max_id=triples_factory.num_entities,",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "docstr-coverage: inherited",
    ": The column names",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    "docstr-coverage: inherited",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "docstr-coverage: inherited",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "docstr-coverage: inherited",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "docstr-coverage: inherited",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "file names for storing results",
    "TODO: rename param?",
    "always save results as json file",
    "save other components only if requested (which they are, by default)",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "TODO: checkpoint_dict not further used; later loaded again by TrainingLoop.train",
    "TODO: allow empty validation / testing",
    "evaluation restriction to a subset of entities/relations",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "8. Evaluation",
    "7. Training (ronaldo style)",
    "Misc",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Misc",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "Start tracking",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "cf. also https://github.com/pykeen/pykeen/issues/1071",
    "TODO: use a class-resolver?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "convert class to string to use caching",
    "Assume it's a file path",
    "note: we only need to set the create_inverse_triples in the training factory.",
    "normalize dataset kwargs",
    "enable passing force option via dataset_kwargs",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "TODO: maybe merge into analyze / make sub-command",
    "only save full data",
    "Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)",
    "Plot: difference between mean head and tail degree",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Utilities",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    "docstr-coverage: inherited",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "TODO: use class-resolver normalize?",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ZENODO_URL = \"https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip\"",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "generate subfolders 'training' and  'inference'",
    "TODO: use class-resolver normalize?",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    "Datasets",
    "-*- coding: utf-8 -*-",
    "graph pairs",
    "graph sizes",
    "graph versions",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "ensure zip file is present",
    "save relative paths beforehand so they are present for loading",
    "delegate to super class",
    "docstr-coverage: inherited",
    "left side has files ending with 1, right side with 2",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "input validation",
    "store *before* calling super to have it available when loading the graphs",
    "ensure zip file is present",
    "shared directory for multiple datasets.",
    "docstr-coverage: inherited",
    "create triples factory",
    "docstr-coverage: inherited",
    "load mappings for both sides",
    "load triple alignments",
    "extract entity alignments",
    "(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2",
    "TODO: support ID-only graphs",
    "load both graphs",
    "load alignment",
    "drop duplicates",
    "combine",
    "store for repr",
    "split",
    "create inverse triples only for training",
    "docstr-coverage: inherited",
    "base",
    "concrete",
    "Abstract class",
    "Concrete classes",
    "Data Structures",
    "a buffer for the triples",
    "the offsets",
    "normalization",
    "append shifted mapped triples",
    "update offsets",
    "merge labels with same ID",
    "for mypy",
    "reconstruct label-to-id",
    "optional",
    "merge entity mapping",
    "merge relation mapping",
    "convert labels to IDs",
    "map labels, using -1 as fill-value for invalid labels",
    "we cannot drop them here, since the two columns need to stay aligned",
    "filter alignment",
    "map alignment from old IDs to new IDs",
    "determine swapping partner",
    "only keep triples where we have a swapping partner",
    "replace by swapping partner",
    ": the merged id-based triples, shape: (n, 3)",
    ": the updated alignment, shape: (2, m)",
    ": additional keyword-based parameters for adjusting label-to-id mappings",
    "concatenate triples",
    "filter alignment and translate to IDs",
    "process",
    "TODO: restrict to only using training alignments?",
    "merge mappings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "add swap triples",
    "e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)",
    "create dense entity remapping for swap",
    "add swapped triples",
    "swap head",
    "swap tail",
    ": the name of the additional alignment relation",
    "docstr-coverage: inherited",
    "add alignment triples with extra relation",
    "docstr-coverage: inherited",
    "determine connected components regarding the same-as relation (i.e., applies transitivity)",
    "apply id mapping",
    "ensure consecutive IDs",
    "only use training alignments?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "noqa: DAR101",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.9.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "See discussion for adding huggingface intersphinx docs at",
    "https://github.com/huggingface/transformers/issues/14728#issuecomment-1133521776",
    "autodoc_member_order = 'bysource'",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "check if within 0.5 std of observed",
    "test error is raised",
    "there is an extra test for this case",
    "docstr-coverage: inherited",
    "same size tensors",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Tests that exception will be thrown when more than or less than two tensors are passed",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "ensure each node participates in at least one edge",
    "check type and shape",
    "number of colors is monotonically increasing",
    "ensure each node participates in at least one edge",
    "normalize",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "See https://github.com/pykeen/pykeen/pull/883",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "cf. https://docs.pytest.org/en/7.1.x/example/parametrize.html#parametrizing-conditional-raising",
    "wrong ndim",
    "wrong last dim",
    "wrong dtype: float",
    "wrong dtype: complex",
    "correct",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "quaternion have four components",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "create triples factory with inverse relations",
    "head prediction via inverse tail prediction",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "verify that the regularizer is stored for both, entity and relation representations",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "regularization term should be zero",
    "updated should be set to false",
    "call method",
    "generate random tensors",
    "generate inputs",
    "call update",
    "check shape",
    "check result",
    "generate single random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "update term",
    "check that the expected term is returned",
    "check that the regularizer is now reset",
    "create another instance with apply_only_once enabled",
    "test initial state",
    "after first update, should change the term",
    "after second update, no change should happen",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "TODO: look into score_r for inverse relations",
    "clear buffers for message passing models",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    "this implicitly tests extra_repr / iter_extra_repr",
    "select random indices",
    "forward pass with full graph",
    "forward pass with restricted graph",
    "verify the results are similar",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": the input dimension",
    ": the output dimension",
    ": the number of entities",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    ": the interaction to use for testing a model",
    "initializers *may* work in-place => clone",
    "actual number may be different...",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    "inferred from triples factory",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "generate random weights such that sum = n",
    "for sanity checking: give the largest weight to best rank => should improve",
    "generate two versions",
    "1. repeat each rank/candidate pair a random number of times",
    "2. do not repeat, but assign a corresponding weight",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "generate random triples factories",
    "generate random alignment",
    "add label information if necessary",
    "prepare alignment data frame",
    "call",
    "check",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "verify that the input is valid",
    "combine",
    "verify shape",
    "to be initialize in subclass",
    "TODO: check subset",
    "-*- coding: utf-8 -*-",
    "TODO: this could be shared with the model tests",
    "FixedModel: dict(embedding_dim=EMBEDDING_DIM),",
    "test combinations of models with training loops",
    "some models require inverse relations",
    "some model require access to the training triples",
    "inductive models require an inductive mode to be set, and an inference factory to be passed",
    "fake an inference factory",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "sum over the softmax dim as weights sum up to 1",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "abstract classes",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "mock prediction data frame",
    "set other parameters",
    "mock prediction data frame",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "clear",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "extra-relation",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "additional",
    "verify shape",
    "verify dtype",
    "verify number of entities/relations",
    "verify offsets",
    "create old, new pairs",
    "simulate merging ids",
    "only a single pair",
    "apply",
    "every key is contained",
    "value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize rotations to unit modulus",
    "check for unit modulus",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "-*- coding: utf-8 -*-",
    "message_dim must be divisible by num_heads",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "-*- coding: utf-8 -*-",
    "complex tensor",
    "check value range",
    "check modulus == 1",
    "quaternion needs dim divisible by 4",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "value range",
    "highest degree node has largest value",
    "Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf",
    "create triples with a dummy relation type 0",
    "0: green: 2, 3, 7, 8",
    "1: red: 1, 4, 6, 9",
    "2: blue: 0, 5",
    "the example includes the first power",
    "requires at least one complex tensor as input",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "inferred from triples factory",
    "inferred from assignment",
    "the representation module infers the max_id from the provided labels",
    "the following entity does not have an image -> will have to use backfill",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "the representation module infers the max_id from the provided labels",
    "max_id is inferred from assignment",
    "create random assignment",
    "update kwargs",
    "empty bases",
    "inconsistent base shapes",
    "invalid base id",
    "invalid local index",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "TODO this is the only place this function is used.",
    "Is there an alternative so we can remove it?",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "x_i ~ N(mu_i, 1)",
    "closed-form solution",
    "sampled confidence interval",
    "check that closed-form is in confidence interval of sampled",
    "positive values only",
    "positive and negative values",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "check correct num_entities",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "-*- coding: utf-8 -*-",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": synonyms of this loss",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Sanity check",
    "prepare for broadcasting, shape: (batch_size, 1, 3)",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "docstr-coverage: inherited",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "docstr-coverage: inherited",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "docstr-coverage: inherited",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "docstr-coverage: inherited",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "docstr-coverage: inherited",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "make sure labels form a proper probability distribution",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "subtract margin from positive scores",
    "divide by temperature",
    "docstr-coverage: inherited",
    "subtract margin from positive scores",
    "normalize positive score shape",
    "divide by temperature",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "we pass *all* scores as negatives, but set the weight of positives to zero",
    "this allows keeping a dense shape",
    "docstr-coverage: inherited",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "fill negative scores with some finite value, e.g., 0 (they will get masked out anyway)",
    "note: this is a reduction along the softmax dim; since the weights are already normalized",
    "to sum to one, we want a sum reduction here, instead of using the self._reduction",
    "docstr-coverage: inherited",
    "Sanity check",
    "docstr-coverage: inherited",
    "Sanity check",
    "negative loss part",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "TODO: maybe we can make this more efficient?",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "TODO delete when deleting _normalize_dim (below)",
    "TODO delete when deleting convert_to_canonical_shape (below)",
    "TODO delete? See note in test_sim.py on its only usage",
    "upgrade to sequence",
    "broadcast",
    "normalize ids: -> ids.shape: (batch_size, num_ids)",
    "normalize batch -> batch.shape: (batch_size, 1, 3)",
    "allocate memory",
    "copy ids",
    "reshape",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "ensure pathlib",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "cf. https://stackoverflow.com/a/1176023",
    "check validity",
    "path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "resolve path to make sure it is an absolute path",
    "ensure directory exists",
    "message passing: collect colors of neighbors",
    "dense colors: shape: (n, c)",
    "adj:          shape: (n, n)",
    "values need to be float, since torch.sparse.mm does not support integer dtypes",
    "size: will be correctly inferred",
    "concat with old colors",
    "hash",
    "create random indicator functions of low dimensionality",
    "collect neighbors' colors",
    "round to avoid numerical effects",
    "hash first",
    "concat with old colors",
    "re-hash",
    "only keep connectivity, but remove multiplicity",
    "note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,",
    "this is rather useless",
    "initial: degree",
    "note: we calculate this separately, since we can use a more efficient implementation for the first step",
    "hash",
    "determine small integer type for dense count array",
    "convergence check",
    "each node has a unique color",
    "the number of colors did not improve in the last iteration",
    "cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "docstr-coverage: inherited",
    "no need to compute anything",
    "docstr-coverage: inherited",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "regularizer-specific parameters",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "orthogonality soft constraint: cosine similarity at most epsilon",
    "The normalization factor to balance individual regularizers' contribution.",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "\"Closed-Form Expectation\",",
    "\"Closed-Form Variance\",",
    "\"\u2713\" if metric.closed_expectation else \"\",",
    "\"\u2713\" if metric.closed_variance else \"\",",
    "Add HPO command",
    "Add NodePiece tokenization command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    "entity alignment sides",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "entity alignment",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "infer shape",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "input normalization",
    "note: the base class does not have any parameters",
    "Heuristic for default value",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: the only parameters are inside the relation representation module, which has its own reset_parameters",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "normalize num blocks",
    "determine necessary padding",
    "determine block sizes",
    "(R, nb, bsi, bso)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(n, di) -> (n, nb, bsi)",
    "(n, nb, bsi), (R, nb, bsi, bso) -> (R, n, nb, bso)",
    "(R, n, nb, bso) -> (R * n, do)",
    "TODO: can we change the dimension order to make this contiguous?",
    "(n, R * n), (R * n, do) -> (n, do)",
    "remove padding if necessary",
    "docstr-coverage: inherited",
    "apply padding if necessary",
    "(R * n, n), (n, di) -> (R * n, di)",
    "(R * n, di) -> (R, n, nb, bsi)",
    "(R, nb, bsi, bso), (R, n, nb, bsi) -> (n, nb, bso)",
    "(n, nb, bso) -> (n, do)",
    "remove padding if necessary",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "TODO: we could cache the stacked adjacency matrices",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "input validation",
    "has to be imported now to avoid cyclic imports",
    "has to be assigned after call to nn.Module init",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "heuristic",
    "normalize *before* repeating",
    "repeat if necessary",
    "regularize *after* repeating",
    "dropout & regularizer will appear automatically, since it is a nn.Module",
    "has to be imported here to avoid cyclic import",
    "docstr-coverage: inherited",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "docstr-coverage: inherited",
    "initialize weights in-place",
    "docstr-coverage: inherited",
    "apply constraints in-place",
    "fixme: work-around until nn.Embedding supports complex",
    "docstr-coverage: inherited",
    "fixme: work-around until nn.Embedding supports complex",
    "verify that contiguity is preserved",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "TODO: might not be true for all compositions",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "docstr-coverage: inherited",
    "check max_id",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "docstr-coverage: inherited",
    ": the base representations",
    ": the combination module",
    "input normalization",
    "has to be imported here to avoid cyclic import",
    "create base representations",
    "verify same ID range",
    "note: we could also relax the requiremen, and set max_id = min(max_ids)",
    "shape inference",
    "assign base representations *after* super init",
    "docstr-coverage: inherited",
    "set up cache",
    "get labels & descriptions",
    "compose labels",
    "delegate to super class",
    ": the assignment from global ID to (representation, local id), shape: (max_id, 2)",
    "import here to avoid cyclic import",
    "instantiate base representations if necessary",
    "there needs to be at least one base",
    "while possible, this might be unintended",
    "extract shape",
    "check for invalid base ids",
    "check for invalid local indices",
    "assign modules / buffers *after* super init",
    "docstr-coverage: inherited",
    "flatten assignment to ease construction of inverse indices",
    "we group indices by the representation which provides them",
    "thus, we need an inverse to restore the correct order",
    "get representations",
    "update inverse indices",
    "invert flattening",
    "import here to avoid cyclic import",
    "comment: not all representations support passing a shape parameter",
    "create assignment",
    "base",
    "other",
    "import here to avoid cyclic import",
    "infer shape",
    "infer max_id",
    "docstr-coverage: inherited",
    "TODO: can be a combined representations, with appropriate tensor-train combination",
    ": shape: (max_id, num_cores)",
    ": the bases, length: num_cores, with compatible shapes",
    "check shape",
    "check value range",
    "do not increase counter i, since the dimension is shared with the following term",
    "i += 1",
    "ids //= m_i",
    "import here to avoid cyclic import",
    "normalize ranks",
    "determine M_k, N_k",
    "TODO: allow to pass them from outside?",
    "normalize assignment",
    "determine shapes and einsum equation",
    "create base representations",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "abstract",
    "concrete classes",
    "default flow",
    ": the message passing layers",
    ": the flow direction of messages across layers",
    ": the edge index, shape: (2, num_edges)",
    "fail if dependencies are missing",
    "avoid cyclic import",
    "the base representations, e.g., entity embeddings or features",
    "verify max_id",
    "verify shape",
    "assign sub-module *after* super call",
    "initialize layers",
    "normalize activation",
    "check consistency",
    "buffer edge index for message passing",
    "TODO: inductiveness; we need to",
    "* replace edge_index",
    "* replace base representations",
    "* keep layers & activations",
    "docstr-coverage: inherited",
    "we can restrict the message passing to the k-hop neighborhood of the desired indices;",
    "this does only make sense if we do not request *all* indices",
    "k_hop_subgraph returns:",
    "(1) the nodes involved in the subgraph",
    "(2) the filtered edge_index connectivity",
    "(3) the mapping from node indices in node_idx to their new location, and",
    "(4) the edge mask indicating which edges were preserved",
    "we only need the base representations for the neighbor indices",
    "get *all* base representations",
    "use *all* edges",
    "perform message passing",
    "select desired indices",
    "docstr-coverage: inherited",
    ": the edge type, shape: (num_edges,)",
    "register an additional buffer for the categorical edge type",
    "docstr-coverage: inherited",
    ": the relation representations used to obtain initial edge features",
    "avoid cyclic import",
    "docstr-coverage: inherited",
    "get initial relation representations",
    "select edge attributes from relation representations according to relation type",
    "perform message passing",
    "apply relation transformation, if necessary",
    "-*- coding: utf-8 -*-",
    "Classes",
    "Resolver",
    "backwards compatibility",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "must be cloned if we want to do backprop",
    "the color initializer",
    "variants for the edge index",
    "additional parameters for iter_weisfeiler_lehman",
    "normalize shape",
    "get coloring",
    "make color initializer",
    "initialize color representations",
    "note: this could be a representation?",
    "init entity representations according to the color",
    "create random walk matrix",
    "stack diagonal entries of powers of rw",
    "abstract",
    "concrete",
    "docstr-coverage: inherited",
    "tokenize",
    "pad",
    "get character embeddings",
    "pool",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "backward compatibility with RGCN",
    "docstr-coverage: inherited",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    "-*- coding: utf-8 -*-",
    "if the sparsity becomes too low, convert to a dense matrix",
    "note: this heuristic is based on the memory consumption,",
    "for a sparse matrix, we store 3 values per nnz (row index, column index, value)",
    "performance-wise, it likely makes sense to switch even earlier",
    "`torch.sparse.mm` can also deal with dense 2nd argument",
    "note: torch.sparse.mm only works for COO matrices;",
    "@ only works for CSR matrices",
    "convert to COO, if necessary",
    "we need to use indices here, since there may be zero diagonal entries",
    ": Wikidata SPARQL endpoint. See https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service#Interfacing",
    "cf. https://meta.wikimedia.org/wiki/User-Agent_policy",
    "cf. https://wikitech.wikimedia.org/wiki/Robot_policy",
    "break into smaller requests",
    "try to load cached first",
    "determine missing entries",
    "retrieve information via SPARQL",
    "save entries",
    "fill missing descriptions",
    "for mypy",
    "we can have multiple images per entity -> collect image URLs per image",
    "entity ID",
    "relation ID",
    "image URL",
    "check whether images are still missing",
    "select on image url per image in a reproducible way",
    "traverse relations in order of preference",
    "now there is an image available -> select reproducible by URL sorting",
    "did not break -> no image",
    "darglint does not like",
    "raise cls(shape=shape, reference=reference)",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "REPRESENTATION",
    "base",
    "concrete",
    "INITIALIZER",
    "INTERACTIONS",
    "Adapter classes",
    "Concrete Classes",
    "combinations",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different.",
    ": Otherwise, the entity_shape is used for head & tail entities",
    ": The symbolic shapes for relation representations",
    "if the interaction function's head parameter should only receive a subset of entity representations",
    "if the interaction function's tail parameter should only receive a subset of entity representations",
    "TODO: cannot cover dynamic shapes, e.g., AutoSF",
    "TODO: we could change that to slicing along multiple dimensions, if necessary",
    ": The functional interaction form",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    "vector & scalar offset",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "normalize kernel height",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The interaction model",
    "docstr-coverage: inherited",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "docstr-coverage: inherited",
    "normalize hidden_dim",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Initialize biases with zero",
    "In the original formulation,",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "default core tensor initialization",
    "cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "normalize initializer",
    "normalize relation dimension",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "docstr-coverage: inherited",
    "instantiate here to make module easily serializable",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "there are separate biases for entities in head and tail position",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "docstr-coverage: inherited",
    "input normalization",
    "Core tensor",
    "docstr-coverage: inherited",
    "initialize core tensor",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "r_head, r_mid, r_tail",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "r_head, r_bias, r_tail",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: excused `wrapped`",
    "TODO: switch to einsum ?",
    "return torch.real(torch.einsum(\"...d, ...d, ...d -> ...\", h, r, torch.conj(t)))",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "while ConvKB uses a convolution operation to calculate scores, its use of convolution is unusual in the",
    "following aspects",
    "1. the \"height\" of the \"image\" is the dimension of the embedding vectors",
    "2. the \"width\" of the \"image\" is 3, i.e., the number of vectors; moreover, since the convolution kernel is of",
    "shape (1, 3), there is no sliding across the input, but only a single column position where the kernel is",
    "applied",
    "3. we always have a single input channel",
    "we utilize these observations for the ConvKB specific convolution filter to simplify and accelerate the code",
    "here, conv.weight.shape = (num_filters, 1, 1, 3)",
    "thus, we have for the output of the convolution operation: x[..., f, d] = conv.weight[f, :] * x[..., d] + b[d]",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "shortcut for same shape",
    "split weight into head-/relation-/tail-specific sub-matrices",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace",
    "ensure constraints",
    "x_1 contraction",
    "x_2 contraction",
    "Rotate (=Hamilton product in quaternion space).",
    "Rotation in quaternion space",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "head",
    "relation",
    "tail",
    "extension: negative (power) norm",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "input normalization",
    "instantiate separate combinations",
    "docstr-coverage: inherited",
    "split complex; repeat real",
    "separately combine real and imaginary parts",
    "combine",
    "docstr-coverage: inherited",
    "symbolic output to avoid dtype issue",
    "we only need to consider real part here",
    "the gate",
    "the combination",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "docstr-coverage: inherited",
    "sort by decreasing degree",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "sort by decreasing page rank",
    "docstr-coverage: inherited",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": A list with ratios per representation in their creation order,",
    ": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`",
    ": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "Create an MLP for string aggregation",
    "note: the token representations' shape includes the number of tokens as leading dim",
    "unique hashes per representation",
    "unique hashes if we concatenate all representations together",
    "TODO: vectorization?",
    "remove self-loops",
    "add inverse edges and remove duplicates",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "TODO: expose num_anchors?",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "To prevent possible segfaults in the METIS C code, METIS expects a graph",
    "(1) without self-loops; (2) with inverse edges added; (3) with unique edges only",
    "https://github.com/KarypisLab/METIS/blob/94c03a6e2d1860128c2d0675cbbb86ad4f261256/libmetis/checkgraph.c#L18",
    "select independently per partition",
    "select adjacency part;",
    "note: the indices will automatically be in [0, ..., high - low), since they are *local* indices",
    "offset",
    "the -1 comes from the shared padding token",
    "note: permutation will be later on reverted",
    "add back 1 for the shared padding token",
    "TODO: check if perm is used correctly",
    "verify pool",
    "docstr-coverage: inherited",
    "choose first num_tokens",
    "TODO: vectorization?",
    "heuristic",
    "heuristic",
    "calculate configuration digest",
    "create anchor selection instance",
    "select anchors",
    "anchor search (=anchor assignment?)",
    "assign anchors",
    "save",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "ensure parent directory exists",
    "save via torch.save",
    "docstr-coverage: inherited",
    "TODO: since we save a contiguous array of (num_entities, num_anchors),",
    "it would be more efficient to not convert to a mapping, but directly select from the tensor",
    "-*- coding: utf-8 -*-",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "Data containers",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "convert to adjacency matrix",
    "convert to scipy sparse csr",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "TODO: padding for unreachable?",
    "select anchor IDs with smallest distance",
    "docstr-coverage: inherited",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "anchor nodes have themselves as a starting found anchor",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output that track the distance to each found anchor",
    "dtype is unsigned int 8 bit, so we initialize the maximum distance to 255 (or max default)",
    "initial anchors are 0-hop away from themselves",
    "propagate one hop",
    "TODO the float() trick for GPU result stability until the torch_sparse issue is resolved",
    "https://github.com/rusty1s/pytorch_sparse/issues/243",
    "convergence check",
    "newly reached is a mask that points to newly discovered anchors at this particular step",
    "implemented as element-wise XOR (will only give True in 0 XOR 1 or 1 XOR 0)",
    "in our case we enrich the set of found anchors, so we can only have values turning 0 to 1, eg 0 XOR 1",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "update the value in the pool by the current hop value (we start from 0, so +1 be default)",
    "stop once we have enough",
    "sort the pool by nearest to farthest anchors",
    "values with distance 255 (or max for unsigned int8 type) are padding tokens",
    "since the output is sorted, no need for random sampling, we just take top-k nearest",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "select k anchors with largest ppr, shape: (batch_size, k)",
    "prepare adjacency matrix only once",
    "prepare result",
    "progress bar?",
    "batch-wise computation of PPR",
    "run page-rank calculation, shape: (batch_size, n)",
    "select PPR values for the anchors, shape: (batch_size, num_anchors)",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "should be exactly 1.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2",
    "use V[x] = E[x^2] - E[x]^2",
    "group by same weight -> compute H_w(n) for multiple n at once",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective",
    "cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: should we return the sum of weights?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "-*- coding: utf-8 -*-",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    ": whether the metric supports weights",
    ": whether there is a closed-form solution of the expectation",
    ": whether there is a closed-form solution of the variance",
    "normalize weights",
    "calculate weighted harmonic mean",
    "calculate cdf",
    "determine value at p=0.5",
    "special case for exactly 0.5",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A description of the metric",
    ": The function that runs the metric",
    "docstr-coverage: inherited",
    ": Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics",
    ": This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`",
    "TODO there's something wrong with this, so add it later",
    "classifier_annotator.higher(",
    "rmc.pr_auc_score,",
    "name=\"AUC-PR\",",
    "description=\"Area Under the Precision-Recall Curve\",",
    "link=\"https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score\",",
    ")",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "noqa:DAR101",
    "noqa:DAR401",
    "Create directory in which all experimental artifacts are saved",
    "noqa:DAR101",
    "clip for node piece configurations",
    "\"pykeen experiments reproduce\" expects \"model reference dataset\"",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: take care that triples aren't removed that are the only ones with any given entity",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "save literal-to-id mapping",
    "save numeric literals",
    "load literal-to-id",
    "load literals",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    ": the number of unique entities",
    ": the number of relations (maybe including \"artificial\" inverse relations)",
    ": whether to create inverse triples",
    ": the number of real relations, i.e., without artificial inverses",
    "ensure torch.Tensor",
    "input validation",
    "always store as torch.long, i.e., torch's default integer dtype",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "docstr-coverage: inherited",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "note: num_relations will be doubled again when instantiating with create_inverse_triples=True",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "docstr-coverage: inherited",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "generate text",
    "docstr-coverage: inherited",
    "vectorized label lookup",
    "Re-order columns",
    "docstr-coverage: inherited",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "docstr-coverage: inherited",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Make sure that the first element has all the right stuff in it",
    "docstr-coverage: inherited",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "noqa:DAR202",
    "noqa:DAR401",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "docstr-coverage: inherited",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "docstr-coverage: inherited",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "note: this epoch loss can be slightly biased towards the last batch, if this is smaller than the rest",
    "in practice, this should have a minor effect, since typically batch_size << num_instances",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "disable automatic batching",
    "docstr-coverage: inherited",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "lazy init",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Split batch components",
    "Send batch to device",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    ": the path to the weights of the best model",
    ": whether to delete the file with the best model weights after termination",
    ": note: the weights will be re-loaded into the model before",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "for mypy",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "dataset",
    "model",
    "stored outside of the training loop / optimizer to give access to auto-tuning from Lightning",
    "optimizer",
    "TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA",
    "docstr-coverage: inherited",
    "call post_parameter_update",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "TODO:",
    "shuffle=shuffle,",
    "drop_last=drop_last,",
    "sampler=sampler,",
    "shuffle=shuffle,",
    "disable automatic batching in data loader",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "note: since this file is executed via __main__, its module name is replaced by __name__",
    "hence, the two classes' fully qualified names start with \"_\" and are considered private",
    "cf. https://github.com/cthoyt/class-resolver/issues/39",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "mixed precision training",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    ": The metric key",
    ": Side of the metric, or \"both\"",
    ": The rank type",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "normalize keys",
    "TODO: this can only normalize rank-based metrics!",
    "TODO: find a better way to handle this",
    "-*- coding: utf-8 -*-",
    "TODO: fix this upstream / make metric.score comply to signature",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "docstr-coverage: inherited",
    "Because the order of the values of an dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    "-*- coding: utf-8 -*-",
    "TODO remove this, it makes code much harder to reason about",
    "add mode parameter",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "do not display progress bar while searching",
    "start by searching for batch_size",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "we do not need to repeat time-consuming checks",
    "infer start value",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r), (h, t) or (r, t)) scored on all",
    "entities/relations, must have failed to start slice_size search, we start with trying half the",
    "entities/relations.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "if inverse triples are used, we only do score_t (TODO: by default; can this be changed?)",
    "otherwise, i.e., without inverse triples, we also need score_h",
    "if relations are to be predicted, we need to slice score_r",
    "raise an error, if any of the required methods cannot slice",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Send to device",
    "Ensure evaluation mode",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optionally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "-*- coding: utf-8 -*-",
    ": the MemoryUtilizationMaximizer instance for :func:`_evaluate`.",
    "batch",
    "tqdm",
    "data loader",
    "set upper limit of batch size for automatic memory optimization",
    "set model to evaluation mode",
    "delegate to AMO wrapper",
    "The key-id for each triple, shape: (num_triples,)",
    ": the number of targets for each key, shape: (num_unique_keys + 1,)",
    ": the concatenation of unique targets for each key (use bounds to select appropriate sub-array)",
    "input verification",
    "group key = everything except the prediction target",
    "initialize data structure",
    "group by key",
    "convert lists to arrays",
    "instantiate",
    "return indices corresponding to the `item`-th triple",
    "input normalization",
    "prepare filter indices if required",
    "sorted by target -> most of the batches only have a single target",
    "group by target",
    "stack groups into a single tensor",
    "avoid cyclic imports",
    "TODO: it would be better to allow separate batch sizes for entity/relation prediction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "note: most of the time, this loop will only make a single iteration, since the evaluation dataset typically is",
    "not shuffled, and contains evaluation ranking tasks sorted by target",
    "TODO: in theory, we could make a single score calculation for e.g.,",
    "{(h, r, t1), (h, r, t1), ..., (h, r, tk)}",
    "predict scores for all candidates",
    "filter scores",
    "extract true scores",
    "replace by nan",
    "rewrite true scores",
    "create dense positive masks",
    "TODO: afaik, dense positive masks are not used on GPU -> we do not need to move the masks around",
    "delegate processing of scores to the evaluator",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "terminate early if there are no ranks",
    "flatten dictionaries",
    "individual side",
    "combined",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Clear buffers",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "docstr-coverage: inherited",
    "TODO: do not require to compute all scores beforehand",
    "cf. Model.score_t(ts=...)",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "compute macro weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "docstr-coverage: inherited",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "TODO: support \"broadcasting\" representation regularizers?",
    "e.g. re-use the same regularizer for everything; or",
    "pass a dictionary with keys \"entity\"/\"relation\";",
    "values are either a regularizer hint (=the same regularizer for all repr); or a sequence of appropriate length",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "instantiate regularizer",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "docstr-coverage: inherited",
    "add broadcast dimension",
    "unsqueeze if necessary",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "deprecated",
    "exactly one of them is None",
    "create input batch",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "exactly one of them is None",
    "",
    "get input & target",
    "get label-to-id mapping and prediction targets",
    "get scores",
    "create raw dataframe",
    "postprocess prediction df",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on device",
    "docstr-coverage: inherited",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "docstr-coverage: inherited",
    "TODO: in the future, we may want to expose this method",
    "set model to evaluation mode",
    "calculate batch scores",
    "base case: infer maximum batch size",
    "base case: single batch",
    "TODO: this could happen because of AMO",
    "TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043",
    "no OOM error.",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Meta Models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the indexed filter triples, i.e., sparse masks",
    "avoid cyclic imports",
    "create base model",
    "assign *after* nn.Module.__init__",
    "save constants",
    "index triples",
    "initialize base model's parameters",
    "get masks, shape: (batch_size, num_entities/num_relations)",
    "combine masks",
    "note: * is an elementwise and, and + and elementwise or",
    "get non-zero entries",
    "set scores for fill value for every non-occuring entry",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "NodePiece",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "entity embedding",
    "ConvE uses one bias for each entity",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the default loss function is the self-adversarial negative sampling loss",
    ": The default parameters for the default loss function class",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "note: DistMult only regularizes the relation embeddings;",
    "entity embeddings are hard constrained instead",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "translation vector in hyperplane",
    "normal vector of hyperplane",
    "normalise the normal vectors to unit l2 length",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "thus, we need to use a weight regularizer instead of having an Embedding regularizer,",
    "which only regularizes the weights used in a batch",
    "note: the following is already the default",
    "default_regularizer=self.regularizer_default,",
    "default_regularizer_kwargs=self.regularizer_default_kwargs,",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "relation embedding",
    "relation projection",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "input normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "the individual combination for real/complex parts",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "no activation",
    "-*- coding: utf-8 -*-",
    ": the interaction class (for generating the overview table)",
    "added by ERModel",
    "max_id=triples_factory.num_entities,",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "docstr-coverage: inherited",
    ": The column names",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    "docstr-coverage: inherited",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "docstr-coverage: inherited",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "docstr-coverage: inherited",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "docstr-coverage: inherited",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "file names for storing results",
    "TODO: rename param?",
    "always save results as json file",
    "save other components only if requested (which they are, by default)",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "convert class to string to use caching",
    "Assume it's a file path",
    "note: we only need to set the create_inverse_triples in the training factory.",
    "normalize dataset kwargs",
    "enable passing force option via dataset_kwargs",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "TODO: maybe merge into analyze / make sub-command",
    "only save full data",
    "Plot: Descriptive Statistics of Degree Distributions per dataset / split vs. number of triples (=size)",
    "Plot: difference between mean head and tail degree",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Utilities",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    "docstr-coverage: inherited",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "TODO: use class-resolver normalize?",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ZENODO_URL = \"https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip\"",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "generate subfolders 'training' and  'inference'",
    "TODO: use class-resolver normalize?",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    "Datasets",
    "-*- coding: utf-8 -*-",
    "graph pairs",
    "graph sizes",
    "graph versions",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "ensure zip file is present",
    "save relative paths beforehand so they are present for loading",
    "delegate to super class",
    "docstr-coverage: inherited",
    "left side has files ending with 1, right side with 2",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "input validation",
    "store *before* calling super to have it available when loading the graphs",
    "ensure zip file is present",
    "shared directory for multiple datasets.",
    "docstr-coverage: inherited",
    "create triples factory",
    "docstr-coverage: inherited",
    "load mappings for both sides",
    "load triple alignments",
    "extract entity alignments",
    "(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2",
    "TODO: support ID-only graphs",
    "load both graphs",
    "load alignment",
    "drop duplicates",
    "combine",
    "store for repr",
    "split",
    "create inverse triples only for training",
    "docstr-coverage: inherited",
    "base",
    "concrete",
    "Abstract class",
    "Concrete classes",
    "Data Structures",
    "a buffer for the triples",
    "the offsets",
    "normalization",
    "append shifted mapped triples",
    "update offsets",
    "merge labels with same ID",
    "for mypy",
    "reconstruct label-to-id",
    "optional",
    "merge entity mapping",
    "merge relation mapping",
    "convert labels to IDs",
    "map labels, using -1 as fill-value for invalid labels",
    "we cannot drop them here, since the two columns need to stay aligned",
    "filter alignment",
    "map alignment from old IDs to new IDs",
    "determine swapping partner",
    "only keep triples where we have a swapping partner",
    "replace by swapping partner",
    ": the merged id-based triples, shape: (n, 3)",
    ": the updated alignment, shape: (2, m)",
    ": additional keyword-based parameters for adjusting label-to-id mappings",
    "concatenate triples",
    "filter alignment and translate to IDs",
    "process",
    "TODO: restrict to only using training alignments?",
    "merge mappings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "add swap triples",
    "e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)",
    "create dense entity remapping for swap",
    "add swapped triples",
    "swap head",
    "swap tail",
    ": the name of the additional alignment relation",
    "docstr-coverage: inherited",
    "add alignment triples with extra relation",
    "docstr-coverage: inherited",
    "determine connected components regarding the same-as relation (i.e., applies transitivity)",
    "apply id mapping",
    "ensure consecutive IDs",
    "only use training alignments?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "noqa: DAR101",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.8.2": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "autodoc_member_order = 'bysource'",
    "autodoc_typehints = 'both' # TODO turn on after 4.1 release",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "ensure each node participates in at least one edge",
    "check type and shape",
    "number of colors is monotonically increasing",
    "ensure each node participates in at least one edge",
    "normalize",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "See https://github.com/pykeen/pykeen/pull/883",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "quaternion have four components",
    ": one bias per layer",
    ": one bias per layer",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "create triples factory with inverse relations",
    "head prediction via inverse tail prediction",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "update term",
    "check that the expected term is returned",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (head, tail) pairs",
    "TODO: look into score_r for inverse relations",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    "select random indices",
    "forward pass with full graph",
    "forward pass with restricted graph",
    "verify the results are similar",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": The input dimension",
    ": the number of entities",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    ": the interaction to use for testing a model",
    "initializers *may* work in-place => clone",
    "actual number may be different...",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "generate random weights such that sum = n",
    "for sanity checking: give the largest weight to best rank => should improve",
    "generate two versions",
    "1. repeat each rank/candidate pair a random number of times",
    "2. do not repeat, but assign a corresponding weight",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "generate random triples factories",
    "generate random alignment",
    "add label information if necessary",
    "prepare alignment data frame",
    "call",
    "check",
    "-*- coding: utf-8 -*-",
    "TODO: this could be shared with the model tests",
    "FixedModel: dict(embedding_dim=EMBEDDING_DIM),",
    "test combinations of models with training loops",
    "some models require inverse relations",
    "some model require access to the training triples",
    "inductive models require an inductive mode to be set, and an inference factory to be passed",
    "fake an inference factory",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "fast run",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "TODO: Remove, since it stems from old-style model",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "clear",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "extra-relation",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "docstr-coverage: inherited",
    "assumes deterministic entity to id mapping",
    "from left_tf",
    "from right_tf with offset",
    "additional",
    "verify shape",
    "verify dtype",
    "verify number of entities/relations",
    "verify offsets",
    "create old, new pairs",
    "simulate merging ids",
    "only a single pair",
    "apply",
    "every key is contained",
    "value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize rotations to unit modulus",
    "check for unit modulus",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "-*- coding: utf-8 -*-",
    "message_dim must be divisible by num_heads",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "-*- coding: utf-8 -*-",
    "complex tensor",
    "check value range",
    "check modulus == 1",
    "quaternion needs dim divisible by 4",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "value range",
    "highest degree node has largest value",
    "Decalin molecule from Fig 4 page 15 from the paper https://arxiv.org/pdf/2110.07875.pdf",
    "create triples with a dummy relation type 0",
    "0: green: 2, 3, 7, 8",
    "1: red: 1, 4, 6, 9",
    "2: blue: 0, 5",
    "the example includes the first power",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "typically, the model takes care of adjusting the dimension size for \"complex\"",
    "tensors, but we have to do it manually here for testing purposes",
    "hotfix for mixed dtypes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO this is the only place this function is used.",
    "Is there an alternative so we can remove it?",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "x_i ~ N(mu_i, 1)",
    "closed-form solution",
    "sampled confidence interval",
    "check that closed-form is in confidence interval of sampled",
    "positive values only",
    "positive and negative values",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "check correct num_entities",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "-*- coding: utf-8 -*-",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": synonyms of this loss",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Sanity check",
    "prepare for broadcasting, shape: (batch_size, 1, 3)",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "docstr-coverage: inherited",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "docstr-coverage: inherited",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "docstr-coverage: inherited",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "docstr-coverage: inherited",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "docstr-coverage: inherited",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "make sure labels form a proper probability distribution",
    "calculate cross entropy loss",
    "docstr-coverage: inherited",
    "determine positive; do not check with == since the labels are floats",
    "subtract margin from positive scores",
    "divide by temperature",
    "docstr-coverage: inherited",
    "subtract margin from positive scores",
    "normalize positive score shape",
    "divide by temperature",
    "docstr-coverage: inherited",
    "Sanity check",
    "determine positive; do not check with == since the labels are floats",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "docstr-coverage: inherited",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "TODO delete when deleting _normalize_dim (below)",
    "TODO delete when deleting convert_to_canonical_shape (below)",
    "TODO delete? See note in test_sim.py on its only usage",
    "upgrade to sequence",
    "broadcast",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "ensure pathlib",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "cf. https://stackoverflow.com/a/1176023",
    "check validity",
    "path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "resolve path to make sure it is an absolute path",
    "ensure directory exists",
    "message passing: collect colors of neighbors",
    "dense colors: shape: (n, c)",
    "adj:          shape: (n, n)",
    "values need to be float, since torch.sparse.mm does not support integer dtypes",
    "size: will be correctly inferred",
    "concat with old colors",
    "hash",
    "create random indicator functions of low dimensionality",
    "collect neighbors' colors",
    "round to avoid numerical effects",
    "hash first",
    "concat with old colors",
    "re-hash",
    "only keep connectivity, but remove multiplicity",
    "note: in theory, we could return this uniform coloring as the first coloring; however, for featurization,",
    "this is rather useless",
    "initial: degree",
    "note: we calculate this separately, since we can use a more efficient implementation for the first step",
    "hash",
    "determine small integer type for dense count array",
    "convergence check",
    "each node has a unique color",
    "the number of colors did not improve in the last iteration",
    "cannot use Optional[pykeen.triples.CoreTriplesFactory] due to cyclic imports",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "docstr-coverage: inherited",
    "no need to compute anything",
    "docstr-coverage: inherited",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "docstr-coverage: inherited",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "could be moved into kwargs, but needs to stay for experiment integrity check",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "\"Closed-Form Expectation\",",
    "\"Closed-Form Variance\",",
    "\"\u2713\" if metric.closed_expectation else \"\",",
    "\"\u2713\" if metric.closed_variance else \"\",",
    "Add HPO command",
    "Add NodePiece tokenization command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    "entity alignment sides",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "entity alignment",
    "-*- coding: utf-8 -*-",
    "pad with zeros",
    "trim",
    "-*- coding: utf-8 -*-",
    "mask, shape: (num_edges,)",
    "bi-directional message passing",
    "Heuristic for default value",
    "docstr-coverage: inherited",
    "other relations",
    "other relations",
    "Select source and target indices as well as edge weights for the",
    "currently considered relation",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, output_dim)",
    "since we may have one node ID appearing multiple times as source",
    "ID, we can save some computation by first reducing to the unique",
    "source IDs, compute transformed representations and afterwards",
    "select these representations for the correct edges.",
    "select unique source node representations",
    "transform representations by relation specific weight",
    "select the uniquely transformed representations for each edge",
    "optional message weighting",
    "message aggregation",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Xavier Glorot initialization of each block",
    "docstr-coverage: inherited",
    "accumulator",
    "view as blocks",
    "other relations",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, num_blocks, block_size)",
    "optional message weighting",
    "message aggregation",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "docstr-coverage: inherited",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "has to be imported now to avoid cyclic imports",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "normalize *before* repeating",
    "regularize *after* repeating",
    "TODO: Remove this property and update code to use shape instead",
    "has to be imported here to avoid cyclic import",
    "docstr-coverage: inherited",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "wrapper around max_id, for backward compatibility",
    "docstr-coverage: inherited",
    "initialize weights in-place",
    "docstr-coverage: inherited",
    "apply constraints in-place",
    "fixme: work-around until nn.Embedding supports complex",
    "docstr-coverage: inherited",
    "fixme: work-around until nn.Embedding supports complex",
    "verify that contiguity is preserved",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "docstr-coverage: inherited",
    "invalidate enriched embeddings",
    "docstr-coverage: inherited",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "docstr-coverage: inherited",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "docstr-coverage: inherited",
    "abstract",
    "concrete classes",
    "default flow",
    ": the message passing layers",
    ": the flow direction of messages across layers",
    ": the edge index, shape: (2, num_edges)",
    "fail if dependencies are missing",
    "avoid cyclic import",
    "the base representations, e.g., entity embeddings or features",
    "assign sub-module *after* super call",
    "initialize layers",
    "normalize activation",
    "check consistency",
    "buffer edge index for message passing",
    "TODO: inductiveness; we need to",
    "* replace edge_index",
    "* replace base representations",
    "* keep layers & activations",
    "docstr-coverage: inherited",
    "we can restrict the message passing to the k-hop neighborhood of the desired indices;",
    "this does only make sense if we do not request *all* indices",
    "k_hop_subgraph returns:",
    "(1) the nodes involved in the subgraph",
    "(2) the filtered edge_index connectivity",
    "(3) the mapping from node indices in node_idx to their new location, and",
    "(4) the edge mask indicating which edges were preserved",
    "we only need the base representations for the neighbor indices",
    "get *all* base representations",
    "use *all* edges",
    "perform message passing",
    "select desired indices",
    "docstr-coverage: inherited",
    ": the edge type, shape: (num_edges,)",
    "register an additional buffer for the categorical edge type",
    "docstr-coverage: inherited",
    ": the relation representations used to obtain initial edge features",
    "avoid cyclic import",
    "docstr-coverage: inherited",
    "get initial relation representations",
    "select edge attributes from relation representations according to relation type",
    "perform message passing",
    "apply relation transformation, if necessary",
    "-*- coding: utf-8 -*-",
    "backwards compatibility",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "must be cloned if we want to do backprop",
    "the color initializer",
    "variants for the edge index",
    "additional parameters for iter_weisfeiler_lehman",
    "get coloring",
    "make color initializer",
    "initialize color representations",
    "init entity representations according to the color",
    "create random walk matrix",
    "stack diagonal entries of powers of rw",
    "-*- coding: utf-8 -*-",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "backward compatibility with RGCN",
    "docstr-coverage: inherited",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    "-*- coding: utf-8 -*-",
    "if the sparsity becomes too low, convert to a dense matrix",
    "note: this heuristic is based on the memory consumption,",
    "for a sparse matrix, we store 3 values per nnz (row index, column index, value)",
    "performance-wise, it likely makes sense to switch even earlier",
    "`torch.sparse.mm` can also deal with dense 2nd argument",
    "note: torch.sparse.mm only works for COO matrices;",
    "@ only works for CSR matrices",
    "convert to COO, if necessary",
    "we need to use indices here, since there may be zero diagonal entries",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "Adapter classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different.",
    ": Otherwise, the entity_shape is used for head & tail entities",
    ": The symbolic shapes for relation representations",
    "if the interaction function's head parameter should only receive a subset of entity representations",
    "if the interaction function's tail parameter should only receive a subset of entity representations",
    "TODO: cannot cover dynamic shapes, e.g., AutoSF",
    "TODO: we could change that to slicing along multiple dimensions, if necessary",
    "The appended \"e\" represents the literals that get concatenated",
    "on the entity representations. It does not necessarily have the",
    "same dimension \"d\" as the entity representations.",
    "alternate way of combining entity embeddings + literals",
    "h = torch.cat(h, dim=-1)",
    "h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore",
    "t = torch.cat(t, dim=-1)",
    "t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore",
    ": The functional interaction form",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    "vector & scalar offset",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "normalize kernel height",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The interaction model",
    "docstr-coverage: inherited",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "docstr-coverage: inherited",
    "normalize hidden_dim",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Initialize biases with zero",
    "In the original formulation,",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "default core tensor initialization",
    "cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "normalize initializer",
    "normalize relation dimension",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "docstr-coverage: inherited",
    "instantiate here to make module easily serializable",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "there are separate biases for entities in head and tail position",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "docstr-coverage: inherited",
    "input normalization",
    "Core tensor",
    "docstr-coverage: inherited",
    "initialize core tensor",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "r_head, r_mid, r_tail",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: excused `wrapped`",
    "TODO: switch to einsum ?",
    "return torch.real(torch.einsum(\"...d, ...d, ...d -> ...\", h, r, torch.conj(t)))",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(*batch_dims, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (..., f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace",
    "ensure constraints",
    "x_1 contraction",
    "x_2 contraction",
    "Rotate (=Hamilton product in quaternion space).",
    "Rotation in quaternion space",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": A list with ratios per representation in their creation order,",
    ": e.g., ``[0.58, 0.82]`` for :class:`AnchorTokenization` and :class:`RelationTokenization`",
    ": A scalar ratio of unique rows when combining all representations into one matrix, e.g. 0.95",
    ": the token representations",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "determine shape",
    "super init; has to happen *before* any parameter or buffer is assigned",
    "assign module",
    "Assign default aggregation",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "unique hashes per representation",
    "unique hashes if we concatenate all representations together",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "docstr-coverage: inherited",
    "sort by decreasing degree",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "sort by decreasing page rank",
    "docstr-coverage: inherited",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "input normalization",
    "power iteration",
    "convert to sparse matrix, shape: (n, n)",
    "symmetrize",
    "TODO: should we add self-links",
    "adj = adj + scipy.sparse.eye(m=adj.shape[0], format=\"coo\")",
    "convert to CSR",
    "adjacency normalization",
    "TODO: vectorization?",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "docstr-coverage: inherited",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "verify pool",
    "docstr-coverage: inherited",
    "choose first num_tokens",
    "TODO: vectorization?",
    "heuristic",
    "heuristic",
    "calculate configuration digest",
    "create anchor selection instance",
    "select anchors",
    "anchor search (=anchor assignment?)",
    "assign anchors",
    "save",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "ensure parent directory exists",
    "save via torch.save",
    "docstr-coverage: inherited",
    "TODO: since we save a contiguous array of (num_entities, num_anchors),",
    "it would be more efficient to not convert to a mapping, but directly select from the tensor",
    "-*- coding: utf-8 -*-",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "Data containers",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "docstr-coverage: inherited",
    "convert to adjacency matrix",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "select anchor IDs with smallest distance",
    "docstr-coverage: inherited",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "select k anchors with largest ppr, shape: (batch_size, k)",
    "prepare adjacency matrix only once",
    "prepare result",
    "progress bar?",
    "batch-wise computation of PPR",
    "create a batch of starting vectors, shape: (n, batch_size)",
    "run page-rank calculation, shape: (batch_size, n)",
    "select PPR values for the anchors, shape: (num_anchors, batch_size)",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "docstr-coverage: inherited",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "should be exactly 1.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "should be exactly 0.0",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2",
    "use V[x] = E[x^2] - E[x]^2",
    "group by same weight -> compute H_w(n) for multiple n at once",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective",
    "cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: should we return the sum of weights?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "docstr-coverage: inherited",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "-*- coding: utf-8 -*-",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    ": whether the metric supports weights",
    ": whether there is a closed-form solution of the expectation",
    ": whether there is a closed-form solution of the variance",
    "normalize weights",
    "calculate weighted harmonic mean",
    "calculate cdf",
    "determine value at p=0.5",
    "special case for exactly 0.5",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A description of the metric",
    ": The function that runs the metric",
    "docstr-coverage: inherited",
    ": Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics",
    ": This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`",
    "TODO there's something wrong with this, so add it later",
    "classifier_annotator.higher(",
    "rmc.pr_auc_score,",
    "name=\"AUC-PR\",",
    "description=\"Area Under the Precision-Recall Curve\",",
    "link=\"https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score\",",
    ")",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "clip for node piece configurations",
    "\"pykeen experiments reproduce\" expects \"model reference dataset\"",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "save literal-to-id mapping",
    "save numeric literals",
    "load literal-to-id",
    "load literals",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    ": the number of unique entities",
    ": the number of relations (maybe including \"artificial\" inverse relations)",
    ": whether to create inverse triples",
    ": the number of real relations, i.e., without artificial inverses",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "docstr-coverage: inherited",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "docstr-coverage: inherited",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "generate text",
    "docstr-coverage: inherited",
    "vectorized label lookup",
    "Re-order columns",
    "docstr-coverage: inherited",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "docstr-coverage: inherited",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Make sure that the first element has all the right stuff in it",
    "docstr-coverage: inherited",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "docstr-coverage: inherited",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "docstr-coverage: inherited",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "disable automatic batching",
    "docstr-coverage: inherited",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Split batch components",
    "Send batch to device",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "dataset",
    "model",
    "stored outside of the training loop / optimizer to give access to auto-tuning from Lightning",
    "optimizer",
    "TODO: In sLCWA, we still want to calculate validation *metrics* in LCWA",
    "docstr-coverage: inherited",
    "call post_parameter_update",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "TODO:",
    "shuffle=shuffle,",
    "drop_last=drop_last,",
    "sampler=sampler,",
    "shuffle=shuffle,",
    "disable automatic batching in data loader",
    "docstr-coverage: inherited",
    "TODO: sub-batching / slicing",
    "docstr-coverage: inherited",
    "note: since this file is executed via __main__, its module name is replaced by __name__",
    "hence, the two classes' fully qualified names start with \"_\" and are considered private",
    "cf. https://github.com/cthoyt/class-resolver/issues/39",
    "automatically choose accelerator",
    "defaults to TensorBoard; explicitly disabled here",
    "disable checkpointing",
    "mixed precision training",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    ": The metric key",
    ": Side of the metric, or \"both\"",
    ": The rank type",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "normalize keys",
    "TODO: this can only normalize rank-based metrics!",
    "TODO: find a better way to handle this",
    "-*- coding: utf-8 -*-",
    "TODO: fix this upstream / make metric.score comply to signature",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "docstr-coverage: inherited",
    "Because the order of the values of an dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    "-*- coding: utf-8 -*-",
    "TODO remove this, it makes code much harder to reason about",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Send to device",
    "Ensure evaluation mode",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optionally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "terminate early if there are no ranks",
    "flatten dictionaries",
    "individual side",
    "combined",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Clear buffers",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "docstr-coverage: inherited",
    "TODO: do not require to compute all scores beforehand",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "compute macro weights",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "docstr-coverage: inherited",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on device",
    "docstr-coverage: inherited",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "docstr-coverage: inherited",
    "TODO: in the future, we may want to expose this method",
    "set model to evaluation mode",
    "calculate batch scores",
    "base case: infer maximum batch size",
    "base case: single batch",
    "TODO: this could happen because of AMO",
    "TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043",
    "no OOM error.",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "docstr-coverage: inherited",
    ": Primary embeddings for entities",
    ": Primary embeddings for relations",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "Old style models should never be looked up",
    "-*- coding: utf-8 -*-",
    "Create an MLP for string aggregation",
    "always create representations for normal and inverse relations and padding",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    "-*- coding: utf-8 -*-",
    "NodePiece",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "entity embedding",
    "ConvE uses one bias for each entity",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "docstr-coverage: inherited",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "docstr-coverage: inherited",
    "Compute scores",
    "Regularization",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": the default loss function is the self-adversarial negative sampling loss",
    ": The default parameters for the default loss function class",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "note: DistMult only regularizes the relation embeddings;",
    "entity embeddings are hard constrained instead",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "relation embedding",
    "relation projection",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "docstr-coverage: inherited",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "There were some performance/memory issues with cdist, cf.",
    "https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks",
    "they are mostly resolved by now. A Benefit would be that we can harness the",
    "future (performance) improvements made by the core torch developers. However,",
    "this will require some benchmarking.",
    "docstr-coverage: inherited",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "docstr-coverage: inherited",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "docstr-coverage: inherited",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "Create an MLP for string aggregation",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "compatibility: new style fft returns complex tensor",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "docstr-coverage: inherited",
    "Embedding Regularization",
    "docstr-coverage: inherited",
    "Embedding Regularization",
    "docstr-coverage: inherited",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "docstr-coverage: inherited",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "docstr-coverage: inherited",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "max_id=max_id,  # will be added by ERModel",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "docstr-coverage: inherited",
    ": The column names",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    "docstr-coverage: inherited",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "docstr-coverage: inherited",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "docstr-coverage: inherited",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "docstr-coverage: inherited",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "file names for storing results",
    "TODO: rename param?",
    "always save results as json file",
    "save other components only if requested (which they are, by default)",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "convert class to string to use caching",
    "Assume it's a file path",
    "note: we only need to set the create_inverse_triples in the training factory.",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Utilities",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    "docstr-coverage: inherited",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "TODO: use class-resolver normalize?",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ZENODO_URL = \"https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip\"",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "generate subfolders 'training' and  'inference'",
    "TODO: use class-resolver normalize?",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    "Datasets",
    "-*- coding: utf-8 -*-",
    "graph pairs",
    "graph sizes",
    "graph versions",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "ensure zip file is present",
    "save relative paths beforehand so they are present for loading",
    "delegate to super class",
    "docstr-coverage: inherited",
    "left side has files ending with 1, right side with 2",
    "docstr-coverage: inherited",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "input validation",
    "store *before* calling super to have it available when loading the graphs",
    "ensure zip file is present",
    "shared directory for multiple datasets.",
    "docstr-coverage: inherited",
    "create triples factory",
    "docstr-coverage: inherited",
    "load mappings for both sides",
    "load triple alignments",
    "extract entity alignments",
    "(h1, r1, t1) = (h2, r2, t2) => h1 = h2 and t1 = t2",
    "TODO: support ID-only graphs",
    "load both graphs",
    "load alignment",
    "drop duplicates",
    "combine",
    "store for repr",
    "split",
    "create inverse triples only for training",
    "base",
    "concrete",
    "Abstract class",
    "Concrete classes",
    "Data Structures",
    "a buffer for the triples",
    "the offsets",
    "normalization",
    "append shifted mapped triples",
    "update offsets",
    "merge labels with same ID",
    "for mypy",
    "reconstruct label-to-id",
    "optional",
    "merge entity mapping",
    "merge relation mapping",
    "convert labels to IDs",
    "map labels, using -1 as fill-value for invalid labels",
    "we cannot drop them here, since the two columns need to stay aligned",
    "filter alignment",
    "map alignment from old IDs to new IDs",
    "determine swapping partner",
    "only keep triples where we have a swapping partner",
    "replace by swapping partner",
    ": the merged id-based triples, shape: (n, 3)",
    ": the updated alignment, shape: (2, m)",
    ": additional keyword-based parameters for adjusting label-to-id mappings",
    "concatenate triples",
    "filter alignment and translate to IDs",
    "process",
    "TODO: restrict to only using training alignments?",
    "merge mappings",
    "docstr-coverage: inherited",
    "docstr-coverage: inherited",
    "add swap triples",
    "e1 ~ e2 => (e1, r, t) ~> (e2, r, t), or (h, r, e1) ~> (h, r, e2)",
    "create dense entity remapping for swap",
    "add swapped triples",
    "swap head",
    "swap tail",
    ": the name of the additional alignment relation",
    "docstr-coverage: inherited",
    "add alignment triples with extra relation",
    "docstr-coverage: inherited",
    "determine connected components regarding the same-as relation (i.e., applies transitivity)",
    "apply id mapping",
    "ensure consecutive IDs",
    "only use training alignments?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "noqa: DAR101",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.8.1": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "autodoc_member_order = 'bysource'",
    "autodoc_typehints = 'both' # TODO turn on after 4.1 release",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "quaternion have four components",
    ": one bias per layer",
    ": one bias per layer",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "create triples factory with inverse relations",
    "head prediction via inverse tail prediction",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "update term",
    "check that the expected term is returned",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (head, tail) pairs",
    "TODO: look into score_r for inverse relations",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": The input dimension",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    "initializers *may* work in-place => clone",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "generate random weights such that sum = n",
    "for sanity checking: give the largest weight to best rank => should improve",
    "generate two versions",
    "1. repeat each rank/candidate pair a random number of times",
    "2. do not repeat, but assign a corresponding weight",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "TODO: Remove, since it stems from old-style model",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "clear",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize rotations to unit modulus",
    "check for unit modulus",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "-*- coding: utf-8 -*-",
    "message_dim must be divisible by num_heads",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "-*- coding: utf-8 -*-",
    "check value range",
    "check sin**2 + cos**2 == 1",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "typically, the model takes care of adjusting the dimension size for \"complex\"",
    "tensors, but we have to do it manually here for testing purposes",
    "hotfix for mixed dtypes",
    "-*- coding: utf-8 -*-",
    "TODO consider making subclass of cases.RepresentationTestCase",
    "that has num_entities, num_relations, num_triples, and",
    "create_inverse_triples as well as a generate_triples_factory()",
    "wrapper",
    "-*- coding: utf-8 -*-",
    "TODO this is the only place this function is used.",
    "Is there an alternative so we can remove it?",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "x_i ~ N(mu_i, 1)",
    "closed-form solution",
    "sampled confidence interval",
    "check that closed-form is in confidence interval of sampled",
    "positive values only",
    "positive and negative values",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "check correct num_entities",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "-*- coding: utf-8 -*-",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "Sanity check",
    "prepare for broadcasting, shape: (batch_size, 1, 3)",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "Sanity check",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "TODO delete when deleting _normalize_dim (below)",
    "TODO delete when deleting convert_to_canonical_shape (below)",
    "TODO delete? See note in test_sim.py on its only usage",
    "upgrade to sequence",
    "broadcast",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "ensure pathlib",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "cf. https://stackoverflow.com/a/1176023",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "-*- coding: utf-8 -*-",
    "\"Closed-Form Expectation\",",
    "\"Closed-Form Variance\",",
    "\"\u2713\" if metric.closed_expectation else \"\",",
    "\"\u2713\" if metric.closed_variance else \"\",",
    "Add HPO command",
    "Add NodePiece tokenization command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "-*- coding: utf-8 -*-",
    "pad with zeros",
    "trim",
    "-*- coding: utf-8 -*-",
    "mask, shape: (num_edges,)",
    "bi-directional message passing",
    "Heuristic for default value",
    "other relations",
    "other relations",
    "Select source and target indices as well as edge weights for the",
    "currently considered relation",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, output_dim)",
    "since we may have one node ID appearing multiple times as source",
    "ID, we can save some computation by first reducing to the unique",
    "source IDs, compute transformed representations and afterwards",
    "select these representations for the correct edges.",
    "select unique source node representations",
    "transform representations by relation specific weight",
    "select the uniquely transformed representations for each edge",
    "optional message weighting",
    "message aggregation",
    "Xavier Glorot initialization of each block",
    "accumulator",
    "view as blocks",
    "other relations",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, num_blocks, block_size)",
    "optional message weighting",
    "message aggregation",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "has to be imported now to avoid cyclic imports",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "invalidate enriched embeddings",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "normalize *before* repeating",
    "regularize *after* repeating",
    "TODO: Remove this property and update code to use shape instead",
    "has to be imported here to avoid cyclic import",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "wrapper around max_id, for backward compatibility",
    "initialize weights in-place",
    "apply constraints in-place",
    "fixme: work-around until nn.Embedding supports complex",
    "fixme: work-around until nn.Embedding supports complex",
    "verify that contiguity is preserved",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "invalidate enriched embeddings",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "-*- coding: utf-8 -*-",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "-*- coding: utf-8 -*-",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "backward compatibility with RGCN",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "Adapter classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different.",
    ": Otherwise, the entity_shape is used for head & tail entities",
    ": The symbolic shapes for relation representations",
    "if the interaction function's head parameter should only receive a subset of entity representations",
    "if the interaction function's tail parameter should only receive a subset of entity representations",
    "TODO: cannot cover dynamic shapes, e.g., AutoSF",
    "TODO: we could change that to slicing along multiple dimensions, if necessary",
    "The appended \"e\" represents the literals that get concatenated",
    "on the entity representations. It does not necessarily have the",
    "same dimension \"d\" as the entity representations.",
    "alternate way of combining entity embeddings + literals",
    "h = torch.cat(h, dim=-1)",
    "h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore",
    "t = torch.cat(t, dim=-1)",
    "t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore",
    ": The functional interaction form",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    "vector & scalar offset",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "The interaction model",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Initialize biases with zero",
    "In the original formulation,",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "default core tensor initialization",
    "cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "normalize initializer",
    "normalize relation dimension",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "instantiate here to make module easily serializable",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "there are separate biases for entities in head and tail position",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "input normalization",
    "Core tensor",
    "initialize core tensor",
    "r_head, r_mid, r_tail",
    "-*- coding: utf-8 -*-",
    "TODO: switch to einsum ?",
    "return torch.real(torch.einsum(\"...d, ...d, ...d -> ...\", h, r, torch.conj(t)))",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(*batch_dims, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (..., f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace",
    "ensure constraints",
    "x_1 contraction",
    "x_2 contraction",
    "Rotate (=Hamilton product in quaternion space).",
    "Rotation in quaternion space",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": the token representations",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "determine shape",
    "super init; has to happen *before* any parameter or buffer is assigned",
    "assign module",
    "Assign default aggregation",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "sort by decreasing degree",
    "sort by decreasing page rank",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "input normalization",
    "power iteration",
    "convert to sparse matrix, shape: (n, n)",
    "symmetrize",
    "TODO: should we add self-links",
    "adj = adj + scipy.sparse.eye(m=adj.shape[0], format=\"coo\")",
    "convert to CSR",
    "adjacency normalization",
    "TODO: vectorization?",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "verify pool",
    "choose first num_tokens",
    "TODO: vectorization?",
    "heuristic",
    "heuristic",
    "calculate configuration digest",
    "create anchor selection instance",
    "select anchors",
    "anchor search (=anchor assignment?)",
    "assign anchors",
    "save",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "ensure parent directory exists",
    "save via torch.save",
    "TODO: since we save a contiguous array of (num_entities, num_anchors),",
    "it would be more efficient to not convert to a mapping, but directly select from the tensor",
    "-*- coding: utf-8 -*-",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "convert to adjacency matrix",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "select anchor IDs with smallest distance",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "select k anchors with largest ppr, shape: (batch_size, k)",
    "prepare adjacency matrix only once",
    "prepare result",
    "progress bar?",
    "batch-wise computation of PPR",
    "create a batch of starting vectors, shape: (n, batch_size)",
    "run page-rank calculation, shape: (batch_size, n)",
    "select PPR values for the anchors, shape: (num_anchors, batch_size)",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "should be exactly 0.0",
    "should be exactly 1.0",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "should be exactly 0.0",
    "V (prod x_i) = prod (V[x_i] - E[x_i]^2) - prod(E[x_i])^2",
    "use V[x] = E[x^2] - E[x]^2",
    "group by same weight -> compute H_w(n) for multiple n at once",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "ensure non-negativity, mathematically not necessary, but just to be safe from the numeric perspective",
    "cf. https://en.wikipedia.org/wiki/Loss_of_significance#Subtraction",
    "TODO: should we return the sum of weights?",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "for each individual ranking task, we have I[r_i <= k] ~ Bernoulli(k/N_i)",
    "-*- coding: utf-8 -*-",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    ": whether the metric supports weights",
    ": whether there is a closed-form solution of the expectation",
    ": whether there is a closed-form solution of the variance",
    "normalize weights",
    "calculate weighted harmonic mean",
    "calculate cdf",
    "determine value at p=0.5",
    "special case for exactly 0.5",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A description of the metric",
    ": The function that runs the metric",
    ": Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics",
    ": This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`",
    "TODO there's something wrong with this, so add it later",
    "classifier_annotator.higher(",
    "rmc.pr_auc_score,",
    "name=\"AUC-PR\",",
    "description=\"Area Under the Precision-Recall Curve\",",
    "link=\"https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score\",",
    ")",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "save literal-to-id mapping",
    "save numeric literals",
    "load literal-to-id",
    "load literals",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "generate text",
    "vectorized label lookup",
    "Re-order columns",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "Make sure that the first element has all the right stuff in it",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "disable automatic batching",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "Split batch components",
    "Send batch to device",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    ": The metric key",
    ": Side of the metric, or \"both\"",
    ": The rank type",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "normalize keys",
    "TODO: this can only normalize rank-based metrics!",
    "TODO: find a better way to handle this",
    "-*- coding: utf-8 -*-",
    "TODO: fix this upstream / make metric.score comply to signature",
    "docstr-coverage:inherited",
    "docstr-coverage:inherited",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "docstr-coverage:inherited",
    "Because the order of the values of an dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    "-*- coding: utf-8 -*-",
    "TODO remove this, it makes code much harder to reason about",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Send to device",
    "Ensure evaluation mode",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optionally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "terminate early if there are no ranks",
    "flatten dictionaries",
    "individual side",
    "combined",
    "docstr-coverage:inherited",
    "docstr-coverage:inherited",
    "docstr-coverage:inherited",
    "docstr-coverage:inherited",
    "Clear buffers",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "docstr-coverage:inherited",
    "TODO: do not require to compute all scores beforehand",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "compute macro weights",
    "docstr-coverage:inherited",
    "docstr-coverage:inherited",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on device",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "TODO: in the future, we may want to expose this method",
    "set model to evaluation mode",
    "calculate batch scores",
    "base case: infer maximum batch size",
    "base case: single batch",
    "TODO: this could happen because of AMO",
    "TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043",
    "no OOM error.",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    ": Primary embeddings for entities",
    ": Primary embeddings for relations",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "Old style models should never be looked up",
    "-*- coding: utf-8 -*-",
    "Create an MLP for string aggregation",
    "always create representations for normal and inverse relations and padding",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    "-*- coding: utf-8 -*-",
    "NodePiece",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "entity embedding",
    "ConvE uses one bias for each entity",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "note: DistMult only regularizes the relation embeddings;",
    "entity embeddings are hard constrained instead",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "relation embedding",
    "relation projection",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "There were some performance/memory issues with cdist, cf.",
    "https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks",
    "they are mostly resolved by now. A Benefit would be that we can harness the",
    "future (performance) improvements made by the core torch developers. However,",
    "this will require some benchmarking.",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "Create an MLP for string aggregation",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "compatibility: new style fft returns complex tensor",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "max_id=max_id,  # will be added by ERModel",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "as_uri() requires the path to be absolute. resolve additionally also normalizes the path",
    ": The column names",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "left side has files ending with 1, right side with 2",
    "For downloading",
    "For splitting",
    "Whether to create inverse triples",
    "ensure file is present",
    "create triples factory",
    "split",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "Input validation.",
    "For downloading",
    "For splitting",
    "Whether to create inverse triples",
    "shared directory for multiple datasets.",
    "ensure file is present",
    "TODO: Re-use ensure_from_google?",
    "read all triples from file",
    "some \"entities\" have numeric labels",
    "pandas.read_csv(..., dtype=str) does not work properly.",
    "create triples factory",
    "split",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "convert class to string to use caching",
    "Assume it's a file path",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Utilities",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ZENODO_URL = \"https://zenodo.org/record/6321299/files/pykeen/ilpc2022-v1.0.zip\"",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "generate subfolders 'training' and  'inference'",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "-*- coding: utf-8 -*-",
    "Base class",
    "Mid-level classes",
    "Datasets",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "noqa: DAR101",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.8.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "'scipy': ('https://docs.scipy.org/doc/scipy/reference', None),",
    "autodoc_member_order = 'bysource'",
    "autodoc_typehints = 'both' # TODO turn on after 4.1 release",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "check result shape",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "not needed for test",
    "assert that reporting another metric for this epoch raises an error",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "serialize",
    "de-serialize",
    "check for equality",
    "TODO: this could be (Core)TriplesFactory.__equal__",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "quaternion have four components",
    ": one bias per layer",
    ": one bias per layer",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "FIXME definitely a type mismatch going on here",
    "check shape",
    "check content",
    "create triples factory with inverse relations",
    "head prediction via inverse tail prediction",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    ": the model's device",
    ": the inductive mode",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (head, tail) pairs",
    "TODO: look into score_r for inverse relations",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "dataset = InductiveFB15k237(create_inverse_triples=self.create_inverse_triples)",
    "check type",
    "check shape",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "use more samples to make sure that enough values can be dropped",
    ": The number of entities",
    ": The number of triples",
    ": the message dim",
    "TODO: separation message vs. entity dim?",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": The input dimension",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    "initializers *may* work in-place => clone",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "the model",
    "Settings",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no duplicates",
    "shape",
    "value range",
    "no repetition, except padding idx",
    ": The batch size",
    ": the maximum number of candidates",
    ": the number of ranks",
    ": the number of samples to use for monte-carlo estimation",
    ": the number of candidates for each individual ranking task",
    ": the ranks for each individual ranking task",
    "data type",
    "value range",
    "original ranks",
    "better ranks",
    "variances are non-negative",
    "check flatness",
    "TODO: does this suffice, or do we really need float as datatype?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "TODO: Remove, since it stems from old-style model",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "clear",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize length of r",
    "check for unit length",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "multiple",
    "single",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "-*- coding: utf-8 -*-",
    "message_dim must be divisible by num_heads",
    "determine pool using anchor searcher",
    "determine expected pool using shortest path distances via scipy.sparse.csgraph",
    "generate random pool",
    "-*- coding: utf-8 -*-",
    "check value range",
    "check sin**2 + cos**2 == 1",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "typically, the model takes care of adjusting the dimension size for \"complex\"",
    "tensors, but we have to do it manually here for testing purposes",
    "-*- coding: utf-8 -*-",
    "TODO consider making subclass of cases.RepresentationTestCase",
    "that has num_entities, num_relations, num_triples, and",
    "create_inverse_triples as well as a generate_triples_factory()",
    "wrapper",
    "-*- coding: utf-8 -*-",
    "TODO this is the only place this function is used.",
    "Is there an alternative so we can remove it?",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "check non-empty metrics",
    ": Should negative samples be filtered?",
    "expectation = (1 + n) / 2",
    "variance = (n**2 - 1) / 12",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "check correct num_entities",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "check true negatives",
    "TODO: check no repetitions (if possible)",
    "return type",
    "columns",
    "value range",
    "relation restriction",
    "with explicit num_entities",
    "with inferred num_entities",
    "test different shapes",
    "test different shapes",
    "value range",
    "value range",
    "check unique",
    "strips off the \"k\" at the end",
    "Populate with real results.",
    "-*- coding: utf-8 -*-",
    "(-1, 1),",
    "(-1, -1),",
    "(-5, -3),",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "Test that half of the subjects and half of the objects are corrupted",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "Sanity check",
    "prepare for broadcasting, shape: (batch_size, 1, 3)",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "Sanity check",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "TODO: extend to relation, cf. https://github.com/pykeen/pykeen/pull/728",
    "SIDES: Tuple[Target, ...] = (LABEL_HEAD, LABEL_TAIL)",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "try to resolve ambiguous device; there has to be at least one cuda device",
    "lower bound",
    "upper bound",
    "input validation",
    "base case",
    "normalize dim",
    "calculate repeats for each tensor",
    "dimensions along concatenation axis do not need to match",
    "get desired extent along dimension",
    "repeat tensors along axes if necessary",
    "concatenate",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "workaround for complex numbers: manually compute norm",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "TODO delete when deleting _normalize_dim (below)",
    "TODO delete when deleting convert_to_canonical_shape (below)",
    "TODO delete? See note in test_sim.py on its only usage",
    "upgrade to sequence",
    "broadcast",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "ensure pathlib",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "cf. https://stackoverflow.com/a/1176023",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    "prediction targets",
    "modes",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": the inductive prediction and training mode",
    ": the prediction target",
    ": the prediction target index",
    ": the rank types",
    "RANK_TYPES: Tuple[RankType, ...] = typing.get_args(RankType) # Python >= 3.8",
    "-*- coding: utf-8 -*-",
    "pad with zeros",
    "trim",
    "-*- coding: utf-8 -*-",
    "mask, shape: (num_edges,)",
    "bi-directional message passing",
    "Heuristic for default value",
    "other relations",
    "other relations",
    "Select source and target indices as well as edge weights for the",
    "currently considered relation",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, output_dim)",
    "since we may have one node ID appearing multiple times as source",
    "ID, we can save some computation by first reducing to the unique",
    "source IDs, compute transformed representations and afterwards",
    "select these representations for the correct edges.",
    "select unique source node representations",
    "transform representations by relation specific weight",
    "select the uniquely transformed representations for each edge",
    "optional message weighting",
    "message aggregation",
    "Xavier Glorot initialization of each block",
    "accumulator",
    "view as blocks",
    "other relations",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, num_blocks, block_size)",
    "optional message weighting",
    "message aggregation",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "has to be imported now to avoid cyclic imports",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "invalidate enriched embeddings",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    ": a normalizer for individual representations",
    ": a regularizer for individual representations",
    ": dropout",
    "normalize *before* repeating",
    "regularize *after* repeating",
    "TODO: Remove this property and update code to use shape instead",
    "has to be imported here to avoid cyclic import",
    "normalize num_embeddings vs. max_id",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "note: this seems to work, as finfo returns the datatype of the underlying floating",
    "point dtype, rather than the combined complex one",
    "use make for initializer since there's a default, and make_safe",
    "for the others to pass through None values",
    "wrapper around max_id, for backward compatibility",
    "initialize weights in-place",
    "apply constraints in-place",
    "verify that contiguity is preserved",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "has to be imported here to avoid cyclic imports",
    "kwargs",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "invalidate enriched embeddings",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "-*- coding: utf-8 -*-",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "-*- coding: utf-8 -*-",
    ": whether the edge weighting needs access to the message",
    "stub init to enable arbitrary arguments in subclasses",
    "Calculate in-degree, i.e. number of incoming edges",
    "backward compatibility with RGCN",
    "view for heads",
    "compute attention coefficients, shape: (num_edges, num_heads)",
    "TODO we can use scatter_softmax from torch_scatter directly, kept this if we can rewrite it w/o scatter",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "Adapter classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "normalize input",
    "get number of head/relation/tail representations",
    "flatten list",
    "split tensors",
    "broadcasting",
    "yield batches",
    "complex typing",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE.",
    ": The symbolic shapes for relation representations",
    "TODO: we could change that to slicing along multiple dimensions, if necessary",
    "The appended \"e\" represents the literals that get concatenated",
    "on the entity representations. It does not necessarily have the",
    "same dimension \"d\" as the entity representations.",
    "alternate way of combining entity embeddings + literals",
    "h = torch.cat(h, dim=-1)",
    "h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore",
    "t = torch.cat(t, dim=-1)",
    "t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore",
    ": The functional interaction form",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "The interaction model",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Initialize biases with zero",
    "In the original formulation,",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "there are separate biases for entities in head and tail position",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "input normalization",
    "Core tensor",
    "initialize core tensor",
    "r_head, r_mid, r_tail",
    "-*- coding: utf-8 -*-",
    "repeat if necessary, and concat head and relation",
    "shape: -1, num_input_channels, 2*height, width",
    "shape: -1, num_input_channels, 2*height, width",
    "-1, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (-1, dim) -> (*batch_dims, dim)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: batch_dims",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(*batch_dims, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (..., f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (*batch_dims, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (*batch_dims, d)",
    "dot product",
    "composite: (*batch_dims, d)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (*batch_dims, d)",
    "dot product with t",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace and ensure constraints",
    "x_1 contraction",
    "x_2 contraction",
    "Rotate (=Hamilton product in quaternion space).",
    "Rotation in quaternion space",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, *batch_dims, dim)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": the token ID of the padding token",
    ": the token representations",
    ": the assigned tokens for each entity",
    "needs to be lazily imported to avoid cyclic imports",
    "fill padding (nn.Embedding cannot deal with negative indices)",
    "sometimes, assignment.max() does not cover all relations (eg, inductive inference graphs",
    "contain a subset of training relations) - for that, the padding index is the last index of the Representation",
    "resolve token representation",
    "input validation",
    "register as buffer",
    "assign sub-module",
    "apply tokenizer",
    "get token IDs, shape: (*, num_chosen_tokens)",
    "lookup token representations, shape: (*, num_chosen_tokens, *shape)",
    ": the token representations",
    "normalize triples",
    "inverse triples are created afterwards implicitly",
    "tokenize",
    "determine shape",
    "super init; has to happen *before* any parameter or buffer is assigned",
    "assign module",
    "Assign default aggregation",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "TODO: allow relative",
    "isin() preserves the sorted order",
    "sort by decreasing degree",
    "sort by decreasing page rank",
    "input normalization",
    "determine absolute number of anchors for each strategy",
    "if pre-instantiated",
    "convert to sparse matrix",
    "symmetrize",
    "TODO: should we add self-links",
    "adj = (adj + adj.transpose() + scipy.sparse.eye(m=adj.shape[0], format=\"coo\")).tocsr()",
    "degree for adjacency normalization",
    "power iteration",
    "TODO: vectorization?",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    "select anchors",
    "find closest anchors",
    "convert to torch",
    "verify pool",
    "choose first num_tokens",
    "TODO: vectorization?",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "contains: anchor_ids, entity_ids, mapping {entity_id -> {\"ancs\": anchors, \"dists\": distances}}",
    "normalize anchor_ids",
    "cf. https://github.com/pykeen/pykeen/pull/822#discussion_r822889541",
    "TODO: keep distances?",
    "-*- coding: utf-8 -*-",
    "Anchor Searchers",
    "Anchor Selection",
    "Tokenizers",
    "Token Loaders",
    "Representations",
    "TODO: use graph library, such as igraph, graph-tool, or networkit",
    "-*- coding: utf-8 -*-",
    "Resolver",
    "Base classes",
    "Concrete classes",
    "convert to adjacency matrix",
    "compute distances between anchors and all nodes, shape: (num_anchors, num_entities)",
    "select anchor IDs with smallest distance",
    "infer shape",
    "create adjacency matrix",
    "symmetric + self-loops",
    "for each entity, determine anchor pool by BFS",
    "an array storing whether node i is reachable by anchor j",
    "an array indicating whether a node is closed, i.e., has found at least $k$ anchors",
    "the output",
    "TODO: take all (q-1) hop neighbors before selecting from q-hop",
    "propagate one hop",
    "convergence check",
    "copy pool if we have seen enough anchors and have not yet stopped",
    "stop once we have enough",
    "TODO: can we replace this loop with something vectorized?",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "",
    "",
    "",
    "",
    "",
    "Misc",
    "",
    "rank based metrics do not need binarized scores",
    ": the supported rank types. Most of the time equal to all rank types",
    ": whether the metric requires the number of candidates for each ranking task",
    "normalize confidence level",
    "sample metric values",
    "bootstrap estimator (i.e., compute on sample with replacement)",
    "cf. https://stackoverflow.com/questions/1986152/why-doesnt-python-have-a-sign-function",
    ": The rank-based metric class that this derived metric extends",
    "since scale and offset are constant for a given number of candidates, we have",
    "E[scale * M + offset] = scale * E[M] + offset",
    "since scale and offset are constant for a given number of candidates, we have",
    "V[scale * M + offset] = scale^2 * V[M]",
    ": Z-adjusted metrics are formulated to be increasing",
    ": Z-adjusted metrics can only be applied to realistic ranks",
    "should be exactly 0.0",
    "should be exactly 1.0",
    ": Expectation/maximum reindexed metrics are formulated to be increasing",
    ": Expectation/maximum reindexed metrics can only be applied to realistic ranks",
    "should be exactly 0.0",
    "we compute log E[r_i^(1/m)] for all N_i = 1 ... max_N_i once",
    "now select from precomputed cumulative sums and aggregate",
    "individual ranks' expectation",
    "individual inverse ranks' variance",
    "rank aggregation",
    "-*- coding: utf-8 -*-",
    ": the lower bound",
    ": whether the lower bound is inclusive",
    ": the upper bound",
    ": whether the upper bound is inclusive",
    ": The name of the metric",
    ": a link to further information",
    ": whether the metric needs binarized scores",
    ": whether it is increasing, i.e., larger values are better",
    ": the value range",
    ": synonyms for this metric",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A description of the metric",
    ": The function that runs the metric",
    ": Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics",
    ": This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`",
    "TODO there's something wrong with this, so add it later",
    "classifier_annotator.higher(",
    "rmc.pr_auc_score,",
    "name=\"AUC-PR\",",
    "description=\"Area Under the Precision-Recall Curve\",",
    "link=\"https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score\",",
    ")",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "load base",
    "load numeric triples",
    "store numeric triples",
    "store metadata",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "store entity/relation to ID",
    "load entity/relation to ID",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "generate text",
    "vectorized label lookup",
    "Re-order columns",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "Make sure that the first element has all the right stuff in it",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "TODO: the same",
    ": the positive triples, shape: (batch_size, 3)",
    ": the negative triples, shape: (batch_size, num_negatives_per_positive, 3)",
    ": filtering masks for negative triples, shape: (batch_size, num_negatives_per_positive)",
    "TODO: some negative samplers require batches",
    "shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "each shape: (1, 3), (1, k, 3), (1, k, 3)?",
    "cf. https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset",
    "indexing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "send model to device before going into the internal training loop",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Register a callback for the early stopper, if given",
    "TODO should mode be passed here?",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "afterwards, some parameters may be on the wrong device",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "disable automatic batching",
    "Slicing is not possible in sLCWA training loops",
    "split batch",
    "send to device",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "TODO how to pass inductive mode",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Explicit mentioning of num_transductive_entities since in the evaluation there will be a different number",
    "of total entities from another inductive inference factory",
    "Split batch components",
    "Send batch to device",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": the number of reported results with no improvement after which training will be stopped",
    "the minimum relative improvement necessary to consider it an improved result",
    "whether a larger value is better, or a smaller.",
    ": The epoch at which the best result occurred",
    ": The best result so far",
    ": The remaining patience",
    "check for improvement",
    "stop if the result did not improve more than delta for patience evaluations",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "TODO need a test that this all re-instantiates properly",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "parsing metrics",
    "metric pattern = side?.type?.metric.k?",
    ": The metric key",
    ": Side of the metric, or \"both\"",
    ": The rank type",
    "normalize metric name",
    "normalize side",
    "normalize rank type",
    "normalize keys",
    "TODO: this can only normalize rank-based metrics!",
    "TODO: find a better way to handle this",
    "-*- coding: utf-8 -*-",
    "TODO: fix this upstream / make metric.score comply to signature",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Because the order of the values of an dictionary is not guaranteed,",
    "we need to retrieve scores and masks using the exact same key order.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": The optimistic rank is the rank when assuming all options with an equal score are placed",
    ": behind the current test triple.",
    ": shape: (batch_size,)",
    ": The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank",
    ": over all permutations of the elements with the same score as the currently considered option.",
    ": shape: (batch_size,)",
    ": The pessimistic rank is the rank when assuming all options with an equal score are placed",
    ": in front of current test triple.",
    ": shape: (batch_size,)",
    ": The number of options is the number of items considered in the ranking. It may change for",
    ": filtered evaluation",
    ": shape: (batch_size,)",
    "The optimistic rank is the rank when assuming all options with an",
    "equal score are placed behind the currently considered. Hence, the",
    "rank is the number of options with better scores, plus one, as the",
    "rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an",
    "equal score are placed in front of the currently considered. Hence,",
    "the rank is the number of options which have at least the same score",
    "minus one (as the currently considered option in included in all",
    "options). As the rank is one-based, we have to add 1, which nullifies",
    "the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic",
    "rank, and hence the expected rank over all permutations of the elements",
    "with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options",
    "which should be considered is given by",
    "-*- coding: utf-8 -*-",
    "TODO remove this, it makes code much harder to reason about",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "TODO: consider switching to torch.DataLoader where the preparation of masks/filter batches also takes place",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Send to device",
    "Ensure evaluation mode",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Create filter",
    "Select scores of true",
    "overwrite filtered scores",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "the rank-based evaluators needs the true scores with trailing 1-dim",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "process scores",
    "optinally restrict triples (nop if no restriction)",
    "evaluation triples as dataframe",
    "determine filter triples",
    "infer num_entities if not given",
    "TODO: unique, or max ID + 1?",
    "optionally restrict triples",
    "compute candidate set sizes for different targets",
    "TODO: extend to relations?",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "terminate early if there are no ranks",
    "flatten dictionaries",
    "individual side",
    "combined",
    "Clear buffers",
    "repeat",
    "default for inductive LP by [teru2020]",
    "verify input",
    "TODO: do not require to compute all scores beforehand",
    "super.evaluation assumes that the true scores are part of all_scores",
    "write back correct num_entities",
    "TODO: should we give num_entities in the constructor instead of inferring it every time ranks are processed?",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: update to hint + kwargs",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "TODO: allow max_id being present in representation_kwargs; if it matches max_id",
    "TODO: we could infer some shapes from the given interaction shape information",
    "check max-id",
    "check shapes",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    ": The interaction function",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "normalize input",
    "Note: slicing cannot be used here: the indices for score_hrt only have a batch",
    "dimension, and slicing along this dimension is already considered by sub-batching.",
    "Note: we do not delegate to the general method for performance reasons",
    "Note: repetition is not necessary here",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on device",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "TODO: in the future, we may want to expose this method",
    "set model to evaluation mode",
    "calculate batch scores",
    "base case: infer maximum batch size",
    "base case: single batch",
    "TODO: this could happen because of AMO",
    "TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043",
    "no OOM error.",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: why do we need to empty the cache?",
    "TODO: this currently compute (batch_size, num_relations) instead,",
    "i.e., scores for normal and inverse relations",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    ": Primary embeddings for entities",
    ": Primary embeddings for relations",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Inductive Models",
    "Evaluation-only models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "Old style models should never be looked up",
    "-*- coding: utf-8 -*-",
    "Create an MLP for string aggregation",
    "always create representations for normal and inverse relations and padding",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "default composition is DistMult-style",
    "Saving edge indices for all the supplied splits",
    "Extract all entity and relation representations",
    "Perform message passing and get updated states",
    "Use updated entity and relation states to extract requested IDs",
    "TODO I got lost in all the Representation Modules and shape casting and wrote this ;(",
    "normalization",
    "-*- coding: utf-8 -*-",
    "NodePiece",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "TODO: other parameters?",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "Since CP uses different representations for entities in head / tail role,",
    "the current solution is a bit hacky, and may be improved. See discussion",
    "on https://github.com/pykeen/pykeen/pull/663.",
    "Override to allow different head and tail entity representations",
    "normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "mean",
    "diagonal covariance",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "embeddings",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "There were some performance/memory issues with cdist, cf.",
    "https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks",
    "they are mostly resolved by now. A Benefit would be that we can harness the",
    "future (performance) improvements made by the core torch developers. However,",
    "this will require some benchmarking.",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation size head",
    "relation position tail",
    "relation shape tail",
    "relation size tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "(head) entity",
    "tail entity",
    "relations",
    "inverse relations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "Create an MLP for string aggregation",
    "always create representations for normal and inverse relations and padding",
    "normalize embedding specification",
    "prepare token representations & kwargs",
    "max_id=triples_factory.num_relations,  # will get added by ERModel",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "compatibility: new style fft returns complex tensor",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "max_id=max_id,  # will be added by ERModel",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "compute relation similarity matrix",
    "mapping from relations to head/tail entities",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "as_uri() requires the path to be absolute. resolve additionally also normalizes the path",
    ": The column names",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "Set the indices",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "Bind the total number of negatives to sample in this batch",
    "Equally corrupt all sides",
    "Do not detach, as no gradients should flow into the indices.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Decide whether to corrupt head or tail",
    "clone positive batch for corruption (.repeat_interleave creates a copy)",
    "flatten mask",
    "Tails are corrupted if heads are not corrupted",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "TODO use pathlib here",
    "note: we do not directly forward discard_seed here, since we want to highlight the different default behaviour:",
    "when replicating (i.e., running multiple replicates), fixing a random seed would render the replicates useless",
    "note: torch.nn.Module.cpu() is in-place in contrast to torch.Tensor.cpu()",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Log loss parameters",
    "the loss was already logged as part of the model kwargs",
    "loss=loss_resolver.normalize_inst(model_instance.loss),",
    "Log regularizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The link to the zip file",
    ": The hex digest for the zip file",
    "Input validation.",
    "left side has files ending with 1, right side with 2",
    "For downloading",
    "For splitting",
    "Whether to create inverse triples",
    "ensure file is present",
    "create triples factory",
    "split",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "Input validation.",
    "For downloading",
    "For splitting",
    "Whether to create inverse triples",
    "shared directory for multiple datasets.",
    "ensure file is present",
    "TODO: Re-use ensure_from_google?",
    "read all triples from file",
    "some \"entities\" have numeric labels",
    "pandas.read_csv(..., dtype=str) does not work properly.",
    "create triples factory",
    "split",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Assume it's a file path",
    "hash kwargs",
    "normalize dataset name",
    "get canonic path",
    "try to use cached dataset",
    "load dataset without cache",
    "store cache",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "note: we do not use the built-in constants here, since those refer to OGB nomenclature",
    "(which happens to coincide with ours)",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "num_triples_validation: Optional[int],",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "expected metrics",
    "Needs simulation",
    "See https://zenodo.org/record/6331629",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Utilities",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": the dataset's name",
    "TODO: Make a constant for the names",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Utilities",
    "Base Classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the inductive inference triples that MIGHT or MIGHT NOT",
    "share indices with the transductive training",
    ": A factory wrapping the testing triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": A factory wrapping the validation triples, that share indices with the INDUCTIVE INFERENCE triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `transductive_training`",
    ": The actual instance of the inductive inference factory,",
    ": which is exposed to the user through `inductive_inference`",
    ": The actual instance of the testing factory, which is exposed to the user through `inductive_testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `inductive_validation`",
    ": The directory in which the cached data is stored",
    "add v1 / v2 / v3 / v4 for inductive splits if available",
    "generate subfolders 'training' and  'inference'",
    "important: inductive_inference shares the same RELATIONS with the transductive training graph",
    "inductive validation shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "inductive testing shares both ENTITIES and RELATIONS with the inductive inference graph",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "TODO: use metric.increasing to determine default direction",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "noqa: DAR101",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.7.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "autodoc_member_order = 'bysource'",
    "autodoc_typehints = 'both' # TODO turn on after 4.1 release",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The tested class",
    "check probability distribution",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Check for correct class",
    "Check value ranges",
    "check mean rank (MR)",
    "check mean reciprocal rank (MRR)",
    "check hits at k (H@k)",
    "check adjusted mean rank (AMR)",
    "check adjusted mean rank index (AMRI)",
    "the test only considered a single batch",
    "all rank types have the same count",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "check result shape",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "Test different combinations of restrictions",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Check if multilabels are working correctly",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "Test that the weight in the MLP is trainable (i.e. requires grad)",
    "quaternion have four components",
    ": one bias per layer",
    ": one bias per layer",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "check shape",
    "check content",
    "create triples factory with inverse relations",
    "head prediction via inverse tail prediction",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "check whether the error originates from batch norm for single element batches",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (head, tail) pairs",
    "TODO: look into score_r for inverse relations",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "check type",
    "check shape",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": The input dimension",
    ": the shape of the tensor to initialize",
    ": to be initialized / set in subclass",
    "initializers *may* work in-place => clone",
    "unfavourable split to ensure that cleanup is necessary",
    "check for unclean split",
    "check that no triple got lost",
    "check that triples where only moved from other to reference",
    "check that all entities occur in reference",
    "check that no triple got lost",
    "check that all entities are covered in first part",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "generated_triples = generate_triples()",
    "check type",
    "check format",
    "check coverage",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check statistics for pre-stratified datasets",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "remove batch/num dimension",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize length of r",
    "check for unit length",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "dimension needs to be divisible by num_heads",
    "FIXME",
    "head * (re_head + self.u * e_h) - tail * (re_tail + self.u * e_t) + re_mid",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check value range",
    "check sin**2 + cos**2 == 1",
    "check value range (actually [-s, +s] with s = 1/sqrt(2*n))",
    "-*- coding: utf-8 -*-",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    "TODO consider making subclass of cases.RepresentationTestCase",
    "that has num_entities, num_relations, num_triples, and",
    "create_inverse_triples as well as a generate_triples_factory()",
    "wrapper",
    ": The number of embeddings",
    "check shape",
    "check attributes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "typically, the model takes care of adjusting the dimension size for \"complex\"",
    "tensors, but we have to do it manually here for testing purposes",
    "-*- coding: utf-8 -*-",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    ": Should negative samples be filtered?",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    "Test that half of the subjects and half of the objects are corrupted",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "Sanity check",
    "prepare for broadcasting, shape: (batch_size, 1, 3)",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "Ensures that for this class incompatible hyper-parameter \"margin\" of superclass is not used",
    "within the ablation pipeline.",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "we need dense negative scores => unfilter if necessary",
    "we may have inf rows, since there will be one additional finite positive score per row",
    "combine scores: shape: (batch_size, num_negatives + 1)",
    "use sparse version of cross entropy",
    "Sanity check",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "Sanity check",
    "we do not allow full -inf rows, since we compute the softmax over this tensor",
    "compute weights (without gradient tracking)",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "get datatype specific epsilon",
    "clamp minimum value",
    "lower bound",
    "upper bound",
    "input validation",
    "base case",
    "normalize dim",
    "calculate repeats for each tensor",
    "dimensions along concatenation axis do not need to match",
    "get desired extent along dimension",
    "repeat tensors along axes if necessary",
    "concatenate",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "workaround for complex numbers: manually compute norm",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "ensure pathlib",
    "Enforce that sizes are strictly positive by passing through ELU",
    "Shape vector is normalized using the above helper function",
    "Size is learned separately and applied to normalized shape",
    "Compute potential boundaries by applying the shape in substraction",
    "and in addition",
    "Compute box upper bounds using min and max respectively",
    "compute width plus 1",
    "compute box midpoints",
    "TODO: we already had this before, as `base`",
    "inside box?",
    "yes: |p - c| / (w + 1)",
    "no: (w + 1) * |p - c| - 0.5 * w * (w - 1/(w + 1))",
    "Step 1: Apply the other entity bump",
    "Step 2: Apply tanh if tanh_map is set to True.",
    "Compute the distance function output element-wise",
    "Finally, compute the norm",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    "-*- coding: utf-8 -*-",
    "pad with zeros",
    "trim",
    "-*- coding: utf-8 -*-",
    "mask, shape: (num_edges,)",
    "bi-directional message passing",
    "Heuristic for default value",
    "other relations",
    "other relations",
    "Select source and target indices as well as edge weights for the",
    "currently considered relation",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, output_dim)",
    "since we may have one node ID appearing multiple times as source",
    "ID, we can save some computation by first reducing to the unique",
    "source IDs, compute transformed representations and afterwards",
    "select these representations for the correct edges.",
    "select unique source node representations",
    "transform representations by relation specific weight",
    "select the uniquely transformed representations for each edge",
    "optional message weighting",
    "message aggregation",
    "Xavier Glorot initialization of each block",
    "accumulator",
    "view as blocks",
    "other relations",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, num_blocks, block_size)",
    "optional message weighting",
    "message aggregation",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/message_gcns/gcn_basis.py#L22-L24  # noqa: E501",
    "there are separate decompositions for forward and backward relations.",
    "the self-loop weight is not decomposed.",
    "self-loop",
    "forward messages",
    "backward messages",
    "activation",
    "Resolve edge weighting",
    "dropout",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "no activation on last layer",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/common/model_builder.py#L275  # noqa: E501",
    "buffering of enriched representations",
    "invalidate enriched embeddings",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "-*- coding: utf-8 -*-",
    "Utils",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    "TODO: Remove this property and update code to use shape instead",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support (torch==1.10 still does not work)",
    "TODO: verify that this is our understanding of complex!",
    "wrapper around max_id, for backward compatibility",
    "initialize weights in-place",
    "apply constraints in-place",
    "verify that contiguity is preserved",
    "TODO: move normalizer / regularizer to base class?",
    "get all base representations, shape: (num_bases, *shape)",
    "get base weights, shape: (*batch_dims, num_bases)",
    "weighted linear combination of bases, shape: (*batch_dims, *shape)",
    ": Initializers",
    ": Constrainers",
    "TODO add normalization functions",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "invalidate enriched embeddings",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "inverse triples are created afterwards implicitly",
    "tokenize: represent entities by bag of relations",
    "collect candidates",
    "randomly sample without replacement num_tokens relations for each entity",
    ": the token representations",
    ": the entity-to-token mapping",
    "create token representations",
    "normal relations + inverse relations + padding",
    "super init; has to happen *before* any parameter or buffer is assigned",
    "Assign default aggregation",
    "assign module",
    "get token IDs, shape: (*, k)",
    "lookup token representations, shape: (*, k, d)",
    "aggregate",
    "infer shape",
    "assign after super, since they should be properly registered as submodules",
    "-*- coding: utf-8 -*-",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "this is usually loaded from somewhere else",
    "the shape must match, as well as the entity-to-id mapping",
    "-*- coding: utf-8 -*-",
    "Calculate in-degree, i.e. number of incoming edges",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "Adapter classes",
    "Concrete Classes",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    "input normalization -> Tuple[Tensor]",
    "shape: (num_representations,)",
    "split each representation along the given dimension",
    "shape: (num_representations,)",
    "create batches, shape: (num_slices,), each being a tuple of shape (num_representations,)",
    "unpack_singletons",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE.",
    ": The symbolic shapes for relation representations",
    "bring to (b, n, *)",
    "bring to (b, h, r, t, *)",
    "unpack singleton",
    "The appended \"e\" represents the literals that get concatenated",
    "on the entity representations. It does not necessarily have the",
    "same dimension \"d\" as the entity representations.",
    "alternate way of combining entity embeddings + literals",
    "h = torch.cat(h, dim=-1)",
    "h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore",
    "t = torch.cat(t, dim=-1)",
    "t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore",
    ": The functional interaction form",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "The interaction model",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Initialize biases with zero",
    "In the original formulation,",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "there are separate biases for entities in head and tail position",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "head position and bump",
    "relation box: head",
    "relation box: tail",
    "tail position and bump",
    "r_head, r_mid, r_tail",
    "-*- coding: utf-8 -*-",
    "repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width",
    "with batch_size' = batch_size * num_heads * num_relations",
    "batch_size', num_input_channels, 2*height, width",
    "batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: (batch_size, num_heads, num_relations, num_tails)",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(b, h, r, t, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (1, 1, 1, 1, f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (b, h, r, 1, d)",
    "transpose t, (b, 1, 1, d, t)",
    "dot product, (b, h, r, 1, t)",
    "composite: (b, h, 1, t, d)",
    "transpose composite: (b, h, 1, d, t)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (b, h, r, 1, d)",
    "dot product with t, shape: (b, h, r, t)",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace and ensure constraints",
    "x_3 contraction",
    "x_1 contraction",
    "x_2 contraction",
    "Rotate (=Hamilton product in quaternion space).",
    "Rotation in quaternion space",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "head",
    "relation box: head",
    "relation box: tail",
    "tail",
    "power norm",
    "the relation-specific head box base shape (normalized to have a volume of 1):",
    "the relation-specific tail box base shape (normalized to have a volume of 1):",
    "head",
    "relation",
    "tail",
    "version 2: relation factor offset",
    "extension: negative (power) norm",
    "note: normalization should be done from the representations",
    "cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L317-L328  # noqa: E501",
    "version 2",
    "r_head = r_head + u * torch.ones_like(r_head)",
    "r_tail = r_tail + u * torch.ones_like(r_tail)",
    "stack h & r (+ broadcast) => shape: (2, batch_size', num_heads, num_relations, 1, *dims)",
    "remember shape for output, but reshape for transformer",
    "get position embeddings, shape: (seq_len, dim)",
    "Now we are position-dependent w.r.t qualifier pairs.",
    "seq_length, batch_size, dim",
    "Pool output",
    "output shape: (batch_size, dim)",
    "reshape",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    "TODO: method is_inverse?",
    "TODO: inverse of inverse?",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "Filter for entities",
    "Filter for relations",
    "No filter",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "prepare metadata",
    "Delegate to function",
    "restrict triples can only remove triples; thus, if the new size equals the old one, nothing has changed",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "generate text",
    "vectorized label lookup",
    "Re-order columns",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "Make sure that the first element has all the right stuff in it",
    "backwards compatibility",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "randomly choose a vertex which has not been chosen yet",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "is already batched!",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "Create training instances. Use the _create_instances function to allow subclasses",
    "to modify this behavior",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "wrap training instances",
    "disable automatic batching",
    "no support for sub-batching",
    "this is already done",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples, shape: (batch_size, num_neg_per_pos, 3)",
    "apply filter mask",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Split batch components",
    "Send batch to device",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Filter triples if necessary",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": Functions with the right signature in the :mod:`rexmex.metrics.classification` that are not themselves metrics",
    ": This dictionary maps from duplicate functions to the canonical function in :mod:`rexmex.metrics.classification`",
    "TODO there's something wrong with this, so add it later",
    "classifier_annotator.higher(",
    "rmc.pr_auc_score,",
    "name=\"AUC-PR\",",
    "description=\"Area Under the Precision-Recall Curve\",",
    "link=\"https://rexmex.readthedocs.io/en/latest/modules/root.html#rexmex.metrics.classification.pr_auc_score\",",
    ")",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extra stats stuff",
    "The optimistic rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an equal score are placed in front of the",
    "currently considered. Hence, the rank is the number of options which have at least the same score minus one",
    "(as the currently considered option in included in all options). As the rank is one-based, we have to add 1,",
    "which nullifies the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank over",
    "all permutations of the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "normalize metric name",
    "handle spaces and case",
    "special case for hits_at_k",
    "TODO: Fractional?",
    "synonym normalization",
    "normalize side",
    "normalize rank type",
    "Adjusted mean rank calculation",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "normalize input",
    "normalization",
    "-*- coding: utf-8 -*-",
    "train model",
    "note: as this is an example, the model is only trained for a few epochs,",
    "but not until convergence. In practice, you would usually first verify that",
    "the model is sufficiently good in prediction, before looking at uncertainty scores",
    "predict triple scores with uncertainty",
    "use a larger number of samples, to increase quality of uncertainty estimate",
    "get most and least uncertain prediction on training set",
    ": The scores",
    ": The uncertainty, in the same shape as scores",
    "Enforce evaluation mode",
    "set dropout layers to training mode",
    "draw samples",
    "compute mean and std",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on device",
    "reshape, shape: (batch_size * num_entities,)",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "initialize buffer on cpu",
    "Explicitly create triples",
    "TODO: in the future, we may want to expose this method",
    "set model to evaluation mode",
    "calculate batch scores",
    "base case: infer maximum batch size",
    "base case: single batch",
    "TODO: this could happen because of AMO",
    "TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043",
    "no OOM error.",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "-*- coding: utf-8 -*-",
    "This empty 1-element tensor doesn't actually do anything,",
    "but is necessary since models with no grad params blow",
    "up the optimizer",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The device on which this model and its submodules are stored",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "send to device",
    "special handling of inverse relations",
    "when trained on inverse relations, the internal relation ID is twice the original relation ID",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    ": Primary embeddings for entities",
    ": Primary embeddings for relations",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Evaluation-only models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "Old style models should never be looked up",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "head representation",
    "tail representation",
    "Since CP uses different representations for entities in head / tail role,",
    "the current solution is a bit hacky, and may be improved. See discussion",
    "on https://github.com/pykeen/pykeen/pull/663.",
    "Override to allow different head and tail entity representations",
    "normalization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "use torch's native complex data type",
    "use torch's native complex data type",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Constraints are applied through post_parameter_update",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    "diagonal entries",
    "off-diagonal",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "embeddings",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "create enriched entity representations",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "Resolve interaction function",
    "set default relation representation",
    "cf. https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/decoders/bilinear_diag.py#L64-L67  # noqa: E501",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "There were some performance/memory issues with cdist, cf.",
    "https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks",
    "they are mostly resolved by now. A Benefit would be that we can harness the",
    "future (performance) improvements made by the core torch developers. However,",
    "this will require some benchmarking.",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "entity bias for head",
    "relation position head",
    "relation shape head",
    "relation shape tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": Secondary embeddings for entities",
    ": Secondary embeddings for relations",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "dim is only a parameter to match the signature of torch.mean / torch.sum",
    "this class is not thought to be usable from outside",
    "Create an MLP for string aggregation",
    "always create representations for normal and inverse relations and padding",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "compatibility: new style fft returns complex tensor",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "These operations do not need to be performed on a GPU",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    ": The name of the run",
    ": The configuration dictionary, a mapping from name -> value",
    ": Should metrics be stored when running ``log_metrics()``?",
    ": The metrics, a mapping from step -> (name -> value)",
    ": A hint for constructing a :class:`MultiResultTracker`",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "always add a Python result tracker for storing the configuration",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "as_uri() requires the path to be absolute. resolve additionally also normalizes the path",
    ": The column names",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "Set the indices",
    "Bind number of negatives to sample",
    "Equally corrupt all sides",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Relations have a different index maximum than entities",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "We at least make sure to not replace the triples by the original value",
    "See below for explanation of why this is on a range of [0, num_entities - 1]",
    "Randomly sample corruption.",
    "Replace heads",
    "Replace tails",
    "To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": The configuration",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "TODO use pathlib here",
    "normalize keys",
    "TODO: this can only normalize rank-based metrics!",
    "only one original value => assume this to be the mean",
    "multiple values => assume they correspond to individual trials",
    "metrics accumulates rows for a dataframe for comparison against the original reported results (if any)",
    "TODO: we could have multiple results, if we get access to the raw results (e.g. in the pykeen benchmarking paper)",
    "summarize",
    "skip special parameters",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Imported from PyTorch",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    ": A resolver for learning rate schedulers",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "Input validation.",
    "For downloading",
    "For splitting",
    "Whether to create inverse triples",
    "shared directory for multiple datasets.",
    "ensure file is present",
    "TODO: Re-use ensure_from_google?",
    "read all triples from file",
    "some \"entities\" have numeric labels",
    "pandas.read_csv(..., dtype=str) does not work properly.",
    "create triples factory",
    "split",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Concrete Classes",
    "Utilities",
    "Assume it's a file path",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "log pruning",
    "trial was successful, but has to be ended",
    "also show info",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "raise the error again (which will be catched in study.optimize)",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.6.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "autodoc_member_order = 'bysource'",
    "autodoc_typehints = 'both' # TODO turn on after 4.1 release",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The tested class",
    "check probability distribution",
    "check probability distribution",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "verify failure",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "check mean rank (MR)",
    "check mean reciprocal rank (MRR)",
    "check hits at k (H@k)",
    "check adjusted mean rank (AMR)",
    "check adjusted mean rank index (AMRI)",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "generate random query tensor",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "check result shape",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "check type",
    "check format",
    "check coverage",
    "Check if multilabels are working correctly",
    "Slightly larger number of triples to guarantee split can find coverage of all entities and relations.",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "quaternion have four components",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "check whether the error originates from batch norm for single element batches",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "TODO: Make sure that inverse triples are created if create_inverse_triples=True",
    "triples factory is added by the pipeline",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "check type",
    "check shape",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": The input dimension",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check statistics for pre-stratified datasets",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "remove batch/num dimension",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize length of r",
    "check for unit length",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    ": The number of embeddings",
    "check shape",
    "check attributes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "typically, the model takes care of adjusting the dimension size for \"complex\"",
    "tensors, but we have to do it manually here for testing purposes",
    "-*- coding: utf-8 -*-",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    ": Should negative samples be filtered?",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    "Test that half of the subjects and half of the objects are corrupted",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "Sanity check",
    "prepare for broadcasting, shape: (batch_size, 1, 3)",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "1. positive & negative margin",
    "2. negative margin & offset",
    "3. positive margin & offset",
    "Sanity check",
    "positive term",
    "implicitly repeat positive scores",
    "shape: (nnz,)",
    "negative term",
    "negative_scores have already been filtered in the sampler!",
    "Sanity check",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "Sanity check",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "Sanity check",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "compute weights (without gradient tracking)",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "lower bound",
    "upper bound",
    "normalize input",
    "input validation",
    "base case",
    "normalize dim",
    "calculate repeats for each tensor",
    "dimensions along concatenation axis do not need to match",
    "get desired extent along dimension",
    "repeat tensors along axes if necessary",
    "concatenate",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "workaround for complex numbers: manually compute norm",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    "-*- coding: utf-8 -*-",
    "pad with zeros",
    "trim",
    "-*- coding: utf-8 -*-",
    "mask, shape: (num_edges,)",
    "bi-directional message passing",
    "Heuristic for default value",
    "weights",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "other relations",
    "other relations",
    "Select source and target indices as well as edge weights for the",
    "currently considered relation",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, output_dim)",
    "since we may have one node ID appearing multiple times as source",
    "ID, we can save some computation by first reducing to the unique",
    "source IDs, compute transformed representations and afterwards",
    "select these representations for the correct edges.",
    "select unique source node representations",
    "transform representations by relation specific weight",
    "select the uniquely transformed representations for each edge",
    "optional message weighting",
    "message aggregation",
    "self-loops first",
    "the last relation_id refers to the self-loop",
    "Xavier Glorot initialization of each block",
    "view as blocks",
    "self-loop first",
    "other relations",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, num_blocks, block_size)",
    "optional message weighting",
    "message aggregation",
    "-*- coding: utf-8 -*-",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    "TODO: Remove this property and update code to use shape instead",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support",
    "TODO: verify that this is our understanding of complex!",
    "wrapper around max_id, for backward compatibility",
    "initialize weights in-place",
    "apply constraints in-place",
    "verify that contiguity is preserved",
    "TODO: move normalizer / regularizer to base class?",
    "freeze",
    "use this instead of a lambda to make sure that it can be pickled",
    "TODO add normalization functions",
    "Resolve edge weighting",
    "dropout",
    "batch norm and bias",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "buffering of enriched representations",
    "invalidate enriched embeddings",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "invalidate enriched embeddings",
    "when changing from evaluation to training mode, the buffered representations have been computed without",
    "gradient tracking. hence, we need to invalidate them.",
    "note: this occurs in practice when continuing training after evaluation.",
    "enrich",
    "-*- coding: utf-8 -*-",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "-*- coding: utf-8 -*-",
    "Calculate in-degree, i.e. number of incoming edges",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE.",
    ": The symbolic shapes for relation representations",
    "bring to (b, n, *)",
    "bring to (b, h, r, t, *)",
    "unpack singleton",
    "The appended \"e\" represents the literals that get concatenated",
    "on the entity representations. It does not necessarily have the",
    "same dimension \"d\" as the entity representations.",
    "alternate way of combining entity embeddings + literals",
    "h = torch.cat(h, dim=-1)",
    "h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore",
    "t = torch.cat(t, dim=-1)",
    "t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore",
    ": The functional interaction form",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "The interaction model",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Initialize biases with zero",
    "In the original formulation,",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "there are separate biases for entities in head and tail position",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "-*- coding: utf-8 -*-",
    "repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width",
    "with batch_size' = batch_size * num_heads * num_relations",
    "batch_size', num_input_channels, 2*height, width",
    "batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: (batch_size, num_heads, num_relations, num_tails)",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(b, h, r, t, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (1, 1, 1, 1, f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (b, h, r, 1, d)",
    "transpose t, (b, 1, 1, d, t)",
    "dot product, (b, h, r, 1, t)",
    "composite: (b, h, 1, t, d)",
    "transpose composite: (b, h, 1, d, t)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (b, h, r, 1, d)",
    "dot product with t, shape: (b, h, r, t)",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace and ensure constraints",
    "x_3 contraction",
    "x_1 contraction",
    "x_2 contraction",
    "Rotate (=Hamilton product in quaternion space).",
    "Rotation in quaternion space",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "Filter for entities",
    "Filter for relations",
    "No filtering happened",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "generate text",
    "vectorized label lookup",
    "Re-order columns",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "select one triple per relation",
    "maintain set of covered entities",
    "Select one triple for each head/tail entity, which is not yet covered.",
    "create mask",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "Make sure that the first element has all the right stuff in it",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    "convert to csr for fast row slicing",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "Create training instances. Use the _create_instances function to allow subclasses",
    "to modify this behavior",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Create a new lr scheduler and add the optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Update learning rate scheduler",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Saving triples factory related states",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "Check whether the triples factory mappings match those from the checkpoints",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples, shape: (batch_size, num_neg_per_pos, 3)",
    "apply filter mask",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "normalize target column",
    "The type inference is so confusing between the function switching",
    "and polymorphism introduced by slicability that these need to be ignored",
    "Split batch components",
    "Send batch to device",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extra stats stuff",
    "The optimistic rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an equal score are placed in front of the",
    "currently considered. Hence, the rank is the number of options which have at least the same score minus one",
    "(as the currently considered option in included in all options). As the rank is one-based, we have to add 1,",
    "which nullifies the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank over",
    "all permutations of the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "Check if it a side or rank type",
    "update old names for metrics and handle spaces",
    "otherwise, assume is hits@k, which is handled differently",
    "Adjusted mean rank calculation",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "normalize input",
    "normalization",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "base case: infer maximum batch size",
    "base case: single batch",
    "TODO: this could happen because of AMO",
    "TODO: Can we make AMO code re-usable? e.g. like https://gist.github.com/mberr/c37a8068b38cabc98228db2cbe358043",
    "no OOM error.",
    "make sure triples are a numpy array",
    "make sure triples are 2d",
    "convert to ID-based",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The device on which this model and its submodules are stored",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    ": Primary embeddings for entities",
    ": Primary embeddings for relations",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Evaluation-only models",
    "Utils",
    "Abstract Models",
    "We might be able to relax this later",
    "baseline models behave differently",
    "Old style models should never be looked up",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Regularization",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "w: (k, d, d)",
    "vh: (k, d)",
    "vt: (k, d)",
    "b: (k,)",
    "u: (k,)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Constraints are applied through post_parameter_update",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "embeddings",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "create enriched entity representations",
    "Resolve interaction function",
    "set default relation representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "There were some performance/memory issues with cdist, cf.",
    "https://github.com/pytorch/pytorch/issues?q=cdist however, @mberr thinks",
    "they are mostly resolved by now. A Benefit would be that we can harness the",
    "future (performance) improvements made by the core torch developers. However,",
    "this will require some benchmarking.",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "Get embeddings",
    "TODO: Use torch.cdist (see note above in score_hrt())",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": Secondary embeddings for entities",
    ": Secondary embeddings for relations",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "compatibility: new style fft returns complex tensor",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create sparse matrix of absolute counts",
    "normalize to relative counts",
    "base case",
    "note: we need to work with dense arrays only to comply with returning torch tensors. Otherwise, we could",
    "stay sparse here, with a potential of a huge memory benefit on large datasets!",
    "-*- coding: utf-8 -*-",
    "These operations are deterministic and a random seed can be fixed",
    "just to avoid warnings",
    "These operations do not need to be performed on a GPU",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "as_uri() requires the path to be absolute. resolve additionally also normalizes the path",
    ": The column names",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "Set the indices",
    "Bind number of negatives to sample",
    "Equally corrupt all sides",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Relations have a different index maximum than entities",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "We at least make sure to not replace the triples by the original value",
    "See below for explanation of why this is on a range of [0, num_entities - 1]",
    "Randomly sample corruption.",
    "Replace heads",
    "Replace tails",
    "To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "TODO use pathlib here",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A wrapper around the hidden scheduler base class",
    ": The default strategy for optimizing the lr_schedulers' hyper-parameters",
    ": A resolver for learning rate schedulers",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "Input validation.",
    "For downloading",
    "For splitting",
    "Whether to create inverse triples",
    "shared directory for multiple datasets.",
    "ensure file is present",
    "TODO: Re-use ensure_from_google?",
    "read all triples from file",
    "some \"entities\" have numeric labels",
    "pandas.read_csv(..., dtype=str) does not work properly.",
    "create triples factory",
    "split",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Concrete Classes",
    "Utilities",
    "Assume it's a file path",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "TODO this fixes the issue for negative samplers, but does not generally address it.",
    "For example, some of them obscure their arguments with **kwargs, so should we look",
    "at the parent class? Sounds like something to put in class resolver by using the",
    "inspect module. For now, this solution will rely on the fact that the sampler is a",
    "direct descendent of a parent NegativeSampler",
    "create result tracker to allow to gracefully close failed trials",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "close run in result tracker",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "5.1 Learning Rate Scheduler",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.5.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "autodoc_member_order = 'bysource'",
    "autodoc_typehints = 'both' # TODO turn on after 4.1 release",
    "autodoc_preserve_defaults = True",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "Since custom data was passed, we can't store any of this",
    "currently, any custom data doesn't get stored.",
    "self.assertEqual(Nations.get_normalized_name(), hpo_pipeline_result.study.user_attrs['dataset'])",
    "Since there's no source path information, these shouldn't be",
    "added, even if it might be possible to infer path information",
    "from the triples factories",
    "Since paths were passed for training, testing, and validation,",
    "they should be stored as study-level attributes",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "check mean rank (MR)",
    "check mean reciprocal rank (MRR)",
    "check hits at k (H@k)",
    "check adjusted mean rank (AMR)",
    "check adjusted mean rank index (AMRI)",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "generate random query tensor",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "check result shape",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "check type",
    "check format",
    "check coverage",
    "Check if multilabels are working correctly",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "DummyModel,",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    "quaternion have four components",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "the combination bias",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "empty lists are falsy",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "The MockModel gives the highest score to the highest entity id",
    "The test triples are created to yield the third highest score on both head and tail prediction",
    "Write new mapped triples to the model, since the model's triples will be used to filter",
    "These triples are created to yield the highest score on both head and tail prediction for the",
    "test triple at hand",
    "The validation triples are created to yield the second highest score on both head and tail prediction for the",
    "test triple at hand",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    "Test consistency of training / validation / testing mapping",
    ": The directory, if there is caching",
    ": The batch size",
    ": The number of negatives per positive for sLCWA training loop.",
    ": The number of entities LCWA training loop / label smoothing.",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "negative scores decreased compared to positive ones",
    "negative scores decreased compared to positive ones",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "check whether the error originates from batch norm for single element batches",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "FIXME isn't any finite number allowed now?",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    ": Static extras to append to the CLI",
    "for reproducible testing",
    "insert shared parameters",
    "move model to correct device",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "TODO: Make sure that inverse triples are created if create_inverse_triples=True",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "check type",
    "check shape",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    ": The input dimension",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "naive implementation, O(n2)",
    "check correct output type",
    "check value range subset",
    "check value range side",
    "check columns",
    "check value range and type",
    "check value range entity IDs",
    "check value range entity labels",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check confidence value range",
    "check support value range",
    "check correct type",
    "check relation_id value range",
    "check pattern value range",
    "check correct type",
    "check relation_id value range",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check statistics for pre-stratified datasets",
    "Check either a github link or author/publication information is given",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "remove batch/num dimension",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize length of r",
    "check for unit length",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "create a new instance with guaranteed dropout",
    "set to training mode",
    "check for different output",
    ": The number of embeddings",
    "check shape",
    "check attributes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "typically, the model takes care of adjusting the dimension size for \"complex\"",
    "tensors, but we have to do it manually here for testing purposes",
    "-*- coding: utf-8 -*-",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "Multiple permutations of loss not necessary for bloom filter since it's more of a",
    "filter vs. no filter thing.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    ": Should negative samples be filtered?",
    "-*- coding: utf-8 -*-",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "-*- coding: utf-8 -*-",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    "Test that half of the subjects and half of the objects are corrupted",
    "same relation",
    "only corruption of a single entity (note: we do not check for exactly 2, since we do not filter).",
    "check that corrupted entities co-occur with the relation in training data",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The instances",
    ": A positive batch",
    ": Kwargs",
    "Generate negative sample",
    "check filter shape if necessary",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "test that the negative triple is not the original positive triple",
    "shape: (batch_size, 1, num_neg)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": The default strategy for optimizing the loss's hyper-parameters",
    "flatten and stack",
    "apply label smoothing if necessary.",
    "TODO: Do label smoothing only once",
    "Sanity check",
    "prepare for broadcasting, shape: (batch_size, 1, 3)",
    "negative_scores have already been filtered in the sampler!",
    "shape: (nnz,)",
    "Sanity check",
    "for LCWA scores, we consider all pairs of positive and negative scores for a single batch element.",
    "note: this leads to non-uniform memory requirements for different batches, depending on the total number of",
    "positive entries in the labels tensor.",
    "This shows how often one row has to be repeated,",
    "shape: (batch_num_positives,), if row i has k positive entries, this tensor will have k entries with i",
    "Create boolean indices for negative labels in the repeated rows, shape: (batch_num_positives, num_entities)",
    "Repeat the predictions and filter for negative labels, shape: (batch_num_pos_neg_pairs,)",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "Sanity check",
    "compute negative weights (without gradient tracking)",
    "clone is necessary since we modify in-place",
    "Split positive and negative scores",
    "Sanity check",
    "negative_scores have already been filtered in the sampler!",
    "(dense) softmax requires unfiltered scores / masking",
    "we need to fill the scores with -inf for all filtered negative examples",
    "EXCEPT if all negative samples are filtered (since softmax over only -inf yields nan)",
    "use filled negatives scores",
    "compute weights (without gradient tracking)",
    "-w * log sigma(-(m + n)) - log sigma (m + p)",
    "p >> -m => m + p >> 0 => sigma(m + p) ~= 1 => log sigma(m + p) ~= 0 => -log sigma(m + p) ~= 0",
    "p << -m => m + p << 0 => sigma(m + p) ~= 0 => log sigma(m + p) << 0 => -log sigma(m + p) >> 0",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "lower bound",
    "upper bound",
    "normalize input",
    "input validation",
    "base case",
    "normalize dim",
    "calculate repeats for each tensor",
    "dimensions along concatenation axis do not need to match",
    "get desired extent along dimension",
    "repeat tensors along axes if necessary",
    "concatenate",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "workaround for complex numbers: manually compute norm",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    "-*- coding: utf-8 -*-",
    "pad with zeros",
    "trim",
    "-*- coding: utf-8 -*-",
    "mask, shape: (num_edges,)",
    "bi-directional message passing",
    "Heuristic for default value",
    "weights",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "other relations",
    "other relations",
    "Select source and target indices as well as edge weights for the",
    "currently considered relation",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, output_dim)",
    "since we may have one node ID appearing multiple times as source",
    "ID, we can save some computation by first reducing to the unique",
    "source IDs, compute transformed representations and afterwards",
    "select these representations for the correct edges.",
    "select unique source node representations",
    "transform representations by relation specific weight",
    "select the uniquely transformed representations for each edge",
    "optional message weighting",
    "message aggregation",
    "self-loops first",
    "the last relation_id refers to the self-loop",
    "Xavier Glorot initialization of each block",
    "view as blocks",
    "self-loop first",
    "other relations",
    "skip relations without edges",
    "compute message, shape: (num_edges_of_type, num_blocks, block_size)",
    "optional message weighting",
    "message aggregation",
    "-*- coding: utf-8 -*-",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    "TODO: Remove this property and update code to use shape instead",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support",
    "TODO: verify that this is our understanding of complex!",
    "wrapper around max_id, for backward compatibility",
    "initialize weights in-place",
    "apply constraints in-place",
    "verify that contiguity is preserved",
    "TODO: move normalizer / regularizer to base class?",
    "freeze",
    "use this instead of a lambda to make sure that it can be pickled",
    "TODO add normalization functions",
    "Resolve edge weighting",
    "dropout",
    "batch norm and bias",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "buffering of enriched representations",
    "invalidate enriched embeddings",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "fixed edges -> pre-compute weights",
    "Cache enriched representations",
    "normalize output dimension",
    "entity-relation composition",
    "edge weighting",
    "message passing weights",
    "linear relation transformation",
    "layer-specific self-loop relation representation",
    "other components",
    "initialize",
    "split",
    "compose",
    "transform",
    "normalization",
    "aggregate by sum",
    "dropout",
    "prepare for inverse relations",
    "update entity representations: mean over self-loops / forward edges / backward edges",
    "Relation transformation",
    "Buffered enriched entity and relation representations",
    "TODO: Check",
    "hidden dimension normalization",
    "Create message passing layers",
    "register buffers for adjacency matrix; we use the same format as PyTorch Geometric",
    "TODO: This always uses all training triples for message passing",
    "initialize buffer of enriched representations",
    "invalidate enriched embeddings",
    "enrich",
    "-*- coding: utf-8 -*-",
    "scaling factor",
    "modulus ~ Uniform[-s, s]",
    "phase ~ Uniform[0, 2*pi]",
    "real part",
    "purely imaginary quaternions unitary",
    "-*- coding: utf-8 -*-",
    "Calculate in-degree, i.e. number of incoming edges",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE.",
    ": The symbolic shapes for relation representations",
    "bring to (b, n, *)",
    "bring to (b, h, r, t, *)",
    "unpack singleton",
    "The appended \"e\" represents the literals that get concatenated",
    "on the entity representations. It does not necessarily have the",
    "same dimension \"d\" as the entity representations.",
    "alternate way of combining entity embeddings + literals",
    "h = torch.cat(h, dim=-1)",
    "h = self.combination(h.view(-1, h.shape[-1])).view(*h.shape[:-1], -1)  # type: ignore",
    "t = torch.cat(t, dim=-1)",
    "t = self.combination(t.view(-1, t.shape[-1])).view(*t.shape[:-1], -1)  # type: ignore",
    ": The functional interaction form",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "The interaction model",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Initialize biases with zero",
    "In the original formulation,",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "there are separate biases for entities in head and tail position",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "-*- coding: utf-8 -*-",
    "repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width",
    "with batch_size' = batch_size * num_heads * num_relations",
    "batch_size', num_input_channels, 2*height, width",
    "batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: (batch_size, num_heads, num_relations, num_tails)",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(b, h, r, t, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (1, 1, 1, 1, f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (b, h, r, 1, d)",
    "transpose t, (b, 1, 1, d, t)",
    "dot product, (b, h, r, 1, t)",
    "composite: (b, h, 1, t, d)",
    "transpose composite: (b, h, 1, d, t)",
    "inner product with relation embedding",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT",
    "global projections",
    "combination, shape: (b, h, r, 1, d)",
    "dot product with t, shape: (b, h, r, t)",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace and ensure constraints",
    "x_3 contraction",
    "x_1 contraction",
    "x_2 contraction",
    "Rotate (=Hamilton product in quaternion space).",
    "Rotation in quaternion space",
    "head interaction",
    "relation interaction (notice that h has been updated)",
    "combination",
    "similarity",
    "-*- coding: utf-8 -*-",
    "Concrete classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "Filter for entities",
    "Filter for relations",
    "No filtering happened",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "pre-filter to keep only topk",
    "if top is larger than the number of available options",
    "generate text",
    "vectorized label lookup",
    "Re-order columns",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "index",
    "select",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "Make sure that the first element has all the right stuff in it",
    "-*- coding: utf-8 -*-",
    "constants",
    "constants",
    "unary",
    "binary",
    "ternary",
    "column names",
    "return candidates",
    "index triples",
    "incoming relations per entity",
    "outgoing relations per entity",
    "indexing triples for fast join r1 & r2",
    "confidence = len(ht.difference(rev_ht)) / support = 1 - len(ht.intersection(rev_ht)) / support",
    "composition r1(x, y) & r2(y, z) => r(x, z)",
    "actual evaluation of the pattern",
    "skip empty support",
    "TODO: Can this happen after pre-filtering?",
    "sort first, for triple order invariance",
    "TODO: what is the support?",
    "cf. https://stackoverflow.com/questions/19059878/dominant-set-of-points-in-on",
    "sort decreasingly. i dominates j for all j > i in x-dimension",
    "if it is also dominated by any y, it is not part of the skyline",
    "group by (relation id, pattern type)",
    "for each group, yield from skyline",
    "determine patterns from triples",
    "drop zero-confidence",
    "keep only skyline",
    "create data frame",
    "iterate relation types",
    "drop zero-confidence",
    "keep only skyline",
    "does not make much sense, since there is always exactly one entry per (relation, pattern) pair",
    "base = skyline(base)",
    "create data frame",
    "-*- coding: utf-8 -*-",
    ": The mapped triples, shape: (num_triples, 3)",
    ": The unique pairs",
    ": The compressed triples in CSR format",
    "convert to csr for fast row slicing",
    ": TODO: do we need these?",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "Create training instances. Use the _create_instances function to allow subclasses",
    "to modify this behavior",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "Ensure the release of memory",
    "Clear optimizer",
    "When using early stopping models have to be saved separately at the best epoch, since the training loop will",
    "due to the patience continue to train after the best epoch and thus alter the model",
    "Create a path",
    "Prepare all of the callbacks",
    "Register a callback for the result tracker, if given",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "Since the model is also used within the stopper, its graph and cache have to be cleared",
    "When the stopper obtained a new best epoch, this model has to be saved for reconstruction",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "During automatic memory optimization only the error message is of interest",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "Includes a call to result_tracker.log_metrics",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "When there wasn't a best epoch the checkpoint path should be None",
    "Delete temporary best epoch model",
    "If the stopper didn't stop the training loop but derived a best epoch, the model has to be reconstructed",
    "at that state",
    "Delete temporary best epoch model",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "This is an entire checkpoint for the optional best model when using early stopping",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "If the checkpoint was saved with a best epoch model from the early stopper, this model has to be retrieved",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples, shape: (batch_size, num_neg_per_pos, 3)",
    "apply filter mask",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Compute negative and positive scores",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A hint for constructing a :class:`MultiTrainingCallback`",
    ": A collection of callbacks",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "To make MyPy happy",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for training (to be used during filtered evaluation)",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extra stats stuff",
    "The optimistic rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The pessimistic rank is the rank when assuming all options with an equal score are placed in front of the",
    "currently considered. Hence, the rank is the number of options which have at least the same score minus one",
    "(as the currently considered option in included in all options). As the rank is one-based, we have to add 1,",
    "which nullifies the \"minus 1\" from before.",
    "The realistic rank is the average of the optimistic and pessimistic rank, and hence the expected rank over",
    "all permutations of the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "Check if it a side or rank type",
    "update old names for metrics and handle spaces",
    "otherwise, assume is hits@k, which is handled differently",
    "Adjusted mean rank calculation",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "Explicitly call reset_parameters to trigger initialization",
    "normalize input",
    "normalization",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The device on which this model and its submodules are stored",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "make sure to call this first, to reset regularizer state!",
    ": Primary embeddings for entities",
    ": Primary embeddings for relations",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Utils",
    "We might be able to relax this later",
    "Old style models should never be looked up",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Initialise relation embeddings to unit length",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Regularization",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "relation offset",
    "diagonal relation transformation matrix",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Constraints are applied through post_parameter_update",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "embeddings",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model\"s hyper-parameters",
    "TODO: Decomposition kwargs",
    "num_bases=dict(type=int, low=2, high=100, q=1),",
    "num_blocks=dict(type=int, low=2, high=20, q=1),",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "create enriched entity representations",
    "Resolve interaction function",
    "set default relation representation",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "combined representation",
    "Resolve interaction function",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.dist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": Secondary embeddings for entities",
    ": Secondary embeddings for relations",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Regular relation embeddings",
    "The relation-specific interaction vector",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    "Normalize relation embeddings",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The LP settings used by [zhang2019]_ for QuatE.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "compatibility: new style fft returns complex tensor",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "if we really need access to the path later, we can expose it as a property",
    "via self.writer.log_dir",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "as_uri() requires the path to be absolute. resolve additionally also normalizes the path",
    ": The column names",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "store set of triples",
    ": some prime numbers for tuple hashing",
    ": The bit-array for the Bloom filter data structure",
    "Allocate bit array",
    "calculate number of hashing rounds",
    "index triples",
    "Store some meta-data",
    "pre-hash",
    "cf. https://github.com/skeeto/hash-prospector#two-round-functions",
    "-*- coding: utf-8 -*-",
    "Set the indices",
    "Bind number of negatives to sample",
    "Equally corrupt all sides",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Relations have a different index maximum than entities",
    "At least make sure to not replace the triples by the original value",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    ": A filterer for negative batches",
    "create unfiltered negative batch by corruption",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "TODO: move this warning to PseudoTypeNegativeSampler's constructor?",
    "create index structure",
    ": The array of offsets within the data array, shape: (2 * num_relations + 1,)",
    ": The concatenated sorted sets of head/tail entities",
    "shape: (batch_size, num_neg_per_pos, 3)",
    "Uniformly sample from head/tail offsets",
    "get corresponding entity",
    "and position within triple (0: head, 2: tail)",
    "write into negative batch",
    "-*- coding: utf-8 -*-",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "We at least make sure to not replace the triples by the original value",
    "See below for explanation of why this is on a range of [0, num_entities - 1]",
    "Randomly sample corruption.",
    "Replace heads",
    "Replace tails",
    "To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training triples",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "TODO use pathlib here",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Build up a list of triples if we want to be in the filtered setting",
    "If the user gave custom \"additional_filter_triples\"",
    "Determine whether the validation triples should also be filtered while performing test evaluation",
    "TODO consider implications of duplicates",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available, only if the validation triples are used again",
    "Add logging about evaluator for debugging",
    "If the evaluation still fail using the CPU, the error is raised",
    "When the evaluation failed due to OOM on the GPU due to a batch size set too high, the evaluation is",
    "restarted with PyKEEN's automatic memory optimization",
    "When the evaluation failed due to OOM on the GPU even with automatic memory optimization, the evaluation",
    "is restarted using the cpu",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The mapping from (graph-pair, side) to triple file name",
    ": The internal dataset name",
    ": The hex digest for the zip file",
    "Input validation.",
    "For downloading",
    "For splitting",
    "Whether to create inverse triples",
    "shared directory for multiple datasets.",
    "ensure file is present",
    "TODO: Re-use ensure_from_google?",
    "read all triples from file",
    "some \"entities\" have numeric labels",
    "pandas.read_csv(..., dtype=str) does not work properly.",
    "create triples factory",
    "split",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "relation typing",
    "constants",
    "unique",
    "compute over all triples",
    "Determine group key",
    "Add labels if requested",
    "TODO: Merge with _common?",
    "include hash over triples into cache-file name",
    "include part hash into cache-file name",
    "re-use cached file if possible",
    "select triples",
    "save to file",
    "Prune by support and confidence",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "TODO: Consider merging with other analysis methods",
    "-*- coding: utf-8 -*-",
    "Raise matplotlib level",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Normalize path",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "relative paths within zip file's always follow Posix path, even on Windows",
    "tarfile does not like pathlib",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    "Concrete Classes",
    "Utilities",
    "Assume it's a file path",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO: make it even easier to specify categorical strategies just as lists",
    "if isinstance(info, (tuple, list, set)):",
    "info = dict(type='categorical', choices=list(info))",
    "get log from info - could either be a boolean or string",
    "otherwise, dataset refers to a file that should be automatically split",
    "this could be custom data, so don't store anything. However, it's possible to check if this",
    "was a pre-registered dataset. If that's the desired functionality, we can uncomment the following:",
    "dataset_name = dataset.get_normalized_name()  # this works both on instances and classes",
    "if has_dataset(dataset_name):",
    "study.set_user_attr('dataset', dataset_name)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.4.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "generate random query tensor",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "check type",
    "check format",
    "check coverage",
    "Check if multilabels are working correctly",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "fix seeds for reproducibility",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    ": The directory, if there is caching",
    "TODO update",
    ": The batch size",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "TODO update",
    ": The number of entities.",
    "TODO update",
    ": The number of negative samples",
    "TODO update",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "check whether the error originates from batch norm for single element batches",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "FIXME isn't any finite number allowed now?",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "check type",
    "check shape",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Check minimal statistics",
    "Check statistics for pre-stratified datasets",
    "Check either a github link or author/publication information is given",
    "TestFB15K237 is a stand-in to test the ZipFileRemoteDataset",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "remove batch/num dimension",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize length of r",
    "check for unit length",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "Since MuRE has offsets, the scores do not need to negative",
    "We do not need this, since we do not check for functional consistency anyway",
    "intra-interaction comparison",
    "LiteralInteraction,",
    "-*- coding: utf-8 -*-",
    "TODO: use triple generation",
    "generate random triples",
    ": The number of embeddings",
    "check shape",
    "check attributes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    "Utils",
    ": The default strategy for optimizing the model's hyper-parameters",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "lower bound",
    "upper bound",
    "normalize input",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "workaround for complex numbers: manually compute norm",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "-*- coding: utf-8 -*-",
    "Base Class",
    "Child classes",
    "Utils",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    "-*- coding: utf-8 -*-",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    "TODO: Remove this property and update code to use shape instead",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support",
    "TODO: verify that this is our understanding of complex!",
    "wrapper around max_id, for backward compatibility",
    "initialize weights in-place",
    "apply constraints in-place",
    "verify that contiguity is preserved",
    "TODO: move normalizer / regularizer to base class?",
    "TODO add normalization functions",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Adapter classes",
    "Concrete Classes",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE.",
    ": The symbolic shapes for relation representations",
    "bring to (b, n, *)",
    "bring to (b, h, r, t, *)",
    "unpack singleton",
    ": The functional interaction form",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "The interaction model",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Initialize biases with zero",
    "In the original formulation,",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "there are separate biases for entities in head and tail position",
    "the base interaction",
    "forward entity/relation shapes",
    "The parameters of the affine transformation: bias",
    "scale. We model this as log(scale) to ensure scale > 0, and thus monotonicity",
    "-*- coding: utf-8 -*-",
    ": The batch size of the head representations.",
    ": The number of head representations per batch",
    ": The batch size of the relation representations.",
    ": The number of relation representations per batch",
    ": The batch size of the tail representations.",
    ": The number of tail representations per batch",
    "repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width",
    "with batch_size' = batch_size * num_heads * num_relations",
    "batch_size', num_input_channels, 2*height, width",
    "batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: (batch_size, num_heads, num_relations, num_tails)",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(b, h, r, t, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (1, 1, 1, 1, f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (b, h, r, 1, d)",
    "transpose t, (b, 1, 1, d, t)",
    "dot product, (b, h, r, 1, t)",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (b, h, 1, t, d)",
    "transpose composite: (b, h, 1, d, t)",
    "inner product with relation embedding",
    "global projections",
    "combination, shape: (b, h, r, 1, d)",
    "dot product with t, shape: (b, h, r, t)",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace and ensure constraints",
    "x_3 contraction",
    "x_1 contraction",
    "x_2 contraction",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "distribute the deteriorated triples across the remaining factories",
    "'kinships',",
    "'umls',",
    "'codexsmall',",
    "'wn18',",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "Filter for entities",
    "Filter for relations",
    "No filtering happened",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "pre-filter to keep only topk",
    "generate text",
    "vectorized label lookup",
    "Re-order columns",
    "FIXME currently the implementation does not consider the non-training (i.e., second-last entries)",
    "for the number of steps. Consider more interesting way to discuss splits w/ valid",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "index",
    "select",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "Make sure that the first element has all the right stuff in it",
    "-*- coding: utf-8 -*-",
    ": The mapped triples, shape: (num_triples, 3)",
    ": The unique pairs",
    ": The compressed triples in CSR format",
    "convert to csr for fast row slicing",
    ": TODO: do we need these?",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Check if it a side or rank type",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "TODO pack/unpack dimensions as default kwargs such that they don't actually need to be used",
    "to create the class",
    "TODO: Does not work for interactions with separate tail_entity_shape (i.e., ConvE)",
    "-*- coding: utf-8 -*-",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    "cf. https://github.com/mberr/ea-sota-comparison/blob/6debd076f93a329753d819ff4d01567a23053720/src/kgm/utils/torch_utils.py#L317-L372   # noqa:E501",
    "Make sure that all modules with parameters do have a reset_parameters method.",
    "Recursively visit all sub-modules",
    "skip self",
    "Track parents for blaming",
    "call reset_parameters if possible",
    "initialize from bottom to top",
    "This ensures that specialized initializations will take priority over the default ones of its components.",
    "emit warning if there where parameters which were not initialised by reset_parameters.",
    "Additional debug information",
    "Important: use ModuleList to ensure that Pytorch correctly handles their devices and parameters",
    ": The entity representations",
    ": The relation representations",
    ": The weight regularizers",
    "Comment: it is important that the regularizers are stored in a module list, in order to appear in",
    "model.modules(). Thereby, we can collect them automatically.",
    "normalize input",
    "normalization",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "-*- coding: utf-8 -*-",
    ": Keep track of if this is a base model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": A triples factory with the training triples",
    ": The device on which this model and its submodules are stored",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "The triples factory facilitates access to the dataset.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "make sure to call this first, to reset regularizer state!",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    "Utils",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Initialise relation embeddings to unit length",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Regularization",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "comment:",
    "https://github.com/ibalazevic/multirelational-poincare/blob/34523a61ca7867591fd645bfb0c0807246c08660/model.py#L52",
    "uses float64",
    "entity bias for head",
    "entity bias for tail",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default entity normalizer parameters",
    ": The entity representations are normalized to L2 unit length",
    ": cf. https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L232-L240  # noqa: E501",
    "update initializer settings, cf.",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L45-L49",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/model.py#L29",
    "https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE/blob/0a95bcd54759207984c670af92ceefa19dd248ad/biokg/run.py#L50",
    "in the original implementation the embeddings are initialized in one parameter",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Constraints are applied through post_parameter_update",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "embeddings",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "normalize representations",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "check decomposition",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "buffering of messages",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "invalidate enriched embeddings",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "TODO: Replace this by interaction function, once https://github.com/pykeen/pykeen/pull/107 is merged.",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Dummy",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.dist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "TODO: this is very similar to ComplExLiteral, except a few dropout differences",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "as_uri() requires the path to be absolute. resolve additionally also normalizes the path",
    ": The column names",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Set the indices",
    "Bind number of negatives to sample",
    "Equally corrupt all sides",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Relations have a different index maximum than entities",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Create mapped triples attribute that is required for filtering",
    "Make sure the mapped triples are initiated",
    "Copy the mapped triples to the device for efficient filtering",
    "Check which heads of the mapped triples are also in the negative triples",
    "Reduce the search space by only using possible matches that at least contain the head we look for",
    "Check in this subspace which relations of the mapped triples are also in the negative triples",
    "Reduce the search space by only using possible matches that at least contain head and relation we look for",
    "Create a filter indicating which of the proposed negative triples are positive in the training dataset",
    "In cases where no triples should be filtered, the subspace reduction technique above will fail",
    "Return only those proposed negative triples that are not positive in the training dataset",
    "-*- coding: utf-8 -*-",
    "Utils",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption. See below for explanation of",
    "why this is on a range of [0, num_entities - 1]",
    "Replace heads",
    "Replace tails",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "FIXME this should never happen.",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "TODO should training be reset?",
    "TODO should kwargs for loss and regularizer be checked and raised for?",
    "Log model parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is",
    "normally the default for all of PyKEEN's remote datasets, so just switch the default here.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is",
    "normally the default for all of PyKEEN's remote datasets, so just switch the default here.",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is",
    "normally the default for all of PyKEEN's remote datasets, so just switch the default here.",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is",
    "normally the default for all of PyKEEN's remote datasets, so just switch the default here.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    ": URL to the data to download",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v1.3.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "show todo's",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "ensure regularizer is on correct device",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    "create broadcastable shapes",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "generate random query tensor",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "check equivalence of re-formulation",
    "e_{\\bot} = M_{re} e = (r_p e_p^T + I^{d_r \\times d_e}) e",
    "= r_p (e_p^T e) + e'",
    "create random array, estimate the costs of addition, and measure some execution times.",
    "then, compute correlation between the estimated cost, and the measured time.",
    "check for strong correlation between estimated costs and measured execution time",
    "get optimal sequence",
    "check caching",
    "get optimal sequence",
    "check correct cost",
    "check optimality",
    "compare result to sequential addition",
    "compare result to sequential addition",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "set create inverse triple to true",
    "split factory",
    "check that in *training* inverse triple are to be created",
    "check that in all other splits no inverse triples are to be created",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "check type",
    "check format",
    "check coverage",
    "Check if multilabels are working correctly",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "fix seeds for reproducibility",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    ": The directory, if there is caching",
    "TODO update",
    ": The batch size",
    "test reduction",
    "test finite loss value",
    "Test backward",
    "TODO update",
    ": The number of entities.",
    "TODO update",
    ": The number of negative samples",
    "TODO update",
    ": The number of entities.",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    ": The equivalence for models with batch norm only holds in evaluation mode",
    "check whether the error originates from batch norm for single element batches",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "set in eval mode (otherwise there are non-deterministic factors like Dropout",
    "test multiple different initializations",
    "calculate by functional",
    "calculate manually",
    "simple",
    "nested",
    "nested",
    "prepare a temporary test directory",
    "check that file was created",
    "make sure to close file before trying to delete it",
    "delete intermediate files",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "move test instance to device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "FIXME isn't any finite number allowed now?",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "TODO: Catch HolE MKL error?",
    "set regularizer term to something that isn't zero",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "check type",
    "check shape",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "W_L drop(act(W_C \\ast ([h; r; t]) + b_C)) + b_L",
    "prepare conv input (N, C, H, W)",
    "f(h,r,t) = u_r^T act(h W_r t + V_r h + V_r t + b_r)",
    "shapes: w: (k, dim, dim), vh/vt: (k, dim), b/u: (k,), h/t: (dim,)",
    "remove batch/num dimension",
    "f(h, r, t) = g(t z(D_e h + D_r r + b_c) + b_p)",
    "f(h, r, t) = h @ r @ t",
    "DO_{hr}(BN_{hr}(DO_h(BN_h(h)) x_1 DO_r(W x_2 r))) x_3 t",
    "normalize length of r",
    "check for unit length",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Compute Scores",
    "-\\|R_h h - R_t t\\|",
    "-\\|h - t\\|",
    "LiteralInteraction,",
    "-*- coding: utf-8 -*-",
    "TODO: use triple generation",
    "generate random triples",
    ": The number of embeddings",
    "check shape",
    "check attributes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "ensure positivity",
    "compute using pytorch",
    "prepare distributions",
    "compute using pykeen",
    "e: (batch_size, num_heads, num_tails, d)",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence = 0 => similarity = -divergence = 0",
    "(h - t), r",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties",
    "divergence >= 0 => similarity = -divergence <= 0",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of optimizers' names to their implementations",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "Helpers",
    "Base Classes",
    "Concrete Classes",
    ": The default strategy for optimizing the model's hyper-parameters",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    ": A mapping of losses' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": A subdirectory for PyKEEN logs",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "lower bound",
    "upper bound",
    "normalize input",
    "create sorted list of shapes to allow utilization of lru cache (optimal execution order does not depend on the",
    "input sorting, as the order is determined by re-ordering the sequence anyway)",
    "Determine optimal order and cost",
    "translate back to original order",
    "determine optimal processing order",
    "heuristic",
    "workaround for complex numbers: manually compute norm",
    "TODO: check if einsum is still very slow.",
    "TODO: t_shape = list(t.shape); del t_shape[i]; t.view(*shape) -> only one reshape operation",
    "unsqueeze",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": Has this regularizer been updated since last being reset?",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "If there are tracked parameters, update based on them",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    ": A mapping of regularizers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "General types",
    "Triples",
    "Others",
    "Tensor Functions",
    "Tensors",
    "Dataclasses",
    ": A function that mutates the input and returns a new object of the same type as output",
    ": A function that can be applied to a tensor to initialize it",
    ": A function that can be applied to a tensor to normalize it",
    ": A function that can be applied to a tensor to constrain it",
    ": A hint for a :class:`torch.device`",
    ": A hint for a :class:`torch.Generator`",
    ": A type variable for head representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for relation representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    ": A type variable for tail representations used in :class:`pykeen.models.Model`,",
    ": :class:`pykeen.nn.modules.Interaction`, etc.",
    "-*- coding: utf-8 -*-",
    ": the maximum ID (exclusively)",
    ": the shape of an individual representation",
    "TODO: Remove this property and update code to use shape instead",
    "normalize embedding_dim vs. shape",
    "work-around until full complex support",
    "TODO: verify that this is our understanding of complex!",
    "wrapper around max_id, for backward compatibility",
    "initialize weights in-place",
    "apply constraints in-place",
    "verify that contiguity is preserved",
    "TODO: move normalizer / regularizer to base class?",
    "TODO add normalization functions",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO test",
    "subtract, shape: (batch_size, num_heads, num_relations, num_tails, dim)",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    "1. Component",
    "\\sum_i \\Sigma_e[i] / Sigma_r[i]",
    "2. Component",
    "(mu_1 - mu_0) * Sigma_1^-1 (mu_1 - mu_0)",
    "with mu = (mu_1 - mu_0)",
    "= mu * Sigma_1^-1 mu",
    "since Sigma_1 is diagonal",
    "= mu**2 / sigma_1",
    "3. Component",
    "4. Component",
    "ln (det(\\Sigma_1) / det(\\Sigma_0))",
    "= ln det Sigma_1 - ln det Sigma_0",
    "since Sigma is diagonal, we have det Sigma = prod Sigma[ii]",
    "= ln prod Sigma_1[ii] - ln prod Sigma_0[ii]",
    "= sum ln Sigma_1[ii] - sum ln Sigma_0[ii]",
    "allocate result",
    "prepare distributions",
    "-*- coding: utf-8 -*-",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "TODO benchmark",
    "h = h_re, -h_im",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base Classes",
    "Concrete Classes",
    ": The symbolic shapes for entity representations",
    ": The symbolic shapes for entity representations for tail entities, if different. This is ony relevant for ConvE.",
    ": The symbolic shapes for relation representations",
    "bring to (b, n, *)",
    "bring to (b, h, r, t, *)",
    "unpack singleton",
    ": The functional interaction form",
    "Store initial input for error message",
    "All are None -> try and make closest to square",
    "Only input channels is None",
    "Only width is None",
    "Only height is none",
    "Width and input_channels are None -> set input_channels to 1 and calculage height",
    "Width and input channels are None -> set input channels to 1 and calculate width",
    ": The head-relation encoder operating on 2D \"images\"",
    ": The head-relation encoder operating on the 1D flattened version",
    ": The interaction function",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "encoders",
    "1: 2D encoder: BN?, DO, Conv, BN?, Act, DO",
    "2: 1D encoder: FC, DO, BN?, Act",
    "store reshaping dimensions",
    "The interaction model",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Initialize biases with zero",
    "In the original formulation,",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "batch norm gets reset automatically, since it defines reset_parameters",
    "shapes",
    "-*- coding: utf-8 -*-",
    ": The batch size of the head representations.",
    ": The number of head representations per batch",
    ": The batch size of the relation representations.",
    ": The number of relation representations per batch",
    ": The batch size of the tail representations.",
    ": The number of tail representations per batch",
    "repeat if necessary, and concat head and relation, batch_size', num_input_channels, 2*height, width",
    "with batch_size' = batch_size * num_heads * num_relations",
    "batch_size', num_input_channels, 2*height, width",
    "batch_size', num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "reshape: (batch_size', embedding_dim) -> (b, h, r, 1, d)",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "output_shape: (batch_size, num_heads, num_relations, num_tails)",
    "add bias term",
    "decompose convolution for faster computation in 1-n case",
    "compute conv(stack(h, r, t))",
    "prepare input shapes for broadcasting",
    "(b, h, r, t, 1, d)",
    "conv.weight.shape = (C_out, C_in, kernel_size[0], kernel_size[1])",
    "here, kernel_size = (1, 3), C_in = 1, C_out = num_filters",
    "-> conv_head, conv_rel, conv_tail shapes: (num_filters,)",
    "reshape to (1, 1, 1, 1, f, 1)",
    "convolve -> output.shape: (*, embedding_dim, num_filters)",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores; use flattened representations, shape: (b, h, r, t, d * f)",
    "same shape",
    "split, shape: (embedding_dim, hidden_dim)",
    "repeat if necessary, and concat head and relation, (batch_size, num_heads, num_relations, 1, 2 * embedding_dim)",
    "Predict t embedding, shape: (b, h, r, 1, d)",
    "transpose t, (b, 1, 1, d, t)",
    "dot product, (b, h, r, 1, t)",
    "Circular correlation of entity embeddings",
    "complex conjugate",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (b, h, 1, t, d)",
    "transpose composite: (b, h, 1, d, t)",
    "inner product with relation embedding",
    "global projections",
    "combination, shape: (b, h, r, 1, d)",
    "dot product with t, shape: (b, h, r, t)",
    "r expresses a rotation in complex plane.",
    "rotate head by relation (=Hadamard product in complex space)",
    "rotate tail by inverse of relation",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is made optional here.",
    "Project entities",
    "h projection to hyperplane",
    "r",
    "-t projection to hyperplane",
    "project to relation specific subspace and ensure constraints",
    "x_3 contraction",
    "x_1 contraction",
    "x_2 contraction",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from labels to IDs.",
    ": The inverse mapping for label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    "Normalize input",
    "label",
    "check new label to ID mappings",
    "Make new triples factories for each group",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "Input validation",
    "convert to numpy",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "Filter for entities",
    "Filter for relations",
    "No filtering happened",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "pre-filter to keep only topk",
    "generate text",
    "vectorized label lookup",
    "Re-order columns",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "index",
    "select",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "Make sure that the first element has all the right stuff in it",
    "-*- coding: utf-8 -*-",
    ": The mapped triples, shape: (num_triples, 3)",
    ": The unique pairs",
    ": The compressed triples in CSR format",
    "convert to csr for fast row slicing",
    ": TODO: do we need these?",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "MyPy overrides are because you should",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "TODO why not call torch.cuda.empty_cache()? or call self._free_graph_and_cache()?",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    ": A mapping of stoppers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "Since squeeze is true, we can expect that evaluate returns a MetricResult, but we need to tell MyPy that",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "values_dict[key] will always be an int at this point",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Check if it a side or rank type",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "-*- coding: utf-8 -*-",
    ": Keep track of if this is a base model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": A triples factory with the training triples",
    ": The device on which this model and its submodules are stored",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "The triples factory facilitates access to the dataset.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Regularizer",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "make sure to call this first, to reset regularizer state!",
    "make sure to call this first, to reset regularizer state!",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    "-*- coding: utf-8 -*-",
    "Base Models",
    "Concrete Models",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO rethink after RGCN update",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Initialise relation embeddings to unit length",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Regularization",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Constraints are applied through post_parameter_update",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "embeddings",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "normalize representations",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "check decomposition",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "buffering of messages",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "invalidate enriched embeddings",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "TODO: Replace this by interaction function, once https://github.com/pykeen/pykeen/pull/107 is merged.",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Dummy",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.dist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default settings for the entity constrainer",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "TODO: this is very similar to ComplExLiteral, except a few dropout differences",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Base classes",
    "Concrete classes",
    "Utilities",
    ": A mapping of trackers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The file extension for this writer (do not include dot)",
    ": The file where the results are written to.",
    "as_uri() requires the path to be absolute. resolve additionally also normalizes the path",
    ": The column names",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Set the indices",
    "Bind number of negatives to sample",
    "Equally corrupt all sides",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Relations have a different index maximum than entities",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Create mapped triples attribute that is required for filtering",
    "Make sure the mapped triples are initiated",
    "Copy the mapped triples to the device for efficient filtering",
    "Check which heads of the mapped triples are also in the negative triples",
    "Reduce the search space by only using possible matches that at least contain the head we look for",
    "Check in this subspace which relations of the mapped triples are also in the negative triples",
    "Reduce the search space by only using possible matches that at least contain head and relation we look for",
    "Create a filter indicating which of the proposed negative triples are positive in the training dataset",
    "In cases where no triples should be filtered, the subspace reduction technique above will fail",
    "Return only those proposed negative triples that are not positive in the training dataset",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption. See below for explanation of",
    "why this is on a range of [0, num_entities - 1]",
    "Replace heads",
    "Replace tails",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is",
    "normally the default for all of PyKEEN's remote datasets, so just switch the default here.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is",
    "normally the default for all of PyKEEN's remote datasets, so just switch the default here.",
    "as pointed out in https://github.com/pykeen/pykeen/issues/275#issuecomment-776412294,",
    "the columns are not ordered properly.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "do not explicitly create inverse triples for testing; this is handled by the evaluation code",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    ": A mapping of HPO samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of HPO pruners' names to their implementations",
    "-*- coding: utf-8 -*-"
  ],
  "v1.1.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "Train a model in one shot",
    "Train a model for the first half",
    "Continue training of the first part",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "check if within 0.5 std of observed",
    "test error is raised",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    ": The number of embeddings",
    ": The embedding dimension",
    "check shape",
    "check values",
    "check shape",
    "check values",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "generate random query tensor",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "comment(mberr): from my pov this behaviour is faulty: the triples factory is expected to say it contains",
    "inverse relations, although the triples contained in it are not the same we would have when removing the",
    "first triple, and passing create_inverse_triples=True.",
    "check for warning",
    "check for filtered triples",
    "check for correct inverse triples flag",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "check compressed triples",
    "reconstruct triples from compressed form",
    "check data loader",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "check type",
    "check format",
    "check coverage",
    "Check if multilabels are working correctly",
    "generate random ratios",
    "check size",
    "check value range",
    "check total split",
    "check consistency with ratios",
    "the number of decimal digits equivalent to 1 / n_total",
    "check type",
    "check values",
    "compare against expected",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Check whether filtering works correctly",
    "First giving an example where all triples have to be filtered",
    "The filter should remove all triples",
    "Create an example where no triples will be filtered",
    "The filter should not remove any triple",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": Additional arguments passed to the training loop's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "For the high/low memory test cases of NTN, SE, etc.",
    "else, leave to default",
    "TODO: Catch HolE MKL error?",
    "set regularizer term",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "call some functions",
    "reset to old state",
    "call some functions",
    "reset to old state",
    "Distance-based model",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "check shape",
    "check content",
    "-*- coding: utf-8 -*-",
    "As the resumption capability currently is a function of the training loop, more thorough tests can be found",
    "in the test_training.py unit tests. In the tests below the handling of training loop checkpoints by the",
    "pipeline is checked.",
    "Set up a shared result that runs two pipelines that should replicate the results of the standard pipeline.",
    "Resume the previous pipeline",
    "-*- coding: utf-8 -*-",
    "expected_frequency = n / (n + len(forwards_extras) + len(inverse_extras))",
    "self.assertLessEqual(min_frequency, expected_frequency)",
    "Test looking up inverse triples",
    "test new label to ID",
    "type",
    "old labels",
    "new, compact IDs",
    "test vectorized lookup",
    "type",
    "shape",
    "value range",
    "only occurring Ids get mapped to non-negative numbers",
    "Ids are mapped to (0, ..., num_unique_ids-1)",
    "check type",
    "check shape",
    "check content",
    "check type",
    "check shape",
    "check 1-hot",
    "check type",
    "check shape",
    "check value range",
    "check self-similarity = 1",
    "base relation",
    "exact duplicate",
    "99% duplicate",
    "50% duplicate",
    "exact inverse",
    "99% inverse",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The expected number of triples",
    ": The tolerance on expected number of triples, for randomized situations",
    ": The dataset to test",
    ": The instantiated dataset",
    ": Should the validation be assumed to have been loaded with train/test?",
    "Not loaded",
    "Load",
    "Test caching",
    "assert (end - start) < 1.0e-02",
    ": The directory, if there is caching",
    ": The class",
    ": Constructor keyword arguments",
    ": The loss instance",
    ": The batch size",
    "test reduction",
    "Test backward",
    ": The number of entities.",
    ": The number of negative samples",
    ": The number of entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "TODO reimplement then test MarginRankingLoss using PR #18 solution",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of optimizers' names to their implementations",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "Helpers",
    "Base Classes",
    "Concrete Classes",
    ": The default strategy for optimizing the model's hyper-parameters",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    ": A mapping of losses' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": A manager around the PyKEEN data folder. It defaults to ``~/.data/pykeen``.",
    "This can be overridden with the envvar ``PYKEEN_HOME``.",
    ": For more information, see https://github.com/cthoyt/pystow",
    ": A path representing the PyKEEN data folder",
    ": A subdirectory of the PyKEEN data folder for datasets, defaults to ``~/.data/pykeen/datasets``",
    ": A subdirectory of the PyKEEN data folder for benchmarks, defaults to ``~/.data/pykeen/benchmarks``",
    ": A subdirectory of the PyKEEN data folder for experiments, defaults to ``~/.data/pykeen/experiments``",
    ": A subdirectory of the PyKEEN data folder for checkpoints, defaults to ``~/.data/pykeen/checkpoints``",
    ": We define the embedding dimensions as a multiple of 16 because it is computational beneficial (on a GPU)",
    ": see: https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "lower bound",
    "upper bound",
    "normalize input",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the no-op regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    "TODO: this only works for x ~ N(0, 1), but not for |x|",
    "cf. https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution",
    "mean = scipy.stats.norm.ppf(1 - 1/d)",
    "scale = scipy.stats.norm.ppf(1 - 1/d * 1/math.e) - mean",
    "return scipy.stats.gumbel_r.mean(loc=mean, scale=scale)",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the LP regularizer's hyper-parameters",
    ": The default strategy for optimizing the power sum regularizer's hyper-parameters",
    ": The default strategy for optimizing the TransH regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    ": A mapping of regularizers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "To allow resuming training from a checkpoint when using a pipeline, the pipeline needs to obtain the",
    "used random_seed to ensure reproducible results",
    "We have to set clear optimizer to False since training should be continued",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "comment: TypeVar expects none, or at least two super-classes",
    "-*- coding: utf-8 -*-",
    "initialize weights in-place",
    "apply constraints in-place",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "don't worry about functions because they can't be specified by JSON.",
    "Could make a better mo",
    "later could extend for other non-JSON valid types",
    "-*- coding: utf-8 -*-",
    "Score with original triples",
    "Score with inverse triples",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "-*- coding: utf-8 -*-",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    "normalize input",
    ": The mapping from entities' labels to their indices",
    ": The mapping from relations' labels to their indices",
    ": A three-column matrix where each row are the head identifier,",
    ": relation identifier, then tail identifier",
    ": Whether to create inverse triples",
    ": Arbitrary metadata to go with the graph",
    "The following fields get generated automatically",
    ": The inverse mapping for entity_label_to_id; initialized automatically",
    ": The inverse mapping for relation_label_to_id; initialized automatically",
    ": A vectorized version of entity_label_to_id; initialized automatically",
    ": A vectorized version of relation_label_to_id; initialized automatically",
    ": A vectorized version of entity_id_to_label; initialized automatically",
    ": A vectorized version of relation_id_to_label; initialized automatically",
    "ID to label mapping",
    "vectorized versions",
    "Check if the triples are inverted already",
    "We re-create them pure index based to ensure that _all_ inverse triples are present and that they are",
    "contained if and only if create_inverse_triples is True.",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "TODO: Check if lazy evaluation would make sense",
    "Make new triples factories for each group",
    "pre-filter to keep only topk",
    "generate text",
    "Input validation",
    "convert to numpy",
    "vectorized label lookup",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "Filter for entities",
    "Filter for relations",
    "No filtering happened",
    "-*- coding: utf-8 -*-",
    "Split indices",
    "Split triples",
    "index",
    "select",
    "Prepare split index",
    "due to rounding errors we might lose a few points, thus we use cumulative ratio",
    "[...] is necessary for Python 3.7 compatibility",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "add to training",
    "remove from testing",
    "Recalculate the move_id_mask",
    "base cases",
    "IDs not in training",
    "triples with exclusive test IDs",
    "Make sure that the first element has all the right stuff in it",
    "-*- coding: utf-8 -*-",
    ": The mapped triples, shape: (num_triples, 3)",
    ": The unique pairs",
    ": The compressed triples in CSR format",
    "convert to csr for fast row slicing",
    ": TODO: do we need these?",
    "-*- coding: utf-8 -*-",
    "check validity",
    "path compression",
    "collect connected components using union find with path compression",
    "get representatives",
    "already merged",
    "make x the smaller one",
    "merge",
    "extract partitions",
    "safe division for empty sets",
    "compute unique pairs in triples *and* inverted triples for consistent pair-to-id mapping",
    "duplicates",
    "we are not interested in self-similarity",
    "compute similarities",
    "Calculate which relations are the inverse ones",
    "get existing IDs",
    "remove non-existing ID from label mapping",
    "create translation tensor",
    "get entities and relations occurring in triples",
    "generate ID translation and new label to Id mappings",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "The internal epoch state tracks the last finished epoch of the training loop to allow for",
    "seamless loading and saving of training checkpoints",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "A checkpoint root is always created to ensure a fallback checkpoint can be saved",
    "If a checkpoint file is given, it must be loaded if it exists already",
    "If the stopper dict has any keys, those are written back to the stopper",
    "The checkpoint frequency needs to be set to save checkpoints",
    "In case a checkpoint frequency was set, we warn that no checkpoints will be saved",
    "If no checkpoints were requested, a fallback checkpoint is set in case the training loop crashes",
    "If the stopper loaded from the training loop checkpoint stopped the training, we return those results",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Save the time to track when the saved point was available",
    "Training Loop",
    "When training with an early stopper the memory pressure changes, which may allow for errors each epoch",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "Save the last successful finished epoch",
    "When the training loop failed, a fallback checkpoint is created to resume training.",
    "If a checkpoint file is given, we check whether it is time to save a checkpoint",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "reset the regularizer to free the computational graph",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The regularizer has to be reset to free the computational graph",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Only if a cuda device is available, the random state is accessed",
    "Cuda requires its own random state, which can only be set when a cuda device is available",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Dummy result tracker",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    ": A mapping of stoppers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Using automatic memory optimization on CPU may result in undocumented crashes due to OS' OOM killer.",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "This should be a reasonable default size that works on most setups while being faster than batch_size=1",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Check if it a side or rank type",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    "Keep track of the hyper-parameters that are used across all",
    "subclasses of BaseModule",
    "The following lines add in a post-init hook to all subclasses",
    "such that the reset_parameters_() function is run",
    "sorry mypy, but this kind of evil must be permitted.",
    ": A dictionary of hyper-parameters to the models that use them",
    ": Keep track of if this is a base model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Check loss functions that require 1 and -1 as label but only",
    "Regularizer",
    "The triples factory facilitates access to the dataset.",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "Sort final result",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "set model to evaluation mode",
    "Do not track gradients",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "Sort final result",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "TODO UNUSED",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "make sure to call this first, to reset regularizer state!",
    "Default for relation dimensionality",
    "make sure to call this first, to reset regularizer state!",
    "-*- coding: utf-8 -*-",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Store initial input for error message",
    "All are None",
    "input_channels is None, and any of height or width is None -> set input_channels=1",
    "input channels is not None, and one of height or width is None",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Initialise relation embeddings to unit length",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Regularization",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "xavier uniform, cf.",
    "https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17",
    "Constrain entity embeddings to unit length",
    "relations are initialized to unit length (but not constraint)",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Constraints are applied through post_parameter_update",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Normalise the normal vectors by their l2 norms",
    "TODO: Add initialization",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Initialize from TransE",
    "embeddings",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "normalize representations",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "check decomposition",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "buffering of messages",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "invalidate enriched embeddings",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "TODO: Replace this by interaction function, once https://github.com/pykeen/pykeen/pull/107 is merged.",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "TODO: Dummy",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Get embeddings",
    "TODO: Use torch.dist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "Get embeddings",
    "TODO: Use torch.cdist",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "TODO: this is very similar to ComplExLiteral, except a few dropout differences",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of trackers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Set the indices",
    "Bind number of negatives to sample",
    "Equally corrupt all sides",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Relations have a different index maximum than entities",
    "To make sure we don't replace the {head, relation, tail} by the",
    "original value we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_{heads, relations, tails} -1]",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Create mapped triples attribute that is required for filtering",
    "Make sure the mapped triples are initiated",
    "Copy the mapped triples to the device for efficient filtering",
    "Check which heads of the mapped triples are also in the negative triples",
    "Reduce the search space by only using possible matches that at least contain the head we look for",
    "Check in this subspace which relations of the mapped triples are also in the negative triples",
    "Reduce the search space by only using possible matches that at least contain head and relation we look for",
    "Create a filter indicating which of the proposed negative triples are positive in the training dataset",
    "In cases where no triples should be filtered, the subspace reduction technique above will fail",
    "Return only those proposed negative triples that are not positive in the training dataset",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption. See below for explanation of",
    "why this is on a range of [0, num_entities - 1]",
    "Replace heads",
    "Replace tails",
    "If filtering is activated, all negative triples that are positive in the training dataset will be removed",
    "To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "If GitHub ever gets upset from too many downloads, we can switch to",
    "the data posted at https://github.com/pykeen/pykeen/pull/154#issuecomment-730462039",
    "GitHub's raw.githubusercontent.com service rejects requests that are streamable. This is",
    "normally the default for all of PyKEEN's remote datasets, so just switch the default here.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The name of the dataset to download",
    "FIXME these are already identifiers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indices with the training triples",
    ": A factory wrapping the validation triples, that share indices with the training triples",
    ": All datasets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    ": The directory in which the cached data is stored",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    ": A mapping of HPO samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of HPO pruners' names to their implementations",
    "-*- coding: utf-8 -*-"
  ],
  "v1.0.5": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "latex_elements = {",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "}",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "latex_documents = [",
    "(",
    "master_doc,",
    "'pykeen.tex',",
    "'PyKEEN Documentation',",
    "author,",
    "'manual',",
    "),",
    "]",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "epub_title = project",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "epub_exclude_files = ['search.html']",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    ": The number of embeddings",
    ": The embedding dimension",
    "check shape",
    "check values",
    "check shape",
    "check values",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Check if multilabels are working correctly",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The dataset to test",
    "Not loaded",
    "Load",
    "Test caching",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "TODO: Catch HolE MKL error?",
    "set regularizer term",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "Distance-based model",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "self.assertAlmostEqual(second_score, -16, delta=0.01)",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "check shape",
    "check content",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The class",
    ": Constructor keyword arguments",
    ": The loss instance",
    ": The batch size",
    "test reduction",
    "Test backward",
    ": The number of entities.",
    ": The number of negative samples",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of optimizers' names to their implementations",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "To add *all* losses implemented in Torch, uncomment:",
    "_LOSSES.update({",
    "loss",
    "for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()",
    "if not loss.__name__.startswith('_')",
    "})",
    ": A mapping of losses' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.",
    "lower bound",
    "upper bound",
    "Allocate weight on device",
    "Initialize if initializer is provided",
    "Wrap embedding around it.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "expected value of |x|_1 = d*E[x_i] for x_i i.i.d.",
    "expected value of |x|_2 when x_i are normally distributed",
    "cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    ": A mapping of regularizers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    ": The WANDB run",
    ": A mapping of trackers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "FIXME is this ever possible, since this function is called in __init__?",
    "-*- coding: utf-8 -*-",
    "Create lists out of sets for proper numpy indexing when loading the labels",
    "TODO is there a need to have a canonical sort order here?",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    ": The mapping from entities' labels to their indexes",
    ": The mapping from relations' labels to their indexes",
    ": A three-column matrix where each row are the head label,",
    ": relation label, then tail label",
    ": A three-column matrix where each row are the head identifier,",
    ": relation identifier, then tail identifier",
    ": A dictionary mapping each relation to its inverse, if inverse triples were created",
    "TODO: Check if lazy evaluation would make sense",
    "Check if the triples are inverted already",
    "extend original triples with inverse ones",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "We can terminate the search after finding the first inverse occurrence",
    "Ensure 2d array in case only one triple was given",
    "FIXME this function is only ever used in tests",
    "Prepare shuffle index",
    "Prepare split index",
    "Take cumulative sum so the get separated properly",
    "Split triples",
    "Make sure that the first element has all the right stuff in it",
    "Make new triples factories for each group",
    "Input validation",
    "convert to numpy",
    "vectorized label lookup",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "Filter for entities",
    "Filter for relations",
    "No filtering happened",
    "manually copy the inverse relation mappings",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "Recalculate the testing triples without that index",
    "Recalculate the training entities, testing entities, to_move, and move_id_mask",
    "-*- coding: utf-8 -*-",
    ": A PyTorch tensor of triples",
    ": A mapping from relation labels to integer identifiers",
    ": A mapping from relation labels to integer identifiers",
    "Create dense target",
    "-*- coding: utf-8 -*-",
    "basically take all candidates",
    "Calculate which relations are the inverse ones",
    "FIXME doesn't carry flag of create_inverse_triples through",
    "A dictionary of all of the head/tail pairs for a given relation",
    "A dictionary for all of the tail/head pairs for a given relation",
    "Calculate the similarity between each relationship (entries in ``forward``)",
    "with all other candidate inverse relationships (entries in ``inverse``)",
    "Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the",
    "same as for ``(b, a)``",
    "A dictionary of all of the head/tail pairs for a given relation",
    "Filter out results between a given relationship and itself",
    "Filter out results below a minimum frequency",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Training Loop",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "reset the regularizer to free the computational graph",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The regularizer has to be reset to free the computational graph",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Dummy result tracker",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    ": A mapping of stoppers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Check if it a side or rank type",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    ": A dictionary of hyper-parameters to the models that use them",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Check loss functions that require 1 and -1 as label but only",
    "Regularizer",
    "The triples factory facilitates access to the dataset.",
    "This allows to store the optimized parameters",
    "Keep track of the hyper-parameters that are used across all",
    "subclasses of BaseModule",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "Sort final result",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "set model to evaluation mode",
    "Do not track gradients",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "Sort final result",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "Default for relation dimensionality",
    "-*- coding: utf-8 -*-",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Store initial input for error message",
    "All are None",
    "input_channels is None, and any of height or width is None -> set input_channels=1",
    "input channels is not None, and one of height or width is None",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "Finalize initialization",
    "embeddings",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Finalize initialization",
    "Initialise left relation embeddings to unit length",
    "Make sure to call super first",
    "Normalise embeddings of entities",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "Finalize initialization",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Finalize initialization",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "Finalize initialization",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Finalize initialization",
    "xavier uniform, cf.",
    "https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize embeddings of entities",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Finalize initialization",
    "Constraints are applied through post_parameter_update",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Finalize initialization",
    "TODO: Add initialization",
    "Make sure to call super first",
    "Normalise the normal vectors by their l2 norms",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "embeddings",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "TODO: Initialize from TransE",
    "Initialise relation embeddings to unit length",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Instantiate model",
    "Heuristic",
    "buffering of messages",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "Finalize initialization",
    "invalidate enriched embeddings",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "If batch is given, compute (num_layers)-hop neighbourhood",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "Only propagate messages on subset of edges",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Finalize initialization",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "Finalize initialization",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "phases randomly between 0 and 2 pi",
    "Make sure to call super first",
    "Normalize relation embeddings",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Finalize initialization",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Finalize initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "Literals",
    "End literals",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Embeddings",
    "Number of columns corresponds to number of literals",
    "apply dropout",
    "-, because lower score shall correspond to a more plausible triple.",
    "TODO check if this is the same as the BaseModule",
    "Choose y = -1 since a smaller score is better.",
    "In TransE for example, the scores represent distances",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Bind number of negatives to sample",
    "Equally corrupt head and tail",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Sample random entities as replacement",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Corrupt tails",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Replace tails",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random_non_negative_int(),",
    "),",
    "Add dataset to current_pipeline",
    "Training, test, and validation paths are provided",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indexes with the training triples",
    ": A factory wrapping the validation triples, that share indexes with the training triples",
    ": All data sets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    ": A mapping of HPO samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "TODO make more informative",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of HPO pruners' names to their implementations",
    "-*- coding: utf-8 -*-"
  ],
  "v1.0.4": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    ": The number of embeddings",
    ": The embedding dimension",
    "check shape",
    "check values",
    "check shape",
    "check values",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "-*- coding: utf-8 -*-",
    "equal value; larger is better",
    "equal value; smaller is better",
    "larger is better; improvement",
    "larger is better; improvement; but not significant",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The best results",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "check correct translation",
    "check column order",
    "apply restriction",
    "check that the triples factory is returned as is, if and only if no restriction is to apply",
    "check that inverse_triples is correctly carried over",
    "verify that the label-to-ID mapping has not been changed",
    "verify that triples have been filtered",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Check if multilabels are working correctly",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The dataset to test",
    "Not loaded",
    "Load",
    "Test caching",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "TODO: Catch HolE MKL error?",
    "set regularizer term",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "Distance-based model",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "self.assertAlmostEqual(second_score, -16, delta=0.01)",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "check shape",
    "check content",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The class",
    ": Constructor keyword arguments",
    ": The loss instance",
    ": The batch size",
    "test reduction",
    "Test backward",
    ": The number of entities.",
    ": The number of negative samples",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of optimizers' names to their implementations",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "To add *all* losses implemented in Torch, uncomment:",
    "_LOSSES.update({",
    "loss",
    "for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()",
    "if not loss.__name__.startswith('_')",
    "})",
    ": A mapping of losses' names to their implementations",
    ": HPO Defaults for losses",
    "Add empty dictionaries as defaults for all remaining losses",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.",
    "lower bound",
    "upper bound",
    "Allocate weight on device",
    "Initialize if initializer is provided",
    "Wrap embedding around it.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "expected value of |x|_1 = d*E[x_i] for x_i i.i.d.",
    "expected value of |x|_2 when x_i are normally distributed",
    "cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    ": A mapping of regularizers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "Start tracking",
    "evaluation restriction to a subset of entities/relations",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    ": A mapping of trackers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "FIXME is this ever possible, since this function is called in __init__?",
    "-*- coding: utf-8 -*-",
    "Create lists out of sets for proper numpy indexing when loading the labels",
    "TODO is there a need to have a canonical sort order here?",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    ": The mapping from entities' labels to their indexes",
    ": The mapping from relations' labels to their indexes",
    ": A three-column matrix where each row are the head label,",
    ": relation label, then tail label",
    ": A three-column matrix where each row are the head identifier,",
    ": relation identifier, then tail identifier",
    ": A dictionary mapping each relation to its inverse, if inverse triples were created",
    "TODO: Check if lazy evaluation would make sense",
    "Check if the triples are inverted already",
    "extend original triples with inverse ones",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "We can terminate the search after finding the first inverse occurrence",
    "Ensure 2d array in case only one triple was given",
    "FIXME this function is only ever used in tests",
    "Prepare shuffle index",
    "Prepare split index",
    "Take cumulative sum so the get separated properly",
    "Split triples",
    "Make sure that the first element has all the right stuff in it",
    "Make new triples factories for each group",
    "Input validation",
    "convert to numpy",
    "vectorized label lookup",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "Filter for entities",
    "Filter for relations",
    "No filtering happened",
    "manually copy the inverse relation mappings",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "Recalculate the testing triples without that index",
    "Recalculate the training entities, testing entities, to_move, and move_id_mask",
    "-*- coding: utf-8 -*-",
    ": A PyTorch tensor of triples",
    ": A mapping from relation labels to integer identifiers",
    ": A mapping from relation labels to integer identifiers",
    "Create dense target",
    "-*- coding: utf-8 -*-",
    "basically take all candidates",
    "Calculate which relations are the inverse ones",
    "FIXME doesn't carry flag of create_inverse_triples through",
    "A dictionary of all of the head/tail pairs for a given relation",
    "A dictionary for all of the tail/head pairs for a given relation",
    "Calculate the similarity between each relationship (entries in ``forward``)",
    "with all other candidate inverse relationships (entries in ``inverse``)",
    "Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the",
    "same as for ``(b, a)``",
    "A dictionary of all of the head/tail pairs for a given relation",
    "Filter out results between a given relationship and itself",
    "Filter out results below a minimum frequency",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Training Loop",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "reset the regularizer to free the computational graph",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The regularizer has to be reset to free the computational graph",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "now: smaller is better",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum relative improvement necessary to consider it an improved result",
    ": The best result so far",
    ": The epoch at which the best result occurred",
    ": The remaining patience",
    ": The metric results from all evaluations",
    ": Whether a larger value is better, or a smaller",
    ": The result tracker",
    ": Callbacks when after results are calculated",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Dummy result tracker",
    "Evaluate",
    "Only perform time consuming checks for the first call.",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Append to history",
    "check for improvement",
    "Stop if the result did not improve more than delta for patience evaluations",
    "-*- coding: utf-8 -*-",
    ": A mapping of stoppers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "verify that the triples have been filtered",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "Predict scores once",
    "Select scores of true",
    "Create positive filter for all corrupted",
    "Needs all positive triples",
    "Create filter",
    "Create a positive mask with the size of the scores from the positive filter",
    "Restrict to entities of interest",
    "Evaluate metrics on these *unfiltered* scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Restrict to entities of interest",
    "Evaluate metrics on these *filtered* scores",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Check if it a side or rank type",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    ": A dictionary of hyper-parameters to the models that use them",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Check loss functions that require 1 and -1 as label but only",
    "Regularizer",
    "The triples factory facilitates access to the dataset.",
    "This allows to store the optimized parameters",
    "Keep track of the hyper-parameters that are used across all",
    "subclasses of BaseModule",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "Sort final result",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "set model to evaluation mode",
    "Do not track gradients",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "Sort final result",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "Default for relation dimensionality",
    "-*- coding: utf-8 -*-",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Store initial input for error message",
    "All are None",
    "input_channels is None, and any of height or width is None -> set input_channels=1",
    "input channels is not None, and one of height or width is None",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "Finalize initialization",
    "embeddings",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Finalize initialization",
    "Initialise left relation embeddings to unit length",
    "Make sure to call super first",
    "Normalise embeddings of entities",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "Finalize initialization",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Finalize initialization",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "Finalize initialization",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Finalize initialization",
    "xavier uniform, cf.",
    "https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize embeddings of entities",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Finalize initialization",
    "Constraints are applied through post_parameter_update",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Finalize initialization",
    "TODO: Add initialization",
    "Make sure to call super first",
    "Normalise the normal vectors by their l2 norms",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "embeddings",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "TODO: Initialize from TransE",
    "Initialise relation embeddings to unit length",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Instantiate model",
    "Heuristic",
    "buffering of messages",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "Finalize initialization",
    "invalidate enriched embeddings",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "If batch is given, compute (num_layers)-hop neighbourhood",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "Only propagate messages on subset of edges",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Finalize initialization",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "Finalize initialization",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "phases randomly between 0 and 2 pi",
    "Make sure to call super first",
    "Normalize relation embeddings",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Finalize initialization",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Finalize initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "Literals",
    "End literals",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Embeddings",
    "Number of columns corresponds to number of literals",
    "apply dropout",
    "-, because lower score shall correspond to a more plausible triple.",
    "TODO check if this is the same as the BaseModule",
    "Choose y = -1 since a smaller score is better.",
    "In TransE for example, the scores represent distances",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Bind number of negatives to sample",
    "Equally corrupt head and tail",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Sample random entities as replacement",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Corrupt tails",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Replace tails",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random.randint(1, 2 ** 32 - 1),",
    "),",
    "Add dataset to current_pipeline",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indexes with the training triples",
    ": A factory wrapping the validation triples, that share indexes with the training triples",
    ": All data sets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    ": A mapping of HPO samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "FIXME difference between dataset class and string",
    "FIXME how to handle if dataset or factories were set? Should have been",
    "part of https://github.com/mali-git/POEM_develop/pull/483",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of HPO pruners' names to their implementations",
    "-*- coding: utf-8 -*-"
  ],
  "v1.0.3": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    ": The number of embeddings",
    ": The embedding dimension",
    "check shape",
    "check values",
    "check shape",
    "check values",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "-*- coding: utf-8 -*-",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "check correct translation",
    "check column order",
    "verify that all entities and relations are present in the training factory",
    "verify that no triple got lost",
    "verify that the label-to-id mappings match",
    "Check if multilabels are working correctly",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The dataset to test",
    "Not loaded",
    "Load",
    "Test caching",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "TODO: Catch HolE MKL error?",
    "set regularizer term",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "Distance-based model",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "check type",
    "check shape",
    "check ID ranges",
    "this is only done in one of the models",
    "this is only done in one of the models",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "self.assertAlmostEqual(second_score, -16, delta=0.01)",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "check shape",
    "check content",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The class",
    ": Constructor keyword arguments",
    ": The loss instance",
    ": The batch size",
    "test reduction",
    "Test backward",
    ": The number of entities.",
    ": The number of negative samples",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of optimizers' names to their implementations",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "To add *all* losses implemented in Torch, uncomment:",
    "_LOSSES.update({",
    "loss",
    "for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()",
    "if not loss.__name__.startswith('_')",
    "})",
    ": A mapping of losses' names to their implementations",
    ": HPO Defaults for losses",
    "Add empty dictionaries as defaults for all remaining losses",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.",
    "lower bound",
    "upper bound",
    "Allocate weight on device",
    "Initialize if initializer is provided",
    "Wrap embedding around it.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "expected value of |x|_1 = d*E[x_i] for x_i i.i.d.",
    "expected value of |x|_2 when x_i are normally distributed",
    "cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    ": A mapping of regularizers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. Tracking",
    "Misc",
    "Start tracking",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    ": A mapping of trackers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "FIXME is this ever possible, since this function is called in __init__?",
    "-*- coding: utf-8 -*-",
    "Create lists out of sets for proper numpy indexing when loading the labels",
    "TODO is there a need to have a canonical sort order here?",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    ": The mapping from entities' labels to their indexes",
    ": The mapping from relations' labels to their indexes",
    ": A three-column matrix where each row are the head label,",
    ": relation label, then tail label",
    ": A three-column matrix where each row are the head identifier,",
    ": relation identifier, then tail identifier",
    ": A dictionary mapping each relation to its inverse, if inverse triples were created",
    "TODO: Check if lazy evaluation would make sense",
    "Check if the triples are inverted already",
    "extend original triples with inverse ones",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "We can terminate the search after finding the first inverse occurrence",
    "Ensure 2d array in case only one triple was given",
    "FIXME this function is only ever used in tests",
    "Prepare shuffle index",
    "Prepare split index",
    "Take cumulative sum so the get separated properly",
    "Split triples",
    "Make sure that the first element has all the right stuff in it",
    "Make new triples factories for each group",
    "Input validation",
    "convert to numpy",
    "vectorized label lookup",
    "Additional columns",
    "convert PyTorch tensors to numpy",
    "convert to dataframe",
    "Re-order columns",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "Recalculate the testing triples without that index",
    "Recalculate the training entities, testing entities, to_move, and move_id_mask",
    "-*- coding: utf-8 -*-",
    ": A PyTorch tensor of triples",
    ": A mapping from relation labels to integer identifiers",
    ": A mapping from relation labels to integer identifiers",
    "Create dense target",
    "-*- coding: utf-8 -*-",
    "basically take all candidates",
    "Calculate which relations are the inverse ones",
    "FIXME doesn't carry flag of create_inverse_triples through",
    "A dictionary of all of the head/tail pairs for a given relation",
    "A dictionary for all of the tail/head pairs for a given relation",
    "Calculate the similarity between each relationship (entries in ``forward``)",
    "with all other candidate inverse relationships (entries in ``inverse``)",
    "Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the",
    "same as for ``(b, a)``",
    "A dictionary of all of the head/tail pairs for a given relation",
    "Filter out results between a given relationship and itself",
    "Filter out results below a minimum frequency",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Training Loop",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "reset the regularizer to free the computational graph",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The regularizer has to be reset to free the computational graph",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum improvement between two iterations",
    ": The metric results from all evaluations",
    ": A ring buffer to store the recent results",
    ": A counter for the ring buffer",
    ": Whether a larger value is better, or a smaller",
    ": The criterion. Set in the constructor based on larger_is_better",
    ": The result tracker",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Dummy result tracker",
    "Evaluate",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Only check if enough values are already collected",
    "Stop if the result did not improve more than delta for patience epochs.",
    "Update ring buffer",
    "Append to history",
    "-*- coding: utf-8 -*-",
    ": A mapping of stoppers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "Predict tail scores once",
    "Create positive filter for all corrupted tails",
    "Create a positive mask with the size of the scores from the positive tails filter",
    "Evaluate metrics on these *unfiltered* tail scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "Predict head scores once",
    "Create positive filter for all corrupted heads",
    "Create a positive mask with the size of the scores from the positive heads filter",
    "Evaluate metrics on these head scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Check if it a side or rank type",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    ": A dictionary of hyper-parameters to the models that use them",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Check loss functions that require 1 and -1 as label but only",
    "Regularizer",
    "The triples factory facilitates access to the dataset.",
    "This allows to store the optimized parameters",
    "Keep track of the hyper-parameters that are used across all",
    "subclasses of BaseModule",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs",
    "initialize buffer on cpu",
    "calculate batch scores",
    "Explicitly create triples",
    "Sort final result",
    "Train a model (quickly)",
    "Get scores for *all* triples",
    "Get scores for top 15 triples",
    "set model to evaluation mode",
    "Do not track gradients",
    "initialize buffer on device",
    "calculate batch scores",
    "get top scores within batch",
    "append to global top scores",
    "reduce size if necessary",
    "Sort final result",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "Default for relation dimensionality",
    "-*- coding: utf-8 -*-",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Store initial input for error message",
    "All are None",
    "input_channels is None, and any of height or width is None -> set input_channels=1",
    "input channels is not None, and one of height or width is None",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "Finalize initialization",
    "embeddings",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Finalize initialization",
    "Initialise left relation embeddings to unit length",
    "Make sure to call super first",
    "Normalise embeddings of entities",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "Finalize initialization",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Finalize initialization",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "Finalize initialization",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Finalize initialization",
    "xavier uniform, cf.",
    "https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize embeddings of entities",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Finalize initialization",
    "Constraints are applied through post_parameter_update",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Finalize initialization",
    "TODO: Add initialization",
    "Make sure to call super first",
    "Normalise the normal vectors by their l2 norms",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "embeddings",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "TODO: Initialize from TransE",
    "Initialise relation embeddings to unit length",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Instantiate model",
    "Heuristic",
    "buffering of messages",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "Finalize initialization",
    "invalidate enriched embeddings",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "If batch is given, compute (num_layers)-hop neighbourhood",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "Only propagate messages on subset of edges",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Finalize initialization",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "Finalize initialization",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "phases randomly between 0 and 2 pi",
    "Make sure to call super first",
    "Normalize relation embeddings",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Finalize initialization",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Finalize initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "Literals",
    "End literals",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Embeddings",
    "Number of columns corresponds to number of literals",
    "apply dropout",
    "-, because lower score shall correspond to a more plausible triple.",
    "TODO check if this is the same as the BaseModule",
    "Choose y = -1 since a smaller score is better.",
    "In TransE for example, the scores represent distances",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Bind number of negatives to sample",
    "Equally corrupt head and tail",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Sample random entities as replacement",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Corrupt tails",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Replace tails",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random.randint(1, 2 ** 32 - 1),",
    "),",
    "Add dataset to current_pipeline",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indexes with the training triples",
    ": A factory wrapping the validation triples, that share indexes with the training triples",
    ": All data sets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    ": A mapping of HPO samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Trackers",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "FIXME difference between dataset class and string",
    "FIXME how to handle if dataset or factories were set? Should have been",
    "part of https://github.com/mali-git/POEM_develop/pull/483",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracking",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. Tracker",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of HPO pruners' names to their implementations",
    "-*- coding: utf-8 -*-"
  ],
  "v1.0.2": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    ": The number of embeddings",
    ": The embedding dimension",
    "check shape",
    "check values",
    "check shape",
    "check values",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "-*- coding: utf-8 -*-",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "Check if multilabels are working correctly",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The dataset to test",
    "Not loaded",
    "Load",
    "Test caching",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "TODO: Catch HolE MKL error?",
    "set regularizer term",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "Distance-based model",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "self.assertAlmostEqual(second_score, -16, delta=0.01)",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "check shape",
    "check content",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The class",
    ": Constructor keyword arguments",
    ": The loss instance",
    ": The batch size",
    "test reduction",
    "Test backward",
    ": The number of entities.",
    ": The number of negative samples",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of optimizers' names to their implementations",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "To add *all* losses implemented in Torch, uncomment:",
    "_LOSSES.update({",
    "loss",
    "for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()",
    "if not loss.__name__.startswith('_')",
    "})",
    ": A mapping of losses' names to their implementations",
    ": HPO Defaults for losses",
    "Add empty dictionaries as defaults for all remaining losses",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.",
    "lower bound",
    "upper bound",
    "Allocate weight on device",
    "Initialize if initializer is provided",
    "Wrap embedding around it.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "expected value of |x|_1 = d*E[x_i] for x_i i.i.d.",
    "expected value of |x|_2 when x_i are normally distributed",
    "cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    ": A mapping of regularizers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "9. MLFlow",
    "Misc",
    "Create result store",
    "Start tracking",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "FIXME is this ever possible, since this function is called in __init__?",
    "-*- coding: utf-8 -*-",
    "Create lists out of sets for proper numpy indexing when loading the labels",
    "TODO is there a need to have a canonical sort order here?",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    ": The mapping from entities' labels to their indexes",
    ": The mapping from relations' labels to their indexes",
    ": A three-column matrix where each row are the head label,",
    ": relation label, then tail label",
    ": A three-column matrix where each row are the head identifier,",
    ": relation identifier, then tail identifier",
    ": A dictionary mapping each relation to its inverse, if inverse triples were created",
    "TODO: Check if lazy evaluation would make sense",
    "Check if the triples are inverted already",
    "extend original triples with inverse ones",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "We can terminate the search after finding the first inverse occurrence",
    "Ensure 2d array in case only one triple was given",
    "FIXME this function is only ever used in tests",
    "Prepare shuffle index",
    "Prepare split index",
    "Take cumulative sum so the get separated properly",
    "Split triples",
    "Make sure that the first element has all the right stuff in it",
    "Make new triples factories for each group",
    "While there are still triples that should be moved to the training set",
    "Pick a random triple to move over to the training triples",
    "Recalculate the testing triples without that index",
    "Recalculate the training entities, testing entities, to_move, and move_id_mask",
    "-*- coding: utf-8 -*-",
    ": A PyTorch tensor of triples",
    ": A mapping from relation labels to integer identifiers",
    ": A mapping from relation labels to integer identifiers",
    "Create dense target",
    "-*- coding: utf-8 -*-",
    "basically take all candidates",
    "Calculate which relations are the inverse ones",
    "FIXME doesn't carry flag of create_inverse_triples through",
    "A dictionary of all of the head/tail pairs for a given relation",
    "A dictionary for all of the tail/head pairs for a given relation",
    "Calculate the similarity between each relationship (entries in ``forward``)",
    "with all other candidate inverse relationships (entries in ``inverse``)",
    "Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the",
    "same as for ``(b, a)``",
    "A dictionary of all of the head/tail pairs for a given relation",
    "Filter out results between a given relationship and itself",
    "Filter out results below a minimum frequency",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Training Loop",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "reset the regularizer to free the computational graph",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The regularizer has to be reset to free the computational graph",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum improvement between two iterations",
    ": The metric results from all evaluations",
    ": A ring buffer to store the recent results",
    ": A counter for the ring buffer",
    ": Whether a larger value is better, or a smaller",
    ": The criterion. Set in the constructor based on larger_is_better",
    ": The result tracker",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Dummy result tracker",
    "Evaluate",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Only check if enough values are already collected",
    "Stop if the result did not improve more than delta for patience epochs.",
    "Update ring buffer",
    "Append to history",
    "-*- coding: utf-8 -*-",
    ": A mapping of stoppers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "Predict tail scores once",
    "Create positive filter for all corrupted tails",
    "Create a positive mask with the size of the scores from the positive tails filter",
    "Evaluate metrics on these *unfiltered* tail scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "Predict head scores once",
    "Create positive filter for all corrupted heads",
    "Create a positive mask with the size of the scores from the positive heads filter",
    "Evaluate metrics on these head scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    ": A dictionary of hyper-parameters to the models that use them",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Check loss functions that require 1 and -1 as label but only",
    "Regularizer",
    "The triples factory facilitates access to the dataset.",
    "This allows to store the optimized parameters",
    "Keep track of the hyper-parameters that are used across all",
    "subclasses of BaseModule",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "Default for relation dimensionality",
    "-*- coding: utf-8 -*-",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Store initial input for error message",
    "All are None",
    "input_channels is None, and any of height or width is None -> set input_channels=1",
    "input channels is not None, and one of height or width is None",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "Finalize initialization",
    "embeddings",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Finalize initialization",
    "Initialise left relation embeddings to unit length",
    "Make sure to call super first",
    "Normalise embeddings of entities",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "Finalize initialization",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Finalize initialization",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "Finalize initialization",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Finalize initialization",
    "xavier uniform, cf.",
    "https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize embeddings of entities",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Finalize initialization",
    "Constraints are applied through post_parameter_update",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Finalize initialization",
    "TODO: Add initialization",
    "Make sure to call super first",
    "Normalise the normal vectors by their l2 norms",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "embeddings",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "TODO: Initialize from TransE",
    "Initialise relation embeddings to unit length",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Instantiate model",
    "Heuristic",
    "buffering of messages",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "Finalize initialization",
    "invalidate enriched embeddings",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "If batch is given, compute (num_layers)-hop neighbourhood",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "Only propagate messages on subset of edges",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Finalize initialization",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "Finalize initialization",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "phases randomly between 0 and 2 pi",
    "Make sure to call super first",
    "Normalize relation embeddings",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Finalize initialization",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Finalize initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "Literals",
    "End literals",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Embeddings",
    "Number of columns corresponds to number of literals",
    "apply dropout",
    "-, because lower score shall correspond to a more plausible triple.",
    "TODO check if this is the same as the BaseModule",
    "Choose y = -1 since a smaller score is better.",
    "In TransE for example, the scores represent distances",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Bind number of negatives to sample",
    "Equally corrupt head and tail",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Sample random entities as replacement",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Corrupt tails",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Replace tails",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random.randint(1, 2 ** 32 - 1),",
    "),",
    "Add dataset to current_pipeline",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indexes with the training triples",
    ": A factory wrapping the validation triples, that share indexes with the training triples",
    ": All data sets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    ": A mapping of HPO samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. MLFlow",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. MLFlow",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "MLFlow",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "FIXME difference between dataset class and string",
    "FIXME how to handle if dataset or factories were set? Should have been",
    "part of https://github.com/mali-git/POEM_develop/pull/483",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "9. MLFlow",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of HPO pruners' names to their implementations",
    "-*- coding: utf-8 -*-"
  ],
  "v1.0.1": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "If true, the current module name will be prepended to all description",
    "unit titles (such as .. function::).",
    "A list of prefixes that are ignored when creating the module index. (new in Sphinx 0.6)",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "generate autosummary pages",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    ": The number of embeddings",
    ": The embedding dimension",
    "check shape",
    "check values",
    "check shape",
    "check values",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "-*- coding: utf-8 -*-",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "Check if multilabels are working correctly",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The dataset to test",
    "Not loaded",
    "Load",
    "Test caching",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "TODO: Catch HolE MKL error?",
    "set regularizer term",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "Distance-based model",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "self.assertAlmostEqual(second_score, -16, delta=0.01)",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "check shape",
    "check content",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The class",
    ": Constructor keyword arguments",
    ": The loss instance",
    ": The batch size",
    "test reduction",
    "Test backward",
    ": The number of entities.",
    ": The number of negative samples",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of optimizers' names to their implementations",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "To add *all* losses implemented in Torch, uncomment:",
    "_LOSSES.update({",
    "loss",
    "for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()",
    "if not loss.__name__.startswith('_')",
    "})",
    ": A mapping of losses' names to their implementations",
    ": HPO Defaults for losses",
    "Add empty dictionaries as defaults for all remaining losses",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.",
    "lower bound",
    "upper bound",
    "Allocate weight on device",
    "Initialize if initializer is provided",
    "Wrap embedding around it.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "expected value of |x|_1 = d*E[x_i] for x_i i.i.d.",
    "expected value of |x|_2 when x_i are normally distributed",
    "cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    ": A mapping of regularizers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "Misc",
    "Create result store",
    "Start tracking",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "FIXME is this ever possible, since this function is called in __init__?",
    "-*- coding: utf-8 -*-",
    "Create lists out of sets for proper numpy indexing when loading the labels",
    "TODO is there a need to have a canonical sort order here?",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    ": The mapping from entities' labels to their indexes",
    ": The mapping from relations' labels to their indexes",
    ": A three-column matrix where each row are the head label,",
    ": relation label, then tail label",
    ": A three-column matrix where each row are the head identifier,",
    ": relation identifier, then tail identifier",
    ": A dictionary mapping each relation to its inverse, if inverse triples were created",
    "TODO: Check if lazy evaluation would make sense",
    "Check if the triples are inverted already",
    "extend original triples with inverse ones",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "We can terminate the search after finding the first inverse occurrence",
    "Ensure 2d array in case only one triple was given",
    "FIXME this function is only ever used in tests",
    "Prepare shuffle index",
    "Prepare split index",
    "Take cumulative sum so the get separated properly",
    "Split triples",
    "Make new triples factories for each group",
    "-*- coding: utf-8 -*-",
    ": A PyTorch tensor of triples",
    ": A mapping from relation labels to integer identifiers",
    ": A mapping from relation labels to integer identifiers",
    "Create dense target",
    "-*- coding: utf-8 -*-",
    "basically take all candidates",
    "Calculate which relations are the inverse ones",
    "FIXME doesn't carry flag of create_inverse_triples through",
    "A dictionary of all of the head/tail pairs for a given relation",
    "A dictionary for all of the tail/head pairs for a given relation",
    "Calculate the similarity between each relationship (entries in ``forward``)",
    "with all other candidate inverse relationships (entries in ``inverse``)",
    "Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the",
    "same as for ``(b, a)``",
    "A dictionary of all of the head/tail pairs for a given relation",
    "Filter out results between a given relationship and itself",
    "Filter out results below a minimum frequency",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Training Loop",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "reset the regularizer to free the computational graph",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The regularizer has to be reset to free the computational graph",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum improvement between two iterations",
    ": The metric results from all evaluations",
    ": A ring buffer to store the recent results",
    ": A counter for the ring buffer",
    ": Whether a larger value is better, or a smaller",
    ": The criterion. Set in the constructor based on larger_is_better",
    ": The result tracker",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Dummy result tracker",
    "Evaluate",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Only check if enough values are already collected",
    "Stop if the result did not improve more than delta for patience epochs.",
    "Update ring buffer",
    "Append to history",
    "-*- coding: utf-8 -*-",
    ": A mapping of stoppers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "Predict tail scores once",
    "Create positive filter for all corrupted tails",
    "Create a positive mask with the size of the scores from the positive tails filter",
    "Evaluate metrics on these *unfiltered* tail scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "Predict head scores once",
    "Create positive filter for all corrupted heads",
    "Create a positive mask with the size of the scores from the positive heads filter",
    "Evaluate metrics on these head scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    ": A dictionary of hyper-parameters to the models that use them",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Check loss functions that require 1 and -1 as label but only",
    "Regularizer",
    "The triples factory facilitates access to the dataset.",
    "This allows to store the optimized parameters",
    "Keep track of the hyper-parameters that are used across all",
    "subclasses of BaseModule",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "Default for relation dimensionality",
    "-*- coding: utf-8 -*-",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Store initial input for error message",
    "All are None",
    "input_channels is None, and any of height or width is None -> set input_channels=1",
    "input channels is not None, and one of height or width is None",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "Finalize initialization",
    "embeddings",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Finalize initialization",
    "Initialise left relation embeddings to unit length",
    "Make sure to call super first",
    "Normalise embeddings of entities",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "Finalize initialization",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Finalize initialization",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "Finalize initialization",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Finalize initialization",
    "xavier uniform, cf.",
    "https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize embeddings of entities",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Finalize initialization",
    "Constraints are applied through post_parameter_update",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Get embeddings",
    "Compute entity distribution",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Finalize initialization",
    "TODO: Add initialization",
    "Make sure to call super first",
    "Normalise the normal vectors by their l2 norms",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "embeddings",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "TODO: Initialize from TransE",
    "Initialise relation embeddings to unit length",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Instantiate model",
    "Heuristic",
    "buffering of messages",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "Finalize initialization",
    "invalidate enriched embeddings",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "If batch is given, compute (num_layers)-hop neighbourhood",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "Only propagate messages on subset of edges",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Finalize initialization",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "Finalize initialization",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "phases randomly between 0 and 2 pi",
    "Make sure to call super first",
    "Normalize relation embeddings",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Finalize initialization",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Finalize initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "Literals",
    "End literals",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Embeddings",
    "Number of columns corresponds to number of literals",
    "apply dropout",
    "-, because lower score shall correspond to a more plausible triple.",
    "TODO check if this is the same as the BaseModule",
    "Choose y = -1 since a smaller score is better.",
    "In TransE for example, the scores represent distances",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Bind number of negatives to sample",
    "Equally corrupt head and tail",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Sample random entities as replacement",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Corrupt tails",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Replace tails",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random.randint(1, 2 ** 32 - 1),",
    "),",
    "Add dataset to current_pipeline",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indexes with the training triples",
    ": A factory wrapping the validation triples, that share indexes with the training triples",
    ": All data sets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    ": A mapping of datasets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    ": A mapping of HPO samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "FIXME difference between dataset class and string",
    "FIXME how to handle if dataset or factories were set? Should have been",
    "part of https://github.com/mali-git/POEM_develop/pull/483",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of HPO pruners' names to their implementations",
    "-*- coding: utf-8 -*-"
  ],
  "v1.0.0": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "autodoc_mock_imports = ['torch', 'torchvision']",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "The name of an image file (relative to this directory) to place at the top",
    "of the sidebar.",
    "",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "Check a model param is optimized",
    "Check a loss param is optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is optimized",
    "Check a model param is optimized",
    "Check a loss param is NOT optimized",
    "Check a model param is NOT optimized",
    "Check a loss param is NOT optimized",
    "-*- coding: utf-8 -*-",
    "check for empty batches",
    "-*- coding: utf-8 -*-",
    "The triples factory and model",
    ": The evaluator to be tested",
    "Settings",
    ": The evaluator instantiation",
    "Settings",
    "Initialize evaluator",
    "Use small test dataset",
    "Use small model (untrained)",
    "Get batch",
    "Compute scores",
    "Compute mask only if required",
    "TODO: Re-use filtering code",
    "shape: (batch_size, num_triples)",
    "shape: (batch_size, num_entities)",
    "Process one batch",
    "Check for correct class",
    "Check value ranges",
    "TODO: Validate with data?",
    "Check for correct class",
    "check value",
    "filtering",
    "true_score: (2, 3, 3)",
    "head based filter",
    "preprocessing for faster lookup",
    "check that all found positives are positive",
    "check in-place",
    "Test head scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "Test tail scores",
    "Assert in-place modification",
    "Assert correct filtering",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The triples factory",
    ": Class of regularizer to test",
    ": The constructor parameters to pass to the regularizer",
    ": The regularizer instance, initialized in setUp",
    ": A positive batch",
    ": The device",
    "Use RESCAL as it regularizes multiple tensors of different shape.",
    "Check if regularizer is stored correctly.",
    "Forward pass (should update regularizer)",
    "Call post_parameter_update (should reset regularizer)",
    "Check if regularization term is reset",
    "Call method",
    "Generate random tensors",
    "Call update",
    "check shape",
    "compute expected term",
    "Generate random tensor",
    "calculate penalty",
    "check shape",
    "check value",
    "Tests that exception will be thrown when more than or less than three tensors are passed",
    "Test that regularization term is computed correctly",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "After first update, should change the term",
    "After second update, no change should happen",
    "-*- coding: utf-8 -*-",
    ": The number of embeddings",
    ": The embedding dimension",
    "check shape",
    "check values",
    "check shape",
    "check values",
    "check correct value range",
    "check maximum norm constraint",
    "unchanged values for small norms",
    "-*- coding: utf-8 -*-",
    ": The window size used by the early stopper",
    ": The mock losses the mock evaluator will return",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    "Set automatic_memory_optimization to false for tests",
    "Step early stopper",
    "check storing of results",
    "check ring buffer",
    ": The window size used by the early stopper",
    ": The (zeroed) index  - 1 at which stopping will occur",
    ": The minimum improvement",
    ": The random seed to use for reproducibility",
    ": The maximum number of epochs to train. Should be large enough to allow for early stopping.",
    ": The epoch at which the stop should happen. Depends on the choice of random seed.",
    ": The batch size to use.",
    "Fix seed for reproducibility",
    "Set automatic_memory_optimization to false during testing",
    "-*- coding: utf-8 -*-",
    "Check if multilabels are working correctly",
    "-*- coding: utf-8 -*-",
    ": The batch size",
    ": The random seed",
    ": The triples factory",
    ": The sLCWA instances",
    ": Class of negative sampling to test",
    ": The negative sampler instance, initialized in setUp",
    ": A positive batch",
    "Generate negative sample",
    "check shape",
    "check bounds: heads",
    "check bounds: relations",
    "check bounds: tails",
    "Check that all elements got corrupted",
    "Generate scaled negative sample",
    "Generate negative samples",
    "test that the relations were not changed",
    "Test that half of the subjects and half of the objects are corrupted",
    "Generate negative sample for additional tests",
    "test that the relations were not changed",
    "sample a batch",
    "check shape",
    "get triples",
    "check connected components",
    "super inefficient",
    "join",
    "already joined",
    "check that there is only a single component",
    "check content of comp_adj_lists",
    "check edge ids",
    "-*- coding: utf-8 -*-",
    ": The expected number of entities",
    ": The expected number of relations",
    ": The dataset to test",
    "Not loaded",
    "Load",
    "Test caching",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The class of the model to test",
    ": Additional arguments passed to the model's constructor method",
    ": The triples factory instance",
    ": The model instance",
    ": The batch size for use for forward_* tests",
    ": The embedding dimensionality",
    ": Whether to create inverse triples (needed e.g. by ConvE)",
    ": The sampler to use for sLCWA (different e.g. for R-GCN)",
    ": The batch size for use when testing training procedures",
    ": The number of epochs to train the model",
    ": A random number generator from torch",
    ": The number of parameters which receive a constant (i.e. non-randomized)",
    "initialization",
    "assert there is at least one trainable parameter",
    "Check that all the parameters actually require a gradient",
    "Try to initialize an optimizer",
    "get model parameters",
    "re-initialize",
    "check that the operation works in-place",
    "check that the parameters where modified",
    "check for finite values by default",
    "check whether a gradient can be back-propgated",
    "assert batch comprises (head, relation) pairs",
    "assert batch comprises (relation, tail) pairs",
    "TODO: Catch HolE MKL error?",
    "set regularizer term",
    "call post_parameter_update",
    "assert that the regularization term has been reset",
    "do one optimization step",
    "call post_parameter_update",
    "check model constraints",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "assert batch comprises (relation, tail) pairs",
    "Distance-based model",
    "3x batch norm: bias + scale --> 6",
    "entity specific bias        --> 1",
    "==================================",
    "7",
    "two bias terms, one conv-filter",
    "Two linear layer biases",
    "Two BN layers, bias & scale",
    ": one bias per layer",
    ": (scale & bias for BN) * layers",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "self.assertAlmostEqual(second_score, -16, delta=0.01)",
    "Use different dimension for relation embedding: relation_dim > entity_dim",
    "relation embeddings",
    "Compute Scores",
    "Use different dimension for relation embedding: relation_dim < entity_dim",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "random entity embeddings & projections",
    "random relation embeddings & projections",
    "project",
    "check shape:",
    "check normalization",
    "entity embeddings",
    "relation embeddings",
    "Compute Scores",
    "second_score = scores[1].item()",
    ": 2xBN (bias & scale)",
    "check shape",
    "check content",
    ": The number of entities",
    ": The number of triples",
    "check shape",
    "check dtype",
    "check finite values (e.g. due to division by zero)",
    "check non-negativity",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "check for finite values by default",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Set into training mode to check if it is correctly set to evaluation mode.",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The class",
    ": Constructor keyword arguments",
    ": The loss instance",
    ": The batch size",
    "test reduction",
    "Test backward",
    ": The number of entities.",
    ": The number of negative samples",
    "\u2248 result of softmax",
    "neg_distances - margin = [-1., -1., 0., 0.]",
    "sigmoids \u2248 [0.27, 0.27, 0.5, 0.5]",
    "pos_distances = [0., 0., 0.5, 0.5]",
    "margin - pos_distances = [1. 1., 0.5, 0.5]",
    "\u2248 result of sigmoid",
    "sigmoids \u2248 [0.73, 0.73, 0.62, 0.62]",
    "expected_loss \u2248 0.34",
    "-*- coding: utf-8 -*-",
    "Create dummy dense labels",
    "Check if labels form a probability distribution",
    "Apply label smoothing",
    "Check if smooth labels form probability distribution",
    "Create dummy sLCWA labels",
    "Apply label smoothing",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the optimizers' hyper-parameters (yo dawg)",
    "-*- coding: utf-8 -*-",
    "scale labels from [0, 1] to [-1, 1]",
    "cross entropy expects a proper probability distribution -> normalize labels",
    "Use numerically stable variant to compute log(softmax)",
    "compute cross entropy: ce(b) = sum_i p_true(b, i) * log p_pred(b, i)",
    "To add *all* losses implemented in Torch, uncomment:",
    "_LOSSES.update({",
    "loss",
    "for loss in Loss.__subclasses__() + WeightedLoss.__subclasses__()",
    "if not loss.__name__.startswith('_')",
    "})",
    "Add empty dictionaries as defaults for all remaining losses",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": An error that occurs because the input in CUDA is too big. See ConvE for an example.",
    "Normalize by the number of elements in the tensors for dimensionality-independent weight tuning.",
    "lower bound",
    "upper bound",
    "Allocate weight on device",
    "Initialize if initializer is provided",
    "Wrap embedding around it.",
    "-*- coding: utf-8 -*-",
    ": The overall regularization weight",
    ": The current regularization term (a scalar)",
    ": Should the regularization only be applied once? This was used for ConvKB and defaults to False.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "no need to compute anything",
    "always return zero",
    ": The dimension along which to compute the vector-based regularization terms.",
    ": Whether to normalize the regularization term by the dimension of the vectors.",
    ": This allows dimensionality-independent weight tuning.",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "expected value of |x|_1 = d*E[x_i] for x_i i.i.d.",
    "expected value of |x|_2 when x_i are normally distributed",
    "cf. https://arxiv.org/pdf/1012.0621.pdf chapter 3.1",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    ": The default strategy for optimizing the regularizer's hyper-parameters",
    "The regularization in TransH enforces the defined soft constraints that should computed only for every batch.",
    "Therefore, apply_only_once is always set to True.",
    "Entity soft constraint",
    "Orthogonality soft constraint",
    "The normalization factor to balance individual regularizers' contribution.",
    "-*- coding: utf-8 -*-",
    "Add HPO command",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The random seed used at the beginning of the pipeline",
    ": The model trained by the pipeline",
    ": The training loop used by the pipeline",
    ": The losses during training",
    ": The results evaluated by the pipeline",
    ": How long in seconds did training take?",
    ": How long in seconds did evaluation take?",
    ": An early stopper",
    ": Any additional metadata as a dictionary",
    ": The version of PyKEEN used to create these results",
    ": The git hash of PyKEEN used to create these results",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training (ronaldo style)",
    "8. Evaluation",
    "Misc",
    "Create result store",
    "Start tracking",
    "FIXME this should never happen.",
    "Log model parameters",
    "Log optimizer parameters",
    "Stopping",
    "Load the evaluation batch size for the stopper, if it has been set",
    "By default there's a stopper that does nothing interesting",
    "Add logging for debugging",
    "Train like Cristiano Ronaldo",
    "Evaluate",
    "Reuse optimal evaluation parameters from training if available",
    "Add logging about evaluator for debugging",
    "-*- coding: utf-8 -*-",
    "This will set the global logging level to info to ensure that info messages are shown in all parts of the software.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Create directory in which all experimental artifacts are saved",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    ": Functions for specifying exotic resources based on their file extension",
    "-*- coding: utf-8 -*-",
    "Prepare literal matrix, set every literal to zero, and afterwards fill in the corresponding value if available",
    "TODO vectorize code",
    "row define entity, and column the literal. Set the corresponding literal for the entity",
    "FIXME is this ever possible, since this function is called in __init__?",
    "-*- coding: utf-8 -*-",
    "Create lists out of sets for proper numpy indexing when loading the labels",
    "TODO is there a need to have a canonical sort order here?",
    "Split triples",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "Sorting ensures consistent results when the triples are permuted",
    "Create mapping",
    "When triples that don't exist are trying to be mapped, they get the id \"-1\"",
    "Filter all non-existent triples",
    "Note: Unique changes the order of the triples",
    "Note: Using unique means implicit balancing of training samples",
    ": The mapping from entities' labels to their indexes",
    ": The mapping from relations' labels to their indexes",
    ": A three-column matrix where each row are the head label,",
    ": relation label, then tail label",
    ": A three-column matrix where each row are the head identifier,",
    ": relation identifier, then tail identifier",
    ": A dictionary mapping each relation to its inverse, if inverse triples were created",
    "TODO: Check if lazy evaluation would make sense",
    "Check if the triples are inverted already",
    "extend original triples with inverse ones",
    "Generate entity mapping if necessary",
    "Generate relation mapping if necessary",
    "Map triples of labels to triples of IDs.",
    "We can terminate the search after finding the first inverse occurrence",
    "Ensure 2d array in case only one triple was given",
    "FIXME this function is only ever used in tests",
    "Prepare shuffle index",
    "Prepare split index",
    "Take cumulative sum so the get separated properly",
    "Split triples",
    "Make new triples factories for each group",
    "-*- coding: utf-8 -*-",
    "Create dense target",
    "-*- coding: utf-8 -*-",
    "basically take all candidates",
    "Calculate which relations are the inverse ones",
    "FIXME doesn't carry flag of create_inverse_triples through",
    "A dictionary of all of the head/tail pairs for a given relation",
    "A dictionary for all of the tail/head pairs for a given relation",
    "Calculate the similarity between each relationship (entries in ``forward``)",
    "with all other candidate inverse relationships (entries in ``inverse``)",
    "Note: uses an asymmetric metric, so results for ``(a, b)`` is not necessarily the",
    "same as for ``(b, a)``",
    "A dictionary of all of the head/tail pairs for a given relation",
    "Filter out results between a given relationship and itself",
    "Filter out results below a minimum frequency",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "preprocessing",
    "initialize",
    "sample iteratively",
    "determine weights",
    "only happens at first iteration",
    "normalize to probabilities",
    "sample a start node",
    "get list of neighbors",
    "sample an outgoing edge at random which has not been chosen yet using rejection sampling",
    "visit target node",
    "decrease sample counts",
    "return chosen edges",
    "-*- coding: utf-8 -*-",
    "Create training instances",
    "During size probing the training instances should not show the tqdm progress bar",
    "In some cases, e.g. using Optuna for HPO, the cuda cache from a previous run is not cleared",
    "Ensure the release of memory",
    "Clear optimizer",
    "Take the biggest possible training batch_size, if batch_size not set",
    "This will find necessary parameters to optimize the use of the hardware at hand",
    "return the relevant parameters slice_size and batch_size",
    "Create dummy result tracker",
    "Sanity check",
    "Force weight initialization if training continuation is not explicitly requested.",
    "Reset the weights",
    "Create new optimizer",
    "Ensure the model is on the correct device",
    "Create Sampler",
    "Bind",
    "When size probing, we don't want progress bars",
    "Create progress bar",
    "Training Loop",
    "Enforce training mode",
    "Accumulate loss over epoch",
    "Batching",
    "Only create a progress bar when not in size probing mode",
    "Flag to check when to quit the size probing",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "Get batch size of current batch (last batch may be incomplete)",
    "accumulate gradients for whole batch",
    "forward pass call",
    "when called by batch_size_search(), the parameter update should not be applied.",
    "update parameters according to optimizer",
    "After changing applying the gradients to the embeddings, the model is notified that the forward",
    "constraints are no longer applied",
    "For testing purposes we're only interested in processing one batch",
    "When size probing we don't need the losses",
    "Track epoch loss",
    "Print loss information to console",
    "forward pass",
    "raise error when non-finite loss occurs (NaN, +/-inf)",
    "correction for loss reduction",
    "backward pass",
    "reset the regularizer to free the computational graph",
    "Set upper bound",
    "If the sub_batch_size did not finish search with a possibility that fits the hardware, we have to try slicing",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The regularizer has to be reset to free the computational graph",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "-*- coding: utf-8 -*-",
    "Shuffle each epoch",
    "Lazy-splitting into batches",
    "-*- coding: utf-8 -*-",
    "Slicing is not possible in sLCWA training loops",
    "Send positive batch to device",
    "Create negative samples",
    "Ensure they reside on the device (should hold already for most simple negative samplers, e.g.",
    "BasicNegativeSampler, BernoulliNegativeSampler",
    "Make it negative batch broadcastable (required for num_negs_per_pos > 1).",
    "Compute negative and positive scores",
    "Repeat positives scores (necessary for more than one negative per positive)",
    "Stack predictions",
    "Create target",
    "Normalize the loss to have the average loss per positive triple",
    "This allows comparability of sLCWA and LCWA losses",
    "Slicing is not possible for sLCWA",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "Split batch components",
    "Send batch to device",
    "Apply label smoothing",
    "This shows how often one row has to be repeated",
    "Create boolean indices for negative labels in the repeated rows",
    "Repeat the predictions and filter for negative labels",
    "This tells us how often each true label should be repeated",
    "First filter the predictions for true labels and then repeat them based on the repeat vector",
    "Split positive and negative scores",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The model",
    ": The evaluator",
    ": The triples to use for evaluation",
    ": Size of the evaluation batches",
    ": Slice size of the evaluation batches",
    ": The number of epochs after which the model is evaluated on validation set",
    ": The number of iterations (one iteration can correspond to various epochs)",
    ": with no improvement after which training will be stopped.",
    ": The name of the metric to use",
    ": The minimum improvement between two iterations",
    ": The metric results from all evaluations",
    ": A ring buffer to store the recent results",
    ": A counter for the ring buffer",
    ": Whether a larger value is better, or a smaller",
    ": The criterion. Set in the constructor based on larger_is_better",
    ": The result tracker",
    ": Callbacks when training gets continued",
    ": Callbacks when training is stopped early",
    ": Did the stopper ever decide to stop?",
    "TODO: Fix this",
    "if all(f.name != self.metric for f in dataclasses.fields(self.evaluator.__class__)):",
    "raise ValueError(f'Invalid metric name: {self.metric}')",
    "Dummy result tracker",
    "Evaluate",
    "After the first evaluation pass the optimal batch and slice size is obtained and saved for re-use",
    "Only check if enough values are already collected",
    "Stop if the result did not improve more than delta for patience epochs.",
    "Update ring buffer",
    "Append to history",
    "-*- coding: utf-8 -*-",
    ": A mapping of training loops' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The batch_size and slice_size should be accessible to outside objects for re-use, e.g. early stoppers.",
    "Clear the ranks from the current evaluator",
    "We need to try slicing, if the evaluation for the batch_size search never succeeded",
    "Since the batch_size search with size 1, i.e. one tuple ((h, r) or (r, t)) scored on all entities,",
    "must have failed to start slice_size search, we start with trying half the entities.",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Due to the caused OOM Runtime Error, the failed model has to be cleared to avoid memory leakage",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "The cache of the previous run has to be freed to allow accurate memory availability estimates",
    "Test if slicing is implemented for the required functions of this model",
    "Split batch",
    "Bind shape",
    "Set all filtered triples to NaN to ensure their exclusion in subsequent calculations",
    "Warn if all entities will be filtered",
    "(scores != scores) yields true for all NaN instances (IEEE 754), thus allowing to count the filtered triples.",
    "Send to device",
    "Ensure evaluation mode",
    "Split evaluators into those which need unfiltered results, and those which require filtered ones",
    "Check whether we need to be prepared for filtering",
    "Check whether an evaluator needs access to the masks",
    "This can only be an unfiltered evaluator.",
    "Prepare for result filtering",
    "Send tensors to device",
    "Prepare batches",
    "Show progressbar",
    "Flag to check when to quit the size probing",
    "Disable gradient tracking",
    "Choosing no progress bar (use_tqdm=False) would still show the initial progress bar without disable=True",
    "batch-wise processing",
    "Predict tail scores once",
    "Create positive filter for all corrupted tails",
    "Create a positive mask with the size of the scores from the positive tails filter",
    "Evaluate metrics on these *unfiltered* tail scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "Predict head scores once",
    "Create positive filter for all corrupted heads",
    "Create a positive mask with the size of the scores from the positive heads filter",
    "Evaluate metrics on these head scores",
    "Filter",
    "The scores for the true triples have to be rewritten to the scores tensor",
    "Evaluate metrics on these *filtered* tail scores",
    "If we only probe sizes we do not need more than one batch",
    "Finalize",
    "-*- coding: utf-8 -*-",
    ": The area under the ROC curve",
    ": The area under the precision-recall curve",
    ": The coverage error",
    "coverage_error: float = field(metadata=dict(",
    "doc='The coverage error',",
    "f=metrics.coverage_error,",
    "))",
    ": The label ranking loss (APS)",
    "label_ranking_average_precision_score: float = field(metadata=dict(",
    "doc='The label ranking loss (APS)',",
    "f=metrics.label_ranking_average_precision_score,",
    "))",
    "#: The label ranking loss",
    "label_ranking_loss: float = field(metadata=dict(",
    "doc='The label ranking loss',",
    "f=metrics.label_ranking_loss,",
    "))",
    "Transfer to cpu and convert to numpy",
    "Ensure that each key gets counted only once",
    "include head_side flag into key to differentiate between (h, r) and (r, t)",
    "Important: The order of the values of an dictionary is not guaranteed. Hence, we need to retrieve scores and",
    "masks using the exact same key order.",
    "TODO how to define a cutoff on y_scores to make binary?",
    "see: https://github.com/xptree/NetMF/blob/77286b826c4af149055237cef65e2a500e15631a/predict.py#L25-L33",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    ": A mapping of evaluators' names to their implementations",
    ": A mapping of results' names to their implementations",
    "-*- coding: utf-8 -*-",
    "The best rank is the rank when assuming all options with an equal score are placed behind the currently",
    "considered. Hence, the rank is the number of options with better scores, plus one, as the rank is one-based.",
    "The worst rank is the rank when assuming all options with an equal score are placed in front of the currently",
    "considered. Hence, the rank is the number of options which have at least the same score minus one (as the",
    "currently considered option in included in all options). As the rank is one-based, we have to add 1, which",
    "nullifies the \"minus 1\" from before.",
    "The average rank is the average of the best and worst rank, and hence the expected rank over all permutations of",
    "the elements with the same score as the currently considered option.",
    "We set values which should be ignored to NaN, hence the number of options which should be considered is given by",
    "The expected rank of a random scoring",
    "The adjusted ranks is normalized by the expected rank of a random scoring",
    "TODO adjusted_worst_rank",
    "TODO adjusted_best_rank",
    ": The mean over all ranks: mean_i r_i. Lower is better.",
    ": The mean over all reciprocal ranks: mean_i (1/r_i). Higher is better.",
    ": The hits at k for different values of k, i.e. the relative frequency of ranks not larger than k.",
    ": Higher is better.",
    ": The mean over all chance-adjusted ranks: mean_i (2r_i / (num_entities+1)). Lower is better.",
    ": Described by [berrendorf2020]_.",
    "Clear buffers",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extend the batch to the number of IDs such that each pair can be combined with all possible IDs",
    "Create a tensor of all IDs",
    "Extend all IDs to the number of pairs such that each ID can be combined with every pair",
    "Fuse the extended pairs with all IDs to a new (h, r, t) triple tensor.",
    ": A dictionary of hyper-parameters to the models that use them",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The instance of the loss",
    ": The default regularizer class",
    ": The default parameters for the default regularizer class",
    ": The instance of the regularizer",
    "Initialize the device",
    "Random seeds have to set before the embeddings are initialized",
    "Loss",
    "TODO: Check loss functions that require 1 and -1 as label but only",
    "Regularizer",
    "The triples factory facilitates access to the dataset.",
    "This allows to store the optimized parameters",
    "Keep track of the hyper-parameters that are used across all",
    "subclasses of BaseModule",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "Enforce evaluation mode",
    "The number of relations stored in the triples factory includes the number of inverse relations",
    "Id of inverse relation: relation + 1",
    "The score_t function requires (entity, relation) pairs instead of (relation, entity) pairs",
    "Extend the hr_batch such that each (h, r) pair is combined with all possible tails",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_t function.",
    "Extend the rt_batch such that each (r, t) pair is combined with all possible heads",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_h function.",
    "Extend the ht_batch such that each (h, t) pair is combined with all possible relations",
    "Calculate the scores for each (h, r, t) triple using the generic interaction function",
    "Reshape the scores to match the pre-defined output shape of the score_r function.",
    "TODO: Why do we need that? The optimizer takes care of filtering the parameters.",
    "Default for relation dimensionality",
    "-*- coding: utf-8 -*-",
    ": A mapping of models' names to their implementations",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Store initial input for error message",
    "All are None",
    "input_channels is None, and any of height or width is None -> set input_channels=1",
    "input channels is not None, and one of height or width is None",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,L)",
    ": If batch normalization is enabled, this is: num_features \u2013 C from an expected input of size (N,C,H,W)",
    "ConvE should be trained with inverse triples",
    "ConvE uses one bias for each entity",
    "Automatic calculation of remaining dimensions",
    "Parameter need to fulfil:",
    "input_channels * embedding_height * embedding_width = embedding_dim",
    "Finalize initialization",
    "embeddings",
    "weights",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Embedding Regularization",
    "For efficient calculation, each of the convolved [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Code to repeat each item successively instead of the entire tensor",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Embeddings",
    "Finalize initialization",
    "Initialise left relation embeddings to unit length",
    "Make sure to call super first",
    "Normalise embeddings of entities",
    "Get embeddings",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "Get embeddings",
    "Project entities",
    "Project entities",
    "Project entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for ComplEx.",
    ": The LP settings used by [trouillon2016]_ for ComplEx.",
    "Finalize initialization",
    "initialize with entity and relation embeddings with standard normal distribution, cf.",
    "https://github.com/ttrouill/complex/blob/dc4eb93408d9a5288c986695b58488ac80b1cc17/efe/models.py#L481-L487",
    "split into real and imaginary part",
    "ComplEx space bilinear product",
    "*: Elementwise multiplication",
    "get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nickel2011]_ for for RESCAL",
    ": According to https://github.com/mnick/rescal.py/blob/master/examples/kinships.py",
    ": a normalized weight of 10 is used.",
    ": The LP settings used by [nickel2011]_ for for RESCAL",
    "Finalize initialization",
    "Get embeddings",
    "shape: (b, d)",
    "shape: (b, d, d)",
    "shape: (b, d)",
    "Compute scores",
    "Regularization",
    "Compute scores",
    "Regularization",
    "Get embeddings",
    "Compute scores",
    "Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [nguyen2018]_ for ConvKB.",
    ": The LP settings used by [nguyen2018]_ for ConvKB.",
    "The interaction model",
    "Finalize initialization",
    "embeddings",
    "Use Xavier initialization for weight; bias to zero",
    "Initialize all filters to [0.1, 0.1, -0.1],",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L34-L36",
    "Output layer regularization",
    "In the code base only the weights of the output layer are used for regularization",
    "c.f. https://github.com/daiquocnguyen/ConvKB/blob/73a22bfa672f690e217b5c18536647c7cf5667f1/model.py#L60-L66",
    "Stack to convolution input",
    "Convolution",
    "Apply dropout, cf. https://github.com/daiquocnguyen/ConvKB/blob/master/model.py#L54-L56",
    "Linear layer for final scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    ": shape: (batch_size, num_entities, d)",
    ": Prepare h: (b, e, d) -> (b, e, 1, 1, d)",
    ": Prepare t: (b, e, d) -> (b, e, 1, d, 1)",
    ": Prepare w: (R, k, d, d) -> (b, k, d, d) -> (b, 1, k, d, d)",
    "h.T @ W @ t, shape: (b, e, k, 1, 1)",
    ": reduce (b, e, k, 1, 1) -> (b, e, k)",
    ": Prepare vh: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare h: (b, e, d) -> (b, e, d, 1)",
    "V_h @ h, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare vt: (R, k, d) -> (b, k, d) -> (b, 1, k, d)",
    ": Prepare t: (b, e, d) -> (b, e, d, 1)",
    "V_t @ t, shape: (b, e, k, 1)",
    ": reduce (b, e, k, 1) -> (b, e, k)",
    ": Prepare b: (R, k) -> (b, k) -> (b, 1, k)",
    "a = f(h.T @ W @ t + Vh @ h + Vt @ t + b), shape: (b, e, k)",
    "prepare u: (R, k) -> (b, k) -> (b, 1, k, 1)",
    "prepare act: (b, e, k) -> (b, e, 1, k)",
    "compute score, shape: (b, e, 1, 1)",
    "reduce",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The regularizer used by [yang2014]_ for DistMult",
    ": In the paper, they use weight of 0.0001, mini-batch-size of 10, and dimensionality of vector 100",
    ": Thus, when we use normalized regularization weight, the normalization factor is 10*sqrt(100) = 100, which is",
    ": why the weight has to be increased by a factor of 100 to have the same configuration as in the paper.",
    ": The LP settings used by [yang2014]_ for DistMult",
    "Finalize initialization",
    "xavier uniform, cf.",
    "https://github.com/thunlp/OpenKE/blob/adeed2c0d2bef939807ed4f69c1ea4db35fd149b/models/DistMult.py#L16-L17",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize embeddings of entities",
    "Bilinear product",
    "*: Elementwise multiplication",
    "Get embeddings",
    "Compute score",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "Get embeddings",
    "Rank against all entities",
    "Only regularize relation embeddings",
    "-*- coding: utf-8 -*-",
    ": a = \\mu^T\\Sigma^{-1}\\mu",
    ": b = \\log \\det \\Sigma",
    ": a = tr(\\Sigma_r^{-1}\\Sigma_e)",
    ": b = (\\mu_r - \\mu_e)^T\\Sigma_r^{-1}(\\mu_r - \\mu_e)",
    ": c = \\log \\frac{det(\\Sigma_e)}{det(\\Sigma_r)}",
    "= sum log (sigma_e)_i - sum log (sigma_r)_i",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Similarity function used for distributions",
    "element-wise covariance bounds",
    "Additional covariance embeddings",
    "Finalize initialization",
    "Constraints are applied through post_parameter_update",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Ensure positive definite covariances matrices and appropriate size by clamping",
    "Get embeddings",
    "Compute entity distribution",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The custom regularizer used by [wang2014]_ for TransH",
    ": The settings used by [wang2014]_ for TransH",
    "embeddings",
    "Finalize initialization",
    "TODO: Add initialization",
    "Make sure to call super first",
    "Normalise the normal vectors by their l2 norms",
    "As described in [wang2014], all entities and relations are used to compute the regularization term",
    "which enforces the defined soft constraints.",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "Get embeddings",
    "Project to hyperplane",
    "Regularization term",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "embeddings",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "TODO: Initialize from TransE",
    "Initialise relation embeddings to unit length",
    "project to relation specific subspace, shape: (b, e, d_r)",
    "ensure constraints",
    "evaluate score function, shape: (b, e)",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    "Construct node neighbourhood mask",
    "Set nodes in batch to true",
    "Compute k-neighbourhood",
    "if the target node needs an embeddings, so does the source node",
    "Create edge mask",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    "pylint: disable=unused-argument",
    "Calculate in-degree, i.e. number of incoming edges",
    ": Interaction model used as decoder",
    ": The blocks of the relation-specific weight matrices",
    ": shape: (num_relations, num_blocks, embedding_dim//num_blocks, embedding_dim//num_blocks)",
    ": The base weight matrices to generate relation-specific weights",
    ": shape: (num_bases, embedding_dim, embedding_dim)",
    ": The relation-specific weights for each base",
    ": shape: (num_relations, num_bases)",
    ": The biases for each layer (if used)",
    ": shape of each element: (embedding_dim,)",
    ": Batch normalization for each layer (if used)",
    ": Activations for each layer (if used)",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Instantiate model",
    "Heuristic",
    "buffering of messages",
    "Save graph using buffers, such that the tensors are moved together with the model",
    "Weights",
    "Finalize initialization",
    "invalidate enriched embeddings",
    "https://github.com/MichSchli/RelationPrediction/blob/c77b094fe5c17685ed138dae9ae49b304e0d8d89/code/encoders/affine_transform.py#L24-L28",
    "Random convex-combination of bases for initialization (guarantees that initial weight matrices are",
    "initialized properly)",
    "We have one additional relation for self-loops",
    "Xavier Glorot initialization of each block",
    "Reset biases",
    "Reset batch norm parameters",
    "Reset activation parameters, if any",
    "use buffered messages if applicable",
    "Bind fields",
    "shape: (num_entities, embedding_dim)",
    "Edge dropout: drop the same edges on all layers (only in training mode)",
    "Get random dropout mask",
    "Apply to edges",
    "Different dropout for self-loops (only in training mode)",
    "If batch is given, compute (num_layers)-hop neighbourhood",
    "Initialize embeddings in the next layer for all nodes",
    "TODO: Can we vectorize this loop?",
    "Choose the edges which are of the specific relation",
    "Only propagate messages on subset of edges",
    "No edges available? Skip rest of inner loop",
    "Get source and target node indices",
    "send messages in both directions",
    "Select source node embeddings",
    "get relation weights",
    "Compute message (b x d) * (d x d) = (b x d)",
    "Normalize messages by relation-specific in-degree",
    "Aggregate messages in target",
    "Self-loop",
    "Apply bias, if requested",
    "Apply batch normalization, if requested",
    "Apply non-linearity",
    "allocate weight",
    "Get blocks",
    "self.bases[i_layer].shape (num_relations, num_blocks, embedding_dim/num_blocks, embedding_dim/num_blocks)",
    "note: embedding_dim is guaranteed to be divisible by num_bases in the constructor",
    "The current basis weights, shape: (num_bases)",
    "the current bases, shape: (num_bases, embedding_dim, embedding_dim)",
    "compute the current relation weights, shape: (embedding_dim, embedding_dim)",
    "Enrich embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Core tensor",
    "Note: we use a different dimension permutation as in the official implementation to match the paper.",
    "Dropout",
    "Finalize initialization",
    "Initialize core tensor, cf. https://github.com/ibalazevic/TuckER/blob/master/model.py#L12",
    "Abbreviation",
    "Compute h_n = DO(BN(h))",
    "Compute wr = DO(W x_2 r)",
    "compute whr = DO(BN(h_n x_1 wr))",
    "Compute whr x_3 t",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "Get embeddings",
    "Compute scores",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Initialise relation embeddings to unit length",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Get embeddings",
    "Get embeddings",
    "Get embeddings",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    ": The regularizer used by [trouillon2016]_ for SimplE",
    ": In the paper, they use weight of 0.1, and do not normalize the",
    ": regularization term by the number of elements, which is 200.",
    ": The power sum settings used by [trouillon2016]_ for SimplE",
    "extra embeddings",
    "Finalize initialization",
    "forward model",
    "Regularization",
    "backward model",
    "Regularization",
    "Note: In the code in their repository, the score is clamped to [-20, 20].",
    "That is not mentioned in the paper, so it is omitted here.",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "The authors do not specify which initialization was used. Hence, we use the pytorch default.",
    "weight initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Compute scores",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "Get embeddings",
    "Embedding Regularization",
    "First layer can be unrolled",
    "Send scores through rest of the network",
    "-*- coding: utf-8 -*-",
    "The dimensions affected by e'",
    "Project entities",
    "r_p (e_p.T e) + e'",
    "Enforce constraints",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Project entities",
    "score = -||h_bot + r - t_bot||_2^2",
    "Head",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "phases randomly between 0 and 2 pi",
    "Make sure to call super first",
    "Normalize relation embeddings",
    "Decompose into real and imaginary part",
    "Rotate (=Hadamard product in complex space).",
    "Workaround until https://github.com/pytorch/pytorch/issues/30704 is fixed",
    "Get embeddings",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "Get embeddings",
    "r expresses a rotation in complex plane.",
    "The inverse rotation is expressed by the complex conjugate of r.",
    "The score is computed as the distance of the relation-rotated head to the tail.",
    "Equivalently, we can rotate the tail by the inverse relation, and measure the distance to the head, i.e.",
    "|h * r - t| = |h - conj(r) * t|",
    "Rank against all entities",
    "Compute scores",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Global entity projection",
    "Global relation projection",
    "Global combination bias",
    "Global combination bias",
    "Finalize initialization",
    "Get embeddings",
    "Compute score",
    "Get embeddings",
    "Rank against all entities",
    "Get embeddings",
    "Rank against all entities",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    "Finalize initialization",
    "Make sure to call super first",
    "Normalize entity embeddings",
    "Initialisation, cf. https://github.com/mnick/scikit-kge/blob/master/skge/param.py#L18-L27",
    "Circular correlation of entity embeddings",
    "complex conjugate, a_fft.shape = (batch_size, num_entities, d', 2)",
    "Hadamard product in frequency domain",
    "inverse real FFT, shape: (batch_size, num_entities, d)",
    "inner product with relation embedding",
    "Embedding Regularization",
    "Embedding Regularization",
    "Embedding Regularization",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Finalize initialization",
    "Get embeddings",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "compare with all t's",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Concatenate them",
    "Predict t embedding",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "Embedding Regularization",
    "Extend each rt_batch of \"r\" with shape [rt_batch_size, dim] to [rt_batch_size, dim * num_entities]",
    "Extend each h with shape [num_entities, dim] to [rt_batch_size * num_entities, dim]",
    "h = torch.repeat_interleave(h, rt_batch_size, dim=0)",
    "Extend t",
    "Concatenate them",
    "Predict t embedding",
    "For efficient calculation, each of the calculated [h, r] rows has only to be multiplied with one t row",
    "The results have to be realigned with the expected output of the score_h function",
    "The application of the sigmoid during training is automatically handled by the default loss.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default loss function class",
    ": The default parameters for the default loss function class",
    "Literal",
    "num_ent x num_lit",
    "Number of columns corresponds to number of literals",
    "Literals",
    "End literals",
    "-*- coding: utf-8 -*-",
    "TODO: Check entire build of the model",
    ": The default strategy for optimizing the model's hyper-parameters",
    ": The default parameters for the default loss function class",
    "Embeddings",
    "Number of columns corresponds to number of literals",
    "apply dropout",
    "-, because lower score shall correspond to a more plausible triple.",
    "TODO check if this is the same as the BaseModule",
    "Choose y = -1 since a smaller score is better.",
    "In TransE for example, the scores represent distances",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Bind number of negatives to sample",
    "Equally corrupt head and tail",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Sample random entities as replacement",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Corrupt tails",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "-*- coding: utf-8 -*-",
    ": A mapping of negative samplers' names to their implementations",
    "-*- coding: utf-8 -*-",
    ": The default strategy for optimizing the negative sampler's hyper-parameters",
    "Preprocessing: Compute corruption probabilities",
    "compute tph, i.e. the average number of tail entities per head",
    "compute hpt, i.e. the average number of head entities per tail",
    "Set parameter for Bernoulli distribution",
    "Bind number of negatives to sample",
    "Copy positive batch for corruption.",
    "Do not detach, as no gradients should flow into the indices.",
    "Decide whether to corrupt head or tail",
    "Tails are corrupted if heads are not corrupted",
    "Randomly sample corruption",
    "Replace heads \u2013 To make sure we don't replace the head by the original value",
    "we shift all values greater or equal than the original value by one up",
    "for that reason we choose the random value from [0, num_entities -1]",
    "Replace tails",
    "-*- coding: utf-8 -*-",
    "TODO what happens if already exists?",
    "TODO incorporate setting of random seed",
    "pipeline_kwargs=dict(",
    "random_seed=random.randint(1, 2 ** 32 - 1),",
    "),",
    "Add dataset to current_pipeline",
    "Add loss function to current_pipeline",
    "Add regularizer to current_pipeline",
    "Add optimizer to current_pipeline",
    "Add training approach to current_pipeline",
    "Add training kwargs and kwargs_ranges",
    "Add evaluation",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A factory wrapping the training triples",
    ": A factory wrapping the testing triples, that share indexes with the training triples",
    ": A factory wrapping the validation triples, that share indexes with the training triples",
    ": All data sets should take care of inverse triple creation",
    ": The actual instance of the training factory, which is exposed to the user through `training`",
    ": The actual instance of the testing factory, which is exposed to the user through `testing`",
    ": The actual instance of the validation factory, which is exposed to the user through `validation`",
    "don't call this function by itself. assumes called through the `validation`",
    "property and the _training factory has already been loaded",
    "see https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content",
    "pattern from https://stackoverflow.com/a/39217788/5775947",
    "TODO replace this with the new zip remote dataset class",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    ": A mapping of data sets' names to their classes",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "TODO update docs with table and CLI wtih generator",
    "-*- coding: utf-8 -*-",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "Misc.",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "Misc.",
    "Will trigger Optuna to set the state of the trial as failed",
    ": The :mod:`optuna` study object",
    ": The objective class, containing information on preset hyper-parameters and those to optimize",
    "Output study information",
    "Output all trials",
    "Output best trial as pipeline configuration file",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "6. Misc",
    "Optuna Study Settings",
    "Optuna Optimization Settings",
    "0. Metadata/Provenance",
    "1. Dataset",
    "FIXME difference between dataset class and string",
    "FIXME how to handle if dataset or factories were set? Should have been",
    "part of https://github.com/mali-git/POEM_develop/pull/483",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "1. Dataset",
    "2. Model",
    "3. Loss",
    "4. Regularizer",
    "5. Optimizer",
    "6. Training Loop",
    "7. Training",
    "8. Evaluation",
    "Optuna Misc.",
    "Pipeline Misc.",
    "Invoke optimization of the objective function.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-"
  ],
  "v0.0.26": [
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "",
    "Configuration file for the Sphinx documentation builder.",
    "",
    "This file does only contain a selection of the most common options. For a",
    "full list see the documentation:",
    "http://www.sphinx-doc.org/en/master/config",
    "-- Path setup --------------------------------------------------------------",
    "If extensions (or modules to document with autodoc) are in another directory,",
    "add these directories to sys.path here. If the directory is relative to the",
    "documentation root, use os.path.abspath to make it absolute, like shown here.",
    "",
    "sys.path.insert(0, os.path.abspath('..'))",
    "-- Mockup PyTorch to exclude it while compiling the docs--------------------",
    "from unittest.mock import Mock",
    "sys.modules['numpy'] = Mock()",
    "sys.modules['numpy.linalg'] = Mock()",
    "sys.modules['scipy'] = Mock()",
    "sys.modules['scipy.optimize'] = Mock()",
    "sys.modules['scipy.interpolate'] = Mock()",
    "sys.modules['scipy.sparse'] = Mock()",
    "sys.modules['scipy.ndimage'] = Mock()",
    "sys.modules['scipy.ndimage.filters'] = Mock()",
    "sys.modules['tensorflow'] = Mock()",
    "sys.modules['theano'] = Mock()",
    "sys.modules['theano.tensor'] = Mock()",
    "sys.modules['torch'] = Mock()",
    "sys.modules['torch.optim'] = Mock()",
    "sys.modules['torch.nn'] = Mock()",
    "sys.modules['torch.nn.init'] = Mock()",
    "sys.modules['torch.autograd'] = Mock()",
    "sys.modules['sklearn'] = Mock()",
    "sys.modules['sklearn.model_selection'] = Mock()",
    "sys.modules['sklearn.utils'] = Mock()",
    "-- Project information -----------------------------------------------------",
    "The full version, including alpha/beta/rc tags.",
    "The short X.Y version.",
    "-- General configuration ---------------------------------------------------",
    "If your documentation needs a minimal Sphinx version, state it here.",
    "",
    "needs_sphinx = '1.0'",
    "Add any Sphinx extension module names here, as strings. They can be",
    "extensions coming with Sphinx (named 'sphinx.ext.*') or your custom",
    "ones.",
    "Add any paths that contain templates here, relative to this directory.",
    "The suffix(es) of source filenames.",
    "You can specify multiple suffix as a list of string:",
    "",
    "source_suffix = ['.rst', '.md']",
    "The master toctree document.",
    "The language for content autogenerated by Sphinx. Refer to documentation",
    "for a list of supported languages.",
    "",
    "This is also used if you do content translation via gettext catalogs.",
    "Usually you set \"language\" from the command line for these cases.",
    "List of patterns, relative to source directory, that match files and",
    "directories to ignore when looking for source files.",
    "This pattern also affects html_static_path and html_extra_path.",
    "The name of the Pygments (syntax highlighting) style to use.",
    "-- Options for HTML output -------------------------------------------------",
    "The theme to use for HTML and HTML Help pages.  See the documentation for",
    "a list of builtin themes.",
    "",
    "Theme options are theme-specific and customize the look and feel of a theme",
    "further.  For a list of options available for each theme, see the",
    "documentation.",
    "",
    "html_theme_options = {}",
    "Add any paths that contain custom static files (such as style sheets) here,",
    "relative to this directory. They are copied after the builtin static files,",
    "so a file named \"default.css\" will overwrite the builtin \"default.css\".",
    "html_static_path = ['_static']",
    "Custom sidebar templates, must be a dictionary that maps document names",
    "to template names.",
    "",
    "The default sidebars (for documents that don't match any pattern) are",
    "defined by theme itself.  Builtin themes are using these templates by",
    "default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",
    "'searchbox.html']``.",
    "",
    "html_sidebars = {}",
    "-- Options for HTMLHelp output ---------------------------------------------",
    "Output file base name for HTML help builder.",
    "-- Options for LaTeX output ------------------------------------------------",
    "The paper size ('letterpaper' or 'a4paper').",
    "",
    "'papersize': 'letterpaper',",
    "The font size ('10pt', '11pt' or '12pt').",
    "",
    "'pointsize': '10pt',",
    "Additional stuff for the LaTeX preamble.",
    "",
    "'preamble': '',",
    "Latex figure (float) alignment",
    "",
    "'figure_align': 'htbp',",
    "Grouping the document tree into LaTeX files. List of tuples",
    "(source start file, target name, title,",
    "author, documentclass [howto, manual, or own class]).",
    "-- Options for manual page output ------------------------------------------",
    "One entry per manual page. List of tuples",
    "(source start file, name, description, authors, manual section).",
    "-- Options for Texinfo output ----------------------------------------------",
    "Grouping the document tree into Texinfo files. List of tuples",
    "(source start file, target name, title, author,",
    "dir menu entry, description, category)",
    "-- Options for Epub output -------------------------------------------------",
    "Bibliographic Dublin Core info.",
    "The unique identifier of the text. This can be a ISBN number",
    "or the project homepage.",
    "",
    "epub_identifier = ''",
    "A unique identification for the text.",
    "",
    "epub_uid = ''",
    "A list of files that should not be packed into the epub file.",
    "-- Extension configuration -------------------------------------------------",
    "-- Options for intersphinx extension ---------------------------------------",
    "Example configuration for intersphinx: refer to the Python standard library.",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "10 % of training set will be used as a test set",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Load Drugbank",
    "Load BioPlex 2.0",
    "-*- coding: utf-8 -*-",
    ": Functions for specifying exotic resources with a given prefix",
    "KG embedding model",
    "Model names",
    "Evaluator",
    "Output paths",
    "Device related",
    "ML params",
    "TransH related",
    "ConvE related",
    "OPTIMIZER",
    "Further Constants",
    "Pipeline outcome parameters",
    "-----------------Command line interface messages-----------------",
    "-*- coding: utf-8 -*-",
    ": The configuration used to train the KGE model",
    ": The pipeline used to train the KGE model",
    ": The results of training the KGE model",
    "In HPO model initial configuration is different from final configurations, that's why we differentiate",
    "Save trained model",
    "Export experimental artifacts",
    "-*- coding: utf-8 -*-",
    "Load configuration file",
    "Load entity to id mapping",
    "Load relation to id mapping",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Device selection",
    "Entity dimensions",
    "Embeddings",
    "num_features \u2013 C from an expected input of size (N,C,L)",
    "num_features \u2013 C from an expected input of size (N,C,H,W)",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "Class 0 represents false fact and class 1 represents true fact",
    "batch_size, num_input_channels, width, height",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "batch_size, num_input_channels, 2*height, width",
    "(N,C_out,H_out,W_out)",
    "batch_size, num_output_channels * (2 * height - kernel_height + 1) * (width - kernel_width + 1)",
    "-*- coding: utf-8 -*-",
    "Embeddings",
    "FIXME @mehdi why aren't the right relation embeddings initialized?",
    "triples = torch.tensor(triples, dtype=torch.long, device=self.device)",
    "Normalise embeddings of entities",
    "-*- coding: utf-8 -*-",
    "Embeddings",
    "triples = torch.tensor(triples, dtype=torch.long, device=self.device)",
    "Compute score and transform result to 1D tensor",
    "scores = torch.bmm(torch.transpose(h_emb, 1, 2), M)  # h^T M",
    "scores = torch.bmm(scores, t_emb)  # (h^T M) h",
    "scores = score.view(-1, 1)",
    "-*- coding: utf-8 -*-",
    "triples = torch.tensor(triples, dtype=torch.long, device=self.device)",
    "Normalize embeddings of entities",
    "Add the vector element wise",
    "-*- coding: utf-8 -*-",
    "A simple lookup table that stores embeddings of a fixed dictionary and size",
    "TODO: Add initialization",
    "Add the vector element wise",
    "Normalise the normal vectors by their l2 norms",
    "Shape: (batch_size, 1, embedding_dimension)",
    "Reshape relation embeddings to the same shape of the projected entities",
    "Reshape relation embeddings to the same shape of the projected entities",
    "-*- coding: utf-8 -*-",
    "Embeddings",
    "triples = torch.tensor(triples, dtype=torch.long, device=self.device)",
    "-*- coding: utf-8 -*-",
    "A simple lookup table that stores embeddings of a fixed dictionary and size",
    "TODO: Add initialization",
    "Add the vector element wise",
    "Normalise the normal vectors by their l2 norms",
    "Shape: (batch_size, 1, embedding_dimension)",
    "Reshape relation embeddings to the same shape of the projected entities",
    "Reshape relation embeddings to the same shape of the projected entities",
    "-*- coding: utf-8 -*-",
    "TODO: max_norm < 1.",
    "max_norm = 1 according to the paper",
    "Embeddings",
    "max_norm = 1 according to the paper",
    "Add the vector element wise",
    "triples = torch.tensor(triples, dtype=torch.long, device=self.device)",
    "Project entities into relation space",
    "-*- coding: utf-8 -*-",
    ": A mapping from KGE model names to KGE model classes",
    "-*- coding: utf-8 -*-",
    "Embeddings",
    "Normalize embeddings of entities",
    "Add the vector element wise",
    "-*- coding: utf-8 -*-",
    ": Embeddings for relations in the knowledge graph",
    "triples = torch.tensor(triples, dtype=torch.long, device=self.device)",
    "-*- coding: utf-8 -*-",
    "Embeddings",
    "A simple lookup table that stores embeddings of a fixed dictionary and size",
    "triples = torch.tensor(triples, dtype=torch.long, device=self.device)",
    "Add the vector element wise",
    "-*- coding: utf-8 -*-",
    "Device selection",
    "Loss",
    "Entity dimensions",
    ": The number of entities in the knowledge graph",
    ": The number of unique relation types in the knowledge graph",
    ": The dimension of the embeddings to generate",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Step 1: Ask whether to evaluate the model",
    "Step 2: Specify test set, if is_evaluation_mode==True",
    "Ask whether to use filtered negative triples",
    "Query number of HPO iterations",
    "Step 1: Welcome + Intro",
    "Step 2: Ask for training file",
    "Step 3: Ask for execution mode",
    "Step 4: Ask for model",
    "Step 5: Query parameters depending on the selected execution mode",
    "Step 5.5: Prompt for evaluation parameters depending on the selected execution mode",
    "Step 6: Please select a random seed",
    "Step 7: Query device to train on",
    "Step 8: Define output directory",
    "config_path = os.path.join(config[OUTPUT_DIREC], 'configuration.json')",
    "with open(config_path, 'w') as file:",
    "json.dump(config, file, indent=2)",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm as scoring function",
    "Step 4: Query learning rate",
    "Step 5: Query batch size",
    "Step 6: Query number of epochs",
    "Step 1: Query embedding dimensions",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norms to use as scoring function",
    "Step 4: Query L_p norms for normalizing the entities",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm as scoring function",
    "Step 4: Query L_p norm for normalizing the entities",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "Step 1: Query embedding dimensions",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norms to use as scoring function",
    "Step 4: Query L_p norms for normalizing the entities",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension for entities",
    "Step 2: Query embedding dimension for relations",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm as scoring function",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "Step 1: Query embedding dimensions for entities",
    "Step 2: Query embedding dimensions for relations",
    "Step 3: Query margin losses",
    "Step 4: Query L_p norms to use as scoring function",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm as scoring function",
    "Step 4: Query L_p norm for normalizing the entities",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "Step 1: Query embedding dimensions",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norms to use as scoring function",
    "Step 4: Query L_p norms for normalizing the entities",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query margin loss",
    "Step 3: Query learning rate",
    "Step 4: Query batch size",
    "Step 5: Query number of epochs",
    "Step 1: Query embedding dimensions",
    "Step 2: Query margin loss",
    "Step 3: Query learning rate",
    "Step 4: Query batch size",
    "Step 5: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm as scoring function",
    "Step 4: Query weight for the soft constraints",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "Step 1: Query embedding dimensions",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norms to use as scoring function",
    "Step 4: Query weight for the soft constraints",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension for entities",
    "Step 2: Query embedding dimension for relations",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm as scoring function",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "Step 1: Query embedding dimensions for entities",
    "Step 2: Query embedding dimensions for relations",
    "Step 3: Query margin losses",
    "Step 4: Query L_p norms to use as scoring function",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm for normalizing the entities",
    "print_entity_normalization_message()",
    "entity_normalization_norm = select_integer_value(print_msg=ENTITIES_NORMALIZATION_PRINT_MSG,",
    "prompt_msg=ENTITIES_NORMALIZATION_PROMPT_MSG,",
    "error_msg=ENTITIES_NORMALIZATION_ERROR_MSG)",
    "config[NORM_FOR_NORMALIZATION_OF_ENTITIES] = entity_normalization_norm",
    "print_section_divider()",
    "Step 4: Query learning rate",
    "Step 5: Query batch size",
    "Step 6: Query number of epochs",
    "Step 1: Query embedding dimensions",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norms for normalizing the entities",
    "print_hpo_entity_normalization_norms_message()",
    "entity_normalization_norm = select_positive_integer_values(",
    "print_msg=NORMS_FOR_NORMALIZATION_OF_ENTITIES_PRINT_MSG,",
    "prompt_msg=NORMS_FOR_NORMALIZATION_OF_ENTITIES_PROMPT_MSG,",
    "error_msg=NORMS_FOR_NORMALIZATION_OF_ENTITIES_ERROR_MSG",
    ")",
    "config[NORM_FOR_NORMALIZATION_OF_ENTITIES] = entity_normalization_norm",
    "print_section_divider()",
    "Step 4: Query learning rate",
    "Step 5: Query batch size",
    "Step 6: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query height and width",
    "Step 3: Query number of input channels",
    "Step 4: Query number of output channels",
    "Step 4: Query kernel height",
    "Step 5: Query kernel width",
    "Step 6: Query dropout for input layer",
    "Step 7: Query dropout for output layer",
    "Step 8: Query feature map dropout for output layer",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "Step 1: Query embedding dimension",
    "Step 2: Query height and width",
    "Step 3: Query number of input channels",
    "Step 4: Query number of output channels",
    "Step 4: Query kernel height",
    "Step 5: Query kernel width",
    "Step 6: Query dropout for input layer",
    "Step 7: Query dropout for output layer",
    "Step 8: Query feature map dropout for output layer",
    "Step 9: Query learning rate",
    "Step 10: Query batch size",
    "Step 11: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "Step 1: Query embedding dimension",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norm as scoring function",
    "Step 4: Query L_p norm for normalizing the entities",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "Step 1: Query embedding dimensions",
    "Step 2: Query margin loss",
    "Step 3: Query L_p norms to use as scoring function",
    "Step 4: Query L_p norms for normalizing the entities",
    "Step 5: Query learning rate",
    "Step 6: Query batch size",
    "Step 7: Query number of epochs",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "model_name in {TRANS_E_NAME, TRANS_H_NAME, TRANS_D_NAME, TRANS_R_NAME, DISTMULT_NAME, UM_NAME, SE_NAME, ERMLP_NAME, RESCAL_NAME}",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old instance",
    "log.info(\"Epoch %s took %s seconds \\n\" % (str(epoch), str(round(stop - start))))",
    "Track epoch loss",
    "TODO: Make sure that batch = num_pos + num_negs",
    "num_negatives = batch_size - num_positives",
    "TODO: Remove original subject and object from entity set",
    "Recall that torch *accumulates* gradients. Before passing in a",
    "new instance, you need to zero out the gradients from the old",
    "instance",
    "log.info(\"Epoch %s took %s seconds \\n\" % (str(epoch), str(round(stop - start))))",
    "Track epoch loss",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Initialize KG embedding model",
    "Prepare Output",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Extract current test tuple: Either (subject,predicate) or (predicate,object)",
    "Copy current test tuple",
    "TODO: Check",
    "Get index of first occurrence that fulfills the condition",
    "Compute hits@k for k in {1,3,5,10}",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Note: Unique changes the order",
    "-*- coding: utf-8 -*-",
    "-*- coding: utf-8 -*-",
    "Sample params which are dependent on each other",
    "Sample hyper-params",
    "Configure defined model",
    "Evaluate trained model",
    "TODO: Define HPO metric",
    "-*- coding: utf-8 -*-"
  ]
}